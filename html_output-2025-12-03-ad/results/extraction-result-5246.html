<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5246 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5246</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5246</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-229923446</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.15793v1.pdf" target="_blank">Promoting Graph Awareness in Linearized Graph-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> Generating text from structured inputs, such as meaning representations or RDF triples, has often involved the use of specialized graph-encoding neural networks. However, recent applications of pretrained transformers to linearizations of graph inputs have yielded state-of-the-art generation results on graph-to-text tasks. Here, we explore the ability of these linearized models to encode local graph structures, in particular their invariance to the graph linearization strategy and their ability to reconstruct corrupted inputs. Our findings motivate solutions to enrich the quality of models' implicit graph encodings via scaffolding. Namely, we use graph-denoising objectives implemented in a multi-task text-to-text framework. We find that these denoising scaffolds lead to substantial improvements in downstream generation in low-resource settings.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5246.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5246.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PENMAN-CANONICAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PENMAN canonical linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard human-created linearization format for AMR that writes AMR graphs as PENMAN trees with a canonical ordering used in corpora; concise and commonly used to fit transformer context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PENMAN canonical linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert an AMR graph to a parenthetical PENMAN tree following the canonical (annotator-prescribed) ordering: nodes, edge labels, and parentheses are serialized into a single token sequence (variables replaced, word senses often removed).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact (concise compared to triple sets), preserves parenthetical structure and edge labels, tends to leak information about sentence/order correlations in corpora (annotation-induced ordering), fits transformer input length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation (LDC2017T10) and robustness tests evaluating generation when alternative linearizations are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU (primary). Reported in paper as baseline for models trained on canonical linearization; specific canonical BLEU values are given in paper tables (used as reference).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Canonical PENMAN linearization is the standard baseline; pretrained transformer models finetuned on canonical PENMAN yield strong generation and typically best performance at evaluation time when canonical ordering is used. However, models trained only on canonical inputs are not invariant to alternate linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Annotator ordering can correlate with target sentence order, enabling models to shortcut; models trained on canonical order perform poorly when evaluated on meaning-preserving alternative linearizations (sensitive to token order).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5246.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5246.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RECONFIGURE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Penman RECONFIGURE linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization operation that constructs a tree from an AMR graph while ignoring canonical ordering except for the top node; can change edge label directions (ARG0 -> ARG0-of) and reorders components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RECONFIGURE (PENMAN-based reconfiguration)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use the Penman library to create a valid PENMAN traversal of the AMR graph that discards canonical ordering (except preserving the top node), producing a different but meaning-preserving serialized tree representation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Meaning-preserving alternative linearization; changes relative order of subtrees and can reverse edge labels; retains parenthetical and edge-label tokens but in different order; exposes invariance (or lack thereof) of models to ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation robustness: evaluate models trained on canonical linearizations when provided RECONFIGURE linearizations; used as auxiliary input for reordering scaffold.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU (used to measure drop or recovery in generation performance). Paper reports adversarial training and scaffolding results when evaluating on RECONFIGURE variants (quantitative values in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Used as an adversarial alternative to canonical PENMAN; models trained only on canonical perform worse on RECONFIGURE; adversarial training (exposing model to RECONFIGURE during training) improves robustness with minor cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although RECONFIGURE retains graph content, models pretrained on text are sensitive to token order; reconfiguration exposes this fragility and motivates adversarial training or scaffolding to encourage order-agnostic graph encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5246.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5246.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RANDOMIZE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RANDOMIZED triple/tree linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more drastic PENMAN-based linearization constructed from randomized ordering of triples or traversal, disregarding canonical order information entirely to produce varied linearized sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RANDOMIZE linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct a tree or linearization by randomizing the order of triples/nodes/edges (via the Penman library) to produce a valid traversal that is meaning-preserving but has no systematic correspondence to canonical ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR (and randomized RDF triple orders for WebNLG experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Meaning-preserving but high variation in token order; breaks correlations between canonical order and sentence order; intended to stress-test linearization-invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation robustness (evaluate models trained on canonical inputs) and training augmentation (adversarial training uses RANDOMIZE at epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU drops observed when evaluating canonical-trained models on RANDOMIZE; adversarially trained models recover performance with only minor cost. Paper notes epochs-to-40-BLEU: CANONICAL/RECONFIGURE/RANDOMIZE reach 40 BLEU at 2/3/5 epochs respectively (training anecdote).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>RANDOMIZE presents a harder generalization than RECONFIGURE; adversarial exposure to RANDOMIZE helps models generalize and sometimes still yields generation outperforming graph-transformer baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Large pretrained transformers retain sensitivity to token order from pretraining; even models trained with randomized inputs still perform best on canonical evaluation, indicating residual bias toward canonical ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5246.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5246.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF-TOK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDF triple tokenized linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple linearization for RDF triple sets used in WebNLG: prepend special tokens to triple components (<rel>, <S>, <V>, <O>) and concatenate triples separated by dedicated tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Tokenized RDF triple serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each triple, emit a fixed sequence of special tokens and component tokens (e.g., <rel> <S> SUBJECT <V> VERBPHRASE <O> OBJECT) and concatenate multiple triples with separators; adversarial variant randomizes triple order.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / knowledge-graph subsets (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explicit component markers make roles (subject, relation, object) explicit; simple, repetitive structure; relatively low sentence complexity in dataset leads to lower perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Data-to-text generation on WebNLG; robustness tested by randomizing triple orders at evaluation/training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU (with beam search decoding); paper shows negative impact when canonical ordering is permuted and improvements with adversarial training. Exact numeric values are presented in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Performs well with pretrained transformers; still sensitive to triple order but less complex than AMR; adversarial training improves robustness similarly to AMR experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Randomizing relations reduces performance for models trained on a fixed ordering; simpler graph structure means less dramatic degradation but still non-negligible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5246.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5246.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADV-LIN-TRAIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial linearization training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training procedure that presents different (adversarially permuted) linearizations of the same graph to the model across epochs to encourage order-agnostic graph encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adversarial linearization exposure</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each training epoch, alter the linearization presented for each graph (canonical, reconfigured, randomized variants) so the model cannot overfit to a single token ordering; effectively data-augment the input linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR and RDF triple sets (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Encourages invariance to linearization strategy, reduces overfitting to annotator-induced order, slightly increases training time (more epochs for some variants), simple to implement with Penman/randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Robustness of graph-to-text generation across linearizations; generation quality measured on canonical and alternative linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU: adversarial training improves robustness with only minor reduction in canonical performance; example: epochs-to-40-BLEU for AMR increased from 2 (canonical) to 3 (reconfigure) and 5 (randomize) but final BLEU comparable; exact BLEU numbers in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Improves over single-linearization training and reduces gap vs. graph-encoding models; remains simpler than designing graph-specific encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not fully erase sensitivity to canonical ordering (models still often perform best on canonical evaluation); may not entirely overcome pretraining-induced token-order biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5246.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5246.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MASK-GRAPH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Graph Modeling (graph-denoising)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of graph-denoising objectives applied to linearized graphs in a multi-task text-to-text framework that mask portions of the linearized graph and require reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Masked graph modeling (graph-denoising scaffold)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply masking to tokens in the linearized graph such that ~15% of tokens are masked overall. Variants mask all tokens, only graph components (edge labels and parentheses), or only semantic nodes; training alternates between graph-to-text and denoising objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR (and general linearized graph inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Promotes internal graph encoding by forcing the model to infer missing graph elements; analogous to MLM/MASS objectives; simple to implement on tokenized linearizations; variants similarly effective.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation, with emphasis on low-resource settings (subsets of data: n = 500, 1000, 5000, 10000, full). Also used as proxy measure of graph-encoding quality via scaffolding loss correlation with MF-score M.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU, BertScore, MF-score M (meaning fidelity). Reported: substantial BLEU improvements in low-resource (n=1000) relative to baseline; paper cites that with <3% of full data results exceed prior state-of-the-art GNN (Ribeiro et al. 2019 BLEU 27.37). On full data, gains are minor and within std. deviation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperforms baseline pretrained linearized model in low-resource regimes; yields similar absolute improvements to more complex graph-aware methods but with less model complexity; on full data, benefits diminish compared to large supervised data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Less beneficial when large training sets are available (data diversity supplants scaffolding); different masking strategies performed similarly — no large wins from more complex graph-aware masking; requires careful tuning of task mixing probability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5246.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5246.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REORDER-SCAFF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph reordering scaffold (reconstruct canonical)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A denoising-style objective that gives the model a reconfigured or randomized linearization and tasks it with reconstructing the canonical linearization to encourage learning of graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Reordering-from-reconfigured scaffold</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Provide a model with a non-canonical (RECONFIGURE or RANDOMIZE) linearization and require it to output the canonical PENMAN linearization; trained jointly with graph-to-text objective in a multi-task framework.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explicitly trains the model to reverse arbitrary traversals to canonical form, encouraging representation of underlying graph connectivity rather than surface order; functions as data augmentation due to nondeterministic reconfiguration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation and measure of implicit graph encoding quality via correlation between scaffolding loss and MF-score M.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU and MF-score M; paper finds reordering loss is a significant predictor (negative relationship) of M-score in regression analysis (Pearson ρ reported between scaffolding loss and M). BLEU improvements observed in low-resource settings; exact numeric BLEU gains in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Among scaffolding objectives, reordering-from-reconfigured is one of the top-performing methods in low-resource; explains semantic fidelity better than alternatives in regression analysis, and provides gains comparable to other graph-aware approaches but with less complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>When training data is abundant, benefit diminishes; training to reconstruct canonical order may not fully eliminate residual ordering biases from pretraining; simultaneous generation of surface text and reordering did not improve results in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5246.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5246.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRETRAINED-LIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained transformer finetuned on linearized graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finetune large pretrained transformer LMs (e.g., T5, GPT-2) on pairs of linearized graph inputs and target surface text, yielding state-of-the-art graph-to-text generation performance without explicit graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Pretrained linearized transformer finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use an off-the-shelf pretrained sequence-to-sequence transformer (e.g., T5-Base/Large or GPT-2) and finetune it on (linearized graph string, target sentence) pairs; linearized input can be PENMAN or tokenized triples.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR and RDF triples (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages rich language priors to produce fluent text; can implicitly encode graph structure from linearized sequences; simple pipeline (no GNN) and strong empirical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text (LDC2017T10) and WebNLG data-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU (primary), BertScore, MF-score M (meaning fidelity), perplexity (for dataset stats). Reported: pretrained linearized models substantially outperform specialized GNN/graph-transformer encoders on these tasks in multiple prior works and in this paper's baselines (exact BLEU values in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperforms graph-encoding architectures (GNNs, graph transformers) across metrics in recent comparisons; however, sensitive to the choice/order of linearization and benefits from scaffolding/adversarial exposure to alternative linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Despite strong generation quality, models can be semantically transgressive if graph structural info is not adequately encoded; sensitive to the particular linearization ordering; may require auxiliary objectives to improve fidelity in low-resource settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5246.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5246.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEQ2SEQ-LIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence linearization (from-scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early approach that linearizes graphs into sequences and trains sequence-to-sequence models from scratch (e.g., LSTM-based) to map serialized graphs to text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Seq2Seq linearization (non-pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize graphs (AMR, triples) into linear token sequences and train encoder-decoder architectures from random initialization (LSTMs, early seq2seq) to generate surface text; does not rely on large pretrained language models.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR, RDF triples (various)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Demonstrates feasibility of linearization approach but historically underperformed GNNs and later pretrained transformers; simpler pipeline but lacks language priors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation and other graph-to-text tasks (earlier literature).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU and related generation metrics (historically lower than GNNs and pretrained transformers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Initially outperformed by graph-based encoders; later, pretrained linearized transformers surpassed both seq2seq-from-scratch and graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Poorer performance without large pretraining; susceptible to overfitting to linearization order; lacks powerful language priors that pretrained models provide.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5246.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5246.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRAPH-ENC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-encoding neural networks (GNNs / graph transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectures that explicitly encode graph structure (message-passing GNNs, graph transformers) as model inductive bias to directly represent nodes and edges for graph-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-encoding architectures (GNNs, graph transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent graph inputs as structured objects (adjacency, node features) and use graph neural networks or graph-aware transformer modifications to encode node relationships explicitly prior to decoding to text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR, RDF graphs, other knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explicit structural bias, direct modeling of local graph topology, interpretability of message passing, often more sample-efficient in some settings but historically outperformed by pretrained linearized transformers on generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text, WebNLG data-to-text; used as baselines and for comparison of fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU; example: prior state-of-the-art GNN reported BLEU 27.37 (Ribeiro et al., 2019) referenced as baseline that masked-graph scaffolding surpassed in low-resource.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Pretrained linearized transformers now often outperform specialized graph encoders in BLEU and human evaluations; some graph-aware losses/architectural additions narrow the gap but increase complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>More complex architectures and training; still valuable for explicit structure modeling but slower to train or require additional engineering relative to simple linearization + pretrained LM finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>GPT-too: A language-model-first approach for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Enhancing AMR-to-text generation with dual graph representations <em>(Rating: 2)</em></li>
                <li>Penman: An opensource library and tool for AMR graphs <em>(Rating: 2)</em></li>
                <li>Generating English from Abstract Meaning Representations <em>(Rating: 1)</em></li>
                <li>Controllable meaning representation to text generation: Linearization and data augmentation strategies <em>(Rating: 1)</em></li>
                <li>The WebNLG challenge: Generating text from RDF data <em>(Rating: 2)</em></li>
                <li>AMR-to-text generation with graph transformer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5246",
    "paper_id": "paper-229923446",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "PENMAN-CANONICAL",
            "name_full": "PENMAN canonical linearization",
            "brief_description": "A standard human-created linearization format for AMR that writes AMR graphs as PENMAN trees with a canonical ordering used in corpora; concise and commonly used to fit transformer context windows.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "PENMAN canonical linearization",
            "representation_description": "Convert an AMR graph to a parenthetical PENMAN tree following the canonical (annotator-prescribed) ordering: nodes, edge labels, and parentheses are serialized into a single token sequence (variables replaced, word senses often removed).",
            "graph_type": "Abstract Meaning Representation (AMR)",
            "representation_properties": "Compact (concise compared to triple sets), preserves parenthetical structure and edge labels, tends to leak information about sentence/order correlations in corpora (annotation-induced ordering), fits transformer input length limits.",
            "evaluation_task": "AMR-to-text generation (LDC2017T10) and robustness tests evaluating generation when alternative linearizations are presented.",
            "performance_metrics": "BLEU (primary). Reported in paper as baseline for models trained on canonical linearization; specific canonical BLEU values are given in paper tables (used as reference).",
            "comparison_to_other_representations": "Canonical PENMAN linearization is the standard baseline; pretrained transformer models finetuned on canonical PENMAN yield strong generation and typically best performance at evaluation time when canonical ordering is used. However, models trained only on canonical inputs are not invariant to alternate linearizations.",
            "limitations_or_challenges": "Annotator ordering can correlate with target sentence order, enabling models to shortcut; models trained on canonical order perform poorly when evaluated on meaning-preserving alternative linearizations (sensitive to token order).",
            "uuid": "e5246.0",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "RECONFIGURE",
            "name_full": "Penman RECONFIGURE linearization",
            "brief_description": "A linearization operation that constructs a tree from an AMR graph while ignoring canonical ordering except for the top node; can change edge label directions (ARG0 -&gt; ARG0-of) and reorders components.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "RECONFIGURE (PENMAN-based reconfiguration)",
            "representation_description": "Use the Penman library to create a valid PENMAN traversal of the AMR graph that discards canonical ordering (except preserving the top node), producing a different but meaning-preserving serialized tree representation.",
            "graph_type": "AMR",
            "representation_properties": "Meaning-preserving alternative linearization; changes relative order of subtrees and can reverse edge labels; retains parenthetical and edge-label tokens but in different order; exposes invariance (or lack thereof) of models to ordering.",
            "evaluation_task": "AMR-to-text generation robustness: evaluate models trained on canonical linearizations when provided RECONFIGURE linearizations; used as auxiliary input for reordering scaffold.",
            "performance_metrics": "BLEU (used to measure drop or recovery in generation performance). Paper reports adversarial training and scaffolding results when evaluating on RECONFIGURE variants (quantitative values in tables).",
            "comparison_to_other_representations": "Used as an adversarial alternative to canonical PENMAN; models trained only on canonical perform worse on RECONFIGURE; adversarial training (exposing model to RECONFIGURE during training) improves robustness with minor cost.",
            "limitations_or_challenges": "Although RECONFIGURE retains graph content, models pretrained on text are sensitive to token order; reconfiguration exposes this fragility and motivates adversarial training or scaffolding to encourage order-agnostic graph encoding.",
            "uuid": "e5246.1",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "RANDOMIZE",
            "name_full": "RANDOMIZED triple/tree linearization",
            "brief_description": "A more drastic PENMAN-based linearization constructed from randomized ordering of triples or traversal, disregarding canonical order information entirely to produce varied linearized sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "RANDOMIZE linearization",
            "representation_description": "Construct a tree or linearization by randomizing the order of triples/nodes/edges (via the Penman library) to produce a valid traversal that is meaning-preserving but has no systematic correspondence to canonical ordering.",
            "graph_type": "AMR (and randomized RDF triple orders for WebNLG experiments)",
            "representation_properties": "Meaning-preserving but high variation in token order; breaks correlations between canonical order and sentence order; intended to stress-test linearization-invariance.",
            "evaluation_task": "AMR-to-text generation robustness (evaluate models trained on canonical inputs) and training augmentation (adversarial training uses RANDOMIZE at epochs).",
            "performance_metrics": "BLEU drops observed when evaluating canonical-trained models on RANDOMIZE; adversarially trained models recover performance with only minor cost. Paper notes epochs-to-40-BLEU: CANONICAL/RECONFIGURE/RANDOMIZE reach 40 BLEU at 2/3/5 epochs respectively (training anecdote).",
            "comparison_to_other_representations": "RANDOMIZE presents a harder generalization than RECONFIGURE; adversarial exposure to RANDOMIZE helps models generalize and sometimes still yields generation outperforming graph-transformer baselines.",
            "limitations_or_challenges": "Large pretrained transformers retain sensitivity to token order from pretraining; even models trained with randomized inputs still perform best on canonical evaluation, indicating residual bias toward canonical ordering.",
            "uuid": "e5246.2",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "RDF-TOK",
            "name_full": "RDF triple tokenized linearization",
            "brief_description": "A simple linearization for RDF triple sets used in WebNLG: prepend special tokens to triple components (&lt;rel&gt;, &lt;S&gt;, &lt;V&gt;, &lt;O&gt;) and concatenate triples separated by dedicated tokens.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Tokenized RDF triple serialization",
            "representation_description": "For each triple, emit a fixed sequence of special tokens and component tokens (e.g., &lt;rel&gt; &lt;S&gt; SUBJECT &lt;V&gt; VERBPHRASE &lt;O&gt; OBJECT) and concatenate multiple triples with separators; adversarial variant randomizes triple order.",
            "graph_type": "RDF triples / knowledge-graph subsets (WebNLG)",
            "representation_properties": "Explicit component markers make roles (subject, relation, object) explicit; simple, repetitive structure; relatively low sentence complexity in dataset leads to lower perplexity.",
            "evaluation_task": "Data-to-text generation on WebNLG; robustness tested by randomizing triple orders at evaluation/training.",
            "performance_metrics": "BLEU (with beam search decoding); paper shows negative impact when canonical ordering is permuted and improvements with adversarial training. Exact numeric values are presented in paper tables.",
            "comparison_to_other_representations": "Performs well with pretrained transformers; still sensitive to triple order but less complex than AMR; adversarial training improves robustness similarly to AMR experiments.",
            "limitations_or_challenges": "Randomizing relations reduces performance for models trained on a fixed ordering; simpler graph structure means less dramatic degradation but still non-negligible.",
            "uuid": "e5246.3",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "ADV-LIN-TRAIN",
            "name_full": "Adversarial linearization training",
            "brief_description": "A training procedure that presents different (adversarially permuted) linearizations of the same graph to the model across epochs to encourage order-agnostic graph encodings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Adversarial linearization exposure",
            "representation_description": "At each training epoch, alter the linearization presented for each graph (canonical, reconfigured, randomized variants) so the model cannot overfit to a single token ordering; effectively data-augment the input linearizations.",
            "graph_type": "AMR and RDF triple sets (WebNLG)",
            "representation_properties": "Encourages invariance to linearization strategy, reduces overfitting to annotator-induced order, slightly increases training time (more epochs for some variants), simple to implement with Penman/randomization.",
            "evaluation_task": "Robustness of graph-to-text generation across linearizations; generation quality measured on canonical and alternative linearizations.",
            "performance_metrics": "BLEU: adversarial training improves robustness with only minor reduction in canonical performance; example: epochs-to-40-BLEU for AMR increased from 2 (canonical) to 3 (reconfigure) and 5 (randomize) but final BLEU comparable; exact BLEU numbers in paper tables.",
            "comparison_to_other_representations": "Improves over single-linearization training and reduces gap vs. graph-encoding models; remains simpler than designing graph-specific encoders.",
            "limitations_or_challenges": "Does not fully erase sensitivity to canonical ordering (models still often perform best on canonical evaluation); may not entirely overcome pretraining-induced token-order biases.",
            "uuid": "e5246.4",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "MASK-GRAPH",
            "name_full": "Masked Graph Modeling (graph-denoising)",
            "brief_description": "A family of graph-denoising objectives applied to linearized graphs in a multi-task text-to-text framework that mask portions of the linearized graph and require reconstruction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Masked graph modeling (graph-denoising scaffold)",
            "representation_description": "Apply masking to tokens in the linearized graph such that ~15% of tokens are masked overall. Variants mask all tokens, only graph components (edge labels and parentheses), or only semantic nodes; training alternates between graph-to-text and denoising objectives.",
            "graph_type": "AMR (and general linearized graph inputs)",
            "representation_properties": "Promotes internal graph encoding by forcing the model to infer missing graph elements; analogous to MLM/MASS objectives; simple to implement on tokenized linearizations; variants similarly effective.",
            "evaluation_task": "AMR-to-text generation, with emphasis on low-resource settings (subsets of data: n = 500, 1000, 5000, 10000, full). Also used as proxy measure of graph-encoding quality via scaffolding loss correlation with MF-score M.",
            "performance_metrics": "BLEU, BertScore, MF-score M (meaning fidelity). Reported: substantial BLEU improvements in low-resource (n=1000) relative to baseline; paper cites that with &lt;3% of full data results exceed prior state-of-the-art GNN (Ribeiro et al. 2019 BLEU 27.37). On full data, gains are minor and within std. deviation.",
            "comparison_to_other_representations": "Outperforms baseline pretrained linearized model in low-resource regimes; yields similar absolute improvements to more complex graph-aware methods but with less model complexity; on full data, benefits diminish compared to large supervised data.",
            "limitations_or_challenges": "Less beneficial when large training sets are available (data diversity supplants scaffolding); different masking strategies performed similarly — no large wins from more complex graph-aware masking; requires careful tuning of task mixing probability.",
            "uuid": "e5246.5",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "REORDER-SCAFF",
            "name_full": "Graph reordering scaffold (reconstruct canonical)",
            "brief_description": "A denoising-style objective that gives the model a reconfigured or randomized linearization and tasks it with reconstructing the canonical linearization to encourage learning of graph structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Reordering-from-reconfigured scaffold",
            "representation_description": "Provide a model with a non-canonical (RECONFIGURE or RANDOMIZE) linearization and require it to output the canonical PENMAN linearization; trained jointly with graph-to-text objective in a multi-task framework.",
            "graph_type": "AMR",
            "representation_properties": "Explicitly trains the model to reverse arbitrary traversals to canonical form, encouraging representation of underlying graph connectivity rather than surface order; functions as data augmentation due to nondeterministic reconfiguration.",
            "evaluation_task": "AMR-to-text generation and measure of implicit graph encoding quality via correlation between scaffolding loss and MF-score M.",
            "performance_metrics": "BLEU and MF-score M; paper finds reordering loss is a significant predictor (negative relationship) of M-score in regression analysis (Pearson ρ reported between scaffolding loss and M). BLEU improvements observed in low-resource settings; exact numeric BLEU gains in paper tables.",
            "comparison_to_other_representations": "Among scaffolding objectives, reordering-from-reconfigured is one of the top-performing methods in low-resource; explains semantic fidelity better than alternatives in regression analysis, and provides gains comparable to other graph-aware approaches but with less complexity.",
            "limitations_or_challenges": "When training data is abundant, benefit diminishes; training to reconstruct canonical order may not fully eliminate residual ordering biases from pretraining; simultaneous generation of surface text and reordering did not improve results in their experiments.",
            "uuid": "e5246.6",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "PRETRAINED-LIN",
            "name_full": "Pretrained transformer finetuned on linearized graphs",
            "brief_description": "Finetune large pretrained transformer LMs (e.g., T5, GPT-2) on pairs of linearized graph inputs and target surface text, yielding state-of-the-art graph-to-text generation performance without explicit graph encoders.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Pretrained linearized transformer finetuning",
            "representation_description": "Use an off-the-shelf pretrained sequence-to-sequence transformer (e.g., T5-Base/Large or GPT-2) and finetune it on (linearized graph string, target sentence) pairs; linearized input can be PENMAN or tokenized triples.",
            "graph_type": "AMR and RDF triples (WebNLG)",
            "representation_properties": "Leverages rich language priors to produce fluent text; can implicitly encode graph structure from linearized sequences; simple pipeline (no GNN) and strong empirical performance.",
            "evaluation_task": "AMR-to-text (LDC2017T10) and WebNLG data-to-text generation.",
            "performance_metrics": "BLEU (primary), BertScore, MF-score M (meaning fidelity), perplexity (for dataset stats). Reported: pretrained linearized models substantially outperform specialized GNN/graph-transformer encoders on these tasks in multiple prior works and in this paper's baselines (exact BLEU values in tables).",
            "comparison_to_other_representations": "Outperforms graph-encoding architectures (GNNs, graph transformers) across metrics in recent comparisons; however, sensitive to the choice/order of linearization and benefits from scaffolding/adversarial exposure to alternative linearizations.",
            "limitations_or_challenges": "Despite strong generation quality, models can be semantically transgressive if graph structural info is not adequately encoded; sensitive to the particular linearization ordering; may require auxiliary objectives to improve fidelity in low-resource settings.",
            "uuid": "e5246.7",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "SEQ2SEQ-LIN",
            "name_full": "Sequence-to-sequence linearization (from-scratch)",
            "brief_description": "Early approach that linearizes graphs into sequences and trains sequence-to-sequence models from scratch (e.g., LSTM-based) to map serialized graphs to text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Seq2Seq linearization (non-pretrained)",
            "representation_description": "Serialize graphs (AMR, triples) into linear token sequences and train encoder-decoder architectures from random initialization (LSTMs, early seq2seq) to generate surface text; does not rely on large pretrained language models.",
            "graph_type": "AMR, RDF triples (various)",
            "representation_properties": "Demonstrates feasibility of linearization approach but historically underperformed GNNs and later pretrained transformers; simpler pipeline but lacks language priors.",
            "evaluation_task": "AMR-to-text generation and other graph-to-text tasks (earlier literature).",
            "performance_metrics": "BLEU and related generation metrics (historically lower than GNNs and pretrained transformers).",
            "comparison_to_other_representations": "Initially outperformed by graph-based encoders; later, pretrained linearized transformers surpassed both seq2seq-from-scratch and graph encoders.",
            "limitations_or_challenges": "Poorer performance without large pretraining; susceptible to overfitting to linearization order; lacks powerful language priors that pretrained models provide.",
            "uuid": "e5246.8",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "GRAPH-ENC",
            "name_full": "Graph-encoding neural networks (GNNs / graph transformers)",
            "brief_description": "Architectures that explicitly encode graph structure (message-passing GNNs, graph transformers) as model inductive bias to directly represent nodes and edges for graph-to-text generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph-encoding architectures (GNNs, graph transformers)",
            "representation_description": "Represent graph inputs as structured objects (adjacency, node features) and use graph neural networks or graph-aware transformer modifications to encode node relationships explicitly prior to decoding to text.",
            "graph_type": "AMR, RDF graphs, other knowledge graphs",
            "representation_properties": "Explicit structural bias, direct modeling of local graph topology, interpretability of message passing, often more sample-efficient in some settings but historically outperformed by pretrained linearized transformers on generation quality.",
            "evaluation_task": "AMR-to-text, WebNLG data-to-text; used as baselines and for comparison of fidelity.",
            "performance_metrics": "BLEU; example: prior state-of-the-art GNN reported BLEU 27.37 (Ribeiro et al., 2019) referenced as baseline that masked-graph scaffolding surpassed in low-resource.",
            "comparison_to_other_representations": "Pretrained linearized transformers now often outperform specialized graph encoders in BLEU and human evaluations; some graph-aware losses/architectural additions narrow the gap but increase complexity.",
            "limitations_or_challenges": "More complex architectures and training; still valuable for explicit structure modeling but slower to train or require additional engineering relative to simple linearization + pretrained LM finetuning.",
            "uuid": "e5246.9",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "GPT-too: A language-model-first approach for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "gpttoo_a_languagemodelfirst_approach_for_amrtotext_generation"
        },
        {
            "paper_title": "Enhancing AMR-to-text generation with dual graph representations",
            "rating": 2,
            "sanitized_title": "enhancing_amrtotext_generation_with_dual_graph_representations"
        },
        {
            "paper_title": "Penman: An opensource library and tool for AMR graphs",
            "rating": 2,
            "sanitized_title": "penman_an_opensource_library_and_tool_for_amr_graphs"
        },
        {
            "paper_title": "Generating English from Abstract Meaning Representations",
            "rating": 1,
            "sanitized_title": "generating_english_from_abstract_meaning_representations"
        },
        {
            "paper_title": "Controllable meaning representation to text generation: Linearization and data augmentation strategies",
            "rating": 1,
            "sanitized_title": "controllable_meaning_representation_to_text_generation_linearization_and_data_augmentation_strategies"
        },
        {
            "paper_title": "The WebNLG challenge: Generating text from RDF data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "AMR-to-text generation with graph transformer",
            "rating": 1,
            "sanitized_title": "amrtotext_generation_with_graph_transformer"
        }
    ],
    "cost": 0.015073749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Promoting Graph Awareness in Linearized Graph-to-Text Generation</p>
<p>Alexander Hoyle hoyle@umd.edu 
Department of Computer Science
University of Maryland
College Park</p>
<p>Ana Marasović anam@allenai.org 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science and Engineering
University of Washington</p>
<p>Noah A Smith 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science and Engineering
University of Washington</p>
<p>Promoting Graph Awareness in Linearized Graph-to-Text Generation</p>
<p>Generating text from structured inputs, such as meaning representations or RDF triples, has often involved the use of specialized graphencoding neural networks. However, recent applications of pretrained transformers to linearizations of graph inputs have yielded stateof-the-art generation results on graph-to-text tasks. Here, we explore the ability of these linearized models to encode local graph structures, in particular their invariance to the graph linearization strategy and their ability to reconstruct corrupted inputs. Our findings motivate solutions to enrich the quality of models' implicit graph encodings via scaffolding. Namely, we use graph-denoising objectives implemented in a multi-task text-to-text framework. We find that these denoising scaffolds lead to substantial improvements in downstream generation in low-resource settings.</p>
<p>Introduction</p>
<p>Parameter-rich pretrained transformer language models succeed at generating text that is prima facie fluent, but that closer inspection will often reveal to be semantically transgressive (Bisk et al., 2020). Indeed, there is limited practical use for unconditional text generation: we expect language to relate to some identifiable, extrinsic meaning. When a system communicates information to an individual in natural language, it will typically rely on a structured representation of that information. Consequently, generating text that faithfully conveys structured data is an important goal in NLP, where inputs can take the form of tables (ToTTo, Parikh et al., 2020), RDF triples (e.g., WebNLG, Gardent et al., 2017), or Abstract Meaning Representations (AMR, Flanigan et al., 2016).</p>
<p>To accomplish this task, models have used neural architectures that explicitly encode graphs, such as graph neural networks (GNNs, Kipf and Welling, * Work undertaken during an internship at AI2. (1) Linearize graph</p>
<p>(2) Finetune with one linearization</p>
<p>The boy wants to go Pretrained Language Model To go the boy wants Finetuned Language Model (go :arg0 (3) Evaluate with an alternative linearization 2017) and graph transformers, in order to accurately capture the structural properties of the input graph (Zhu et al., 2019;Zhao et al., 2020; to name a few). As an alternative to constraining a model architecture with a graph structure, another line of work linearizes a graph into a string ( Figure 2) and trains a sequenceto-sequence model from scratch (Pourdamghani et al., 2016;Konstas et al., 2017;Vinyals et al., 2015). Initially, this approach was outperformed by graph-based encoders, but such models have recently seen their generation performance far surpassed by pretrained transformer language models (LMs) finetuned on pairs of linearized graphs and their corresponding surface realizations (Mager et al., 2020;Kale and Rastogi, 2020;Harkous et al., 2020;Ribeiro et al., 2020, henceforth termed pretrained linearized models). Moreover, both au-tomated and human assessments indicate that text generated with LMs retains meaning at least as well as graph-encoding baselines (Mager et al., 2020). This is not the sole product of pretrained models' general language knowledge: Mager et al. (2020), using a GPT-2-based (Radford et al., 2019) model, report that ablating structural graph information (e.g., edges) in the linearized representation notably degrades generation performance, particularly in AMR-to-text tasks. The remarkable performance of pretrained linearized models is intriguing: explicit representation of the input graph by way of the model architecture appears to be well-substituted by simply writing the graph as a linear sequence.</p>
<p>In this work, we further investigate the extent to which pretrained models can leverage linearized graph inputs. Focusing on AMR graphs and sets of RDF triples in English-language datasets, we structure our investigation by first testing whether models' encodings are invariant to the linearization strategy-the way in which a graph is traversed and encoded when producing the linearized representation (see Figure 1). We discover that generation suffers under adversarial permutations of the linearization, and embrace a simple-buteffective training strategy to mitigate this problem: adversarial training (Goodfellow et al., 2015). Motivated by this finding, we encourage more faithful encodings of graph structure via denoising objectives in the more complex AMR setting. This multi-task scaffolding  reveals that straightforward masking of the graph input is sufficient to improve generation quality in low resource settings.</p>
<p>Moreover, when treating this denoising performance as a proxy for the quality of models' implicit graph encoding, we find that it explains the semantic fidelity of the resulting generation better than reasonable alternatives, suggesting possibilities for future evaluation metrics. 2 Background: Graph-to-Text Generation</p>
<p>In a graph-to-text setting, we transduce graph inputs g to their corresponding surface realization y = y 1 , . . . , y N via a parameterized probabilsitic model p θ (·). In linearized models specifically, the graph g is first mapped to text by way of a (usually deterministic) linearization function x = l(g), where p θ (·) is an off-the-shelf sequenceto-sequence model. This leads to the likelihood objective:
p θ (y | g) = N i=1 p θ (y i | x, y 1:i−1 ). When p θ (·)
is a left-to-right (autoregressive) pretrained transformer, generation quality far exceeds architectures with encoders specifically engineered to encode graphs (Mager et al., 2020;Kale and Rastogi, 2020;Harkous et al., 2020;Ribeiro et al., 2020).</p>
<p>Graph-to-Text Generation Datasets</p>
<p>We explore two datasets for generation from a graph structure to English text.</p>
<p>Abstract Meaning Representation (AMR, Banarescu et al., 2013) is a formalism intended to represent the propositional meaning of utterances-"who is doing what to whom"-using graphs that have minimal dependence on the surface form. AMR graphs are directed and acyclic with a single "top" node (Goodman, 2020). They can be represented as either a graph, a tree, or sets of triples (van Noord and Bos, 2017). For our data, we use the AMR 2.0 release (LDC2017T10), 1 both because it spans a varied set of domains and styles, and because of its extensive use in prior work.</p>
<p>A simpler graph-to-text problem involves converting a set of RDF triples to natural text realizations of the information contained in the set, exemplified by the WebNLG dataset (Gardent et al., 2017). WebNLG pulls information from an existing knowledge base (DBPedia, Mendes et al., 2012) for a specific subset of 15 categories (e.g., "astro-naut"). To generate the paired sentences, crowdworkers verbalize individual triples. Then, for examples consisting of multiple triples, they merge already-annotated sentences and apply minimal changes (leading to reduced sentence complexity relative to AMR, see perplexity scores in Table 1). There can be multiple surface realizations per input.</p>
<p>Models To study pretrained linearized models' invariance to graph linearization, we use T5 (Raffel et al., 2020), an encoder-decoder transformer (Vaswani et al., 2017) that has led to state-of-the-art generation on AMR (specifically, LDC2017T10) and WebNLG (Kale and Rastogi, 2020;Ribeiro et al., 2020).</p>
<p>We modify the T5 implementation from the transformers library (Wolf et al., 2020). 2 We use the Adafactor optimizer (Shazeer and Stern, 2018) with a learning rate of 0.0001, selected from the set {0.001, 0.0001, 3 × 10 −5 , 1 × 10 −5 , 1 × 10 −6 } after tuning on 1000 training examples across five random seeds. 3 We set the batch size to 6 and train until development set BLEU has not improved for 10 epochs. During decoding, we use a beam size of 10 for WebNLG and 5 for AMR.</p>
<p>Evaluation Measures As a primary metric, we evaluate generated text using BLEU (Papineni et al., 2002), calculated with SacreBLEU (Post, 2018). Despite its limitations in generation settings, BLEU still generally accords with rankings of models, either by human evaluations or by alternate metrics (Manning et al., 2020). We also evaluate our scaffolding models ( §4) using BertScore , which measures token similarity with contextual embeddings, permitting a more nuanced measure of semantic similarity. Lastly, we use the M portion of the MF-score (Opitz and Frank, 2020), which measures how well the source AMR graph can be reconstructed from the generated target sentence using an off-the-shelf parser. Unlike BLEU, which applies corpus-wide, this metric provides a best-guess at sentence-level accuracy. particular method used to linearize the input graph. Motivated by the strong graph-to-text performance of these models, we ask: do they implicitly develop a robust internal encoding of the input graph? Whereas a GNN-based model has an architecture designed for graph representation (e.g., information flows between adjacent nodes in a message-passing update), a linearized model must infer how connections are specified in a sequence during training.</p>
<p>If linearized models do form a representation, then the their estimates of the target sentence should be invariant to an alternative linearization of the same graph, so long as the original linearization is in principle recoverable from this alternative. If a model meets this criterion, we call it linearization-invariant.</p>
<p>Experimental Setup</p>
<p>To better understand models' graph-encoding behavior, we experiment with adversarial linearization strategies in two graph-to-text settings.</p>
<p>Permutations of AMR-Graph Linearizations</p>
<p>Standard AMR corpora are linearized as spanning trees over the graphs in PENMAN notation (Matthiessen and Bateman 1991, see Fig. 2a). In the present work, we also linearize graphs using PENMAN, doing so for several reasons: (1) it is sufficiently flexible to accommodate significant changes to the linearization, discussed below; (2) it is more concise than sets of directed triples, both reducing training time and ensuring that inputs fit in the transformer context window; (3) the format leads to superior generation over reasonable alternatives, e.g., DFS traversal paths (Mager et al., 2020).</p>
<p>We will refer to the human-created linearizations in AMR corpora as CANONICAL, since annotators follow a standardized process. There is evidence that this format, in particular the relative ordering of edge types, leaks information about the associated sentence order (Konstas et al., 2017). We speculate that overparametrized models may overfit to such correlations rather than develop robust implicit graph encodings, since it has been repeatedly reported that large models use dataset shortcuts (Jia and Liang, 2017;Gururangan et al., 2018;Geva et al., 2019, among others).</p>
<p>As an alternative linearization, Goodman (2020) defines the RECONFIGURE operation as creating a tree from an AMR graph, where order information from the canonical linearization is ignored, except for the top node (e.g., and in Figs. 2a and 2b).</p>
<p>(a / and :op1 (d / dream-01 :ARG1 (f / film :ARG0-of (d2 / disturb-01)) :ARG2-of (r / resemble-01 :ARG1 a2)) :op2 (a2 / and :op1 (f2 / fascinate-01 :ARG0 f) :op2 d2)) (a) Canonical (a / and :op1 (d / dream-01 :ARG2-of (r / resemble-01) :ARG1 (f / film :ARG0-of (f2 / fascinate-01) :ARG0-of d2)) :op2 (a2 / and :op2 (d2 / disturb-01) :op1 f2 :ARG1-of r))</p>
<p>(b) Reconfigured</p>
<p>(r / resemble-01 :ARG2 (d / dream-01 :op1-of (a / and :op2 a2) :ARG1 (f / film)) :ARG1 (a2 / and :op1 (f2 / fascinate-01 :ARG0 f) :op2 (d2 / disturb-01 :ARG0 f))) (c) Randomized Figure 2: Three PENMAN-based linearizations of AMR graphs corresponding to the sentence, "The film is a dream and, like a dream, is both fascinating and disturbing." Note that the bolded relation in the graph, (resemble-01 :ARG1 and), is represented differently depending on the linearization.</p>
<p>Although it is not a labeled element in the graph, the top node conveys structural information about the sentence-for instance, it is often the main verb. Reconfiguration can include reversals of edge labels (e.g., ARG0 to ARG0-of), therefore constituting a substantive change to the linearization. We also experiment with a more drastic restructuring of the graph, where we construct a tree from a RANDOMIZED triple set alone, disregarding all order information from the canonical format (Fig. 2c). Since it remains a valid traversal of the graph, in principle a model should be able to use this information to construct the surface sentence.</p>
<p>We parse, reconfigure, and randomize graphs using the Penman library (Goodman, 2020), 4 then replace variable names with their references and remove word sense information, following Ribeiro et al. (2019).</p>
<p>Permutations of RDF-Triple Linearizations</p>
<p>We follow the procedure of Ribeiro et al. (2020) to form our standard linearization: we prepend a special token to each element of the triple, and separate triples with another dedicated token. For the output sentence "Ned is the father of Rod and Todd," we would have:</p>
<p>In: (Ned fatherOf Rod), (Ned fatherOf Todd) Out: <rel> <S> Ned <V> father of <O> Rod <rel> <S> Ned <V> father of <O> Todd For our adversarial permutation, we RANDOMIZE the ordering of the relations.</p>
<p>Encouraging Robustness to Linearization We train additional models with the goal of encouraging an agnosticism to graph linearization strategy. We adopt an adversarial training approach (Goodfellow et al., 2015), and alter the graph linearization 4 github.com/goodmami/penman presented to the model at each epoch. We argue that this scheme ought to reduce any model dependence on the human-derived annotation.</p>
<p>Robustness Results</p>
<p>For both tasks, we train the model on the canonical linearization, then evaluate on the various linearizations described in Section 3.1.</p>
<p>Impact of Adversarial Linearizations</p>
<p>The CANONICAL columns of Table 2 show results for models trained on that linearization, then evaluated on permuted graph linearizations. We note a strong negative impact in models' generation capacity for both tasks, with a starker decrease for the AMR data. These results suggest that pretrained linearized models are not linearization-invariant, failing to learn robust implicit graph representations, even in the case of the much simpler WebNLG data.</p>
<p>The remaining columns of Table 2 show that our straightforward adversarial training technique improves robustness, with only minor cost to generation performance. This is the case even with the more drastic RANDOMIZED AMR linearization. Moreover, it only incurs a minor impact on training time-for AMR, the CANONICAL, RECONFIGURE, and RANDOMIZE variants attain 40 BLEU at 2, 3, and 5 epochs, respectively.</p>
<p>Given that elements of canonical annotations are known to correlate with the target sentence order (Konstas et al., 2017), we do not find it surprising that the models trained and evaluated on the permuted linearizations show decreased performance. However, it is meaningful that the canonical linearization at evaluation time still leads to the best results, even for models trained with the randomized inputs-these models did not learn to associate the canonical ordering signal with the input graph. One possible explanation is that the earlier pretrain- ing induces a sensitivity to input token order that persists despite the adversarial fine-tuning, but the behavior merits further exploration.</p>
<p>RQ2: Better Implicit Graph Encodings with Text-to-Text Scaffolding</p>
<p>The positive results of our adversarial training procedure ( §3.2) suggest that pretrained linearized models can form a robust internal graph representation, even though they rely on linearized inputs. Under substantively different linearizations, models retain the ability to generate accurately (even the RANDOMIZE model outperforms best-in-class graph transformers; . Prior work, involving both GNNs and pretrained linearized models, has explored various ways of improving models' sensitivity to the structure of the input graph. To better maintain fidelity to the graph, previous graph-to-text methods incorporate additional loss terms, specialized architectures, or generation-time ranking to influence the semantic accuracy of generation: ranking outputs by the correctness of the AMR parse (Mager et al., 2020;Harkous et al., 2020), jointly "back-parsing" graphs when decoding (Bai et al., 2020), or using distinct components to model different graph traversals (Ribeiro et al., 2019).</p>
<p>These efforts suggest that explicitly accounting for graph structure can assist generation. Can we expand on this idea, and improve generation quality by inducing more robust internal graph representations? To answer this question, we propose secondary objectives designed to promote graph "awareness." In addition to the above graph-to-text approaches, we also draw inspiration from denoising methods used in language model pretraining (Raffel et al., 2020;Lewis et al., 2020), as well as syntactic scaffolds that support semantic tasks with an auxiliary syntax-dependent loss . Intermediate auxiliary pretraining has been repeatedly shown to be successful in other contexts (Phang et al., 2018;Gururangan et al., 2020).</p>
<p>Experimental Setup</p>
<p>In particular, we propose unsupervised graphdenoising tasks that we train alongside AMR-totext generation, following the multi-task setup of Raffel et al. (2020). For each batch, we either optimize the likelihood in Section 2 or one of the objectives described below. 5</p>
<p>Masked Graph Modeling When training transformers to have wide-ranging natural language capabilities, unsupervised denoising objectives like masked language modeling have proved extremely successful (Devlin et al., 2019;Raffel et al., 2020). We argue that a similar principle ought to apply to graph understanding, and therefore apply masking directly to linearized graphs.</p>
<p>In masked language modeling, each word token is masked with probability 15%. Here, we mask different sets of tokens, depending on the experimental condition, always setting the probability such that 15% of all tokens will be masked. Specifically, we mask: all tokens in the linearized graph, the graph components alone (edge labels and parentheses), and the semantic nodes. We also experiment with standard masking of the surface sentence, which mirrors the unsupervised domain-adapted pretraining employed by Ribeiro et al. (2020)  Graph masking can also be performed on any of the linearization variants defined in Section 3.1. 7</p>
<p>Graph Reordering Building on our findings from Section 3.2, we introduce a reordering objective. Specifically, we provide the model with a RECONFIGURED or RANDOMIZED linearization, then task the model with reconstructing the canonical version. We suspect that learning this mapping requires that the model captures the graph structure better, leading to superior graph-to-text generation. Unlike the joint re-generation approach of Mager et al. (2020), where the input graph is copied alongside the target text, our method both requires a nontrivial encoding of the graph and has the effect of augmenting the data (due to the nondeterministic reconfiguration). 8</p>
<p>Scaffolding Results</p>
<p>We find that, overall, denoising objectives drive substantial improvements over the baseline when training on the reduced n = 1000 dataset (Table 3). In fact, using less than 3% of the full data produces results that exceed that of state-of-the-art GNN models from a year prior to this writing (BLEU 27.37, Ribeiro et al., 2019). Moreover, the results 7 We restrict ourselves to the RECONFIGURE setting given that early results showed little difference from RANDOMIZE. 8 Simultaneously generating the surface text and reordering to the canonical linearization did not improve results.  Table 4: Test-set results of scaffolding objectives and baselines trained on the full AMR dataset (LDC2017T10). Bai et al. (2020) is a state-of-theart graph transformer. Ribeiro et al. (2020) finetunes T5-LARGE, which we re-implement as our baseline model. BS is BertScore , and M is the meaning component of the MF-score (Opitz and Frank, 2020  suggest that focusing on the graph representation itself is most important: standard sentence masking (i.e., MLM-style) is less beneficial than graph masking, although it still outperforms the baseline. Surprisingly, the various graph-masking objectives perform similarly to one another-there is little benefit to more complex strategies that specifically account for the graph structure.</p>
<p>While the increased generation quality from the graph-denoising methods is not drastic relative to the MLM case, we contextualize our gains by noting that other ways of promoting greater graph awareness yield similar improvements in absolute terms-and come at the cost of greater model complexity or generation time. For instance, the use of two graph representations in Ribeiro et al.</p>
<p>(2019) achieve a roughly 1-BLEU increase over the use of one alone.</p>
<p>Based on the findings from the n = 1000 setting (Table 3), we select three of the best-Target Both Norway and Sweden have been spared violent terror acts but authorities in both countries have voiced concern about terrorists or terror financiers operating out of Scandinavia. Baseline Norwegian and Swedish authorities have spared Norway and Sweden from violent acts of terror but have voiced concern about terrorists or financiers of terror operating out of Scandinavia.</p>
<p>Ours</p>
<p>Norway and Sweden have been spared terror acts of violence but Norwegian and Swedish authorities have voiced concern about terrorists or financiers of terror operating out of Scandinavia.</p>
<p>Target</p>
<p>The 30-day simple yield fell to an average 8.19% from 8.22%; the 30-day compound yield slid to an average 8.53% from 8.56%. Baseline The simple 30 day yield fell to 8.22 percent from 8.19 percent on average and the compound 30 day yield slid to 8.56 percent from 8.53 percent on average.</p>
<p>Ours</p>
<p>Simple 30 day yields fell from 8.22 to an average 8.19% and compound 30 day yields slid from 8.56 to an average 8.53%.</p>
<p>Target</p>
<p>Many young Saudi radicals have crossed the long and porous border between the Kingdom and Iraq and joined up with Sunni Muslim insurgents there. Baseline Many young Saudi radicals have crossed the porous border from Iraq to the Kingdom and joined up with Sunni Islamic insurgents there.</p>
<p>Ours</p>
<p>Many young Saudi radicals have crossed the porous long-term border with Iraq and joined up with Sunni Islamic insurgents there. performing scaffolding objectives-mask nodes, reconfigure &amp; mask all tokens, and reorder from reconfigured-and train them at n ∈ {500, 1000, 5000, 10000, N }. Results are shown in Fig. 3. At n = 5000, representing 14% of the data, the impact of scaffolding is no longer strong across all objectives. When evaluating on the full dataset, the difference is minor (Table 4). For both BLEU and BertScore, we observe slight improvement over the baseline on average for the mask nodes case, but it is within a standard deviation of the baseline (estimated over 5 seeds). M-score does not vary between models, but it is also not yet established for fine-grained model selection. It appears that the increased size of the data supplants the need for scaffolding losses: the sheer diversity of the source graphs encourages a graph-reasoning ability sufficient to generate accurate sentences. Of course, in a realistic application, hundreds or thousands of training examples are more attainable than tens of thousands. That such straightforward methods can yield strong gains is extremely promising for future work in low-resource graph-to-text generation.</p>
<p>Qualitative Analysis In a manual analysis of 100 random model predictions, we generally observe broad agreement between the model trained with the reordering-from-reconfigured scaffold and the baseline (73% agreement in fidelity), both trained with the full dataset. However, in three cases, the baseline model fails to capture the order of arguments (e.g., "from y to x" when "from x to y" is correct), whereas the scaffolded model remains true to the graph (see Table 5; we did not note instances of the reverse case). While we fail to note "hallucinations"-material information that is not contained in the graph input-both models occasionally drop modifiers (e.g., adjectives or adverbs). Finally, a common error in both models is word-sense confusion (see the third row in Tab. 5, where "long [in length]" is substituted with "long [in duration]"). This is likely due to the removal of word-sense suffixes during preprocessing to avoid sparsity issues (long-03 → long). While currently standard practice, a system aiming to achieve perfect fidelity would require this data.</p>
<p>Encoding Graphs and Generation Performance</p>
<p>The results of Section 4.2 show that the denoising scaffolds impact generation performance. If we consider the sentence-level scaffolding loss as a proxy for the quality of its implicit graph encoding, can it help explain generation fidelity? In order to determine this relationship, we quantify generation accuracy using the M component of the MFscore (Opitz and Frank, 2020). It is calculated by first using an off-the-shelf parser to create an AMR graph from the generated target sentence, then by measuring the overlap with the gold source AMR (from 0 to 1). As seen in Fig. 4, there is a substantial negative relationship (Pearson's ρ = −0.35 * ) between these two variables, measured using outputs from the model trained with the reorderingfrom-reconfigured scaffold on the full data.  To fully operationalize the above question, we estimate a linear regression on the M score of predicted sentences from the validation set. As covariates, we include the above (logged) scaffolding loss, in addition to other metrics that have a significant independent correlation with generation quality. In particular, we use sentence-BLEU, the number of edges in the graph, graph re-entrancies, words in the target sentence, and the (also logged) sentence generation loss. 9 We use the Bayesian information criterion (BIC) to select the model from all possible combinations of the above covariates. We find that the preferred model with p covariates, p = 1 . . . 6, includes the reordering loss in all but one case (p = 2), suggesting its validity as an indicator of graph fidelity above and beyond other alternatives. As seen in Table 6, it has a significant negative relationship with the M score, larger than that of the comparablyscaled generation loss. These results indicate that the reordering loss captures important information about the quality of the graph encoding.</p>
<p>Related Work</p>
<p>Pretrained transformers for Graph-to-Text Generation Mager et al. (2020) condition GPT-2 (Radford et al., 2019) on a linearized AMR graph, then fine-tune on the corresponding surface representation text. Later work using transformers has also found success on both AMR-to-text and data-to-text tasks (Kale and Rastogi, 2020;Harkous et al., 2020;Ribeiro et al., 2020). To our knowledge, across a diverse set of tasks and automated 10 metrics, a pretrained transformer of sufficient capacity will always outperform a specialized GNN, often by a large margin. Ribeiro et al. (2020), following Gururangan et al. 2020, further pretrain on additional in-domain data, using both supervised (silver AMR parses to text) and unsupervised (denoising target text) objectives. Mager et al. (2020) use various heuristics to improve fidelity. During training, they regenerate the input graph, and in inference, they parse generations and rank their consistency with the original graph. Harkous et al. (2020) instead rank with a trained classifier, and introduce additional "state embeddings" to help indicate the ordering of graph components. The encoder-decoder methods cited in the previous paragraph eschew these approaches and nonetheless perform better. In preliminary replications of the Mager et al. experiments with T5, we find that joint re-generation leads to no improvement and moreover that the longer output sequences increase training time. Experimenting with other graphsensitive embeddings is a valuable direction for future work.</p>
<p>Graph-Dependent Losses</p>
<p>Graph Linearization Other work also studies linearizations for AMR-to-text settings.</p>
<p>As opposed to our efforts, the focus is not on enriching or measuring models' graph encoding, but instead on determining what elements of linearization (e.g., parentheses and edge labels) are necessary for generation.</p>
<p>Closest to our work is Konstas et al. (2017), who experiment with alternative graph traversals by randomizing the edge type order (less drastic than either RECONFIGURE or RANDOMIZE) with an LSTM-based model. Rather than randomizing at each epoch, as in our approach, they employ a consistent random ordering for each example during training, and do not evaluate models across different linearizations. The results help establish that LSTMs can be made agnostic to ordering, but fail to measure the extent to which models overfit to the training order (Section 3.2).</p>
<p>Ribeiro et al. (2020) report paired training and evaluation shuffling results (as in Table 2), but they ignore parentheses, only reodering node labels. Hence, their results cannot establish models' graph-encoding ability, instead revealing that node order is informative of word order, corroborating findings in Konstas et al. (2017). Both works, along with Mager et al. (2020), run ablations by removing parenthetical markers, finding that graph structure is necessary for strong generation.</p>
<p>Finally, Kedzie and McKeown (2020), appearing contemporaneously to our work, seek to control the output generation by manipulating the input linearization order, using a randomization similar to ours as an "uncontrolled" baseline. Given their focus on task-oriented dialogue planning, which uses simpler meaning representations and sentences than the AMR dataset used here (i.e., shallower graphs and limited domains), we view their work as complementary to our own.</p>
<p>Conclusion</p>
<p>In this work, we explore the graph-encoding ability of pretrained transformers through the lens of graph-to-text generation that relies on linearized graph inputs. First, we determine the extent to which these models are invariant to the method by which graphs are linearized, finding that models trained on the fixed, canonical linearizations fail to generalize to meaning-preserving alternatives. We rectify this shortcoming by training models on linearizations corresponding to alternative random traversals of the graph. Following prior work that has used graph-aware losses to improve generation quality, we then explore ways of improving models' sensitivity to the input graphs. Motivated by the success of denoising objectives in other text-to-text settings, we encourage robust internal graph encodings through additional scaffolding losses. Although scaffolding leads to tepid improvements in generation quality when training data is plentiful, it yields substantial gains in low-resource settings.</p>
<p>Figure 1 :
1Diagram of our adversarial evaluation procedure for graph-to-text generation using pretrained language models ( §3.2). (1) A graph can admit multiple possible linearizations. (2) Following standard practice, we train with a single linearization. (3) At evaluation time, we present the model with a meaning-preserving alternative.</p>
<p>Figure 3 :
3Test set BLEU on the AMR dataset (LDC2017T10) under different amounts of training data for selected scaffolding objectives (over 5 seeds).</p>
<p>Figure 4 :
4Sentence-level scaffolding loss and M-score on the validation set, using a model trained with the reordering-from-reconfigured scaffold. M-score is a measure of the generated sentence's semantic fidelity, and the scaffolding loss is a proxy for the graph encoding accuracy.</p>
<p>We organize our investigation around two research questions:RQ1 To what extent are pretrained linearized 
models invariant to graph linearization 
strategy? ( §3) 
RQ2 Does encouraging pretrained linearized 
models' implicit graph representation lead to 
better generation? ( §4) </p>
<p>N Dev. ppl. Avg. edges </p>
<p>LDC2017T10 36k 
21.1 
11.4 
WebNLG 
18k 
9.2 
3.0 </p>
<p>Table 1: Dataset statistics. Perplexity estimated on the 
development set with GPT-2 (Radford et al., 2019) fine-
tuned on the training data using default hyperparame-
ters in the transformers library (Wolf et al., 2020). </p>
<p>Table 3 :
3Development set BLEU across scaffolding ob-
jectives and baselines, trained on 1000-example subsets 
of the AMR dataset (LDC2017T10). Mean (s.d.) over 
5 seeds. </p>
<p>example, when masking components alone: </p>
<p>orig ( stupefy :ARG1 ( we ) 
) 
in ( stupefy <M> 
( we <M> ) 
out original text </p>
<p>). Mean (s.d.) over 5 seeds.500 
(1.4%) 
1,000 
(2.7%) 
5,000 
(13.7%) 
10,000 
(27.4%) 
36,520 
(100.0%) 
Training Set Size </p>
<p>0 </p>
<p>10 </p>
<p>20 </p>
<p>30 </p>
<p>40 </p>
<p>BLEU Score </p>
<p>Baseline 
Mask nodes 
Reconfigured, mask all 
Reorder from reconfigured </p>
<p>Table 5 :
5Selected predictions from the baseline and a model using the reordering-from-reconfigured scaffold 
(trained on the full data). </p>
<p>Table 6 :
6OLS regression results on validation sentence M-score, a measure of semantic fidelity. Model trained with the reordering-from-reconfigured scaffold. *Significance at p &lt; 0.001.
catalog.ldc.upenn.edu/LDC2017T10
RQ1: Robustness to Permutation of Graph LinearizationIn this section, we explore the extent to which pretrained linearized models are invariant to the 2 We use T5-Base for WebNLG and T5-Large for AMR, finding that the larger model did not benefit the WebNLG task.3  Less extensive experiments with the full dataset indicated the same optimal setting, although in general it is relatively robust to learning rate.
Per-task batches proved marginally better than mixing within a batch. The scaffolding task probability is a hyperparameter, which we set to 0.5.6  We use MASS-style masking(Song et al., 2019) for the tokens, rather than the span-replacing of T5, as it performed somewhat better.
We eliminate outliers consisting of the bottom 0.5% of target lengths and M-scores and the top 0.5% of the losses.
Human evaluation has been less thorough, althoughMager et al. (2020) report improved human judgments on AMR-to-text generation. We note similar results in our own experiments.</p>
<p>Online back-parsing for AMR-to-text generation. Xuefeng Bai, Linfeng Song, Yue Zhang, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsXuefeng Bai, Linfeng Song, and Yue Zhang. 2020. On- line back-parsing for AMR-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1206-1219, Online. Association for Computa- tional Linguistics.</p>
<p>Abstract Meaning Representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with DiscourseSofia, BulgariaAssociation for Computational LinguisticsLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguis- tic Annotation Workshop and Interoperability with Discourse, pages 178-186, Sofia, Bulgaria. Associa- tion for Computational Linguistics.</p>
<p>Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lap- ata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. Experience grounds language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8718-8735, Online. Association for Computational Linguistics.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.</p>
<p>Generation from Abstract Meaning Representation using tree transducers. Jeffrey Flanigan, Chris Dyer, Noah A Smith, Jaime Carbonell, 10.18653/v1/N16-1087Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsJeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from Abstract Meaning Representation using tree transducers. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 731-739, San Diego, California. Association for Computational Linguistics.</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational LinguisticsClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Pro- ceedings of the 10th International Conference on Natural Language Generation, pages 124-133, San- tiago de Compostela, Spain. Association for Compu- tational Linguistics.</p>
<p>Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. Mor Geva, Yoav Goldberg, Jonathan Berant, 10.18653/v1/D19-1107Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsMor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an inves- tigation of annotator bias in natural language under- standing datasets. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 1161-1166, Hong Kong, China. As- sociation for Computational Linguistics.</p>
<p>Explaining and harnessing adversarial examples. Ian J Goodfellow, Jonathon Shlens, Christian Szegedy, International Conference on Learning Representations (ICLR). Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and harnessing adversar- ial examples. In International Conference on Learn- ing Representations (ICLR).</p>
<p>Penman: An opensource library and tool for AMR graphs. Michael Wayne Goodman, 10.18653/v1/2020.acl-demos.35Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System DemonstrationsOnline. Association for Computational LinguisticsMichael Wayne Goodman. 2020. Penman: An open- source library and tool for AMR graphs. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstra- tions, pages 312-319, Online. Association for Com- putational Linguistics.</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Ana Suchin Gururangan, Swabha Marasović, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey, Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsSuchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.</p>
<p>Annotation artifacts in natural language inference data. Swabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Samuel Schwartz, Noah A Bowman, Smith, 10.18653/v1/N18-2017Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, Louisiana2Short Papers. Association for Computational LinguisticsSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural lan- guage inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Associa- tion for Computational Linguistics.</p>
<p>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. Hamza Harkous, Isabel Groves, Amir Saffari, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, SpainInternational Committee on Computational LinguisticsHamza Harkous, Isabel Groves, and Amir Saffari. 2020. Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2410-2424, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Adversarial examples for evaluating reading comprehension systems. Robin Jia, Percy Liang, 10.18653/v1/D17-1215Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsRobin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2021-2031, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Text-to-text pre-training for data-to-text tasks. Mihir Kale, Abhinav Rastogi, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language GenerationDublin, IrelandAssociation for Computational LinguisticsMihir Kale and Abhinav Rastogi. 2020. Text-to-text pre-training for data-to-text tasks. In Proceedings of the 13th International Conference on Natural Lan- guage Generation, pages 97-102, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Controllable meaning representation to text generation: Linearization and data augmentation strategies. Chris Kedzie, Kathleen Mckeown, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Chris Kedzie and Kathleen McKeown. 2020. Con- trollable meaning representation to text generation: Linearization and data augmentation strategies. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5160-5185, Online. Association for Computa- tional Linguistics.</p>
<p>Semi-supervised classification with graph convolutional networks. Thomas Kipf, M Welling, International Conference on Learning Representations. ICLRThomas Kipf and M. Welling. 2017. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representa- tions (ICLR).</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsLong Papers)Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and gener- ation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 146-157, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Story ending prediction by transferable bert. Zhongyang Li, Xiao Ding, Ting Liu, 10.24963/ijcai.2019/249Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19Zhongyang Li, Xiao Ding, and Ting Liu. 2019. Story ending prediction by transferable bert. In Proceed- ings of the Twenty-Eighth International Joint Con- ference on Artificial Intelligence, IJCAI-19, pages 1800-1806. International Joint Conferences on Ar- tificial Intelligence Organization.</p>
<p>GPT-too: A language-model-first approach for AMR-to-text generation. Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Arafat Md, Young-Suk Sultan, Radu Lee, Salim Florian, Roukos, 10.18653/v1/2020.acl-main.167Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsManuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, and Salim Roukos. 2020. GPT-too: A language-model-first approach for AMR-to-text gen- eration. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 1846-1852, Online. Association for Computa- tional Linguistics.</p>
<p>A human evaluation of AMR-to-English generation systems. Emma Manning, Shira Wein, Nathan Schneider, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (OnlineInternational Committee on Computational LinguisticsEmma Manning, Shira Wein, and Nathan Schneider. 2020. A human evaluation of AMR-to-English gen- eration systems. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 4773-4786, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics.</p>
<p>Text Generation and Systemic-functional Linguistics: Experiences from English and Japanese. M I M Christian, John A Matthiessen, Bateman, Communication in Artificial Intelligence Series. Pinter Pub LtdChristian M.I.M. Matthiessen and John A. Bateman. 1991. Text Generation and Systemic-functional Lin- guistics: Experiences from English and Japanese. Communication in Artificial Intelligence Series. Pin- ter Pub Ltd.</p>
<p>DBpedia: A multilingual cross-domain knowledge base. Pablo Mendes, Max Jakob, Christian Bizer, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12). the Eighth International Conference on Language Resources and Evaluation (LREC'12)Istanbul, TurkeyEuropean Language Resources Association (ELRAPablo Mendes, Max Jakob, and Christian Bizer. 2012. DBpedia: A multilingual cross-domain knowledge base. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 1813-1817, Istanbul, Turkey. Eu- ropean Language Resources Association (ELRA).</p>
<p>Towards a decomposable metric for explainable evaluation of text generation from amr. Juri Opitz, Anette Frank, arXiv:2008.08896Juri Opitz and Anette Frank. 2020. Towards a decom- posable metric for explainable evaluation of text gen- eration from amr. arXiv:2008.08896.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>ToTTo: A controlled table-totext generation dataset. Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsOnlineAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-to- text generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1173-1186, On- line. Association for Computational Linguistics.</p>
<p>Bowman. Jason Phang, Thibault Févry, Samuel R , arXiv:1811.01088Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. Jason Phang, Thibault Févry, and Samuel R. Bow- man. 2018. Sentence encoders on STILTs: Supple- mentary training on intermediate labeled-data tasks. arXiv:1811.01088.</p>
<p>A call for clarity in reporting BLEU scores. Matt Post, 10.18653/v1/W18-6319Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBrussels, BelgiumAssociation for Computational LinguisticsMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186- 191, Brussels, Belgium. Association for Computa- tional Linguistics.</p>
<p>Generating English from Abstract Meaning Representations. Nima Pourdamghani, Kevin Knight, Ulf Hermjakob, 10.18653/v1/W16-6603Proceedings of the 9th International Natural Language Generation conference. the 9th International Natural Language Generation conferenceEdinburgh, UKAssociation for Computational LinguisticsNima Pourdamghani, Kevin Knight, and Ulf Herm- jakob. 2016. Generating English from Abstract Meaning Representations. In Proceedings of the 9th International Natural Language Generation confer- ence, pages 21-25, Edinburgh, UK. Association for Computational Linguistics.</p>
<p>Language Models are Unsupervised Multitask Learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1-67.</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsLeonardo F. R. Ribeiro, Claire Gardent, and Iryna Gurevych. 2019. Enhancing AMR-to-text genera- tion with dual graph representations. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 3183-3194, Hong Kong, China. Association for Computational Lin- guistics.</p>
<p>Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for Graph-to-Text generation. F R Leonardo, Martin Ribeiro, Schmitt, arXiv:2007.08426Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for Graph-to-Text gen- eration. arXiv:2007.08426.</p>
<p>Adafactor: Adaptive learning rates with sublinear memory cost. Noam Shazeer, Mitchell Stern, PMLRProceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningStockholm, SwedenNoam Shazeer and Mitchell Stern. 2018. Adafac- tor: Adaptive learning rates with sublinear mem- ory cost. In Proceedings of the 35th International Conference on Machine Learning, pages 4596-4604, Stockholm, Sweden. PMLR.</p>
<p>MASS: Masked sequence to sequence pre-training for language generation. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu, PMLRProceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningLong Beach, California, USAKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie- Yan Liu. 2019. MASS: Masked sequence to se- quence pre-training for language generation. In Pro- ceedings of the 36th International Conference on Machine Learning, pages 5926-5936, Long Beach, California, USA. PMLR.</p>
<p>Syntactic scaffolds for semantic structures. Swabha Swayamdipta, Sam Thomson, Kenton Lee, Luke Zettlemoyer, Chris Dyer, Noah A Smith, 10.18653/v1/D18-1412Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsSwabha Swayamdipta, Sam Thomson, Kenton Lee, Luke Zettlemoyer, Chris Dyer, and Noah A. Smith. 2018. Syntactic scaffolds for semantic structures. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 3772-3782, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Neural semantic parsing by character-based translation: Experiments with Abstract Meaning Representations. Rik Van Noord, Johan Bos, Computational Linguistics in the Netherlands Journal. 7Rik van Noord and Johan Bos. 2017. Neural semantic parsing by character-based translation: Experiments with Abstract Meaning Representations. Computa- tional Linguistics in the Netherlands Journal, 7:93- 108.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems (NeurIPS). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems (NeurIPS).</p>
<p>Grammar as a foreign language. Oriol Vinyals, Terry Kaiser, Slav Koo, Ilya Petrov, Geoffrey Sutskever, Hinton, Advances in Neural Information Processing Systems. Curran Associates, Inc28Oriol Vinyals, Ł ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Gram- mar as a foreign language. In Advances in Neural Information Processing Systems, volume 28, pages 2773-2781. Curran Associates, Inc.</p>
<p>AMR-to-text generation with graph transformer. Tianming Wang, Xiaojun Wan, Hanqi Jin, 10.1162/tacl_a_00297Transactions of the Association for Computational Linguistics. 8Tianming Wang, Xiaojun Wan, and Hanqi Jin. 2020. AMR-to-text generation with graph transformer. Transactions of the Association for Computational Linguistics, 8:19-33.</p>
<p>Transformers: State-of-the-Art Natural Language Processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Drame, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsQuentin Lhoest, and Alexander RushOnline. Association for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-Art Natural Language Process- ing. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, V Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. ICLRTianyi Zhang, V. Kishore, Felix Wu, Kilian Q. Wein- berger, and Yoav Artzi. 2020. BERTScore: Evalu- ating Text Generation with BERT. In International Conference on Learning Representations (ICLR).</p>
<p>Bridging the structural gap between encoding and decoding for data-to-text generation. Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, 10.18653/v1/2020.acl-main.224Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsChao Zhao, Marilyn Walker, and Snigdha Chaturvedi. 2020. Bridging the structural gap between encod- ing and decoding for data-to-text generation. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 2481- 2491, Online. Association for Computational Lin- guistics.</p>
<p>Modeling graph structure in transformer for better AMR-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, 10.18653/v1/D19-1548Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better AMR-to-text gen- eration. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 5459-5468, Hong Kong, China. Association for Computational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
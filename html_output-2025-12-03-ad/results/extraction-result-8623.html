<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8623 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8623</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8623</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-252070866</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.00840v2.pdf" target="_blank">FOLIO: Natural Language Reasoning with First-Order Logic</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8623.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8623.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bidirectional Transformer encoder pretrained with masked language modeling and next-sentence prediction, evaluated here via supervised fine-tuning for 3-way truth-value classification on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bidirectional Transformer encoder (Devlin et al.); fine-tuned with an added two-layer classification head to predict True/False/Unknown on NL stories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>110M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Determine truth values (True/False/Unknown) of natural-language conclusions given NL premises; problems are annotated with first-order logic and verified by an FOL inference engine.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fully supervised fine-tuning on FOLIO training set with classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>56.83% accuracy on FOLIO test set (three-way truth-value classification).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Above majority baseline (38.5%) and random (33.3%); similar to RoBERTa-base; lower than larger models and best finetuned model (Flan-T5-Large 65.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles on examples with longer reasoning depths and complex FOL patterns compared to larger models and specialized neuro-symbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Supervised fine-tuning on FOLIO gives nontrivial competence but is insufficient to reach expert-level reasoning; capturing deep multi-step FOL patterns remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8623.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger variant of BERT used as a fine-tuning baseline for predicting truth values on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT large Transformer encoder fine-tuned with a two-layer classification head for the 3-way classification task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>3-way truth-value classification of NL conclusions under premises with FOL annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fully supervised fine-tuning on FOLIO training set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>59.0% accuracy on FOLIO test set.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>About 2.2% absolute improvement over BERT-base; still below RoBERTa-large and Flan-T5-Large.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails on longer multi-step reasoning chains and high logical depth examples; tends to overpredict True labels relative to False/Unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Scaling encoder size brings modest gains but does not close the gap to expert performance or to models augmented with logical solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8623.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robustly optimized BERT-style encoder evaluated via supervised fine-tuning on FOLIO for logical truth-value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder (Robustly optimized BERT) fine-tuned with a classification head for the three-way FOLIO task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>110M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict True/False/Unknown for conclusions given NL premises annotated in FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fully supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>56.8% accuracy on FOLIO test set.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Similar to BERT-base; underperforms RoBERTa-large and Flan-T5-Large.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limited on deep multi-step reasoning chains and HybLogic subset with high reasoning depth.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Standard pretrained encoders provide a baseline but require further architectural or symbolic support to handle FOL-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8623.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large variant of RoBERTa fine-tuned on FOLIO; shows improved supervised performance over base models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large Transformer encoder fine-tuned for 3-way classification on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Classify conclusion truth values given premises, with parallel FOL verification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fully supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>62.1% accuracy on FOLIO test set.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improves ~3.1% over BERT-large and ~5.3% over RoBERTa-base; still below best fine-tuned Flan-T5-Large and top prompting/method results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although better on HybLogic than WikiLogic under fine-tuning, still fails on many long reasoning chains and complex AST patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Fine-tuning can learn some template-like patterns (HybLogic) but generalization to diverse, deep FOL patterns remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8623.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (Large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned encoder-decoder Transformer (T5 family) fine-tuned on FOLIO; obtained the highest accuracy among supervised fine-tuning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned T5 sequence-to-sequence model used here as a classifier via fine-tuning for the FOLIO three-way task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>783M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Input: NL premises and conclusions; output: truth labels; problems annotated in first-order logic.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fully supervised fine-tuning on FOLIO training data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>65.7% accuracy on FOLIO test set (highest among finetuned baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Beats other fine-tuned encoder baselines (BERT, RoBERTa) by several points but is below the best prompting/method-enhanced LLM results (e.g., GPT-4 with advanced prompting and Logic-LM).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still far from expert human performance (95.98%); struggles on the highest reasoning depths and some HybLogic patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Instruction tuning + finetuning improves model's ability to map NL to truth-value judgments, but deeper symbolic reasoning requires additional mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8623.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source large language model evaluated in few-shot prompting settings on FOLIO; shows near-chance performance under few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13-billion-parameter autoregressive Transformer (LLaMA family) used with few-shot prompting (NL examples) to predict conclusion truth values.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning) - few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Few-shot NL prompting to predict True/False/Unknown on NL stories requiring FOL reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot natural language prompting (NL examples), no specialized logical augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>33.63% accuracy (8-shot NL prompt) â€” only slightly above random (33.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially worse than larger LLaMA (70B) and commercial LLMs (GPT-3.5/GPT-4); near random.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Cannot reliably perform multi-step FOL reasoning in few-shot; fails on almost all high-depth examples.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Model scale and pretraining matter strongly for emergent chain-of-thought capabilities; small-ish LLMs without special prompting fail on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8623.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>70B-parameter LLaMA model evaluated with few-shot prompting and reasoning prompts (CoT, ToT); achieves better but still limited performance on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>70-billion-parameter autoregressive Transformer (LLaMA family) used with few-shot prompting and reasoning strategies (Chain-of-Thought, Tree-of-Thought).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning) - few-shot prompting with CoT/ToT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict truth values of NL conclusions given premises; evaluate effect of prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>8-shot NL prompting; Chain-of-Thought (CoT) and Tree-of-Thought (ToT) prompting variants were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>44.0% (8-shot NL prompt); CoT: 47.8%; ToT: 48.4% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>~10% absolute better than LLaMA-13B; still well below GPT-3.5/GPT-4 few-shot performance and advanced prompting results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Improvements from CoT/ToT modest; still fails many deep multi-step reasoning problems and HybLogic examples.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Scaling improves capability, and reasoning prompts help modestly, but scale + prompting alone are insufficient for complex FOL reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8623.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI GPT-3.5-era model variant used as a few-shot/few-shot-CoT baseline for FOLIO; intermediate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 family variant used with few-shot prompting (8-shot) for truth-value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>hidden / not reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning) - few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>8-shot NL prompts to classify conclusions as True/False/Unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot NL prompting (8-shot); no CoT variants reported for this model in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>49.53% accuracy (8-shot NL prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Below GPT-3.5-Turbo (58.3%) and GPT-4 variants; above LLaMA models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limited chain-of-thought reasoning in this task; lower performance compared to later GPT-3.5/GPT-4 results.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Intermediate capability suggests instruction-tuning improves over base models but more advanced models/strategies significantly help.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8623.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5-generation OpenAI model used both zero-shot and few-shot prompting for NL reasoning and NL->FOL translation tasks on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer LLM by OpenAI; used with zero-shot and few-shot prompts for truth-value prediction and NL->FOL translation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>hidden / not reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning) and NL-FOL translation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>NL reasoning: 3-way truth-value classification; NL-FOL translation: generate FOL formulas and execution accuracy via FOL prover.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot NL prompting (8-shot); zero-shot also evaluated for reasoning; few-shot NL-FOL translation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>NL reasoning: zero-shot 53.1% (reported), few-shot 58.34% accuracy; NL-FOL translation: few-shot SynV 93.3% and ExcAcc 56.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Better than text-davinci-002 and LLaMA models; worse than GPT-4 and specialized logic methods (Logic-LM, DetermLR).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower execution accuracy in NL-FOL translation compared to GPT-4; struggles on high-depth HybLogic examples; does not benefit from FOL concatenation as GPT-4 does.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Sufficient scale and prompting give moderate performance, but GPT-3.5 lacks some of GPT-4's ability to use explicit FOL in prompts and to generalize to complex FOL reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8623.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art closed-source LLM used as the strongest few-shot baseline and base model for logic-specific methods; evaluated with advanced prompting and neuro-symbolic methods on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal Transformer LLM from OpenAI (exact size undisclosed); used with few-shot prompting, Chain-of-Thought, Self-Consistency, Tree-of-Thought, and as the base for Logic-LM/LINC/DetermLR experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>hidden / not reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning) and NL-FOL translation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict True/False/Unknown labels and generate parallel FOL formulas (NL->FOL translation) evaluated via an FOL prover.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting (8-shot) with CoT, CoT+Self-Consistency (SC), Tree-of-Thought (ToT); used as base model for neuro-symbolic methods (Logic-LM, LINC, DetermLR). Also evaluated with concatenated NL+FOL in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>NL reasoning: few-shot 64.2% (8-shot baseline); CoT 68.9%; CoT+SC 69.5%; ToT 70.0%; best logic-specific method results using GPT-4 base: Logic-LM 78.1%, DetermLR 77.5%, LINC 73.1%. NL-FOL translation (few-shot): SynV 93.9%, ExcAcc 63.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially better than GPT-3.5, LLaMA, and finetuned medium models; neuro-symbolic augmentations (Logic-LM, DetermLR, LINC) further improve performance by ~9â€“13% absolute over vanilla few-shot GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Significant gap to expert human accuracy (95.98%); fails frequently on HybLogic (high depth) and on examples requiring long reasoning chains; human error analysis attributes ~65% of GPT-4 errors to faulty reasoning chains, 25% to wrong derivations, 5% to syntactic comprehension, 5% to spurious shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GPT-4 is the strongest baseline but still struggles to reliably perform deep FOL deduction; combining LLMs with symbolic provers or structured reasoning workflows substantially improves logical fidelity; supplying FOL alongside NL helps GPT-4 (NL+FOL > NL alone).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8623.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs to improve multi-step problem solving; applied to GPT-4 and others on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting (applied to GPT-4, LLaMA-70B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt engineering method that asks the model to output step-by-step intermediate reasoning (reasoning traces) before final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a (method applied to models of various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic (NL reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to coax intermediate deductions to improve multi-step logical inference for the 3-way truth-value classification task.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot CoT prompting (8-shot); also combined with Self-Consistency and Tree-of-Thought variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Improves GPT-4 few-shot from 64.2% to 68.9% (absolute +4.7%); LLaMA-70B improved ~3â€“4 points with CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms vanilla few-shot prompting for capable LLMs (GPT-4), but gains are modest and insufficient to reach expert-level performance alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>CoT sometimes fails to produce correct intermediate steps, leading to faulty final conclusions; self-consistency yields small extra gains.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>CoT helps but does not fully remedy failures on long FOL deduction chains; combining with symbolic checking or neuro-symbolic architectures is more effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8623.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that samples multiple chain-of-thought traces and selects the most consistent final answer; applied to GPT-4 on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency (applied to GPT-4 Chain-of-Thought outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregation technique that draws multiple reasoning traces and picks the most frequent final answer to reduce reasoning noise.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO NL reasoning (few-shot with CoT + SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to improve stability and accuracy of chain-of-thought reasoning for truth-value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Combine CoT prompting with sampling and majority-selection over outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Small improvement over CoT for GPT-4: CoT 68.9% -> CoT+SC 69.5% (absolute +0.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Marginal gain over CoT; less impactful than Tree-of-Thought or neuro-symbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Aggregation cannot fix fundamentally incorrect deduction chains; limited benefit on highly complex FOL problems.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Helpful as a noise-reduction mechanism but not a substitute for symbolic verification or stronger deductive procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8623.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberative search-based prompting approach that explores multiple reasoning paths (tree search) to find better solutions; applied to GPT-4 and LLaMA-70B on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tree-of-Thought prompting (applied to GPT-4, LLaMA-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting/control-flow approach that constructs and explores a tree of intermediate reasoning states rather than a single linear chain.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO NL reasoning (few-shot with ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to search alternative reasoning chains to better handle multi-step FOL deductions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Tree-of-Thought reasoning prompting and search over intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-4 ToT achieved 70.0% accuracy (slightly better than CoT+SC); LLaMA-70B ToT improved modestly to 48.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Slight improvement over CoT and SC for high-capability models, but still inferior to neuro-symbolic methods like Logic-LM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exploration increases compute and may still fail to find required intermediate steps for deep FOL chains.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Search-based prompting can yield additional gains but does not eliminate the need for symbolic reasoning or formal provers for strict FOL deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8623.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic method that augments an LLM with symbolic solvers to improve logical fidelity; evaluated using GPT-4 as backbone on FOLIO and produces large accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic-LM (GPT-4 base in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic pipeline that integrates GPT-4 with symbolic reasoning components/solvers to generate logically faithful deductions and verify results.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 base (size undisclosed)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Improves multi-step logical deduction by combining LLM generation with symbolic provers and verification to produce trustworthy truth-value predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neuro-symbolic integration: LLM + symbolic solver(s) for deduction and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>78.1% accuracy on FOLIO test set (reported as Logic-LM(GPT-4) result).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>About +13.9% absolute over vanilla GPT-4 few-shot (64.2%) and +8.1% over GPT-4 ToT (70.0%); the best reported method-level performance in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>While much improved, still below expert human accuracy (95.98%); complexity and engineering for integration may limit scaling and generality.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Combining LLMs with symbolic provers substantially improves strict FOL reasoning accuracy, highlighting the promise of neuro-symbolic approaches for formal deduction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8623.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LINC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neuro-symbolic method combining an LLM with an FOL prover (LINC) to improve logical deduction; evaluated with GPT-4 base on FOLIO and yields large gains over vanilla prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LINC (GPT-4 base in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neurosymbolic pipeline linking LLM-generated formalizations with an FOL prover to perform formal inference and verify conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 base (size undisclosed)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Translate NL to FOL (or generate intermediate formal steps) and use a symbolic prover to derive truth-values robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neuro-symbolic integration: language model + FOL prover; combines generation and formal execution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>73.1% accuracy on FOLIO test set.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>â‰ˆ+9% absolute over vanilla GPT-4 few-shot; outperformed by Logic-LM and DetermLR in this study but still a substantial improvement over raw prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dependent on quality of generated formalization; errors in translation to FOL can limit prover effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Neuro-symbolic pipelines that reliably produce formal input to provers are effective for improving strict logical reasoning in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8623.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8623.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DetermLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DetermLR: From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLMs with deterministic logical reasoning components; applied with GPT-4 base and shows large improvements on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DetermLR (GPT-4 base in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach that augments LLM outputs with deterministic logical modules or reasoning checks to reduce indeterminate/incorrect deductions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 base (size undisclosed)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO: Natural language reasoning with first-order logic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Enhance LLM reasoning by determinizing intermediate steps or integrating symbolic checks to improve final truth-value judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neuro-symbolic / hybrid approach combining LLM reasoning with deterministic logical augmentation or checking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>77.5% accuracy on FOLIO test set.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Comparable to Logic-LM (78.1%) and superior to vanilla GPT-4 prompting; about +13% absolute improvement over GPT-4 few-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires robust mechanisms for converting NL to deterministic logical representations; residual failures persist on highest-depth cases.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>DetermLR demonstrates that adding determinizing/symbolic layers to an LLM substantially improves formal logical reasoning on complex FOL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 1)</em></li>
                <li>RuleTaker: Transformers as soft theorem provers (Transformers as soft reasoners over language) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8623",
    "paper_id": "paper-252070866",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "BERT-base",
            "name_full": "BERT (base)",
            "brief_description": "A bidirectional Transformer encoder pretrained with masked language modeling and next-sentence prediction, evaluated here via supervised fine-tuning for 3-way truth-value classification on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-base",
            "model_description": "Bidirectional Transformer encoder (Devlin et al.); fine-tuned with an added two-layer classification head to predict True/False/Unknown on NL stories.",
            "model_size": "110M",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning)",
            "reasoning_task_description": "Determine truth values (True/False/Unknown) of natural-language conclusions given NL premises; problems are annotated with first-order logic and verified by an FOL inference engine.",
            "method_or_approach": "Fully supervised fine-tuning on FOLIO training set with classification head.",
            "performance": "56.83% accuracy on FOLIO test set (three-way truth-value classification).",
            "baseline_comparison": "Above majority baseline (38.5%) and random (33.3%); similar to RoBERTa-base; lower than larger models and best finetuned model (Flan-T5-Large 65.9%).",
            "limitations_or_failures": "Struggles on examples with longer reasoning depths and complex FOL patterns compared to larger models and specialized neuro-symbolic methods.",
            "insights_or_conclusions": "Supervised fine-tuning on FOLIO gives nontrivial competence but is insufficient to reach expert-level reasoning; capturing deep multi-step FOL patterns remains challenging.",
            "uuid": "e8623.0",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "BERT-large",
            "name_full": "BERT (large)",
            "brief_description": "Larger variant of BERT used as a fine-tuning baseline for predicting truth values on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-large",
            "model_description": "BERT large Transformer encoder fine-tuned with a two-layer classification head for the 3-way classification task.",
            "model_size": "340M",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning)",
            "reasoning_task_description": "3-way truth-value classification of NL conclusions under premises with FOL annotations.",
            "method_or_approach": "Fully supervised fine-tuning on FOLIO training set.",
            "performance": "59.0% accuracy on FOLIO test set.",
            "baseline_comparison": "About 2.2% absolute improvement over BERT-base; still below RoBERTa-large and Flan-T5-Large.",
            "limitations_or_failures": "Fails on longer multi-step reasoning chains and high logical depth examples; tends to overpredict True labels relative to False/Unknown.",
            "insights_or_conclusions": "Scaling encoder size brings modest gains but does not close the gap to expert performance or to models augmented with logical solvers.",
            "uuid": "e8623.1",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "RoBERTa-base",
            "name_full": "RoBERTa (base)",
            "brief_description": "A robustly optimized BERT-style encoder evaluated via supervised fine-tuning on FOLIO for logical truth-value prediction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base",
            "model_description": "Transformer encoder (Robustly optimized BERT) fine-tuned with a classification head for the three-way FOLIO task.",
            "model_size": "110M",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning)",
            "reasoning_task_description": "Predict True/False/Unknown for conclusions given NL premises annotated in FOL.",
            "method_or_approach": "Fully supervised fine-tuning.",
            "performance": "56.8% accuracy on FOLIO test set.",
            "baseline_comparison": "Similar to BERT-base; underperforms RoBERTa-large and Flan-T5-Large.",
            "limitations_or_failures": "Limited on deep multi-step reasoning chains and HybLogic subset with high reasoning depth.",
            "insights_or_conclusions": "Standard pretrained encoders provide a baseline but require further architectural or symbolic support to handle FOL-level reasoning.",
            "uuid": "e8623.2",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "RoBERTa-large",
            "name_full": "RoBERTa (large)",
            "brief_description": "Large variant of RoBERTa fine-tuned on FOLIO; shows improved supervised performance over base models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "Large Transformer encoder fine-tuned for 3-way classification on FOLIO.",
            "model_size": "340M",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning)",
            "reasoning_task_description": "Classify conclusion truth values given premises, with parallel FOL verification.",
            "method_or_approach": "Fully supervised fine-tuning.",
            "performance": "62.1% accuracy on FOLIO test set.",
            "baseline_comparison": "Improves ~3.1% over BERT-large and ~5.3% over RoBERTa-base; still below best fine-tuned Flan-T5-Large and top prompting/method results.",
            "limitations_or_failures": "Although better on HybLogic than WikiLogic under fine-tuning, still fails on many long reasoning chains and complex AST patterns.",
            "insights_or_conclusions": "Fine-tuning can learn some template-like patterns (HybLogic) but generalization to diverse, deep FOL patterns remains limited.",
            "uuid": "e8623.3",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Flan-T5-Large",
            "name_full": "Flan-T5 (Large)",
            "brief_description": "Instruction-tuned encoder-decoder Transformer (T5 family) fine-tuned on FOLIO; obtained the highest accuracy among supervised fine-tuning baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Large",
            "model_description": "Instruction-finetuned T5 sequence-to-sequence model used here as a classifier via fine-tuning for the FOLIO three-way task.",
            "model_size": "783M",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning)",
            "reasoning_task_description": "Input: NL premises and conclusions; output: truth labels; problems annotated in first-order logic.",
            "method_or_approach": "Fully supervised fine-tuning on FOLIO training data.",
            "performance": "65.7% accuracy on FOLIO test set (highest among finetuned baselines).",
            "baseline_comparison": "Beats other fine-tuned encoder baselines (BERT, RoBERTa) by several points but is below the best prompting/method-enhanced LLM results (e.g., GPT-4 with advanced prompting and Logic-LM).",
            "limitations_or_failures": "Still far from expert human performance (95.98%); struggles on the highest reasoning depths and some HybLogic patterns.",
            "insights_or_conclusions": "Instruction tuning + finetuning improves model's ability to map NL to truth-value judgments, but deeper symbolic reasoning requires additional mechanisms.",
            "uuid": "e8623.4",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LLaMA-13B",
            "name_full": "LLaMA 13B",
            "brief_description": "Open-source large language model evaluated in few-shot prompting settings on FOLIO; shows near-chance performance under few-shot.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-13B",
            "model_description": "13-billion-parameter autoregressive Transformer (LLaMA family) used with few-shot prompting (NL examples) to predict conclusion truth values.",
            "model_size": "13B",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning) - few-shot prompting",
            "reasoning_task_description": "Few-shot NL prompting to predict True/False/Unknown on NL stories requiring FOL reasoning.",
            "method_or_approach": "Few-shot natural language prompting (NL examples), no specialized logical augmentation.",
            "performance": "33.63% accuracy (8-shot NL prompt) â€” only slightly above random (33.3%).",
            "baseline_comparison": "Substantially worse than larger LLaMA (70B) and commercial LLMs (GPT-3.5/GPT-4); near random.",
            "limitations_or_failures": "Cannot reliably perform multi-step FOL reasoning in few-shot; fails on almost all high-depth examples.",
            "insights_or_conclusions": "Model scale and pretraining matter strongly for emergent chain-of-thought capabilities; small-ish LLMs without special prompting fail on FOLIO.",
            "uuid": "e8623.5",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LLaMA-70B",
            "name_full": "LLaMA 70B",
            "brief_description": "70B-parameter LLaMA model evaluated with few-shot prompting and reasoning prompts (CoT, ToT); achieves better but still limited performance on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-70B",
            "model_description": "70-billion-parameter autoregressive Transformer (LLaMA family) used with few-shot prompting and reasoning strategies (Chain-of-Thought, Tree-of-Thought).",
            "model_size": "70B",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning) - few-shot prompting with CoT/ToT",
            "reasoning_task_description": "Predict truth values of NL conclusions given premises; evaluate effect of prompting strategies.",
            "method_or_approach": "8-shot NL prompting; Chain-of-Thought (CoT) and Tree-of-Thought (ToT) prompting variants were applied.",
            "performance": "44.0% (8-shot NL prompt); CoT: 47.8%; ToT: 48.4% accuracy.",
            "baseline_comparison": "~10% absolute better than LLaMA-13B; still well below GPT-3.5/GPT-4 few-shot performance and advanced prompting results.",
            "limitations_or_failures": "Improvements from CoT/ToT modest; still fails many deep multi-step reasoning problems and HybLogic examples.",
            "insights_or_conclusions": "Scaling improves capability, and reasoning prompts help modestly, but scale + prompting alone are insufficient for complex FOL reasoning.",
            "uuid": "e8623.6",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "text-davinci-002",
            "name_full": "text-davinci-002",
            "brief_description": "An OpenAI GPT-3.5-era model variant used as a few-shot/few-shot-CoT baseline for FOLIO; intermediate performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_description": "Instruction-tuned GPT-3 family variant used with few-shot prompting (8-shot) for truth-value prediction.",
            "model_size": "hidden / not reported in paper",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning) - few-shot prompting",
            "reasoning_task_description": "8-shot NL prompts to classify conclusions as True/False/Unknown.",
            "method_or_approach": "Few-shot NL prompting (8-shot); no CoT variants reported for this model in paper.",
            "performance": "49.53% accuracy (8-shot NL prompt).",
            "baseline_comparison": "Below GPT-3.5-Turbo (58.3%) and GPT-4 variants; above LLaMA models.",
            "limitations_or_failures": "Limited chain-of-thought reasoning in this task; lower performance compared to later GPT-3.5/GPT-4 results.",
            "insights_or_conclusions": "Intermediate capability suggests instruction-tuning improves over base models but more advanced models/strategies significantly help.",
            "uuid": "e8623.7",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo",
            "brief_description": "A GPT-3.5-generation OpenAI model used both zero-shot and few-shot prompting for NL reasoning and NL-&gt;FOL translation tasks on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Autoregressive Transformer LLM by OpenAI; used with zero-shot and few-shot prompts for truth-value prediction and NL-&gt;FOL translation.",
            "model_size": "hidden / not reported in paper",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning) and NL-FOL translation",
            "reasoning_task_description": "NL reasoning: 3-way truth-value classification; NL-FOL translation: generate FOL formulas and execution accuracy via FOL prover.",
            "method_or_approach": "Few-shot NL prompting (8-shot); zero-shot also evaluated for reasoning; few-shot NL-FOL translation prompts.",
            "performance": "NL reasoning: zero-shot 53.1% (reported), few-shot 58.34% accuracy; NL-FOL translation: few-shot SynV 93.3% and ExcAcc 56.0%.",
            "baseline_comparison": "Better than text-davinci-002 and LLaMA models; worse than GPT-4 and specialized logic methods (Logic-LM, DetermLR).",
            "limitations_or_failures": "Lower execution accuracy in NL-FOL translation compared to GPT-4; struggles on high-depth HybLogic examples; does not benefit from FOL concatenation as GPT-4 does.",
            "insights_or_conclusions": "Sufficient scale and prompting give moderate performance, but GPT-3.5 lacks some of GPT-4's ability to use explicit FOL in prompts and to generalize to complex FOL reasoning.",
            "uuid": "e8623.8",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "State-of-the-art closed-source LLM used as the strongest few-shot baseline and base model for logic-specific methods; evaluated with advanced prompting and neuro-symbolic methods on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large multimodal Transformer LLM from OpenAI (exact size undisclosed); used with few-shot prompting, Chain-of-Thought, Self-Consistency, Tree-of-Thought, and as the base for Logic-LM/LINC/DetermLR experiments.",
            "model_size": "hidden / not reported in paper",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning) and NL-FOL translation",
            "reasoning_task_description": "Predict True/False/Unknown labels and generate parallel FOL formulas (NL-&gt;FOL translation) evaluated via an FOL prover.",
            "method_or_approach": "Few-shot prompting (8-shot) with CoT, CoT+Self-Consistency (SC), Tree-of-Thought (ToT); used as base model for neuro-symbolic methods (Logic-LM, LINC, DetermLR). Also evaluated with concatenated NL+FOL in prompt.",
            "performance": "NL reasoning: few-shot 64.2% (8-shot baseline); CoT 68.9%; CoT+SC 69.5%; ToT 70.0%; best logic-specific method results using GPT-4 base: Logic-LM 78.1%, DetermLR 77.5%, LINC 73.1%. NL-FOL translation (few-shot): SynV 93.9%, ExcAcc 63.8%.",
            "baseline_comparison": "Substantially better than GPT-3.5, LLaMA, and finetuned medium models; neuro-symbolic augmentations (Logic-LM, DetermLR, LINC) further improve performance by ~9â€“13% absolute over vanilla few-shot GPT-4.",
            "limitations_or_failures": "Significant gap to expert human accuracy (95.98%); fails frequently on HybLogic (high depth) and on examples requiring long reasoning chains; human error analysis attributes ~65% of GPT-4 errors to faulty reasoning chains, 25% to wrong derivations, 5% to syntactic comprehension, 5% to spurious shortcuts.",
            "insights_or_conclusions": "GPT-4 is the strongest baseline but still struggles to reliably perform deep FOL deduction; combining LLMs with symbolic provers or structured reasoning workflows substantially improves logical fidelity; supplying FOL alongside NL helps GPT-4 (NL+FOL &gt; NL alone).",
            "uuid": "e8623.9",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs to improve multi-step problem solving; applied to GPT-4 and others on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought prompting (applied to GPT-4, LLaMA-70B, etc.)",
            "model_description": "Prompt engineering method that asks the model to output step-by-step intermediate reasoning (reasoning traces) before final answer.",
            "model_size": "n/a (method applied to models of various sizes)",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic (NL reasoning)",
            "reasoning_task_description": "Used to coax intermediate deductions to improve multi-step logical inference for the 3-way truth-value classification task.",
            "method_or_approach": "Few-shot CoT prompting (8-shot); also combined with Self-Consistency and Tree-of-Thought variants.",
            "performance": "Improves GPT-4 few-shot from 64.2% to 68.9% (absolute +4.7%); LLaMA-70B improved ~3â€“4 points with CoT.",
            "baseline_comparison": "Outperforms vanilla few-shot prompting for capable LLMs (GPT-4), but gains are modest and insufficient to reach expert-level performance alone.",
            "limitations_or_failures": "CoT sometimes fails to produce correct intermediate steps, leading to faulty final conclusions; self-consistency yields small extra gains.",
            "insights_or_conclusions": "CoT helps but does not fully remedy failures on long FOL deduction chains; combining with symbolic checking or neuro-symbolic architectures is more effective.",
            "uuid": "e8623.10",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency",
            "brief_description": "A technique that samples multiple chain-of-thought traces and selects the most consistent final answer; applied to GPT-4 on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Self-Consistency (applied to GPT-4 Chain-of-Thought outputs)",
            "model_description": "Aggregation technique that draws multiple reasoning traces and picks the most frequent final answer to reduce reasoning noise.",
            "model_size": "n/a",
            "reasoning_task_name": "FOLIO NL reasoning (few-shot with CoT + SC)",
            "reasoning_task_description": "Used to improve stability and accuracy of chain-of-thought reasoning for truth-value prediction.",
            "method_or_approach": "Combine CoT prompting with sampling and majority-selection over outputs.",
            "performance": "Small improvement over CoT for GPT-4: CoT 68.9% -&gt; CoT+SC 69.5% (absolute +0.6%).",
            "baseline_comparison": "Marginal gain over CoT; less impactful than Tree-of-Thought or neuro-symbolic methods.",
            "limitations_or_failures": "Aggregation cannot fix fundamentally incorrect deduction chains; limited benefit on highly complex FOL problems.",
            "insights_or_conclusions": "Helpful as a noise-reduction mechanism but not a substitute for symbolic verification or stronger deductive procedures.",
            "uuid": "e8623.11",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Tree-of-Thought (ToT)",
            "name_full": "Tree-of-Thought prompting",
            "brief_description": "A deliberative search-based prompting approach that explores multiple reasoning paths (tree search) to find better solutions; applied to GPT-4 and LLaMA-70B on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Tree-of-Thought prompting (applied to GPT-4, LLaMA-70B)",
            "model_description": "Prompting/control-flow approach that constructs and explores a tree of intermediate reasoning states rather than a single linear chain.",
            "model_size": "n/a",
            "reasoning_task_name": "FOLIO NL reasoning (few-shot with ToT)",
            "reasoning_task_description": "Used to search alternative reasoning chains to better handle multi-step FOL deductions.",
            "method_or_approach": "Tree-of-Thought reasoning prompting and search over intermediate steps.",
            "performance": "GPT-4 ToT achieved 70.0% accuracy (slightly better than CoT+SC); LLaMA-70B ToT improved modestly to 48.4%.",
            "baseline_comparison": "Slight improvement over CoT and SC for high-capability models, but still inferior to neuro-symbolic methods like Logic-LM.",
            "limitations_or_failures": "Exploration increases compute and may still fail to find required intermediate steps for deep FOL chains.",
            "insights_or_conclusions": "Search-based prompting can yield additional gains but does not eliminate the need for symbolic reasoning or formal provers for strict FOL deduction.",
            "uuid": "e8623.12",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "brief_description": "A neuro-symbolic method that augments an LLM with symbolic solvers to improve logical fidelity; evaluated using GPT-4 as backbone on FOLIO and produces large accuracy gains.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Logic-LM (GPT-4 base in experiments)",
            "model_description": "Neuro-symbolic pipeline that integrates GPT-4 with symbolic reasoning components/solvers to generate logically faithful deductions and verify results.",
            "model_size": "GPT-4 base (size undisclosed)",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic",
            "reasoning_task_description": "Improves multi-step logical deduction by combining LLM generation with symbolic provers and verification to produce trustworthy truth-value predictions.",
            "method_or_approach": "Neuro-symbolic integration: LLM + symbolic solver(s) for deduction and verification.",
            "performance": "78.1% accuracy on FOLIO test set (reported as Logic-LM(GPT-4) result).",
            "baseline_comparison": "About +13.9% absolute over vanilla GPT-4 few-shot (64.2%) and +8.1% over GPT-4 ToT (70.0%); the best reported method-level performance in the paper.",
            "limitations_or_failures": "While much improved, still below expert human accuracy (95.98%); complexity and engineering for integration may limit scaling and generality.",
            "insights_or_conclusions": "Combining LLMs with symbolic provers substantially improves strict FOL reasoning accuracy, highlighting the promise of neuro-symbolic approaches for formal deduction tasks.",
            "uuid": "e8623.13",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LINC",
            "name_full": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "brief_description": "Neuro-symbolic method combining an LLM with an FOL prover (LINC) to improve logical deduction; evaluated with GPT-4 base on FOLIO and yields large gains over vanilla prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LINC (GPT-4 base in experiments)",
            "model_description": "Neurosymbolic pipeline linking LLM-generated formalizations with an FOL prover to perform formal inference and verify conclusions.",
            "model_size": "GPT-4 base (size undisclosed)",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic",
            "reasoning_task_description": "Translate NL to FOL (or generate intermediate formal steps) and use a symbolic prover to derive truth-values robustly.",
            "method_or_approach": "Neuro-symbolic integration: language model + FOL prover; combines generation and formal execution.",
            "performance": "73.1% accuracy on FOLIO test set.",
            "baseline_comparison": "â‰ˆ+9% absolute over vanilla GPT-4 few-shot; outperformed by Logic-LM and DetermLR in this study but still a substantial improvement over raw prompting.",
            "limitations_or_failures": "Dependent on quality of generated formalization; errors in translation to FOL can limit prover effectiveness.",
            "insights_or_conclusions": "Neuro-symbolic pipelines that reliably produce formal input to provers are effective for improving strict logical reasoning in LLMs.",
            "uuid": "e8623.14",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "DetermLR",
            "name_full": "DetermLR: From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models",
            "brief_description": "A method that augments LLMs with deterministic logical reasoning components; applied with GPT-4 base and shows large improvements on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DetermLR (GPT-4 base in experiments)",
            "model_description": "Approach that augments LLM outputs with deterministic logical modules or reasoning checks to reduce indeterminate/incorrect deductions.",
            "model_size": "GPT-4 base (size undisclosed)",
            "reasoning_task_name": "FOLIO: Natural language reasoning with first-order logic",
            "reasoning_task_description": "Enhance LLM reasoning by determinizing intermediate steps or integrating symbolic checks to improve final truth-value judgments.",
            "method_or_approach": "Neuro-symbolic / hybrid approach combining LLM reasoning with deterministic logical augmentation or checking.",
            "performance": "77.5% accuracy on FOLIO test set.",
            "baseline_comparison": "Comparable to Logic-LM (78.1%) and superior to vanilla GPT-4 prompting; about +13% absolute improvement over GPT-4 few-shot baseline.",
            "limitations_or_failures": "Requires robust mechanisms for converting NL to deterministic logical representations; residual failures persist on highest-depth cases.",
            "insights_or_conclusions": "DetermLR demonstrates that adding determinizing/symbolic layers to an LLM substantially improves formal logical reasoning on complex FOL tasks.",
            "uuid": "e8623.15",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models",
            "rating": 2,
            "sanitized_title": "from_indeterminacy_to_determinacy_augmenting_logical_reasoning_capabilities_with_large_language_models"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 1,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "RuleTaker: Transformers as soft theorem provers (Transformers as soft reasoners over language)",
            "rating": 1,
            "sanitized_title": "ruletaker_transformers_as_soft_theorem_provers_transformers_as_soft_reasoners_over_language"
        }
    ],
    "cost": 0.019789,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FOLIO: Natural Language Reasoning with First-Order Logic
11 Oct 2024</p>
<p>Simeng Han 
Yale University</p>
<p>Hailey Schoelkopf 
Yale University</p>
<p>Yilun Zhao 
Yale University</p>
<p>Zhenting Qi 
Harvard University</p>
<p>Martin Riddell 
Yale University</p>
<p>Wenfei Zhou 
NVIDIA</p>
<p>James Coady 
Yale University</p>
<p>David Peng 
Yale University</p>
<p>Yujie Qiao 
Yale University</p>
<p>Luke Benson 
Yale University</p>
<p>Lucy Sun 
Yale University</p>
<p>Alex Wardle-Solano 
Yale University</p>
<p>Hannah Szabo 
Yale University</p>
<p>Ekaterina Zubova 
Yale University</p>
<p>Matthew Burtell 
Yale University</p>
<p>Jonathan Fan 
Iowa City West High School</p>
<p>Yixin Liu 
Yale University</p>
<p>Brian Wong 
Yale University</p>
<p>Malcolm Sailor 
Yale University</p>
<p>Ansong Ni 
Yale University</p>
<p>Linyong Nan 
Yale University</p>
<p>Jungo Kasai 
University of Washington</p>
<p>Tao Yu 
University of Hong Kong</p>
<p>Rui Zhang 
Penn State University
8 Meta AI</p>
<p>Alexander R Fabbri 
Salesforce Research</p>
<p>Wojciech KryÅ›ci Åƒski 
Salesforce Research</p>
<p>Semih Yavuz 
Salesforce Research</p>
<p>Ye Liu 
Salesforce Research</p>
<p>Victoria Xi 
Lin 
Shafiq Joty 
Salesforce Research</p>
<p>Yingbo Zhou 
Salesforce Research</p>
<p>Caiming Xiong 
Salesforce Research</p>
<p>Rex Ying 
Yale University</p>
<p>Arman Cohan 
Yale University</p>
<p>Dragomir Radev 
Yale University</p>
<p>Salesforce Research</p>
<p>FOLIO: Natural Language Reasoning with First-Order Logic
11 Oct 202491EE60091D091474A0C7AA791AED16AEarXiv:2209.00840v3[cs.CL]
Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks.However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model.We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations.FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion.The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine.In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset.Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models.For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models.Our results show that a subset of FOLIO presents a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved remarkable performance on a variety of natural language tasks (OpenAI et al., 2023;Touvron et al., 2023;Srivastava et al., 2023;Wang et al., 2019a).Logical reasoning is a central component for intelligent systems and should be sufficiently and independently evaluated (Russell and Norvig, 2010).</p>
<p>However, existing natural language tasks are inadequate in measuring the complex logical reasoning capability of a model (Srivastava et al., 2023;Saparov and He, 2023;Tian et al., 2021).</p>
<p>Several datasets related to logical reasoning have recently been proposed.However, existing benchmarks often exhibit limited complexity in reasoning or lack language naturalness.Some of these common benchmarks do not specifically evaluate logical reasoning independently of other forms of reasoning (Yu et al., 2020;Liu et al., 2021).Those specifically designed for measuring logical reasoning are insufficient in terms of logical reasoning complexity and natural language variety.As shown in Table 1, examples in RuleTaker (Clark et al., 2020) and LogicNLI (Tian et al., 2021) need at most five depths of reasoning.The entire corpus of RuleTaker or LogicNLI has fewer than 50 distinct abstract syntax trees.RuleTaker has only 101 words in its vocabulary and LogicNLI has 1077 words in the vocabulary.Moreover, none of them are written by humans with information drawn from real-world knowledge, making them less applicable to real-world reasoning scenarios.The logical deduction portion in BigBench (Srivastava et al., 2023) requires commonsense reasoning besides logical deduction.ProntoQA (Saparov and He, 2023) only contains logical reasoning questions that are answerable with repeated applications of the Modus Ponens inference rule.</p>
<p>We present a natural language reasoning dataset, FOLIO, with first-order logic reasoning problems which require the models to decide the correctness of conclusions given a world defined by the premises.In FOLIO, we aim to ensure high lan- A FOLIO example based on the Wild Turkey Wikipedia page: https://en.wikipedia.org/wiki/Wild_turkeyNL premises NL Conclusions -&gt; Labels 1.There are six types of wild turkeys: Eastern wild turkey, Osceola wild turkey, Gould's wild turkey, A. Tom is an Ocellated wild turkey.-&gt; True Merriam's wild turkey, Rio Grande wild turkey, and the Ocellated wild turkey.</p>
<p>B. Tom is an Eastern wild turkey.-&gt; False 2. Tom is not an Eastern wild turkey.</p>
<p>C. Joey is a wild turkey.-&gt; Unknown 3. Tom is not an Osceola wild turkey.4. Tom is also not a Gould's wild turkey.5. Tom is neither a Merriam's wild turkey, nor a Rio Grande wild turkey.6. Tom is a wild turkey.</p>
<p>FOL Premises</p>
<p>FOL conclusions -&gt; Labels
1. âˆ€x(WildTurkey(x) â†’ (EasternWildTurkey(x) âˆ¨ OsceolaWildTurkey(x) âˆ¨ GouldsWildTurkey(x) A. OcellatedWildTurkey(tom) -&gt; True âˆ¨ MerriamsWildTurkey(x) âˆ¨ RiograndeWildTurkey(x) âˆ¨ OcellatedWildTurkey(x))) B. EasternWildTurkey(tom) -&gt; False 2. Â¬EasternWildTurkey(tom)
C. WildTurkey(joey) -&gt; Unknown 3. Â¬OsceolaWildTurkey(tom)) 4. Â¬GouldsWildTurkey(tom) 5. Â¬MerriamsWildTurkey(tom) âˆ§ Â¬RiograndeWildTurkey(tom) 6. WildTurkey(tom) Table 2: An example story in FOLIO based on the knowledge from the Wikipedia page on wild turkeys.The story consists of five premises and three conclusions with their corresponding FOL formulas and labels for the conclusions.All five premises are needed to infer the conclusions.The model needs to reason under logic patterns with universal quantification (âˆ€), negation (Â¬), conjunction (âˆ§), and disjunction (âˆ¨).guage naturalness and complexity, an abundant vocabulary, and factuality while also maintaining high reasoning complexity.FOLIO is a high-quality and manually curated dataset, written by CS undergraduate and graduate students and researchers in academia and industry.To ensure the conclusions of our examples follow the premises logically, we annotated all reasoning examples with first-order logic (FOL) formulas.An example of FOLIO is shown in Table 2. Based on our annotations, we propose a new NL-FOL translation task where an NL reasoning example is translated into its FOL counterpart.Finally, we benchmark the performance of strong LMs in both fully supervised and few-shot settings to understand their capabilities in logical reasoning (i.e., deriving the truth value of a logical conclusion from NL premises).</p>
<p>Under the few-shot setting, the most capable publicly available LLM so far achieves only 53.1% on the stories written in a hybrid manner, which is slightly better than random.</p>
<p>To sum up, the contributions of this paper are threefold.1) We release a natural language reasoning dataset written by expert annotators, FOLIO, with first-order logical reasoning problems.2) We use formal logic, i.e., FOL to ensure the logical validity of the examples written in NL and propose a new NL-FOL translation task.3) We benchmark the performance of LMs by fine-tuning models and prompting LLMs with few-shot examples, on the FOLIO reasoning task.We hope that FOLIO, as a challenging logical reasoning dataset, will be used to facilitate measuring progress in the logical reasoning capabilities of language models.</p>
<p>Related Work</p>
<p>Datasets for reasoning from text</p>
<p>Developing models that can reason in texts has been a core goal in NLP since the field's early days (Cooper et al., 1996).Since then, there has been massive progress in reasoning over text.Various benchmarks that focus on different aspects of reasoning over textual inputs are proposed, including natural language inference (NLI) (Bowman et al., 2015;Wang et al., 2019b), reasoning for commonsense knowledge (Talmor et al., 2019;He et al., 2021) and multi-hop reasoning (Yang et al., 2018;Chen et al., 2020).Among these reasoning abilities, logical reasoning has recently attracted an increasing amount of study.ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2021) both collected multiplechoice questions from standardized graduate admission examinations, answering which requires various types of logical reasoning.However, these datasets cover mixed forms of reasoning and are not intended to test logical reasoning in isolation.</p>
<p>Meanwhile, testing logical reasoning in isolation without involving other forms of reasoning has also attracted researchers in recent years.CLUTRR (Sinha et al., 2019) covers inductive reasoning, which is beyond the scope of first-order logic.Synthetic corpuses of deductive reasoning are proposed to evaluate the deductive reasoning ability of pretrained LMs (Clark et al., 2021;Saeed et al., 2021;Tian et al., 2021).However, these datasets do not contain highly natural sentences and often cover limited forms of logic while FOL is much more expressive.Kazemi et al. (2023) created a dataset for reasoning with contradictory information.Kawabata and Sugawara (2023) crowdsourced rationales for over 3000 examples based on ReClor (Yu et al., 2020).ProntoQA (Saparov and He, 2023) is comprised solely of logical reasoning queries that can be resolved through applying the Modus Ponens inference rule while FOLIO questions require applications of multiple types of inference rules.As shown in Table 1, FOLIO is the first large-scale first-order logic (FOL) reasoning dataset with formal logic annotations in FOL.FO-LIO is logically diverse and complex with complex natural language sentences and a rich vocabulary.</p>
<p>Reasoning using large language models</p>
<p>Reasoning has been demonstrated as one of the emergent abilities of LLMs of sufficient scale recently (Talmor et al., 2020;Wei et al., 2022a;Chowdhery et al., 2022).One such emergent behavior, Chain-of-Thought prompting (Wei et al., 2022b), consists of a series of intermediate reasoning steps output by an LLM.This improves the performance on arithmetic, commonsense, and symbolic reasoning benchmarks significantly.There has been a line of research continuing on from Chain-of-Thought (Kojima et al., 2022;Li et al., 2022;Yao et al., 2023) to elicit reasoning behavior from LLMs.Building on Chain-of-Thought prompting, many techniques used on top of LLMs to improve downstream performance have been formalized into control flows and programs.These are called language model cascades (Dohan et al., 2022), subsuming techniques such as Chain-of-Thought prompting, STaR (Zelikman et al., 2022), and Selection-Inference (Creswell et al., 2022) for reasoning.Dasgupta et al. (2022) studied the reasoning ability of LLMs but only used a small set of 48 syllogisms with only two premises each.Saparov and He (2023) created a synthetic dataset that and showed that LLMs are capable of making correct individual deduction steps.</p>
<p>With FOLIO, we aim to set a high standard, ensuring that achieving high performance through superficial strategies and shallow heuristics is prevented, allowing a robust evaluation of the firstorder logic reasoning capabilities of LLMs.We show that many LLMs fall short on complex firstorder logic reasoning, and that significant room for improvement in this area remains.</p>
<p>FOLIO Corpus Construction</p>
<p>We collected FOLIO through a carefully designed manual annotation process to achieve high-quality examples that necessitate complex logical reasoning.Writing natural language reasoning stories with FOL requires sufficient knowledge in both semantic parsing and first-order logic, as well as strong analytical skills.Given the complexities of such annotations, we selected annotators based on a few important criteria to ensures that our dataset is annotated with the highest level of precision and expertise, reflecting the complexity and nuance required for first-order logical reasoning.1).Our annotators are either college or graduate students who are native English speakers or possess nearnative proficiency in English.4 2).They possess formal education in first-order logic, having either completed relevant coursework or undertaken self-directed studies in first-order logic or seman-tic parsing.At the NL quality check stage, only annotators who are experts in natural language processing or computational linguistics are involved.For the FOL quality check, only annotators who are experts in first-order logic are involved.We also give the annotators several training sessions on how to write a story, by providing them with detailed annotation guidelines.All stories and FOL annotations in FOLIO are written and reviewed by expert annotators, including CS undergraduate and graduate students, and senior researchers, who met the aforementioned criteria.</p>
<p>We develop our dataset in six stages: WikiLogic collection, HybLogic collection, NL quality control, FOL quality control, NL-FOL alignment and FOL verification, spending 980 man-hours in total.</p>
<p>Example collection</p>
<p>We collected our dataset using two different methods in order to obtain examples that are both logically diverse and complex and have abundant abstract syntax tree (AST) variations.The annotators are free to write stories based on any topic they want while writing the stories.</p>
<p>WikiLogic: annotation from scratch using Wikipedia articles as seeds.At this annotation stage, the annotators are asked to select random Wikipedia pages by repeatedly using the Wikipedia Special Random link. 1 The Wikipedia articles are used to develop ideas for topics to write new stories.We ask the annotators to create new stories from scratch without using templates based on realworld knowledge, which should be plausible in general.Each of the stories is composed of several premises and conclusions with truth values of True, False, or Unknown (see Table 2 for an example).We also ask the annotators to write parallel FOL sentences for both the premises and conclusions.This results in a wide range of topics, abundant AST variations, and a wide vocabulary for FOLIO.Table 1 shows a comparison of FOLIO with other reasoning datasets that purely evaluate first-order logic or deductive reasoning.</p>
<p>HybLogic: hybrid annotation The task of generating logically sound stories from scratch for a set of facts is very time-consuming for human writers, where the main challenge is to create complex and varied logical patterns to arrive at a conclusion.To address the problems of solely using manual 1 https://en.wikipedia.org/wiki/Special:Randomannotation, we also consider a hybrid approach to facilitate the process.Our hybrid method is based on a common form of logical stories: syllogisms.A syllogism consists of two premises and a single conclusion, and the conclusion states some facts about the entities and categories in the premises.</p>
<p>In this approach, we first generate logically valid stories, which are templates containing abstract categories and entities, by combining multiple syllogisms into a single story template: the conclusion of one syllogism is used as a premise for the next syllogism.There are 256 logically distinct types of syllogisms and 24 of them are valid (Lehman, 1973).We use various combinations of 24 valid syllogisms.We also add in conjunction, disjunction, and implication.We show an example of the resulting templates in Appendix B. We then ask human annotators to assign nouns, phrases, or clauses to the abstract entities or categories that reflect real-life scenarios to each template and write logically-valid stories in natural language.The usage of the template is to ensure that we have a set of varied and complex logical stories with multiple conclusions.There are many ways of expressing the same logic template in natural language, and so the generated templates augment, rather than limit, the creativity of humans.</p>
<p>Quality control for NL sentences</p>
<p>To ensure the highest quality of the dataset, we dedicated considerable attention to the following key aspects of the natural language sentences during the quality control process.</p>
<p>Factuality and bias Our dataset prioritizes realism and factual accuracy, steering clear of biases and stereotypes linked to identity markers like race, ethnicity, gender, sexuality, nationality, class, and religion.Toward these objectives, we manually screened all stories and found that 39.2% of the stories suffer from at least one of these issues.We implemented a detailed protocol to rewrite these stories.The protocol is in Appendix C.</p>
<p>Language quality Apart from grammar, we make sure the sentences in our dataset are highly natural.All the sentences are first checked with a grammar checking tool, Grammarly.Our annotators who have graduated from or are senior students studying English Literature conducted a thorough round of review for grammatical correctness and language naturalness.We also eliminate natural language ambiguity when it is possible.We include rules on eliminating ambiguity in Appendix D. Employing these rules effectively reduces the ambiguity of natural language in this reasoning dataset, but incurs the tradeoff of limiting variations in some usage of language.However, we note that there is still sufficient variation in terms of sentence structures and logical structures as shown in Table 1.</p>
<p>Quality control for FOL formulas</p>
<p>We adopt the FOL definitions and syntax most widely used in the AI community (Russell and Norvig, 2010).We include more details on the definition of FOL we consider and the FOL modelling convention in Appendix E In preliminary investigations, we found that the human-written FOL formulas suffer from FOL consistency issues, which necessitates an additional round of quality control for FOL formulas.</p>
<p>FOL consistency One NL sentence can be translated into FOL through multiple non-equivalent ways.For example, sometimes additional information inferred from a sentence can be represented in FOL, leading to multiple representations.We therefore design an annotation protocol for FOL translation in order to ensure that our FOL translations are as consistent as possible across all examples in our dataset.We highlight a few important strategies used in the annotation protocol in Appendix F.</p>
<p>NL-FOL alignment review</p>
<p>Apart from checking whether NL and FOL express equivalent meanings, we also add necessary commonsense knowledge in both the NL and FOL premises.Sometimes humans do not write certain commonsense knowledge in the premises that is required in the FOL reasoning process, which is based solely on the premises given.We add such knowledge as additional premises at this stage.In particular, intrinsic properties of some predicates are required in the FOL reasoning process.For example, "LocatedIn(x,y)" should be transitive and "BeFamily(x,y)" should be symmetric.</p>
<p>FOL verification</p>
<p>Recognizing that the FOL formula annotations can be error-prone, we verify the syntactic validity and label consistency of FOL formula annotations with an FOL inference engine.We include the details of the FOL inference engine in Appendix G.</p>
<p>Dataset statistics</p>
<p>We show basic statistics of FOLIO and demonstrate the abundant vocabulary and logical complexity of FOLIO: Tables 1, 3 and Figure 1.Natural language complexity We use the Dale-Chall Readability Formula (Dale andChall, 1948, 1995) to show the text complexity of FOLIO following (Singh et al., 2023;Arps et al., 2022;Wei et al., 2021).We show the distribution of readability in Appendix H.</p>
<p>Basic statistics</p>
<p>Logical complexity and diversity statistics</p>
<p>As shown in Figure 1, the mode of reasoning depths is four in FOLIO.28.7% of the examples need five or more depths of reasoning to infer the conclusions, while the previous datasets needed at most five reasoning depths as shown in Table 1.This illustrates the logical complexity of FOLIO.Table 1 shows that FOLIO also has a much larger number of distinct ASTs than the previous datasets, indicating that FOLIO is much more logically diverse.larger than the previous synthetically constructed datasets for logical reasoning.</p>
<p>Vocabulary and topics</p>
<p>Task Definition</p>
<p>We define two new tasks based on FOLIO, natural language reasoning with first-order logic and NL-FOL translation.</p>
<p>Natural language reasoning with first-order logic</p>
<p>Each natural language (NL) story S in FOLIO consists of n premises: P = {p 1 , p 2 , ..., p n } and m conclusions: H = {h 1 , h 2 , ..., h m }.All NL stories are annotated with parallel FOL stories SF , which are sets of FOL formulas consisting of n premises P F = {pf 1 , pf 2 , ..., pf n } and m conclusions HF = {hf 1 , hf 2 , ..., hf m }. pf i and hf i are logically and semantically similar to p i and h i , respectively.Given P and H, the goal is to determine the truth values of the conclusions: "True", "False" or "Unknown", based on FOL reasoning.</p>
<p>NL-FOL translation</p>
<p>We propose a new natural language to first-order logic translation (NL-FOL translation) task alongside our reasoning dataset.The goal of this task is to translate an NL story S to an FOL story F S.</p>
<p>In particular, each of the NL sentence p i or h i and the parallel FOL formula pf i or hf i should be logically and semantically equivalent.Moreover, the truth values for the conclusions should be the same based on the NL story S and the parallel FOL story F S. In our dataset, the premises and conclusions are set up in such a way to ensure that the inference engine always returns an answer given enough resources such as time and memory.Unlike previous work (Singh et al., 2020) which translates problems with a single premise and a single hypothesis, our task is on translating examples of various lengths with a focus on stories with multiple premises.Thus, it also requires the models to consider discourse-level consistencies as opposed to translation at the sentence level.</p>
<p>NL-FOL evaluation metrics Two metrics are adopted to evaluate NL-FOL translation to capture different aspects of the generation results: 1).Syntactic validity (SynV).The Syntactic Validity score measures whether the FOL formulas are syntactically valid.The score will be 1 if all FOL formulas of an example can pass the syntactic check and 0 otherwise 2).Inference Engine execution accuracy (ExcAcc).The group of translated FOL for premises and conclusions in one story is fed into our inference engine to output the truth value for each conclusion.We define the accuracy of the output labels as the execution accuracy.We leave for future work the design of a more reliable metric of NL-FOL translation.</p>
<p>Experiments</p>
<p>In this section, we describe our experiments and main results.</p>
<p>Experimental setup</p>
<p>Tasks We conduct experiments on the two tasks in Â§4: NL reasoning with first-order logic (logical reasoning) and NL-FOL translation (NL-FOL).</p>
<p>Dataset split We split FOLIO by 70%/15%/15% split for the train/validation/test sets with 1,001/203/226 examples respectively.We split by story so that models are evaluated on unseen stories.</p>
<p>Evaluation metrics</p>
<p>We use accuracy for evaluating logical reasoning performance.For NL-FOL translation, we use the metrics in Section 4.2.</p>
<p>Models</p>
<p>We test the logical reasoning capabilities of LMs using fully supervised fine-tuning and few-shot prompting.We also test NL-FOL translation with few-shot prompting.</p>
<p>Fully supervised fine-tuning As fine-tuning baselines, we experiment with BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2020).We finetune the base and large versions of both BERT and RoBERTa, with an additional two-layer classification layer to predict the truth values.For the second task, i.e., NL-FOL translation, we only report fewshot prompting methods.Prompting strategies We experiment with incorporating recent prompting strategies into GPT-4 as they have shown improvements in the general reasoning performance of LLMs.The prompting strategies include chain-of-thought (CoT) prompting (Wei et al., 2022b), chain-of-thought prompting with self-consistency (Wang et al., 2023) and treeof-thought prompting (Yao et al., 2023).</p>
<p>Few-shot prompting</p>
<p>Logical reasoning methods</p>
<p>We also test recent methods specifically designed for logical reasoning: Logic-LM (2023), LINC (Olausson et al., 2023) and DetermLR (Sun et al., 2023), using GPT-4 as the base model.For the second task (NL-FOL translation), we use the same examples as in the Few-Shot NL experiments except that all the conclusions are included in each example.</p>
<p>We run experiments on five randomly sampled sets of examples from the training set and report the average accuracy.</p>
<p>Main results</p>
<p>Logical reasoning</p>
<p>The majority baseline of our dataset is 38.5% since in our test set, there are 87, 78 and 61 examples with labels of true, false and unknown respectively.As shown in Table 4, BERTbase and RoBERTa-base have similar performance on FOLIO with 56.83% accuracy.BERT-large has a 2.2% improvement over BERT-base.RoBERTalarge improves 3.1% over BERT-large.Flan-T5-Large achieves the highest performance in the finetuning setting and the accuracy is 65.7%.The model sizes of text-davinci-002, GPT-3.5-Turbo and GPT-4 are hidden from public3 .CoT stands for chain-of-thought prompting (Wei et al., 2022b).SC stands for self-consistency (Wang et al., 2023).ToT stands for tree-of-thought prompting (Yao et al., 2023).</p>
<p>We show that zero-shot prompting GPT-3.5 achieves better results than few-shot prompting text-davinci-002.Under few-shot NL prompting setting, LLama-13B achieves 33.63%, which is only slightly better than chance (33%).LLama-70B achieves 43.97%, around 10% better than LLaMA-13B and obtains improvements of around 4% with Chain-of-thought prompting and Tree of Thought prompting.Text-davinci-002 achieves 49.53% and GPT-3.5 achieves 58.34%.GPT-4 achieves the best results among GPT series models.</p>
<p>Incorporating recent prompting strategies increases the performance of vanilla few-shot prompting.Chain-of-thought prompting achieves more than a 4% increase over GPT-4.Self-consistency (SC) improves chain-of-thought prompting by 0.6% percent.Tree-of-thought prompting achieves slightly better result than self-consistency with chain-of-thought prompting.For the results of recent methods developed for logical reasoning, LINC (Olausson et al., 2023) achieves around a 9% increase over few-shot prompting GPT-4.Both Logic-LM (GPT-4)( 2023) and DetermLR ( 2023) achieves more than a 13% increase over few-shot prompting GPT-4, showing the superiority of the methods on logical reasoning.</p>
<p>NL-FOL translation</p>
<p>Table 5 shows the results of NL-FOL translation.The syntactic validity scores are around 93% with both GPT-3.5-Turbo and GPT-4.This indicates that language models with sufficient scales are good at picking up the patterns for FOL formulas and generating syntactically valid FOL formulas.However, GPT-3.5-Turbo and GPT-4 are not yet good at translating an NL story to a logically or semantically similar FOL counterpart, as indicated by the low inference engine execution accuracy score.</p>
<p>Error Analysis</p>
<p>Below we provide analysis of our results and key findings, providing additional insights into our dataset FOLIO and the current capabilities of LLMs in logical reasoning.example chains.We note that the presence and prevalence of these difficult examples are unique to FOLIO.FOLIO's unique complexity reveals that current LMs are limited in their ability to extrapolate to longer and more complex reasoning chains, and suggests an avenue for further study.Models have higher accuracy on WikiLogic than on HybLogic As shown in Table 6, in logical reasoning, GPT-3.5 and GPT-4 achieve substantially lower results on HybLogic than on WikiLogic and the result is slightly higher than chance.We hypothesize that this is because HybLogic has high logical complexity that the SoTA LLMs like GPT-4 cannot solve yet while WikiLogic examples require shorter reasoning chains which the model is already capable of solving.Moreover, since the examples in WikiLogic are created from scratch by humans, it is possible that LLMs have seen similar texts with similar logical patterns in the training data.However, fine-tuning RoBERTa-large achieves higher performance on HybLogic than on WikiLogic.This is likely because HybLogic is created from templates and some of the logical patterns can be learned during fine-tuning.</p>
<p>Models have higher accuracy on examples with fewer reasoning depths than on those with higher number of reasoing depths</p>
<p>In NL-FOL translation, performs 10 points better on HybLogic than WikiLogic.This could be because WikiLogic has more distinct and diverse sentence-level logical and language patterns and FOL annotations.WikiLogic has 53 ASTs while HybLogic has 33.Despite being more logically complex on a story level, FOL translations for Hy-bLogic stories have simpler logical structures on a statement level.We include case study for one WikiLogic example and one HybLogic example in Appendix I and further analysis on model performance in Appendix J.</p>
<p>Faulty path 65%</p>
<p>Wrong derivation 25%</p>
<p>Wrong syntactic comprehension 5%</p>
<p>Spurious shortcut 5%</p>
<p>GPT-4 Output</p>
<p>We know that all children are human (premise 1) and if someone is underage, they are a child (premise 2).People are either underage or of age (premise 3).If someone is of age, they can vote (premise 4) and get married (premise 5).If Jack is a child and a human, then Jack is neither able to vote nor get married (premise 6).We don't have any information about Jack's age, so we cannot determine if he is a child or of age.Therefore, we cannot determine if Jack is able to vote and get married.</p>
<p>Table 8: Case study for the scenario where a model is unable to form the correct reasoning chain.</p>
<p>Human evaluation on model outputs We conduct human evaluation on the GPT-4 model outputs with wrong truth value predictions.As shown in Table 7, approximately 65% of the time, the model struggles to construct accurate reasoning chains for complex problems with intricate steps, leading to faulty reasoning paths and indicating a limited ability to solve problems with long reasoning chains.In 25% of cases, erroneous derivations occur within certain reasoning steps, highlighting potential inaccuracies and flaws in logical deductions.5% of conclusions in FOLIO have a complex syntactic structure, posing comprehension challenges for GPT-4.5% of outputs show that GPT-4 leverage commonsense reasoning to employ spurious shortcuts that lead to the wrong truth value for the conclusion.We provide a case study for the "Faulty path" scenario in Table 8.In this instance, the model can perform simple derivations from the premises, like "If someone is of age, they can vote and get married."However, because of the problem's complexity, the model struggles to identify the essential intermediate steps and cannot ascertain the truth value of conclusions, such as "Jack is not a child."</p>
<p>Human performance</p>
<p>We collected truth value annotations of logical reasoning for FOLIO test set from expert and nonexpert annotators.Our expert annotators are computer science college students familiar with FOL.Non-expert annotators are community college or high school students who have not taken the SAT.Both expert and non-expert annotators are native English speakers.Expert annotations achieve an accuracy of 95.98% while non-expert annotations achieves 61.82%, with a gap of 34.16%.This shows that sufficient domain knowledge of FOL is necessary for good performance on FOLIO.The expert and GPT-4 gap is 31.82%,suggesting significant room for model improvement.</p>
<p>Conclusion</p>
<p>We introduced FOLIO, an expert-written dataset for logical reasoning equipped with FOL formulas.The examples in FOLIO are created based on real-world knowledge with natural language.It exhibits a large number of distinct logic patterns and a large vocabulary.Experiments show that FOLIO presents a challenge for one of the most capable Large Language Model publicly available.</p>
<p>Limitations</p>
<p>We focus on collecting a very high-quality dataset in evaluating logical reasoning rather than merely a large dataset.Optimizing for quality required us to adopt a rigorous annotation process with domain experts selected based on a few important criteria as mentioned in Appendix A: Annotator Selection.Significantly scaling up this process would have required resources beyond our current means and we are unable further expand our dataset for investigating how the size of training data affects the performance of fine-tuning experiments.We encourage the community to apply our annotation protocol to expand this realistic and complex FOL reasoning story set.</p>
<p>A Annotator Selection</p>
<p>Given the complexities of our annotations, we selected annotators based on a few important criteria 1).Our annotators are either college or graduate students who are native English speakers or possess near-native proficiency in English. 42).They possess formal education in first-order logic, having either completed relevant coursework or undertaken self-directed studies in first-order logic or semantic parsing.At the NL quality check stage, only annotators who are experts in natural language processing or computational linguistics are involved.For the FOL quality check, only annotators who are experts in first-order logic are involved.We also give the annotators several training sessions on how to write a story, by providing them with detailed annotation guidelines.All stories and FOL annotations in FOLIO are written and reviewed by expert annotators, including CS undergraduate and graduate students, and senior researchers, who met the aforementioned criteria.</p>
<p>B HybLogic Template Example</p>
<p>An example the resulting template is as follows:</p>
<p>Conclusions:</p>
<p>[Unknown] a is an S.</p>
<p>[True] If a is either a C or a D, then a is not either an A or a B.</p>
<p>C Factuality and Bias Elimination Protocol</p>
<p>We rewrote those that are not reflective of wellestablished scientific, historical, or legal facts.We took out stories that had strongly opinionated language and contained gender, racial, and classist biases.We accept certain classes of "psychologically fundamental generalizations" (Leslie, 2008), however, such as "Covid is transmitted through the air" or "Tigers eat other animals," that may not be factually invariant but add logical and semantic nuances to the stories.For stories that pertain to generalization, such as "All As are Bs," we have added specifiers like "all Dan knows" to give a degree of reasonable factuality.For example, "All science fiction that Dan knows comes from an imaginative process" has a more reasonable degree of factuality than "All science fiction comes from an imaginative process."</p>
<p>D Language Quality Control</p>
<p>â€¢ We always use "either-or" to express exclusive disjunction.We use either "A or B" or "A or B, or both" to express inclusive disjunction.In English "or" itself can be interpreted as either inclusive disjunction or exclusive disjunction.Adding "or both" cancels the exclusive disjunction distinctly.However, it is less common in the wild than just using "or".we could add "or both" if it is important to emphasize the inclusive part semantically or contextually or for factuality; and do not add "or both" if it is not.We rely on the language model to figure out if it should be inclusive or exclusive, therefore not sacrificing naturalness.</p>
<p>â€¢ It is more natural to say "Some A is B" rather than "there exists an A such that A is B." "All A are B" can be more natural than "If A then B".</p>
<p>â€¢ Writing NL sentences that express negation over exclusive-or ("either both or neither") can be cumbersome but we found one natural ways of expressing these situations: "Each morning, John either works out and stretches, or he does neither".</p>
<p>Other common issues in NL quality include singular/plural issues, especially in statements that deal with both categories and individual members of those categories; as well as ambiguities resulting from improper introduction of, or failure to introduce, proper nouns.</p>
<p>E First-Order Logic E.1 First-Order Logic VS Natural Language FOL enables deriving facts from other facts (Russell and Norvig, 2010).In the context of logical reasoning in modern NLP, FOL, as a logical form, is a more explicit logical representation than its NL counterpart and can be used as input to an FOL prover in order to obtain the exact truth values for the conclusions.FOL has no ambiguity while ambiguity can occur at various levels of NLP.FOL can thus be a good interface between how LMs are trained and how logical conclusions are reasoned.</p>
<p>E.2 FOL definition</p>
<p>We include the following operators: negation Â¬, conjunction âˆ§, disjunction âˆ¨, implication â†’, universal quantifier âˆ€, existential quantifier âˆƒ, equal =.Following (Russell and Norvig, 2010), we consider temporal logic and modal logic as special-purpose logics.Consequently, they are beyond the scope of the definition of first-order logic used in our dataset.</p>
<p>E.3 FOL modeling conventions</p>
<p>We use n-place predicates when applicable for the expressivity of the FOL formulas.However, we do not use the Davidsonian (Davidson, 2001) or neo-Davidsonian semantics (Parsons, 1990) because translating the majority of the FOL formulas in our dataset only requires one-place and twoplace predicates.Therefore the Davidsonian or neo-Davidsonian semantics are not necessary for the expressivity of the FOL formulas.</p>
<p>For example, "Enjoy dressing up in oldfashioned clothing" is rendered as "Enjoy(x, dressingUp, oldFashionedClothing)".</p>
<p>F FOL Annotation Protocol</p>
<p>We therefore design an annotation protocol for first-order logic translation in order to ensure that our FOL translations are as consistent as possible across all examples in our dataset.We highlight a few important strategies used in the annotation protocol.a).First-order logic formulas need to preserve as much as possible the semantics of natural language sentences.b).First-order logic formulas should stay as faithful to the structure of the original NL sentence as possible.c).Semantic decomposition is not needed unless necessary for maintaining the NL expressivity.This means that "John is a bachelor" can be translated into FOL simply as "Bachelor(John)".d).In terms of abstraction, we neglect tense and remove all the plural forms of verbs.</p>
<p>G FOL Inference Engine</p>
<p>Although there are many provers widely used in the community (McCune, 2005(McCune, -2010;;Sutcliffe, 2017;Nipkow et al., 2002) , we adopt the inference engine provided in the Stanford CS221 course page5 , which is a compact module designed specifically for procesing first-order logic statements.The inference engine does not support input in the FOL syntax adopted by standard education material (Russell and Norvig, 2010), which is used in our dataset.We therefore developed a FOL parser in order to convert the FOL formulas written by humans to the input format of the inference engine.The converter is a semantic parser tool written in Python.Although LLMs such as GPT-4 can be utilized to conduct the conversion, it is hard to ensure the GPT-4 outputs are always correct.</p>
<p>Proving a story requires three steps.First, the FOL statements of the premises and conclusions of a story annotated by humans are converted to Python code.Then, the code snippets are used as input to the theorem prover.Finally, the theorem prover outputs whether the conclusions are True / False / Unknown, based on the premises.predictions are wrong for all conclusions.</p>
<p>Table 10 shows a story from HybLogic with a more complex FOL reasoning process.Inferred from premises 4 and 5, James does not perform better than others.With premises 3, 2 and 1, we know that James is not good at time management.Therefore, conclusion B is False.It cannot be determined if James exercises every week, thus the first conclusion is Unknown.The truth value of p â†’ q is the same as Â¬p âˆ¨ q.It is not true that James does not perform better than others.It is also false that James exercises every week and is good at time management.Thus conclusion C is False.For this example, GPT-4 predicted the correct truth value only for conclusion A and RoBERTa made correct predictions for conclusions A and B.</p>
<p>J Model Performance Analysis</p>
<p>Models have more tendency to predict "True" compared with "False" or "Unknown" labels Confusion matrices in Figure 4 for the fine-tuning and 8-shot NL prompt results both show that LLMs are significantly better at making the correct predictions for conclusions with labels of True than the conclusions with labels of False or Unknown.The accuracy on examples with False or Unknown conclusions is 61.9% with fine-tuning and 54.0% with few-shot prompting.They also tend to make more predictions of True than the other labels.</p>
<p>Model performance is not affected by the premise ordering To test if the premise ordering in FOLIO has spurious correlations with the conclusion label which a model can exploit, we shuffle the input premises to evaluate models.We find that accuracy increases or decreases by roughly 1% in most settings compared to our unshuffled premises.This indicates that the ordering of premises in FO-LIO examples does not yield significant information about the label, and thus models will not be able to use the premise ordering as a strong heuris- tic or statistical feature for its predictions.</p>
<p>Using both NL sentences and FOL formulas in the prompt performs better FOL formulas have a clearer and more straightforward logical structure than NL sentences.Therefore, we test GPT-3.5 and GPT-4 with another two settings for truth value prediction using few-shot prompting: 1) using only FOL formulas in the prompt; 2) using both NL sentences and FOL formulas by concatenating each NL sentence and its annotated FOL statement.As shown in Table 11, the performance slightly increases in the NL+FOL setting for GPT-4 while GPT-3.5 performs worse in both the NL+FOL and the FOL-only settings.In other words, FOL always serves as additional useful information for GPT-4, but not for GPT-3.5 regardless of whether FOL is concatenated with NL.This observation resonates with the finding that GPT-4 performs much better than GPT-3.5 on code-related tasks (Ni et al., 2023).</p>
<p>Figure 1 :
1
Figure 1: Distribution of reasoning depths</p>
<p>Figure 1 demonstrates the distribution of the number of examples in the WikiLogic and HybLogic sets versus the number of premises needed to arrive at a conclusion, showing that most of the conclusions from WikiLogic require one to five premises while those from HybLogic require five to eight premises.</p>
<p>Figure</p>
<p>Figure Accuracies of different models categorized into examples with different reasoning depths.</p>
<p>P. All S are M. Either S or A. All A are B. All D are B. No C are B. a is either a C or a P.</p>
<p>Figure 4 :
4
Figure 4: Confusion matrices for the results of finetuning RoBERTa-Large and few-shot prompting GPT-4.</p>
<p>Table 1 :
1
Comparison of FOLIO with other datasets related to logical reasoning.#Distinct AST stands for the number of distinct abstract syntax trees, representing the number of distinct sentence-level logic structures in the corpus.FOLIO is the first expert-written dataset for FOL reasoning equipped with parallel FOL formulas.The examples are mostly aligned with real-world knowledge and use highly natural wordings.It also has a greater variety than the previous datasets in terms of reasoning depths with a larger number of distinct logic patterns and a large vocabulary.
DatasetSizeReasoningText SourceReal-World Resources# Reasoning DepthVocab# Distinct ASTCLUTTER (2019)6k InductiveSyntheticÃ—Ã—-Ã—RECLOR (2020)6k Mixed forms GMAT, LSAT examsâœ“Ã—-Ã—LogiQA (2021)8.6k Mixed forms NCSE examsâœ“Ã—-Ã—RuleTaker (2020)500k DeductiveSyntheticÃ—0 âˆ¼ 510148ProofWriter (2021) 500k DeductiveSyntheticÃ—0 âˆ¼ 510148LogicNLI (2021)20k FOLSyntheticÃ—1 âˆ¼ 5107730BigBench (2022)1300 Mixed forms Human-WrittenPartiallyÃ—--ProntoQA (2023)200 DeductiveSyntheticâœ“1, 3, 5--FOLIO (ours)1,435 FOLExpert-writtenâœ“0 âˆ¼ 7435176</p>
<p>Table 3 shows that examples based on Wikipedia make up the largest portion of FOLIO, with 304 stories, 1,353 NL and FOL premise pairs, and 753 NL and FOL conclusion pairs.Hybrid annotations consist of 183 stories with 1,054 NL and FOL premise pairs, and 682 NL and FOL conclusion pairs in total.</p>
<p>Table 3 :
3
Table 3 shows that our dataset has a vocabulary of 4,351 words, and the examples based on Wikipedia account for 74% of the total vocabulary even though the WikiLogic stories take up only 63% of the total number of stories.The vocabulary of FOLIO is also significantly Statistics based on different data collection methods of FOLIO.#Words is the average number of words per NL sentence.
Source#Stories #Premises #ConclusionsNLLogicVocab #Words Complexity #Depth ASTWikiLogic304135375332508.500 -14 grade1 -551HybLogic1831054682190211.520 -14 grade5 -825Total4872407143543519.860 -14 grade765-8</p>
<p>Table 4 :
4
Logical reasoning results of fully supervised fine-tuning and few-shot prompting on FOLIO test set.
ModelSizeAcc (%)majority baseline-38.5%random probability-33.3 %Fully supervised fine-tuneBERT-base110M56.8BERT-large340M59.0RoBERTa-base110M56.8RoBERTa-large340M62.1Flan-T5-Large783M65.90-shot NL PromptGPT-3.5-Turbo-53.1GPT-4-61.38-shot NL PromptLLama-13B13B33.6LLama-70B70B44.0LLama-70B -CoT70B47.8LLama-70B -ToT70B48.4text-davinci-002-49.5GPT-3.5-Turbo-58.3GPT-4-64.2GPT-4 -CoT (2022b)-68.9GPT-4 -CoT with SC (2023) -69.5GPT-4 ToT (2023)-70.0LR-specific MethodsLogic-LM (2023)-78.1LINC (2023)-73.1DetermLR (2023)-77.5</p>
<p>Table 5 :
5
NL-FOL translation results on FOLIO.SynV measures syntactic validity and ExcAcc measures the inference engine execution accuracy.
ModelZero-ShotFew-ShotSynv ExcAcc Sync ExcAccGPT-3.5-Turbo 68.450.4 93.3 56.0GPT-486.151.7 93.9 63.8</p>
<p>Table 6 :
6
Performance differences on the WikiLogic and HybLogic subset of FOLIO.WikiLogic has more diverse logical structures while HybLogic stories have higher reasoning depths.
MethodModelWikiHybFine-tuningRoBERTa-large 60.71 63.48NL PromptingGPT-3.5-Turbo68.88 47.70GPT-475.43 53.10NL-FOL ExcAcc GPT-3.5-Turbo45.17 61.82GPT-459.12 67.93</p>
<p>Table 7 :
7
Human evaluation on GPT-4 model outputs with incorrect truth value predictions Example Premises 1.All children are human.2. If someone is underage, then they are a child.3.People are either underage or of age. 4. If someone is of age, then they can vote.5.If someone is of age, they can legally get married.6.If Jack is a child and a human, then Jack is neither able to vote nor able to get married.Conclusion -&gt; Label: Jack is able to vote and get married.-&gt; True.</p>
<p>Table 11 :
11
Comparison of the results across different input formats with few-shot prompting.NL, NL-FOL, FOL, NL + FOL stands for NL prompting, execution accuracy of NL-FOL translation, using only FOL in the prompt and using concatenated NL and FOL in the prompt respectively.
ModelNLNL-FOL FOL NL+FOLGPT-3.5 58.3455.9657.9257.75GPT-464.1663.8264.0165.21
In experimenting with different prompts, we found 8 shot examples to perform slightly better. It is also the maximum number of examples that fits in the text-davinci-002 context.
Hereafter, "GPT-3.5" refers to GPT-3.5-Turbo.
By "near-native" we mean with English speaking and understanding ability that closely mirrors that of a native English speakers.
https://stanford-cs221.github.io/spring2022/ assignments/logic/index.html
H Distribution of ReadabilityWe show the distribution of readability in Figure3.I Case studyTable9shows a story from WikiLogic along with the GPT-4 and RoBERTa-Large predictions.Conclusion A is True given premises 5 and 3. From the premises, it cannot be determined if Cerura vinula has thin antennae or if it is a pest.Thus conclusions B and C are Unknown.GPT-4 predictions are correct for conclusions A and C while RoBERTa
HHUplexity at text complexity DE challenge 2022. David Arps, Jan Kels, Florian KrÃ¤mer, Yunus Renz, Regina Stodden, Wiebke Petersen, Proceedings of the GermEval 2022 Workshop on Text Complexity Assessment of German Text. the GermEval 2022 Workshop on Text Complexity Assessment of German TextPotsdam, GermanyAssociation for Computational Linguistics2022</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Shivanshu Usvsn Sai Prashanth, Laria Purohit, Jonathan Reynolds, Ben Tow, Samuel Wang, Weinbach, 10.48550/ARXIV.2204.06745Gpt-neox-20b: An open-source autoregressive language model. 2022arXiv preprint</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Hy-bridQA: A dataset of multi-hop question answering over tabular and textual data. Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Yang, Wang , 10.18653/v1/2020.findings-emnlp.91Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Peter Clark, Oyvind Tafjord, Kyle Richardson, CoRR, abs/2002.05867Transformers as soft reasoners over language. 2020</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Using the framework. Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, LRE 62-051 D-16The FraCaS Consortium. 1996Technical Report</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>A formula for predicting readability. Edgar Dale, Jeanne S Chall, Educational Research Bulletin. 2711948</p>
<p>Readability Revisited: The New Dale-Chall Readability Formula. Edgar Dale, Jeanne S Chall, 1995Brookline Books</p>
<p>Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv:2207.07051Language models show human-like content effects on reasoning. 2022arXiv preprint</p>
<p>105The Logical Form of Action Sentences. Donald Davidson, 10.1093/0199246270.003.0006Essays on Actions and Events. Oxford University Press2001</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, 10.48550/ARXIV.2207.10342Language model cascades. Rif A. Saurous; Kevin Murphy, and Charles Sutton2022arXiv preprint</p>
<p>WinoLogic: A zero-shot logic-based diagnostic dataset for Winograd Schema Challenge. Weinan He, Canming Huang, Yongmei Liu, Xiaodan Zhu, 10.18653/v1/2021.emnlp-main.307Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Evaluating the rationale understanding of critical reasoning in logical reading comprehension. Akira Kawabata, Saku Sugawara, arXiv:2311.183532023Preprint</p>
<p>Boardgameqa: A dataset for natural language reasoning with contradictory information. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, arXiv:2306.079342023PreprintXin Xu, Vaiva Imbrasaite, and Deepak Ramachandran</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162022arXiv preprint</p>
<p>Two sets of perfect syllogisms. Anne Lehman, 10.1305/ndjfl/1093891016Notre Dame Journal of Formal Logic. 1431973</p>
<p>Generics: Cognition and Acquisition. Sarah-Jane Leslie, 10.1215/00318108-2007-023The Philosophical Review. 11712008</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.02336On the advance of making language models better reasoners. 2022arXiv preprint</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Ro{bert}a: A robustly optimized {bert} pretraining approach. 2020arXiv preprint</p>
<p>. W Mccune, 2005-2010. Prover9 and mace4</p>
<p>Dragomir Radev, and Arman Cohan. 2023. L2ceval: Evaluating language-to-code generation capabilities of large language models. Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong, Shafiq Joty, Yingbo Zhou, arXiv:2309.17446Preprint</p>
<p>Isabelle/Hol a Proof Assistant for Higher-Order Logic. Tobias Nipkow, Lawrence C Paulson, Markus Wenzel, 2002Springer</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Josh Openai, Others Achiam, arXiv:2303.08774Gpt-4 technical report. 2023Preprint</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, 10.18653/v1/2023.findings-emnlp.248Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Events in the Semantics of English. Terence Parsons, 1990MIT PressCambridge, MA, USA</p>
<p>Stuart Russell, Peter Norvig, Artificial Intelligence: A Modern Approach. Prentice Hall20103 edition</p>
<p>RuleBERT: Teaching soft rules to pre-trained language models. Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti, 10.18653/v1/2021.emnlp-main.110Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Language models can (kind of) reason: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, International Conference on Learning Representations. 2023</p>
<p>Exploring neural models for parsing natural language into first-order logic. Hrituraj Singh, Milan Aggrawal, Balaji Krishnamurthy, arXiv:2002.065442020arXiv preprint</p>
<p>Misery loves complexity: Exploring linguistic complexity in the context of emotion detection. Pranaydeep Singh, Luna De Bruyne, OrphÃ©e De Clercq, Els Lefever, 10.18653/v1/2023.findings-emnlp.857Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/v1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, +447 AuthorsAbhinav Rastogi, +447 AuthorsarXiv:2206.046152023Preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, AdriÃ  Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models. Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, Rui Yan, arXiv:2310.186592023Preprint</p>
<p>G Sutcliffe, The TPTP Problem Library and Associated Infrastructure. From CNF to TH0, TPTP. 201759v6.4.0.</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant, Advances in Neural Information Processing Systems. 202033</p>
<p>Diagnosing the firstorder logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Preprint</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in Neural Information Processing Systems. Curran Associates, Inc2019a32</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 2019b32</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Linguistic complexity loss in text-based therapy. Jason Wei, Kelly Finn, Emma Templeton, Thalia Wheatley, Soroush Vosoughi, 10.18653/v1/2021.naacl-main.352Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022barXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2020</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, 10.48550/ARXIV.2203.14465Star: Bootstrapping reasoning with reasoning. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
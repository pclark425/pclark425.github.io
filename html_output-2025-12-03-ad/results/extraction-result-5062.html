<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5062 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5062</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5062</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-453fc588d97958c6fefad96e79edd896873b3e09</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/453fc588d97958c6fefad96e79edd896873b3e09" target="_blank">Chess as a Testbed for Language Model State Tracking</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> It is found that with enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences and for small training sets providing access to board state information during training can yield significant improvements.</p>
                <p><strong>Paper Abstract:</strong> Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. “full attention”. Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5062.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5062.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-chess (full-attn)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT2-small transformer language model trained on chess move sequences (full attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper trains GPT2-small (12-layer transformer with 12 heads, 768-dim embeddings) from scratch on UCI-formatted chess games to evaluate whether an autoregressive LM can track board state and predict legal moves via simple prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small (full attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT2-small architecture: 12 layers, 12 attention heads, embedding size 768, context size 512 (trained from scratch). Trained on up to ~200K filtered games from Millionbase for up to 10 epochs with Adam (LR 5e-4), mixed precision. Variants trained on plain UCI, UCI+RAP (randomly annotated piece types with tuned p), and UCI+AP (always piece type during training).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess (blindfolded, move-sequence modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Standard 2-player chess represented as sequences of UCI moves; requires spatial reasoning to track piece locations on 64-square board, determine whose turn it is, legal moves for piece types, and consequences of prior moves.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Autoregressive language modeling of UCI move sequences; probing via prompt tokens: 'ending-square' prompts (give starting square token and read next-token predictions) and 'starting-square' prompts (give piece-type token and read predicted square). Training variants include RAP (Randomly Annotated Piece type) where piece-type tokens are injected randomly during training to provide extra supervision for piece locations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Probing tasks specifically target board state: starting-square prediction (given piece type, model predicts where that piece is) and ending-square prediction (given starting square, model predicts legal/actual destination). High legal-move (LgM) accuracies and R-Precision indicate the model represents piece locations and move legality. Error analysis shows RAP reduces piece-tracking related errors. The paper demonstrates the model can output only legal squares in many cases, indicating learned rules and spatial state tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When trained on the large set (Train-L ≈200K games): starting-square (Start-Actual) LgM accuracy ≈100% (UCI+RAP), starting-square ExM accuracy ≈91.8%; ending-square (End-Actual) LgM accuracy ≈97.0–98.2% (notation dependent), ending-square R-Precision ≈85–87%, and exact-move (ExM) accuracy ≈52.0% (i.e., predicts the true next move ~52% of time under best config). With medium data (Train-M): End-Actual LgM ≈92.9%, ExM ≈42.2%; with small data (Train-S): End-Actual LgM ≈74.0% and ExM ≈26.7%. RAP (UCI+RAP) improved perplexity and probe metrics, especially for smaller training sets (e.g., Train-S End-Actual LgM improved from 74.0% to 88.4%). Perplexity improvements: canonical perplexity fell from 23.6 (UCI) to 15.9 (UCI+RAP) on Train-S; on Train-L from 7.7 to 7.4 (UCI+RAP).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Exact-move (ExM) accuracy remains substantially lower than legal-move accuracy, reflecting strategic choice difficulty (many legal moves exist); performance strongly depends on dataset size (small training sets suffer large drops). The model requires full attention over long history—limiting attention window (GPT2 w=50) or using LSTM drastically reduces performance. RAP introduces a train/inference distribution mismatch (piece-type tokens present only during training), though models are robust to this and benefit from it.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT2 with limited attention window (w=50), approximate-attention transformers (Reformer, Performer), and LSTM baselines. Full-attention GPT2-small substantially outperforms LSTM and attention-windowed GPT2 on board-state probing (e.g., Train-M End-Actual LgM: GPT2 92.9% vs LSTM 73.8%; Train-L ExM: GPT2 52.0% vs LSTM 45.2%). RAP and AP notations improve performance over plain UCI, especially with small training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chess as a Testbed for Language Model State Tracking', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5062.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5062.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Approx-attn (Reformer/Performer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Approximate-attention transformers (Reformer and Performer) applied to chess move modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates Reformer and Performer architectures (both approximate attention variants) as alternatives to full-attention GPT2, measuring their ability to track chess board state from move sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reformer; Performer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reformer: locality-sensitive hashing based attention (approximate global attention). Performer: random-feature based linearized attention approximation. Implementations matched to 12 layers and 12 attention heads like GPT2-small; trained on same data/configs as other models for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess (move-sequence modeling, same probing tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same chess tasks requiring tracking of piece positions and legality of moves; probes test starting-square and ending-square prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Replace full quadratic attention with approximate attention mechanisms (LSH-based or random-feature approximations) while keeping autoregressive LM training on UCI move sequences; evaluate via same prompt-based probing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Measured via same board-state probing tasks; approximate-attention models achieve substantially better-than-random legal-move accuracies but generally fall short of full-attention GPT2, indicating some spatial state representation but inferior fidelity. Performer often outperforms Reformer (except small-data setting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative numbers (End-Actual LgM accuracy, R-Precision, ExM): small data (Train-S): Reformer LgM ≈71.0% (R-Prec ≈57.2), ExM ≈24.8%; Performer LgM ≈65.4% (R-Prec ≈54.3), ExM ≈20.5%. Medium data (Train-M): Reformer LgM ≈86.4% (R-Prec ≈73.2), ExM ≈32.4%; Performer LgM ≈89.2% (R-Prec ≈76.3), ExM ≈36.0%. Large data (Train-L): Reformer LgM ≈88.0% (R-Prec ≈74.9), ExM ≈33.5%; Performer LgM ≈95.8% (R-Prec ≈84.5), ExM ≈51.6%. Generally underperforms full-attention GPT2 for small/medium datasets; Performer approaches GPT2 performance at larger data amounts but not consistently better.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Approximate attention degrades board-state tracking compared to full attention, especially for small and medium training sets; neither Reformer nor Performer consistently matched full-attention performance across settings. These architectures struggled with accurately predicting all legal moves (lower R-Precision) in some regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to full-attention GPT2-small and GPT2 with limited attention window: Performer often outperforms Reformer and LSTM but generally does not exceed full-attention GPT2; Reformer typically underperforms Performer. Both outperform LSTM in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chess as a Testbed for Language Model State Tracking', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5062.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5062.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM autoregressive language model trained on UCI chess move sequences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based language model baseline (standard LSTM with hidden state and cell) is trained on the same UCI and UCI+RAP data to compare sequence modeling and board-state tracking performance against transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard LSTM language model (architecture details not exhaustively specified) trained with same data splits; experiments include LSTM trained on UCI and LSTM + RAP (with randomly annotated piece tokens during training).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess (move-sequence modeling, same probing tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same 64-square chess board state tracking via UCI move sequences; tasks require spatial reasoning about where pieces are and legal moves.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Autoregressive next-token prediction with LSTM hidden/cell states summarizing history; tested with same start/end probing prompts. Also tested with RAP augmentation during training.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>LSTM achieves above-chance legal-move prediction showing some ability to track state, but substantially lower than full-attention transformers; LSTM+RAP yields modest improvements on larger datasets, indicating RAP's supervisory signal helps recurrent models too but to a lesser extent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative numbers (End-Actual LgM accuracy, R-Precision, ExM): Train-S: LSTM LgM ≈60.2% (R-Prec ≈51.0), ExM ≈20.9%; Train-M: LSTM LgM ≈73.8% (R-Prec ≈61.6), ExM ≈32.0%; Train-L: LSTM LgM ≈93.4% (R-Prec ≈79.5), ExM ≈45.2%. LSTM+RAP yields small improvements for larger training sets (e.g., Train-L ExM ≈46.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LSTM lacks explicit attention over the full history and underperforms transformers, particularly for small and medium datasets. It struggles more with legal-move and exact-move prediction and benefits less from RAP than transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Performs worse than full-attention GPT2 across most metrics and data regimes; sometimes comparable to approximate-attention transformers in some settings but generally inferior for board-state probing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chess as a Testbed for Language Model State Tracking', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DeepChess: End-to-End Deep Neural Network for Automatic Learning in Chess <em>(Rating: 2)</em></li>
                <li>Transformers Play Chess <em>(Rating: 2)</em></li>
                <li>The Chess Transformer: Mastering Play using Generative Language Models <em>(Rating: 2)</em></li>
                <li>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play <em>(Rating: 1)</em></li>
                <li>Reformer: The Efficient Transformer <em>(Rating: 1)</em></li>
                <li>Rethinking Attention with Performers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5062",
    "paper_id": "paper-453fc588d97958c6fefad96e79edd896873b3e09",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "GPT2-chess (full-attn)",
            "name_full": "GPT2-small transformer language model trained on chess move sequences (full attention)",
            "brief_description": "This paper trains GPT2-small (12-layer transformer with 12 heads, 768-dim embeddings) from scratch on UCI-formatted chess games to evaluate whether an autoregressive LM can track board state and predict legal moves via simple prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small (full attention)",
            "model_description": "GPT2-small architecture: 12 layers, 12 attention heads, embedding size 768, context size 512 (trained from scratch). Trained on up to ~200K filtered games from Millionbase for up to 10 epochs with Adam (LR 5e-4), mixed precision. Variants trained on plain UCI, UCI+RAP (randomly annotated piece types with tuned p), and UCI+AP (always piece type during training).",
            "puzzle_name": "Chess (blindfolded, move-sequence modeling)",
            "puzzle_description": "Standard 2-player chess represented as sequences of UCI moves; requires spatial reasoning to track piece locations on 64-square board, determine whose turn it is, legal moves for piece types, and consequences of prior moves.",
            "mechanism_or_strategy": "Autoregressive language modeling of UCI move sequences; probing via prompt tokens: 'ending-square' prompts (give starting square token and read next-token predictions) and 'starting-square' prompts (give piece-type token and read predicted square). Training variants include RAP (Randomly Annotated Piece type) where piece-type tokens are injected randomly during training to provide extra supervision for piece locations.",
            "evidence_of_spatial_reasoning": "Probing tasks specifically target board state: starting-square prediction (given piece type, model predicts where that piece is) and ending-square prediction (given starting square, model predicts legal/actual destination). High legal-move (LgM) accuracies and R-Precision indicate the model represents piece locations and move legality. Error analysis shows RAP reduces piece-tracking related errors. The paper demonstrates the model can output only legal squares in many cases, indicating learned rules and spatial state tracking.",
            "performance_metrics": "When trained on the large set (Train-L ≈200K games): starting-square (Start-Actual) LgM accuracy ≈100% (UCI+RAP), starting-square ExM accuracy ≈91.8%; ending-square (End-Actual) LgM accuracy ≈97.0–98.2% (notation dependent), ending-square R-Precision ≈85–87%, and exact-move (ExM) accuracy ≈52.0% (i.e., predicts the true next move ~52% of time under best config). With medium data (Train-M): End-Actual LgM ≈92.9%, ExM ≈42.2%; with small data (Train-S): End-Actual LgM ≈74.0% and ExM ≈26.7%. RAP (UCI+RAP) improved perplexity and probe metrics, especially for smaller training sets (e.g., Train-S End-Actual LgM improved from 74.0% to 88.4%). Perplexity improvements: canonical perplexity fell from 23.6 (UCI) to 15.9 (UCI+RAP) on Train-S; on Train-L from 7.7 to 7.4 (UCI+RAP).",
            "limitations_or_failure_cases": "Exact-move (ExM) accuracy remains substantially lower than legal-move accuracy, reflecting strategic choice difficulty (many legal moves exist); performance strongly depends on dataset size (small training sets suffer large drops). The model requires full attention over long history—limiting attention window (GPT2 w=50) or using LSTM drastically reduces performance. RAP introduces a train/inference distribution mismatch (piece-type tokens present only during training), though models are robust to this and benefit from it.",
            "comparison_baseline": "Compared against GPT2 with limited attention window (w=50), approximate-attention transformers (Reformer, Performer), and LSTM baselines. Full-attention GPT2-small substantially outperforms LSTM and attention-windowed GPT2 on board-state probing (e.g., Train-M End-Actual LgM: GPT2 92.9% vs LSTM 73.8%; Train-L ExM: GPT2 52.0% vs LSTM 45.2%). RAP and AP notations improve performance over plain UCI, especially with small training sets.",
            "uuid": "e5062.0",
            "source_info": {
                "paper_title": "Chess as a Testbed for Language Model State Tracking",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Approx-attn (Reformer/Performer)",
            "name_full": "Approximate-attention transformers (Reformer and Performer) applied to chess move modeling",
            "brief_description": "The paper evaluates Reformer and Performer architectures (both approximate attention variants) as alternatives to full-attention GPT2, measuring their ability to track chess board state from move sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reformer; Performer",
            "model_description": "Reformer: locality-sensitive hashing based attention (approximate global attention). Performer: random-feature based linearized attention approximation. Implementations matched to 12 layers and 12 attention heads like GPT2-small; trained on same data/configs as other models for comparison.",
            "puzzle_name": "Chess (move-sequence modeling, same probing tasks)",
            "puzzle_description": "Same chess tasks requiring tracking of piece positions and legality of moves; probes test starting-square and ending-square prediction.",
            "mechanism_or_strategy": "Replace full quadratic attention with approximate attention mechanisms (LSH-based or random-feature approximations) while keeping autoregressive LM training on UCI move sequences; evaluate via same prompt-based probing tasks.",
            "evidence_of_spatial_reasoning": "Measured via same board-state probing tasks; approximate-attention models achieve substantially better-than-random legal-move accuracies but generally fall short of full-attention GPT2, indicating some spatial state representation but inferior fidelity. Performer often outperforms Reformer (except small-data setting).",
            "performance_metrics": "Representative numbers (End-Actual LgM accuracy, R-Precision, ExM): small data (Train-S): Reformer LgM ≈71.0% (R-Prec ≈57.2), ExM ≈24.8%; Performer LgM ≈65.4% (R-Prec ≈54.3), ExM ≈20.5%. Medium data (Train-M): Reformer LgM ≈86.4% (R-Prec ≈73.2), ExM ≈32.4%; Performer LgM ≈89.2% (R-Prec ≈76.3), ExM ≈36.0%. Large data (Train-L): Reformer LgM ≈88.0% (R-Prec ≈74.9), ExM ≈33.5%; Performer LgM ≈95.8% (R-Prec ≈84.5), ExM ≈51.6%. Generally underperforms full-attention GPT2 for small/medium datasets; Performer approaches GPT2 performance at larger data amounts but not consistently better.",
            "limitations_or_failure_cases": "Approximate attention degrades board-state tracking compared to full attention, especially for small and medium training sets; neither Reformer nor Performer consistently matched full-attention performance across settings. These architectures struggled with accurately predicting all legal moves (lower R-Precision) in some regimes.",
            "comparison_baseline": "Compared to full-attention GPT2-small and GPT2 with limited attention window: Performer often outperforms Reformer and LSTM but generally does not exceed full-attention GPT2; Reformer typically underperforms Performer. Both outperform LSTM in many settings.",
            "uuid": "e5062.1",
            "source_info": {
                "paper_title": "Chess as a Testbed for Language Model State Tracking",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "LSTM baseline",
            "name_full": "LSTM autoregressive language model trained on UCI chess move sequences",
            "brief_description": "An LSTM-based language model baseline (standard LSTM with hidden state and cell) is trained on the same UCI and UCI+RAP data to compare sequence modeling and board-state tracking performance against transformers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM (autoregressive)",
            "model_description": "Standard LSTM language model (architecture details not exhaustively specified) trained with same data splits; experiments include LSTM trained on UCI and LSTM + RAP (with randomly annotated piece tokens during training).",
            "puzzle_name": "Chess (move-sequence modeling, same probing tasks)",
            "puzzle_description": "Same 64-square chess board state tracking via UCI move sequences; tasks require spatial reasoning about where pieces are and legal moves.",
            "mechanism_or_strategy": "Autoregressive next-token prediction with LSTM hidden/cell states summarizing history; tested with same start/end probing prompts. Also tested with RAP augmentation during training.",
            "evidence_of_spatial_reasoning": "LSTM achieves above-chance legal-move prediction showing some ability to track state, but substantially lower than full-attention transformers; LSTM+RAP yields modest improvements on larger datasets, indicating RAP's supervisory signal helps recurrent models too but to a lesser extent.",
            "performance_metrics": "Representative numbers (End-Actual LgM accuracy, R-Precision, ExM): Train-S: LSTM LgM ≈60.2% (R-Prec ≈51.0), ExM ≈20.9%; Train-M: LSTM LgM ≈73.8% (R-Prec ≈61.6), ExM ≈32.0%; Train-L: LSTM LgM ≈93.4% (R-Prec ≈79.5), ExM ≈45.2%. LSTM+RAP yields small improvements for larger training sets (e.g., Train-L ExM ≈46.0%).",
            "limitations_or_failure_cases": "LSTM lacks explicit attention over the full history and underperforms transformers, particularly for small and medium datasets. It struggles more with legal-move and exact-move prediction and benefits less from RAP than transformers.",
            "comparison_baseline": "Performs worse than full-attention GPT2 across most metrics and data regimes; sometimes comparable to approximate-attention transformers in some settings but generally inferior for board-state probing tasks.",
            "uuid": "e5062.2",
            "source_info": {
                "paper_title": "Chess as a Testbed for Language Model State Tracking",
                "publication_date_yy_mm": "2021-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DeepChess: End-to-End Deep Neural Network for Automatic Learning in Chess",
            "rating": 2
        },
        {
            "paper_title": "Transformers Play Chess",
            "rating": 2
        },
        {
            "paper_title": "The Chess Transformer: Mastering Play using Generative Language Models",
            "rating": 2
        },
        {
            "paper_title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play",
            "rating": 1
        },
        {
            "paper_title": "Reformer: The Efficient Transformer",
            "rating": 1
        },
        {
            "paper_title": "Rethinking Attention with Performers",
            "rating": 1
        }
    ],
    "cost": 0.012413249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Chess as a Testbed for Language Model State Tracking</h1>
<p>Shubham Toshniwal ${ }^{1}$, Sam Wiseman ${ }^{2}$, Karen Livescu ${ }^{1}$, Kevin Gimpel ${ }^{1}$<br>${ }^{1}$ Toyota Technological Institute at Chicago<br>${ }^{2}$ Duke University<br>{shtoshni, klivescu, kgimpel}@ttic.edu, swiseman@cs.duke.edu</p>
<h4>Abstract</h4>
<p>Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. "full attention". Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models.</p>
<h2>1 Introduction</h2>
<p>Recently, transformer-based language models have stretched notions of what is possible with the simple self-supervised objective of language modeling, becoming a fixture in state of the art language technologies (Vaswani et al. 2017; Devlin et al. 2019; Brown et al. 2020). However, the black box nature of these models combined with the complexity of natural language makes it challenging to measure how accurately they represent the world state underlying the text.</p>
<p>In order to better measure the extent to which these models can capture the world state underlying the symbolic data they consume, we propose training and studying transformer language models for the game of chess. Chess provides a simple, constrained, and deterministic domain where the exact world state is known. Chess games can also be transcribed exactly and unambiguously using chess notations (Section 2). Most importantly, the form of chess notations allows us to probe our language models for aspects of the board state using simple prompts (Section 3) and without changing the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>language modeling objective or introducing any new classifiers. ${ }^{1}$</p>
<p>Due to the simplicity and precision of chess, we can evaluate language model predictions at a more fine-grained level than merely comparing them to the ground truth. For example, even if the next move prediction doesn't match the ground truth move, we can still evaluate whether the move is legal given the board state, and if it is illegal, the error can be automatically analyzed. Moreover, since world state transitions are deterministic and known, we can evaluate models using counterfactual queries as well. Our proposed evaluation sets and metrics are described in Section 3.2.</p>
<p>While chess represents a controlled domain, it is by no means trivial for a language model. To illustrate the challenges of language modeling for chess, consider the left board shown in Figure 1b, where white is next to move. In order to generate a valid next move, the language model needs to (a) infer that it is white's turn, (b) represent the locations of all pieces, both white and black, (c) select one of the white pieces which can be legally moved, and finally (d) make a legal move with the selected piece. Thus, a language model has to learn to track the board state, learn to generate moves according to the rules of chess, and on top of that learn chess strategies to predict the actual move.</p>
<p>We find that when given enough training data, transformers can learn to both track piece locations and predict legal moves with high accuracy. However, when trained on small training sets, predictive ability suffers. In this more challenging setting, introducing parts of the board state as tokens in the training sequences (Section 3.1) improves piece tracking significantly.</p>
<p>Our results also provide some key insights on transformer language models: (i) They are robust to changes in input distribution where additional tokens, related to board state, are added to input sequence only during training (Section 3.1). In contrast to LSTMs, transformers achieve this robustness even with smaller training sets (Section 5.3). (ii) Even though chess is Markovian, the model relies on having access to the whole history, and the performance drops when limiting this access (Section 5.3).</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Square naming
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Board state before (left) and after (right) the bishop at $f 1$ is moved to b5. UCI notation represents the move as fib5.</p>
<p>Figure 1: Chess Notation</p>
<p>To summarize, our contributions are to:</p>
<ul>
<li>Propose chess as a testbed for evaluating world state tracking capabilities of language models which can be used for development and analysis of these models.</li>
<li>Show that with the appropriate chess notation, we can probe language models for aspects of the world state using simple prompts (Section 3).</li>
<li>Show that given enough training data, transformer language models can learn to track piece locations and predict legal moves with high accuracy.</li>
<li>Demonstrate that transformer language models are robust to certain changes in input distribution, and that access to world state during training improves performance with small datasets.</li>
</ul>
<h2>2 Chess Preliminaries</h2>
<p>We represent moves using Universal Chess Interface (UCI) notation, which combines the starting square and the destination square to represent a move. ${ }^{2}$ The move in Figure 1b is represented as fib5 in UCI where fl indicates the starting square and b5 denotes the ending square. While the SAN notation is the standard choice for gameplay, we prefer UCI as it allows for pieces to be referenced unambiguously via their starting square, something we later exploit in designing prompt-based probes in Section 3.</p>
<p>For training language models, we first tokenize games represented in UCI notation using a simple regular expression based tokenizer, which considers a board square symbol such as bl as a single token. This gives us a vocabulary of 77 token types, which includes the 64 squares, piece type symbols, and other special symbols (see Table 1). ${ }^{3}$ For example, the move sequence "e2e4 e7e5 glf3" is tokenized to</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Board state before (left) and after (right) the bishop at fl is moved to b5. UCI notation represents the move as fib5.</p>
<p>Figure 1: Chess Notation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Examples</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Square names</td>
<td style="text-align: left;">e4, d1</td>
<td style="text-align: right;">64</td>
</tr>
<tr>
<td style="text-align: left;">Piece type</td>
<td style="text-align: left;">P, K, Q, R, B, N</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Promoted Pawn Piece type</td>
<td style="text-align: left;">q, r, b, n</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">Special symbols</td>
<td style="text-align: left;">BOS, EOS, PAD</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;"></td>
<td style="text-align: right;">77</td>
</tr>
</tbody>
</table>
<p>Table 1: Model Vocabulary
"e2, e4, e7, e5, g1, f3". We then train an autoregressive language model on these move sequences, using the standard maximum likelihood objective.</p>
<h2>3 Language Model Prompts as Board State Probes</h2>
<p>One attractive property of having a language model trained on chess games represented in UCI notation (as described in the previous section) is that the notation itself allows us to probe the trained model's state tracking abilities. In particular, by feeding the trained language model a prefix of a game as a prompt, we can determine - using the language model's next-token predictions what the model understands about the board state implied by this prefix. For example, consider the prompt "e2e4 e7e5 glf3 b8c6 d2d4 h7h6 f1," where the underlined move sequence leads to the left board state in Figure 1b. A language model's next-token prediction (after consuming the prompt) can be interpreted as the ending square predicted for the bishop at $f 1$, which can be used to determine the level of board state awareness of the model. If, for instance, the model predicts g1, this may indicate that the model does not recognize that the piece type at $f 1$ is a bishop, as such a move is not possible for a bishop. If, on the other hand, the model predicts g2, that may indicate that the model is not aware that another piece is currently at g2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Notation</th>
<th style="text-align: left;">Training</th>
<th style="text-align: left;">Inference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UCI</td>
<td style="text-align: left;">e2, e4, e7, e5, g1, f3</td>
<td style="text-align: left;">e2, e4, e7, e5, g1, f3</td>
</tr>
<tr>
<td style="text-align: left;">UCI + RAP 15</td>
<td style="text-align: left;">e2, e4, P, e7, e5, g1, f3</td>
<td style="text-align: left;">e2, e4, e7, e5, g1, f3</td>
</tr>
<tr>
<td style="text-align: left;">UCI + RAP 100</td>
<td style="text-align: left;">P, e2, e4, P, e7, e5, N, g1, f3</td>
<td style="text-align: left;">e2, e4, e7, e5, g1, f3</td>
</tr>
<tr>
<td style="text-align: left;">UCI + AP</td>
<td style="text-align: left;">P, e2, e4, P, e7, e5, N, g1, f3</td>
<td style="text-align: left;">P, e2, e4, P, e7, e5, N, g1, f3</td>
</tr>
</tbody>
</table>
<p>Table 2: Token sequences corresponding to the move sequence e2e4 e7e5 g1f3 for different notations during training and inference. Notice that regardless of the RAP probability used during training, at inference time the token sequences have no piece types.</p>
<h3>3.1 Randomly Annotated Piece type (RAP)</h3>
<p>While predicting the token representing the ending-square of a move given a prompt allows us to assess the model's state tracking abilities, it also to some extent conflates the model's understanding of the board state with its understanding of chess strategy. If we could easily probe for where the model thinks a piece currently is (rather than where it is likely to end up) given a game prefix, this would allow us to more directly probe the model's state tracking abilities. In particular, we would like to give a language model a prompt such as "e2e4 e7e5 g1f3 b8c6 d2d4 h7h6 N", where N represents knight, and expect it to generate a valid starting position for a knight of the correct color. While UCI notation does not ordinarily include these piece type tokens, to allow for testing the model with such prompts, we propose to randomly include these piece types tokens in moves during training with some fixed probability $p$. We refer to this strategy as "randomly annotated piece type" (RAP) and use the nomenclature "UCI + RAP $p$ " to indicate that with $p \%$ probability, piece type is part of the move notation during training. Note that for $p=0$, the notation reduces to UCI.</p>
<p>When testing with these starting square prediction prompts, we only include piece type for the prompt, not for any moves in the history. Thus, using RAP during training allows us to probe, at test time, where the model thinks each piece is, given any game history's prefix; by simply providing the desired piece type (e.g., N) the model outputs the predicted starting square for a piece of that type. For example, given the prompt "e2e4 e7e5 g1f3 b8c6 d2d4 h7h6 N", a prediction of f 3 or b1 shows that the model is aware of where the knights are.</p>
<p>We also experiment with an "oracle" variant of RAP where piece types are added both during training and testing. We refer to this notation as "UCI + AP " where AP stands for "always piece type". For our running example the equivalent prompt in this notation would be "Pe2e4 Pe7e5 Ng1f3 Nb8c6 Pd2d4 Ph7h6 N".</p>
<p>In terms of the language modeling training objective, addition of RAP represents a distribution change between training and inference. Table 2 illustrates how the use of RAP changes the token sequence during training but not during inference. While there's a distribution mismatch, we hypothesize that addition of RAP can aid the model in learning to track the pieces by providing additional supervision which, in turn, can improve language modeling performance as well.</p>
<h3>3.2 Board State Probing Tasks</h3>
<p>In this subsection we describe the probing tasks introduced above more concretely. In each probing task we feed the model a prefix of a game followed by a single prompt token, and the model is evaluated based on the highest probability next-token under the model given this context. We show an example of each probing task in Table 3 (which we further describe below), assuming the model has been fed the move sequence prefix e2e4 e7e5 g1f3 b8c6 d2d4 h7h6, which is visualized as the left board in Figure 1b. The actual next move played in the game is f1b5, which takes the white bishop at square f1 to square b5, as shown in the right board of Figure 1b.</p>
<h3>3.3 Ending Square Tasks</h3>
<p>In this set of tasks, the model is given a game prefix and prompted with the starting square of the next move (f1 in the example of Table 3). The model's next-token prediction represents its prediction for the ending square of this move, which tests the model's ability to track the board state and follow the rules of chess, as well as strategic awareness. ${ }^{4}$ We consider two task variants:</p>
<ol>
<li>End-Actual: Given a move sequence prefix, the model is prompted with the starting square of the actual piece moved next in the game.</li>
<li>End-Other: Given a move sequence prefix, the model is prompted with the starting square of any piece on the board that can be legally moved according to the rules of chess.</li>
</ol>
<p>We evaluate End-Actual predictions in terms of both exact move (ExM) accuracy (whether the model predicted the true ending square, b5 in our running example) and legal move $(\mathrm{LgM})$ accuracy (whether the model predicted a legal ending square for the piece starting at the square in the prompt). For LgM evaluation, we also calculate the R-Precision which is the Precision@R where R is the total number of legal ending squares (Manning, Raghavan, and Schütze 2008). In our running example, there are 5 legal ending squares, and RPrecision will be calculated for the model's top-5 predictions. ExM accuracy evaluation is similar to the typical evaluation of language models on natural language data, while LgM is less stringent and focuses on testing just the model's understanding of chess rules and the board state. Note that for</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Prompt Token</th>
<th>Correct Answers (ExM)</th>
<th>Correct Answers (LgM)</th>
</tr>
</thead>
<tbody>
<tr>
<td>End-Actual</td>
<td>f1</td>
<td>{b5}</td>
<td>{e2, d3, c4, b5 ,a6}</td>
</tr>
<tr>
<td>End-Other</td>
<td>f3</td>
<td>N/A</td>
<td>{d2, g1, h4, g5, e5}</td>
</tr>
<tr>
<td>Start-Actual</td>
<td>B</td>
<td>{f1}</td>
<td>{f1, c1}</td>
</tr>
<tr>
<td>Start-Other</td>
<td>N</td>
<td>N/A</td>
<td>{f3, b1}</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of each probing task, as well as the corresponding exact move (ExM) and legal move (LgM) correct answers, are shown below. All examples assume the language model was fed the prefix e2e4 e7e5 g1f3 b8c6 d2d4 h7h6 (see Figure 1b), and that the actual next move was fib5. While there is only one valid prompt token for both End-Actual and Start-Actual tasks, there are many valid prompt tokens for the other tasks, and we show just one possibility for each. Start-tasks (bottom sub-table) assume the model was trained on games described in UCI+RAP notation.</p>
<p>End-Other, only LgM evaluation is available. See Table 3 for examples.</p>
<h3>3.4 Starting Square Tasks</h3>
<p>In this category of task, the model is again given a game prefix, but prompted with just the piece type of the next move, such as B for bishop in the example in Table 3. The model's next-token prediction thus represents its prediction for where the prompted piece type currently is on the board. This task tests the model's ability to track pieces. ${ }^{5}$ Note that only models which have seen piece types during training, i.e. "UCI + RAP" models, can actually be tested on this task. Also, no piece types are used in the game prefix. We again have two variants of this task:</p>
<ol>
<li>Start-Actual: Given a move sequence prefix, the model is prompted with the piece type of the actual piece moved next in the game.</li>
<li>Start-Other: Given a move sequence prefix, the model is prompted with the piece type of any piece on the board that can be legally moved according to the rules of chess.
We again evaluate Start-Actual both in terms of ExM accuracy (whether the model predicts the starting square of the piece actually moved next in the game), as well as in terms of LgM accuracy (whether the model predicts the starting square of a legally movable piece of the given piece type) and LgM R-Precision (precision of the model's top-R predictions with respect to all of the R starting squares of legally movable pieces of the given piece type). For Start-Other, only LgM evaluation is applicable; see Table 3 for examples.</li>
</ol>
<h2>4 Experimental Setup</h2>
<p>Data We use the Millionbase dataset which is freely available and has close to 2.9 million quality chess games. ${ }^{6}$ After filtering out duplicate games, games with fewer than 10 moves, and games with more than 150 moves (for the complete game to fit into one transformer window), we are left with around 2.5 million games. From this filtered set we randomly select 200 K games for training, 15 K games each for dev and test, and another 50 K games to create board state</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>probing evaluation sets described in Section 3.2. The dev and test sets are used for perplexity evaluations. The dev set perplexity is used for choosing hyperparameters. From the 200K training set, we create subsets of size 15 K and 50 K which we refer to as "Train-S" and "Train-M", while the full training set is referred to as "Train-L". All the data processing steps requiring chess knowledge, including parsing chess databases, are carried out using python-chess (Fiekas 2012).</p>
<p>To create the board state probing evaluation sets, we use the 50 K games reserved for this task. We only consider prompts for non-pawn pieces since the dynamics of pawns are fairly limited. We ensure that the game prefixes selected are never seen in the training data. The final evaluation set consists of 1000 instances with prefix length (in number of moves) in the range $51 \leq l \leq 100$.
Model Details We use the GPT2-small architecture for our base language model (Vaswani et al. 2017; Radford et al. 2019). GPT2-small is a 12-layer transformer model with 12 attention heads and an embedding size of 768 dimensions. The context size of the model is limited to 512, which is sufficient to cover the longest game in our training set. Note that we only borrow the model architecture; the models themselves are trained from scratch. ${ }^{7}$</p>
<p>For the UCI + RAP $p$ models, we tune over $p \in$ ${5,15,25,50,75,100}$ based on perplexity on the validation set. Note that for perplexity evaluation, logits corresponding to piece type tokens are masked out since piece type tokens are only available during training. We find that $p=25$ performs the best for Train-S and Train-M, while $p=15$ is best for Train-L (Figure 2). Larger values of $p$ lead to greater mismatch between training and inference, while smaller values likely do not provide enough training signal.</p>
<p>We also experiment with other transformer and nontransformer models in Section 5.3. Among the transformer models, we experiment with two "approximate" attention models (i.e., models which approximate the full attention of vanilla transformer models), namely, Reformer (Kitaev, Kaiser, and Levskaya 2020) and Performer (Choromanski et al. 2021). We set the number of layers and attention heads to 12 for both architectures, as in GPT2-small. We also train LSTM language models with and without RAP.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Training Set</th>
<th>Model</th>
<th>Dev set</th>
<th>Test set</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train-S</td>
<td>UCI</td>
<td>23.6</td>
<td>23.6</td>
</tr>
<tr>
<td></td>
<td>UCI + RAP</td>
<td>15.9</td>
<td>15.9</td>
</tr>
<tr>
<td></td>
<td>UCI + AP</td>
<td>16.1</td>
<td>16.2</td>
</tr>
<tr>
<td>Train-M</td>
<td>UCI</td>
<td>11.6</td>
<td>11.6</td>
</tr>
<tr>
<td></td>
<td>UCI + RAP</td>
<td>10.4</td>
<td>10.4</td>
</tr>
<tr>
<td></td>
<td>UCI + AP</td>
<td>10.1</td>
<td>10.0</td>
</tr>
<tr>
<td>Train-L</td>
<td>UCI</td>
<td>7.7</td>
<td>7.7</td>
</tr>
<tr>
<td></td>
<td>UCI + RAP</td>
<td>7.4</td>
<td>7.4</td>
</tr>
<tr>
<td></td>
<td>UCI + AP</td>
<td>7.2</td>
<td>7.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Canonical validation and test set perplexity. By canonical we mean that one move, say flb5, counts as one token.</p>
<p>Training Details Models are trained for 10 epochs with a batch size of 60. Validation is performed at the end of every epoch and training stops whenever the validation loss starts increasing. For optimization we use Adam (Kingma and Ba 2014) with learning rate of $5 \times 10^{-4}$ and L2 weight decay of 0.01 . The learning rate is warmed up linearly over the first $10 \%$ of training followed by a linear decay. To accelerate training, we use mixed precision training (Micikevicius et al. 2018). All experiments are carried out using the PyTorch Lightning framework built on top of PyTorch (Falcon et al. 2019; Paszke et al. 2019). We use the transformers library (Wolf et al. 2019) for all models ${ }^{8}$ except for the Performer model for which we use a popular unofficial implementation. 9</p>
<h2>5 Results</h2>
<p>We first present language modeling results, where we show significant improvements with the addition of RAP (Section 5.1). Next, we show results on the board state probing tasks for the base language model, where we demonstrate that the model trained on the large training set can learn to track pieces and predict legal moves with high accuracy (Section 5.2). Finally, we present results on the probing task with approximate attention transformer architectures and LSTMs, where we show a performance drop in comparison to the base model with full attention (Section 5.3).</p>
<h3>5.1 Language Modeling</h3>
<p>Table 4 presents the perplexity results on the validation and test sets. Figure 2 plots the validation set perplexities as a function of RAP probability for different training set sizes. The addition of RAP and AP leads to a decrease in perplexity for all training sizes, particularly for small training sets. For small training sets, RAP probabilities as high as $50 \%$ can improve the validation perplexity, but for larger training sets,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 2: Validation set perplexities as a function of RAP probabilities for the different training set sizes. RAP 0 is the standard UCI notation. RAP 100 is not shown as perplexities are too high.
lower RAP probabilities are preferred. The reductions in perplexity for RAP are surprising given that the extra tokens added via RAP are not present in the validation and test sets, and thus there is a data distribution shift. Models trained with UCI + AP achieve the lowest perplexities on larger training sets. Both RAP and AP aid the model in piece tracking, as we will see in later results, and in the case of chess this can significantly improve the language modeling results as well. Note that for calculating the perplexity of UCI + RAP models, we mask out the logits corresponding to piece type tokens since they are never present during inference.</p>
<h3>5.2 Board State Tracking</h3>
<p>Tables 5 and 6 show results when predicting starting squares and ending squares, respectively. There are several observations to note. First, transformers can learn to identify where pieces are located. This is shown by the LgM accuracies in Table 5. UCI + RAP can predict legal starting positions with perfect accuracy and R-Precision. However, this capability requires Train-L, and the accuracy drops to $91.3 \%$ for Train-S. The gap between UCI + RAP and its "oracle" counterpart, UCI + AP, also reduces with an increase in training set size with UCI + RAP achieving parity for Train-L. When asked to identify the location of a piece other than the one selected to be moved next, this accuracy drops only slightly to $99.6 \%$. Typically, the piece location tracking is slightly better for the piece type that is actually moved than for other piece types.</p>
<p>The difference between the location of the piece in the exact move (ExM) and the location of either piece of the given type ( LgM ) is substantial, at more than $8 \%$ absolute. However, this difference relates to chess strategy rather than board state tracking.</p>
<p>Second, transformers can learn to predict legal moves. This is shown by the LgM accuracies in Table 6, for which both UCI and UCI + RAP exceed $97 \%$ accuracy. However, while the top predictions of the models have high accuracy, their ability to predict all legal moves is significantly lower, with R-precision of about $85 \%$. This is to be expected, since</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Notation</th>
<th style="text-align: center;">LgM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ExM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Actual</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">R-Prec.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">R-Prec.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">UCI + RAP</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">78.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UCI + AP</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">86.9</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">UCI + RAP</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">88.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UCI + AP</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">90.2</td>
</tr>
<tr>
<td style="text-align: center;">L</td>
<td style="text-align: center;">UCI + RAP</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">91.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UCI + AP</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">91.1</td>
</tr>
<tr>
<td style="text-align: center;">Random Legal</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">86.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracies and R-Precisions (\%) for predicting starting squares ("Start-Actual" and "Start-Other" tasks). S, M, L in the first column refer to the training set sizes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Notation</th>
<th style="text-align: center;">LgM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ExM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Actual</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">R-Prec.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">R-Prec.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">UCI</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">26.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UCI + RAP</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UCI + AP</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">36.1</td>
</tr>
<tr>
<td style="text-align: center;">L</td>
<td style="text-align: center;">UCI</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UCI + RAP</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">45.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UCI + AP</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">47.3</td>
</tr>
<tr>
<td style="text-align: center;">L</td>
<td style="text-align: center;">UCI</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UCI + RAP</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">54.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UCI + AP</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: center;">Random Legal</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">19.6</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracies and R-Precisions (\%) for predicting ending squares ("End-Actual" and "End-Other" tasks). S, M, L in the first column refer to the training set sizes.
the model is trained on only actual games, where the emphasis is on "meaningful" moves rather than any legal move. Due to similar reasons, there's a significant drop in performance when predicting ending squares for starting squares other than the one in the actual game. The "other" starting square would, by design, have legal continuations, but lack any "meaningful" ones.</p>
<p>We find consistent gains in almost all metrics with the addition of RAP during training, with the gains being particularly impressive for small training sets. Thus, not only are the transformers robust to distribution shift due to RAP (available only during training), they are in fact able to utilize this additional information. Error analysis of illegal predictions shows that the addition of RAP improves piece tracking related errors.</p>
<p>The relatively low ExM accuracies of the models can be attributed to the inherent difficulty of the task. Randomly selecting an ending square from all legal ending squares has an accuracy of only around $20 \%$, implying that on average there are roughly 5 legal choices, which might explain the</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">LgM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ExM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Actual</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">R-Prec.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">R-Prec.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">26.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT2 $(w=50)$</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">23.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reformer</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">24.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Performer</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">20.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">20.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM + RAP</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">21.9</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT2 $(w=50)$</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reformer</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">32.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Performer</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">36.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">32.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM + RAP</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">32.1</td>
</tr>
<tr>
<td style="text-align: center;">L</td>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT2 $(w=50)$</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">51.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reformer</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Performer</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">51.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM + RAP</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">46.0</td>
</tr>
</tbody>
</table>
<p>Table 7: Accuracy and R-Precision (\%) for predicting ending squares ("End-Actual" and "End-Other" tasks) with varying attention window sizes. LSTM + RAP refers to LSTM trained with UCI + RAP.
difficulty of the task.</p>
<h3>5.3 Compressing the Game History</h3>
<p>The base transformer language model, based on GPT2, attends to the entire history (i.e., it uses "full attention"), which results in complexity quadratic in the length of the sequence. We might wonder whether attending to this entire history is necessary for the impressive state tracking performance observed in the previous section. We accordingly explore models that do not attend to the entire history in Table 7.</p>
<p>We first experiment with a variant of the GPT2 model that limits its attention to a window of only the 50 most recent tokens ("GPT2 $(w=50)$ "). In Table 7 we see worse performance for this model across data sizes, but especially for small- and medium-sized datasets.</p>
<p>In Table 7 we also consider a language model based on the LSTM (Hochreiter and Schmidhuber 1997), which considers only its current hidden state and cell state in making its predictions, and does not explicitly attend to the history. Here we find an even more significant drop in performance, in all settings. (Interestingly, we also find that training LSTM language models on sequences with RAP improves performance, but only for larger training sets; transformer language models generally improve when trained with RAP data).</p>
<p>The results of GPT2 $(w=50)$ and of the LSTM language model suggest that attending to the full game history is, unsurprisingly, useful for board state tracking in chess. This finding further suggests that the task of board state tracking in chess can serve as an excellent testbed for recently</p>
<p>proposed transformer variants (Kitaev, Kaiser, and Levskaya 2020; Katharopoulos et al. 2020; Choromanski et al. 2021, inter alia) that attempt to make use of long histories or contexts, but without incurring a quadratic runtime.</p>
<p>Approximate Attention Transformers We experiment with the recently proposed Reformer (Kitaev, Kaiser, and Levskaya 2020) and Performer (Choromanski et al. 2021) architectures. Reformer replaces the "full attention" with attention based on locality-sensitive hashing, while Performer approximates the "full attention" with random features. ${ }^{10}$</p>
<p>The results, in Table 7, suggest that the Performer generally outperforms the Reformer, except in the small datasetsetting. Furthermore, we find that neither of these architectures significantly outperforms the GPT2 $(w=50)$ baseline, except for Performer in the medium-sized data setting. These models do, however, typically outperform the LSTM models. These results demonstrate the challenge of modeling chess with an approximate attention. We hope that future work will use this task as a way of benchmarking more efficient transformer architectures.</p>
<h2>6 Related Work</h2>
<p>Simulated Worlds. There have been several prior efforts in relating simulated worlds to natural language. The bAbI framework simulates a world modeled via templates to generate question answering tasks (Weston et al. 2015). The recent TextWorld framework facilitates generating, training, and evaluating interactive text-based games (Côté et al. 2018). Hermann et al. (2017) and Hill et al. (2017) develop and use 3D world simulations for learning grounded language. These efforts are similar to our work in the sense that the true world state is, by construction, available, but our setup differs in that it provides a natural way of probing the state tracking of a model trained with an LM objective.</p>
<p>Cloze Tasks for Natural Language Models. There has been a great deal of work on cloze tasks for evaluating natural language models (Hermann et al. 2015; Hill et al. 2016). These tasks range from testing general text understanding (Paperno et al. 2016) to targeting particular aspects of natural language, such as commonsense/pragmatics (Mostafazadeh et al. 2016; Ettinger 2020), narrative understanding (Mostafazadeh et al. 2017), and factual knowledge (Petroni et al. 2019). Creating these tasks often requires human curation, and the evaluation is typically limited to exact match. ${ }^{11}$ Our proposed tasks are a form of cloze tasks, but can be precisely automated so that they require no human curation, and can be evaluated at a fine-grained level.</p>
<p>Probing. One of the goals of this work is to probe the language model's board state tracking capability. A typical solution used by prior work is to train a probing model on top of a pretrained model (Ettinger, Elgohary, and Resnik 2016; Alain and Bengio 2017; Adi et al. 2017; Tenney et al. 2019;</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Hewitt and Liang 2019). This setup is time-consuming as it requires training probing models for all tasks. Moreover, the complexity of the probing model can also affect the conclusions (Pimentel et al. 2020). In our case, by using an appropriate choice of notation, probing for board state can be accomplished via simple prompts (Section 3).</p>
<p>Deep Learning for Chess. Deep networks have been used in prior work to predict the next move given the true game state (David, Netanyahu, and Wolf 2016; Oshri and Khandwala 2015). For example, using only self-play and the rules of chess, AlphaZero achieves superhuman performance starting from random play (Silver et al. 2018). The focus of this prior work is the quality of game play given the true board state, while we use chess as a testbed for evaluating a language model's board state tracking capability. Recently there has also been work focusing on transformer language models for chess (Presser and Branwen 2020; Cheng 2020; Noever, Ciolino, and Kalin 2020). This work is similar to ours in the sense that the input is limited to the move sequence without the true board state, but the focus is again the quality of game play rather than the model's awareness of the underlying state.</p>
<h2>7 Conclusion</h2>
<p>We propose the game of chess as a testbed for evaluating how well language models capture the underlying world state. We show that with an appropriate choice of chess notation, a language model can be probed for different aspects of the board state via simple prompts. The simple and precise dynamics of chess allow for (a) training models with varying amount of explicit state, and (b) evaluating model predictions at a fine-grained level. Results show that transformer language models are able to track the board state when given enough data, but with limited data, providing access to board state information during training can yield consistent improvement.</p>
<h2>Wider Implications for Natural Language Processing.</h2>
<p>Our results shed light on the following properties of transformers: (a) they are robust to RAP-like changes in input distribution, and (b) for high performance the models require access to the entire context, as well as large training sets (Section 5.3). Future work can use the first finding to introduce the world state, or more specifically the output of linguistic analyzers such as coreference, via RAP-like tokens during pre-training and fine-tuning of transformers. RAP-like tokens can also be used for debugging/diagnosing a model's understanding, similarly to the starting square prediction tasks. The second finding implies that the proposed benchmark can guide the search for new transformer architectures that are adept at understanding long text, and that can learn from small training sets. The proposed framework allows for probing and understanding new architectures that address these challenges.</p>
<h2>Acknowledgements</h2>
<p>We thank Ed Schröder for permitting us to use the Millionbase database for this project. We thank Allyson Ettinger and colleagues at TTI Chicago for their valuable feedback.</p>
<p>This material is based upon work supported by the National Science Foundation under Award No. 1941178.</p>
<h2>References</h2>
<p>Adi, Y.; Kermany, E.; Belinkov, Y.; Lavi, O.; and Goldberg, Y. 2017. Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks. In $I C L R$.
Alain, G.; and Bengio, Y. 2017. Understanding intermediate layers using linear classifier probes. In ICLR Workshop.
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165.
Cheng, R. 2020. Transformers Play Chess. https://github.com/ricsonc/transformers-play-chess.
Choromanski, K. M.; Likhosherstov, V.; Dohan, D.; Song, X.; Gane, A.; Sarlos, T.; Hawkins, P.; Davis, J. Q.; Mohiuddin, A.; Kaiser, L.; Belanger, D. B.; Colwell, L. J.; and Weller, A. 2021. Rethinking Attention with Performers. In ICLR.
Côté, M.-A.; Kádár, A.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Tao, R. Y.; Hausknecht, M.; Asri, L. E.; Adada, M.; Tay, W.; and Trischler, A. 2018. TextWorld: A Learning Environment for Text-based Games. CoRR, abs/1806.11532.
David, E.; Netanyahu, N. S.; and Wolf, L. 2016. DeepChess: End-to-End Deep Neural Network for Automatic Learning in Chess. In International Conference on Artificial Neural Networks (ICANN).
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL.
Ettinger, A. 2020. What BERT is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models. TACL, 8(0).
Ettinger, A.; Elgohary, A.; and Resnik, P. 2016. Probing for semantic evidence of composition by means of simple classification tasks. In 1st Workshop on Evaluating VectorSpace Representations for NLP.
Falcon et al., W. 2019. PyTorch Lightning. https://github.com/PyTorchLightning/pytorch-lightning.
Fiekas, N. 2012. python-chess: a chess library for Python. https://github.com/niklasf/python-chess.
Hermann, K. M.; Hill, F.; Green, S.; Wang, F.; Faulkner, R.; Soyer, H.; Szepesvari, D.; Czarnecki, W. M.; Jaderberg, M.; Teplyashin, D.; Wainwright, M.; Apps, C.; Hassabis, D.; and Blunsom, P. 2017. Grounded Language Learning in a Simulated 3D World. CoRR, abs/1706.06551.
Hermann, K. M.; Kočiský, T.; Grefenstette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blunsom, P. 2015. Teaching Machines to Read and Comprehend. In NeurIPS.</p>
<p>Hewitt, J.; and Liang, P. 2019. Designing and Interpreting Probes with Control Tasks. In EMNLP-IJCNLP.
Hill, F.; Bordes, A.; Chopra, S.; and Weston, J. 2016. The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations. In ICLR.
Hill, F.; Hermann, K. M.; Blunsom, P.; and Clark, S. 2017. Understanding Grounded Language Learning Agents. CoRR, abs/1710.09867.
Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term memory. Neural computation, 9.
Katharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F. 2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In ICML.
Kingma, D. P.; and Ba, J. 2014. Adam: A method for stochastic optimization. In ICLR.
Kitaev, N.; Kaiser, L.; and Levskaya, A. 2020. Reformer: The Efficient Transformer. In ICLR.
Manning, C. D.; Raghavan, P.; and Schütze, H. 2008. Introduction to Information Retrieval. Cambridge University Press.
Micikevicius, P.; Narang, S.; Alben, J.; Diamos, G.; Elsen, E.; Garcia, D.; Ginsburg, B.; Houston, M.; Kuchaiev, O.; Venkatesh, G.; and Wu, H. 2018. Mixed Precision Training. In ICLR.
Mostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Batra, D.; Vanderwende, L.; Kohli, P.; and Allen, J. 2016. A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories. In NAACL.
Mostafazadeh, N.; Roth, M.; Louis, A.; Chambers, N.; and Allen, J. 2017. LSDSem 2017 Shared Task: The Story Cloze Test. In 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics.
Noever, D.; Ciolino, M.; and Kalin, J. 2020. The Chess Transformer: Mastering Play using Generative Language Models. arXiv:2008.04057.
Oshri, B.; and Khandwala, N. 2015. Predicting Moves in Chess using Convolutional Neural Networks. In Stanford CS231n Course Report.
Paperno, D.; Kruszewski, G.; Lazaridou, A.; Pham, N. Q.; Bernardi, R.; Pezzelle, S.; Baroni, M.; Boleda, G.; and Fernández, R. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In $A C L$.
Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.; and Chintala, S. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS.
Petroni, F.; Rocktäschel, T.; Riedel, S.; Lewis, P.; Bakhtin, A.; Wu, Y.; and Miller, A. 2019. Language Models as Knowledge Bases? In EMNLP-IJCNLP.
Pimentel, T.; Valvoda, J.; Hall Maudslay, R.; Zmigrod, R.; Williams, A.; and Cotterell, R. 2020. Information-Theoretic Probing for Linguistic Structure. In ACL.</p>
<p>Presser, S.; and Branwen, G. 2020. A Very Unlikely Chess Game. https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language Models are Unsupervised Multitask Learners. In Technical Report, OpenAI.
Silver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai, M.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel, T.; Lillicrap, T.; Simonyan, K.; and Hassabis, D. 2018. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419).
Tenney, I.; Xia, P.; Chen, B.; Wang, A.; Poliak, A.; McCoy, R. T.; Kim, N.; Durme, B. V.; Bowman, S. R.; Das, D.; and Pavlick, E. 2019. What do you learn from context? Probing for sentence structure in contextualized word representations. In $I C L R$.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. Attention is All you Need. In NeurIPS.
Weston, J.; Bordes, A.; Chopra, S.; Rush, A. M.; van Merriënboer, B.; Joulin, A.; and Mikolov, T. 2015. Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks. arXiv:1502.05698.
Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest, Q.; and Rush, A. M. 2019. HuggingFace's Transformers: State-of-the-art Natural Language Processing. ArXiv, abs/1910.03771.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ In practice, these models often use a combination of the proposed approximate global attention and simple local attention.
${ }^{11}$ Automated cloze tasks without human filtering can yield instances which even humans can't answer (Hill et al. 2016).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ Colab notebook to play chess against the base language model https://github.com/shtoshn//learning-chessblindfolded/blob/master/GPT2_Chess_Model.ipynb&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
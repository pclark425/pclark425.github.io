<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1565 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1565</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1565</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-8c78c9de81ef31a6550f07a65a37dfa38057a091</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8c78c9de81ef31a6550f07a65a37dfa38057a091" target="_blank">Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games</a></p>
                <p><strong>Paper TL;DR:</strong> This work builds agents that learn to tackle simple scenarios before more complex ones using curriculum learning, that familiarize themselves in an unfamiliar environment by navigating before acting, and that explore uncertain environments more thoroughly using contextual multi-armed bandit decision policies.</p>
                <p><strong>Paper Abstract:</strong> We consider the task of learning to play families of text-based computer adventure games, i.e., fully textual environments with a common theme (e.g. cooking) and goal (e.g. prepare a meal from a recipe) but with different specifics; new instances of such games are relatively straightforward for humans to master after a brief exposure to the genre but have been curiously difficult for computer agents to learn. We find that the deep Q-learning strategies that have been successfully leveraged for superhuman performance in single-instance action video games can be applied to learn families of text video games when adopting simple strategies that correlate with human-like learning behavior. Specifically, we build agents that learn to tackle simple scenarios before more complex ones using curriculum learning, that familiarize themselves in an unfamiliar environment by navigating before acting, and that explore uncertain environments more thoroughly using contextual multi-armed bandit decision policies. We demonstrate improved task completion rates over reasonable baselines when evaluating on never-before-seen games of that theme.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1565.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1565.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>curric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum Learning (tiered difficulty curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-difficulty curriculum that trains an agent on increasingly complex families of TextWorld cooking games by starting with simplest single-room, single-ingredient tasks and progressing through multi-ingredient and multi-room tasks, with intermediate mixing and a final fine-tuning stage; used to teach reusable cooking skills and navigation strategies to a DRRN agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN (Deep Reinforcement Relevance Network)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A DQN-style architecture where states are encoded (CNN) and text actions are encoded (LSTM) and scored via bilinear interaction f(s,a)=h_s^T W h_a; trained with replay memory and epsilon-greedy exploration. Encodes trajectories and variable action sets and was the primary model used with curriculum training.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Microsoft 'First TextWorld Problems' cooking games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated text-based adventure cooking games (4,440 games) where the agent receives textual observations and selects admissible text actions (navigation, take/open/drop/cook/etc.), with per-step rewards for correct acquisitions/preparations and a goal of preparing a specified recipe. Admissible actions can be requested at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures (household/cooking tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Prepare recipes by finding and acquiring ingredients, using appliances (e.g., stove), cooking ingredients correctly (e.g., fry potato), opening containers/doors, dropping items when necessary, and navigating between rooms to reach the kitchen.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are compositional: preparing a recipe decomposes into subtasks (locate ingredient, pick up, possibly drop another item, apply correct cooking action with appropriate tool), and navigation composes with manipulation actions; complexity arises from combining multiple ingredients and multi-room navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>tiered difficulty curriculum (curric)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Training proceeds in stages: start with tier-1 games (1 ingredient, 1 room) until convergence, initialize next-stage model from previous stage, train on mixed lower tiers for tiers 2-3 (mixing tier-1 and tier-2 when training tier-2, etc.), for tiers 4-6 train on that stage's data only, and finally fine-tune on all training games starting from the best model of the last tier. Epsilon is initialized at 1 and decayed (to 1e-4) over up to two million steps per stage (ablation used longer decay for mixed).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task difficulty (increasing number of ingredients and number of rooms)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Tier 1: 1 ingredient, 1 room; Tier 2: 2 ingredients, 1 room; Tier 3: 3 ingredients, 1 room; Tier 4: <=3 ingredients, 6 rooms; Tier 5: <=3 ingredients, 9 rooms; Tier 6: <=3 ingredients, 12 rooms</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Overall test score: 64% of achievable points on both Test-1 and Test-2 (Table 2). Tier breakdown (Test-1): Tier1 96%, Tier2 75%, Tier3 61%, Tier4 68%, Tier5 64%, Tier6 46% (Table 3); after final fine-tuning best overall performance is reported as 64% (All tiers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Training all tiers together (mixed) yielded lower overall performance: Test-1 50% and Test-2 54% (Table 2). Tier breakdown (Test-1 mixed): Tier1 88%, Tier2 53%, Tier3 57%, Tier4 55%, Tier5 40%, Tier6 36% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared 'mixed' (no curriculum) versus 'curric' (tiered): curric improved overall score from 50%->64% (Test-1) and 54%->64% (Test-2). Per-tier gains were largest on mid tiers (e.g., Tier5 improved from 40% to 64% on Test-1). The curriculum also included a final fine-tuning stage on all tiers which yielded best overall performance; ablation used longer epsilon-decay when training mixed (10M steps) versus 2M per curriculum stage.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Zero-shot generalization to unseen games of the same theme was evaluated: the curriculum-trained agent generalized substantially better (64% overall on unseen test sets) than mixed training (50-54%). Performance generalizes best to the most recently trained tier (recency effect), and final fine-tuning across all tiers produced the best overall generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A difficulty-ordered curriculum substantially improves zero-shot performance on unseen TextWorld cooking games (overall +14 percentage points). Curriculum helps learn core reusable cooking skills early (tier-1) before adding navigation and multi-ingredient complexity; recency effects occur (best performance on the latest trained tier), and final fine-tuning on all tiers gives the best aggregate performance. Curriculum is less effective for highest-complexity (tier-6) than for mid-level tiers, indicating diminishing returns as environment combinatorics (rooms) increase; combining curriculum with map familiarization (go-room) and uncertainty-aware evaluation (LinUCB) further improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1565.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1565.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixed Training (no curriculum / all-tiers-at-once)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline training strategy where the agent is trained on all tiers/games simultaneously (random/mixed sampling from training pool) rather than in staged difficulty increments; used as a direct contrast to the tiered curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN (Deep Reinforcement Relevance Network)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same DRRN architecture as used with curriculum: CNN state encoder and LSTM action encoder, bilinear scoring for Q-values, trained with replay memory and epsilon-greedy exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Microsoft 'First TextWorld Problems' cooking games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated cooking games with textual observations, admissible-text actions (navigation, manipulation, cooking), per-action rewards, and goals of preparing specified recipes; test evaluates zero-shot on unseen instances of the theme.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures (cooking / household tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Same cooking tasks: locate and take ingredients, use appliances to cook (fry, etc.), open containers, drop items, navigate between rooms to access kitchen and tools.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Procedures compose navigation and manipulation subtasks; mixed training exposes the agent to the full distribution of complexity from the start rather than staged buildup.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>random/mixed sampling (no explicit ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Training pool contains all tiers simultaneously: tier-1 through tier-6 as described above (1 ingredient/1 room up to <=3 ingredients/12 rooms).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Mixed training overall scores: Test-1 50% and Test-2 54% (Table 2). Per-tier (Test-1 mixed): Tier1 88%, Tier2 53%, Tier3 57%, Tier4 55%, Tier5 40%, Tier6 36% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Mixed training was compared directly to the tiered 'curric' approach; 'curric' outperformed mixed training across nearly all tiers and in overall zero-shot evaluation (e.g., Test-1 all tiers: mixed 50% vs curric 64%).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Mixed training produced weaker zero-shot generalization to unseen games compared to curriculum training (50-54% vs 64%), indicating that staged difficulty exposure improved transfer of commonsense cooking and navigation skills.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training on all tasks at once leads to substantially worse zero-shot generalization than a staged curriculum; mixed training converges more slowly (authors used a longer epsilon-decay in ablation) and yields lower per-tier scores, particularly on mid-to-high complexity tiers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Deep reinforcement learning with a natural language action space <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1565",
    "paper_id": "paper-8c78c9de81ef31a6550f07a65a37dfa38057a091",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "curric",
            "name_full": "Curriculum Learning (tiered difficulty curriculum)",
            "brief_description": "A task-difficulty curriculum that trains an agent on increasingly complex families of TextWorld cooking games by starting with simplest single-room, single-ingredient tasks and progressing through multi-ingredient and multi-room tasks, with intermediate mixing and a final fine-tuning stage; used to teach reusable cooking skills and navigation strategies to a DRRN agent.",
            "citation_title": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games",
            "mention_or_use": "use",
            "agent_name": "DRRN (Deep Reinforcement Relevance Network)",
            "agent_description": "A DQN-style architecture where states are encoded (CNN) and text actions are encoded (LSTM) and scored via bilinear interaction f(s,a)=h_s^T W h_a; trained with replay memory and epsilon-greedy exploration. Encodes trajectories and variable action sets and was the primary model used with curriculum training.",
            "agent_size": null,
            "environment_name": "TextWorld (Microsoft 'First TextWorld Problems' cooking games)",
            "environment_description": "Procedurally generated text-based adventure cooking games (4,440 games) where the agent receives textual observations and selects admissible text actions (navigation, take/open/drop/cook/etc.), with per-step rewards for correct acquisitions/preparations and a goal of preparing a specified recipe. Admissible actions can be requested at each step.",
            "procedure_type": "commonsense procedures (household/cooking tasks)",
            "procedure_examples": "Prepare recipes by finding and acquiring ingredients, using appliances (e.g., stove), cooking ingredients correctly (e.g., fry potato), opening containers/doors, dropping items when necessary, and navigating between rooms to reach the kitchen.",
            "compositional_structure": "Tasks are compositional: preparing a recipe decomposes into subtasks (locate ingredient, pick up, possibly drop another item, apply correct cooking action with appropriate tool), and navigation composes with manipulation actions; complexity arises from combining multiple ingredients and multi-room navigation.",
            "uses_curriculum": true,
            "curriculum_name": "tiered difficulty curriculum (curric)",
            "curriculum_description": "Training proceeds in stages: start with tier-1 games (1 ingredient, 1 room) until convergence, initialize next-stage model from previous stage, train on mixed lower tiers for tiers 2-3 (mixing tier-1 and tier-2 when training tier-2, etc.), for tiers 4-6 train on that stage's data only, and finally fine-tune on all training games starting from the best model of the last tier. Epsilon is initialized at 1 and decayed (to 1e-4) over up to two million steps per stage (ablation used longer decay for mixed).",
            "curriculum_ordering_principle": "task difficulty (increasing number of ingredients and number of rooms)",
            "task_complexity_range": "Tier 1: 1 ingredient, 1 room; Tier 2: 2 ingredients, 1 room; Tier 3: 3 ingredients, 1 room; Tier 4: &lt;=3 ingredients, 6 rooms; Tier 5: &lt;=3 ingredients, 9 rooms; Tier 6: &lt;=3 ingredients, 12 rooms",
            "performance_with_curriculum": "Overall test score: 64% of achievable points on both Test-1 and Test-2 (Table 2). Tier breakdown (Test-1): Tier1 96%, Tier2 75%, Tier3 61%, Tier4 68%, Tier5 64%, Tier6 46% (Table 3); after final fine-tuning best overall performance is reported as 64% (All tiers).",
            "performance_without_curriculum": "Training all tiers together (mixed) yielded lower overall performance: Test-1 50% and Test-2 54% (Table 2). Tier breakdown (Test-1 mixed): Tier1 88%, Tier2 53%, Tier3 57%, Tier4 55%, Tier5 40%, Tier6 36% (Table 3).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared 'mixed' (no curriculum) versus 'curric' (tiered): curric improved overall score from 50%-&gt;64% (Test-1) and 54%-&gt;64% (Test-2). Per-tier gains were largest on mid tiers (e.g., Tier5 improved from 40% to 64% on Test-1). The curriculum also included a final fine-tuning stage on all tiers which yielded best overall performance; ablation used longer epsilon-decay when training mixed (10M steps) versus 2M per curriculum stage.",
            "transfer_generalization": "Zero-shot generalization to unseen games of the same theme was evaluated: the curriculum-trained agent generalized substantially better (64% overall on unseen test sets) than mixed training (50-54%). Performance generalizes best to the most recently trained tier (recency effect), and final fine-tuning across all tiers produced the best overall generalization.",
            "key_findings": "A difficulty-ordered curriculum substantially improves zero-shot performance on unseen TextWorld cooking games (overall +14 percentage points). Curriculum helps learn core reusable cooking skills early (tier-1) before adding navigation and multi-ingredient complexity; recency effects occur (best performance on the latest trained tier), and final fine-tuning on all tiers gives the best aggregate performance. Curriculum is less effective for highest-complexity (tier-6) than for mid-level tiers, indicating diminishing returns as environment combinatorics (rooms) increase; combining curriculum with map familiarization (go-room) and uncertainty-aware evaluation (LinUCB) further improves results.",
            "uuid": "e1565.0",
            "source_info": {
                "paper_title": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "mixed",
            "name_full": "Mixed Training (no curriculum / all-tiers-at-once)",
            "brief_description": "Baseline training strategy where the agent is trained on all tiers/games simultaneously (random/mixed sampling from training pool) rather than in staged difficulty increments; used as a direct contrast to the tiered curriculum.",
            "citation_title": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games",
            "mention_or_use": "use",
            "agent_name": "DRRN (Deep Reinforcement Relevance Network)",
            "agent_description": "Same DRRN architecture as used with curriculum: CNN state encoder and LSTM action encoder, bilinear scoring for Q-values, trained with replay memory and epsilon-greedy exploration.",
            "agent_size": null,
            "environment_name": "TextWorld (Microsoft 'First TextWorld Problems' cooking games)",
            "environment_description": "Procedurally generated cooking games with textual observations, admissible-text actions (navigation, manipulation, cooking), per-action rewards, and goals of preparing specified recipes; test evaluates zero-shot on unseen instances of the theme.",
            "procedure_type": "commonsense procedures (cooking / household tasks)",
            "procedure_examples": "Same cooking tasks: locate and take ingredients, use appliances to cook (fry, etc.), open containers, drop items, navigate between rooms to access kitchen and tools.",
            "compositional_structure": "Procedures compose navigation and manipulation subtasks; mixed training exposes the agent to the full distribution of complexity from the start rather than staged buildup.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": "random/mixed sampling (no explicit ordering)",
            "task_complexity_range": "Training pool contains all tiers simultaneously: tier-1 through tier-6 as described above (1 ingredient/1 room up to &lt;=3 ingredients/12 rooms).",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "Mixed training overall scores: Test-1 50% and Test-2 54% (Table 2). Per-tier (Test-1 mixed): Tier1 88%, Tier2 53%, Tier3 57%, Tier4 55%, Tier5 40%, Tier6 36% (Table 3).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Mixed training was compared directly to the tiered 'curric' approach; 'curric' outperformed mixed training across nearly all tiers and in overall zero-shot evaluation (e.g., Test-1 all tiers: mixed 50% vs curric 64%).",
            "transfer_generalization": "Mixed training produced weaker zero-shot generalization to unseen games compared to curriculum training (50-54% vs 64%), indicating that staged difficulty exposure improved transfer of commonsense cooking and navigation skills.",
            "key_findings": "Training on all tasks at once leads to substantially worse zero-shot generalization than a staged curriculum; mixed training converges more slowly (authors used a longer epsilon-decay in ablation) and yields lower per-tier scores, particularly on mid-to-high complexity tiers.",
            "uuid": "e1565.1",
            "source_info": {
                "paper_title": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Deep reinforcement learning with a natural language action space",
            "rating": 1
        }
    ],
    "cost": 0.01002875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games</h1>
<p>Xusen Yin ${ }^{1}$, Jonathan May ${ }^{1}$<br>${ }^{1}$ Information Sciences Institute/University of Southern California<br>{xusenyin, jonmay} @isi.edu</p>
<h4>Abstract</h4>
<p>We consider the task of learning to play families of text-based computer adventure games, i.e., fully textual environments with a common theme (e.g. cooking) and goal (e.g. prepare a meal from a recipe) but with different specifics; new instances of such games are relatively straightforward for humans to master after a brief exposure to the genre but have been curiously difficult for computer agents to learn. We find that the deep Q-learning strategies that have been successfully leveraged for superhuman performance in singleinstance action video games can be applied to learn families of text video games when adopting simple strategies that correlate with human-like learning behavior. Specifically, we build agents that learn to tackle simple scenarios before more complex ones using curriculum learning, that familiarize themselves in an unfamiliar environment by navigating before acting, and that explore uncertain environments more thoroughly using contextual multi-armed bandit decision policies. We demonstrate improved task completion rates over reasonable baselines when evaluating on never-before- seen games of that theme.</p>
<h2>1 Introduction</h2>
<p>Building agents able to play text-based adventure games is a useful proxy task for learning open-world goal-oriented problem-solving dialogue agents. Via an alternating sequence of natural language descriptions given by the game and natural language commands given by the player, a player-agent navigates an environment, discovers and interacts with entities, and accomplishes a goal, receiving explicit rewards (a.k.a. scores) for doing so. Human players are skilled at text games when they understand the situation they are placed in and can make rational decisions based on their life and game playing experience. For example, in the classic text game Zork [Lebling et al., 1979], the adventurer discovers an air pump and an uninflated plastic boat; common sense leads human players to inflate the boat with the pump; or in an unfamiliar environment, human players try new actions other than relying on their game experience.</p>
<p>Games such as Zork are very complicated and are designed to be played repeatedly until all the puzzles contained within have been solved; in this way, they are not very similar to real human experiences. Another kind of text game, as exemplified by the TextWorld learning environment [Côté et al., 2018] and competition, expects agents to learn a particular task theme (such as rescuing victims from a burning building or preparing a meal) but evaluates on never-before-seen instances of that theme in a zero-shot evaluation setting. This is a much more realistic scenario. A person who has never cooked a meal before would no doubt flounder when asked to prepare one. In order to learn to cook, one does not begin by learning to make Coq au Vin, but rather starts simply and works up to more complicated tasks. However, once the cooking skill is learned, one would reasonably expect to be able to prepare a new recipe the first time it is seen. Furthermore, even if the recipe was prepared in a somewhat unfamiliar location (say, the kitchen of a vacation home), a reasonable person would explore the new space, recognize the familiar rooms and elements, and then begin cooking.</p>
<p>In this work, we approach this more-realistic scenario and consider how we might train models to learn to play familiar but unseen text games by adopting a training regimen that mirrors human skill acquisition. We additionally show that, by exploring the search space more thoroughly and evenly by leveraging multi-armed bandit feedback, an agent can reach higher scores in the zero-shot evaluation setting. Specifically, we make the following contributions in our text game agent learning models:</p>
<ul>
<li>We build agents that can play unseen text-based games. We show how the proper use of domain-aware curriculum learning strategies can lead to a better learned agent than learning with all games at once.</li>
<li>We draw a distinction between knowledge into the universal (e.g., that cooking can be done in the kitchen) and instance (e.g. that the kitchen is east of the bedroom); the former can be usefully learned with training data, but the latter cannot. We show how environment familiarization through construction of a knowledge graph improves learning.</li>
<li>We show that incorporating bandit feedback during evaluation leads to a better agent by exploring environments more thoroughly, especially in a zero-shot test with new games.</li>
</ul>
<p>2 Reinforcement Learning for Text Games</p>
<p>The influential Deep Q-Network (DQN) approach of learning simple action video games pioneered by Mnih et al. [2015] has motivated research into the limits of this technique when applied to other kinds of games. We follow recent work that ports this approach to text-based games [Narasimhan et al., 2015; He et al., 2016; Fulda et al., 2017; Zahavy et al., 2018; Ansari et al., 2018; Kostka et al., 2017; Yuan et al., 2018; Ammanabrolu and Riedl, 2018; Yin and May, 2019]. The core approach of DQN as described by Mnih et al. [2015] is to build a replay memory of partial games with associated scores, and use this to learn a function $f_{D Q N}:(S,A) \rightarrow \mathcal{R}$, where $f_{D Q N}(s, a)$ predicts a Q-value, which is the future discounted return until the game terminates, obtained by choosing action $a \in A$ when in state $s \in S$. From $s$, choosing $\arg \max <em D="D" N="N" Q="Q">{a \in A} f</em>(s, a)$ affords the optimal action policy.</p>
<p>As in the original work, a key innovation is using the appropriate input to determine the game state; for video games, it is using a sequence of images (e.g. 4-frame of images in [Mnih et al., 2015]) from the game display; while for text games we use a history of system description-player action sequences, which we call a trajectory; an abbreviated example is given in Figure 1. A means of efficiently representing infinite $S$ is necessary; most related work uses LSTMs [Narasimhan et al., 2015; Ammanabrolu and Riedl, 2018; Yuan et al., 2018; Kostka et al., 2017; Ansari et al., 2018], though we follow [Zahavy et al., 2018; Yin and May, 2019], which uses CNNs, to achieve greater speed in training. The DQN is trained in an exploration-exploitation method ( $\epsilon$-greedy): with probability $\epsilon$, the agent chooses a random action (explores), and otherwise the agent chooses the action that maximizes the DQN function. The hyperparameter $\epsilon$ usually decays from 1 to 0 during the training process. At inference time, $\epsilon=0$ is often used to choose the optimal action from the policy.</p>
<p>Much game-learning research is concerned with the optimization of a single game, e.g. applying DQN repeatedly on Pac-Man with the goal of learning to be very good at playing Pac-Man. While this is a realistic goal when strictly limited to the domain of video game play, ${ }^{1}$ single-game optimization is rather unsatisfying. It is difficult to tell if a single gametrained model has managed to simply overfit on its target or if it has learned something general about the task it is trying to complete. More concretely, if we consider game playing as a proxy for real-world navigation (in the action game genre) or task-oriented dialogue (in the text genre), it is clear that a properly trained agent should be able to succeed in a new, yet familiar environment. We thus depart from the singlegame approach taken by others [Narasimhan et al., 2015; He et al., 2016; Ammanabrolu and Riedl, 2018; Zahavy et al., 2018] and evaluate principally on games that are in the same genre as those seen in training, but that have not previously been played during training.</p>
<h3>2.1 Handling Unbounded Action Representations</h3>
<p>A consequence of learning to play a game that has not been seen before is that actions not seen in training may be necessary at test time. Vanilla DQNs as introduced by Mnih</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The architecture of the DRRN model. Trajectories and actions are encoded by a CNN and an LSTM into hidden states and hidden actions, respectively, followed by a dense layer to compute the Q-vector. On the bottom, we show a truncated example of dialogue from a text game in the cooking genre, with S1 and S2 representing system description, and P1 showing the player's first actions towards S1. S1 + P1 + S2 is an example of a trajectory. P2 shows a set of actions to choose from.
et al. [2015] are incompatible with this modification; they presume a predefined finite action space and were tested for a space of up to 18 (each of nine joystick directions and a potential button push). Additionally, vanilla DQNs presume no semantic relatedness among action spaces, while in text games it would make sense for, e.g., open the door to be semantically closer to shut the door than dice the carrot. In our experiments we assume a game's action set is fully known at inference time but not beforehand, and that actions have some relatedness. ${ }^{2}$ We thus represent actions using Deep Reinforcement Relevance Networks (DRRN) [He et al., 2016] (Figure 1), a modification of the standard DQN. Actions are encoded via an LSTM [Hochreiter and Schmidhuber, 1997] and scored against state representations for Q-values according to this equation: $f_{D R R N}(s, a)=h_{s} W h_{a}$, where $W$ is a learned weight matrix, $h_{s}$ is the encoded state and $h_{a}$ is the encoded action. In preliminary experiments we found that LSTMs worked better than CNNs on the small and similar actions in our space such as take yellow potato from fridge and dice purple potato. In practice, we compute the Q-vector for each state at once with all actions $A$ as $f_{D R R N}(s, A)$, making the DRRN as efficient as a normal DQN.</p>
<h2>3 Methods</h2>
<p>A direct application of DQN-like algorithms on families of games may cause problems. Though there has been some work [Mnih et al., 2015; Ansari et al., 2018] learning to play multiple games with single DQN models, these work deviate from our situation: families of games (over thousands) training and zero-shot evaluation. We consider three problems that</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>arise in our scenario, 1) how to arrange the training process with over thousands games; 2) how to learn possible contradicted knowledge from game to game; and 3) how to utilize the inadequately learned policies for zero-shot evaluation. We propose three methods to alleviate these problems.</p>
<h3>3.1 Curriculum Learning</h3>
<p>Correctly training a DQN-like model to play even a single game can take millions of training steps [Mnih et al., 2015; Yin and May, 2019] due to the need for heavy exploration. If our models are able to learn critical general skills in the early parts of training, they can focus on more fine-grained skills later on. For example, recognizing that the action cook potato with stove matches the cookbook instruction fry potato allows generalization to, e.g., fry eggplant. This skill is needed across all games. More specific skills, like knowing to drop items before picking up other items are less commonly used. Curriculum learning [Bengio et al., 2009] is a good way of structuring our learning to capture core skills first and gradually build in more complicated knowledge. For families of games with natural levels of difficulty, we can arrange games into difficulty levels, and train an agent with simple games first.</p>
<h3>3.2 Learning Universally from Local Information</h3>
<p>Since knowledge like the connection between the behavior of fry and using a stove can be learned from past experience and applied to future scenarios, we call this universal knowledge. Other knowledge that is specific to a particular scenario and not reusable we term instance knowledge. In a specific game, for example, the player may have to go north to reach the kitchen. However, this will not be the case in general. Thus, naively learning a policy for the action go east given a particular state is likely to be sub-optimal. We'd like to ensure that training does not overfit for each single game by turning instance knowledge into universal knowledge.</p>
<p>As it turns out, in the domain we are studying, learning that we must go from the room we are in (generally to reach the kitchen or a room containing missing ingredients) is universal knowledge. A simple way to turn instance into universal knowledge, which we call random-go, is to conflate all actions of the form go direction into a single go action when choosing actions from learned policy, but then randomly choose a cardinal direction.</p>
<p>Since the room we are trying to reach is more universally important than the direction chosen in a particular game, another approach to converting instance to universal knowledge is to augment directions with the name of the room that will be reached before encoding actions. If, in a particular game, the bedroom is east of the hallway, the action go east is modified to be go east to hallway, enabling the action representation to incorporate the more globally useful room type of context into its representation.</p>
<h3>3.3 Zero-shot Evaluation with Uncertainty</h3>
<p>Using the learned policy in a greedy way by choosing actions with argmax (Section 2) leaves no possibility of randomization. Since the policy is learned by the function $h_{s} W h_{a}$,
underestimated representations for infrequently seen stateaction pairs may contain high variance, which leads to the selection of incorrect Q-values and hence sub-optimal policies, leading to repetition of choosing bad actions [Yin and May, 2019]. The phenomenon becomes more severe in the setting of zero-shot evaluation with unseen games, especially when encoding long unseen trajectories.</p>
<p>To mitigate the problem, instead of using the learned policy greedily, two commonly used variants during evaluation to add stochasticity are to 1) use a fixed small $\epsilon$, e.g. 0.05 , to allow additional exploration [Narasimhan et al., 2015; Yin and May, 2019; Zahavy et al., 2018; Yuan et al., 2018] and 2) sample actions from the policy according to action distributions [He et al., 2016]. However, actions that are randomly chosen can have dire results. For example, making the decision to cook an ingredient that has already been cooked will result in destruction of that ingredient and a game failure. Also, when playing unseen games, we should give more opportunity to those state-action pairs that are less seen in the training games, i.e. exploring more in a new environment.</p>
<p>We instead model the uncertainty of choosing actions by employing contextual multi-armed bandit feedback. We assume that during evaluation the Q-value for each action $a$ is linearly related with the encoded state $h_{s}$, w.r.t. the coefficient $\theta_{a}$, plus noise $\eta_{a}$ drawn from an R-sub-Gaussian with a covariance $V_{a}: Q_{s, a}=h_{s}^{T} \theta_{a}+\eta_{a}$. We can solve for parameters $\theta_{a}$ with ridge regression loss $\left|Q_{s, a}-h_{s}^{T} \theta_{a}\right|<em a="a">{2}^{2}+\lambda\left|\theta</em>\right|<em a="a">{2}^{2}$, where $\lambda$ is the hyperparameter to control the size of $\theta</em>$ at step $t$ of one game play:}$. With ridge regression, we derive the covariance matrix $V_{a</p>
<p>$$
V_{a}^{t}=\lambda I+\sum_{j: a^{\prime}=a} h_{s}^{j}\left(h_{s}^{j}\right)^{T}
$$</p>
<p>For the next step $t+1$ of game playing, we compute a confidence bound for $Q_{s, a}^{t+1}$ as $c_{t} \sqrt{\left(h_{s}^{t+1}\right)^{T}\left(V_{a}^{t}\right)^{-1} h_{s}^{t+1}}$, where $h_{s}^{t+1}$ is the next step's hidden state and $c_{t}$ is a normalization value related to $V_{a}^{t}$, as detailed in [Zahavy et al., 2018; Abbasi-yadkori et al., 2011]. Then we select an action that has the largest upper bound of $Q_{s, a}^{t+1}$ :</p>
<p>$$
a^{t+1}=\underset{a \in A}{\arg \max }\left(Q_{s, a}^{t+1}+c_{t} \sqrt{\left(h_{s}^{t+1}\right)^{T}\left(V_{a}^{t}\right)^{-1} h_{s}^{t+1}}\right)
$$</p>
<p>The Q-value with upper bound solved with the linear relation assumption is known as Linear Upper Confidence Bound (LinUCB) [Auer, 2003; Abe et al., 2003; Abbasi-yadkori et al., 2011]. The intuition behind this is that rarely seen stateaction pairs should be explored more, so that we may have more confidence in the $Q$ value of this (state, action) pair. The LinUCB method is not commonly used in the training of DQN-like models, since it requires the feature space for context encoding ( $h_{s}$ in our scenario) to not change during training. This is inherently contradicted by the DQN framework. Although Zahavy et al. [2018] use LinUCB to estimate the elimination signal of actions along with DQN training, they utilize a batch-update training framework that updates $h_{s}$ in a batch, as opposed to in every training step. We instead use LinUCB at evaluation time, when the feature space is fixed.</p>
<table>
<thead>
<tr>
<th>tier</th>
<th>#ingredients</th>
<th>#rooms</th>
<th>#games</th>
<th>max scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>420</td>
<td>1317</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>1</td>
<td>420</td>
<td>1891</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>1</td>
<td>420</td>
<td>2440</td>
</tr>
<tr>
<td>4</td>
<td>$\leq 3$</td>
<td>6</td>
<td>1040</td>
<td>4120</td>
</tr>
<tr>
<td>5</td>
<td>$\leq 3$</td>
<td>9</td>
<td>1040</td>
<td>4028</td>
</tr>
<tr>
<td>6</td>
<td>$\leq 3$</td>
<td>12</td>
<td>1040</td>
<td>4110</td>
</tr>
</tbody>
</table>
<p>Table 1: Tiers of games. The tiers are selected by the difficulty level of games. Tier-1 is the simplest, containing only one ingredient in a recipe and one room to explore per game. Tier-6 is the most difficult, including up to three ingredients in a recipe, and twelve rooms to explore per game. The first three tiers only contain one room, which means there need be no go actions involved in these games. Maximum scores summed up over each tier are shown in the last column.</p>
<h2>4 Experiment Setup</h2>
<h3>4.1 Games</h3>
<p>We use the games released by Microsoft for the 'First TextWorld Problems’ competition. The competition provides 4,440 cooking games generated by the TextWorld framework [Côté et al., 2018]. The goal of each game is to prepare a recipe. The action space is simple, yet expressive, and has a fairly large, though domain-limited, vocabulary. A portion of a simple example is shown in Figure 1.</p>
<p>The games are divided into 222 different types, with 20 games per type. A type is a set of attributes that increase the complexity of a game. These attributes include the number of ingredients, the set of necessary actions, and the number of rooms in the environment. One example of such a type is recipe3 + take3 + open + drop + go9 that implies the game contains three ingredients in the recipe, and players need to find and take the three items. In the process of finding these items, there could be doors to open, e.g. a door of a fridge, or a door of a room. The agent may also need to drop an item it is holding before taking another. Finally, the go9 means there are nine different rooms in the game. A constant reward (i.e. one point) is given for each acquisition or proper preparation of a necessary ingredient as well as for accomplishing the goal (preparing the correct recipe), according to the game design. Each game has a different maximum score, so we report aggregate scores as a percentage of achievable points for evaluation results. Admissible actions are available upon request in the TextWorld cooking games at each step.</p>
<h3>4.2 Game Preparation</h3>
<p>We divide the game types into six tiers of increasing difficulty. The easiest games take place inside a single room and require only one (tier-1), two (tier-2), or three (tier-3) ingredients. More complicated are the multi-room games; these may have six (tier-4), nine (tier-5), or twelve (tier-6) rooms. Intuitively, it should be very easy to learn a tier-1 game. Adding additional ingredients requires knowing how to prepare each ingredient correctly, and adding additional rooms requires finding the kitchen and other locations. Table 1 contains pertier details.</p>
<p>We hold out a selection of $10 \%$ of the games and divide this portion into two separate test sets, each consisting of 222 games, one from each type. We randomly select an additional 400 games as a dev set and keep the remaining games for training. We consider an episode to be a play-through of a game; there are multiple episodes of each game run during training and scores are taken over a 10-episode run of each game when evaluating test. An episode is run until a loss (an ingredient is damaged or the maximum of 100 steps is reached) or a win, by completing the recipe successfully. Apart from the inherent game reward, we add -0.1 reward (i.e. punishment) to every step, to encourage more direct gameplay. Also, if the game stops early because of a loss, we set the instant reward to -1 to penalize the last dire action. We use the reward shaping methods only for model training, and we report raw scores earned from games without any modification during evaluation.</p>
<h3>4.3 Hyperparameters</h3>
<p>During training, we use 50,000 observation steps and 500,000 replay memory entries. From a training run, we select the model with the highest score on the dev set for test inference. The maximum total steps of evaluating on one test set is thus $222 \times 10 \times 100$. The maximum total score is not unique since different games could have different scores (Table 1). We use the percentage of scores as the evaluation criteria in the following sections. The higher the score, the better the agent. We also show a detailed percentage of scores for each tier. The average of scores of all tiers may not be equal to the all-tier result, because the total maximum scores for each tier may differ. Unless otherwise noted (e.g. in Section 5.4), we use the greedy method with $\operatorname{argmax}$ to choose the best action from the learned policy during evaluation.</p>
<p>We use a CNN with 32 of each size-3, 4, 5 convolutional filters, followed by a max-pooling layer, following the encoder architecture of [Yin and May, 2019]. The LSTM action encoder contains 32 units in a single layer. We use the last LSTM hidden state as the encoded action state. We initialize our models with random word embeddings and position embeddings. We use a fixed embedding size of 64 . At every training step, we draw a minibatch of 32 samples and use a learning rate of $1 e-5$ with the Adam optimizer [Kingma and Ba, 2015]. We trim trajectories to contain no more than 11 sentences and 1,000 tokens to avoid unnecessarily long concatenated strings.</p>
<h2>5 Experiments and Discussion</h2>
<h3>5.1 Core Results</h3>
<p>Core findings are shown in Table 2. For a simple, trainingfree baseline, we choose a random action from the set of admissible actions at each state. Our main comparisons are that of curriculum learning (curric) as described in Section 3.1 to the default (mixed), and between the three different approaches to handling instance knowledge as described in Section 3.2, and evaluation that compares the use of LinUCB to the greedy method, a small $\epsilon$ to allow some inference-time stochasticity, and sampling from policy distribution as described in Section 3.3. We next take a more in-depth look at the differences in learning behavior.</p>
<table>
<thead>
<tr>
<th></th>
<th>Experiment</th>
<th>Score %</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Test 1</td>
<td>Test 2</td>
<td></td>
</tr>
<tr>
<td>baseline</td>
<td>random</td>
<td>14</td>
<td>14</td>
</tr>
<tr>
<td>curriculum</td>
<td>mixed</td>
<td>50</td>
<td>54</td>
</tr>
<tr>
<td>(Section 3.1)</td>
<td>curric</td>
<td>64</td>
<td>64</td>
</tr>
<tr>
<td>familiarization</td>
<td>go-cardinal</td>
<td>50</td>
<td>52</td>
</tr>
<tr>
<td>(Section 3.2)</td>
<td>go-random</td>
<td>55</td>
<td>57</td>
</tr>
<tr>
<td></td>
<td>go-room</td>
<td>55</td>
<td>58</td>
</tr>
<tr>
<td>uncertainty</td>
<td>$\epsilon=0$</td>
<td>64</td>
<td>64</td>
</tr>
<tr>
<td>(Section 3.3)</td>
<td>$\epsilon=0.05$</td>
<td>63</td>
<td>63</td>
</tr>
<tr>
<td></td>
<td>sampling</td>
<td>69</td>
<td>67</td>
</tr>
<tr>
<td></td>
<td>LinUCB</td>
<td>72</td>
<td>68</td>
</tr>
</tbody>
</table>
<p>Table 2: Core overall results on unseen games of all difficulty levels. The random action baseline gives predictably poor results. Using curriculum learning (curric) is preferred to training with all games simultaneously (mixed). Casting directions in terms of the room destination (go-room) generalizes better than learning specific cardinal directions (go-cardinal), but the alternative of picking a direction at random (go-random) appears surprisingly competitive. Evaluation with LinUCB can further improve scores by more thorough exploration, comparing with greedy policy usage $\epsilon=0, \epsilon=0.05$ to allow stochasticity, and sampling from policy distribution.</p>
<h3>5.2 Curriculum Analysis</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">Tier</th>
<th style="text-align: center;">Test 1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test 2</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mixed</td>
<td style="text-align: center;">curric</td>
<td style="text-align: center;">mixed</td>
<td style="text-align: center;">curric</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">71</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">69</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">63</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">43</td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">64</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparing the evaluation results of training all tiers together (mixed) and training with curriculum learning (curric) on the two separate test sets. Rows 1-6 show the breakdown of total scores and steps on each tier. The curriculum learning method generally shows better results on both test sets.</p>
<p>We initially only train with tier-1 training data. After convergence we then use the best model selected from a dev set of games to initialize the model of tier-2, and so on. Because tiers 1-3 differ significantly from tiers 4-6 (the latter have movements and more games per tier), we alter our approach slightly as training proceeds.</p>
<p>We start training tier-1 with the games of tier-1 only. When we train tier-2, we mix the games of tier-1 and tier-2 in order to make the agent perform well on both tiers. We then mix tier-3 data in. But for tier-4 to tier-6, we only use the data for the specific stage of training, and do not mix in data from previous tiers. As a final stage, we fine-tune the agent with all training games starting from the best model of the last tier. For each stage of the curriculum learning we initialize $\epsilon$ to 1 and decay evenly to $1 \mathrm{e}-4$ across a maximum of two million steps. In ablation experiments without curriculum learning ('mix go-room') we instead decay over 10 million steps.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The training process of 'curric' with go-room strategy broken down by tier. Fine tuning over all tiers is marked as 'all'. Results on tier-specific dev sets are shown. Each tier is trained starting with the best model of its previous tier. The learning is generally rational (scores go up) but is less effective in tiers 2 and 3.</p>
<p>Training graphs for 'curric' with the 'go-room' strategy broken down by tier are shown in Figure 2. For tier-1 we converge to around $95 \%$ of total score after 140 epochs, which means our agent grasps basic cooking abilities. However, the results of tier-2 and tier-3 are flat, indicating there is minor ingredient confusion but it is never resolved. For tiers 4 through 6 , scores generally improve from $40 \%$ to roughly $60 \%$, indicating progressive ability to learn to navigate rooms. Here we can see that a) curriculum training is generally helpful at every tier, and that b) the ability to reach full scores generally decreases by tier.</p>
<p>Table 3 breaks down the test results 'mixed go-room' and 'curric go-room' by tier, evaluating after all training is complete. Curriculum learning generally outperforms mixed training in each tier.</p>
<h3>5.3 Analysis of Universal Information Conversion</h3>
<p>We create a simple knowledge graph to collect the room information for each game during both training and inference. When there is a game with unknown room information, we collect this information by random walking with go and open actions (e.g. open a sliding door) that are essential for walking through each game. To avoid incurring extra steps at inference time for building the knowledge graph, we take the first 50 steps out of the total 100 available steps to perform the random walk when the floor map has not been collected for a room, and the remaining steps to perform normal game playing for all episodes of each game. The knowledge graph provides extra room information for each go action, and can be used even when partially built.</p>
<p>Table 4 breaks down the performance of each strategy for dealing with instance information in each relevant tier. The model used for evaluation is the best model on the dev set after the tier-6 training of curriculum learning, without the final fine-tune. It is clear that 'go-cardinal,' which does not convert any instance information, is less able to learn than the other methods at any tier. As the number of rooms to navigate grows from tier-4 to tier-6, the random navigation strategy becomes less effective, such that the 'go-room' transferring from instance-level cardinal information into universal-level</p>
<table>
<thead>
<tr>
<th>Test</th>
<th>Tier</th>
<th>go-cardinal</th>
<th>go-random</th>
<th>go-room</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>4</td>
<td>49</td>
<td>58</td>
<td>56</td>
</tr>
<tr>
<td>Test-1</td>
<td>5</td>
<td>40</td>
<td>48</td>
<td>49</td>
</tr>
<tr>
<td></td>
<td>6</td>
<td>36</td>
<td>44</td>
<td>47</td>
</tr>
<tr>
<td></td>
<td>4</td>
<td>55</td>
<td>58</td>
<td>58</td>
</tr>
<tr>
<td>Test-2</td>
<td>5</td>
<td>50</td>
<td>55</td>
<td>60</td>
</tr>
<tr>
<td></td>
<td>6</td>
<td>28</td>
<td>41</td>
<td>45</td>
</tr>
</tbody>
</table>
<p>Table 4: Breakdown of information conversion strategies of tier-4 to tier-6 on both Test 1 and Test 2 with the best model after curriculumn learning on tier-6, without the final fine-tuning; ‘go-cardinal’ is the worst since the instance-knowledge learning is overfit on each single game. The ‘go-random’ approach is less effective as map size increases, while the ‘go-room’ method performs the best for both test sets.</p>
<p>Table 5 shows that there is a correlation between the most recently trained tier and performance on test data from that tier; we run ‘curric go-room’ but stop after the tier indicated, then subdivide test data per-tier. We see strongest performance on the main diagonal. This is reasonable because the six-room games of tier-4 use the same six rooms each time and so on; the extra rooms of tier-6 aren’t known during tier-4 training, and some decay of tier-4 rooms is observed as learning is rededicated to new rooms. Nevertheless, by fine-tuning on all tiers we get the best overall performance.</p>
<table>
<thead>
<tr>
<th></th>
<th>Tier-4</th>
<th>Tier-5</th>
<th>Tier-6</th>
<th>All</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tier-4</td>
<td>62</td>
<td>59</td>
<td>56</td>
<td>68</td>
</tr>
<tr>
<td>Tier-5</td>
<td>41</td>
<td>50</td>
<td>49</td>
<td>64</td>
</tr>
<tr>
<td>Tier-6</td>
<td>26</td>
<td>35</td>
<td>47</td>
<td>46</td>
</tr>
</tbody>
</table>
<p>Table 5: Recency effect of curriculum learning (using go-room) on Test 1; performance on tier-specific subsets is best on the last tier used for training. After fine-tuning on all tiers (the All column) we get the best overall performance.</p>
<h3>5.4 Improvement from Uncertain Exploration</h3>
<p>At inference time, we compare different methods of using learned policies. First, as a baseline, we set $\epsilon=0$ to use the learned policy without randomization; this is shown in Table 3 (curric). Then, we add $\epsilon=0.05$ to allow a small stochasticity. We consider sampling from the policy distribution as the third method, by treating Q-values as logits and sampling directly from them by the Gumbel-max sampler [Vieira, 2014]. To increase the sampling accuracy, we use temperature $T=0.01$ on logits to avoid random selection due to a flat distribution [Hinton et al., 2015]. Finally, we use the LinUCB method, as discussed in Section 3.3. Since raw scores in our games are either 0 or 1, the learned Q-values will fall in a small range around 1. To avoid large bounds affecting Q-values too much, we normalize bounds at each step for all actions according to the largest bound and use a coefficient of 0.2.</p>
<table>
<thead>
<tr>
<th>Tier</th>
<th>Test 1</th>
<th></th>
<th></th>
<th>Test 2</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>eps</td>
<td>sampl</td>
<td>UCB</td>
<td>eps</td>
<td>sampl</td>
<td>UCB</td>
</tr>
<tr>
<td>1</td>
<td>96$\pm$2</td>
<td>97$\pm$1</td>
<td>100$\pm$0</td>
<td>97$\pm$1</td>
<td>100$\pm$0</td>
<td>100$\pm$0</td>
</tr>
<tr>
<td>2</td>
<td>71$\pm$2</td>
<td>75$\pm$0</td>
<td>75$\pm$0</td>
<td>68$\pm$2</td>
<td>65$\pm$1</td>
<td>67$\pm$0</td>
</tr>
<tr>
<td>3</td>
<td>58$\pm$1</td>
<td>62$\pm$1</td>
<td>64$\pm$0</td>
<td>67$\pm$3</td>
<td>70$\pm$1</td>
<td>71$\pm$0</td>
</tr>
<tr>
<td>4</td>
<td>67$\pm$3</td>
<td>74$\pm$2</td>
<td>77$\pm$1</td>
<td>65$\pm$1</td>
<td>69$\pm$2</td>
<td>69$\pm$0</td>
</tr>
<tr>
<td>5</td>
<td>62$\pm$2</td>
<td>70$\pm$2</td>
<td>69$\pm$0</td>
<td>60$\pm$2</td>
<td>66$\pm$2</td>
<td>70$\pm$1</td>
</tr>
<tr>
<td>6</td>
<td>47$\pm$2</td>
<td>53$\pm$2</td>
<td>64$\pm$0</td>
<td>47$\pm$2</td>
<td>52$\pm$1</td>
<td>51$\pm$3</td>
</tr>
<tr>
<td>All</td>
<td>63$\pm$1</td>
<td>69$\pm$1</td>
<td>72$\pm$0</td>
<td>63$\pm$1</td>
<td>67$\pm$1</td>
<td>68$\pm$1</td>
</tr>
</tbody>
</table>
<p>Table 6: We compare the evaluation results of eps: using small randomization with $\epsilon=0.05$; sampl: sampling from policy distribution with temperature $T=0.01$; and UCB: using LinUCB. LinUCB shows the best results on both test sets. We report the percentage of scores with a confidence level of 0.95 over 10 episodes.</p>
<p>We show the evaluation results in Table 6 with a confidence level of 0.95, computed over 10 episodes of game playing for each games in test sets. Using $\epsilon=0.05$ to add stochasticity shows worse results than the greedy method with $\epsilon=0$ (Table 3), because randomly choosing a dangerous action could lead to direct failure in these games, as discussed in Section 3.3. Sampling from the policy distribution works much better than these two methods, but is still outperformed by LinUCB on most tiers and the final overall results.</p>
<h2>6 Related Work</h2>
<p>Many recent works [Narasimhan et al., 2015; He et al., 2016; Li et al., 2016; Ansari et al., 2018; Fulda et al., 2017; Côté et al., 2018; Kostka et al., 2017] on building agents of text-based games apply the DQN [Mnih et al., 2015] from playing video games or its variants. Different aspects of DQN have been presented, such as action reduction with language correlation [Fulda et al., 2017], a bounding method [Zahavy et al., 2018], the introduction of a knowledge graph [Ammanabrolu and Riedl, 2018], text understanding with dependency parsing [Yin and May, 2019] and an entity relation graph [Ammanabrolu and Riedl, 2018]. However, previous work is chiefly focused on learning to self-train on games and then do well on the same games, instead of playing unseen games. Yuan et al. [2018] work on generalization of agents on variants of a very simple coin-collecting game. The simplicity of their games enables them to use an LSTM-DQN method with a counting-based reward. Ammanabrolu and Riedl [2018] use a knowledge graph as a persistent memory to encode states, while we use a knowledge graph to make actions more informative.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we train agents to play a family of text-based games. Instead of repeatedly optimizing on a single game, we train agents to play familiar but unseen games. We show that curriculum learning helps the agent learn better. We convert instance knowledge into universal knowledge via map familiarization. We also show how the incorporation of bandit feedback to both training and evaluation phases leads the agent to explore more thoroughly and reach higher scores.</p>
<h2>References</h2>
<p>[Abbasi-yadkori et al., 2011] Yasin Abbasi-yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2312-2320. Curran Associates, Inc., 2011.
[Abe et al., 2003] Naoki Abe, Alan W. Biermann, and Philip M. Long. Reinforcement learning with immediate rewards and linear hypotheses. Algorithmica, 37:263-293, 2003.
[Ammanabrolu and Riedl, 2018] Prithviraj Ammanabrolu and Mark O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. CoRR, abs/1812.01628, 2018.
[Ansari et al., 2018] Ghulam Ahmed Ansari, Sagar J. P, Sarath Chandar, and Balaraman Ravindran. Language expansion in text-based games. CoRR, abs/1805.07274, 2018.
[Auer, 2003] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res., 3:397-422, March 2003.
[Bengio et al., 2009] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, pages 41-48, New York, NY, USA, 2009. ACM.
[Côté et al., 2018] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532, 2018.
[Fulda et al., 2017] Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock? affordance extraction via word embeddings. In Carles Sierra, editor, IJCAI, pages 1039-1045. ijcai.org, 2017.
[He et al., 2016] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630. Association for Computational Linguistics, 2016.
[Hinton et al., 2015] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015.
[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9:1735-80, 121997.
[Kingma and Ba, 2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR</p>
<p>2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
[Kostka et al., 2017] Bartosz Kostka, Jaroslaw Kwiecieli, Jakub Kowalski, and Pawel Rychlikowski. Text-based adventures of the golovin AI agent. In CIG, pages 181-188. IEEE, 2017.
[Lebling et al., 1979] Lebling, Blank, and Anderson. Special feature zork: A computerized fantasy simulation game. Computer, 12(4):51-59, April 1979.
[Li et al., 2016] Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep reinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1192-1202, Austin, Texas, November 2016. Association for Computational Linguistics.
[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529 EP -, 022015.
[Narasimhan et al., 2015] Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for textbased games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11. Association for Computational Linguistics, 2015.
[Vieira, 2014] Tim Vieira. Gumbel-max trick and weighted reservoir sampling, 2014.
[Yin and May, 2019] Xusen Yin and Jonathan May. Comprehensible context-driven text game playing. CoRR, abs/1905.02265, 2019.
[Yuan et al., 2018] Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew J. Hausknecht, and Adam Trischler. Counting to explore and generalize in text-based games. CoRR, abs/1806.11525, 2018.
[Zahavy et al., 2018] Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J. Mankowitz, and Shie Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada., pages 3566-3577, 2018.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ occasional stochasticity notwithstanding&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ This is itself still a simplification, as many text games allow open text generation and thus infinite action space. Our approach does not preclude abandoning this simplification, but the difficulty of the problem is sufficient to leave this for future work.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
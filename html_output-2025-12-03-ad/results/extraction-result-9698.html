<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9698 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9698</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9698</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-276409203</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.11221v3.pdf" target="_blank">PlanGenLLMs: A Modern Survey of LLM Planning Capabilities</a></p>
                <p><strong>Paper Abstract:</strong> LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9698.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9698.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model used as an automatic judge for plan quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper describes using an LLM to automatically assess the quality of LLM-generated plans as an alternative to human evaluation, noting benefits (speed, cost) and limitations (systematic biases and sensitivity to prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>LLM planning evaluation (plan generation; planning-focused and open-ended planning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Described generically: another LLM is prompted to assess generated plans' quality (instructions/prompts and context provided), but the paper does not give a specific prompt template, model name, or experiment configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Described generically: human evaluation is used when no automated verifier exists or for open-ended problems; specifics (number of annotators, expertise, instructions) are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges trades off human judgment nuance for speed/cost savings; the paper highlights degraded reliability due to position bias, length bias, self-consistency issues, and high sensitivity to prompt phrasing—leading to potentially unreliable assessments compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>This paper does not provide dataset-level case studies or concrete example judgments where an LLM judge diverged from humans; it cites external works (e.g., Zheng et al., 2023; Ye et al., 2024; Wei et al., 2024) as demonstrating these limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLM judges are faster and more cost-effective, making them valuable for large-scale evaluation; they are increasingly adopted and can be useful when human evaluation is infeasible. The paper points to literature that studies mitigation and reliability (Li et al., 2024a,b; Gu et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Evaluation section (paragraph discussing LLM-as-a-Judge) in PlanGenLLMs: A Modern Survey of LLM Planning Capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PlanGenLLMs: A Modern Survey of LLM Planning Capabilities', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9698.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9698.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Judge Biases</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Systematic biases observed in LLM-as-a-Judge evaluations (position/length/self-consistency/prompt-sensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The survey lists concrete failure modes of LLM judges: position bias, length bias, self-inconsistency, and sensitivity to prompts, which degrade evaluation fidelity relative to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation of generated plans and open-ended planning outputs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Not specified in detail; biases are reported as general phenomena affecting LLM-based judgment pipelines rather than from a single experimental setup in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Specific evaluation reliability is degraded: position bias can make judgement dependent on ordering rather than content; length bias can favor longer (or shorter) plans irrespective of quality; self-inconsistency leads to unstable ratings across runs; prompt-sensitivity makes scores contingent on prompt wording—together these reduce the trustworthiness of automatic judgments compared to human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>No worked examples in this survey; it references studies documenting these biases (Zheng et al., 2023; Ye et al., 2024; Wei et al., 2024) but does not reproduce their experimental cases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The survey notes that these shortcomings are active research topics and that methodological remedies (e.g., improved prompting, calibration, ensemble/jury methods, or verification pipelines) are being explored in cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Evaluation section (discussion of limitations of LLM-as-a-Judge) and citations in PlanGenLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PlanGenLLMs: A Modern Survey of LLM Planning Capabilities', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9698.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9698.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-eval necessity (open-ended/no-verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>When human evaluation is required instead of automated LLM judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper states human evaluation remains necessary in scenarios lacking verifiers or for open-ended/generative tasks because LLM judges may not capture subjective nuance or correctly validate outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended planning tasks, simulated environments without automated verifiers, and generative outputs (text, images, plans)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Not applicable — the paper contrasts LLM judges (generic use) with human evaluation rather than detailing a judge setup.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Used in cases where no automated verifier exists or for open-ended problems; the survey does not supply details about annotator counts, expertise, or protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>By replacing humans with LLM judges in these cases, one risks losing subjective human preference judgments, contextual understanding, and the ability to evaluate ambiguous or multi-modal good solutions; the survey implies that LLM judges may fail to reflect human preferences and nuanced correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>The paper does not present concrete conflicting judgments between humans and LLM judges on specific planning samples; it only motivates human evaluation for unverifiable or open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLM judges are pragmatic for large-scale datasets where human evaluation is prohibitively expensive; hybrid pipelines (human-in-the-loop for samples flagged as uncertain by LLM judges) are implicit mitigations discussed across cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Evaluation section (paragraph contrasting verifier-based, human, and LLM-as-a-Judge evaluations) in PlanGenLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PlanGenLLMs: A Modern Survey of LLM Planning Capabilities', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A survey on llm-as-a-judge <em>(Rating: 2)</em></li>
                <li>LLMs-as-judges: a comprehensive survey on llm-based evaluation methods <em>(Rating: 2)</em></li>
                <li>From generation to judgment: Opportunities and challenges of llm-as-a-judge <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9698",
    "paper_id": "paper-276409203",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge (general)",
            "name_full": "Large Language Model used as an automatic judge for plan quality",
            "brief_description": "The paper describes using an LLM to automatically assess the quality of LLM-generated plans as an alternative to human evaluation, noting benefits (speed, cost) and limitations (systematic biases and sensitivity to prompts).",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "LLM planning evaluation (plan generation; planning-focused and open-ended planning tasks)",
            "llm_judge_model": null,
            "llm_judge_setup": "Described generically: another LLM is prompted to assess generated plans' quality (instructions/prompts and context provided), but the paper does not give a specific prompt template, model name, or experiment configuration.",
            "human_evaluation_setup": "Described generically: human evaluation is used when no automated verifier exists or for open-ended problems; specifics (number of annotators, expertise, instructions) are not provided in this paper.",
            "agreement_metric": null,
            "losses_identified": "Using LLMs as judges trades off human judgment nuance for speed/cost savings; the paper highlights degraded reliability due to position bias, length bias, self-consistency issues, and high sensitivity to prompt phrasing—leading to potentially unreliable assessments compared to humans.",
            "examples_of_loss": "This paper does not provide dataset-level case studies or concrete example judgments where an LLM judge diverged from humans; it cites external works (e.g., Zheng et al., 2023; Ye et al., 2024; Wei et al., 2024) as demonstrating these limitations.",
            "counterexamples_or_caveats": "LLM judges are faster and more cost-effective, making them valuable for large-scale evaluation; they are increasingly adopted and can be useful when human evaluation is infeasible. The paper points to literature that studies mitigation and reliability (Li et al., 2024a,b; Gu et al., 2024).",
            "paper_reference": "Evaluation section (paragraph discussing LLM-as-a-Judge) in PlanGenLLMs: A Modern Survey of LLM Planning Capabilities",
            "uuid": "e9698.0",
            "source_info": {
                "paper_title": "PlanGenLLMs: A Modern Survey of LLM Planning Capabilities",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Judge Biases",
            "name_full": "Systematic biases observed in LLM-as-a-Judge evaluations (position/length/self-consistency/prompt-sensitivity)",
            "brief_description": "The survey lists concrete failure modes of LLM judges: position bias, length bias, self-inconsistency, and sensitivity to prompts, which degrade evaluation fidelity relative to humans.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "Evaluation of generated plans and open-ended planning outputs",
            "llm_judge_model": null,
            "llm_judge_setup": "Not specified in detail; biases are reported as general phenomena affecting LLM-based judgment pipelines rather than from a single experimental setup in this survey.",
            "human_evaluation_setup": null,
            "agreement_metric": null,
            "losses_identified": "Specific evaluation reliability is degraded: position bias can make judgement dependent on ordering rather than content; length bias can favor longer (or shorter) plans irrespective of quality; self-inconsistency leads to unstable ratings across runs; prompt-sensitivity makes scores contingent on prompt wording—together these reduce the trustworthiness of automatic judgments compared to human raters.",
            "examples_of_loss": "No worked examples in this survey; it references studies documenting these biases (Zheng et al., 2023; Ye et al., 2024; Wei et al., 2024) but does not reproduce their experimental cases.",
            "counterexamples_or_caveats": "The survey notes that these shortcomings are active research topics and that methodological remedies (e.g., improved prompting, calibration, ensemble/jury methods, or verification pipelines) are being explored in cited literature.",
            "paper_reference": "Evaluation section (discussion of limitations of LLM-as-a-Judge) and citations in PlanGenLLMs",
            "uuid": "e9698.1",
            "source_info": {
                "paper_title": "PlanGenLLMs: A Modern Survey of LLM Planning Capabilities",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Human-eval necessity (open-ended/no-verifier)",
            "name_full": "When human evaluation is required instead of automated LLM judges",
            "brief_description": "The paper states human evaluation remains necessary in scenarios lacking verifiers or for open-ended/generative tasks because LLM judges may not capture subjective nuance or correctly validate outcomes.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "Open-ended planning tasks, simulated environments without automated verifiers, and generative outputs (text, images, plans)",
            "llm_judge_model": null,
            "llm_judge_setup": "Not applicable — the paper contrasts LLM judges (generic use) with human evaluation rather than detailing a judge setup.",
            "human_evaluation_setup": "Used in cases where no automated verifier exists or for open-ended problems; the survey does not supply details about annotator counts, expertise, or protocols.",
            "agreement_metric": null,
            "losses_identified": "By replacing humans with LLM judges in these cases, one risks losing subjective human preference judgments, contextual understanding, and the ability to evaluate ambiguous or multi-modal good solutions; the survey implies that LLM judges may fail to reflect human preferences and nuanced correctness.",
            "examples_of_loss": "The paper does not present concrete conflicting judgments between humans and LLM judges on specific planning samples; it only motivates human evaluation for unverifiable or open-ended tasks.",
            "counterexamples_or_caveats": "LLM judges are pragmatic for large-scale datasets where human evaluation is prohibitively expensive; hybrid pipelines (human-in-the-loop for samples flagged as uncertain by LLM judges) are implicit mitigations discussed across cited works.",
            "paper_reference": "Evaluation section (paragraph contrasting verifier-based, human, and LLM-as-a-Judge evaluations) in PlanGenLLMs",
            "uuid": "e9698.2",
            "source_info": {
                "paper_title": "PlanGenLLMs: A Modern Survey of LLM Planning Capabilities",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A survey on llm-as-a-judge",
            "rating": 2,
            "sanitized_title": "a_survey_on_llmasajudge"
        },
        {
            "paper_title": "LLMs-as-judges: a comprehensive survey on llm-based evaluation methods",
            "rating": 2,
            "sanitized_title": "llmsasjudges_a_comprehensive_survey_on_llmbased_evaluation_methods"
        },
        {
            "paper_title": "From generation to judgment: Opportunities and challenges of llm-as-a-judge",
            "rating": 2,
            "sanitized_title": "from_generation_to_judgment_opportunities_and_challenges_of_llmasajudge"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        }
    ],
    "cost": 0.010893,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PlanGenLLMs: A Modern Survey of LLM Planning Capabilities
23 Jun 2025</p>
<p>Hui Wei huiwei2@ucmerced.edu 
Zihao Zhang zihao.zhang@emory.edu 
Shenghua He 
Tian Xia 
Shijia Pan 
University of California
Merced</p>
<p>Emory University § PAII Inc</p>
<p>Fei Liu fei.liu@emory.edu 
Zehui Chen 
Kuikun Liu 
Qiuchen Wang 
Wenwei Zhang 
Jiangning Liu 
Dahua Lin 
Kai Chen 
Zhenfang Chen 
Delin Chen 
Wenjun Sun 
Abd Rui 
Chuang Liu 
2025 Gan 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Mark Chen 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Reiichiro Nakano 
Xiang Deng 
Yu Gu 
Boyuan Zheng 
Shijie Chen 
Sam Stevens 
Boshi Wang 
Huan Sun 
Yu Su 
Gonzalo Gonzalez-Pumariega 
Wayne Chen 
Kushal Ke- Dia 
Sanjiban 2024 Choudhury 
Archiki Prasad 
Alexander Koller 
Mareike Hartmann 
Peter Clark 
Ashish Sabharwal 
Mohit Bansal 
Xavier Puig 
Kevin Ra 
Marko Boben 
Jiaman Li 
Tingwu Wang 
Sanja Fidler 
Antonio Torralba 
Virtualhome 
Alec Radford 
Jeffrey Wu 
Rewon Child 
David Luan 
Rafael Rafailov 
Archit Sharma 
Eric Mitchell 
Christo- Pher D Manning 
Stefano Ermon 
Chelsea Finn 
Colin Raffel 
Noam Shazeer 
Adam Roberts 
Katherine Lee 
Sharan Narang 
Michael Matena 
Yanqi Zhou 
Wei Li 
Peter J 2020 Liu 
Sundara Shreyas 
Vanya Raman 
Ifrah Cohen 
Eric Idrees 
Raymond Rosen 
Mooney 
Eric Cohen 
Rosen 
Allen Z Ren 
Anushri Dixit 
Alexandra Bodrova 
Sumeet Singh 
Stephen Tu 
Noah Brown 
Peng Xu 
Leila Takayama 
Fei Xia 
Jake Varley 
Jingqing Ruan 
Yihong Chen 
Bin Zhang 
Zhiwei Xu 
Tianpeng Bao 
Guoqing Du 
Shiwei Shi 
Hangyu Mao 
Ziyue Li 
Xingyu Zeng 
Rui Zhao 
Mohit Shridhar 
Jesse Thomason 
Daniel Gordon 
Yonatan Bisk 
Winson Han 
Roozbeh Mottaghi 
Luke Zettlemoyer 
Dieter 2020a Fox 
Alfred 
Xingdi Yuan 
Marc-Alexandre Côté 
Chan Hee Song 
Jiaman Wu 
Clayton Washington 
Brian M Sadler 
Wei-Lun Chao 
Zihao Wang 
Shaofei Cai 
Guanzhou Chen 
Anji Liu 
Xiaojian Ma 
Yitao Liang 
Andy Wong 
Jingyang Lin 
Jason Wei 
Xuezhi Wang 
Dale Schuurmans 
Maarten Bosma 
Ed Chi 
V Quoc 
Denny Le 
Zhou 
Xixi Wu 
Yifei Shen 
Caihua Shan 
Kaitao Song 
Siwei Wang 
Bohang Zhang 
Jiarui Feng 
Hong Cheng 
Wei Chen 
Yun Xiong 
Yue Wu 
Xuan Tang 
Tom M Mitchell 
Yuanzhi Li 
Smartplay 
Zhenyu Wu 
Ziwei Wang 
Xiuwei Xu 
Jiwen Lu 
Jian Xie 
Kai Zhang 
Jiangjie Chen 
Tinghui Zhu 
Renze Lou 
Yuandong Tian 
Yanghua Xiao 
Ruihan Yang 
Yikai Zhang 
Siyu Yuan 
Aili Chen 
Kyle Richardson 
De- Qing Yang 
Ryan Yang 
Tom Silver 
Aidan Curtis 
Tomas Lozano- Perez 
University of California
Merced</p>
<p>Emory University § PAII Inc</p>
<p>Leslie Pack 
Kaelbling 2022 Pg3 
Shunyu Yao 
Dian Yu 
Jeffrey Zhao 
Izhak Shafran 
Tom Griffiths 
Yuan Cao 
Karthik Narasimhan 
Jiayi Ye 
Yanbo Wang 
Yue Huang 
Dongping Chen 
Qihui Zhang 
Nuno Moniz 
Tian Gao 
Werner Geyer 
Pin-Chao Huang 
Yu Chen </p>
<p>Jiangjie Chen
Rong YeSiyu Yuan, Bodhisattwa Prasad</p>
<p>Advances in Neural Information Processing Systems
36, Xiang Deng, Yu Gu, Boyuan Zheng, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su28091-28114, 2024Shijie Chen</p>
<p>Advances in Neural Information Processing Systems
36</p>
<p>Ifrah Idrees
Stefanie Tellex
David Paulius2022</p>
<p>PlanGenLLMs: A Modern Survey of LLM Planning Capabilities
23 Jun 2025105CD8AC3D06D27AE562110336305C3FarXiv:2502.11221v3[cs.AI]
LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state.A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying.However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks.There is also a lack of clear and consistent evaluation criteria.Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap.It builds on foundational work byKartam and Wilkins (1990)and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency.For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses.Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.</p>
<p>Introduction</p>
<p>Planning, which involves generating a sequence of actions to reach a desired goal state (Newell et al., 1958;Kartam and Wilkins, 1990), is fundamental to human intelligence.For example, when planning a trip to San Francisco, one would search for flights, book tickets based on budget and schedule, arrange local transportation to the airport, and consider alternatives in case of cancellations.These planning tasks require complex reasoning, world knowledge, decision-making, and the ability to adapt, making them a significant challenge for humans.To date, there has been a growing focus on developing LLM planners to automate these complex tasks.</p>
<p>A comprehensive survey of LLM planners would significantly propel research in this field.Prior studies have explored planning methods and evaluation benchmarks (Huang et al., 2024c;Li et al., 2024d).Huang et al. (2024c) categorized planning methods into decomposition, plan selection, external modules, reflection, and memory, while Li et al. (2024d) reviewed evaluation benchmarks across various domains.However, many of these benchmarks and systems are tailored to specific problems, making it hard to compare LLM planners across domains or determine the best planner for new tasks.Further, there is a lack of clear and consistent evaluation criteria.We believe this gap may hinder the development of advanced LLM planners.</p>
<p>Our survey builds on the foundational work of Kartam and Wilkins (1990) to address key evaluation criteria for LLM planners.The original paper highlighted challenges in evaluating early AI planning systems, which relied on heuristics and were confined to research labs.The initial criteria were categorized into performance, representation, and communication issues.With more advanced LLM planning, we reexamine this critical framework and focus on six key evaluation criteria: completeness, executability, optimality, representation, generalization, and efficiency.For each criterion, we provide a thorough analysis of representative works, highlighting their strengths and weaknesses.</p>
<p>We contribute to the literature by addressing key research questions in LLM planning: What foundational capabilities distinguish them from earlier AI planners?How can we comprehensively measure their performance?We examine the datasets, evaluation methods, and metrics available to the commu-Figure 1: Taxonomy of LLM Planning techniques.For those new to LLM planning, we recommend a thorough read, while experts can focus on specific sections.Each section offers clear definitions, relevant works, and includes links to tables in the Appendix.We will dive into the details in the following sections.</p>
<p>2 LLM Planning Foundations (Tables 1-3) We begin by exploring LLM planning foundations, covering widely-used paradigms to provide background for readers unfamiliar with the field.It is broken down into four parts.Task Decomposition Task decomposition breaks down abstract goals into specific, manageable subgoals.It helps mitigate errors by enabling verification at each step and makes LLM reasoning more tractable by narrowing the knowledge space.</p>
<p>Task decomposition can be performed sequentially, in parallel, or asynchronously.Specifically, sequential decomposition (Wang et al., 2023b;Singh et al., 2023;Sun et al., 2024;Wu et al., 2024) requires that the precondition of the subsequent subgoal is the effect of the preceding subgoal.In contrast, parallel decomposition (Yang et al., 2024) involves subgoals that share the same precondition and effect, where achieving the final goal requires completing only one of these subgoals.Asynchronous decomposition (Lin et al., 2024) involves parallelizing subgoals as well.However, these subgoals in distinct branches have unique preconditions and effects.Asynchronous decomposition requires the completion of all subgoals to achieve the overall goal.</p>
<p>Moreover, task decomposition can be performed recursively, applying any of the above three approaches at each step.For example, Prasad et al. (2023) recursively break down the goal until each subgoal can be easily executed in the environment.LLM + Classical Planner Studies (Valmeekam et al., 2023b;Kambhampati, 2024;Kambhampati et al., 2024) show that LLMs struggle with independent planning.Classical planners, such as Fast Downward (Helmert, 2006), ensure correct plans but depend on experts to translate user queries into formal representations, limiting scalability.A hybrid approach integrating LLMs with classical planners combines the world knowledge of LLMs with the precision and reliability of classical methods, addressing their individual limitations.</p>
<p>When integrated with classical planners, LLMs translate natural language problems into formal representations or generate initial plans.For example, LLM+P (Liu et al., 2023a), LLM-DP (Dagan et al., 2023), andGuan et al. (2023) use LLMs to convert planning problems into PDDL (McDermott et al., 1998), solved by Fast Downward or BFS(f) (Lipovetzky et al., 2014). Valmeekam et al. (2023b) employs LLMs to generate an initial plan, guiding the LPG planner (Gerevini et al., 2002), which iteratively refines it until a correct solution is found.Search Algorithm Search algorithms, including Breadth-First Search, Depth-First Search (Yao et al., 2024;Katz et al., 2024), Monte Carlo Tree Search (Hao et al., 2023;Zhou et al., 2023a;Zhao et al., 2024;Shi et al., 2025), andGreedy Best-First Search (Koh et al., 2024;Hirsch et al., 2024), have been applied to improve LLM-based planning.These algorithms treat planning as a search problem, using search policy to guide the exploration of various possibilities.Search algorithms excel in planning problems by offering systematic exploration, optimality guarantees, and formal verification without requiring extensive domain knowledge, though they may be computationally intensive compared to more specialized methods like task decomposition.</p>
<p>All search algorithms consist of four core components: (1) Search Policy determines node exploration order, which are defined by the underlying search algorithm and are independent of LLMs.</p>
<p>(2) Expansion generates possible actions from a state, often using LLMs to propose actions based on user instructions and current environment.(3) World Models define state transitions based on action preconditions and effects, using LLMs (Hao et al., 2023), classical planners (Hirsch et al., 2024), or external environment simulators (Zhou et al., 2023a;Zhao et al., 2024;Koh et al., 2024).( 4) Evaluation assesses state progress toward the goal via scores computed by predefined functions (Katz et al., 2024), LLM/LVM ratings (Yao et al., 2024;Hao et al., 2023;Zhou et al., 2023a), log-likelihood scores (Hirsch et al., 2024), voting (Yao et al., 2024), self-consistency scores (Zhou et al., 2023a) or reward models (Chen et al., 2025).Fine-tuning Current LLMs are not specifically trained for agentic tasks like planning, and promptbased methods, which do not update model parameters, cannot fundamentally improve performance in these areas (Chen et al., 2023a;Wang et al., 2024).Fine-tuning, either focused on planningspecific tasks or broader agentic capabilities, en-hances planning correctness by directly updating LLM parameters.</p>
<p>Planning-specific fine-tuning involves training a pretrained model on planning-focused tasks (e.g., Blocksworld or ALFWorld (Shridhar et al., 2020b)), to improve planning performance.For example, Jansen (2020) and Chalvatzaki et al. (2023) fine-tuned GPT-2 (Radford et al., 2019) on ALF-World, demonstrating its effectiveness in robotics planning.</p>
<p>Generalized agentic fine-tuning optimizes models using datasets that include both general tasks (e.g., question answering) and diverse agentic tasks (e.g., reasoning, planning, and tool use).This approach is motivated by two key insights: (1) focusing too narrowly on planning may degrade general capabilities (Chen et al., 2024b), and (2) agentic tasks share overlapping capabilities.For instance, reasoning and tool-use tasks often involve planning components.Thus, fine-tuning on a broader set of agentic tasks can simultaneously enhance planning performance and other interrelated capabilities, like reasoning, which are integral to planning.Moreover, standardizing trajectory formats from different tasks (Zhang et al., 2024;Chen et al., 2024b), as well as incorporating unsuccessful reasoning or planning trajectories (Wang et al., 2024;Chen et al., 2024b;Song et al., 2024) can further enhance learning and performance.</p>
<p>3 Criterion I: Completeness (Table 4)</p>
<p>The completeness of planning has two key aspects:</p>
<p>(1) if a valid plan exists, the model should generate it correctly, and (2) if no feasible plan is possible, the model should recognize this and refrain from generating an incorrect or arbitrary plan.</p>
<p>A plan is correct if it achieves the goal within a fixed budget while avoiding excessive complexity and infinite loops.To ensure correctness, the LLM must work with classical sound and complete solvers (Guan et al., 2023;Hao et al., 2024a).Also, the LLM has to accurately translate the domain and problem into the specific format (e.g., PDDL), required by these solvers (Guan et al., 2023).</p>
<p>In terms of identifying unsolvable planning problems, those with inherently unachievable goals, even top LLMs (e.g., GPT-4 (Achiam et al., 2023)) and Large Reasoning Models (e.g., OpenAI O1 (Jaech et al., 2024)) struggle due to hallucination issues (Aghzal et al., 2023;Valmeekam et al., 2024;Katz et al., 2024).</p>
<p>4 Criterion II: Executability (Tables 5-6)</p>
<p>Executability checks if a plan can be carried out in a given environment while meeting all constraints.A executable plan must use only allowed actions and recognizable objects.Beyond basic precondition and postcondition rules, planners must consider extra constraints, such as avoiding sugar when baking a cake for diabetics (Yuan et al., 2023).Importantly, executability and correctness are orthogonal: an executable plan isn't necessarily correct, since it might be grounded and follow all constraints but still fail to reach the goal; likewise, a correct plan isn't always executable since it may only include high-level steps that can't be executed in a specific environment.Real-world applications typically require plans that are both correct and executable, especially when the executors are not humans (e.g., robots and computers).</p>
<p>To ensure plans are executable, researchers have proposed several approaches, including Object Grounding, Action Grounding, Closed-Loop Systems, and Sample-then-Filter.Object Grounding Object grounding ensures the LLM planner uses objects available in the current environment when generating plans.For example, if a stove is available but a microwave is not, the planner should choose the stove to heat a pancake and disregard the unavailable microwave in its plan.The simplest way to do this is by feeding observed or available objects into the planner via prompts (Huang et al., 2022b;Song et al., 2023;Lin et al., 2023a;Singh et al., 2023) or neural embeddings (Sharma et al., 2021;Ahn et al., 2022).In partially observed environments, where some object information are uncertain (e.g., needing to clean a cup that could be in a cabinet, drawer, or fridge), the planner can generate multiple possible plans, one for each scenario, and select the first feasible one (Prasad et al., 2023;Dagan et al., 2023;Zhao et al., 2024).Sun et al. ( 2024) takes a different approach, first generating a plan with placeholders for objects, then filling in the blanks with observed objects during execution.Action Grounding Action grounding ensures all actions in a plan can actually be executed in the current environment.E.g., the planner should avoid including actions such as 'draw a star' if the robot arm cannot perform such a complex operation.Instead, the task should be decomposed into simpler, supported actions, such as multiple steps of 'draw a line'.Like object grounding, the simplest way is to explicitly list all admissible actions in LLMs' inputs (Singh et al., 2023).If a step goes beyond the executor's capabilities (e.g., combining multiple allowed actions into one), the LLM planner should be reprompted to break it down until every step is executable (Prasad et al., 2023).</p>
<p>Hierarchical Planning is another common method for grounding actions in LLM planning (Huang et al., 2022a;Raman et al., 2022;Song et al., 2023;Hazra et al., 2024;Bhat et al., 2024).It starts with high-level steps and then translates each one into a sequence of executable actions.This can be done in two ways: either generating all high-level steps first and then refining them into actions or translating each step as it's generated.If an action isn't exactly admissible, the closest valid action is retrieved instead (Huang et al., 2022a;Raman et al., 2022).</p>
<p>Sample-then-Filter Since LLMs alone can't guarantee plans meet all constraints, this approach first generates multiple plans and then verifies them, selecting only those that pass all checks.Yuan et al. (2023) ranks InstructGPT-generated plans using cosine similarity with task embeddings and selects the most similar one.Brahman et al. (2024) applies a verifier-guided beam search, keeping the top-K plans based on correctness and constraint adherence at each step.Curtis et al. (2024) generates Pythonic plans with parameter ranges, tests them with a simulator or classifier, and prompts the LLM to revise if constraints are still violated.</p>
<p>Closed-Loop Systems A closed-loop system in LLM planning means the model adapts its plan based on feedback from executors (Prasad et al., 2023;Yang et al., 2024), simulators (Bhat et al., 2024), validators (Zhou et al., 2024;Silver et al., 2024), other LLMs (Wang et al., 2023b;Zhou et al., 2023a), or even humans (Huang et al., 2022b), when the initial plan are inexecutable.It reprompts the LLM planner to replan until the plan is fully executable.Unlike open-loop systems (Huang et al., 2022a), which lack feedback, closed-loop planning helps reduce hallucinations and enables LLMs to handle complex, long-horizon, and dynamic environments (Wang et al., 2023b).</p>
<p>Closed-loop systems fall into two types: implicit and explicit (Sun et al., 2024).Implicit systems only fix the failed action (Raman et al., 2022;Singh et al., 2023;Zhou et al., 2024;Prasad et al., 2023;Yang et al., 2024), while explicit systems regenerate the entire plan (Sun et al., 2024).Though explicit systems require more computation, they prevent errors from compounding across steps.</p>
<p>5 Criterion III: Optimality (Table 7)</p>
<p>Optimality means achieving the goal state through the best possible plan.It poses a greater challenge than standard planning, which only requires reaching the goal state.Researchers have proposed two paradigms for achieving the optimal plans: LLM + Optimizer and A * search-based methods.LLM + Optimizer It combines the LLM, which turns user requests into symbolic optimization problems, with an optimizer that solves them and finds the best solution (Ju et al., 2024;Hao et al., 2024b).For example, TTG (Ju et al., 2024) uses the LLM to convert travel planning requests of minimum total costs into Mixed Integer Linear Programming problems, then runs an optimizer such as SCIP (Bestuzheva et al., 2021) to provide the optimal plan.LLM + Optimizers ensure optimal solutions by leveraging LLMs to formulate constrained optimization problems and classical optimizers to solve them.In contrast to classical planners, which typically rely on search algorithms, heuristics, and logical deduction and may not guarantee optimality (Russell and Norvig, 2016), optimizers, often based on gradient-based methods (e.g., Newton's methods), can guarantee optimal solutions (Boyd and Vandenberghe, 2004).A<em> Search-Based Methods A</em> search finds the lowest-cost optimal solution, particularly when using admissible heuristics that do not overestimate the actual cost to the goal.This makes it a natural choice for LLM-based planners to achieve optimality.ToolChain<em> (Zhuang et al., 2023) combines A</em> tree search with an LLM, which suggests next steps and estimates heuristic scores, to create plans with the fewest tool API calls.SayCanPay (Hazra et al., 2024) uses A<em> search with LLMs to generate the shortest possible plans.Beyond A</em> (Lehnert et al., 2024) trains a Transformer model, Searchformer, to mimic A<em> search paths for complex tasks like Maze navigation and Sokoban puzzles, optimizing for the fewest steps.Besides A</em> search, other search algorithms (e.g., DFS and MCTS) could also be used to find optimal solutions, although without guarantees.</p>
<p>6 Criterion IV: Representation (Tab.8-9)</p>
<p>In LLM planning, representation refers to how inputs and outputs are formatted.Inputs include do-mains (predicates and actions), problems (initial and goal states), and environmental observations, while outputs are the generated plans.Effective representation enhances problem comprehension and execution efficiency, especially given LLMs' sensitivity to prompts.We discuss this in two contexts: LLM-as-a-Translator and LLM-as-a-Planner.</p>
<p>LLM-as-a-Translator LLM-as-a-Translator converts between natural language (NL) and formal planning languages (e.g.PDDL), making classic planners more accessible to non-experts.By converting natural language tasks into formal representations and translating the resulting plans back into NL, LLMs reduce ambiguity, minimize hallucinations, and enable external validation, improving both usability and reliability in planning systems (Xie et al., 2023;Zhou et al., 2024;Sun et al., 2024;Silver et al., 2024).</p>
<p>Recent work has used LLMs to translate natural language descriptions into PDDL (Liu et al., 2023a;Guan et al., 2023;Xie et al., 2023;Dagan et al., 2023;Zhou et al., 2024;Tantakoun et al., 2025), LTL (Pan et al., 2023), and STL (Chen et al., 2024a).To ensure reliability, translations should be tested on development or external datasets like Planetarium (Zuo et al., 2024).If there are syntax or semantic errors, validators (e.g.VAL (Howey et al., 2004)) or human experts can provide feedback for the LLM to fix them.</p>
<p>LLM-as-a-Planner When LLMs act as standalone planners without classical planners or optimizers, various methods help encode environmental information, domains, and plans beyond just natural language.Environment and domain details have been represented using tables (Lin et al., 2023a), condensed symbols (Hu et al., 2024), Pythonic code (Aghzal et al., 2023;Singh et al., 2023;Sun et al., 2024), neural embeddings (Li et al., 2022;Ahn et al., 2022), and graphs (Lin et al., 2024;Wu et al., 2024).For generated plans, Pythonic code is a common alternative to natural language (Singh et al., 2023;Silver et al., 2024).</p>
<p>7 Criterion V: Generalization (Table 10) Generalization refers to LLM planners' ability to apply learned strategies to new, more complex outof-domain scenarios beyond its training environment, which can be enhanced through three key approaches: fine-tuning (described previously in Section 2), generalized planning, and skill storage.Given the diverse user queries in the real-world deployments, ensuring LLM planners' generalizability is important alongside other performance.</p>
<p>Generalized Planning Generalized planning extracts common patterns from a limited set of training solutions (i.e., plans) to solve unseen tasks within the same domain, which may be larger and more complex than the training tasks (Srivastava et al., 2011).For example, in the Delivery dataset (Yang et al., 2022), models trained on small-scale deliveries (9-17 locations) can generalize to larger ones (70-100 locations) using the same core strategy.Silver et al. ( 2024) approached this by prompting LLMs to summarize the domain and generate a minimal, generalizable Python-based plan.</p>
<p>Skill Storage Skill storage focuses on learning and reusing previously acquired skills to tackle new problems.E.g., Wang et al. (2023a) introduced a skill library that stores successfully executed skills (e.g., Combat Zombie).These skills are abstracted and generalized for reuse in similar situations (e.g., fighting spiders involves similar actions to fighting zombies).When encountering an unseen task, the LLM planning system retrieves relevant learned skills based on the task and current states, then applies them to generate an effective solution.</p>
<p>8 Criterion VI: Efficiency (Table 11)</p>
<p>Efficiency in LLM planning means reducing computational and monetary costs by decreasing LLM calls, world model interactions, input and output lengths, and model sizes.This is crucial especially developing planners based on commercial LLMs.</p>
<p>Reduced LLM and World Model Calls</p>
<p>To reduce the number of LLM and world model calls, several tricks are used: (1) generating the entire plan in one shot instead of step-by-step to reduce redundant prompts (Hu et al., 2023b;Sun et al., 2024;Gonzalez-Pumariega et al., 2024); (2) checking feasibility by world models only at the end of each subgoal, not after every action (Sun et al., 2024;Gonzalez-Pumariega et al., 2024); (3) merging plans with the same prefix actions or subgoals to avoid duplicate world model checks when sampling multiple plans (Hu et al., 2023b); and (4) in tree search-based methods, querying the LLM once to generate a successor function and a goal-check function.The successor function lists all possible actions and resulting states based on the current state, while the goal-check function determines if a state meets the goal.This approach avoids repeated LLM and world model calls at each node (Katz et al., 2024).</p>
<p>Shorter Inputs and Outputs Reducing input and output length includes decreasing prompt and plan tokens and minimizing actions in the final plan to alleviate the load on executors.For spatial reasoning and planning, (Hu et al., 2024) introduces Chain-of-Symbols (CoS), a compact symbolic representation that replaces natural language descriptions in CoT (Wei et al., 2022)</p>
<p>Evaluation</p>
<p>Datasets LLM planning evaluation is conducted on two types of datasets: planning-focused datasets and downstream-task datasets.</p>
<p>Planning-focused datasets primarily assess planning abilities.The most common scenarios include (1) Embodied environments, (2) Task scheduling , (3) Games, and (4) Task decomposition (Li et al., 2024d).Figure 1 presents commonly used planning datasets; readers can refer to Li et al. (2024dLi et al. ( , 2025) ) for further details.</p>
<p>While most of the datasets mentioned above assess whether the generated plans are correct, some specifically target key performance criteria in LLM planning.For grounding, Open Grounded Planning (Guo et al., 2024) and Embodied Agent Interface (Li et al., 2024f) evaluate performance in embodied environments, while CoScript (Yuan et al., 2023), TravelPlanner (Xie et al., 2023), and PPNL (Aghzal et al., 2023) focus on planning problems with constraints.For representation, Planetarium (Zuo et al., 2024) assesses LLMs' ability to translate natural language into PDDL.For optimality, Lin et al. (2024) and Gonzalez-Pumariega et al. ( 2025) introduce tasks requiring optimal plans us-ing asynchronous actions.PPNL (Aghzal et al., 2023) can also evaluate a planner's ability to identify unachievable goals (i.e., completeness).</p>
<p>Planning abilities can also be evaluated through downstream tasks, where planning is integral to task completion, and stronger planning skills enhance overall performance.Downstream tasks can be categorized as follows: (1) Agentic tasks, including reasoning-oriented tasks, tool-use-oriented tasks, programming tasks, and web tasks (Yu et al., 2018;Cobbe et al., 2021;Saparov and He, 2022;Zhou et al., 2023b;Deng et al., 2023;Liu et al., 2023b;Li et al., 2023;Xu et al., 2023;Jimenez et al., 2023;Ruan et al., 2023;Bairi et al., 2023;Li et al., 2024e), (2) Generation tasks, including video (Lin et al., 2023b), image (Zala et al., 2023) and text generation (Moryossef et al., 2019).Please refer to Figure 1 for example datasets.</p>
<p>Methods The most common approach to evaluating LLM planning is to test it in a simulated environment and validate the generated plans using either an internal verifier provided by the environment or external verifier (e.g., VAL (Howey et al., 2004)) to ensure they achieve the intended goal.When ground-truth plans are available, LLMgenerated plans can also be compared against these reference plans (Zheng et al., 2024).</p>
<p>The second evaluation method is human evaluation, typically used in the following cases: (1) No available verifier: when certain simulated environments (e.g., VirtualHome) or real-world scenarios (e.g., using a mobile manipulator) lack automated verification;</p>
<p>(2) Open-ended problems: tasks with ambiguous instructions or generative outputs (e.g., text or images) where multiple valid solutions may differ from the ground truth.</p>
<p>The final evaluation method, LLM-as-a-Judge, uses another LLM to automatically assess the quality of generated plans in the cases mentioned above.This approach has been increasingly adopted in recent LLM planning research (Guo et al., 2024;O'Donoghue et al., 2023).Compared to human evaluation, LLM judges are faster and more costeffective, making them especially valuable for evaluating large datasets.However, this method has limitations, such as position bias, length bias, selfinconsistency, and sensitivity to prompts (Zheng et al., 2023;Ye et al., 2024;Wei et al., 2024).Addressing these issues is crucial to ensure reliable assessments.For more details on LLM-as-a-Judge, please see Li et al. (2024a,b); Gu et al. (2024).</p>
<p>Metrics</p>
<p>Figure 1 summarizes commonly used evaluation metrics for planning-focused tasks, along with representative works.Performance criteria are measured using specific metrics: (1) Completeness: success rate and goal condition recall measure whether the generated plan reaches final or stepwise goals, while classification metrics (e.g., true negative rate, false negative rate, and unreachable accuracy) assess the planner's ability to identify unachievable tasks.When ground-truth plans are available, the exact match score is used.(2) Executability: executability rate evaluates whether the plan can be executed in the environment, while constraint pass rate checks if constraints are met.(3) Optimality: measured by the optimality rate (i.e., the percentage of optimally solved tasks).( 4) Efficiency: common metrics include inference time, input and output token counts, number of plan steps, and model size.( 5) Representation: the number of parseable problems indicates correct translations.( 6) Generalization: all these metrics can also be applied to unseen scenarios to assess generalization.See Figure 1 for definitions of individual metrics and representative works.</p>
<p>Discussion</p>
<p>In this section, we discuss the limitations in current LLM planning research studies, and suggest future directions for improvement and more comprehensive evaluations of LLM planning performance.</p>
<p>Datasets and Baselines</p>
<p>The current evaluation of LLM planning has its limitations, primarily because studies often rely on limited datasets and baselines.This makes it tough to fairly and comprehensively compare different methods.Most studies only use a few datasets from a single domain and difficulty level, and they do not evaluate all the six performance criteria.Inconsistent dataset choices make direct comparisons difficult.On top of that, many studies only compare against basic baselines such as CoT or ReAct, which does not help in comparing more advanced approaches.To fix this, a public, standardized leaderboard should be set up that covers all performance criteria, uses consistent evaluation metrics, includes a variety of baseline and advanced methods, and utilizes diverse datasets spanning multiple domains and difficulty levels.Another useful direction would be to create multilingual planning datasets and assess LLM performance across different languages.</p>
<p>Representation</p>
<p>LLMs are highly sensitive to prompts (Sclar et al., 2024;Razavi et al., 2025), but most research relies on natural language without comparing them to alternative formats, such as PDDL or Python, for describing domains and problems.Some studies (Singh et al., 2023;Aghzal et al., 2024) suggest that using Python to represent planning problems can improve performance, but automatically translating natural language problem descriptions into Python remains challenging, particularly for non-experts.If LLMs are to handle this translation effectively, additional datasets and evaluations are needed to assess their performance.Furthermore, little research has been conducted on how different prompt templates impact LLM planning performance, or on the best output formats for representing plans.Lastly, most fine-tuning methods rely on natural language data without exploring other formats, such as symbolic representations.Filling these gaps requires building benchmarks like Planetarium (Zuo et al., 2024) and carefully choosing representation formats in experiments.</p>
<p>Hallucination LLMs often experience hallucinations (Huang et al., 2023), which present two major challenges in planning.First, they might struggle to assess if a plan is achievable given a specific problem description (Aghzal et al., 2023;Kambhampati, 2024).Second, they can generate inadmissible actions and non-existent objects, requiring translation or expert intervention to correct them (Huang et al., 2022a;Raman et al., 2022).This increases the cost of planning systems.Further research is needed to understand the root causes and improve LLMs' ability to accurately identify unachievable plans.Evaluating the impact of these hallucinations remains an important research direction.</p>
<p>Human Preference Alignment</p>
<p>There is a gap in understanding whether system generated plans align with human preferences.It is crucial for openended problems where humans execute the plans.Ensuring alignment with human preferences is vital for safety, feasibility, and usability, particularly in personalized planning tasks such as calendar and travel planning.Additionally, Aghzal et al. (2024) found that LLM planners often fail to achieve optimality in path planning, frequently producing unnecessarily long plans.This may stem from inherent length bias in LLMs, which tend to generate longer sequences.Alignment techniques such as RLHF (Ouyang et al., 2022) and DPO (Rafailov et al., 2024) may help alleviate this issue, as hu-mans generally prefer shorter plans for their efficiency, simplicity, and cognitive ease.Further investigation is needed to better align LLM planners with human preferences.Cost Effectiveness Current methods, particularly task decomposition and search-based approaches, often consume a large number of tokens due to lengthy prompts and repeated LLM queries.While heuristic search is considered more efficient than task decomposition, it still requires substantial repeated prompting.To improve cost-effectiveness, we may summarize problem descriptions and enhance heuristic evaluations, e.g., by improving LLM uncertainty estimation (Huang et al., 2024a) and verification (Li et al., 2024c).These improvements would help reduce prompt length and enable the early stopping of unpromising partial plans.Multi-Agent Planning Most existing research focuses on single-agent planning, where only one agent performs a task.Multi-agent planning (Konolige and Nilsson, 1980;Torreno et al., 2017) is more challenging, as it involves multiple agents (e.g., robots) working together or competing in parallel.Despite its complexity, multi-agent planning has received limited attention in AI planning research.It often requires coordinating multiple agents in collaborative or competitive environments where they operate simultaneously.The major challenge lies in developing effective communication protocols and cooperation strategies while generating viable plans for their collective actions.</p>
<p>Reasoning, Tool Use, and Memory</p>
<p>There is often limited discussion on how other components of LLM agents, such as reasoning, tool use, and memory, affect planning performance.In particular, when LLMs are combined with classical planners or optimizers, it is crucial that the LLM accurately translates the planning problem into the appropriate domain representation to ensure correct plan generation.Currently, these approaches rely on human-selected planners and optimizers.Treating them as tools that LLMs can autonomously choose from could be an exciting prospect.This also raises the question of whether LLMs can effectively select the best tool for a given planning task.Future research should look into enhancing these agentic capabilities in LLM-based planning.</p>
<p>Conclusion</p>
<p>In our survey, we explore the landscape of modern LLM planners, proposing key performance crite-ria and discussing evaluation challenges.Our proposed criteria offer a structured approach to assess LLM planners across diverse domains.By systematically analyzing existing systems, datasets, and evaluation strategies, we aim to provide a foundation for future research in this space.We encourage researchers to build on our findings to create robust, highly adaptable, and efficient LLM planners.</p>
<p>Limitations</p>
<p>This work primarily focuses on commonly studied domains involving single-agent scenarios, such as robotics, household tasks, and computer-based tasks.We acknowledge that LLM planning is also applied in other areas, including the natural sciences (O'Donoghue et al., 2023;Liu et al., 2024), the Internet of Things (Cui et al., 2024), and multiagent scenarios.However, these studies follow similar methodologies and evaluations, suggesting our survey's comprehensiveness.We focus on six commonly used performance criteria and exclude others, such as security and personalization, due to limited research in these areas.Instead, we discuss them in our future directions section.</p>
<p>trajectories.Lehnert et al. (2024) uses search dynamic bootstrapping to iteratively fine-tune a LLM, replacing training cases with solutions with less tokens and equal optimality.To minimize actions, Dagan et al. (2023) and Wang et al. (2023b) use action selectors based on predefined rules or trained models to find the shortest successful plan.Smaller Model Sizes Shrinking the model size can reduce the computational burden, accelerating training and inference while lowering costs.To train a smaller planning model,Brahman et al. (2024) usesGPT-3 (Brown et al., 2020)  as the teacher and T5(Raffel et al., 2020)  as the student, distilling the teacher's planning capabilities into the more compact student model.</p>
<p>Table 1 :
1
Summary of Foundations in LLM Planning (Section 2) SelfGoal achieves a 94% success rate in dynamic multi-agent environments, outperforming baselines (e.g., ReAct, ADAPT) by dynamically adjusting subgoal guidance.It significantly improves task performance, especially in cooperative and competitive tasks.2. SelfGoal enhances correctness and adaptability in planning by dynamically adjusting subgoals and actions based on ongoing task performance and feedback.
Method NameDatasetEvaluation MetricMethodsMajor ContributionDEPSAlfworldSuccess Rate, Average1. LLM generates PDDL goal from task1.LLM-DP achieves 96% success in Alfworld,(Wang et al., 2023b)Episode Lengthdescriptionoutperforming the ReAct baseline (53%) with2. Sampling world beliefs and using sym-fewer actions.bolic planning2.Enhances correctness in LLM planning by dy-3. Plan generator (BFS(f)) for generatingnamically adjusting plans in response to environ-plansmental feedback and sub-goal feasibility.ProgPromptVirtualHome,Success Rate, Goal1. Pythonic program generation for task1. ProgPrompt significantly outperforms base-(Singh et al., 2023)Real-WorldConditionsRecall,planningline methods by using programmatic LLMRobot TasksExecutability2. Use of natural language comments andprompts to generate executable task plans,assertions for feedbackachieving up to 1.00 SR and 1.00 Exec in var-3. Integration with real-time environmentious VirtualHome tasks. It also adapts well tostate feedback during task executionreal-world robot tasks, with a Plan SR of 1 inmost cases.2. Improves correctness by incorporating envi-ronmental state feedback directly into the plan-ning process.AdaplannerALFWorld andSuccess RateClosed-loop approach allowing LLM agent1. Uses code-style LLM prompt structure and(Sun et al., 2024)MiniWoB++to refine self-generated plan adaptivelyskill discovery mechanism. Achieves 91.79%success rate on ALFWorld tasks.2. Achieves 91.11% success rate on MiniWoB++tasks with feedback.3. Improves planning correctness by adaptivelyrefining plans based on environmental feedback,effectively managing complex sequential tasks.GNN-Enhanced TaskHuggingFace,Node F1-Score, Link1. Integration of GNNs for task graph navi-1. GNN-enhanced planning outperforms LLM-PlannerMultimedia tasks,F1-Score, Task Accu-gationbased solutions by improving task accuracy (up(Wu et al., 2024)Daily Life APIracy, Token Consump-2.Training-free (SGC) and training-to 9%) with reduced token consumption. Thetasks,TMDBtionrequired (GraphSAGE) approachesproposed method scales well with larger taskAPI tasks3. GNN-based node and edge selection forgraphs and improves planning efficiency by atask planningsignificant margin.4. Training fine-tuned models for better task2.Addresses correctness by effectively mappingretrievaltask dependencies in a graph structure, enhanc-ing sub-task selection and sequence planning.SelfGoal (Yang et al., 2024) 1. ADaPT Public Goods Game, Guess 2/3 of the Average, First-Price Auc-tion, Bargaining Success Rate, TrueSkill Score, Contribution Consistency 1. Constructs GOALTREE to decompose high-level goals dynamically 2. Uses Search Module to select the most relevant subgoals 3. Decomposition updates based on environ-mental feedback 4. Adaptive subgoal tree refinement during task execution ALFWorld, Web-Success Rate, Task 1. As-needed recursive decomposition of ADaPT improves success rates by up to 33%(Prasad et al., 2023)Shop, TextCraftComplexity Handling,complex tasksover baselines, dynamically decomposing com-&amp; hotel dataSub-Task Decomposi-2. Planner and executor modules with dy-plex tasks as needed. It handles task complexitytion Efficiencynamic failure adaptationefficiently with up to 28.3% higher success rates3. Multi-level decomposition for task com-on ALFWorld and 27% higher on WebShop.plexity handlingLLM+PBlocksworld,Success Rate, Optimal1. Converts natural language problem de-1. LLM+P significantly outperforms LLMs in(Liu et al., 2023a)Grippers,Bar-Plan Success, Plan Exe-scription into PDDL formatsolving long-horizon planning problems, achiev-man, Termes, andcution Time2. Uses classical planners (e.g., FAST-ing optimal plans in robot domains with highother robot plan-DOWNWARD) to generate optimal planssuccess rates (up to 100% on Blocksworld) andning scenarios3. Translates planner output back into natu-minimal execution time. 2. LLM+P improvesral languagecorrectness by integrating LLMs with classicalplanning techniques to generate and execute op-timal plans.LLM-DPAlfworldSuccess Rate, Average1. LLM converts task descriptions into exe-LLM-DP outperforms the ReAct baseline,(Dagan et al., 2023)Episode Lengthcutable PDDL goalsachieving 96% success in Alfworld, with sig-2. Symbolic planner (BFS(f)) generatesnificantly fewer actions (13.16 vs. 18.69) pervalid planstask and higher efficiency due to belief sampling3. Belief sampling to generate multipleand symbolic planning integration.world states for planningPDDL World ModelHousehold, Tyre-Error Count, Success1. LLM-based PDDL generation for task1. The paper shows that GPT-4 is capable of gen-Generationworld, LogisticsRateplanningerating high-quality PDDL models with fewer(Guan et al., 2023)2. Error correction using LLMs as feedbackerrors than GPT-3.5-Turbo. It demonstrates thatinterfacesGPT-4-based world models lead to a 95% suc-3. Use of external planners for generatingcess rate in planning tasks using classical plan-feasible plans from PDDL modelsners like Fast Downward.2. Enhances plan correctness by using LLM-generated PDDL models in conjunction withdomain-independent planners, ensuring plansare not only feasible but also optimal.LLM Planning Evalu-Blocksworld,Success Rate, Leven-1. Evaluates LLMs (GPT-3, BLOOM) onLLMs struggle with autonomous plan generationationMysteryshtein Edit Distancecommon planning domains(3% success rate), but perform better when used(Valmeekam et al.,Blocksworld2. Three evaluation modes: autonomous,for heuristic guidance in planning tasks (with2023b)heuristic, and human-in-the-loopLPG) and assist human planners by improving3. Comparison of LLM-generated planstask completion accuracy (74% to 82%).with optimal solutions using automated plan-ning tools
A curated list of papers and resources related to this survey are available at https://github.com/wll199566/ Awesome-LLM-Planning-Capability.
AcknowledgementsWe thank the reviewers for their insightful feedback, which has helped improve our paper.H.W. and S.P. are supported by a UC Merced Spring 2023 Climate Action Seed Competition Grant, CAHSI-Google Institutional Research Program Award, and F3 R&amp;D GSR Award funded by the US Department of Commerce, Economic Development Administration Build Back Better Regional Challenge.Z.Z. and F.L. are partially supported by NSF CAREER Award #2303655.: Policyguided planning for generalized policy generation.arXiv preprint arXiv:2204.10420.Autonomous agents from automatic reward modeling and planning.In The Thirteenth International Conference on Learning Representations.Stefanie Tellex, and David Paulius.2024.Cape: Corrective actions from precondition errors using large language models.In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 14070-14077.IEEE.carlo planning with large language model for textbased games.In The Thirteenth International Conference on Learning Representations., et al. 2024.Can graph learning improve planning in llm-based agents?In The Thirtyeighth Annual Conference on Neural Information Processing Systems.The paper shows that GPT-4 is capable of generating high-quality PDDL models with fewer errors than GPT-3.5-Turbo.It demonstrates that GPT-4-based world models lead to a 95% success rate in planning tasks using classical planners like Fast Downward.1. Introduced a novel approach to improve plan executability and correctness using error-driven re-prompting.2. Demonstrated substantial improvements in task execution rates and reduced the number of necessary re-prompts compared to baselines.2. Achieves significant reductions in search steps (up to 26.8% fewer than A*) and improves task-solving performance on complex puzzles and navigation tasks.LLMFP(Hao et al., 2024b)9 diverse planning tasks (multi-constraint decision-making and multi-step planning)Plan optimality, constraint satisfaction rate 1. Converts natural language planning problems into formal optimization problems using zero-shot LLM reasoning.2. Uses SMT solvers to guarantee that the generated plans are both logically correct and executable.1. Introduces a novel method of achieving optimality in LLM planning by rigorously constructing optimization problems from natural language inputs, significantly outperforming traditional planning models.2. Demonstrates strong performance across a range of complex tasks, achieving over 83% optimal rates, thereby highlighting the effectiveness of integrating LLMs with classical optimization approaches for high-quality planning outcomes.The paper shows that GPT-4 is capable of generating high-quality PDDL models with fewer errors than GPT-3.5-Turbo.It demonstrates that GPT-4-based world models lead to a 95% success rate in planning tasks using classical planners like Fast Downward.LLM-DP
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. Mohamed Aghzal, Erion Plaku, Ziyu Yao, arXiv:2310.032492023arXiv preprint</p>
<p>Mohamed Aghzal, Erion Plaku, Ziyu Yao, arXiv:2406.12000Look further ahead: Testing the limits of gpt-4 in path planning. 2024arXiv preprint</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Codeplan: Repository-level coding using llms and planning. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, D C Vageesh, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, Shashank Shet, arXiv:2309.124992023Preprint</p>
<p>Ksenia Bestuzheva, Mathieu Besançon, Wei-Kun Chen, Antonia Chmiela, Tim Donkiewicz, Leon Jasper Van Doornmalen, Oliver Eifler, Gerald Gaul, Ambros Gamrath, Gleixner, arXiv:2112.08872The scip optimization suite 8.0. 2021arXiv preprint</p>
<p>Grounding llms for robot task planning using closed-loop state feedback. Vineet Bhat, Ali Umut Kaypak, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami, arXiv:2402.085462024arXiv preprint</p>
<p>Stephen Boyd, Lieven Vandenberghe, Convex Optimization. Cambridge University Press2004</p>
<p>Plasma: Procedural knowledge models for language-based planning and re-planning. Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D Hwang, Xiang , Lorraine Li, Jacqueline Hirona, Soumya Arai, Keisuke Sanyal, Xiang Sakaguchi, Yejin Ren, Choi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Automating thought of search: A journey towards soundness and completeness. Daniel Cao, Michael Katz, Harsha Kokel, Kavitha Srinivas, Shirin Sohrabi, arXiv:2408.113262024arXiv preprint</p>
<p>Learning to reason over scene graphs: a case study of finetuning gpt-2 into a robot language model for grounded task planning. Georgia Chalvatzaki, Ali Younes, Daljeet Nandha, An Thai Le, Leonardo Fr Ribeiro, Iryna Gurevych, Frontiers in Robotics and AI. 2023101221739</p>
<p>Matthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, Ruta Desai, Michal Hlavac, Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, arXiv:2411.00081Partnr: A benchmark for planning and reasoning in embodied multiagent tasks. 2024arXiv preprint</p>
<p>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, arXiv:2310.05915Fireact: Toward language agent fine-tuning. 2023aarXiv preprint</p>
<p>Gonzalo Gonzalez-Pumariega, Leong Su Yean, Neha Sunkara, Sanjiban Choudhury, arXiv:2502.05227Robotouille: An asynchronous planning benchmark for llm agents. 2025arXiv preprint</p>
<p>Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, arXiv:2411.15594A survey on llm-as-a-judge. 2024arXiv preprint</p>
<p>Leveraging pretrained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Open grounded planning: Challenges and benchmark construction. Shiguang Guo, Ziliang Deng, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun, 10.18653/V1/2024.ACL-LONG.272Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Large language models can plan your travels rigorously with formal verification tools. Yilun Hao, Yongchao Chen, Yang Zhang, Chuchu Fan, arXiv:2404.118912024aarXiv preprint</p>
<p>Planning anything with rigor: General-purpose zero-shot planning with llm-based formalized programming. Yilun Hao, Yang Zhang, Chuchu Fan, arXiv:2410.121122024barXiv preprint</p>
<p>Saycanpay: Heuristic planning with large language models using learnable domain knowledge. Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De, Raedt , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>The fast downward planning system. Malte Helmert, Journal of Artificial Intelligence Research. 262006</p>
<p>What's the plan? evaluating and developing planning-aware techniques for llms. Eran Hirsch, Guy Uziel, Ateret Anaby-Tavor, arXiv:2402.114892024arXiv preprint</p>
<p>Val: Automatic plan validation, continuous effects and mixed initiative planning using pddl. Richard Howey, Derek Long, Maria Fox, 16th IEEE International Conference on Tools with Artificial Intelligence. IEEE2004</p>
<p>Chain-of-symbol prompting elicits planning in large langauge models. Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, Yue Zhang, arXiv:2305.102762023aarXiv preprint</p>
<p>Chain-of-symbol prompting for spatial reasoning in large language models. Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, Yue Zhang, First Conference on Language Modeling. 2024</p>
<p>Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, Ping Luo, arXiv:2310.08582Tree-planner: Efficient close-loop task planning with large language models. 2023barXiv preprint</p>
<p>A survey of uncertainty estimation in llms: Theory meets practice. Hsiu-Yuan Huang, Yutong Yang, Zhaoxi Zhang, Sanwoo Lee, Yunfang Wu, arXiv:2410.153262024aarXiv preprint</p>
<p>How far are we on the decision-making of llms? evaluating llms' gaming ability in multi-agent environments. Jen-Tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R Lyu, arXiv:2403.118072024barXiv preprint</p>
<p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.05232A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. 2023arXiv preprint</p>
<p>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International conference on machine learning. PMLR2022a</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.05608Inner monologue: Embodied reasoning through planning with language models. 2022barXiv preprint</p>
<p>Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, arXiv:2402.02716Ruiming Tang, and Enhong Chen. 2024c. Understanding the planning of llm agents: A survey. arXiv preprint</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Visually-grounded planning without vision: Language models infer detailed plans from high-level instructions. Peter A Jansen, arXiv:2009.142592020arXiv preprint</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. Ofir Press, and Karthik Narasimhan2023arXiv preprint</p>
<p>Da Ju, Song Jiang, Andrew Cohen, Aaron Foss, Sasha Mitts, Arman Zharmagambetov, Brandon Amos, Xian Li, Justine T Kao, Maryam Fazel-Zarandi, arXiv:2410.16456To the globe (ttg): Towards languagedriven guaranteed travel planning. 2024arXiv preprint</p>
<p>Can large language models reason and plan?. Subbarao Kambhampati, Annals of the New York Academy of Sciences. 153412024</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, arXiv:2402.018172024arXiv preprint</p>
<p>Towards a foundation for evaluating ai planners. A Nabil, David E Kartam, Wilkins, AI EDAM. 411990</p>
<p>Thought of search: Planning with language models through the lens of efficiency. Michael Katz, Harsha Kokel, Kavitha Srinivas, Shirin Sohrabi, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Jing Yu Koh, Stephen Mcaleer, Daniel Fried, Ruslan Salakhutdinov, arXiv:2407.01476Tree search for language model agents. 2024arXiv preprint</p>
<p>Multiple-agent planning systems. Kurt Konolige, Nils J Nilsson, AAAI. 198080</p>
<p>Lucas Lehnert, Sainbayar Sukhbaatar, Dijia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, Yuandong Tian, arXiv:2402.14083Beyond a*: Better planning with transformers via search dynamics bootstrapping. 2024arXiv preprint</p>
<p>Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, arXiv:2411.16594From generation to judgment: Opportunities and challenges of llm-as-a-judge. 2024aarXiv preprint</p>
<p>Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, Yiqun Liu, arXiv:2412.05579Llms-as-judges: a comprehensive survey on llm-based evaluation methods. 2024barXiv preprint</p>
<p>Systematic analysis of llm contributions to planning: Solver, verifier, heuristic. Haoming Li, Zhaoliang Chen, Songyuan Liu, Yiming Lu, Fei Liu, arXiv:2412.096662024cPreprint</p>
<p>Haoming Li, Zhaoliang Chen, Jonathan Zhang, Fei Liu, arXiv:2409.01806Lasp: Surveying the state-of-the-art in large language model-assisted ai planning. 2024darXiv preprint</p>
<p>Planet: A collection of benchmarks for evaluating llms' planning capabilities. Haoming Li, Zhaoliang Chen, Jonathan Zhang, Fei Liu, arXiv:2504.147732025Preprint</p>
<p>Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Advances in Neural Information Processing Systems. 2024e36</p>
<p>Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, arXiv:2410.07166Embodied agent interface: Benchmarking llms for embodied decision making. 2024farXiv preprint</p>
<p>Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, arXiv:2304.08244Api-bank: A comprehensive benchmark for tool-augmented llms. 2023arXiv preprint</p>
<p>Pretrained language models for interactive decisionmaking. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Advances in Neural Information Processing Systems. 202235</p>
<p>On grounded planning for embodied tasks with language models. Chengsong Bill Yuchen Lin, Qian Huang, Wenda Liu, Sam Gu, Xiang Sommerer, Ren, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023a37</p>
<p>Fangru Lin, La Emanuele, Valentin Malfa, Elle Michelle Hofmann, Anthony Yang, Janet B Cohn, Pierrehumbert, arXiv:2402.02805Graph-enhanced large language models in asynchronous plan reasoning. 2024arXiv preprint</p>
<p>Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. Han Lin, Abhay Zala, Jaemin Cho, Mohit Bansal, arXiv:2309.150912023barXiv preprint</p>
<p>Width and inference based planners: Siw, bfs (f), and probe. Nir Lipovetzky, Miquel Ramirez, Christian Muise, Hector Geffner, Proceedings of the 8th International Planning Competition (IPC-2014). the 8th International Planning Competition (IPC-2014)201443</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023aarXiv preprint</p>
<p>Gang Liu, Michael Sun, Wojciech Matusik, Meng Jiang, Jie Chen, arXiv:2410.04223Multimodal large language models for inverse molecular design with retrosynthetic planning. 2024arXiv preprint</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, arXiv:2308.03688Agentbench: Evaluating llms as agents. 2023barXiv preprint</p>
<p>Yujie Lu, Pan Lu, Zhiyu Chen, Wanrong Zhu, Xin , Eric Wang, William Yang, Wang , arXiv:2305.01795Multimodal procedural planning via dual text-image prompting. 2023arXiv preprint</p>
<p>. Drew Mcdermott, Malik Ghallab, Adele E Howe, Craig A Knoblock, Ashwin Ram, Manuela M Veloso, Daniel S Weld, David E Wilkins, 1998Pddl-the planning domain definition language</p>
<p>Amit Moryossef, Yoav Goldberg, Ido Dagan, arXiv:1904.03396Step-by-step: Separating planning from realization in neural data-to-text generation. 2019arXiv preprint</p>
<p>Elements of a theory of human problem solving. Allen Newell, John Calman Shaw, Herbert A Simon, Psychological review. 6531511958</p>
<p>Aleksandar Odhran O'donoghue, John Shtedritski, Ralph Ginger, Ali Essa Abboud, Justin Ghareeb, Samuel G Booth, Rodriques, arXiv:2310.10632Bioplanner: automatic evaluation of llms on protocol planning in biology. 2023arXiv preprint</p>
<p>Worldapis: The world is worth how many apis? a thought experiment. Jiefu Ou, Arda Uzunoglu, Benjamin Van Durme, Daniel Khashabi, arXiv:2407.077782024arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Data-efficient learning of natural language to linear temporal logic translators for robot task specification. Jiayi Pan, Glen Chou, Dmitry Berenson, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, arXiv:1809.088872018arXiv preprint</p>
<p>Tasklama: probing the complex task understanding of language models. Quan Yuan, Mehran Kazemi, Xin Xu, Isaac Noble, Vaiva Imbrasaite, Deepak Ramachandran, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Distilling script knowledge from large language models for constrained language planning. Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang, arXiv:2305.052522023arXiv preprint</p>
<p>Diagrammergpt: Generating open-domain, open-platform diagrams via llm planning. Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal, arXiv:2310.121282023arXiv preprint</p>
<p>Agentohana: Design unified data and training pipeline for effective agent learning. Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, arXiv:2402.155062024arXiv preprint</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, Advances in Neural Information Processing Systems. 202436</p>
<p>Swaroop Huaixiu Steven Zheng, Hugh Mishra, Xinyun Zhang, Minmin Chen, Azade Chen, Le Nova, Hou, Heng-Tze, Quoc V Cheng, Ed H Le, Chi, arXiv:2406.04520Natural plan: Benchmarking llms on natural language planning. 2024arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, arXiv:2310.044062023aarXiv preprint</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, arXiv:2307.13854Webarena: A realistic web environment for building autonomous agents. 2023barXiv preprint</p>
<p>Isr-llm: Iterative self-refined large language model for long-horizon sequential task planning. Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, Lei Ma, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel, Chao Zhang, arXiv:2310.13227Toolchain<em>: Efficient action space navigation in large language models with a</em> search. 2023arXiv preprint</p>
<p>Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Stephen H Michael L Littman, Bach, arXiv:2407.03321Planetarium: A rigorous benchmark for translating text to structured planning languages. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8006 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8006</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8006</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-c7a4946eb49bc6a9c01eaa79e84a35316595bd5a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c7a4946eb49bc6a9c01eaa79e84a35316595bd5a" target="_blank">Language Models as Inductive Reasoners</a></p>
                <p><strong>Paper Venue:</strong> Conference of the European Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and creates a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language.</p>
                <p><strong>Paper Abstract:</strong> Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language models as ”reasoners”. Moreover, we provide the first and comprehensive analysis of how well pretrained language models can induce natural language rules from natural language facts. We also propose a new framework drawing insights from philosophy literature for this task, which we show in the experiment section that surpasses baselines in both automatic and human evaluations. We discuss about our future perspectives for inductive reasoning in Section 7. Dataset and code are available at https://github.com/ZonglinY/Inductive_Reasoning.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8006.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8006.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DEER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DEER (inDuctive rEasoning with natural languagE Representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-curated dataset of 1,200 natural-language (rule, fact) pairs across 6 scientific topics and 4 FOL-derived rule templates, designed to train/evaluate models that induce general natural-language rules from factual text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various PLMs (primary experiments with GPT-J; also LLaMA, GPT-Neo, GPT-NeoX, GPT-3 reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Primarily GPT-J (6B); also LLaMA-7B, GPT-Neo (125M/1.3B/2.7B), GPT-NeoX-20B, GPT-3/Davinci variants</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General — commonsense and scientific-like hypotheses across zoology, botany, geology, astronomy, history, physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Natural-language rules / hypotheses (induced generalizations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Dataset for generation / gold-rule reference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Gold rules (written by expert annotator) paired with supporting facts; generated rules are compared to gold rules using automatic metrics and human annotation on multiple axes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>METEOR (primary automatic), WRecall, GREEN; human precision/recall/F1 and DEERLET label scores</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Dataset itself (1,200 pairs) used as gold references; automatic metrics defined elsewhere (METEOR, WRecall, GREEN). DEER correctness validated at 95.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEER</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>DEER correctness was manually checked by 4 graduate students (random split) giving overall correctness 95.5%; used as gold for automatic metrics and for qualitative analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as gold reference across experiments reported in Table 4/5/6/7; primary statistics: 1,200 rule-fact pairs, six topics, four rule templates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset is relatively small (1.2k) and mainly covers commonsense-like knowledge; collecting high-quality general rules is difficult and limits scale and domain coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8006.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DEERLET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DEERLET (classification of inDucEd rulEs with natuRal LanguagE representaTion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A labeled dataset (846 tuples) of (fact, generated-rule, label0..label3) where generated rules from PLMs are annotated on four inductive-quality axes to train/evaluate verification classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (generation source for collected rules); classifiers finetuned on DEERLET (GPT-J used for M2/3/4/5 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Rule generation: GPT-J (6B) used to produce candidates; classifiers also implemented with GPT-J (6B) and evaluated in-context and finetuned</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General — supports evaluation of induced natural-language rules across multiple scientific/commonsense topics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation labels / classification of generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>DEERLET human annotation schema (label0..label3)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each generated rule paired with facts is annotated on four aspects: (label0) deductive consistency with facts, (label1) reflects reality, (label2) more general than facts, (label3) non-trivial — using 3-point scales for 0/1/2 and 2-point for label3.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision/Recall/F1 for human-evaluation aggregated metrics; classifier metrics: accuracy, F1, averaged precision</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>label0/1/2: 3-point scale {2=true,1=partially true,0=false}; label3: binary {1=non-trivial,0=trivial}; DEERLET size: 846 tuples (546 train /100 val /200 test).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEERLET</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>DEERLET annotations collected by the first author (expert); labels follow explicit rubric (Appendix A.5). Dataset split: 546 train / 100 val / 200 test.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to train/evaluate M2–M5. Example classifier outcomes (finetuned GPT-J) in Table 8: M2 accuracy 74.0, M3 accuracy 70.5, M4 accuracy 86.0, M5 accuracy 89.5 (finetuned results shown second in pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>DEERLET rules were collected from GPT-J generation to avoid annotation bias; annotations by a single expert may introduce bias; scales are coarse (3- and 2-point).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8006.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>METEOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic sentence-level similarity metric (originally for MT) used here to measure closeness of generated rules to gold rules; chosen because it correlates better with human judgments than BLEU on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>METEOR: an automatic metric for MT evaluation with improved correlation with human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to outputs from various PLMs (GPT-J primary; also LLaMA, GPT-Neo, GPT-NeoX, GPT-3 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Metric is model-agnostic (applied to outputs from models sized 6B, 7B, 20B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural-language hypothesis/rule generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automatic similarity/evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>METEOR automatic similarity scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute METEOR between generated rule and gold (DEER) rule; averaged across generated rules (after any filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>METEOR score (higher better); reported as averaged METEOR per model/setting</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard METEOR score (not constrained to [0,1] in practice as used here); values reported in paper (examples: M1 METEOR 25.28; CoLM 26.44 in in-context setting; finetuned CoLM 27.32).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEER (gold rules)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Correlation with human evaluation on DEERLET: Pearson r = 0.29 (p = 4.48e-6), indicating statistically significant but moderate correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>METEOR used as main 'precision-like' automatic metric; sample reported values: R+F 11.20, M1 25.28, CoLM 26.44 (in-context) / 27.32 (finetuned).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>METEOR reflects surface similarity to a single gold rule and therefore underestimates valid but diverse inductive generalizations; not normalized to [0,1] in their GREEN formula design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8006.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU (Bilingual Evaluation Understudy) score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An n-gram based automatic metric for generated text compared to reference; discussed and compared to METEOR, but METEOR preferred due to higher correlation with human judgments for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bleu: a method for automatic evaluation of machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mentioned in context of evaluating outputs from PLMs (GPT-J and others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Metric is model-agnostic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation of generated hypotheses/rules</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automatic similarity/evaluation metric (baseline comparator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute BLEU between generated rule and reference rule; authors compared correlation with human scores and found lower correlation than METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU score</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard BLEU (0–100 or 0–1 depending on normalization); correlation with human eval on DEERLET reported lower (Pearson r = 0.24) than METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEER</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Correlation with human evaluation on DEERLET: Pearson r = 0.24 (p-value significant).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>BLEU reported in early experiments (Table 11) for GPT-3 vs GPT-J rule proposer; BLEU correlation with human judgments lower than METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>BLEU correlates less well with human judgments for open-ended inductive generation and is therefore not preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8006.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WRecall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted Recall (WRecall)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new automatic metric introduced in this paper to approximate 'recall' for generated rules by weighting recall computed on METEOR-sorted blocks of candidates under the assumption that higher METEOR implies higher true-rule probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to outputs from PLMs (GPT-J primary; also others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Metric is model-agnostic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation of generated natural-language hypotheses/rules</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automatic recall surrogate metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Weighted Recall (WRecall)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Sort generated rules by METEOR score into blocks, assume only rules in each block are 'true' for recall computation, compute per-block recall, weight blocks by pre-specified linear weights, sum and linearly normalize to [0,1] using formula WRecall = (sum_i weight_i * recall_i + 125) / 250.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WRecall (0–1 normalized score)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>WRecall computed via weighted sum of block-level recall values, normalized to range [0,1]; weights reflect importance of METEOR-ranked blocks (illustrated in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEER (for METEOR-based ranking) and generated candidate pools from M1</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Designed because human labels for recall over large candidate sets are unavailable; relies on empirically observed correlation between METEOR and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported WRecall values (in Table 4): R+F 0.50, M1 0.50, CoLM 0.54 (in-context) / 0.62 (finetuned).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>WRecall depends on the assumption that METEOR rank correlates with true-label probability; approximate and sensitive to METEOR's limitations; designed as a practical surrogate rather than ground-truth recall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8006.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GREEN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GREEN (GeometRic mEan of METEOR aNd WRecall)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A composite automatic metric introduced by the authors combining METEOR and WRecall via geometric mean to capture both precision-like (METEOR) and recall-like (WRecall) aspects of induced-rule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to outputs from PLMs (GPT-J primary; others evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Model-agnostic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation of generated natural-language rules/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Composite automatic evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>GREEN score</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute GREEN = sqrt(METEOR) * WRecall to combine METEOR and WRecall; geometric mean chosen because METEOR is not normalized to [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GREEN (composite score; units match METEOR^0.5 * WRecall)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>GREEN = sqrt(METEOR) * WRecall; reported values in paper: e.g., M1 3.56, CoLM 3.78 (in-context) / 4.11 (finetuned).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEER (for METEOR reference) and generated candidate pools</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Designed to provide a more comprehensive automatic assessment when both precision and recall matter; authors note METEOR (precision) often more important practically.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GREEN values reported across many experiments (Tables 4–7); CoLM outperforms baselines on GREEN (e.g., 3.78 / 4.11).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>GREEN inherits limitations of METEOR and WRecall; authors caution that precision (METEOR) may be more important than recall in this task and that GREEN must be interpreted accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8006.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Language-Models (CoLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular framework of five PLM-based modules: M1 (rule proposer/generator) and four classifiers M2–M5 (deductive consistency, indiscriminate-confirmation handler, generalization checker, triviality detector) that filter generated rule candidates according to philosophical requirements for induction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Implemented primarily with GPT-J (6B); tested with LLaMA-7B and other PLMs for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Modules implemented with GPT-J (6B) primarily; other model sizes used in ablations (7B, 20B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General inductive reasoning / hypothesis induction in natural language</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Framework for generation + verification of induced hypotheses (rule generation plus discriminative filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Modular verification via M2–M5 classifiers informed by philosophical criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>M1 generates many candidate rules; M2–M5 independently score candidates along four inductive desiderata; candidates passing module thresholds are selected; probabilistically interpretable via P(rule|fact) ≈ P_M24(fact|rule) P_M35(rule).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>METEOR, WRecall, GREEN for generation; human metrics (precision/recall/F1) and DEERLET labels for classifier evaluation (accuracy, F1, averaged precision)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See METEOR/WRecall/GREEN definitions; classifiers evaluated with accuracy, F1, averaged precision on DEERLET (e.g., finetuned GPT-J M4 accuracy 86.0).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEER for generation; DEERLET for classifier training/eval</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluation based on DEERLET labels used to tune module thresholds; overall human metrics reported in Table 4 (e.g., CoLM precision 48.1% / 70.0% in in-context / finetune, recall 72.2% / 54.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CoLM outperforms baselines: METEOR 26.44/27.32 (in-context/finetuned), WRecall 0.54/0.62, GREEN 3.78/4.11; human-eval metrics improved across several axes (see Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Module thresholds require tuning on DEERLET; performance depends on quality and finetuning of module models; framework does not by itself verify 'reflects reality' via experiments (no external world tests).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8006.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inductive Criteria (DEERLET labels)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Four inductive-evaluation criteria: Deductive consistency, Reflects reality (no indiscriminate confirmation), Generalization (ampliative), Non-triviality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of four evaluation criteria, derived from philosophy of induction, used to judge induced rules: (1) consistent with facts, (2) not violating reality/commonsense, (3) more general than the facts, (4) not trivial/complete sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Operationalized via classifiers (M2–M5) implemented with GPT-J (6B) and evaluated on DEERLET</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Classifiers primarily GPT-J (6B), finetuned variants also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Philosophy-informed evaluation of natural-language hypotheses (general scientific/commonsense domains)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human-annotation criteria and classifier targets for evaluating induced hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Four-label human annotation (label0..label3) and corresponding classifier modules M2–M5</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>label0: deductive consistency with facts (3-point); label1: reflects reality/commonsense (3-point); label2: generalization/ampliative (3-point); label3: non-trivial (2-point). Classifiers trained and evaluated on DEERLET.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human-labeled scores aggregated into precision/recall/F1; classifiers evaluated with accuracy, F1, averaged precision</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Label scoring: 2=true, 1=partially true, 0=false for label0/1/2; label3 binary. Classifier outputs are probabilities used with tuned thresholds (Appendix A.11).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEERLET (labels derived directly from philosophy literature—Norton 2005, Salmon 1989)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Annotation rubric provided in Appendix A.5; DEERLET annotated by the first author (expert); thresholds for classifiers tuned on DEERLET validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Classifier performance (finetuned GPT-J) in Table 8: M2 acc 74.0, M3 acc 70.5, M4 acc 86.0, M5 acc 89.5; these modules improve downstream generation filtering quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Some subjectivity remains in labels; label annotation primarily by a single expert; 'reflects reality' is hard to assess without world interaction or experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8006.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R+F baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R+F (Random Fill) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-neural baseline that constructs rules by randomly filling a provided rule template with sentences/phrases extracted from the input facts, used as a simple heuristic comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Non-neural heuristic baseline (no PLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (non-neural)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Baseline method for inductive rule generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Baseline rule-generation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Template random-fill generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given a rule template, randomly fill template slots with content sampled from facts to produce candidate rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Evaluated with METEOR, WRecall, GREEN and human metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported metrics in Table 4: METEOR 11.20, WRecall 0.50, GREEN 2.37; human precision 9.0%, recall 100% (because majority class baseline), F1 0.17.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEER (generation), DEERLET (human eval labels)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluation figures from Table 4 reflect low precision high recall behavior for R+F.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>R+F much worse than PLM baselines (e.g., M1 METEOR 25.28 vs R+F 11.20); human precision low (9.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Toy baseline that ignores induction beyond template filling; high recall but trivial/uninformative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8006.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TF-IDF baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TF–IDF thresholding baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-parametric baseline that scores fact–rule pairs by TF–IDF similarity and thresholds to predict rule correctness; used as a simple classifier baseline for M2–M5 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A statistical interpretation of term specificity and its application in retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TF–IDF (classical information retrieval technique) applied to candidate rule–fact pairs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Baseline classifier for rule–fact compatibility/generalization detection</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Baseline verification method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>TF–IDF similarity with tuned threshold</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute TF–IDF similarity score between fact(s) and rule; tune a threshold on DEERLET val set to split yes/no correctness predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1, averaged precision (for DEERLET classification tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported in Table 8: TF-IDF achieved similar performance to majority-class baseline on DEERLET modules (thresholds best found were zero in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEERLET (training/validation for threshold tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>TF–IDF thresholds tuned on DEERLET validation; resulted thresholds often trivial (zero) indicating low discriminative utility.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>TF-IDF baseline performed close to majority-class on DEERLET (e.g., accuracy 62.5 for M2 baseline), indicating limited utility for the classification tasks here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>TF–IDF lacks semantic generalization and is insensitive to paraphrase; threshold tuning often degenerated to trivial values.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8006.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8006.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thresholding & Heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Threshold tuning and heuristic filtering (token-length filter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational procedures used to decide when generated candidates are accepted by M2–M5: thresholds tuned on DEERLET validation (heuristics for selecting global/local optima) plus a 45-token minimum filter to avoid trivial generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to classifier outputs from GPT-J (and other PLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Classifier models primarily GPT-J (6B) for threshold calibration</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Operational evaluation/filtering of induced hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Practical method for controlling selection precision vs recall</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Validation-set threshold selection and token-length heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>On DEERLET validation, select global/local optimal thresholds maximizing accuracy/F1 while keeping thresholds away from extremes and recall in [0.7,0.9]; discard rules shorter than 45 tokens before passing to M2–M5.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Heuristics influence reported automatic/human metrics (precision/recall/F1, METEOR/WRecall/GREEN)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Threshold selection criteria detailed in Appendix A.11; 45-token minimum chosen based on observed triviality of short outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DEERLET (validation used for threshold tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Thresholds tuned to avoid filtering too many rules while ensuring modules reject low-quality outputs; heuristics intended to balance precision and recall.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Thresholding approach used to produce Table 4 results; token-length filter prevented trivial examples from contaminating DEERLET annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Thresholds are heuristic and dataset-dependent; 45-token cutoff is an empirical heuristic that may not generalize; threshold tuning depends on DEERLET which itself is limited in size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Inductive Reasoners', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>METEOR: an automatic metric for MT evaluation with improved correlation with human judgments <em>(Rating: 2)</em></li>
                <li>Bleu: a method for automatic evaluation of machine translation <em>(Rating: 2)</em></li>
                <li>A little survey of induction <em>(Rating: 2)</em></li>
                <li>Inductive logic programming <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Inductive relation prediction by subgraph reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8006",
    "paper_id": "paper-c7a4946eb49bc6a9c01eaa79e84a35316595bd5a",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "DEER",
            "name_full": "DEER (inDuctive rEasoning with natural languagE Representation)",
            "brief_description": "A human-curated dataset of 1,200 natural-language (rule, fact) pairs across 6 scientific topics and 4 FOL-derived rule templates, designed to train/evaluate models that induce general natural-language rules from factual text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various PLMs (primary experiments with GPT-J; also LLaMA, GPT-Neo, GPT-NeoX, GPT-3 reported)",
            "model_size": "Primarily GPT-J (6B); also LLaMA-7B, GPT-Neo (125M/1.3B/2.7B), GPT-NeoX-20B, GPT-3/Davinci variants",
            "scientific_domain": "General — commonsense and scientific-like hypotheses across zoology, botany, geology, astronomy, history, physics",
            "theory_type": "Natural-language rules / hypotheses (induced generalizations)",
            "evaluation_method_name": "Dataset for generation / gold-rule reference",
            "evaluation_method_description": "Gold rules (written by expert annotator) paired with supporting facts; generated rules are compared to gold rules using automatic metrics and human annotation on multiple axes.",
            "evaluation_metric": "METEOR (primary automatic), WRecall, GREEN; human precision/recall/F1 and DEERLET label scores",
            "metric_definition": "Dataset itself (1,200 pairs) used as gold references; automatic metrics defined elsewhere (METEOR, WRecall, GREEN). DEER correctness validated at 95.5%.",
            "dataset_or_benchmark": "DEER",
            "human_evaluation_details": "DEER correctness was manually checked by 4 graduate students (random split) giving overall correctness 95.5%; used as gold for automatic metrics and for qualitative analysis.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used as gold reference across experiments reported in Table 4/5/6/7; primary statistics: 1,200 rule-fact pairs, six topics, four rule templates.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Dataset is relatively small (1.2k) and mainly covers commonsense-like knowledge; collecting high-quality general rules is difficult and limits scale and domain coverage.",
            "uuid": "e8006.0",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DEERLET",
            "name_full": "DEERLET (classification of inDucEd rulEs with natuRal LanguagE representaTion)",
            "brief_description": "A labeled dataset (846 tuples) of (fact, generated-rule, label0..label3) where generated rules from PLMs are annotated on four inductive-quality axes to train/evaluate verification classifiers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J (generation source for collected rules); classifiers finetuned on DEERLET (GPT-J used for M2/3/4/5 experiments)",
            "model_size": "Rule generation: GPT-J (6B) used to produce candidates; classifiers also implemented with GPT-J (6B) and evaluated in-context and finetuned",
            "scientific_domain": "General — supports evaluation of induced natural-language rules across multiple scientific/commonsense topics",
            "theory_type": "Evaluation labels / classification of generated hypotheses",
            "evaluation_method_name": "DEERLET human annotation schema (label0..label3)",
            "evaluation_method_description": "Each generated rule paired with facts is annotated on four aspects: (label0) deductive consistency with facts, (label1) reflects reality, (label2) more general than facts, (label3) non-trivial — using 3-point scales for 0/1/2 and 2-point for label3.",
            "evaluation_metric": "Precision/Recall/F1 for human-evaluation aggregated metrics; classifier metrics: accuracy, F1, averaged precision",
            "metric_definition": "label0/1/2: 3-point scale {2=true,1=partially true,0=false}; label3: binary {1=non-trivial,0=trivial}; DEERLET size: 846 tuples (546 train /100 val /200 test).",
            "dataset_or_benchmark": "DEERLET",
            "human_evaluation_details": "DEERLET annotations collected by the first author (expert); labels follow explicit rubric (Appendix A.5). Dataset split: 546 train / 100 val / 200 test.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used to train/evaluate M2–M5. Example classifier outcomes (finetuned GPT-J) in Table 8: M2 accuracy 74.0, M3 accuracy 70.5, M4 accuracy 86.0, M5 accuracy 89.5 (finetuned results shown second in pairs).",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "DEERLET rules were collected from GPT-J generation to avoid annotation bias; annotations by a single expert may introduce bias; scales are coarse (3- and 2-point).",
            "uuid": "e8006.1",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "METEOR",
            "name_full": "METEOR (Metric for Evaluation of Translation with Explicit ORdering)",
            "brief_description": "An automatic sentence-level similarity metric (originally for MT) used here to measure closeness of generated rules to gold rules; chosen because it correlates better with human judgments than BLEU on this task.",
            "citation_title": "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments",
            "mention_or_use": "use",
            "model_name": "Applied to outputs from various PLMs (GPT-J primary; also LLaMA, GPT-Neo, GPT-NeoX, GPT-3 variants)",
            "model_size": "Metric is model-agnostic (applied to outputs from models sized 6B, 7B, 20B, etc.)",
            "scientific_domain": "Natural-language hypothesis/rule generation evaluation",
            "theory_type": "Automatic similarity/evaluation metric",
            "evaluation_method_name": "METEOR automatic similarity scoring",
            "evaluation_method_description": "Compute METEOR between generated rule and gold (DEER) rule; averaged across generated rules (after any filtering).",
            "evaluation_metric": "METEOR score (higher better); reported as averaged METEOR per model/setting",
            "metric_definition": "Standard METEOR score (not constrained to [0,1] in practice as used here); values reported in paper (examples: M1 METEOR 25.28; CoLM 26.44 in in-context setting; finetuned CoLM 27.32).",
            "dataset_or_benchmark": "DEER (gold rules)",
            "human_evaluation_details": "Correlation with human evaluation on DEERLET: Pearson r = 0.29 (p = 4.48e-6), indicating statistically significant but moderate correlation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "METEOR used as main 'precision-like' automatic metric; sample reported values: R+F 11.20, M1 25.28, CoLM 26.44 (in-context) / 27.32 (finetuned).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "METEOR reflects surface similarity to a single gold rule and therefore underestimates valid but diverse inductive generalizations; not normalized to [0,1] in their GREEN formula design.",
            "uuid": "e8006.2",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "BLEU",
            "name_full": "BLEU (Bilingual Evaluation Understudy) score",
            "brief_description": "An n-gram based automatic metric for generated text compared to reference; discussed and compared to METEOR, but METEOR preferred due to higher correlation with human judgments for this task.",
            "citation_title": "Bleu: a method for automatic evaluation of machine translation",
            "mention_or_use": "mention",
            "model_name": "Mentioned in context of evaluating outputs from PLMs (GPT-J and others)",
            "model_size": "Metric is model-agnostic",
            "scientific_domain": "NLP evaluation of generated hypotheses/rules",
            "theory_type": "Automatic similarity/evaluation metric (baseline comparator)",
            "evaluation_method_name": "BLEU scoring",
            "evaluation_method_description": "Compute BLEU between generated rule and reference rule; authors compared correlation with human scores and found lower correlation than METEOR.",
            "evaluation_metric": "BLEU score",
            "metric_definition": "Standard BLEU (0–100 or 0–1 depending on normalization); correlation with human eval on DEERLET reported lower (Pearson r = 0.24) than METEOR.",
            "dataset_or_benchmark": "DEER",
            "human_evaluation_details": "Correlation with human evaluation on DEERLET: Pearson r = 0.24 (p-value significant).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "BLEU reported in early experiments (Table 11) for GPT-3 vs GPT-J rule proposer; BLEU correlation with human judgments lower than METEOR.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "BLEU correlates less well with human judgments for open-ended inductive generation and is therefore not preferred.",
            "uuid": "e8006.3",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "WRecall",
            "name_full": "Weighted Recall (WRecall)",
            "brief_description": "A new automatic metric introduced in this paper to approximate 'recall' for generated rules by weighting recall computed on METEOR-sorted blocks of candidates under the assumption that higher METEOR implies higher true-rule probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to outputs from PLMs (GPT-J primary; also others)",
            "model_size": "Metric is model-agnostic",
            "scientific_domain": "Evaluation of generated natural-language hypotheses/rules",
            "theory_type": "Automatic recall surrogate metric",
            "evaluation_method_name": "Weighted Recall (WRecall)",
            "evaluation_method_description": "Sort generated rules by METEOR score into blocks, assume only rules in each block are 'true' for recall computation, compute per-block recall, weight blocks by pre-specified linear weights, sum and linearly normalize to [0,1] using formula WRecall = (sum_i weight_i * recall_i + 125) / 250.",
            "evaluation_metric": "WRecall (0–1 normalized score)",
            "metric_definition": "WRecall computed via weighted sum of block-level recall values, normalized to range [0,1]; weights reflect importance of METEOR-ranked blocks (illustrated in Table 3).",
            "dataset_or_benchmark": "DEER (for METEOR-based ranking) and generated candidate pools from M1",
            "human_evaluation_details": "Designed because human labels for recall over large candidate sets are unavailable; relies on empirically observed correlation between METEOR and human judgments.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Reported WRecall values (in Table 4): R+F 0.50, M1 0.50, CoLM 0.54 (in-context) / 0.62 (finetuned).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "WRecall depends on the assumption that METEOR rank correlates with true-label probability; approximate and sensitive to METEOR's limitations; designed as a practical surrogate rather than ground-truth recall.",
            "uuid": "e8006.4",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GREEN",
            "name_full": "GREEN (GeometRic mEan of METEOR aNd WRecall)",
            "brief_description": "A composite automatic metric introduced by the authors combining METEOR and WRecall via geometric mean to capture both precision-like (METEOR) and recall-like (WRecall) aspects of induced-rule generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to outputs from PLMs (GPT-J primary; others evaluated)",
            "model_size": "Model-agnostic",
            "scientific_domain": "Evaluation of generated natural-language rules/hypotheses",
            "theory_type": "Composite automatic evaluation metric",
            "evaluation_method_name": "GREEN score",
            "evaluation_method_description": "Compute GREEN = sqrt(METEOR) * WRecall to combine METEOR and WRecall; geometric mean chosen because METEOR is not normalized to [0,1].",
            "evaluation_metric": "GREEN (composite score; units match METEOR^0.5 * WRecall)",
            "metric_definition": "GREEN = sqrt(METEOR) * WRecall; reported values in paper: e.g., M1 3.56, CoLM 3.78 (in-context) / 4.11 (finetuned).",
            "dataset_or_benchmark": "DEER (for METEOR reference) and generated candidate pools",
            "human_evaluation_details": "Designed to provide a more comprehensive automatic assessment when both precision and recall matter; authors note METEOR (precision) often more important practically.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GREEN values reported across many experiments (Tables 4–7); CoLM outperforms baselines on GREEN (e.g., 3.78 / 4.11).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "GREEN inherits limitations of METEOR and WRecall; authors caution that precision (METEOR) may be more important than recall in this task and that GREEN must be interpreted accordingly.",
            "uuid": "e8006.5",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "CoLM",
            "name_full": "Chain-of-Language-Models (CoLM)",
            "brief_description": "A modular framework of five PLM-based modules: M1 (rule proposer/generator) and four classifiers M2–M5 (deductive consistency, indiscriminate-confirmation handler, generalization checker, triviality detector) that filter generated rule candidates according to philosophical requirements for induction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Implemented primarily with GPT-J (6B); tested with LLaMA-7B and other PLMs for comparison",
            "model_size": "Modules implemented with GPT-J (6B) primarily; other model sizes used in ablations (7B, 20B, etc.)",
            "scientific_domain": "General inductive reasoning / hypothesis induction in natural language",
            "theory_type": "Framework for generation + verification of induced hypotheses (rule generation plus discriminative filtering)",
            "evaluation_method_name": "Modular verification via M2–M5 classifiers informed by philosophical criteria",
            "evaluation_method_description": "M1 generates many candidate rules; M2–M5 independently score candidates along four inductive desiderata; candidates passing module thresholds are selected; probabilistically interpretable via P(rule|fact) ≈ P_M24(fact|rule) P_M35(rule).",
            "evaluation_metric": "METEOR, WRecall, GREEN for generation; human metrics (precision/recall/F1) and DEERLET labels for classifier evaluation (accuracy, F1, averaged precision)",
            "metric_definition": "See METEOR/WRecall/GREEN definitions; classifiers evaluated with accuracy, F1, averaged precision on DEERLET (e.g., finetuned GPT-J M4 accuracy 86.0).",
            "dataset_or_benchmark": "DEER for generation; DEERLET for classifier training/eval",
            "human_evaluation_details": "Human evaluation based on DEERLET labels used to tune module thresholds; overall human metrics reported in Table 4 (e.g., CoLM precision 48.1% / 70.0% in in-context / finetune, recall 72.2% / 54.4%).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "CoLM outperforms baselines: METEOR 26.44/27.32 (in-context/finetuned), WRecall 0.54/0.62, GREEN 3.78/4.11; human-eval metrics improved across several axes (see Table 4).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Module thresholds require tuning on DEERLET; performance depends on quality and finetuning of module models; framework does not by itself verify 'reflects reality' via experiments (no external world tests).",
            "uuid": "e8006.6",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Inductive Criteria (DEERLET labels)",
            "name_full": "Four inductive-evaluation criteria: Deductive consistency, Reflects reality (no indiscriminate confirmation), Generalization (ampliative), Non-triviality",
            "brief_description": "A set of four evaluation criteria, derived from philosophy of induction, used to judge induced rules: (1) consistent with facts, (2) not violating reality/commonsense, (3) more general than the facts, (4) not trivial/complete sentence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Operationalized via classifiers (M2–M5) implemented with GPT-J (6B) and evaluated on DEERLET",
            "model_size": "Classifiers primarily GPT-J (6B), finetuned variants also evaluated",
            "scientific_domain": "Philosophy-informed evaluation of natural-language hypotheses (general scientific/commonsense domains)",
            "theory_type": "Human-annotation criteria and classifier targets for evaluating induced hypotheses",
            "evaluation_method_name": "Four-label human annotation (label0..label3) and corresponding classifier modules M2–M5",
            "evaluation_method_description": "label0: deductive consistency with facts (3-point); label1: reflects reality/commonsense (3-point); label2: generalization/ampliative (3-point); label3: non-trivial (2-point). Classifiers trained and evaluated on DEERLET.",
            "evaluation_metric": "Human-labeled scores aggregated into precision/recall/F1; classifiers evaluated with accuracy, F1, averaged precision",
            "metric_definition": "Label scoring: 2=true, 1=partially true, 0=false for label0/1/2; label3 binary. Classifier outputs are probabilities used with tuned thresholds (Appendix A.11).",
            "dataset_or_benchmark": "DEERLET (labels derived directly from philosophy literature—Norton 2005, Salmon 1989)",
            "human_evaluation_details": "Annotation rubric provided in Appendix A.5; DEERLET annotated by the first author (expert); thresholds for classifiers tuned on DEERLET validation set.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Classifier performance (finetuned GPT-J) in Table 8: M2 acc 74.0, M3 acc 70.5, M4 acc 86.0, M5 acc 89.5; these modules improve downstream generation filtering quality.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Some subjectivity remains in labels; label annotation primarily by a single expert; 'reflects reality' is hard to assess without world interaction or experiments.",
            "uuid": "e8006.7",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "R+F baseline",
            "name_full": "R+F (Random Fill) baseline",
            "brief_description": "A non-neural baseline that constructs rules by randomly filling a provided rule template with sentences/phrases extracted from the input facts, used as a simple heuristic comparator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Non-neural heuristic baseline (no PLM)",
            "model_size": "N/A (non-neural)",
            "scientific_domain": "Baseline method for inductive rule generation evaluation",
            "theory_type": "Baseline rule-generation method",
            "evaluation_method_name": "Template random-fill generation",
            "evaluation_method_description": "Given a rule template, randomly fill template slots with content sampled from facts to produce candidate rules.",
            "evaluation_metric": "Evaluated with METEOR, WRecall, GREEN and human metrics",
            "metric_definition": "Reported metrics in Table 4: METEOR 11.20, WRecall 0.50, GREEN 2.37; human precision 9.0%, recall 100% (because majority class baseline), F1 0.17.",
            "dataset_or_benchmark": "DEER (generation), DEERLET (human eval labels)",
            "human_evaluation_details": "Human evaluation figures from Table 4 reflect low precision high recall behavior for R+F.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "R+F much worse than PLM baselines (e.g., M1 METEOR 25.28 vs R+F 11.20); human precision low (9.0%).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Toy baseline that ignores induction beyond template filling; high recall but trivial/uninformative outputs.",
            "uuid": "e8006.8",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "TF-IDF baseline",
            "name_full": "TF–IDF thresholding baseline",
            "brief_description": "A non-parametric baseline that scores fact–rule pairs by TF–IDF similarity and thresholds to predict rule correctness; used as a simple classifier baseline for M2–M5 tasks.",
            "citation_title": "A statistical interpretation of term specificity and its application in retrieval",
            "mention_or_use": "use",
            "model_name": "TF–IDF (classical information retrieval technique) applied to candidate rule–fact pairs",
            "model_size": "N/A",
            "scientific_domain": "Baseline classifier for rule–fact compatibility/generalization detection",
            "theory_type": "Baseline verification method",
            "evaluation_method_name": "TF–IDF similarity with tuned threshold",
            "evaluation_method_description": "Compute TF–IDF similarity score between fact(s) and rule; tune a threshold on DEERLET val set to split yes/no correctness predictions.",
            "evaluation_metric": "Accuracy, F1, averaged precision (for DEERLET classification tasks)",
            "metric_definition": "Reported in Table 8: TF-IDF achieved similar performance to majority-class baseline on DEERLET modules (thresholds best found were zero in experiments).",
            "dataset_or_benchmark": "DEERLET (training/validation for threshold tuning)",
            "human_evaluation_details": "TF–IDF thresholds tuned on DEERLET validation; resulted thresholds often trivial (zero) indicating low discriminative utility.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "TF-IDF baseline performed close to majority-class on DEERLET (e.g., accuracy 62.5 for M2 baseline), indicating limited utility for the classification tasks here.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "TF–IDF lacks semantic generalization and is insensitive to paraphrase; threshold tuning often degenerated to trivial values.",
            "uuid": "e8006.9",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Thresholding & Heuristics",
            "name_full": "Threshold tuning and heuristic filtering (token-length filter)",
            "brief_description": "Operational procedures used to decide when generated candidates are accepted by M2–M5: thresholds tuned on DEERLET validation (heuristics for selecting global/local optima) plus a 45-token minimum filter to avoid trivial generations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to classifier outputs from GPT-J (and other PLMs)",
            "model_size": "Classifier models primarily GPT-J (6B) for threshold calibration",
            "scientific_domain": "Operational evaluation/filtering of induced hypotheses",
            "theory_type": "Practical method for controlling selection precision vs recall",
            "evaluation_method_name": "Validation-set threshold selection and token-length heuristic",
            "evaluation_method_description": "On DEERLET validation, select global/local optimal thresholds maximizing accuracy/F1 while keeping thresholds away from extremes and recall in [0.7,0.9]; discard rules shorter than 45 tokens before passing to M2–M5.",
            "evaluation_metric": "Heuristics influence reported automatic/human metrics (precision/recall/F1, METEOR/WRecall/GREEN)",
            "metric_definition": "Threshold selection criteria detailed in Appendix A.11; 45-token minimum chosen based on observed triviality of short outputs.",
            "dataset_or_benchmark": "DEERLET (validation used for threshold tuning)",
            "human_evaluation_details": "Thresholds tuned to avoid filtering too many rules while ensuring modules reject low-quality outputs; heuristics intended to balance precision and recall.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Thresholding approach used to produce Table 4 results; token-length filter prevented trivial examples from contaminating DEERLET annotations.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Thresholds are heuristic and dataset-dependent; 45-token cutoff is an empirical heuristic that may not generalize; threshold tuning depends on DEERLET which itself is limited in size.",
            "uuid": "e8006.10",
            "source_info": {
                "paper_title": "Language Models as Inductive Reasoners",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments",
            "rating": 2
        },
        {
            "paper_title": "Bleu: a method for automatic evaluation of machine translation",
            "rating": 2
        },
        {
            "paper_title": "A little survey of induction",
            "rating": 2
        },
        {
            "paper_title": "Inductive logic programming",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Inductive relation prediction by subgraph reasoning",
            "rating": 1
        }
    ],
    "cost": 0.0227455,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Models as Inductive Reasoners</h1>
<p>Zonglin Yang ${ }^{\Delta *}$ Li Dong ${ }^{\diamond}$ Xinya Du ${ }^{\Delta}$ Hao Cheng ${ }^{\diamond}$<br>Erik Cambria ${ }^{\Delta}$ Xiaodong Liu ${ }^{\circ}$ Jianfeng Gao ${ }^{\circ}$ Furu Wei ${ }^{\diamond}$<br>${ }^{\Delta}$ Nanyang Technological University ${ }^{\circ}$ Microsoft Research<br>${ }^{\Delta}$ University of Texas at Dallas<br>{zonglin.yang, cambria}@ntu.edu.sg<br>{lidong1,cheng.hao,xiaodl,jfgao,fuwei}@microsoft.com<br>xinya.du@utdallas.edu</p>
<h4>Abstract</h4>
<p>Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2 k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language models as "reasoners". Moreover, we provide the first and comprehensive analysis of how well pretrained language models can induce natural language rules from natural language facts. We also propose a new framework drawing insights from philosophy literature for this task, which we show in the experiment section that surpasses baselines in both automatic and human evaluations. We discuss our future perspectives on inductive reasoning in detail in Section 7. Dataset and code are available at https://github.com/ ZonglinY/Inductive_Reasoning.</p>
<h2>1 Introduction</h2>
<p>Inductive reasoning is to reach to a hypothesis (usually a rule that explains an aspect of the law of nature) based on pieces of evidence (usually observed facts of the world), where the observations can not provide conclusive support to the hypothesis (Salmon, 1989). It is ampliative, which means</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>that the hypothesis supports more than mere reformulation of the content of the evidence (Norton, 2005). An example is shown in Table 1 that after observing three carnivorous plants each having a trapping structure, one might reach to a hypothesis (rule) that every carnivorous plant has a trapping structure. Inductive reasoning was firstly proposed by Aristotle in the 4th century B.C. in his Posterior Analytics (Aristotle, 1994). Since then it is used as a fundamental tool to obtain axioms, and therefore subjects can be developed from these axioms. It is also recognized as a core component of human intelligence (Mercier, 2018).</p>
<p>Past research works on inductive reasoning within computer science are investigated by Inductive Logic Programming (ILP) (Muggleton et al., 2012). ILP investigates the inductive construction of first-order logic (FOL) (Smullyan, 1995) rules from examples and background knowledge (Muggleton and Raedt, 1994). However, ILP uses formal language as representation and uses symbolic reasoner, which results in systematic disadvantages (Cropper et al., 2022). Specifically, ILP systems heavily rely on human effort, since it typically assumes that the input has already been preprocessed into symbolic declarative form, otherwise ILP systems cannot handle raw inputs such as natural language and images. In addition, ILP systems are very sensitive to label error and ambiguity in data, since the final induced rules are required to satisfy all input facts, and symbolic systems can not recognize different symbols with the same meaning (e.g. be capable of, be able to).</p>
<p>To overcome the challenges above, we present a novel paradigm for inductive reasoning based entirely on natural language, i.e., inducing natural language rules from natural language facts. In particular, we create a first-of-its-kind natural language inductive reasoning dataset named DEER containing 1.2 k rule-fact pairs (more details illustrated in $\S 3.1$ ). With this dataset, we investigate</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Short fact 1</th>
<th style="text-align: center;">Short fact 2</th>
<th style="text-align: center;">Short fact 3</th>
<th style="text-align: center;">Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The Venus flytrap is a carnivorous <br> plant native to subtropical wetlands <br> on the East Coast of the United States <br> in North Carolina and South Carolina. <br> It catches its prey-chiefly insects <br> and arachnids-with a trapping structure <br> formed by the terminal portion of each <br> of the plant's leaves, which is triggered <br> by tiny hairs on their inner surfaces.</td>
<td style="text-align: center;">Pitcher plants are several different <br> carnivorous plants which have modified <br> leaves known as pitfall traps-a prey <br> -trapping mechanism featuring a deep <br> cavity filled with digestive liquid. The <br> traps of what are considered to be "true" <br> pitcher plants are formed by <br> specialized leaves. The plants attract <br> and drown their prey with nectar.</td>
<td style="text-align: center;">Drosera, which is commonly known <br> as the sundews, is one of the largest genera <br> of carnivorous plants, with at least <br> 194 species. The trapping and digestion <br> mechanism of Drosera usually employs <br> two types of glands: stalked glands that <br> secrete sweet mucilage to attract and ensnare <br> insects and enzymes to digest them, and sessile <br> glands that absorb the resulting nutrient soup.</td>
<td style="text-align: center;">If a <br> plant is <br> carnivorous <br> , then it <br> probably <br> has a <br> trapping <br> structure.</td>
</tr>
</tbody>
</table>
<p>Table 1: An example of inductive reasoning in DEER dataset. We embolden the words in facts that contain the key information to induce this rule (just to explain the relation between facts and rule, in DEER there's no special word annotations for fact).
a modern approach to inductive reasoning where both facts and rules are in natural language, and pretrained language models (PLMs) are used as the inductive reasoner. Note that the inductive reasoning considered in this paper has several distinctions considered by other reasoning tasks over text (Clark et al., 2020; Bhagavatula et al., 2020; Sinha et al., 2019). We defer a more detailed discussion to $\S 2$.</p>
<p>With natural language as representation and PLMs as the reasoner, such an inductive reasoning system can avoid the systematic disadvantages of formal language and symbolic reasoners. Specifically, with natural language as representation, it can naturally handle raw input as natural language text. In addition, different from symbolic methods, PLMs contain knowledge via pretraining (Davison et al., 2019) and use embedding for concepts (Mikolov et al., 2013), making it less affected by input errors (Meng et al., 2021) and more robust to paraphrasing.</p>
<p>Based on the proposed dataset, we study the PLM's ability to induce (generate) natural language rules from natural language facts under different settings, such as different FOL rule types and topics with varying input facts and PLM model sizes.</p>
<p>We also propose a new framework for this task, named chain-of-language-models (CoLM) which is shown in Figure 1. It draws insights from the requirements of rule induction in philosophy literature (Norton, 2005). Specifically, CoLM consists of five modules all based on PLMs, where one model proposes rules (rule proposer M1), and the other four models (M2, M3, M4, M5) each classify whether a generated rule satisfies one particular requirement of induction. In our experiments, we find that our framework surpasses the baselines in terms of both automatic and human evaluations.</p>
<p>To sum up, our contributions are three-fold:</p>
<ul>
<li>We propose a new paradigm (task) of inducing
natural language rules from natural language facts, which naturally overcomes three systematic disadvantages of past works on inductive reasoning. In particular, we create a first-of-its-kind natural language inductive reasoning dataset DEER containing 1.2 k rule-fact pairs, where fact and rule are both written in natural language. New automatic metrics are also proposed for task evaluation, which shows strong consistency with human evaluation.</li>
<li>We provide the first and comprehensive analysis of how well PLMs can induce natural language rules from natural language facts.</li>
<li>Drawing insights from philosophy literature (Norton, 2005), we propose a framework for inductive reasoning. Empirically, we show that it surpasses baselines substantially in both automatic and human evaluations.</li>
</ul>
<p>In $\S 7$ we discuss our future perspectives on inductive reasoning in detail.</p>
<h2>2 Related Work</h2>
<p>Definition of Inductive Reasoning It is still under debate on the definition of inductive reasoning in philosophy research (Yang et al., 2023c). Here we adopt Flach and Kakas (2000)'s view that an inductive argument should satisfy (1) its premise cannot provide conclusive support to its conclusion since its conclusion amplify or go beyond the information found in their premises; (2) its conclusion generalize over its premise in a way that the conclusion can be applied to more instances other than instances mentioned in its premise. An example of inductive argument is that "if a white ball is found in a bag, then all balls in this bag are white." In this paper, we call the premises as "facts", and conclusions as "rules". Prior computa-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rule Template <br> (First Order Logic)</th>
<th style="text-align: center;">Rule Template <br> (Natural Language)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\forall z$, condition $(x) \Longrightarrow$ conclusion</td>
<td style="text-align: center;">If $\qquad$ then $\qquad$ .</td>
</tr>
<tr>
<td style="text-align: center;">$\exists z$, condition $(x) \Longrightarrow$ conclusion</td>
<td style="text-align: center;">There exists $\qquad$ which $\qquad$ .</td>
</tr>
<tr>
<td style="text-align: center;">$\forall z$, condition $[x][\wedge$ condition $(x)]^{+}$ <br> $\Longrightarrow$ conclusion</td>
<td style="text-align: center;">If $\qquad$ and $\qquad$ then $\qquad$ .</td>
</tr>
<tr>
<td style="text-align: center;">$\forall z$, condition $[x][\vee$ condition $(x)]^{+}$ <br> $\Longrightarrow$ conclusion</td>
<td style="text-align: center;">If $\qquad$ or $\qquad$ then $\qquad$ .</td>
</tr>
</tbody>
</table>
<p>Table 2: The mapping relation between basic first-order logic rule template and natural language rule template.
tional method for inductive reasoning is inductive logic programming, which is introduced in §A.13.</p>
<p>Inductive Reasoning \&amp; Neural Networks Sinha et al. (2019) propose CLUTRR dataset, but they do not focus on inducing explicit natural language rules. Instead they try to "learn" certain rules internally with PLMs, and use the PLMs to predict the correctness of other facts. Inductive relation induction task (Teru et al., 2020; Misra et al., 2022) focuses on prediction of relation that involves unseen entities, which only involves an induction from specific entities to specific entities, where we focus on the induction from specific entities or individual phenomenons to general knowledge. Yang and Deng (2021) also works on rule induction, but their induced rule is not in real natural language, and uses symbolic reasoners.</p>
<p>Relation with Other Reasoning Tasks The goal is quite different from (1) deductive reasoning as given facts and rules and reach to new facts (Clark et al., 2020) (2) abductive reasoning as given facts and finding the casual reasons (Bhagavatula et al., 2020). Rather, we want to induce rules that generalize over facts. Yang et al. (2023c) provide a comprehensive discussion on the difference between deductive, inductive, and abductive reasoning.</p>
<h2>3 Dataset Collection and New Metrics</h2>
<p>In this section, we discuss the data collection process for our proposed dataset, and our proposed metrics for automatic and human evaluation.</p>
<p>In general, we propose two datasets. The first one, named DEER (inDuctive rEasoning with natural languagE Representation), contains 1.2 k rulefact pairs, where rules are written by human annotators in English, and facts are existing English sentences on the web. The other one, named DEERLET (classification of inDucEd rulEs with natuRal LanguagE representaTion), including (fact, rule, label0, label1, label2, label3) tuples, where facts
are the same as in DEER, rules are generated output from PLMs, and label0/1/2/3 are classification labels describing different aspects of induced rules. Specifically, rules in DEERLET are collected from GPT-J (Wang and Komatsuzaki, 2021) using the in-context learning setting. We choose this setting because (1) GPT-J in this setting can generate reasonable rules, and (2) not all generated rules are correct so that the annotations on the generated rules can be used for fine-tuning. Overall, DEER is used as the main dataset for the task, and DEERLET is used to measure the classification performance of specific capabilities described in §3.2.</p>
<h3>3.1 Dataset Collection of DEER</h3>
<p>Collected by a human expert (the first author), DEER contains 1.2 k natural language rule-fact pairs where rules cover 6 topics and 4 common rule types of FOL. The 6 topics are zoology, botany, geology, astronomy, history, and physics. Shown in Table 2, sequentially the 4 FOL rule types are implications with universal quantifier, implications with existential quantifier, conjunctive implications with universal quantifier, and disjunctive implications with universal quantifier. In practice we collect rules with the natural language rule templates.</p>
<p>Natural language rule is firstly written by a human expert, then for each rule 6 supporting facts ( 3 long facts and 3 short facts) are collected from existing human-written text from commercial search engines and Wikipedia. Long facts are paragraphs collected from different web pages to for more difference, and short facts are core sentences selected from corresponding long facts. Each fact itself should contain enough information that is possible to induce the full corresponding rule (an example is shown in Table 1).</p>
<p>To validate the correctness of the DEER dataset, we randomly split DEER data to 4 subsets, and 4 graduate students manually check each of the subsets on whether each fact contains enough information that is possible to induce the given rule. The overall correctness of DEER is $95.5 \%$.</p>
<p>The reason that DEER is not larger is that it requires experts who are familiar enough with inductive reasoning and possesses a relatively high level of science knowledge to annotate.</p>
<h3>3.2 Dataset Collection of DEERLET</h3>
<p>DEERLET is a dataset collected by a human expert (the first author) in inductive reasoning for classification tasks to evaluate the specific capabil-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Generated rules <br> with top <br> $0 \% \sim$ top $0 \%$ <br> METEOR</th>
<th style="text-align: center;">Generated rules <br> with top <br> $10 \% \sim$ top $20 \%$ <br> METEOR</th>
<th style="text-align: center;">-.-</th>
<th style="text-align: center;">Generated rules <br> with top <br> $90 \% \sim$ top $100 \%$ <br> METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Weight</td>
<td style="text-align: center;">weight $_{0}(45)$</td>
<td style="text-align: center;">weight $_{1}(35)$</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">weight $_{9}(-45)$</td>
</tr>
<tr>
<td style="text-align: left;">Recall</td>
<td style="text-align: center;">recall $_{0}$</td>
<td style="text-align: center;">recall $1_{1}$</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">recall $_{9}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Illustration of the weights and recalls in WRecall, one of our proposed automatic evaluation metrics. Here weights reflect the importance of blocks of rules.
ities required by inductive reasoning. It contains 846 tuples with format (fact, rule, label0, label1, label2, label3). Among the tuples, 546 are used for training, 100 for validation, and 200 for testing. Here, facts are directly from DEER, and the corresponding rules are collected from PLMs. Label0 to label3 are classification labels evaluating specific aspects of the generated rules. The reason in DEERLET we collect rules from the generation of PLMs is that we want to avoid human annotation biases (Amidei et al., 2020).</p>
<p>We develop label $0 / 1 / 2$ based on the requirements of induced rules in philosophy literature (Norton, 2005), and develop label 3 based on a NLP aspect. In particular, label0 measures whether a rule is not in conflict with its fact; label1 measures whether a rule reflects reality; label2 measures whether a rule is more general than its fact, as inductive reasoning is "ampliative", and requires the induced rule to have higher coverage than facts (Norton, 2005). More details on label2 is illustrated in §A.10. Label3 measures whether a rule is not trivial (mostly incomplete sentence or the latter part is a repetition of its former part).</p>
<p>Inspired by Obeid and Hoque (2020), label 0/1/2 are annotated on a 3-point scale (true / partially true / false), and label 3 are annotated on a 2-point scale (true / false). More details on annotation of DEERLET are illustrated in §A.5.</p>
<h3>3.3 Adopted \&amp; New Evaluation Metrics</h3>
<h3>3.3.1 Human Evaluation Metric</h3>
<p>DEERLET provides human annotations for evaluation of the generated rules from four different aspects. Here we use precision / recall / f1, and the four aspects in DEERLET for human evaluation.</p>
<h3>3.3.2 Automatic Evaluation Metric</h3>
<p>For the DEER dataset, as it requires generating rules based on input facts, the first metric we adopt is METEOR (Banerjee and Lavie, 2005), which has been widely used for evaluating machine-
generated text quality. §A. 7 compares METEOR and BLEU (Papineni et al., 2002), and illustrates the reasons why METEOR should be a better metric for this task. More specifically, we calculate the averaged METEOR score of the generated rules (after filtering, if a model had a filtering phase). From the observation that even humans still constantly make mistakes on inductive reasoning, we assume any framework for this task might (but not necessarily) contain two phases as generation and filtering to obtain higher performance. However, if with a filtering phase, METEOR only considers the rules that are not filtered.</p>
<p>It makes the METEOR metric here a similar metric to "precision", as it only calculates the score for rules that are classified as "true". As a result, the model might have a low recall in that it might only keep the rule with the highest confidence score, and classify many reasonable good rules as "false".</p>
<p>To measure the "recall" of inductive reasoning models, we propose "weighted recall (WRecall)" as the second automatic evaluation metric for this task. The difficulty lies in that we don't have the ground truth labels for generated rules without human evaluation. To calculate WRecall, we make an assumption, which is that the higher METEOR a rule has, generally the higher probability it is a reasonable rule for given facts. This assumption is reasonable given the relatively high correlation coefficient between METEOR and human evaluation shown in §A.7. Specifically, as shown in table 3, we can first calculate the METEOR for each generated rule, and sort them based on the value of METEOR. Then we calculate the recall value for each block of generated rules, during which we assume only the rules in that block have "true" ground truth label. We also add a linearly changing weight for each block according to their importance. To ensure WRecall is in the range $[0,1]$, WRecall is linearly normalized:</p>
<p>$$
\text { WRecall }=\frac{\sum_{i=0}^{9} \text { weight }<em i="i">{i} * \text { recall }</em>
$$}+125}{250</p>
<p>Now that we have a METEOR metric that provides a similar measurement of "precision", and WRecall for "recall", we propose GREEN (GeometRic mEan of METEOR aNd WRecall) to consider METEOR and WRecall together. It is defined as a geometric mean instead of a harmonic mean because METEOR is not in the range $[0,1]$. More specifically,</p>
<p>$$
G R E E N=\sqrt{M E T E O R} * W R e c a l l
$$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our proposed framework (CoLM) for inductive reasoning with natural language representation task. Rule Proposer is a generative model based on input facts and desired rule template, aiming at generating (a large number of) rule candidates. Deductive consistency evaluator, indiscriminate confirmation handler, generalization checker, and triviality detector are classification models that filter improper rules according to four requirements of the induced rules in inductive reasoning. Texts with $\boldsymbol{x}$ are representative filtered rules for each module.</p>
<p>In general, compared with METEOR, GREEN gives a more comprehensive evaluation of the induced rules. Therefore GREEN can be a more favorable metric when the recall is an important factor (e.g., computational power is limited). However, when the precision of the induced rules is more favored, METEOR should be a more proper metric than GREEN. §A. 6 discusses more on the importance of each metric for this task. More discussions on the usage of automatic evaluation metrics and how should we interpret the results of automatic metrics can be found in §A.8.</p>
<h2>4 Methodology</h2>
<p>In this section, we formally present the task definition and our proposed framework for natural language inductive reasoning. Figure 1 illustrates the general architecture of our proposed approach.</p>
<h3>4.1 Task Definition</h3>
<p>DEER dataset is used as the dataset for the natural language inductive reasoning task. The data format for DEER is (rule, fact), where both rule and fact are natural language sentences. The goal of the task is to generate reasonable natural language rules given fact in an inductive reasoning way (the rules should be more general and therefore cover more information than fact).</p>
<h3>4.2 Our Framework</h3>
<p>Hypothetical Induction is an important induction type in inductive reasoning (Norton, 2005). It can be understood as when people make observations, they might propose a hypothesis as a general rule that can entail the observations. For example, when people observe that the Sun rises and falls every day, they might induce a hypothesis that the Earth is rotating itself, which is more general than the
observations as the hypothesis can also help to explain the observable movements of the other Milky Way stars relative to the Earth.</p>
<p>Hypothetical induction fits our task well, as in DEER we also want to induce a hypothesis as a more general rule that can entail the facts. We borrow insights from the requirements for the induced rules in hypothetical induction to develop our framework. Specifically, there are mainly three requirements (Salmon, 1989; Norton, 2005). The first is that a correct hypothesis should be able to entail deductively as many observations as possible. The second is that the hypothesis should follow the laws of nature, as one could always concoct some imaginary hypothesis that is able to explain the observations but violates reality (e.g., the Earth is the center of the Universe so that the Sun orbits around the Earth). In inductive reasoning, the failure to recognize a rule that runs counter to reality is called "indiscriminate confirmation". The third is a basic requirement for inductive reasoning, where the hypothesis should be a more general statement than the observations (Appendix A. 10 illustrates the meaning of "general"). We additionally introduce a fourth requirement from NLP aspects since this task uses natural language as knowledge representation. It is that a rule should not be trivial (e.g. incomplete sentence or the latter sub-sentence simply repeats its former sub-sentence).</p>
<p>More concretely, we define the requirements for designing our framework as 1) there should be as fewer contradictions between facts and the rule as possible, and 2) the rule should reflect the reality, 3) the content in facts should be relevant specific statements that are covered by the rule, 4) the rule should not be trivial.</p>
<p>Based on this, we develop our framework as shown in Figure 1. It consists of five modules,</p>
<p>where module 1 (M1) is the rule proposer, module 2 (M2) is the deductive consistency evaluator, module 3 (M3) is the indiscriminate confirmation handler, module 4 (M4) is the generalization checker, and module 5 (M5) is the triviality detector. Specifically, M1 is in charge of the generation of rules. M2, M3, M4, M5 are independent classification models each verifying rules with different requirement. The role of M2/3/4/5 is similar to the verifier developed for deductive reasoning to make more solid reasoning steps (Yang et al., 2022). The independence of M2/3/4/5 makes it possible to run them in parallel.</p>
<p>In practice, we implement all five modules with PLMs. We call our implementation as CoLM (Chain-of-Language-Models). The goal of M1 is to generate rules based on the input facts and a given rule template. Thus, M1's input contains facts, a rule template, and prompts that demonstrate the rule induction task.M2 and M4's inputs include prompts that explain the rule-fact compatibility, a rule, and fact(s); M3 and M5's inputs include again prompts that explain the task and a rule, as their targets are independent of fact.</p>
<p>More interestingly, although our framework is solely based on the insights from philosophy literature, we also find a mathematical interpretation of this approach. Here, we denote $P(A)$ as the probability indicating whether $A$ is valid for simplicity. Thus, M2 and M4 jointly measure the validness of a fact given the corresponding rule $P($ fact $\mid$ rule $) \approx P_{M 24}($ fact $\mid$ rule $)=$ $P_{M 2}($ fact $\mid$ rule $) P_{M 4}($ fact $\mid$ rule $)$, M3 and M5 directly measure the validness of the rule itself $P($ rule $) \approx P_{M 35}($ rule $)=$ $P_{M 3}($ rule $) P_{M 5}($ rule $)$. Here $P_{M 24}$ and $P_{M 35}$ are parameterized as the product of two corresponding probabilities. By using Bayes' rule, we can easily show that the validness of a rule based on the input fact is (here we omit constant $P($ facts $)$ )</p>
<p>$$
P(\text { rule } \mid \text { fact }) \approx P_{M 24}(\text { fact } \mid \text { rule }) P_{M 35}(\text { rule })
$$</p>
<p>Note that this score is merely a discrimination score and thus different from the generation probability from M1. In other words, the rules proposed by M1 are then selected by M2/3/4/5 in a Bayesian inference fashion.</p>
<h2>5 Experiments</h2>
<p>In this section, we discuss the evaluation metrics and baselines, and then present the main results of our framework (all are averaged by 5 runs).</p>
<h3>5.1 Evaluation Metrics</h3>
<p>We carry out evaluations for the framework (the rule generation task with DEER) and individual modules for classification using DEERLET.</p>
<p>For evaluation of the rule generation of the overall framework, we use METEOR, WRecall, and GREEN as automatic evaluation metrics; And use precision, recall, f1, and the four metrics in DEERLET as human evaluation metrics. WRecall, GREEN, and the four metrics in DEERLET are our newly proposed metrics for inductive reasoning introduced in §3.3.</p>
<p>For evaluation of the classification tasks on DEERLET, we use accuracy, f1, and averaged precision as metrics.</p>
<h3>5.2 Baselines</h3>
<p>We use a non-neural method and a neural method as baselines for the framework. We call the nonneural baseline "R+F", as it randomly fills the given rule template with sentences or phases from the given fact. The neural baseline we use is the rule proposer itself in Figure 1.</p>
<p>We use majority class and TF-IDF (Jones, 2004) as baselines for individual modules. The majority class baseline always predicts "yes", which is equivalent to not using M2/3/4/5 to filter rules from M1. TF-IDF is another reasonable baseline as the induced rules contain similar contents compared to input facts. In practice, each input fact-rule pair is assigned a TF-IDF value, and a threshold for correctness (to compare with the TF-IDF value) is tuned on the DEERLET validation set.</p>
<h3>5.3 Main Results</h3>
<p>Most modules are implemented with GPT-J (Wang and Komatsuzaki, 2021), a pre-trained language model with 6 billion parameters. Results on other LLMs such as LLaMA (Touvron et al., 2023) can be found in §A.9. For better analysis, we conduct the experiments in two settings, including incontext learning setting (Liu et al., 2021; Brown et al., 2020) and finetuning setting. The only exception is that we do not test finetuning setting on M1 (the only generative module), since we are mainly investigating (out-of-box) PLM's ability. However if with finetuning, language model might perform worse on out-of-distribution data and lose their generality for input facts from different topics (Kumar et al., 2022). For this reason we do not implement with T5 (Raffel et al., 2020).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">WRecall</th>
<th style="text-align: center;">GREEN</th>
<th style="text-align: center;">Precision (\%)</th>
<th style="text-align: center;">Recall (\%)</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Consistent</th>
<th style="text-align: center;">Reality</th>
<th style="text-align: center;">General</th>
<th style="text-align: center;">Non-trivial</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">R+F</td>
<td style="text-align: center;">11.20</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">2.37</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;">M1</td>
<td style="text-align: center;">25.28</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">3.56</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;">M1 + M2</td>
<td style="text-align: center;">25.68 / 25.69</td>
<td style="text-align: center;">0.53 / 0.54</td>
<td style="text-align: center;">3.68 / 3.71</td>
<td style="text-align: center;">45.9 / 59.8</td>
<td style="text-align: center;">87.8 / 71.1</td>
<td style="text-align: center;">0.60 / 0.65</td>
<td style="text-align: center;">0.63 / 0.75</td>
<td style="text-align: center;">0.62 / 0.72</td>
<td style="text-align: center;">0.83 / 0.92</td>
<td style="text-align: center;">0.86 / 0.94</td>
</tr>
<tr>
<td style="text-align: center;">M1 + M3</td>
<td style="text-align: center;">25.39 / 26.57</td>
<td style="text-align: center;">0.50 / 0.59</td>
<td style="text-align: center;">3.57 / 3.95</td>
<td style="text-align: center;">45.2 / 60.2</td>
<td style="text-align: center;">84.4 / 75.6</td>
<td style="text-align: center;">0.59 / 0.67</td>
<td style="text-align: center;">0.63 / 0.77</td>
<td style="text-align: center;">0.60 / 0.74</td>
<td style="text-align: center;">0.83 / 0.89</td>
<td style="text-align: center;">0.87 / 0.91</td>
</tr>
<tr>
<td style="text-align: center;">M1 + M4</td>
<td style="text-align: center;">26.12 / 26.30</td>
<td style="text-align: center;">0.53 / 0.58</td>
<td style="text-align: center;">3.74 / 3.92</td>
<td style="text-align: center;">$\mathbf{4 8 . 5 / 5 3 . 3}$</td>
<td style="text-align: center;">92.2 / 88.9</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 / 0 . 6 7}$</td>
<td style="text-align: center;">0.64 / 0.67</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 / 0 . 6 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 / 0 . 9 1}$</td>
<td style="text-align: center;">0.88 / 0.89</td>
</tr>
<tr>
<td style="text-align: center;">M1 + M5</td>
<td style="text-align: center;">25.28 / 25.76</td>
<td style="text-align: center;">0.50 / 0.54</td>
<td style="text-align: center;">3.55 / 3.74</td>
<td style="text-align: center;">46.1 / 48.1</td>
<td style="text-align: center;">97.8 / 97.8</td>
<td style="text-align: center;">0.63 / 0.65</td>
<td style="text-align: center;">0.64 / 0.66</td>
<td style="text-align: center;">0.61 / 0.63</td>
<td style="text-align: center;">0.83 / 0.83</td>
<td style="text-align: center;">0.88 / 0.91</td>
</tr>
<tr>
<td style="text-align: center;">CoLM</td>
<td style="text-align: center;">$\mathbf{2 6 . 4 4 / 2 7 . 3 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 / 0 . 6 2}$</td>
<td style="text-align: center;">$\mathbf{3 . 7 8 / 4 . 1 1}$</td>
<td style="text-align: center;">48.1 / 70.0</td>
<td style="text-align: center;">72.2 / 54.4</td>
<td style="text-align: center;">0.58 / 0.61</td>
<td style="text-align: center;">$\mathbf{0 . 6 5 / 0 . 8 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 / 0 . 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 / 0 . 9 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 / 0 . 9 7}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Result of CoLM and baselines on DEER under in-context learning / finetuning setting. The first three metrics are automatic metrics, and the last seven metrics are human evaluation metrics.</p>
<p>We report the results of in-context learning setting and finetuning setting in Table 4 and Table 8. The thresholds of M2/3/4/5 used in Table 4 and Table 8 are tuned on the DEERLET validation set. More details on setting up thresholds are illustrated in §A.11. The results on DEER are shown in Table 4. As expected, the M1 alone outperforms the R+F baseline across the board, indicating that the PLM has some rule induction capability. Augmenting the M1 with some filtering mechanism can reliably improve the generated rule quality further. Lastly, our full model, CoLM, outperforms all baselines justifying the effectiveness of our proposed framework for natural language inductive reasoning. Due to page limit, DEERLET results are analyzed in § A.2.</p>
<h2>6 Analysis</h2>
<p>In this section, we investigate the question of "how well can pretrained language models perform inductive reasoning?". Specifically, we provide analyses in terms of rule types, topics, variations of input fact, and scales of language models. Except for Table 7, the input used is short fact, 3 fact, full fact. Except for Table 2, the model used is GPT-J. All experiments in this section are based on the incontext learning setting, each averaged by 5 runs. Similar trends are also observed in other settings. We report METEOR and GREEN as metrics in this section. In addition to analyses with automatic evaluation results in this section, we also manually analyze the failure cases of CoLM in §A.3, by classifying error types and give a statistics on the percentage of the identified error types.</p>
<h3>6.1 Different Rule Types</h3>
<p>Table 5 shows the breakdown evaluation of CoLM based on four basic rule types in formal language (Russell and Norvig, 2020). The mapping between the logic forms and corresponding natural language templates can be found in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">$\mathrm{E} _$then $\qquad$</th>
<th style="text-align: center;">There exists $\qquad$ which $\qquad$</th>
<th style="text-align: center;">$\mathrm{E} _$and $\qquad$ then $\qquad$</th>
<th style="text-align: center;">$\mathrm{E} _$or $\qquad$ then $\qquad$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">R+F</td>
<td style="text-align: center;">9.87 / 2.22</td>
<td style="text-align: center;">17.45 / 2.95</td>
<td style="text-align: center;">10.63 / 2.30</td>
<td style="text-align: center;">12.53/ 2.50</td>
</tr>
<tr>
<td style="text-align: center;">M1</td>
<td style="text-align: center;">23.05 / 3.39</td>
<td style="text-align: center;">32.03 / 4.00</td>
<td style="text-align: center;">27.01 / 3.67</td>
<td style="text-align: center;">29.09 / 3.81</td>
</tr>
<tr>
<td style="text-align: center;">M1+M2</td>
<td style="text-align: center;">23.76 / 3.58</td>
<td style="text-align: center;">33.13 / 4.39</td>
<td style="text-align: center;">26.00 / 3.43</td>
<td style="text-align: center;">28.76 / 3.69</td>
</tr>
<tr>
<td style="text-align: center;">M1+M3</td>
<td style="text-align: center;">23.34 / 3.46</td>
<td style="text-align: center;">31.35 / 3.80</td>
<td style="text-align: center;">26.64 / 3.58</td>
<td style="text-align: center;">29.56 / 3.95</td>
</tr>
<tr>
<td style="text-align: center;">M1+M4</td>
<td style="text-align: center;">23.58 / 3.43</td>
<td style="text-align: center;">32.16 / 4.06</td>
<td style="text-align: center;">25.94 / 3.48</td>
<td style="text-align: center;">29.80 / 4.05</td>
</tr>
<tr>
<td style="text-align: center;">M1+M5</td>
<td style="text-align: center;">23.04 / 3.40</td>
<td style="text-align: center;">32.60 / 4.17</td>
<td style="text-align: center;">27.05 / 3.68</td>
<td style="text-align: center;">29.08 / 3.81</td>
</tr>
<tr>
<td style="text-align: center;">CoLM</td>
<td style="text-align: center;">24.15 / 3.55</td>
<td style="text-align: center;">32.50 / 4.16</td>
<td style="text-align: center;">26.41 / 3.58</td>
<td style="text-align: center;">29.60 / 3.96</td>
</tr>
</tbody>
</table>
<p>Table 5: Analysis of PLM (GPT-J)'s performance (measured in METEOR / GREEN) in with different rule templates.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Zoology</th>
<th style="text-align: center;">Botany</th>
<th style="text-align: center;">Astronomy</th>
<th style="text-align: center;">Geology</th>
<th style="text-align: center;">History</th>
<th style="text-align: center;">Physics</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">R+F</td>
<td style="text-align: center;">9.65 / 2.20</td>
<td style="text-align: center;">10.24 / 2.26</td>
<td style="text-align: center;">13.09 / 2.56</td>
<td style="text-align: center;">13.28 / 2.58</td>
<td style="text-align: center;">11.07 / 2.35</td>
<td style="text-align: center;">11.44 / 2.39</td>
</tr>
<tr>
<td style="text-align: left;">M1</td>
<td style="text-align: center;">28.88 / 3.80</td>
<td style="text-align: center;">31.14 / 3.95</td>
<td style="text-align: center;">34.40 / 4.15</td>
<td style="text-align: center;">27.71 / 3.72</td>
<td style="text-align: center;">22.17 / 3.33</td>
<td style="text-align: center;">20.01 / 3.16</td>
</tr>
<tr>
<td style="text-align: left;">M1+M2</td>
<td style="text-align: center;">$\mathbf{2 9 . 7 0 / 4 . 0 0}$</td>
<td style="text-align: center;">30.59 / 3.76</td>
<td style="text-align: center;">32.88 / 3.82</td>
<td style="text-align: center;">28.67 / 4.08</td>
<td style="text-align: center;">22.65 / 3.50</td>
<td style="text-align: center;">20.49 / 3.30</td>
</tr>
<tr>
<td style="text-align: left;">M1+M3</td>
<td style="text-align: center;">29.17 / 3.85</td>
<td style="text-align: center;">31.03 / 3.88</td>
<td style="text-align: center;">33.86 / 4.04</td>
<td style="text-align: center;">28.16 / 3.87</td>
<td style="text-align: center;">22.30 / 3.36</td>
<td style="text-align: center;">20.16 / 3.17</td>
</tr>
<tr>
<td style="text-align: left;">M1+M4</td>
<td style="text-align: center;">29.00 / 3.77</td>
<td style="text-align: center;">$\mathbf{3 1 . 5 4 / 4 . 0 6}$</td>
<td style="text-align: center;">34.17 / 4.28</td>
<td style="text-align: center;">28.63 / 4.04</td>
<td style="text-align: center;">$\mathbf{2 5 . 0 0 / 3 . 8 9}$</td>
<td style="text-align: center;">20.16 / 3.22</td>
</tr>
<tr>
<td style="text-align: left;">M1+M5</td>
<td style="text-align: center;">28.72 / 3.76</td>
<td style="text-align: center;">31.26 / 3.99</td>
<td style="text-align: center;">34.60 / 4.21</td>
<td style="text-align: center;">27.33 / 3.62</td>
<td style="text-align: center;">22.01 / 3.26</td>
<td style="text-align: center;">20.00 / 3.10</td>
</tr>
<tr>
<td style="text-align: left;">CoLM</td>
<td style="text-align: center;">29.25 / 3.84</td>
<td style="text-align: center;">31.00 / 3.86</td>
<td style="text-align: center;">$\mathbf{3 5 . 3 3 / 4 . 4 6}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 5 1 / 4 . 2 3}$</td>
<td style="text-align: center;">24.34 / 3.72</td>
<td style="text-align: center;">$\mathbf{2 0 . 6 7 / 3 . 3 0}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Analysis of PLM (GPT-J)'s performance (measured in METEOR / GREEN) in under different topics.</p>
<p>The table shows that "there exists $\qquad$ which $\qquad$ " achieves the best performance. It is reasonable, as simply copying the contents of facts to compose a rule will be acceptable for $\exists$ quantifier in logic.</p>
<h3>6.2 Different Topics</h3>
<p>Table 6 shows the performance of CoLM over different topics. CoLM performs much worse on History and Physics than the other topics. We attribute it to that the rules in history and physics have high variance, demand a higher level of abstraction, and are not very similar to the input facts. For example, in physics, many rules are natural language descriptions of physical laws such as Newton's law of universal gravitation, while the input facts might be the values of gravitational force and mass of specific objects. In contrast, CoLM achieves better performance in Botany. One possible reason is that many rules in botany can be very similar to the input facts (an example is shown in Table 1).</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Long facts <br> 1 full facts</th>
<th>Short facts <br> 1 full facts</th>
<th>Short facts <br> 2 full facts</th>
<th>Short facts <br> 3 full facts</th>
<th>Short facts <br> 3 missing facts</th>
</tr>
</thead>
<tbody>
<tr>
<td>B+P</td>
<td>9.35 / 2.16</td>
<td>10.87 / 2.33</td>
<td>11.16 / 2.36</td>
<td>11.20 / 2.37</td>
<td>11.52 / 2.40</td>
</tr>
<tr>
<td>M1</td>
<td>23.12 / 3.40</td>
<td>24.75 / 3.52</td>
<td>25.22 / 3.55</td>
<td>25.28 / 3.56</td>
<td>24.67 / 3.51</td>
</tr>
<tr>
<td>M1+M2</td>
<td>23.43 / 3.49</td>
<td>25.30 / 3.68</td>
<td>25.88 / 3.74</td>
<td>25.68 / 3.68</td>
<td>25.01 / 3.58</td>
</tr>
<tr>
<td>M1+M3</td>
<td>23.25 / 3.44</td>
<td>24.91 / 3.55</td>
<td>25.32 / 3.57</td>
<td>25.39 / 3.57</td>
<td>24.77 / 3.52</td>
</tr>
<tr>
<td>M1+M4</td>
<td>23.65 / 3.52</td>
<td>25.48 / 3.65</td>
<td>26.04 / 3.73</td>
<td>26.12 / 3.74</td>
<td>25.09 / 3.59</td>
</tr>
<tr>
<td>M1+M5</td>
<td>23.23 / 3.44</td>
<td>24.81 / 3.54</td>
<td>25.31 / 3.58</td>
<td>25.28 / 3.55</td>
<td>24.81 / 3.57</td>
</tr>
<tr>
<td>CoLM</td>
<td>$\mathbf{2 4 . 0 3 / 3 . 6 0}$</td>
<td>$\mathbf{2 5 . 8 9 / 3 . 7 3}$</td>
<td>$\mathbf{2 6 . 7 1 / 3 . 8 5}$</td>
<td>$\mathbf{2 6 . 4 4 / 3 . 7 8}$</td>
<td>$\mathbf{2 5 . 4 1 / 3 . 6 5}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Analysis of PLM (GPT-J)'s performance (measured in METEOR / GREEN) with different input lengths and whether fact contains enough information.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Influence of the scale of PLM on inductive reasoning task with DEER (measured with METEOR).</p>
<h3>6.3 Variations of Input Facts</h3>
<p>In table 7, long facts mean the paragraph-level facts in DEER, and short facts mean the core sentence-level facts selected from corresponding paragraph-level facts. The different number of facts indicates the different number of facts given as input that exhibit similar rule patterns (e.g. Lemon tree / orange tree / apple tree can conduct photosynthesis). We consider the number of facts as an important factor because psychological research shows that more facts with similar patterns can help with inductive reasoning (Heit, 2000). Missing fact experiments are also conducted, where for each fact we randomly throw the former half or the latter half of the sentences. It is an important setting as it is hard for the input facts to cover all the elements of the desired rule in a realistic scenario. As a result, it might be common that some required pieces of fact are missing. The results indicate that larger number of concise but full facts are beneficial for rule induction, while too many facts with similar patterns might not be helpful.</p>
<h3>6.4 Different Scales of PLMs</h3>
<p>Figure 2 shows the influence of the scale of pretrained language models (under in-context learning setting) on induction. Here, we consider GPT-Neo 125M, GPT-Neo 1.3B, GPT-Neo 2.7B, GPT-J</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Error Analysis of CoLM with finetuned Module 2/3/4/5. In total 100 rules are manually checked.</p>
<p>6B and GPT-NeoX 20B (Wang and Komatsuzaki, 2021). The figure shows that generally performance of M1 steadily improves as the scale being larger, and M2/3/4/5 are only helpful since 6B parameters. The only exception is that both M1 and M2/3/4/5 might reach a plateau in 20B parameters.</p>
<h3>6.5 Error Analysis</h3>
<p>We sampled 100 rules from CoLM (rules that generated by M1 and pass all M2/3/4/5), and have conducted an error analysis of the samples. Figure 3 shows the results. Among them, "Conflict with Facts", "Not Fits Commonsense (not reflects reality)", "Not General", and "Trivial" corresponds to the rules that should be filtered by CoLM but not. We find that beyond "Correct" and errors made by classification modules, there are also some other classes that worth mentioning, but they could be seen as other kinds of "Trivial". This figure shows that the four criteria we proposed are important for verification. More details about error analysis can be found at § A.3.</p>
<h3>7 Overview and Future Perspectives of Inductive Reasoning</h3>
<p>The first version of this paper was finished in 2022. At that time, inductive reasoning—in the sense of deriving explicit natural language hypotheses (rules) from observations (input facts), where the hypotheses and observations adhere to specific relations defined by induction—was a new and unexplored research area.</p>
<p>Previously, the most closely related works came from the ILP (Inductive Logic Programming) community, which focused on symbolic approaches to</p>
<p>the task of inductive reasoning (inducing explicit formal language hypotheses). This paper aims to act as a bridge between the ILP and NLP communities by (1) demonstrating how natural language and related techniques (foundation models) can address challenges within the ILP community, and (2) introducing the definition and task of inductive reasoning to NLP. Moreover, this paper can serve as a preliminary study, suggesting that language models have the potential to function as inductive reasoners. The transcription of requirements for inductive arguments from philosophical literature, as illustrated in Section 4.2, could remain useful even in the era of powerful LLMs.</p>
<p>The possible future challenges of research on inductive reasoning include (1) establishing and solving more challenging tasks for inductive reasoning, and (2) overcoming the fundamental challenges inherent in induction.</p>
<h3>7.1 Establishing and Solving More Challenging Tasks for Inductive Reasoning</h3>
<p>A naturally more challenging task is scientific hypotheses discovery, which is to generate novel and valid scientific hypotheses. Here, "novel" means "not known or recognized by any literature". In fact, inductive reasoning is one of the primary types of reasoning in the development of science. Essentially, scientists use inductive reasoning whenever they move from limited data to a more general conclusion (Okasha, 2002). Thus, exploring how to generate preliminary hypotheses (a.k.a. research ideas) and possibly act as a "copilot" for scientists could be an intriguing research topic. Yang et al. (2023b) extend inductive reasoning to the task of scientific hypothesis discovery, demonstrating that LLMs can generate novel and valid hypotheses in some social science disciplines. However, there are still many challenging questions to address, such as how to develope a system for other disciplines.</p>
<p>Another challenging task is pattern induction, which is to induce (executable) rules/patterns from complex (synthetic) facts. This task currently encompass (1) identifying patterns in a sequence of numbers (Qiu and Jiang, 2023), (2) discerning arithmetic calculation patterns (Zhu et al., 2023), and (3) detecting change patterns of 2D grid images (Wang et al., 2023b). The term "executable" is used here because many of these patterns can be described in the form of program. An advantage of pattern induction tasks is that challenging datasets can
be efficiently constructed using synthetic methods. This direction is also interesting as it can aid in understanding the inductive reasoning capabilities of LLMs and requires a combination of this understanding with the ability to generate program.</p>
<h3>7.2 Overcoming Fundamental Challenges Inherent in Induction</h3>
<p>This challenge stems from certain fundamental requirements for the induced rules. As illustrated in Section 4.2, some of these requirements include</p>
<ul>
<li>Checking whether the induced rule accurately reflects reality.</li>
<li>Determining whether the hypotheses are more general than the observations.</li>
</ul>
<p>Here, the "reflects reality" in the first requirement refers to whether the rule mirrors the objective world (or the environment of the task). In certain task settings, such as scientific hypothesis discovery, verifying whether an induced hypothesis mirrors the objective world can be very challenging, given that LLMs do not directly interact with the world. To ascertain the validity of the hypotheses, LLMs might need to utilize tools to conduct actual experiments to test the induced hypotheses. In other tasks, such as pattern induction, meeting this requirement could be much simpler, as whether it catches the designed patterns can be examined by executing the program and checking whether it produces the expected output.</p>
<p>The second requirement can be interpreted as "whether the hypothesis is novel compared to the all existing literature" in the task of scientific hypothesis discovery (Yang et al., 2023b). Meeting this requirement involves key challenges including information retrieval and novelty checking.</p>
<h2>8 Conclusion</h2>
<p>To overcome the systematic problems of using formal language for inductive reasoning, we propose a new paradigm (task) of inducing natural language rules from natural language facts, and correspondingly propose a dataset DEER and new evaluation metrics for this task. We provide the first and comprehensive analysis of PLM's ability to induce natural language rules from natural language facts. We also propose a new framework, drawing insights from philosophical literature, which, as shown in the experimental section, surpasses baselines in both automatic and human evaluations.</p>
<h2>Limitations</h2>
<p>In this work, the size of dataset (DEER) contains 1.2 k fact-rule pairs, which is relatively small. The reason is that the "rules" in this task are required to be very general. It is not easy to collect a large set of such rules in high-quality. Additionally, a rule can be collected only if (1) there are several facts findable in online texts, and (2) these facts satisfy certain relation with the rule required by induction (the rule generalizes over the facts).</p>
<p>In addition, the DEER dataset mainly covers commonsense knowledge. A successive work to this paper (Yang et al., 2023b) focuses on a more challenging setting of inductive reasoning, which is to generate novel and valid scientific hypotheses (e.g., Newton's Laws are scientific hypotheses). Here novel is defined as "not known or recognized by any literature", which means this new setting is very challenging even for the most advanced LLMs.</p>
<h2>Acknowledgments</h2>
<p>This research/project is supported by the Ministry of Education, Singapore under its MOE Academic Research Fund Tier 2 (STEM RIE2025 Award MOE-T2EP20123-0005).</p>
<p>We sincerely appreciate the anonymous reviewers who have given careful reviews to this paper, and the anonymous chair who have closely looked into the this paper.</p>
<h2>References</h2>
<p>Jacopo Amidei, Paul Piwek, and Alistair Willis. 2020. Identifying annotator bias: A new irt-based method for bias identification. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4787-4797. International Committee on Computational Linguistics.</p>
<p>Aristotle Aristotle. 1994. Posterior analytics, volume 1. Clarendon Press Oxford, UK.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, pages 65-72. Association for Computational Linguistics.</p>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin</p>
<p>Choi. 2020. Abductive commonsense reasoning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages 3882-3890. ijcai.org.</p>
<p>Andrew Cropper, Sebastijan Dumancic, Richard Evans, and Stephen H. Muggleton. 2022. Inductive logic programming at 30. Mach. Learn., 111(1):147-172.</p>
<p>Wang-Zhou Dai and Stephen H. Muggleton. 2021. Abductive knowledge induction from raw data. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 1845-1851. ijcai.org.</p>
<p>Rajarshi Das, Ameya Godbole, Ankita Naik, Elliot Tower, Manzil Zaheer, Hannaneh Hajishirzi, Robin Jia, and Andrew McCallum. 2022. Knowledge base question answering by case-based reasoning over subgraphs. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 4777-4793. PMLR.</p>
<p>Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. 2021. Casebased reasoning for natural language queries over knowledge bases. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 9594-9611. Association for Computational Linguistics.</p>
<p>Joe Davison, Joshua Feldman, and Alexander M. Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages</p>
<p>1173-1178. Association for Computational Linguistics.</p>
<p>Luc De Raedt. 2010. Inductive logic programming.
Peter A Flach and Antonis C Kakas. 2000. Abductive and inductive reasoning: background and issues. In Abduction and induction, pages 1-27. Springer.</p>
<p>Evan Heit. 2000. Properties of inductive reasoning. Psychonomic Bulletin \&amp; Review, 7(4):569-592.</p>
<p>Karen Spärck Jones. 2004. A statistical interpretation of term specificity and its application in retrieval. J. Documentation, 60(5):493-502.</p>
<p>Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. 2022. Finetuning can distort pretrained features and underperform out-of-distribution. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Yiyang Li and Hai Zhao. 2023. EM pre-training for multi-party dialogue response generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 92-103. Association for Computational Linguistics.</p>
<p>Yufei Li, Zexin Li, Yingfan Gao, and Cong Liu. 2023. White-box multi-objective adversarial attack on dialogue generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1778-1792. Association for Computational Linguistics.</p>
<p>Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael D. Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 2122-2132. The Association for Computational Linguistics.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. CoRR, abs/2107.13586.</p>
<p>Yu Meng, Yunyi Zhang, Jiaxin Huang, Xuan Wang, Yu Zhang, Heng Ji, and Jiawei Han. 2021. Distantlysupervised named entity recognition with noiserobust learning and language model augmented selftraining. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 1036710378. Association for Computational Linguistics.</p>
<p>Hugo Mercier. 2018. The enigma of reason. In The enigma of reason. Harvard University Press.</p>
<p>Tomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111-3119.</p>
<p>Kanishka Misra, Julia Taylor Rayz, and Allyson Ettinger. 2022. A property induction framework for neural language models. CoRR, abs/2205.06910.</p>
<p>Stephen H. Muggleton and Luc De Raedt. 1994. Inductive logic programming: Theory and methods. J. Log. Program., 19/20:629-679.</p>
<p>Stephen H. Muggleton, Luc De Raedt, David Poole, Ivan Bratko, Peter A. Flach, Katsumi Inoue, and Ashwin Srinivasan. 2012. ILP turns 20 - biography and future challenges. Mach. Learn., 86(1):3-23.</p>
<p>John D Norton. 2005. A little survey of induction.
Jason Obeid and Enamul Hoque. 2020. Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model. In Proceedings of the 13th International Conference on Natural Language Generation, INLG 2020, Dublin, Ireland, December 15-18, 2020, pages 138-147. Association for Computational Linguistics.</p>
<p>Samir Okasha. 2002. Philosophy of science: A very short introduction, volume 67. Oxford Paperbacks.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.</p>
<p>Linlu Qiu and Liwei Jiang. 2023. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Stuart Russell and Peter Norvig. 2020. Artificial Intelligence: A Modern Approach (4th Edition). Pearson.</p>
<p>Merrilee H Salmon. 1989. Introduction to logic and critical thinking.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4505-4514. Association for Computational Linguistics.</p>
<p>Raymond M Smullyan. 1995. First-order logic. Courier Corporation.</p>
<p>Komal K. Teru, Etienne G. Denis, and William L. Hamilton. 2020. Inductive relation prediction by subgraph reasoning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9448-9457. PMLR.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. 2023a. Learning to generate novel scientific directions with contextualized literature-based discovery. arXiv preprint arXiv:2305.14259.</p>
<p>Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D. Goodman. 2023b. Hypothesis search: Inductive reasoning with language models. CoRR, abs/2309.05660.</p>
<p>Kaiyu Yang and Jia Deng. 2021. Learning symbolic rules for reasoning in quasi-natural language. CoRR, abs/2111.12038.</p>
<p>Kaiyu Yang, Jia Deng, and Danqi Chen. 2022. Generating natural language proofs with verifier-guided search. CoRR, abs/2205.12443.</p>
<p>Zonglin Yang, Xinya Du, Erik Cambria, and Claire Cardie. 2023a. End-to-end case-based reasoning for commonsense knowledge base completion. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 3491-3504. Association for Computational Linguistics.</p>
<p>Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. 2023b. Large language models for automated open-domain scientific hypotheses discovery. CoRR, abs/2309.02726.</p>
<p>Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, and Erik Cambria. 2023c. Logical reasoning over natural language as knowledge representation: A survey. CoRR, abs/2303.12023.</p>
<p>Zonglin Yang, Xinya Du, Alexander M. Rush, and Claire Cardie. 2020. Improving event duration prediction via time-aware pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 3370-3378. Association for Computational Linguistics.</p>
<p>Kun Zhao, Bohao Yang, Chenghua Lin, Wenge Rong, Aline Villavicencio, and Xiaohui Cui. 2023. Evaluating open-domain dialogues in latent space with next sentence prediction and mutual information. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 562-574. Association for Computational Linguistics.</p>
<p>Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. 2023. Goal driven discovery of distributional differences via language descriptions. arXiv preprint arXiv:2302.14233.</p>
<p>Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. 2023. Large language models can learn rules. arXiv preprint arXiv:2310.07064.</p>
<h2>A Appendix</h2>
<h2>A. 1 Hyperparameters</h2>
<p>For finetuning experiments, we use learning rate 1e-5; weight decay 0.1 ; adam epsilon $1 \mathrm{e}-8$; batch size 4 ; and early stopping with accuracy as the metric. We perform our experiments on RTXA6K GPU. We use nltk package to calculate BLEU and METEOR.</p>
<h2>A. 2 DEERLET Results</h2>
<p>The results on DEERLET are summarized in Table 8. In this experiment, we investigate the classification performance of language models in terms of different aspects required by inductive reasoning, which includes deductive consistency, indiscriminate confirmation, and generalization / triviality classification. It shows that TF-IDF achieves the same performance with majority class baseline in accuracy and f1 metrics. The reason is that the best thresholds obtained for TF-IDF are all zero, which means that TF-IDF value is not effective for the four tasks. It also shows that with in-context learning GPTJ performs worse than the majority class baseline, while finetuned GPTJ steadily performs better.</p>
<h2>A. 3 Failure Analysis</h2>
<p>We sampled 100 rules from CoLM (rules that generated by M1 and pass all M2/3/4/5), and have conducted an error analysis of the samples. Figure 3 shows the results.</p>
<p>Among them, "Conflict with Facts", "Not Fits Commonsense (not reflects reality)", "Not General", and "Trivial" corresponds to the rules that should be filtered by CoLM but not. However,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">Accuracy (\%)</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Averaged Precision</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Deductive Consistency Evaluator (M2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Majority class</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: center;">TF-IDF</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">$61.5 / 74.0$</td>
<td style="text-align: center;">$0.71 / 0.83$</td>
<td style="text-align: center;">$0.75 / 0.83$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Indiscriminate Conformation Handler (M3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Majority class</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: center;">TF-IDF</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">$56.0 / 70.5$</td>
<td style="text-align: center;">$0.57 / 0.77$</td>
<td style="text-align: center;">$0.66 / 0.79$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generalization Checker (M4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Majority class</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">TF-IDF</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">$71.0 / 86.0$</td>
<td style="text-align: center;">$0.82 / 0.92$</td>
<td style="text-align: center;">$0.87 / 0.97$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Triviality Detector (M5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Majority class</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;">TF-IDF</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">$78.5 / 89.5$</td>
<td style="text-align: center;">$0.87 / 0.94$</td>
<td style="text-align: center;">$0.89 / 0.94$</td>
</tr>
</tbody>
</table>
<p>Table 8: Results on DEERLET for different modules under in-context learning / finetuning settings.
we find that beyond "Correct" and errors made by classification modules, there are also some other classes that worth mentioning.
"Correct but less informative" means some facts that is not trivial (by our former description of triviality - incomplete sentences or the conclusion simply repeats some part of premises.), not incorrect, but not very informative. Examples include "if a bird can help a plant to reproduce, then it is probably a good thing for the plant", and "if a land is green, then it probably contains forests".
"Correct but not very related" means although the rule is correct, but it is not very related to the facts given. For example, the facts are only about the depth and shape of Marianas Trench, while the rule is "if there exists a place with a greater depth, then it is possible to find something strange and interesting" (the "find something strange and interesting" aspect is not mentioned in facts).
"Correct but not completely" means the rule is somewhat to mostly correct, such as "if a fruit has a strong smell, then it probably tastes good" (while facts are about durian, champedek, and morinda citrifolia); "if an economy is based on textiles, then it might experience an industrial revolution" (this rule is only true during a specific period of time in history); "if a wire moves, then it might induce voltage in the conductor" (this rule is only true if given magnetic fields).
"Meaningless" means the rule is from a strange angle and it's hard to justify whether it is correct or not, such as "if an event has a positive impact on
an individual and on family, then the impact on the family is greater", and "if a man has experienced hardships and life has been tough, then he might be able to understand and change his ways in the future".</p>
<h2>A. 4 More Details on Difference with Other Reasoning Tasks</h2>
<p>In this paper, we strictly follows the definition and categorization of logical reasoning (including deductive, inductive, and abductive reasoning) in a survey of logical reasoning (Yang et al., 2023c).</p>
<p>There have been some NLP works on case-based reasoning (Das et al., 2021, 2022; Yang et al., 2023a), which can also be seen as inductive reasoning. However, CBR is a different inductive reasoning type than the "generalization" process (from facts to rules) described in Flach and Kakas (2000), but more on the general description on inductive reasoning (Salmon, 1989) that premises cannot conclusively provide support to the conclusion.</p>
<p>Inductive reasoning is also different from commonsense reasoning (Yang et al., 2020), where commonsense reasoning focuses more on the "knowledge" aspect, and inductive reasoning focuses more on the "reasoning" aspect (Yang et al., 2023c).</p>
<h2>A. 5 Annotation Details for DEERLET</h2>
<p>In DEERLET, given fact(s) and a rule, the annotation targets are whether the rule satisfies four requirements.</p>
<p>Specifically, the requirements are "if the rule is deductively consistent with the fact", "if the rule reflects reality", "if the rule is more general than the fact", and "if the rule is not trivial".</p>
<p>The first three requirements are annotated on a 3-point scale (true / partially true / false), and the last is annotated on a 2-point scale (true / false).</p>
<p>Here we explain the standards of annotation on the four requirements.</p>
<p>For "if the rule is deductively consistent with the fact", a 2-point will be assigned if the rule is totally relevant and consistent with the facts; a 1-point will be assigned if the rule introduces new information that does not show in facts but is consistent with the given fact as well as some limited amount of commonsense knowledge related to the facts; a 0-point will be assigned if the rule is (1) in conflict with given facts or (2) totally irrelevant to given facts or (3) introduces new information that is obviously wrong.</p>
<p>For "if the rule reflects reality", a 2-point will be assigned if the rule totally reflects reality; a 1-point will be assigned if the rule reflects reality at most of the time; a 0 -point will be assigned if (1) the rule is totally incorrect or (2) the rule is only occasionally correct.</p>
<p>For "if the rule is more general than the fact", a 2point will be assigned if (1) the rule is more general than the facts or (2) it is obvious that the rule is trying to be more general than the facts; a 1-point will be assigned if (1) it is even hard for humans to induce a more general rule from the given facts or (2) the rule copies part of the given facts that are already containing very general information; a 0 -point will be assigned if (1) from the facts it's easy for humans to induce a more general rule but the rule is not more general or (2) the rule is totally irrelevant to the facts.</p>
<p>For "if the rule is not trivial", a 0-point will be assigned if (1) the rule is an incomplete sentence or (2) the latter sub-sentence of the rule only repeats the information in the former sub-sentence of the rule; otherwise, a 1-point will be assigned.</p>
<h2>A. 6 METEOR or GREEN?</h2>
<p>Since inductive reasoning over natural language is a new task, and new metrics are designed (e.g., WRecall, GREEN), it is important to understand which aspects each metric focus on and which metric should we pay more attention to.</p>
<p>As mentioned in $\S 3.3$, METEOR can be seen as evaluating the "precision" of the final rules, while GREEN evaluates "precision" and "recall" at the same time.</p>
<p>However, it should be aware that the "recall" here is not as important as the "recall" in other tasks. More specifically, here "recall" measures how many good rules generated by M1 are filtered by M2/3/4/5. However, we can use M1 to generate a large number of rules, and as long as CoLM has good precision, it is easy to obtain a large number of high-quality rules, especially considering that the computational cost of only inference of M1 is relatively very low.</p>
<p>Based on this observation, we argue that "precision" should be a much more important aspect of evaluation compared to "recall" (measured by WRecall) or even "f1" (measured by GREEN) for this task. More specifically, "recall" can be used to mainly measure at what efficiency can the system obtain rules with high precision.</p>
<p>This viewpoint of evaluation metrics, of course,
can raise the question of whether some typical kinds of rules are mostly filtered when pursuing rules with high precision, and in the end inductive reasoning system with high precision might only be able to obtain some other typical kinds of rules. We leave this question as an open question for this task to solve in the future.</p>
<h2>A. 7 Why METEOR not BLEU</h2>
<p>We choose METEOR since METEOR has a higher correlation coefficient with human evaluation than BLEU.</p>
<p>More specifically, on DEERLET, we calculate the METEOR and BLEU for each generated rule with its golden rule in DEER and collect the human evaluation for the generated rule from label0/1/2/3 annotations in DEERLET (we normalize each label to $[0,1]$ and use the product of label0/1/2/3 as the overall human evaluation score for the generated rule). Then, we can calculate the correlation coefficient between METEOR / BLEU and the overall human evaluation score.</p>
<p>On DEERLET, the correlation coefficient between METEOR and human evaluation is 0.29 , it is statistically significant as its $p$-value is $4.48 * 10^{-6}$, smaller than the significance level ( 0.05 ). Similarly, the correlation coefficient between BLEU and human evaluation is 0.24 , with $p$-value of $1.17 * 10^{-72}$, which is also significant.</p>
<p>We called 0.29 relatively high since in other open-ended NLP tasks such as dialogue systems, the Pearson correlation is typically only around 0.140 .19 (shown in Table 3 in (Liu et al., 2016), BLEU's Pearson correlation is lower than METEOR's in most of the time). However recent papers published in ACL 2023 on dialogue systems still adopt METEOR or BLEU as automatic evaluation metrics (Li and Zhao, 2023; Zhao et al., 2023; Li et al., 2023).</p>
<p>Developing better metrics for measuring the similarity between sentences is a challenging topic in NLP. Of course, METEOR is not a "perfect" automatic evaluation metric for inductive reasoning. We leave the question of "what is a better metric for inductive reasoning over natural language" as an open question for future works in the field.</p>
<p>One good thing is that WRecall and GREEN can be applied with many metrics measuring sentence similarity such as METEOR and BLEU, so the evaluation of "recall" should be able to also benefit from the advance of metrics that evaluate "precision".</p>
<h2>A. 8 Difficulty in Designing Automatic Evaluation Metrics for Inductive Reasoning Tasks and How Should We Interpret the Results of Automatic Metrics</h2>
<p>Designing automatic evaluation methods for inductive reasoning is fundamentally difficult, mainly because of two reasons. Firstly, generalizing over existing facts is not restricted in a single way. Given existing facts, multiple rules that are very diverse from each other could all be true. Secondly, when it comes to more difficult inductive reasoning data, it is nearly inevitable to use long sentences for facts and rule, which make it even harder for common evaluation metrics such as BLEU or METEOR.</p>
<p>However, we argue that although we don't have perfect automatic evaluation metrics for inductive reasoning now, it is not a reason to stop exploring research on inductive reasoning. In fact, with the fast development of LLMs, more difficult tasks are needed to further explore the scientific boundary in NLP, and many recently proposed tasks are so difficult to be evaluated with automatic evaluation metrics that they fully rely on human evaluation (Zhong et al., 2023; Wang et al., 2023a). In terms of human evaluation metrics, we also have proposed meaningful human evaluation metrics for inductive reasoning tasks shown in the last four columns in Table 4, which are derived from philosophy literature (the four requirements for induced rules, and the four requirements are also used to develop the CoLM framework).</p>
<p>The reason we try to propose suitable automatic evaluation metrics is that we hope to simplify the evaluation process for the inductive reasoning task (at least for preliminary evaluations). We have illustrated why these metrics should be reasonable in $\S$ A. 6 and $\S$ A.7. Similar to inductive reasoning, abductive reasoning also have multiple diverse correct generations, however abductive reasoning generation task also utilizes METEOR or BLEU (Bhagavatula et al., 2020) as automatic metrics. In the future, the automatic metrics are possible to be further improved with the help of the community. While for now, just like other recent difficult tasks (Zhong et al., 2023; Wang et al., 2023a), human evaluations are always preferred, but automatic evaluation metrics, though not perfect, can still be used as a fast evaluation metrics that can provide some insights for experiments.</p>
<h2>A. 9 Results on Other LLMs</h2>
<p>Table 9 shows the results of CoLM using LLaMA, under in-context learning setting. Overall, CoLM outperforms all baselines, but the gap between M1 and CoLM are smaller. The reason is that LLaMA tends to generate very sound rules, thus the M2/3/4/5 of CoLM barely filter any rules. Therefore the results of CoLM and M1 are closer. We think there are two reasons: (1) with the fast development of LLMs, our proposed dataset is less challenging for more recent LLMs such as LLaMA; (2) M2/3/4/5 instantiating with LLaMA have not been finetuned, but just in-context learning setting. Given that finetuned GPT-J largely improves GPTJ under in-context learning setting in Table 4, a finetuned LLaMA should be able to filer more unreasonable generations.</p>
<p>While our work takes the first step to inductive reasoning in NLP and provide the first analysis, introducing more challenging inductive reasoning benchmarks would be beneficial to the the further development of the inductive reasoning field in NLP.</p>
<h2>A. 10 Meaning of "More General" Required by Inductive Reasoning</h2>
<p>Given an argument consisting of a premise and a conclusion, if the conclusion involves new information that is not covered by the premise and can not be conclusively entailed by the premise, the argument is an inductive argument (Salmon, 1989).</p>
<p>When the conclusion has a larger scope of information coverage than the premise, and can entail the premise, it can be said that the conclusion is "more general" to the premise. In this case, we termed the premise as a "fact", and the conclusion as a "rule"; When the conclusion contains new pieces of information and cannot entail the premise, as defined by Salmon (1989), the argument is still an inductive argument. But in this case, we termed the premise as a "fact", and the conclusion as another "fact".</p>
<p>For instance, if facts that are about cats and dogs are good accompaniment of humans, then some examples of a "more general" rule can be (1) mammals are good accompaniment of humans, or (2) domesticated animals are good accompaniment of humans, or (3) animals with four legs are good accompaniment of human.</p>
<p>In these examples, the rules cover a larger scope than the facts (e.g., mammals compared to cats;</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">LLaMA-7B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{R}+\mathrm{F}$</td>
<td style="text-align: left;">$11.20 / 2.37$</td>
</tr>
<tr>
<td style="text-align: left;">M1</td>
<td style="text-align: left;">$24.94 / 3.53$</td>
</tr>
<tr>
<td style="text-align: left;">M1+M2</td>
<td style="text-align: left;">$25.12 / 3.54$</td>
</tr>
<tr>
<td style="text-align: left;">M1+M3</td>
<td style="text-align: left;">$24.77 / 3.49$</td>
</tr>
<tr>
<td style="text-align: left;">M1+M4</td>
<td style="text-align: left;">$25.42 / 3.60$</td>
</tr>
<tr>
<td style="text-align: left;">M1+M5</td>
<td style="text-align: left;">$25.74 / 3.68$</td>
</tr>
<tr>
<td style="text-align: left;">CoLM</td>
<td style="text-align: left;">$\mathbf{2 9 . 3 7 / 3 . 9 5}$</td>
</tr>
</tbody>
</table>
<p>Table 9: In context learning results of LLaMA, measured in METEOR and GREEN.
domesticated animals compared to cats), and therefore the rules are "more general" than the facts.
"More general" means not only about finding higher taxonomic rank, but can be in unlimited forms. For instance, if the fact is about the Sun rises and falls every day, then some examples of a "more general" rule can be (1) the Earth is the king of the universe or (2) the Earth is rotating itself.</p>
<p>Both rule examples are "more general" than the given fact, since the rule can entail not only the given fact, but also other not mentioned facts such as the observable movements of the other stars in the Milky Way.</p>
<h2>A. 11 Set up Thresholds for M2/3/4/5</h2>
<p>Setting up thresholds is an important step for our framework, since different thresholds can lead to different inductive reasoning results. We discuss the details of setting up thresholds in the section.</p>
<p>We design the standard for setting up thresholds based on heuristics that the thresholds should be set up that each module (in M2/3/4/5) should filter some rules but a single module should not filter too many rules (in this case, since we have many modules, there might not remain a reasonable proportion of rules left).</p>
<p>More specifically, given a rule (and facts), M2/3/4/5 can produce a score on evaluating the validity of the rule from a specific aspect. The score is the ratio of the probability of the "yes" token and "no" token obtained from the last layer of PLM. The score is in the range of $[0,1]$.</p>
<p>We find that getting a specific threshold for each module is more beneficial than using the default 0.5 threshold. We obtain the thresholds on the DEERLET validation set.</p>
<p>More concretely, on the validation set, if there exists a global optimal threshold that (1) achieves the best f 1 or accuracy and (2) the threshold should not be very close to 0 or 1 and (3) recall is not very close to 0 (when close to 1 , it should not be in
the case that the threshold accepts nearly all generated rules but should be that the threshold already rejects some rules), then the global optimal threshold is adopted; if there is no such global optimal threshold, then find a local optimal threshold that (1) achieves the best f 1 or accuracy compared to its neighboring thresholds and (2) the threshold should not be very close to 0 or 1 , and (3) the recall range is in $[0.7,0.9]$, then the local optimal threshold is adopted.</p>
<h2>A. 12 More Details to Prevent Collection of Generated Trivial Rules</h2>
<p>We use a simple heuristic method to prevent collection of generated trivial rules. Specifically, only rules generated from Module 1 that is with more than 45 tokens (not 45 words) do we pass to it Module 2/3/4/5, otherwise we directly filter it.</p>
<p>The reason that we set it up is that we find generated rules with less than 45 tokens are mostly (if not all) incomplete sentences. If we collect and label these incomplete sentences to finetune Module 2/3/4/5, then Module 2/3/4/5 mostly learn to classify whether the rules are complete or not, but not to learn the designed patterns (since the label0/1/2/3 in DEERLET for incomplete sentences are all false).</p>
<p>For this reason, all annotated data in DEERLET only use rules that contain at least 45 tokens.</p>
<h2>A. 13 Related Works on Inductive Logic Programming</h2>
<p>Inductive Logic Programming (ILP) is a subfield of machine learning that uses FOL to represent hypotheses and data. It relies on formal language for knowledge representation and reasoning purposes (De Raedt, 2010). We propose a new paradigm that can naturally avoid three systematic disadvantages of ILP (Cropper et al., 2022). Cropper et al. (2022) summarizes the challenges for ILP, including disability of handling raw input such as natural language and image, sensitiveness to mislabeled data and incapacity to handle ambiguous input. In this work, we propose a new paradigm/ for inductive reasoning to use natural language as representation for knowledge and PLM as inductive reasoners, which can naturally avoid these challenges.</p>
<p>Recently, Dai and Muggleton (2021) propose to use logic programming to induce knowledge form image raw input. Our work instead focus on natural</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Specific facts</th>
<th style="text-align: center;">General facts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">R+F</td>
<td style="text-align: center;">$10.15 / 2.25$</td>
<td style="text-align: center;">$12.79 / 2.53$</td>
</tr>
<tr>
<td style="text-align: center;">M1</td>
<td style="text-align: center;">$25.61 / 3.58$</td>
<td style="text-align: center;">$24.57 / 3.51$</td>
</tr>
<tr>
<td style="text-align: center;">M1+M2</td>
<td style="text-align: center;">$26.47 / 3.82$</td>
<td style="text-align: center;">$24.14 / 3.42$</td>
</tr>
<tr>
<td style="text-align: center;">M1+M3</td>
<td style="text-align: center;">$25.88 / 3.64$</td>
<td style="text-align: center;">$24.38 / 3.45$</td>
</tr>
<tr>
<td style="text-align: center;">M1+M4</td>
<td style="text-align: center;">$27.19 / 3.91$</td>
<td style="text-align: center;">$24.36 / 3.48$</td>
</tr>
<tr>
<td style="text-align: center;">M1+M5</td>
<td style="text-align: center;">$25.59 / 3.57$</td>
<td style="text-align: center;">$\mathbf{2 4 . 6 1 / 3 . 5 1}$</td>
</tr>
<tr>
<td style="text-align: center;">CoLM</td>
<td style="text-align: center;">$\mathbf{2 7 . 7 4 / 3 . 9 8}$</td>
<td style="text-align: center;">$24.34 / 3.47$</td>
</tr>
</tbody>
</table>
<p>Table 10: Analysis of PLM (GPT-J)'s performance (measured in METEOR / GREEN) in with specific or general input facts (Under in-context learning setting).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Ada</th>
<th style="text-align: center;">Babbage</th>
<th style="text-align: center;">Curie</th>
<th style="text-align: center;">GPTJ</th>
<th style="text-align: center;">Davinci</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">R+F</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">1.81</td>
<td style="text-align: center;">1.88</td>
<td style="text-align: center;">1.86</td>
<td style="text-align: center;">1.86</td>
</tr>
<tr>
<td style="text-align: center;">M1</td>
<td style="text-align: center;">5.41</td>
<td style="text-align: center;">4.29</td>
<td style="text-align: center;">5.76</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">7.52</td>
</tr>
</tbody>
</table>
<p>Table 11: GPT-3's performance as well as GPT-J's performance as Rule Proposer (Measured in BLEU).
language raw input, and use PLMs as reasoning methods to induce knowledge.</p>
<h2>A. 14 Induce Rules from General Facts and Specific Facts</h2>
<p>Sixty percent of the rules in DEER are more general than any of their facts alone at least in one dimension. We describe this process as "inducing general rules from specific facts". However, we find that there are many general statements (also referred to as general fact) of a rule on the web. Therefore, for rule induction systems to be able to utilize both "specific facts" and "general facts", forty percent of the rules in DEER are equipped with general facts. We describe this process as "inducing general rules from general facts".</p>
<p>Table 10 shows the result from specific vs general facts under in-context learning and finetuning settings correspondingly. We have discussed that a rule induction system would be more widely applicable if it can utilize both specific fact and general fact. In table 10, general facts cases result in lower performance. We think one of the most possible reasons is that in DEER many general facts do not directly contain the content of the corresponding gold rules. For example, general facts can be mottos from philosophers such as Socrates, and rules can be an understandable description of such mottos in natural language rule format.</p>
<h2>A. 15 GPT3's Performance as Rule Proposer</h2>
<p>Table 11 shows the result to use GPT-3 and GPT-J as rule proposer (M1). It is measured in BLEU
because it's a very early result, and we haven't adopted METEOR yet. If use METEOR as metric, the trend should be similar (the trend of BLEU and METEOR are very similar in our other experiments). The reason we do not test the scale performance of CoLM compared to M1 is that OpenAI's API does not support return full embeddings, and our current code relies on embedding to implement M2/3/4/5 of CoLM. We will modify our code and try it on GPT-3 in the next version of our paper.</p>
<h2>A. 16 Method for Prevention of Personal Information</h2>
<p>The first author collected the datasets. During collection, (1) most of the data are collected from Wikipedia, where personal information is nearly none; (2) the first author checks the data first before collects them.</p>
<h2>A. 17 Prompt for ALL Modules</h2>
<p>We have uploaded the full code to GitHub, containing the full prompts. The full prompts can be also found in the uploaded supplementary materials along with this submission in utils.py.</p>
<h2>A. 18 Dataset Split of DEER and DEERLET</h2>
<p>Out of the 1,200 rule-fact paris of DEER, 438 / 762 are designed for train / test. Out of 846 examples of DEERLET, 546 / 100 / 200 are designed for train / val / test.</p>
<p>In our previous arXiv version, we use a different dataset split (train 100 rules / test 100 rules), the current dataset split is (train 73 rules / test 127 rules) to better utilize the data (each rule has 6 annotated facts). The last 22 rules in test set (id: 105 126) are inspired by gpt-3.5-turbo, while all other rules are proposed by an expert. All facts are existing texts collected from the web using search engine, after given a rule.</p>
<h2>A. 19 More Illustration on Human Evaluation</h2>
<p>Here the human annotations for human evaluation in Table 4 are from the DEERLET annotations. DEERLET is annotated by an expert (the first author). The dataset (DEERLET) is annotated before M2/3/4/5 (full CoLM) or any baseline experiments, so that the human evaluation is not influenced by the performance of any specific method.</p>
<p>More details about the DEERLET annotation are illustrated in §A.5.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Contribution during internship at Microsoft Research.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
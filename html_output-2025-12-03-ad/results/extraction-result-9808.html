<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9808 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9808</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9808</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-ec592e12f45e20819afe203164bbbd0de8990510</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ec592e12f45e20819afe203164bbbd0de8990510" target="_blank">AmadeusGPT: a natural language interface for interactive animal behavioral analysis</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A novel dual-memory mechanism to allow communication between short-term and long-term memory using symbols as context pointers for retrieval and saving and AmadeusGPT presents a novel way to merge deep biological knowledge, large-language models, and core computer vision modules into a more naturally intelligent system.</p>
                <p><strong>Paper Abstract:</strong> The process of quantifying and analyzing animal behavior involves translating the naturally occurring descriptive language of their actions into machine-readable code. Yet, codifying behavior analysis is often challenging without deep understanding of animal behavior and technical machine learning knowledge. To limit this gap, we introduce AmadeusGPT: a natural language interface that turns natural language descriptions of behaviors into machine-executable code. Large-language models (LLMs) such as GPT3.5 and GPT4 allow for interactive language-based queries that are potentially well suited for making interactive behavior analysis. However, the comprehension capability of these LLMs is limited by the context window size, which prevents it from remembering distant conversations. To overcome the context window limitation, we implement a novel dual-memory mechanism to allow communication between short-term and long-term memory using symbols as context pointers for retrieval and saving. Concretely, users directly use language-based definitions of behavior and our augmented GPT develops code based on the core AmadeusGPT API, which contains machine learning, computer vision, spatio-temporal reasoning, and visualization modules. Users then can interactively refine results, and seamlessly add new behavioral modules as needed. We benchmark AmadeusGPT and show we can produce state-of-the-art performance on the MABE 2022 behavior challenge tasks. Note, an end-user would not need to write any code to achieve this. Thus, collectively AmadeusGPT presents a novel way to merge deep biological knowledge, large-language models, and core computer vision modules into a more naturally intelligent system. Code and demos can be found at: https://github.com/AdaptiveMotorControlLab/AmadeusGPT.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9808",
    "paper_id": "paper-ec592e12f45e20819afe203164bbbd0de8990510",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00504525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AmadeusGPT: a natural language interface for interactive animal behavioral analysis</h1>
<p>Shaokai Ye<br>EPFL<br>Geneva, CH<br>Jessy Lauer<br>EPFL<br>Geneva, CH<br>Mu Zhou<br>EPFL<br>Geneva, CH<br>Alexander Mathis<br>EPFL<br>Geneva, CH<br>Mackenzie.mathis@epfl.ch</p>
<h4>Abstract</h4>
<p>The process of quantifying and analyzing animal behavior involves translating the naturally occurring descriptive language of their actions into machine-readable code. Yet, codifying behavior analysis is often challenging without deep understanding of animal behavior and technical machine learning knowledge. To limit this gap, we introduce AmadeusGPT: a natural language interface that turns natural language descriptions of behaviors into machine-executable code. Large-language models (LLMs) such as GPT3.5 and GPT4 allow for interactive language-based queries that are potentially well suited for making interactive behavior analysis. However, the comprehension capability of these LLMs is limited by the context window size, which prevents it from remembering distant conversations. To overcome the context window limitation, we implement a novel dual-memory mechanism to allow communication between short-term and long-term memory using symbols as context pointers for retrieval and saving. Concretely, users directly use languagebased definitions of behavior and our augmented GPT develops code based on the core AmadeusGPT API, which contains machine learning, computer vision, spatiotemporal reasoning, and visualization modules. Users then can interactively refine results, and seamlessly add new behavioral modules as needed. We benchmark AmadeusGPT and show we can produce state-of-the-art performance on the MABE 2022 behavior challenge tasks. Note, an end-user would not need to write any code to achieve this. Thus, collectively AmadeusGPT presents a novel way to merge deep biological knowledge, large-language models, and core computer vision modules into a more naturally intelligent system. Code and demos can be found at: https://github.com/AdaptiveMotorControlLab/AmadeusGPT</p>
<h2>1 Introduction</h2>
<p>Efficiently describing and analyzing animal behavior offers valuable insights into their motivations and underlying neural circuits, making it a critical aspect of modern ethology, neuroscience, medicine, and technology [1, 2, 3, 4]. Yet behavior is complex, often multi-faceted, and context-dependent, making it challenging to quantify and analyze [5, 6]. The process of translating animal behavior into machine-readable code often involves handcrafted features, unsupervised pre-processing, or neural network training, which may not be intuitive to develop for life scientists.</p>
<p>To understand animal behavior one needs to complete a series of sub-tasks, such as obtaining animal poses and identities, object locations and segmentation masks, and then specifying events or actions the animal performs. Significant progress has been made in automating sub-tasks of behavioral analysis such as animal tracking [7, 8, 9], object segmentation [10, 11], and behavior classification [3, $12,13,14,4]$, yet behavioral phenotyping requires additional analysis and reasoning [2, 15, 16, 13,</p>
<p>17, 8]. This is typically done with feature computations such as measuring time spent in regions of interest or general statistics of locomotion [13, 8, 18].</p>
<p>The challenge of making behavior analysis easy-to-access and interpretable is hindered by the difficulty of combining taskspecific models and the lack of an intuitive natural language interface to produce machine code. In an ideal scenario, a behavior analysis practitioner would be able to explore behavioral data, define their desired actions using natural language, and visualize captured behaviors without needing to learn how to train models, write scripts, or integrate code bases. Our framework, AmadeusGPT, takes the first step towards achieving this goal. AmadeusGPT provides a natural language interface that bridges users' prompts and behavioral modules designed for sub-tasks of behavior analysis. Our results show that AmadeusGPT outperforms machine learning-based behavior analysis classification tasks in the MABe [4, 19] benchmark by using prompts highly similar to their official definitions of each behavior, namely with small modifications and only three tunable parameters.
AmadeusGPT offers a novel human-computer interaction approach to behavior analysis, providing a unique user experience for those interested in exploring their behavioral data. Through natural language prompts, users can ask questions, define behaviors on-the-fly, and visualize the resulting analyses plus the language output (Figure 1). We show that with our novel dual-memory mechanism, defined behaviors are not lost (due to being beyond the token limit), wording can be automatically rephrased for robustness, and the state of the application can be restored when relaunched, providing seamless and intuitive use (Figures 1-4).
To capture animal-environment states, AmadeusGPT leverages state-of-the-art pretrained models, such as SuperAnimals [17] for animal pose estimation and Segment-Anything (SAM) for object segmentation [11]. The platform enables spatio-temporal reasoning to parse the outputs of computer vision models into quantitative behavior analysis. Additionally, AmadeusGPT simplifies the integration of arbitrary behavioral modules, making it easier to combine tools for task-specific models and interface with machine code.</p>
<h1>2 Related Work</h1>
<p>Large Language Models. In recent years there has been a surge in the development of large language models (LLMs) [20, 21, 22, 23] that show exceptional abilities in natural language processing tasks such as language generation, interaction, and reasoning. One notable example is ChatGPT, which is built on GPT-3+ and trained on a massive corpus of text data [22]. ChatGPT has demonstrated impressive performance in a wide range of natural language processing tasks, including text classification, language modeling, and question-answering. In addition to language processing, LLMs have</p>
<p>also been applied to other domains such as image and video processing [24], audio processing, and natural language generation for multi-modal inputs [24, 25].
LLM Integrations. Recent works leverage LLMs to generate code for computers to execute, including visual programming with VisProg [26] and ViperGPT [27], and HuggingGPT [28] that exploits pre-trained AI models and smartly handle API-GPT interactions [26, 27]. However, animal behavior analysis lacks a natural language-computer vision pipeline and requires diverse open-source code-base to complete the sub-tasks. Concurrent work by Park et al. [29] on Generative Agents uses a dual-memory system and iterative chatting, but has a more expensive execution flow compared to our approach, which uses a memory mechanism with symbols as pointers.
Behavioral Analysis. Machine learning and computer vision techniques have increasingly been employed in animal behavior analysis in recent years [7, 9, 12, 30, 31, 32]. These techniques offer advantages such as automation, high throughput, and objectivity, compared to traditional methods that rely on manual observation and annotation [1]. Deep learning models, such as convolutional neural networks (CNNs) and transformers, have been utilized for feature extraction and classification of animal behaviors in various domains such as social behavior, locomotion, and posture analysis (reviewed in [2]). Yet, universal interfaces to the plethora of pose estimation tools and downstream analysis methods are lacking.
We used a rule-based behavior capturing on top of tracked animal poses to capture animal behaviors, similar to the approach used in LiveMouseTracker (LMT) [33]. However, unlike LMT, which requires users to interact with code and requires a specific hardware to collect data, AmadeusGPT is agnostic to data collection and provides a natural language interface that is easy to use and customize. Additionally, unlike LMT, AmadeusGPT includes object segmentation, enabling it to capture animal-object interactions.
Task Programming for Behavior. Task programming for behavior analysis [30] aims to leverage the domain knowledge from experts to help extract the most relevant features that are useful for the downstream behavior classification task. However, task programs were only considered as Python functions that help extract better features (i.e., features built from poses). In this work, we generalize the scope of task programming: any sub-task that can be achieved as machine-executable Python code are considered task programs in AmadeusGPT, and can be queried and developed with natural language. Thus, we name the generated function in AmadeusGPT (target) task programs.</p>
<h1>3 AmadeusGPT</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Schematic of AmadeusGPT design and features.</p>
<p>AmadeusGPT is a human-computer interactive platform to describe with language the behavioral analysis the user wants performed. It utilizes ChatGPT as the user-guided controller and a range of machine learning and computer vision models as collaborative executors to analyze animal behavior, starting from raw video and leveraging new pretrained pose estimation models that can run inference across species and settings [17], and powerful objection segmentation models (SAM [11]) (Figure 2). Here we focus on mice, as they are the most common model organism used in biotechnology research [34], but nothing precludes AmadeusGPT to be used on other animals.
AmadeusGPT uses LLMs to generate Python executable code that fulfills user-specified queries in the prompt. Building such a system requires LLMs to learn to manipulate core process resources in a constrained way. If the user's prompt is unclear or beyond the system's capacity, the generated code might result in errors that require programming expertise. Intuitive error messages are therefore essential for ensuring a consistent natural language experience, while maintaining the ability for users</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: AmadeusGPT: a natural language model enhanced behavioral analysis system. (a) Users can start by uploading a video and asking a question to AmadeusGPT about what they want to do. AmadeusGPT will run computer vision models if the target task depends on pose estimation (e.g., the posture or location of the animal) and object segmentation (e.g., when does the animal overlaps with an object) and/or ask clarifying questions. Once correct, the user can also save these queries/code outputs. Three example queries and outputs are shown. (b) Alternatively, users can provide a detailed recipe for how they want the data analyzed. The colored text highlights action-items for AmadeusGPT (matching the colored boxes).
to iteratively refine generated code with language. Therefore, to build AmadeusGPT we leveraged GPT3.5, augmented it, developed a dual-memory system and core behavioral utilities that together make AmadeusGPT an end-to-end, human language-AI system (Figure 2).</p>
<h1>3.1 LLMs as natural interfaces for code development and execution</h1>
<p>ChatGPT (i.e., with GPT3.5) [23] is able to generate code that corresponds to users' prompts due to instruction tuning paired with access to a large amount of code in its training data. However, the native ChatGPT/GPT3.5 API cannot serve as the language interface for behavior analysis for the following reasons: (1) it cannot work with private APIs that are not in its training data; (2) it</p>
<p>hallucinates functions if too many implementation details are exposed or asked to be provided; (3) the context window doesn't have a sufficient capacity size for complex tasks that require reading large source code files; (4) it has no Python interpreter to execute the code it suggests. Therefore, we built an augmented version of GPT3.5 to overcome these limitations (see Sections 3.2 and 3.3).</p>
<h1>3.2 Augmented-GPT3.5 and API design</h1>
<p>AmadeusGPT sends our API documentation to ChatGPT in order to smartly constrain the output code and immediate Python execution and thereby always augments the data available to GPT3.5. In the API documentation, implementation details are encapsulated and only the function documentation is exposed (see Appendix for example API docs). Importantly, there is an "explanation prompt" with each function example that serves as a hint for GPT3.5 to understand what the API is for. This separation of API documentation and implementation has two advantages: it reduces token use, and prevents hallucinating resources that do not exist in the core functions or modules linked to AmadeusGPT. Concretely, we prompt GPT3.5 to strictly follow our API (see Appendix). This design improves the reliability of code generation while imposing quality control.</p>
<p>We followed three core principles when designing the API. Firstly, we developed a code with an atomic API that consists of simple functions designed to perform specific operations (Figure 2). These functions are modular, reusable, and composable, making them easy to combine with other API functions to create more complex functionality. The use of atomic API improves the readability of the code for users and developers and reduces the chance of LLMs confusing ambiguous API calls. Secondly, we leverage polymorphism in code design to generalize our API to variants of a sub-task routed by parameter values and inputs types since it is not desirable nor realistic to provide an example code for every possible sub-task. Thirdly, to make AmadeusGPT cover a range of behavioral analysis tasks, we identify core behavioral modules that cover common behavior analysis sub-tasks and additionally use integration behavioral modules for task-specific sub-tasks.</p>
<h3>3.3 Dual Memory Mechanism</h3>
<p>As GPT3.5 is implemented with a transformer architecture that has a context window size limitation of 4,096 tokens for hosting the history of conversations [22]. This would limit how far back AmadeusGPT can use the context, such as a user-defined behavior or its own generated code. We demonstrate in Figure 4 that without tackling the context window limitation, forgetting will happen, which results in wrong code that hallucinates functions or variables that do not exist.</p>
<p>To tackle this limitation, we implemented a dual-memory mechanism that consists of both short-term memory and long-term memory banks. In this work, short-term memory is implemented as a dynamic deque that holds the chat history, pulling in new tokens and removing older ones when full (i.e., beyond the 4,096 tokens). At inference time, all text in the deque is sent to the ChatGPT API call.</p>
<p>Long-term memory is implemented as a dictionary in RAM. The keys of such a dictionary are called symbols in this work. We define read and write symbols to instruct the communication between short-term memory and long-term memory. With the help of regular expression, a symbol that is enclosed by $&lt;|&gt;$ triggers a memory writing operation that writes the context of the corresponding chat into the dictionary using the symbol name as the key. As in Figure 3, keyword $&lt;|$ head dips $|&gt;$ triggers memory writing that stores the whole sentence under the key name "head dips". Similarly, $&lt;&gt;$ triggers a reading operation that pushes the stored sentence into the front of short-term memory with an additional prompt to remind GPT3.5 that the retrieved memory is used for context instead of a formal instruction. This allows a read operation retrieval from the long-term memory to use as a context for a new instruction. Additionally, with the long-term memory, AmadeusGPT can store its states into disk and the user can restore the states after re-launching the application (See Appendix). Collectively, we call this augmented-GPT3.5.</p>
<h3>3.4 Core Behavioral Modules</h3>
<p>The language query feature of ChatGPT inspired us to provide a more natural way for humans to perform behavior analysis. Namely, it provides users with a platform to perform behavior analysis by asking questions or instructing tasks in an interactive manner. We imagine that users will either ask a question or provide longer instructions to complete a task (Figure 3). In addition, if users ask followup questions to a previous answer from AmadeusGPT, it attempts to answer with executable code</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Dual Memory Mechanism for augmenting GPT3.5. (a) We introduce a long term memory module that overcomes running out of tokens, and a dynamic loading system for code integrations such as for advanced uses like dimensionality reduction with UMAP [35] or cebra [36] (see Appendix). (b) An example query and output from short term memory only (early in an interactive session), (c) if long term memory is ablated after running out of tokens the output is non-functional. (d) Yet, with our new system, it can easily retrieve the identical, correct output within a long session or upon restarting.
called "task programs" that are executed by the backend Python interpreter. In the API documentation we mostly specify code examples for uni-purpose task programs (such as counting events). However, we show in Figure 3 that our augmented-GPT3.5 API is able to compose multi-purpose task programs (such as computing events and interactions with objects over time to produce plots).
Figure 3b shows how the user can give a complex instruction that covers multiple sub-tasks, including pose extraction, behavioral definitions, interactively drawing regions of interest (ROIs), then visualizing and performing tasks, such as behavior event counting. In this case, AmadeusGPT is able to decompose the description into multiple task programs and assemble the final program. Alternatively, the user can also ask a question "When is the mouse on the treadmill?" or "Count the head dips per ROI" as follow-up queries to AmadeusGPT's answer, or change the color map of the previous plot, etc. This all relies on the core behavioral modules and their ability to be combinatorically used (Figure 2).
Our core behavioral modules try to cover the most common behavioral sub-tasks. Generally speaking, a typical behavior analysis task asks question about what animals do through a spatio-temporal space. This can simply be the amount of time that an animal spends moving in a particular ROI, to more advanced analysis like measuring animal-animal or animal-object interactions. This requires varying levels of key-point tracking, object segmentation, and video (temporal) reasoning. Thus, we implemented the following core modules and always send them with the queries during augmentedGPT3.5 API calls (Figure 2).</p>
<ul>
<li>Kinematic feature analysis. As many behaviors are concerned with the kinematics of bodyparts, we provide a module for kinematics analysis. This includes filtering the pose data, calculating speed, velocity, and acceleration. It also includes movement-based statistical analysis, such as queries related</li>
</ul>
<p>to computing the amount of time spent locomoting (i.e., a velocity over some minimal threshold computed across the video, or time spent in an ROI).</p>
<ul>
<li>Animal-Object environment states. To capture animals' and environment states, we deploy pretrained computer vision models such as pose estimation with SuperAnimal models [17, 7] and objects with SAM [11]. Note they can be easily substituted by tailored supervised models. To support end-to-end behavior analysis with raw video only, we use them as the default models for their wide coverage of animals and objects. We also implemented code infrastructure that abstracts "static objects" and "moving objects" to represent objects in the environment as well as customized ROI objects and animals respectively. The animal-object relation and animal-animal relation are modeled and saved in lookup tables. These relations mostly cover binary spatial relations such as "to the left", "to the right", "overlap", "orientation", and numerical spatial relations such as "distance", "angle" (i.e., the angle between animals, based on computed neck-to-tailbase body axes) and "gazing angle" (i.e., the angle between animals' head coordinate systems, determined from the nose-neck line) (see Appendix).</li>
<li>Spatio-temporal reasoning. After computer vision models track animals and segment objects, their outputs are used to build internal spatio-temporal relation tables among animals-animals and animalsobjects. We provide code infrastructures to support queries of events (i.e., sequential spatio-temporal relations and simultaneous spatio-temporal relations). Users can query events where animals move from one state to the other (see also Figure 5b).</li>
</ul>
<h1>3.5 Integrations Beyond Core Behavioral Modules</h1>
<p>Integration modules aim to cover task-specific behavior analysis sub-tasks such as dataset loading, embedding calculations, and visualization tools. Because they are task-specific, we do not send the API documentation of these modules. Instead, we rely on "dynamic loading" to load only few of the selected API documents for integration modules at inference time. To allow for dynamic loading, we use the embedding API from OpenAI to turn API documents of integration modules into text embedding vectors and cache them in RAM or disk. The user's prompt then acts as a query to retrieve K (hyper-parameters) most relevant integration modules by cosine similarity (Figure 4). We also allow users to manually load integration modules by prompting "loading module \module-path" if users want to save the cost of using the embedding API and/or they are creating their own modules.</p>
<p>Error handling. We include error handling to help users understand when instructions are beyond the system's capabilities or ambiguous. Here, AmadeusGPT forwards prompts, error messages, and API docs to the ChatGPT API for natural language explanations (Figure 3a, Query 3 example).
Rephraser. Users can ask questions with language that might be very different from those in our API docs, which can cause performance degradation due to out-of-distribution prompts (See Section 4). To overcome this we leverage a native GPT3.5 (i.e., without our augmentation) that we call "Rephraser" that is tasked to turn users' expressions into a style that is similar to that found in our API docs. We wrote in the system prompt of Rephraser a few examples of such rephrasing, hoping the GPT3.5 can learn it via few-shot learning. Therefore, Rephraser is tasked to act as test-time domain adaptation component for our system.
Self-correction. There are incidences where ChatGPT API makes obvious mistakes that cause runtime errors, therefore we implemented a self-correction mechanism to help correct this. When there is an error executing the generated python code, we forward the error message and the output of our error handler to an independent ChatGPT API connection with a system prompt that encourages it to revise the function. The number of times for such retrying is flexible, but setting this to three in practice generally improves the success rates (See Appendix for an example). We keep the history of retries in the context window to help it to learn from failures if it takes more than one trial to correct the code. Note that ChatGPT API does not have read/write access to the implementation of our APIs thus self-correction cannot correctly revise the code if the errors stem from our code.
Explainer module. AmadeusGPT takes users' queries and generates Python code to address the queries. The executed function returns results of multiple data types, including plots, strings, numpy arrays, etc. However, it might not always be straightforward for users to link those returned results to the queries. Moreover, the users do not know how much they can trust the returned results. In many cases, checking the generated code can help. However, inspecting the code requires python programming knowledge. Therefore we add and explainer module, which is another LLM whose job is to explain to the users how to link the results to the queries. The explainer is implemented as an</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Results on classical behavioral tasks with AmadeusGPT. (a) Result on EPM showing limited time in the open arms by the mouse. The raster plot is an ethogram where there is a tick for every related event in time, i.e., the mouse is in the closed arm or in the open arm. (a.1) shows AmadeusGPT counts vs. three human raters [13] across 5 videos (dot colors). (b) Animal-object interactions can be computed in natural home cage settings. (c) Behavioral images and original description given in MABe, vs. our prompts that produce the quantitative results shown in Table 1. For Chase, we visualize the chasing animal's trajectory that overlaps with the predicted mask and the chasing animal's trajectory for the ground-truth mask. For Watching, we visualize the visual cone for each animal from each animal's head coordinate (axis by neck-nose vector and its normal vector).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tasks</th>
<th style="text-align: left;">T5 <br> chase</th>
<th style="text-align: left;">T6 <br> close</th>
<th style="text-align: left;">T7 <br> contact</th>
<th style="text-align: left;">T8 <br> huddles</th>
<th style="text-align: left;">T9 <br> oral_ear_contact</th>
<th style="text-align: left;">T12 <br> watching</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PCA baseline</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.13</td>
<td style="text-align: left;">0.0008</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Top-entry 1 [4]</td>
<td style="text-align: left;">0.010</td>
<td style="text-align: left;">$\mathbf{0 . 7 0 8}$</td>
<td style="text-align: left;">0.558</td>
<td style="text-align: left;">0.310</td>
<td style="text-align: left;">0.0056</td>
<td style="text-align: left;">0.182</td>
</tr>
<tr>
<td style="text-align: left;">Top-entry 2 [4]</td>
<td style="text-align: left;">0.050</td>
<td style="text-align: left;">0.7072</td>
<td style="text-align: left;">0.561</td>
<td style="text-align: left;">0.214</td>
<td style="text-align: left;">0.0084</td>
<td style="text-align: left;">0.159</td>
</tr>
<tr>
<td style="text-align: left;">Top-entry 3 [4]</td>
<td style="text-align: left;">0.029</td>
<td style="text-align: left;">0.655</td>
<td style="text-align: left;">0.515</td>
<td style="text-align: left;">0.249</td>
<td style="text-align: left;">0.0062</td>
<td style="text-align: left;">0.162</td>
</tr>
<tr>
<td style="text-align: left;">BAMS [37]</td>
<td style="text-align: left;">0.023</td>
<td style="text-align: left;">0.664</td>
<td style="text-align: left;">0.533</td>
<td style="text-align: left;">0.302</td>
<td style="text-align: left;">0.0045</td>
<td style="text-align: left;">0.191</td>
</tr>
<tr>
<td style="text-align: left;">AmadeusGPT (Ours)</td>
<td style="text-align: left;">$\mathbf{0 . 2 7 4}$</td>
<td style="text-align: left;">0.700</td>
<td style="text-align: left;">$\mathbf{0 . 5 7 2}$</td>
<td style="text-align: left;">$\mathbf{0 . 3 8 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 2 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 6 0 0}$</td>
</tr>
</tbody>
</table>
<p>Table 1: F1 scores by AmadeusGPT on several tasks from the MABe Behavior Challenge 2022 [4]. We do not use representation learning (the aim of the benchmark), only rule-based task programming.
independent ChatGPT API connection with its independent system prompt. In the system prompt, we ask the explainer to take the thought process parsed from the code generator, the return values from python as well as the user queries to explain whether the code and its return values meet the queries (See Appendix for an example).</p>
<h1>4 Experiments</h1>
<p>To evaluate AmadeusGPT we ran a series of qualitative and quantitative experiments. We used three datasets that highlight standard behavioral neuroscience settings. The first is an open-access Elevated Plus Maze (EPM) dataset from Sturman et al. [13]. The second is called MausHaus and is video of a mouse for one hour ( 108 K frames) in an enriched home-cage setting [17]. The third is video data of three mice interacting from the MABe 2022 Challenge [4].
EPM. The EPM is a classical task in neuroscience research to test an animal's anxiety about being in exposed "open arms" vs. the "closed arms" [13]. We show that with minimal prompts and ROI interactive plotting a user can measure these events (Figure 5a, also see Figure 3). Here, we query AmadeusGPT to report both open and closed arms and note that the resulting raster plots (ethograms) do not identify the same frames, as one expects if it is correct (i.e., the mouse cannot be in both states at once). We also show that AmadeusGPT counts the number of events similar to those reported in ground truth annotations by three raters across five different videos (Figure 5a.1).
MausHaus: enriched mouse-cage monitoring. Studying more natural behavior is becoming important in research settings. Here, we demonstrate intuitive prompts that run pretrained models (SuperAnimal-TopViewMouse and SAM) then query spatio-temporal relationships between the model outputs (Figure 5b). As SAM only provides object labels, we use object number or interactive clicking on the object to ground the analysis (see also Figure 3).
MABe 2022 Benchmark. The benchmark had two rounds for the three mouse dataset. We used the more popular first round (evaluation split) and therefore provided the pre-computed keypoints as inputs to AmadeusGPT. In Figure 5c, we show how AmadeusGPT captures two representative tasks from MABe benchmark, Chase and Watching. We use text that is close to the original definition to define the behaviors we want to capture. Note the units in our prompt are pixels for distance, radians for degree and pixel per frame for speed. We tested six behaviors and report the F1 score as computed in the MABe evaluation (Table 1). Our approach is purely rule-based, thus no machine learning is needed and only three parameters are needed to be given or tuned: a smoothing window size for merging neighboring bouts, a minimal window size for dropping short events, and the pixel per centimeter. Note that tasks that are hard for machine learning models are also hard for our rule-based approach (Table 1, Task 9).
We do not intend to formally claim state-of-the-art on the benchmark, as the goal was to evaluate unsupervised representational learning models. Nevertheless, we show that in practice one can use a text definition of behavior to capture behaviors that are on par with, or better than, the competitive representation learning approaches.
Robustness tests and stress-testing AmadeusGPT. Users can query AmadeusGPT with expressions that are very different from the explanation text we provided in the API documentation. A potential pitfall is that AmadeusGPT overfits to our (the developers) expressions and biases. To test how robust AmadeusGPT is, we crafted five out-of-distribution (OOD) base questions for an EPM video. Then</p>
<p>we asked a native GPT3.5 (part of our reflection module, see Figure 2) to generate five variants of each OOD question. We manually checked whether AmadeusGPT generated consistently correct results on those 25 generated questions. We report an $88 \%$ success rate in those tests with our Rephraser module vs. $32 \%$ without Rephraser. The total number of tokens consumed using the ChatGPT-API was 4,322 and 5,002 with and without using Rephraser, respectively. Note that in both cases, the consumed number of tokens is larger than the maximal context window size of 4,096. This also shows that AmadeusGPT passes the stress-test we set for it in two key ways: (1) The short-term memory deque correctly maintains the constrained size without getting an error from the OpenAI API due to maximal token size error; (2) The diverse questions in the short-term memory do not result in mode collapse or severe performance degradation. See more details in the Appendix.</p>
<h1>5 Discussion</h1>
<p>AmadeusGPT is a novel system for interactive behavior analysis. By providing a user-friendly interface and leveraging the power of language models, AmadeusGPT enables researchers to more efficiently and effectively study animal behavior. We believe this system offers a significant contribution to the field of behavioral analysis. The natural language interface of AmadeusGPT empowers non-experts to conduct advanced behavioral analysis with cutting-edge computer vision (such as with SAM [11] and SuperAnimal [17]), but its reliance on LLMs raises potential ethical concerns such as bias amplification. The use of a custom-designed API module in AmadeusGPT helps limit potential biases in outputs, but future work should consider how integration modules can introduce biases [38]. It also allows for domain-specific human-AI interactions [39, 40].
Our proposed natural language interface with the augmented-GPT3.5 shows promise, but there are limitations that need to be addressed in future work. These include enhancing robustness by reducing bias in prompt writing, enabling more goal-driven code reasoning to make AmadeusGPT more collaborative, and improving generalization by extending the available APIs and preventing ChatGPT from writing incorrect code. We also only support the English language, but multi-lingual support with be explored in the future. Collectively, we believe AmadeusGPT promises to open new avenues for both leveraging the utility of, and developing on, cutting-edge computer vision and machine learning tools with no coding - namely, by only using intuitive language.
It is also worth noting that AmadeusGPT leverages ChatGPT API calls, which means we do not have access to the weights of the underlying LLMs thus we cannot yet fine-tune or augment the LLMs. Therefore, our dual-memory leverages the process memory and disks to augment the language model, in contrast to works that augment LLMs by assuming the access to the LLMs [41, 42]. Similarly, without fine-tuning the underlying LLMs, we constrain the behaviors of the LLMs only via in-context learning for both the code generator and rephraser. While recent models such as GPT4 [43], CLAUDE [44] continue to make context window bigger and in-context learning more powerful, deploying them can be slower and more expensive and a strategy of combining a smaller model and a bigger model is worth exploring [45]. In the future, it would be interesting to compare our in-process-memory with in-context memory in terms of reliability and cost. It is also interesting to compare fine-tuned LLMs vs. LLMs that are constrained via in-context learning for future works that use natural language interface for behavior analysis.
Of course, AmadeusGPT is not limited to using GPT3.5, and new models such as GPT4 [43], CLAUDE [44] or others could be used. However, the larger the model the slower the response time and the higher the computational cost. Thus, while these excellent works have started to overcome token limits, there are still limits such that our computationally efficient dual-memory system will be of broad interest to those developing both domain-specific solutions (such as AmadeusGPT) and generalist models (the base GPT models). Collectively, we believe AmadeusGPT promises to open new avenues for both leveraging the utility of, and developing on, cutting-edge computer vision and machine learning tools with no coding - namely, by only using intuitive language.</p>
<h1>6 Acknowledgements</h1>
<p>This work was supported by The Vallee Foundation Scholar award to MWM. We thank members of the M.W.Mathis Lab and A.Mathis Group at EPFL for feedback, and St. Schneider and Kinematik AI for assistance and hosting our demo. COI: MWM is a co-founder and equity holder in Kinematik AI. Author Contributions: Conceptualization: SY, MWM; Methodology \&amp; Software: SY, JL, MZ, MWM; Writing: MWM, SY; Writing-Editing: JL, AM; Visualization: MWM, SY, MZ; Funding acquisition: MWM.</p>
<h2>References</h2>
<p>[1] David J Anderson and Pietro Perona. Toward a science of computational ethology. Neuron, 84(1):18-31, 2014.
[2] Mackenzie Weygandt Mathis and Alexander Mathis. Deep learning tools for the measurement of animal behavior in neuroscience. Current opinion in neurobiology, 60:1-11, 2020.
[3] Mayank Kabra, Alice Robie, Marta Rivera-Alba, Steve Branson, and Kristin Branson. Jaaba: interactive machine learning for automatic annotation of animal behavior. Nature Methods, 10:64-67, 2013.
[4] Jennifer J Sun, Andrew Ulmer, Dipam Chakraborty, Brian Geuther, Edward Hayes, Heng Jia, Vivek Kumar, Zachary Partridge, Alice Robie, Catherine E Schretter, et al. The mabe22 benchmarks for representation learning of multi-agent behavior. arXiv preprint arXiv:2207.10553, 2022.
[5] Konrad Lorenz. The foundations of ethology. Springer verlag, 1981.
[6] Patrick C. Friman. Cooper, heron, and heward's applied behavior analysis (2nd ed.): Checkered flag for students and professors, yellow flag for the field. Journal of Applied Behavior Analysis, 43:161-174, 2010.
[7] Alexander Mathis, Pranav Mamidanna, Kevin M Cury, Taiga Abe, Venkatesh N Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. Deeplabcut: markerless pose estimation of user-defined body parts with deep learning. Nature neuroscience, 21(9):1281-1289, 2018.
[8] Fabrice de Chaumont, Elodie Ey, Nicolas Torquet, Thibault Lagache, Stéphane Dallongeville, Albane Imbert, Thierry Legou, Anne-Marie Le Sourd, Philippe Faure, Thomas Bourgeron, and Jean-Christophe Olivo-Marin. Live mouse tracker: real-time behavioral analysis of groups of mice. bioRxiv, 2018.
[9] Talmo D Pereira, Diego E Aldarondo, Lindsay Willmore, Mikhail Kislin, Samuel S-H Wang, Mala Murthy, and Joshua W Shaevitz. Fast animal pose estimation using deep neural networks. Nature methods, 16(1):117-125, 2019.
[10] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017.
[11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023.
[12] James P Bohnslav, Nivanthika K Wimalasena, Kelsey J Clausing, Yu Y Dai, David A Yarmolinsky, Tomás Cruz, Adam D Kashlan, M Eugenia Chiappe, Lauren L Orefice, Clifford J Woolf, et al. Deepethogram, a machine learning pipeline for supervised behavior classification from raw pixels. Elife, 10:e63377, 2021.
[13] Oliver Sturman, Lukas von Ziegler, Christa Schläppi, Furkan Akyol, Mattia Privitera, Daria Slominski, Christina Grimm, Laetitia Thieren, Valerio Zerbi, Benjamin Grewe, et al. Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions. Neuropsychopharmacology, 45(11):1942-1952, 2020.
[14] Simon RO Nilsson, Nastacia L Goodwin, Jia Jie Choong, Sophia Hwang, Hayden R Wright, Zane C Norville, Xiaoyu Tong, Dayu Lin, Brandon S Bentzley, Neir Eshel, et al. Simple behavioral analysis (simba)-an open source toolkit for computer classification of complex social behaviors in experimental animals. BioRxiv, pages 2020-04, 2020.
[15] Markus Marks, Qiuhan Jin, Oliver Sturman, Lukas M. von Ziegler, Sepp Kollmorgen, Wolfger von der Behrens, Valerio Mante, Johannes Bohacek, and Mehmet Fatih Yanik. Deep-learning based identification, tracking, pose estimation, and behavior classification of interacting primates and mice in complex environments. Nature machine intelligence, 4:331 - 340, 2020.</p>
<p>[16] Cristina Segalin, Jalani Williams, Tomomi Karigo, May Hui, Moriel Zelikowsky, Jennifer J Sun, Pietro Perona, David J Anderson, and Ann Kennedy. The mouse action recognition system (mars) software pipeline for automated analysis of social behaviors in mice. Elife, 10, 2021.
[17] Shaokai Ye, Anastasiia Filippova, Jessy Lauer, Maxime Vidal, Steffen Schneider, Tian Qiu, Alexander Mathis, and Mackenzie Weygandt Mathis. Superanimal models pretrained for plug-and-play analysis of animal behavior, 2023.
[18] Zachary T. Pennington, Zhe Dong, Yu Feng, Lauren M. Vetere, Lucia Page-Harley, Tristan Shuman, and Denise J. Cai. eztrack: An open-source video analysis pipeline for the investigation of animal behavior. Scientific Reports, 9, 2019.
[19] Mehdi Azabou, Michael Mendelson, Maks Sorokin, Shantanu Thakoor, Nauman Ahad, Carolina Urzay, and Eva L Dyer. Learning behavior representations through multi-timescale bootstrapping. arXiv preprint arXiv:2206.07041, 2022.
[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[21] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.
[22] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[24] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.
[25] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022.
[26] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. arXiv preprint arXiv:2211.11559, 2022.
[27] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.
[28] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.
[29] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.
[30] Jennifer J Sun, Ann Kennedy, Eric Zhan, David J Anderson, Yisong Yue, and Pietro Perona. Task programming: Learning data efficient behavior representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2876-2885, 2021.
[31] Kevin Luxem, Petra Mocellin, Falko Fuhrmann, Johannes Kürsch, Stefan Remy, and Pavol Bauer. Identifying behavioral structure from deep variational embeddings of animal motion. Communications Biology, 5, 2020.
[32] Eleanor Batty, Matthew R Whiteway, Shreya Saxena, Dan Biderman, Taiga Abe, Simon Musall, Winthrop F. Gillis, Jeffrey E. Markowitz, Anne K. Churchland, John P. Cunningham, Sandeep Robert Datta, Scott W. Linderman, and Liam Paninski. Behavenet: nonlinear embedding and bayesian neural decoding of behavioral videos. In Neural Information Processing Systems, 2019.
[33] Fabrice De Chaumont, Elodie Ey, Nicolas Torquet, Thibault Lagache, Stéphane Dallongeville, Albane Imbert, Thierry Legou, Anne-Marie Le Sourd, Philippe Faure, Thomas Bourgeron, et al. Real-time analysis of the behaviour of groups of mice via a depth-sensing camera and machine learning. Nature biomedical engineering, 3(11):930-942, 2019.</p>
<p>[34] Debra Hickman, J. Johnson, Tracy H. Vemulapalli, J.R. Crisler, and R. Shepherd. Commonly used animal models. Principles of Animal Research for Graduate and Undergraduate Students, pages 117 - 175, 2016.
[35] Leland McInnes and John Healy. Umap: Uniform manifold approximation and projection for dimension reduction. ArXiv, abs/1802.03426, 2018.
[36] Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. Learnable latent embeddings for joint behavioural and neural analysis. Nature, May 2023.
[37] Mehdi Azabou, Michael Mendelson, Maks Sorokin, Shantanu Thakoor, Nauman Ahad, Carolina Urzay, and Eva L. Dyer. Learning behavior representations through multi-timescale bootstrapping. ArXiv, abs/2206.07041, 2022.
[38] Hanzhou Li, John T. Moon, Saptarshi Purkayastha, Leo Anthony Celi, Hari Trivedi, and Judy Wawira Gichoya. Ethics of large language models in medicine and medical research. The Lancet. Digital health, 2023.
[39] Tongshuang Sherry Wu, Michael Terry, and Carrie J. Cai. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, 2021.
[40] April Yi Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park, Justin D Weisz, Xuye Liu, Lingfei Wu, and Casey Dugan. Documentation matters: Human-centered ai system to assist data science code documentation in computational notebooks. ACM Transactions on Computer-Human Interaction, 29(2):1-33, 2022.
[41] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. arXiv preprint arXiv:2205.12674, 2022.
[42] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory. arXiv preprint arXiv:2306.07174, 2023.
[43] OpenAI. Gpt-4 technical report, 2023.
[44] Anthropic. Introducing claude, 2023.
[45] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.
[46] Alexander I. Hsu and Eric A. Yttri. B-soid, an open-source unsupervised algorithm for identification and fast prediction of behaviors. Nature Communications, 12, 2019.
[47] Mackenzie Mathis, Jessy Lauer, Tanmay Nath, Kai Sandbink, Michael Beauzile, Sébastien Hausmann, Steffen Schneider, and Alexander Mathis. DLC2Kinematics: a post-deeplabcut module for kinematic analysis. Zenodo, February 2020.</p>
<h1>7 Appendix</h1>
<h2>System Prompts</h2>
<p>Below are the core API system prompts used to create AmadeusGPT; related to Main Section 3.2.</p>
<div class="codehilite"><pre><span></span><code><span class="n">system_prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;Your name is AmadeusGPT and you are helping our users by first</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">understanding</span><span class="w"> </span><span class="n">my</span><span class="w"> </span><span class="n">API</span><span class="w"> </span><span class="n">docs</span><span class="p">:</span><span class="w"> </span>\<span class="n">n</span><span class="p">{</span><span class="n">interface_str</span><span class="p">}</span>\<span class="n">n</span><span class="p">{</span><span class="n">cls</span><span class="o">.</span><span class="n">behavior_modules_str</span><span class="p">}</span>\<span class="n">n</span><span class="s2">&quot;</span>
<span class="n">system_prompt</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s2">&quot;Now that you understand my API docs, If user asks things that are</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">achievable</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">my</span><span class="w"> </span><span class="n">APIs</span><span class="p">,</span><span class="w"> </span><span class="n">write</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">write</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">named</span><span class="w"> </span><span class="n">task_program</span><span class="p">()</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">values</span><span class="o">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">achieveable</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">API</span><span class="w"> </span><span class="n">docs</span><span class="p">,</span><span class="w"> </span><span class="n">explain</span><span class="w"> </span><span class="n">why</span><span class="o">.</span><span class="w"> </span><span class="n">Think</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">step</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">generating</span><span class="w"> </span><span class="n">code</span><span class="o">.</span><span class="w"> </span>\<span class="n">n</span><span class="w"> </span><span class="s2">&quot;</span>
<span class="n">system_prompt</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s2">&quot;Rule 1: (1) Do not access attriibutes of objects, unless attributes are</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Arributes</span><span class="w"> </span><span class="n">part</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">docs</span><span class="o">.</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">Do</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">call</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">functions</span><span class="w"> </span><span class="ow">or</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">methods</span><span class="w"> </span><span class="n">unless</span><span class="w"> </span><span class="n">they</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">Methods</span><span class="w"> </span><span class="n">part</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">func</span><span class="w"> </span><span class="n">docs</span><span class="o">.</span><span class="w"> </span>\<span class="n">n</span><span class="s2">&quot;</span>
<span class="n">system_prompt</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s2">&quot;Rule 2: Pay attention to whether the user means for single animal or</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">animals</span><span class="o">.</span>\<span class="n">n</span><span class="s2">&quot;</span>
<span class="n">system_prompt</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s2">&quot;Rule 3: Never call instance functions of matplotlib objects such as ax</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">plt</span><span class="o">.</span>\<span class="n">n</span><span class="s2">&quot;</span>
<span class="n">system_prompt</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s2">&quot;Rule 4: Do not write any comments in the function.</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="n">system_prompt</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s2">&quot;Rule 5: Must pay attention to the typing for the parameters and returns</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">functions</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">writing</span><span class="w"> </span><span class="n">code</span><span class="o">.</span>\<span class="n">n</span><span class="s2">&quot;</span>
<span class="n">system_prompt</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s2">&quot;Rule 6: When generating events, pay attention to whether it is</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">simultaneous</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">sequential</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">prompt</span><span class="o">.</span><span class="w"> </span><span class="n">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="n">events</span><span class="w"> </span><span class="n">descirbing</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">bodyparts</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">simultaneous</span><span class="w"> </span><span class="n">events</span><span class="o">.</span><span class="w"> </span><span class="n">Animal</span><span class="w"> </span><span class="n">moving</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">one</span>
\<span class="n">cdots</span><span class="o">&gt;</span><span class="w"> </span><span class="n">place</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">sequential</span><span class="w"> </span><span class="n">events</span><span class="o">.</span><span class="w"> </span>\<span class="n">n</span><span class="s2">&quot;</span>
</code></pre></div>

<h2>Dual memory mechanism, reload and relaunch example</h2>
<p>Related to Main Section 3.3, we discuss how short-term memory can be supplemented by long-term memory to restore chat history. AmadeusGPT stores information including chat histories in both short-term and long-term memory as well as segmentation masks and relation look-up tables. As they are all implemented as Python objects, it is easy to serialize them into disk and deserialize them from the disk. More specifically, we support two special prompts called "save" and "load" respectively. Those two special prompts can save the state of AmadeusGPT before exiting and load the state of AmadeusGPT after relaunching.</p>
<p>Query: "save"</p>
<h2>AmadeusGPT Output:</h2>
<p>Saving state into state.pickle
Query: "load"</p>
<h2>AmadeusGPT Output:</h2>
<p>Loaded state.pickle</p>
<h2>Examples of API core behavioral modules</h2>
<p>This is part of the core API docs that are always sent to AmadeusGPT when queried; related to Main Section 3.4.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nl">AnimalBehaviorAnalysis</span><span class="p">:</span>
<span class="w">    </span><span class="o">~~~</span>
<span class="w">    </span><span class="n">Methods</span>
<span class="w">    </span><span class="c1">-------</span>
<span class="w">    </span><span class="n">get_object_names</span><span class="p">()</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span>
<span class="w">    </span><span class="o">~~~</span><span class="w"> </span><span class="k">get</span><span class="w"> </span><span class="k">names</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="n">objects</span>
<span class="w">    </span><span class="o">~~~</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">animals_state_events</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">                    </span><span class="n">state_type</span><span class="p">,</span>
<span class="w">                    </span><span class="n">comparison</span><span class="p">,</span>
<span class="w">                    </span><span class="n">bodyparts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">&#39;all&#39;</span><span class="o">]</span><span class="p">,</span>
<span class="w">                    </span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="k">Parameters</span>
<span class="w">    </span><span class="nl">state_type</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span>
<span class="w">        </span><span class="n">Must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="s1">&#39;speed&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;acceleration&#39;</span>
<span class="w">    </span><span class="nl">comparison</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span>
<span class="w">        </span><span class="n">Must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comparison</span><span class="w"> </span><span class="n">operator</span><span class="w"> </span><span class="n">followed</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="o">&lt;</span><span class="mi">50</span><span class="p">,</span>
<span class="w">    </span><span class="n">Examples</span>
<span class="w">    </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="k">when</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="n">moving</span><span class="w"> </span><span class="n">faster</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">pixels</span><span class="w"> </span><span class="n">across</span><span class="w"> </span><span class="n">frames</span><span class="vm">?</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">task_program</span><span class="p">()</span><span class="err">:</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">behavior_analysis</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">AnimalBehaviorAnalysis</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">animal_faster_than_3_events</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span><span class="n">behavior_analysis</span><span class="p">.</span><span class="n">animals_state_events</span><span class="p">(</span><span class="s1">&#39;speed&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;&gt;3&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">animal_faster_than_3_events</span>
<span class="ss">&quot;&quot;</span><span class="err">&quot;</span>
<span class="k">return</span><span class="w"> </span><span class="n">animals_state_events</span><span class="p">(</span><span class="n">state_type</span><span class="p">,</span><span class="w"> </span><span class="n">comparison</span><span class="p">)</span>
<span class="n">def</span><span class="w"> </span><span class="n">superanimal_video_inference</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="k">None</span><span class="err">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Examples</span>
<span class="w">    </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="k">extract</span><span class="w"> </span><span class="n">keypoints</span><span class="w"> </span><span class="p">(</span><span class="n">aka</span><span class="w"> </span><span class="n">pose</span><span class="p">)</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">video</span><span class="w"> </span><span class="k">file</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">task_program</span><span class="p">()</span><span class="err">:</span>
<span class="w">    </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">behavior_analysis</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">AnimalBehaviorAnalysis</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">behavior_analysis</span><span class="p">.</span><span class="n">superanimal_video_inference</span><span class="p">()</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">superanimal_video_inference</span><span class="p">()</span>
<span class="n">def</span><span class="w"> </span><span class="n">animals_object_events</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
<span class="w">                        </span><span class="nf">object_name</span><span class="err">:</span><span class="w"> </span><span class="nf">str</span><span class="p">,</span>
<span class="w">                        </span><span class="n">relation_query</span><span class="p">,</span>
<span class="w">                        </span><span class="n">comparison</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span>
<span class="w">                        </span><span class="n">negate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">False</span><span class="p">,</span>
<span class="w">                        </span><span class="nl">bodyparts</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">&#39;all&#39;</span><span class="o">]</span><span class="p">,</span>
<span class="w">                        </span><span class="n">min_window</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">                        </span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nl">EventDict</span><span class="p">:</span>
<span class="w">    </span><span class="nf">object_name</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="nf">str</span><span class="p">.</span><span class="w"> </span><span class="n">Name</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span>
<span class="w">    </span><span class="nl">relation_query</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="p">.</span><span class="w"> </span><span class="n">Must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="s1">&#39;to_left&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;to_right&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;to_below&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="err">\</span><span class="w"> </span><span class="n">几o</span><span class="w"> </span><span class="s1">&#39;to_above&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;overlap&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;distance&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;angle&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;orientation&#39;</span>
<span class="w">    </span><span class="n">comparison</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="nf">str</span><span class="p">,</span><span class="w"> </span><span class="n">Must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comparison</span><span class="w"> </span><span class="n">operator</span><span class="w"> </span><span class="n">followed</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="o">&lt;</span><span class="mi">50</span><span class="p">,</span>
<span class="w">    </span><span class="err">\</span><span class="w"> </span><span class="n">optional</span>
<span class="w">    </span><span class="nl">bodyparts</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">optional</span>
<span class="w">    </span><span class="nl">min_window</span><span class="p">:</span><span class="w"> </span><span class="nf">min</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">event</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">include</span>
<span class="w">    </span><span class="nl">max_window</span><span class="p">:</span><span class="w"> </span><span class="nf">max</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">event</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">include</span>
<span class="w">    </span><span class="nl">negate</span><span class="p">:</span><span class="w"> </span><span class="n">bool</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="k">false</span>
<span class="w">        </span><span class="n">whether</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">negate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">spatial</span><span class="w"> </span><span class="n">events</span><span class="p">.</span><span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">negate</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">set</span><span class="w"> </span><span class="k">True</span><span class="p">,</span>
<span class="w">        </span><span class="err">\</span><span class="w"> </span><span class="n">inside</span><span class="w"> </span><span class="n">roi</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">outside</span><span class="w"> </span><span class="n">roi</span>
<span class="w">    </span><span class="k">Returns</span><span class="err">:</span>
<span class="w">    </span><span class="n">EventDict</span>
<span class="n">Examples</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="mi">6</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">task_program</span><span class="p">()</span><span class="err">:</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">behavior_analysis</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">AnimalBehaviorAnalysis</span><span class="p">()</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&gt;&gt;&gt; left_to_object6_events = behavior_analysis.animals_object_events(&#39;6&#39;,
~ &#39;to_left&#39;, bodyparts = [&#39;all&#39;])
&gt;&gt;&gt; return left_to_object6_events
&gt;&gt;&gt; # how much time the animal spends in closed arm?
&gt;&gt;&gt; def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
&gt;&gt;&gt; in_closed_arm_events = behavior_analysis.animals_object_events(&#39;closed
~ arm&#39;, &#39;overlap&#39;, bodyparts = [&#39;all&#39;], negate = False)
&gt;&gt;&gt; return in_closed_arm_events
&gt;&gt;&gt; # get events where animals&#39;s distance to animal0 is less than 30
&gt;&gt;&gt; def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
&gt;&gt;&gt; distance_social_events =
~ behavior_analysis.animals_object_events(&#39;animal0&#39;, &#39;disntace&#39;, comparison =
~ &#39;&lt;30&#39;, bodyparts = [&#39;nose&#39;])
&gt;&gt;&gt; return distance_social_events
&quot; &quot; &quot;
return animals_object_events(object_name, relation_query)
</code></pre></div>

<h1>Integration of behavioral modules (non-exhaustive)</h1>
<p>As described in the Main Section 3.5 and Main Figure 2, AmadeusGPT can use integration modules for task-specific behavior analysis, and implements dynamic loading by converting API documents into text embedding vectors and caching them in RAM or disk, allowing users to retrieve the most relevant modules by cosine similarity using their prompt as a query. Namely, the user could use these modules to integrate functionality. Or, they can write simple modules and contribute them to the AmadeusGPT codebase for other users to leverage as well. Below we provide examples of integration with the newly introduced dimensionality reduction tool called cebra [36], and UMAP [35]. In short, to integrate into the codebase the user should write a minimal working example in the style of the core API docs. Then, after dynamic loading, queries relating to UMAP or cebra (as shown here) would generate the following code (below) and related output plots (Figure 6).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Integration examples. Left to right: MausHaus masks with human annotated names, examples of UMAP and cebra integration modules, plots colored as in annotations. See below for user prompts to produce the plots.</p>
<h2>Cebra</h2>
<p>Cebra is a nonlinear dimensionality reduction tool for joint modeling of neural data and behavioral labels [36], which might be of interest to users of AmadeusGPT. In Appendix Figure 6 we show a 3D cebra embedding of mouse' poses (aligned) colored by when it overlaps with different objects. Note that the cebra embedding in Figure 6 is on a 3D n-sphere that is equivalent to a 2D Euclidean space [36]. Below the user query is shown below to produce images in Figure 6, along with the related API docs.</p>
<p>Query: "get 3D embeddings using cebra and plot the embedding."
def compute_embedding_with_cebra(n_dimension = 3,
" " " max_iterations=100):
Examples</p>
<blockquote>
<blockquote>
<blockquote>
<p># create a 3 dimensional embedding with cebra
def task_program():
behavior_analysis = AnimalBehaviorAnalysis()
embedding = behavior_analysis.compute_embedding_with_cebra(n_dimension = 3)
return embedding
" " "
return compute_embedding_with_cebra(n_dimension = n_dimension,
$\sim$ max_iterations=max_iterations)</p>
</blockquote>
</blockquote>
</blockquote>
<h1>UMAP</h1>
<p>UMAP [35] is a popular nonlinear dimensionality reduction tool that is also core to several behavioral analysis tools such as B-SOID [46]. In Appendix Figure 6 we show 2D UMAP embedding of poses (aligned) when it overlaps with different objects.</p>
<p>Query: "Get 2D embeddings using umap and plot the embedding"
def compute_embedding_with_umap(n_dimension = 3):
" " "
Examples</p>
<blockquote>
<blockquote>
<blockquote>
<p># create a 3 dimensional embedding umap
def task_program(features):
behavior_analysis = AnimalBehaviorAnalysis()
embedding = behavior_analysis.compute_embedding_with_umap(n_dimension = 3)
return embedding
" " "
return compute_embedding_with_umap(n_dimension = n_dimension)</p>
</blockquote>
</blockquote>
</blockquote>
<h2>Global spatio-temporal behavioral analyses</h2>
<p>Many core behavioral analysis tools rely on computing metrics based on kinematic or movement analysis. For example, time spent locomoting, time in an ROI, or computing the distance traveled within a specific area is highly popular in behavioral neuroscience research. Thus, inspired by DLC2Kinematics [47], we show API docs to integrate two such common integration modules:</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">calculate_distance_travelled</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">events</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="ss">&quot; &quot;</span><span class="w"> </span><span class="ss">&quot;</span>
<span class="ss">    Parameters</span>
<span class="ss">    --.-.-.-.-.-</span>
<span class="ss">    events: EventDict</span>
<span class="ss">    Return</span>
<span class="ss">    -------</span>
<span class="ss">    int</span>
<span class="ss">    Examples</span>
<span class="ss">    -------</span>
<span class="ss">    &gt;&gt;&gt; # what is the distance travelled in closed arm?</span>
<span class="ss">    &gt;&gt;&gt; def task_program():</span>
<span class="ss">    &gt;&gt;&gt; behavior_analysis = AnimalBehaviorAnalysis()</span>
<span class="ss">    &gt;&gt;&gt; roi_events = behavior_analysis.animals_object_events(&#39;closed arm&#39;, &#39;overlap&#39;,</span>
<span class="ss">    \ bodyparts = [&#39;all&#39;])</span>
<span class="ss">    &gt;&gt;&gt; distance_in_roi = behavior_analysis.calculate_distance_travelled(roi_events)</span>
<span class="ss">    &gt;&gt;&gt; return distance_in_roi</span>
<span class="ss">    &quot;</span><span class="w"> </span><span class="ss">&quot; &quot;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">calculate_distance_travelled</span><span class="p">(</span><span class="n">events</span><span class="p">)</span>
<span class="n">def</span><span class="w"> </span><span class="n">plot_object_ethogram</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">object_names</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="ss">&quot; &quot;</span><span class="w"> </span><span class="err">&quot;</span>
<span class="w">    </span><span class="k">Parameters</span>
<span class="w">    </span><span class="c1">---------</span>
<span class="w">    </span><span class="nl">object_names</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span>
<span class="w">    </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">names</span>
<span class="w">    </span><span class="k">Return</span><span class="err">:</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>-------
Tuple[plt.Figure, plt.Axes]
    tuple of figure and axes
Examples
-------
&gt;&gt;&gt; # plot ethogram for animal overlapping all objects
&gt;&gt;&gt; def task_program():
&gt;&gt;&gt; behavior_analysis = AnimalBehaviorAnalysis()
&gt;&gt;&gt; object_names = behavior_analysis.get_object_names()
&gt;&gt;&gt; fig, ax = behavior_analysis.plot_object_ethogram(object_names)
&gt;&gt;&gt; return fig, ax
&quot;&quot;&quot;
return plot_object_ethogram(object_names)
</code></pre></div>

<h1>Additional Experiments: behavioral analysis examples</h1>
<p>Here, as related to Main Section 4, we provide more example queries and responses from AmadeusGPT to support Main Figure 5a (EPM), to Main Figure 5b (MausHaus), and to Main Figure 5c (MABe) and results in Table 1.</p>
<h2>Extended Results on EPM with AmadeusGPT</h2>
<p>We show additional user queries and generated code from AmadeusGPT related to the EPM data presented in the Main Figure 5a.</p>
<h2>Extended Results on MausHaus with AmadeusGPT</h2>
<p>We show the user queries and generated code from AmadeusGPT that produce the images shown in the Main Figure 5b.</p>
<p>Query: "Give me the trajectories of the animal entering object 18 from right"</p>
<h2>AmadeusGPT Code Output:</h2>
<div class="codehilite"><pre><span></span><code>def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
    right_to_object_18_events = behavior_analysis.animals_object_events(&#39;18&#39;,&#39;to_right&#39;,
    ~ bodyparts = [&#39;all&#39;])
    enter_object_18_events = behavior_analysis.enter_object(&#39;18&#39;, bodyparts = [&#39;all&#39;])
    enter_object_18_from_right_events =
    ~ Event.add_sequential_events(right_to_object_18_events, enter_object_18_events)
    fig, ax = behavior_analysis.plot_trajectory([&quot;all&quot;], events =
    ~ enter_object_18_from_right_events)
    return fig, ax
</code></pre></div>

<p>Query: "plot trajectories of the animal for the shortest event where it enters object 18 and then its neck enters object 17"</p>
<h2>AmadeusGPT Code Output:</h2>
<div class="codehilite"><pre><span></span><code>def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
    enter_object_18_events = behavior_analysis.enter_object(&#39;18&#39;, bodyparts=[&#39;all&#39;])
    neck_enter_object_17_events = behavior_analysis.enter_object(&#39;17&#39;,
    ~ bodyparts=[&#39;neck&#39;])
    enter_object_18_and_neck_enter_object_17_events =
    ~ Event.add_sequential_events(enter_object_18_events, neck_enter_object_17_events)
    fig, ax = behavior_analysis.plot_trajectory(bodyparts=[&#39;all&#39;],
    ~ events=enter_object_18_and_neck_enter_object_17_events)
    return fig, ax
</code></pre></div>

<h1>Query:</h1>
<p>"Define &lt;|head dips|&gt; as a behavior where the animal's bodyparts mouse_center and at the same time neck in the roi and its head_midpoint outside roi. Get head dips."</p>
<h2>$\checkmark$ Amadeus Code Outputs:</h2>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h2>$\checkmark$ Query:</h2>
<p>"When is the animal moving faster than 8 pixels per frame?"
$\checkmark$ Amadeus Code Outputs:
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<h2>$\checkmark$ Query:</h2>
<p>"When is the animal moving faster than 8 pixels per frame?"
$\checkmark$ Amadeus Code Outputs:
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<h2>$\checkmark$ Amadeus Chat Outputs (Execution):</h2>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<h2>$\checkmark$ Query:</h2>
<p>"Define &lt;|freezing|&gt; as a behavior where the animal is slower than 0.5 . When is the
mouse freezing?"
$\checkmark$ Amadeus Code Outputs:
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<h2>$\checkmark$ Amadeus Chat Outputs (Execution):</h2>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 7: Extended Results on classical behavioral task (EPM) with AmadeusGPT. (a) Using the EPM dataset [13], we show additional queries and responses from AmadeusGPT. The raster plot is an ethogram where there is a tick for every related event in time, i.e., the mouse is in the closed arm or in the open arm.</p>
<h2>MABe Mouse Triplet Benchmark, extended details</h2>
<p>In the MABe 2022 Mouse Triplet Benchmark, participants are given a dataset of tracking data of triplets of socially interacting mice and asked to submit a frame-by-frame representation of the dataset. Here are the following user prompts and AmadeusGPT code outputs of the sub-tasks we considered and the generated code by AmadeusGPT.
Query: "Define &lt;|chases|&gt; as a social behavior where the closest distance between animals is less than 40 pixels and the chased animal is in front of the chasing animal and the chasing animal has to</p>
<p>travel faster than 3.4 pixels per frame. The smooth window size is 25 and min window is 30 . Get chases."</p>
<h1>AmadeusGPT Code Output:</h1>
<div class="codehilite"><pre><span></span><code>def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
    chase_events = behavior_analysis. animals_social_events([&#39;closest_distance&#39;,
    &#39;orientation&#39;],
    [&#39;&lt;40&#39;, &#39;==&#39;+str(Orientation.FRONT)],
    animal_state_relation_query_list=[&#39;speed&#39;],
    animal_state_comparison_list=[&#39;&gt;3.4&#39;],
    bodyparts=[&#39;all&#39;],
    otheranimal_bodyparts=[&#39;all&#39;],
    smooth_window_size=25,
    min_window=30)
    return chase_events
</code></pre></div>

<p>Query: "Define &lt;|huddles|&gt; as a social behavior where the distance between animals is less than 50, and the relative speed between two animals is less than 4 pixels per frame. The smooth window size is 61 and min window is 75 . Get huddles."</p>
<h2>AmadeusGPT Code Output:</h2>
<div class="codehilite"><pre><span></span><code>def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
    huddle_events = behavior_analysis. animals_social_events([&#39;distance&#39;, &#39;speed&#39;],
    [&#39;&lt;50&#39;, &#39;&lt;4&#39;],
    bodyparts=[&#39;all&#39;],
    min_window=75,
    smooth_window_size=61)
    return huddle_events
</code></pre></div>

<p>Query: "Define &lt;|contactl&gt; as a social behavior where the closest distance between animals is less than 12. The smooth window size is 11 and min window is 5 . Get contact."</p>
<h2>AmadeusGPT Code Output:</h2>
<div class="codehilite"><pre><span></span><code>def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
    contact_events = behavior_analysis. animals_social_events([&#39;closest_distance&#39;],
    ~ [ &#39;&lt;12&#39;], smooth_window_size=11, min_window=5)
    return contact_events
</code></pre></div>

<p>Query: "Define &lt;|closel&gt; as a social behavior where the closest distance between animals is less than 24. The min window is 5 . Get close."</p>
<h2>AmadeusGPT Code Output:</h2>
<div class="codehilite"><pre><span></span><code>def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
    close_events = behavior_analysis. animals_social_events([&#39;closest_distance&#39;], [&#39;&lt;24&#39;],
    ~ min_window=5)
    return close_events
</code></pre></div>

<p>Query: "Define &lt;|watchingl&gt; as a social behavior where distance between animals is less than 260 and larger than 50 and angle between animals is less than 15 . The min window is 100 and smooth window is 15 . Get watching."</p>
<h2>AmadeusGPT Code Output:</h2>
<div class="codehilite"><pre><span></span><code>def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
    watching_events = behavior_analysis. animals_social_events([&#39;distance&#39;, &#39;distance&#39;,
    ~ &#39;angle&#39;],
    [&#39;&lt;260&#39;, &#39;&gt;50&#39;, &#39;&lt;15&#39;],
</code></pre></div>

<div class="codehilite"><pre><span></span><code>bodyparts=[&#39;all&#39;],
min_window=100,
smooth_window_size=15)
return watching_events
</code></pre></div>

<p>Query: "Define &lt;|oral ear contact|&gt; as a social behavior where the closest distance between animal's nose and other animals' left ear and right ear is less than 10 . The smooth window size is 5 and min window is 15 . Get oral ear contact."</p>
<h1>AmadeusGPT Code Output:</h1>
<div class="codehilite"><pre><span></span><code>def task_program():
    behavior_analysis = AnimalBehaviorAnalysis()
    oral_ear_contact_events =
    ~ behavior_analysis.animals_social_events([&#39;closest_distance&#39;,
    ~ &#39;closest_distance&#39;],
    [&#39;&lt;10&#39;, &#39;&lt;10&#39;],
    bodyparts=[&#39;nose&#39;],
    otheranimal_bodyparts=[&#39;left ear&#39;, &#39;right ear&#39;],
    min_window=15,
    smooth_window_size=5)
    return oral_ear_contact_events
</code></pre></div>

<h2>Prompt robustness testing</h2>
<p>Related to Main Section 4, we show the full base questions and generated permutations we tested to gauge robustness.</p>
<h3>7.1 The input, base out of distribution (OOD) questions</h3>
<div class="codehilite"><pre><span></span><code><span class="s">&quot; &quot;</span><span class="w"> </span><span class="s">&quot;</span>
<span class="s">(1) define &lt;|head dips|&gt; as a behavior where the animal&#39;s bodypart mouse_center bodypart</span>
<span class="s">~ neck in the roi and its bodypart head_midpoint outside roi</span>
<span class="s">(2) When is the animal moving faster than 8?</span>
<span class="s">(3) define &lt;|freezing|&gt; as a behavior where the animal is slower than 0.5. When is the</span>
<span class="s">~ mouse freezing?</span>
<span class="s">(4) How much time the animal spends in the closed arm?</span>
<span class="s">(5) How much time the animal spends outside the closed arm?</span>
<span class="s">&quot;</span><span class="w"> </span><span class="s">&quot; &quot;</span>
</code></pre></div>

<h2>The 25 generated OOD questions</h2>
<div class="codehilite"><pre><span></span><code><span class="ss">&quot; &quot;</span><span class="w"> </span><span class="err">&quot;</span>
<span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">Dips</span><span class="w"> </span><span class="n">refer</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">behavior</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">animal</span><span class="s1">&#39;s bodypart, specifically the mouse_center</span>
<span class="s1">~ and neck, are within the ROI while the head_midpoint is outside the ROI.</span>
<span class="s1">(2) When an animal&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">mouse_center</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">neck</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">inside</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ROI</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">head_midpoint</span><span class="w"> </span><span class="k">is</span>
<span class="o">~</span><span class="w"> </span><span class="n">outside</span><span class="p">,</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">known</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">dips</span><span class="p">.</span>
<span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="n">Dips</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">characterized</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">presence</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">animal</span><span class="s1">&#39;s mouse_center and neck within</span>
<span class="s1">~ the ROI, while the head_midpoint is located outside the ROI.</span>
<span class="s1">(4) The behavior known as dips occurs when an animal&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">mouse_center</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">neck</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">within</span>
<span class="o">~</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ROI</span><span class="p">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">head_midpoint</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">outside</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="n">behavior</span><span class="p">,</span><span class="w"> </span><span class="n">dips</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">defined</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">presence</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">mouse_center</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">neck</span>
<span class="o">~</span><span class="w"> </span><span class="k">within</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ROI</span><span class="p">,</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">head_midpoint</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">situated</span><span class="w"> </span><span class="n">outside</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="w"> </span><span class="k">At</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="n">exceed</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">speed</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="mi">8</span><span class="vm">?</span>
<span class="p">(</span><span class="mi">7</span><span class="p">)</span><span class="w"> </span><span class="k">When</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="n">faster</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="mi">8</span><span class="vm">?</span>
<span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="w"> </span><span class="k">At</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">moment</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="n">surpass</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">speed</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="mi">8</span><span class="vm">?</span>
<span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="w"> </span><span class="k">When</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="n">traveling</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">speed</span><span class="w"> </span><span class="n">greater</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="mi">8</span><span class="vm">?</span>
<span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="w"> </span><span class="k">At</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="nc">time</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="n">beyond</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">speed</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="mi">8</span><span class="vm">?</span>
<span class="p">(</span><span class="mi">11</span><span class="p">)</span><span class="w"> </span><span class="k">At</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">say</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mouse</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">freezing</span><span class="vm">?</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">define</span><span class="w"> </span><span class="n">freezing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">behavior</span>
<span class="o">~</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">animal</span><span class="w"> </span><span class="n">moves</span><span class="w"> </span><span class="n">slower</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="mf">0.5</span><span class="p">.</span>
<span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="w"> </span><span class="k">When</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mouse</span><span class="w"> </span><span class="n">exhibit</span><span class="w"> </span><span class="n">freezing</span><span class="w"> </span><span class="n">behavior</span><span class="vm">?</span><span class="w"> </span><span class="n">Freezing</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">defined</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">state</span><span class="w"> </span><span class="k">where</span>
<span class="o">~</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">animal</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">movement</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">slower</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="mf">0.5</span><span class="p">.</span>
<span class="p">(</span><span class="mi">13</span><span class="p">)</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">definition</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">freezing</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">mice</span><span class="vm">?</span><span class="w"> </span><span class="n">Freezing</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">characterized</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">movement</span>
<span class="o">~</span><span class="w"> </span><span class="n">speed</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="k">less</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="mf">0.5</span><span class="p">.</span><span class="w"> </span><span class="k">When</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mouse</span><span class="w"> </span><span class="n">exhibit</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">behavior</span><span class="vm">?</span>
</code></pre></div>            </div>
        </div>

    </div>
</body>
</html>
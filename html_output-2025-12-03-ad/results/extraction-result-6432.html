<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6432 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6432</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6432</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-276574652</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.16090v1.pdf" target="_blank">Echo: A Large Language Model with Temporal Episodic Memory</a></p>
                <p><strong>Paper Abstract:</strong> Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation. However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries. This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers. To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We propose a Multi-Agent Data Generation Framework that guides the model in generating multi-turn, complex scenario episodic memory dialogue data (EM-Train). Temporal information is innovatively incorporated into the LLM training process, and Echo is trained using the EM-Train. Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities. The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues. Our experiments demonstrate that Echo significantly outperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities. We will open-source all datasets, code, and model weights.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6432.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6432.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Echo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Echo: A Large Language Model with Temporal Episodic Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ChatGLM-3‑6B based LLM fine‑tuned on a temporally annotated, multi‑turn episodic dialogue dataset (EM‑Train) and trained with a user‑time‑assistant paradigm to internalize temporal episodic memory and improve multi‑turn episodic recall and temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Echo</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Decoder‑only LLM (ChatGLM3‑6B base) fully fine‑tuned on EM‑Train; incorporates temporal episodic memory by (1) training on dialogues where each turn is annotated with explicit timestamps (an added 'observation' role/time tokens that are part of the context), and (2) internalizing episodic patterns into model weights so that at inference time the model uses time‑stamped context plus its learned parameters to recall and reason about events.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈6B (ChatGLM3-6B base, full fine‑tune)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Internalized temporal episodic memory via fine‑tuning on time‑stamped dialogues (implicit episodic memory stored in model weights and as time‑stamped context in session), not an external datastore</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw multi‑turn dialogue text with explicit timestamp observations added as an 'observation' role (time‑stamped tokens embedded in the context); episodic structure implicitly encoded in model weights after fine‑tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Standard transformer attention over the augmented context (user + time/observation + assistant) at inference; retrieval is performed by attending to time‑stamped context and leveraging internalized patterns encoded in weights; writes/updates performed by full fine‑tuning on EM‑Train (no online gradient updates during inference), and real‑time insertion of timestamped observations into context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>EM‑Test (episodic memory multi‑turn dialogue benchmark) and temporal reasoning evaluation datasets (including a temporally aware reasoning dataset in appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Dialogue / episodic memory evaluation and temporal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Human score: Easy 6.7, Hard 5.9 (scale 1–10); Similarity metric (sentence‑embedding cosine ×100): Easy 84.0, Hard 74.5; Temporal reasoning (appendix): >90 (short & long) on temporally aware dataset (Echo short 98.1, long 94.6 as reported in appendix analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>EM‑Test‑Without‑Time (no temporal context provided): Echo reported 96.0 (Easy) and 81.8 (Hard) in Table 4 (these are comparison numbers for episodic memory evaluation without explicit temporal context); note this is evaluation without temporal information rather than disabling the model's learned episodic capability entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human score (1–10) and sentence‑embedding cosine similarity (×100); appendix uses keyword matching for temporal reasoning dataset (percent correct).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Authors argue their approach avoids costly external memory operations (so lower retrieval latency compared to external stores) and avoids context segmentation loss; they do not report quantitative latency or memory‑footprint numbers for Echo, but note observation tokens do not participate in gradient updates and attention masks remain standard (so no special architectural cost). No explicit compute/memory cost numbers provided for the fine‑tuning step on EM‑Train.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Models (including Echo) still show lower performance on 'hard' difficulty items versus 'easy'; models across the board perform poorly on some intermediate time spans (few days, few months, few years) indicating difficulty with certain temporal concepts. Echo did not always outperform on specific easy subcases ('one day' and 'several decades' human‑score exceptions). The paper also acknowledges that prior external‑storage methods retrieve copies rather than enabling constructive episodic recall, implying this internal approach may still be limited in lifelong scaling/very long history storage (not explicitly solved).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wentao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, Jiali Liu, 2025. Echo: A Large Language Model with Temporal Episodic Memory. Proceedings of the 41st International Conference on Machine Learning (ICML), PMLR 267 / arXiv:2502.16090.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Echo: A Large Language Model with Temporal Episodic Memory', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6432.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6432.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory mechanism for LLMs that processes historical conversation to extract summaries and a user portrait, then retrieves relevant items based on similarity to form a Meta Prompt to assist the model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>External memory augmentation that summarizes conversation history and stores user portrait; at query time retrieves similar items and assembles them into a meta prompt preceding model input.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External memory / retrieval‑augmented prompt (summary + user profile store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Summaries of historical conversation and user portrait metadata (textual summaries/records).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Similarity retrieval over stored summaries to construct a Meta Prompt that is prepended to the model input.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long‑term conversational memory / assisting responses (no single standardized benchmark reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Dialogue / long‑term memory augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Related‑work critique in this paper: such external storage approaches require operations that can be time‑consuming and may arbitrarily segment context causing information loss.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors state these methods retrieve copies rather than supporting constructive episodic recall; potential info loss from segmentation and runtime cost from external operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>W. Zhong et al., 2024. MemoryBank: Enhancing large language models with long-term memory. (cited in Echo paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Echo: A Large Language Model with Temporal Episodic Memory', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6432.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6432.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EM-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EM-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that incorporates key information from long context into preceding prompts to enable handling of large context lengths efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>EM-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EM-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval/augmentation approach that extracts key information and inserts it into previous prompts (context augmentation) to allow near‑unlimited effective context length with efficient computation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval‑augmented prompt (context/key insertion)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Extracted key information / summaries from long context (textual snippets).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Extraction of key info and incorporation into earlier parts of the prompt to make it available to the model; retrieval likely via similarity/heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Handling very long contexts / extended conversational history</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Dialogue / long context handling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Claimed to maintain high computational efficiency vs naive long‑context processing; paper criticizes external approaches broadly for being time‑consuming.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Does not explicitly address constructive episodic memory (retrieves/places copies of key info rather than constructing memories); potential segmentation/information loss issues as noted by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Z. Fountas et al., 2024. EM-LLM (cited in Echo paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Echo: A Large Language Model with Temporal Episodic Memory', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6432.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6432.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that simulates extended virtual memory for LLMs by paging between the model's context and disk, analogous to OS virtual memory, to allow tasks beyond current context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory system that implements paging between in‑context memory and disk storage so an LLM can handle information exceeding its immediate context window.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External storage with paging (virtual memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Pages of context stored on disk (textual pages / segments).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Paging mechanism that swaps segments between disk and context (analogous to OS paging) to present relevant content to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Extended context tasks / beyond context window capability</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Dialogue / long context / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Related‑work note: such external operations can be time‑consuming; paging introduces runtime overhead and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Potential latency and complexity from frequent paging; authors of Echo claim these approaches retrieve copies rather than supporting constructive episodic recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>C. Packer et al., 2023. MemGPT: Towards LLMs as operating systems (cited in Echo paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Echo: A Large Language Model with Temporal Episodic Memory', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6432.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6432.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MS (Memory Sharing framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory‑sharing framework proposed for storing and retrieving historical information to support LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture that provides a memory‑sharing structure to store historical conversational artifacts and permit retrieval for response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External memory / memory‑sharing framework</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored historical information in some structured or shared format (textual records).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Retrieval of pertinent historical data from the shared memory to support response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long‑term conversational support</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Dialogue / long‑term memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper notes external memory approaches involve extra operations and potential segmentation loss; specifics for MS not given in Echo paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Does not inherently implement constructive episodic memory; external operation overhead and possible information loss from segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>H. Gao & Y. Zhang, 2024. MS (cited in Echo paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Echo: A Large Language Model with Temporal Episodic Memory', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6432.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6432.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>H-EMV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>H-EMV</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory architecture using a hierarchical/tree‑based storage structure to hold historical information for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>H-EMV</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>H-EMV</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Introduces a tree‑based storage structure for organizing and retrieving historical conversation data to assist LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Tree‑based external storage</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured nodes in a tree representing historical dialogue or events (textual content attached to nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Tree traversal/retrieval to find relevant historical nodes for inclusion in prompts or retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dialogue memory retrieval and usage</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Dialogue / long‑term memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>External structure operations may be time‑consuming; potential segmentation issues as general critique in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Focuses on retrieval of stored copies; does not claim to improve constructive memory internalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>L. Bärmann et al., 2024. H-EMV (cited in Echo paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Echo: A Large Language Model with Temporal Episodic Memory', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6432.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6432.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHATDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHATDB</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLMs with a specialized database as symbolic memory to store and retrieve historical data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CHATDB</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CHATDB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Connects LLMs to a symbolic database storing historical information; retrieval from the DB is used to augment model inputs for response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Specialized external database (symbolic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured database entries representing historical conversation or facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Database queries to fetch relevant records which are then used to augment prompts/input to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Conversational long‑term memory and retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Dialogue / retrieval / symbolic memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>External DB operations may add latency; the Echo paper states such approaches retrieve copies rather than enable constructive episodic recall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>May suffer from cost/latency of DB operations and potential segmentation or information‑loss issues when extracting relevant context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>C. Hu et al., 2023. CHATDB: Augmenting LLMs with databases as their symbolic memory (cited in Echo paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Echo: A Large Language Model with Temporal Episodic Memory', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MemoryBank <em>(Rating: 2)</em></li>
                <li>EM-LLM <em>(Rating: 2)</em></li>
                <li>MemGPT <em>(Rating: 2)</em></li>
                <li>H-EMV <em>(Rating: 2)</em></li>
                <li>CHATDB <em>(Rating: 2)</em></li>
                <li>MS <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6432",
    "paper_id": "paper-276574652",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "Echo",
            "name_full": "Echo: A Large Language Model with Temporal Episodic Memory",
            "brief_description": "A ChatGLM-3‑6B based LLM fine‑tuned on a temporally annotated, multi‑turn episodic dialogue dataset (EM‑Train) and trained with a user‑time‑assistant paradigm to internalize temporal episodic memory and improve multi‑turn episodic recall and temporal reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Echo",
            "agent_description": "Decoder‑only LLM (ChatGLM3‑6B base) fully fine‑tuned on EM‑Train; incorporates temporal episodic memory by (1) training on dialogues where each turn is annotated with explicit timestamps (an added 'observation' role/time tokens that are part of the context), and (2) internalizing episodic patterns into model weights so that at inference time the model uses time‑stamped context plus its learned parameters to recall and reason about events.",
            "model_size": "≈6B (ChatGLM3-6B base, full fine‑tune)",
            "memory_used": true,
            "memory_type": "Internalized temporal episodic memory via fine‑tuning on time‑stamped dialogues (implicit episodic memory stored in model weights and as time‑stamped context in session), not an external datastore",
            "memory_representation": "Raw multi‑turn dialogue text with explicit timestamp observations added as an 'observation' role (time‑stamped tokens embedded in the context); episodic structure implicitly encoded in model weights after fine‑tuning.",
            "memory_access_mechanism": "Standard transformer attention over the augmented context (user + time/observation + assistant) at inference; retrieval is performed by attending to time‑stamped context and leveraging internalized patterns encoded in weights; writes/updates performed by full fine‑tuning on EM‑Train (no online gradient updates during inference), and real‑time insertion of timestamped observations into context.",
            "task_name": "EM‑Test (episodic memory multi‑turn dialogue benchmark) and temporal reasoning evaluation datasets (including a temporally aware reasoning dataset in appendix)",
            "task_category": "Dialogue / episodic memory evaluation and temporal reasoning",
            "performance_with_memory": "Human score: Easy 6.7, Hard 5.9 (scale 1–10); Similarity metric (sentence‑embedding cosine ×100): Easy 84.0, Hard 74.5; Temporal reasoning (appendix): &gt;90 (short & long) on temporally aware dataset (Echo short 98.1, long 94.6 as reported in appendix analysis).",
            "performance_without_memory": "EM‑Test‑Without‑Time (no temporal context provided): Echo reported 96.0 (Easy) and 81.8 (Hard) in Table 4 (these are comparison numbers for episodic memory evaluation without explicit temporal context); note this is evaluation without temporal information rather than disabling the model's learned episodic capability entirely.",
            "has_comparative_results": true,
            "performance_metric": "Human score (1–10) and sentence‑embedding cosine similarity (×100); appendix uses keyword matching for temporal reasoning dataset (percent correct).",
            "tradeoffs_reported": "Authors argue their approach avoids costly external memory operations (so lower retrieval latency compared to external stores) and avoids context segmentation loss; they do not report quantitative latency or memory‑footprint numbers for Echo, but note observation tokens do not participate in gradient updates and attention masks remain standard (so no special architectural cost). No explicit compute/memory cost numbers provided for the fine‑tuning step on EM‑Train.",
            "limitations_or_failure_cases": "Models (including Echo) still show lower performance on 'hard' difficulty items versus 'easy'; models across the board perform poorly on some intermediate time spans (few days, few months, few years) indicating difficulty with certain temporal concepts. Echo did not always outperform on specific easy subcases ('one day' and 'several decades' human‑score exceptions). The paper also acknowledges that prior external‑storage methods retrieve copies rather than enabling constructive episodic recall, implying this internal approach may still be limited in lifelong scaling/very long history storage (not explicitly solved).",
            "citation": "Wentao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, Jiali Liu, 2025. Echo: A Large Language Model with Temporal Episodic Memory. Proceedings of the 41st International Conference on Machine Learning (ICML), PMLR 267 / arXiv:2502.16090.",
            "uuid": "e6432.0",
            "source_info": {
                "paper_title": "Echo: A Large Language Model with Temporal Episodic Memory",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank",
            "brief_description": "A memory mechanism for LLMs that processes historical conversation to extract summaries and a user portrait, then retrieves relevant items based on similarity to form a Meta Prompt to assist the model.",
            "citation_title": "MemoryBank",
            "mention_or_use": "mention",
            "agent_name": "MemoryBank",
            "agent_description": "External memory augmentation that summarizes conversation history and stores user portrait; at query time retrieves similar items and assembles them into a meta prompt preceding model input.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "External memory / retrieval‑augmented prompt (summary + user profile store)",
            "memory_representation": "Summaries of historical conversation and user portrait metadata (textual summaries/records).",
            "memory_access_mechanism": "Similarity retrieval over stored summaries to construct a Meta Prompt that is prepended to the model input.",
            "task_name": "Long‑term conversational memory / assisting responses (no single standardized benchmark reported in this paper)",
            "task_category": "Dialogue / long‑term memory augmentation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Related‑work critique in this paper: such external storage approaches require operations that can be time‑consuming and may arbitrarily segment context causing information loss.",
            "limitations_or_failure_cases": "Authors state these methods retrieve copies rather than supporting constructive episodic recall; potential info loss from segmentation and runtime cost from external operations.",
            "citation": "W. Zhong et al., 2024. MemoryBank: Enhancing large language models with long-term memory. (cited in Echo paper)",
            "uuid": "e6432.1",
            "source_info": {
                "paper_title": "Echo: A Large Language Model with Temporal Episodic Memory",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "EM-LLM",
            "name_full": "EM-LLM",
            "brief_description": "A method that incorporates key information from long context into preceding prompts to enable handling of large context lengths efficiently.",
            "citation_title": "EM-LLM",
            "mention_or_use": "mention",
            "agent_name": "EM-LLM",
            "agent_description": "Retrieval/augmentation approach that extracts key information and inserts it into previous prompts (context augmentation) to allow near‑unlimited effective context length with efficient computation.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Retrieval‑augmented prompt (context/key insertion)",
            "memory_representation": "Extracted key information / summaries from long context (textual snippets).",
            "memory_access_mechanism": "Extraction of key info and incorporation into earlier parts of the prompt to make it available to the model; retrieval likely via similarity/heuristics.",
            "task_name": "Handling very long contexts / extended conversational history",
            "task_category": "Dialogue / long context handling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Claimed to maintain high computational efficiency vs naive long‑context processing; paper criticizes external approaches broadly for being time‑consuming.",
            "limitations_or_failure_cases": "Does not explicitly address constructive episodic memory (retrieves/places copies of key info rather than constructing memories); potential segmentation/information loss issues as noted by authors.",
            "citation": "Z. Fountas et al., 2024. EM-LLM (cited in Echo paper)",
            "uuid": "e6432.2",
            "source_info": {
                "paper_title": "Echo: A Large Language Model with Temporal Episodic Memory",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT",
            "brief_description": "An approach that simulates extended virtual memory for LLMs by paging between the model's context and disk, analogous to OS virtual memory, to allow tasks beyond current context limits.",
            "citation_title": "MemGPT",
            "mention_or_use": "mention",
            "agent_name": "MemGPT",
            "agent_description": "Memory system that implements paging between in‑context memory and disk storage so an LLM can handle information exceeding its immediate context window.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "External storage with paging (virtual memory)",
            "memory_representation": "Pages of context stored on disk (textual pages / segments).",
            "memory_access_mechanism": "Paging mechanism that swaps segments between disk and context (analogous to OS paging) to present relevant content to the model.",
            "task_name": "Extended context tasks / beyond context window capability",
            "task_category": "Dialogue / long context / retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Related‑work note: such external operations can be time‑consuming; paging introduces runtime overhead and complexity.",
            "limitations_or_failure_cases": "Potential latency and complexity from frequent paging; authors of Echo claim these approaches retrieve copies rather than supporting constructive episodic recall.",
            "citation": "C. Packer et al., 2023. MemGPT: Towards LLMs as operating systems (cited in Echo paper)",
            "uuid": "e6432.3",
            "source_info": {
                "paper_title": "Echo: A Large Language Model with Temporal Episodic Memory",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MS",
            "name_full": "MS (Memory Sharing framework)",
            "brief_description": "A memory‑sharing framework proposed for storing and retrieving historical information to support LLM responses.",
            "citation_title": "MS",
            "mention_or_use": "mention",
            "agent_name": "MS",
            "agent_description": "Architecture that provides a memory‑sharing structure to store historical conversational artifacts and permit retrieval for response generation.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "External memory / memory‑sharing framework",
            "memory_representation": "Stored historical information in some structured or shared format (textual records).",
            "memory_access_mechanism": "Retrieval of pertinent historical data from the shared memory to support response generation.",
            "task_name": "Long‑term conversational support",
            "task_category": "Dialogue / long‑term memory",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper notes external memory approaches involve extra operations and potential segmentation loss; specifics for MS not given in Echo paper.",
            "limitations_or_failure_cases": "Does not inherently implement constructive episodic memory; external operation overhead and possible information loss from segmentation.",
            "citation": "H. Gao & Y. Zhang, 2024. MS (cited in Echo paper)",
            "uuid": "e6432.4",
            "source_info": {
                "paper_title": "Echo: A Large Language Model with Temporal Episodic Memory",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "H-EMV",
            "name_full": "H-EMV",
            "brief_description": "A memory architecture using a hierarchical/tree‑based storage structure to hold historical information for LLMs.",
            "citation_title": "H-EMV",
            "mention_or_use": "mention",
            "agent_name": "H-EMV",
            "agent_description": "Introduces a tree‑based storage structure for organizing and retrieving historical conversation data to assist LLM responses.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Tree‑based external storage",
            "memory_representation": "Structured nodes in a tree representing historical dialogue or events (textual content attached to nodes).",
            "memory_access_mechanism": "Tree traversal/retrieval to find relevant historical nodes for inclusion in prompts or retrieval augmentation.",
            "task_name": "Dialogue memory retrieval and usage",
            "task_category": "Dialogue / long‑term memory",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "External structure operations may be time‑consuming; potential segmentation issues as general critique in paper.",
            "limitations_or_failure_cases": "Focuses on retrieval of stored copies; does not claim to improve constructive memory internalization.",
            "citation": "L. Bärmann et al., 2024. H-EMV (cited in Echo paper)",
            "uuid": "e6432.5",
            "source_info": {
                "paper_title": "Echo: A Large Language Model with Temporal Episodic Memory",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CHATDB",
            "name_full": "CHATDB",
            "brief_description": "A method that augments LLMs with a specialized database as symbolic memory to store and retrieve historical data.",
            "citation_title": "CHATDB",
            "mention_or_use": "mention",
            "agent_name": "CHATDB",
            "agent_description": "Connects LLMs to a symbolic database storing historical information; retrieval from the DB is used to augment model inputs for response generation.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Specialized external database (symbolic memory)",
            "memory_representation": "Structured database entries representing historical conversation or facts.",
            "memory_access_mechanism": "Database queries to fetch relevant records which are then used to augment prompts/input to the LLM.",
            "task_name": "Conversational long‑term memory and retrieval",
            "task_category": "Dialogue / retrieval / symbolic memory",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "External DB operations may add latency; the Echo paper states such approaches retrieve copies rather than enable constructive episodic recall.",
            "limitations_or_failure_cases": "May suffer from cost/latency of DB operations and potential segmentation or information‑loss issues when extracting relevant context.",
            "citation": "C. Hu et al., 2023. CHATDB: Augmenting LLMs with databases as their symbolic memory (cited in Echo paper)",
            "uuid": "e6432.6",
            "source_info": {
                "paper_title": "Echo: A Large Language Model with Temporal Episodic Memory",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MemoryBank",
            "rating": 2,
            "sanitized_title": "memorybank"
        },
        {
            "paper_title": "EM-LLM",
            "rating": 2
        },
        {
            "paper_title": "MemGPT",
            "rating": 2
        },
        {
            "paper_title": "H-EMV",
            "rating": 2
        },
        {
            "paper_title": "CHATDB",
            "rating": 2
        },
        {
            "paper_title": "MS",
            "rating": 2
        }
    ],
    "cost": 0.01368875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Echo: A Large Language Model with Temporal Episodic Memory
22 Feb 2025</p>
<p>Wentao Liu 
Ruohua Zhang 
Aimin Zhou 
Feng Gao 
Jiali Liu 
Echo: A Large Language Model with Temporal Episodic Memory
22 Feb 2025F3898DB2E107C4DFC90FCC2341A746D3arXiv:2502.16090v1[cs.CL]
Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation.However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries.This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers.To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory.We propose a Multi-Agent Data Generation Framework that guides the model in generating multi-turn, complex scenario episodic memory dialogue data (EM-Train).Temporal information is innovatively incorporated into the LLM training process, and Echo is trained using the EM-Train.Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities.The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multiturn episodic memory dialogues.Our experiments demonstrate that Echo significantly outperforms state-of-the-art LLMs on EM-Test.Additionally, a qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities.We will open-source all datasets, code, and model weights.</p>
<p>Introduction</p>
<p>Research on large language models (LLMs) has made significant advances in many fields (Naveed et al., 2023; Zhao   1 Institute of AI Education, East China Normal University, Shanghai, China 2 School of Computer Science and Technology, East China Normal University, Shanghai, China.Correspondence to: Aimin Zhou <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#109;&#122;&#104;&#111;&#117;&#64;&#99;&#115;&#46;&#101;&#99;&#110;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#97;&#109;&#122;&#104;&#111;&#117;&#64;&#99;&#115;&#46;&#101;&#99;&#110;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a>.</p>
<p>Proceedings of the 41 st International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025.Copyright 2025 by the author(s).Figure 2. The relationship between episodic memory and longterm memory (Squire, 2004).et al., 2023), such as mathematical problem (Liu et al., 2023), programming (Zhang et al., 2023), and tool usage (Qin et al., 2024).However, these tasks primarily rely on semantic memory, with little focus on evaluating and enhancing the LLMs' episodic memory capabilities.</p>
<p>Episodic memory is a crucial component of human memory (Tulving, 1983;1972).As shown in Fig. 2, long-term memory (Squire, 2004) is mainly divided into declarative and non-declarative memory.Declarative memory further comprises semantic memory and episodic memory.Semantic memory involves the recollection of widely accepted concrete facts, which relate to knowledge independent of its context of acquisition (Moscovitch et al., 2016).It includes world knowledge, entity memory, language memory, and concept memory, among others.For example, "China is in Asia" or "1+2=3".In contrast, episodic memory refers to time-related event memories centered around the individual, such as "Last night, I bought tomatoes at Walmart".In fact, episodic memory is not only a fundamental ability of humans but also a critical capability for LLMs, impacting their performance in any multi-turn Q&amp;A scenarios, such as role-playing (Wang et al., 2023), psychological counseling (Ke et al., 2024), and AI teaching (Dan et al., 2023).Unfortunately, even the most advanced models (e.g., GPT-4) still perform poorly in terms of episodic memory, often suffering from logical inconsistencies and hallucinations.</p>
<p>Some methods (Zhong et al., 2024;Bärmann et al., 2024;Fountas et al., 2024;Packer et al., 2023;Gao &amp; Zhang, 2024;Hu et al., 2023) have been proposed to enhance the long-term memory capabilities of LLMs.These methods primarily use external storage to retain historical records and design operations to help LLMs retrieve this information for responses.However, these approaches can be timeconsuming due to the operations on external storage, and context information may be arbitrarily segmented, leading to information loss.Additionally, these methods do not improve the model's inherent ability to process episodic memory.Episodic memory is thought to be constructive, meaning recall is the (re)construction of a past experience rather than the retrieval of a copy (Sprott, 1933;Schacter, 2012).</p>
<p>In practice, generative models have an inherent capability to construct and consolidate memories (Spens &amp; Burgess, 2024).We argue that LLMs face a significant challenge in developing robust episodic memory capabilities due to the limited availability of high-quality episodic memory data.Such data is essential for training models to effectively handle complex, context-dependent interactions.</p>
<p>First, we propose MADGF, a innovative Multi-Agent Data Generation Framework.MADGF simulates and controls multi-turn scenario dialogues between multiple human roles and an AI assistant.The collected dialogue data, named EM-Train, is used to train our Echo model.In MADGF, three key components are designed: characters, plots, and environments.The design of characters and environments ensures a diverse range of dialogues, while plots guide the LLM to generate dialogue data with enhanced episodic memory capabilities.Additionally, the LLM's training paradigm is modified by incorporating temporal information into each conversation, enriching the temporal background in the interaction process.</p>
<p>Next, we introduce EM-Test, a novel multi-turn dialogue benchmark designed to evaluate episodic memory capabilities.Each instance in EM-Test may contain multiple evaluation points, requiring the model not only to process long-context text effectively but also to recall, reason, and cognitively handle episodic memory information.Each evaluation point is tagged with both time and difficulty levels, enabling a comprehensive assessment.To reduce manual evaluation efforts, we propose an approach that assesses model performance based on semantic similarity.The feasibility and effectiveness of approach is validated by its strong correlation with human evaluations.</p>
<p>Finally, we conducted both quantitative and qualitative experiments.The quantitative results show that Echo significantly outperforms state-of-the-art LLMs on the EM-Test.</p>
<p>Additionally, the qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities.</p>
<p>Related Work</p>
<p>Methods for Enhancing Long-Term Memory Capabilities Some methods have been proposed to enhance the long-term memory capabilities of large models, such as MemoryBank (Zhong et al., 2024), H-EMV (Bärmann et al., 2024), EM-LLM (Fountas et al., 2024), MemGPT (Packer et al., 2023), MS (Gao &amp; Zhang, 2024), and CHATDB (Hu et al., 2023).These methods use external storage to retain historical information and design various operations to help LLMs utilize information.</p>
<p>MemoryBank (Zhong et al., 2024) introduces a novel memory mechanism specifically designed for LLM.This mechanism processes historical conversation to extract summary information and user portrait.When a user poses a question, the mechanism retrieves relevant information based on similarity and combines it with the summary information and user portrait, to form a Meta Prompt that assists the model in generating responses.EM-LLM (Fountas et al., 2024) adopts a similar method by incorporating key information into preceding prompts.This method effectively handles nearly unlimited context lengths while maintaining high computational efficiency.MemGPT (Packer et al., 2023) enables LLMs to perform tasks beyond the current context limits by simulating extended virtual memory through paging between physical memory and disk storage, akin to how operating systems manage memory to extend LLM context.MS (Gao &amp; Zhang, 2024), H-EMV (Bärmann et al., 2024), and CHATDB (Hu et al., 2023) introduce distinct data structures designed for the storage of historical information: namely, a memory-sharing framework, a tree-based storage structure, and a specialized database, respectively.Each of these architectures facilitates the retrieval of pertinent historical data to support the response generation.</p>
<p>These methods require various operations on external storage that can be time-consuming.Moreover, they primarily focus on retrieving a copy of the data, rather than implementing the constructive nature of episodic memory (Sprott, 1933;Schacter, 2012), failing to enhance the model's inherent ability to process episodic memory.</p>
<p>Methods for Data Generation utilizing LLM Manually annotated data is expensive, so many methods (Xu et al., 2023;Luo et al., 2023;Zhao et al., 2024;Wang et al., 2022;Ding et al., 2023;Li et al., 2023) have been proposed to automate data generation utilizing LLMs.Besides obtaining data through user interactions on online platforms using ChatGPT, like WILDCHAT (Zhao et al., 2024), Self-Instruct (Wang et al., 2022) was one of the first to propose generating instructions, inputs, and outputs using LLMs to build instruction fine-tuning data.To increase the diversity of instructions, WizardLM (Xu et al., 2023) introduced an evolutionary instruction approach starting from a small set of seed instructions to generate more complex and diverse instruction.Further, WizardMath (Luo et al., 2023) incorporated a reward model to select better instruction data from multiple outputs, collecting higher-quality generated data.</p>
<p>Additionally, some methods (Ding et al., 2023;Li et al., 2023) propose having LLMs play the roles of both AI assistant and user to collect data, which allows for the collection of multi-turn dialogues.UltraChat (Ding et al., 2023) uses this approach to extract instruction data covering various tasks, such as Questions about the World and Creation and Generation.In contrast, CAMEL (Li et al., 2023) focuses on generating instruction data for specific tasks, such as "Develop a trading bot for the stock market."</p>
<p>These LLM-based data generation methods primarily focus on extracting high-quality instruction fine-tuning data grounded in semantic memory from LLMs.In contrast, our MADGF mainly aims to simulate real-life scenarios to generate dialogue content rich in episodic memory.</p>
<p>Mutil-Agent Data Generation Framework</p>
<p>The purpose of the Multi-Agent Data Generation Framework (MADGF) is to design multiple human characters interacting with an AI assistant.Through simulating daily conversations, a large multi-turn dialogue dataset enriched with episodic memories is collected for the training of the Echo model.To enhance the diversity and effectiveness of the conversation content, we initially devised three key elements: characters, plots, and environments.Extensive character cards, plots, and temporal information were then generated.Subsequently, we formulated a data generation process that utilizes this information to guide the LLM in producing high-quality episodic memory data (EM-Train).</p>
<p>Characters, plots, and Environments</p>
<p>Characters As illustrated in Figure 3, the design of character cards encompasses seven attributes: "Name," "Occupation," "Age," "Gender," "Hobbies," "Personality," and "Social Relationships."Specifically, we randomly generated attribute values for all attributes except for "Social Relationships."Subsequently, we utilized the LLM to generate the Plots The plots generated by LLMs differ significantly from actual real-life scenarios.Therefore, we manually created an event library, from which 20 events are sampled to form a plot.The library contains three types of events: common events, real events, and hallucinatory events.Common events are designed to enable the model to generate data based on semantic memory questions and answers while enriching the context.They include routine occurrences in daily life, such as greetings, inquiries about common knowledge, and discussions about career-related issues.Real events are events that have actually occurred and are related to episodic memory.They are used to prompt the human role to ask the Echo assistant if it remembers a related event.</p>
<p>Hallucinatory events are fabricated events that have never occurred.They are used to prompt the human agent to ask the AI assistant about non-existent events and simultaneously remind the AI assistant not to be misled.Notably, since all event prompts are removed during the training of the Echo model, hallucinatory events help reduce the LLM's tendency to generate false information and enhance the model's understanding and reasoning abilities regarding episodic memory.</p>
<p>Environments In the design of environments, we initially considered only temporal information.We first established a series of time-stamped nodes arranged in chronological order (e.g., Monday, September 4, 2006, 21:42:56, Monday, September 4, 2006, 21:55:38).These time-stamped nodes are then automatically added to the conversation history between the human role and the Echo assistant, indicating the time at which each round of dialogue takes place.</p>
<p>Data generation process</p>
<p>Prompt Design As illustrated in Figure 4, we designed distinct prompt templates for both the human role and the Echo assistant.The highlighted sections in the figure are replaced with information from Section 3.1.Specifically:</p>
<p>• Human Role Prompt: This includes the character card and all plot details, enabling the LLM to assume various human roles and engage in dialogues with the AI assistant according to different plots.</p>
<p>• AI Assistant Prompt: This incorporates both hallucinatory plots and common plots.This setup helps the LLM acting as the AI assistant to reduce episodic memory hallucinations and proactively seek relevant information in a human-like manner.</p>
<p>Based on these prompt templates, we generate initial prompts for the human and the Echo assistant, denoted as P u and P a , respectively.</p>
<p>The Pseudocode of Data Generation Process Algorithm 1 provides the pseudocode for the data generation process.We initialize and maintain two separate history records, H u and H a , for the human role and the AI assistant using initial prompts P u and P a , respectively.In lines 4-12 of Algorithm 1, we alternately control the two agents representing the human and the assistant to engage in dialogue.Temporal information is incorporated during the conversation in lines 6-8.We check if farewell phrases such as "goodbye" or "talk to you later" appear in the response.If any of these phrases are detected, or if the number of conversation rounds exceeds 60, the stopping criterion is considered to be met, and the current data generation process is terminated.Finally, we remove the initial prompt P a from H a to obtain the final dataset, denoted as Data, which constitutes one piece of data in our EM-Train dataset.assistant.As shown in Figure 5 (a), in traditional LLMs, the chat template for instruction fine-tuning alternates between two roles: user and assistant.In our modified approach, as highlighted in red in Figure 5 (b), we introduced an additional role, "observation," which includes temporal information.During training, the content of the observation does not participate in gradient updates, and the attention mask remains consistent with the traditional decoder-only method.During inference, whenever a user inputs a prompt, real-time information is automatically integrated into the context, enabling the creation of a time-aware AI assistant.</p>
<p>Dataset</p>
<p>EM-Test</p>
<p>We manually developed a benchmark called EM-Test for evaluating the episodic memory of LLMs.Each test instance consists of multi-turn dialogues.In addition to dialogues that are not directly related to episodic memory testing, these dialogues may include multiple related historical dialogues and a corresponding test point, as shown in Figure 6.At each test point, we annotate the test question (Question), the temporal context (Observation), and the reference answer (Reference Answer).</p>
<p>During testing, we provide all historical dialogues as conversation history to the model.Then, we input the test question and the temporal context to obtain the model's output.The model's output is either manually scored or compared against the reference answer to quantitatively evaluate the episodic memory capabilities of the LLMs.</p>
<p>We labeled the time span and difficulty of the test points to achieve more granular results.In terms of time span, we categorized them into eight types based on the required duration of episodic memory for answering questions: "just now," "one day," "few days," "one month," "few months," "one year," "few years," and "several decades."We also divided the difficulty of test points into easy and hard levels.</p>
<p>For an easy-level test point, the model only needs to recall a simple scenario.For a hard-level test point, the model must possess complex episodic memory capabilities.Figure 6 provides an example of a hard-level test point.In this example, to answer "Did I take any days off from school this year?", the model needs to recall writing a leave note for the user and the timing of that event being in the same</p>
<p>Experiment</p>
<p>Experimental Setups</p>
<p>Selected LLMs.We evaluate a series of LLMs on EM-Test, including the current state-of-the-art open-source and closed-source models.Particularly, we select LLAMA3-8b (Dubey et al., 2024), ChatGLM3-6B (GLM et al., 2024) for open-source models, and for closed-source models, we employ GPT-3.5-turbo (OpenAI, 2023), GPT-4 (OpenAI, 2023), ChatGLM3-trubo (GLM et al., 2024).</p>
<p>Implementation Details.In MADGF, the LLM serving as the agent is Qwen2-72B-Instruct (Yang et al., 2024), which is a high-performance open-source LLM.We use chatglm3-6B (GLM et al., 2024) as the base model for Echo, and implement it with full fine-tuning.</p>
<p>Evaluation Methods and Metrics.In the quantitative analysis, we first collect the responses of LLMs at the test points, then ask human annotators to score these responses on a scale of 1 to 10.Additionally, we use the widely adopted Sentence Transformer model, all-MiniLM-L6-v2 1 , to encode the LLMs' responses and the reference standard outputs provided by the test set, obtaining E LLM and E Standard .</p>
<p>We then calculate the cosine similarity S using the Equation 1.</p>
<p>1 https://huggingface.co/sentence-transformers/ all-MiniLM-L6-v2</p>
<p>Easy</p>
<p>ChatGLM3-6B 2.7 2.8 7.2 2.7 1.8 2.8 0.0 1.0 3.5 LLAMA3-8B 5.6 6.0 4.2 6.1 4.6 6.0 5.6 4.7 8.0 ChatGLM3-Turbo 3.9 4.5 5.6 3.5 3.0 2.0 4.2 3.3 5.0 GPT-3.5-turbo5.2 5.4 4.6 6.6 5.3 5.8 4.6 4.1 5.6 GPT-4 5.8 5.0 6.6 6.1 5.0 5.8 6.0 4.0 7.8 Echo (Ours) 6.7 6.2 6.4 6.4 8.0 6.8 6.6 6.1 7.5</p>
<p>Mean Value (Easy) 5.0 5.0 5.8 5.2 4.5 4.8 4.5 3.9 6.2</p>
<p>Hard</p>
<p>ChatGLM3-6B 2.6 2.0 4.8 1.9 3.5 1.0 1.8 1.9 2.0 LLAMA3-8B 5.5 7.0 5.2 4.0 4.5 6.4 5.0 5.1 7.0 ChatGLM3-Turbo 3.5 2.7 6.2 2.4 1.8 4.9 4.3 3.2 3.0 GPT-3.5-turbo4.0 5.7 4.4 2.9 1.8 6.1 4.8 3.0 3.0 GPT-4 5.4 5.7 5.4 5.0 3.5 5.6 6.0 5.1 7.0 Echo (Ours) 5.9 5.9 6.0 6.1 5.5 6.9 5.8 5.7 5.6</p>
<p>Mean Value (Hard) 4.5 4.8 5.3 3.7 3.4 5.1 4.6 4.0 4.6
S = cos sim(E LLM , E Standard ) × 100 (1)
We consider using the Pearson correlation coefficient, denoted by R, to measure the correlation between the human score and the similarity metric.It is calculated using Equation 2.
R = n i=1 (x i − x)(y i − ȳ) n i=1 (x i − x) 2 n i=1 (y i − ȳ) 2
(2)</p>
<p>where:</p>
<p>• x i and y i are individual sample points indexed with i,</p>
<p>• x and ȳ are the mean values of x and y respectively,</p>
<p>• n is the number of sample points.</p>
<p>If the R value between two datasets is greater than 0.8, it is considered to be highly positively correlated (Cohen, 2013).</p>
<p>Quantitative Analysis</p>
<p>Overall Performance We present the results of LLMs on human scores and similarity metrics in Table 2 and Table 3, respectively.Our experimental analysis is provided from the following aspects: performance of several LLMs, human score and similarity metrics, comparisons across easy and hard levels, comparisons across different time spans, and consistency between human score and similarity metric.</p>
<p>Performance of Several LLMs.It can be observed that our Echo model achieved the best performance in both Human Score and Similarity Metric.Specifically, it scored 6.7 and 5.9 in the easy and hard levels of Human Score, respectively, and 84.0 and 74.5 in the Similarity Metric.Over different time spans, Echo nearly obtained the best or second-best scores across all metrics, except for the easy level "One day" and "Several Decades" in Human Score.These results indicate that the Echo model excels in EM capability.In contrast, among all models, the open-source ChatGLM3-6B performed the worst overall.As the base model of Echo, this indirectly demonstrates the effectiveness of the EM-Train data generated using the MADGF framework in enhancing a model's EM capability.Moreover, GPT-4 also showed excellent comprehensive performance, achieving second-best scores in the easy level of Human Score and the hard level of Similarity Metric.In the hard level of Human Score, GPT-4 (5.4) narrowly trailed behind LLAMA3-8B (5.5), which had the second-best performance.In the easy level of Similarity Metric, GPT-4 (72.3) slightly lagged behind LLAMA3-8B (74.8), which also had the second-best performance.</p>
<p>Comparisons across Easy and Hard Levels.Most LLMs perform well on easy-level problems, but their performance drops at the hard level.Specifically, we found that all LLMs performed better overall at the Easy Level compared to the Hard Level.For instance, in Table 2, GPT-3.5-turboscored 5.2 at the Easy Level, but only 4.0 at the Hard Level.Additionally, we observed that the average score for all models at the Hard Level in Table 2 was 4.5, which is 0.5 lower than the Easy Level (5.0).Similar results were seen in Table 3.</p>
<p>Comparisons across Different Time Spans.Models exhibit different performances across various time spans.Due to the inconsistency in model performance between the human score and similarity metric, we first consider the common performance under both metrics, then focus on the analysis based primarily on the human score.On the easy time span, we found that models perform better on "several decades" questions.This is because the mean values for "several decades" are the highest for all models according to both metrics (6.2 for human score and 82.8 for similarity metric).Meanwhile, we observed that models perform poorly on certain time spans (few days, few months, few years), which may be attributed to the difficulty models have in understanding these temporal concepts.</p>
<p>Consistency Between Human Scores and Similarity Metrics.</p>
<p>We consider calculating the Pearson correlation coefficient R for two metrics to observe their correlation.The overall results of the Human Score (i.e., 2.7, 5.6, ..., 6.7) and the Similarity Metrics (i.e.,57.0,70.2,...,84.0)Performance in Episodic Memory Without Temporal Information We tested the EM capability of the models without considering time information, as shown in Table 4.We found that the models perform similarly whether or not time information is considered.Our Echo model and GPT-4 still performed well, achieving the first and second highest scores, respectively; while ChatGLM3-6B continued to perform the worst.These results indicate that the dataset EM-Train, obtained using our MADGF framework, can effectively improve the EM capability of models even when time information is not considered.In addition, we provide extended experiments regarding the model's temporal awareness and reasoning capability in the appendix for further analysis.</p>
<p>Qualitative Analysis</p>
<p>Analysis of Complex Episodic Memory Ability We conducted experiments on the Echo model using real-life scenarios that require complex Episodic Memory abilities, as shown in Figure 7.Some dialogues unrelated to episodic memory have been omitted using vertical ellipses.These di- alogues are intended to increase the challenge of improving the model's Episodic Memory ability in long texts.Additionally, some content details unrelated to episodic memory skills have also been omitted.To test the model's performance over longer time spans, all time information was manually provided.</p>
<p>The test dialogue on the left side of Figure 7 demonstrates that the model can accurately recall recent conversation content and timing, indicating its ability to understand time and associate it with events, which is a sign of Episodic Memory capability.In the test questions on the right side, the model shows even stronger Episodic Memory ability by recalling what human characters ate on a specific day from lengthy historical records, and understanding and judging whether conversations took place during a certain period.</p>
<p>Analysis of Episodic Memory Ability Without Temporal Information</p>
<p>We tested Echo using questions that do not require considering time information for responses, as shown in Figure 8. From the test dialogue, it is clear that Echo can accurately recall the human character's favorite band and food, and provide relevant information even after multiple rounds of dialogue.Additionally, in the final round of test questions, Echo did not confuse any content that we had not actually told it, avoiding the hallucination issue.This problem often occurs when conversing with other LLMs.</p>
<p>Conclusion</p>
<p>In this paper, we investigate the episodic memory capabilities of LLMs.We propose an innovative Multi-agent data generation framework to collect high-quality, context-rich fine-tuning data, named EM-Train, which we used to further train the Echo model.We innovatively introduce time information into the training paradigm of LLMs.We also develop a multi-round dialogue test set, EM-Test, to evaluate the episodic memory capabilities of LLMs.Experimental results show that EM-Train significantly improves the Episodic Memory of LLMs.The experiments also verify that LLMs can gain time perception and reasoning abilities by incorporating time information into their training paradigms.Furthermore, qualitative experimental analysis indicates that Echo exhibits some human-like episodic memory capabilities.Our research provides a preliminary exploration of complex episodic memory capabilities with temporal information for LLMs.</p>
<p>B. Extended Experiments on Temporal Awareness and Reasoning Capability of the Model</p>
<p>To enhance and evaluate the temporal awareness and reasoning capabilities of the model, we have developed temporally aware and reasoning-enhanced training and testing datasets.We then conducted both quantitative and qualitative experimental analyses of Echo.</p>
<p>B.1. Temporal Reasoning Dataset</p>
<p>Training Dataset We improved upon a portion of the training set proposed by Tan et.al (Tan et al., 2023) to create a dataset that emphasizes temporal awareness and reasoning.In the work by Tan et.al (Tan et 2023), the data were entirely synthesized programmatically, with questions being relatively simplistic, lacking inquiries about specific days of the week or recent dates.Utilizing both programming techniques and manual annotations, we constructed an 8K training dataset.The data format adheres to Echo's training paradigm of user-time-assistant, making it highly suitable for Echo model training.</p>
<p>Table 7 provides examples from our training dataset, which includes various complex scenarios for temporal reasoning questions, aiding in developing Echo's robust temporal awareness and reasoning skills after training.Answer: Today, 432 years ago, was 1581-9-3.key word: "1581", "9|Sep|September", "3" Evaluation Dataset We manually annotated a temporally aware and reasoning-enhanced evaluation dataset consisting of 292 instances, including 32 short-term (within one week) and 260 long-term test questions, as shown in Table 8.Each test question provides all possible keywords contained in the standard answer, allowing for accurate quantitative analysis of whether the model's output is correct through string matching.</p>
<p>Figure 1 .
1
Figure 1.The performance of LLMs across 7 time spans and and two difficulty levels in our EM-Test.</p>
<p>Figure 3 .
3
Figure 3. Example of character card.</p>
<p>Figure 5 .
5
Figure 5. Illustration of training paradigm changes.</p>
<p>Figure 7 .
7
Figure 7. Examples of complex episodic memory capability in the Echo.</p>
<p>Figure 8 .
8
Figure 8. Examples of episodic memory ability without temporal information in the Echo.</p>
<p>You will be playing the role of the following character in a conversation with an AI assistant: {character card} Please strictly follow the topic order below to conduct the conversation: {plots} Your responses should be as concise and brief as possible, like a real person, without too much detail.Special Note!!!After each round of dialogue, you need to wait for the AI assistant's response.You should only output your own part of the conversation and not include any content from the AI assistant.海螺.Symbolizing the crystallization of wisdom and memory.Function:An AI with advanced contextual memory, possessing very strong memory capabilities, able to remember a vast amount of information.Hobbies: Listening to people's stories and making friends with people.As a reminder, here are some points for attention about information that humans have not actually told you.If humans test your memory and ask for this information, you need to humbly say that you don't know: {hallucinatory plots} After each round of dialogue, you need to wait for the human's reply and continue the conversation.During the dialogue, at appropriate times, you should proactively ask to obtain some basic information about the human, such as, {common plots}.It's important to note that you can obtain the current time information from the user's input in each round of dialogue.You can use this time information to answer questions that require consideration of the current time.Your responses should be as concise and brief as possible, without needing to be very detailed.If a memory information has not appeared in the dialogue, you should humbly say that you do not know.Let's begin your conversation with the human!
If the AI assistant makes a mistake during the dialogue, you need to correct it. Let's begin your conversation with the AI assistant! You will be playing the role of an AI assistant named Echo in a conversation with a human: English Name: Echo. Chinese Name: Prompt for Human Role Prompt for AI AssistantFigure 4. Prompt template in data generation process.4.1. EM-Train and Training ParadigmBased on MADGF in Section 3, we collected and createdEM-Train. It consists of 15, 533 data entries, with an av-erage of 16.75 conversation rounds per data entry and an
average length of 8, 597 characters.Then, we trained the Echo model using the EM-Train dataset.Compared to the conventional LLM training paradigm (userassistant), we modified the training paradigm to user-time-</p>
<p>Algorithm 1 Pseudocode of Data Generation Process Require: Initial prompts P u and P a Ensure: Data 1: Initialize H u , H a 2: H u ← H u ∪ P u 3: H a ← H a ∪ P a 4: while stopping criterion not met do
5:answer u ← LLM (H u )6:time ← RandomN extT ime(time)7:H u ← H u ∪ answer u ∪ time8:H a ← H a ∪ answer u ∪ time9:answer a ← LLM (H a )10:H u ← H u ∪ answer a11:H a ← H a ∪ answer a12: end while13: Data ← H a \ P aRelated Historical DialogueTest PointUser: My name is Carol. I am notTest time: Few monthsfeeling well today and do not want to go to school. PleaseTest difficulty: Hardhelp me write a sick note.Question: Did I take any daysObservation: Sunday, May 17th,off from school this year?2009, 10: 25: 39.Assistant:Observation: Saturday, August 15th, 2009 23: 12: 3.Dear [Teacher's Name]:I am writing to inform you that my daughter, Carol, will not be able to attend school today due to illness. She has been experiencing symptoms such as a sore throat, headache, …Reference Answer: Yes, you did take a day off this year. On May 17th, you weren't feeling well, so I wrote a sick note for you and you took one day off.
Test Point Figure 6.Example of hard-level test point in EM-Test.</p>
<p>Table 1 .
1
Detailed statistics of EM-Test.
Time SpanEasy Hard TotalEM-Testjust now18725one day5510few days10818one month448few months4711one year549few years7916several decades 459Overall Number 5749106EM-Test-Without-TimeOverall Number 8934123year as the current one, to deduce the correct answer.Additionally, we manually created the EM-Test-Without-Time scenario test set to evaluate the model's episodic mem-ory ability without considering time information. Comparedto EM-Test, EM-Test-Without-Time does not include tem-poral context and only considers easy and hard difficultylevels. Table 1 presents the relevant statistical informationfor both EM-Test and EM-Test-Without-Time.</p>
<p>Table 2 .
2
Comparison of model results based on human score.The first and second highest score are marked in red and blue.JN: just now, OD: one day, FD: few days, OM: one month, FM: few months, OY: one year, FY: few years, SD: several decades.
ModelsOverall JN OD FD OM FM OY FY SD</p>
<p>Table 3 .
3
Comparison of model results based on similarity metric.
ModelsOverall JN OD FD OM FM OY FY SDEasyChatGLM3-6B57.042.7 76.7 50.8 52.2 37.2 50.1 68.2 77.9LLAMA3-8B70.264.3 77.0 73.6 64.2 56.6 78.1 59.7 87.7ChatGLM3-Turbo58.345.6 68.0 66.2 43.5 40.4 57.4 63.0 81.9GPT-3.5-turbo74.868.2 66.5 90.4 74.8 56.7 74.9 85.8 81.5GPT-472.363.6 72.3 76.8 64.2 53.5 81.0 80.8 86.5Echo (Ours)84.085.6 77.1 86.2 81.5 79.4 86.7 88.1 87.1Mean Value (Easy)68.460.7 71.9 73.0 62.4 53.0 70.4 73.3 82.8HardChatGLM3-6B56.353.2 71.8 45.6 58.9 37.6 60.9 71.1 51.4LLAMA3-8B64.372.9 63.8 52.7 71.1 64.1 53.4 69.3 67.4ChatGLM3-Turbo52.555.2 59.2 44.3 46.9 55.0 40.6 66.9 51.8GPT-3.5-turbo65.176.1 63.3 52.0 71.8 63.8 53.8 74.0 66.1GPT-467.771.9 66.8 59.1 70.7 68.9 64.3 70.5 69.8Echo (Ours)74.582.1 73.1 69.9 78.6 71.7 73.3 75.4 72.0Mean Value (Hard)63.468.6 66.3 53.9 66.4 60.2 57.7 71.2 63.1</p>
<p>Table 4 .
4
Comparison of performance in episodic memory without temporal information.
ModelsEasy HardChatGLM3-6B79.9 66.9LLAMA3-8B89.0 73.3ChatGLM3-Turbo 84.3 67.0GPT-3.5-turbo90.0 77.8GPT-490.4 78.3Echo (Ours)96.0 81.8Mean Value88.3 74.2</p>
<p>Table 7 .
7
Examples of temporal reasoning dataset.What is the time 10 years and 6 months after June 1856?Time: Monday, September 9, 2024, 16:03:45 Answer: The time is Dec, 1866 2 Question: What was the date a year ago today?Time: Friday, April 3, 2020, 5:04:46 Answer: Today is 2020-4-3, therefore a year ago today should be 2019-4-3.Will the day after tomorrow be a weekend?Time: Saturday, July 3, 2027, 19:17:33 Answer: No, the day after tomorrow will be Monday, July 5th, 2027.Question:How can I use PHP to compare the existentialist philosophies of ... Time: Tuesday, June 22nd, 2038, 11: 01: 39 Answer: As an AI assistant, I can provide guidance on how to approach the task, ... Question: How long ago was our last chat?Time: Tuesday, June 22nd, 2038, 11: 31: 7 Answer: Our last conversation was just now, 30 minutes ago, at 11:1:39.Question: 432 years ago today, which day was it?Time: Tuesday,September 3, 2013, 23: 42: 54
Training DatasetNo.Question, Time, Answer1 Question: 5Evaluation DatasetNo.Question, Time, Answer, key word1
3 Question: What day of the week was yesterday?Time: Friday, April 23, 2049, 23:30:07 Answer: Yesterday was Thursday.4 Question:</p>
<p>Table 8 .
8
Statistics of the evaluation dataset for temporal reasoning
Time SpanShort Long TotalOverall Number 32260 292
AppendixA. Implementation Details of MADGFA.1.Plots used in Human Prompt Template Table5presents the plots used in the human prompt template.It includes 20 plots, with the numbers of true episodic memories and hallucinatory episodic memories marked in blue and red, respectively.The final plot is fixed as "say goodbye" to guide the conclusion of the conversation.6provides an example of hallucinatory plots used in the assistant prompt template, aimed at guiding the assistant to avoid hallucination issues.The example includes four memories that did not occur in actual conversations, corresponding to the plots marked in red ( 8, 10, 18 , and 19 ) in Table5.The common plots designed to prompt the AI assistant to proactively seek relevant information in a human-like manner.One example is "name, old, hobby, gender".B.2. Quantitative Analysis of Temporal Perception and Reasoning AbilityOn the Evaluation Dataset for Temporal Reasoning, we conducted a quantitative analysis.As in Section 5, we selected LLAMA3-8b(Dubey et al., 2024)and ChatGLM3-6B(GLM et al., 2024)for open-source models, and for closed-source models, we employed GPT-3.5-turbo(OpenAI, 2023),GPT-4 (OpenAI, 2023), andChatGLM3-turbo (GLM et al., 2024)for evaluation and comparison.When calculating the metrics, we detected keywords within the models' responses.As shown in Table9, we found that our Echo model still performs the best, with time-aware and reasoning abilities exceeding 90 in both short-term (98.1) and long-term (94.6) scenarios.In contrast, ChatGLM3-6B performed very poorly, with time-aware and reasoning abilities below 10 in both short-term (9.4) and long-term (8.8) scenarios.This indicates that the EM-Train dataset significantly improves the time-aware and reasoning capabilities of the models.Additionally, we observed that GPT-4 achieved suboptimal performance on long-term tests, but did not achieve suboptimal performance on short-term tests.Upon examining the model's outputs, we noticed that GPT-4 tends to produce errors and hallucinations in short-term temporal reasoning.For example, the correct answer was "The date the day before yesterday was July 1st, 2023.",but GPT-4's output was "The day before yesterday would have been July 2, 2023.".The date 20 years from today is October 8th, 2043.
Thanks! Please help me remember that my TikTok account is 1130701. Of course, I've remembered it for you. 14: 09: 27April 5, 2034Wednesday</p>
<p>Okay, do you know my mom's TikTok account? Sorry, I don't know. You only told me your TikTok account. Thursday. 20238April 6, 2034</p>
<p>Great! Please remember that my favorite band is The Beatles. Got it! Your favorite band is The Beatles. November 7, 2011118Monday</p>
<p>Who is my favorite band? Your favorite band is The Beatles. Monday. November 7, 20111202: 1 I also love eating sushi</p>
<p>I'll remember that you love eating sushi. Friday. August 18, 2012223</p>
<p>From which country does my favorite food originate? Your favorite food is sushi, and it originates from Japan. August 21, 20121739Monday</p>
<p>can you tell me some information about my favorite band?. ! Great, Okay, </p>
<p>The Beatles were an English rock band formed in Liverpool in 1960. Sure, John Lennon, Paul McCartney, George Harrison, and ... MondayNovember 7, 20331552</p>
<p>Episodic memory verbalization using hierarchical representations of lifelong robot experience. L Bärmann, C Dechant, J Plewnia, F Peller-Konrad, D Bauer, T Asfour, A Waibel, arXiv:2409.177022024arXiv preprint</p>
<p>Statistical power analysis for the behavioral sciences. routledge. J Cohen, 2013</p>
<p>Y Dan, Z Lei, Y Gu, Y Li, J Yin, J Lin, L Ye, Z Tie, Y Zhou, Y Wang, arXiv:2308.02773A largescale language model-based chatbot system for intelligent education. 2023arXiv preprint</p>
<p>Enhancing chat language models by scaling high-quality instructional conversations. N Ding, Y Chen, B Xu, Y Qin, Z Zheng, S Hu, Z Liu, M Sun, B Zhou, arXiv:2305.142332023arXiv preprint</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Human-like episodic memory for infinite context llms. Z Fountas, M A Benfeghoul, A Oomerjee, F Christopoulou, G Lampouras, H Bou-Ammar, J Wang, arXiv:2407.094502024arXiv preprint</p>
<p>H Gao, Y Zhang, arXiv:2404.09982Memory sharing for large language model based agents. 2024arXiv preprint</p>
<p>. T Glm, A Zeng, B Xu, B Wang, C Zhang, D Yin, D Rojas, G Feng, H Zhao, H Lai, H Yu, H Wang, J Sun, J Zhang, J Cheng, J Gui, J Tang, J Zhang, J Li, L Zhao, L Wu, L Zhong, M Liu, M Huang, P Zhang, Q Zheng, R Lu, S Duan, S Zhang, S Cao, S Yang, W L Tam, W Zhao, X Liu, X Xia, X Zhang, X Gu, X Lv, X Liu, X Liu, X Yang, X Song, X Zhang, Y An, Y Xu, Y Niu, Y Yang, Y Li, Y Bai, Y Dong, Z Qi, Z Wang, Z Yang, Z Du, Z Hou, Wang , 2024Z. Chatglm: A family of large language models from glm-130b to glm-4 all tools</p>
<p>C Hu, J Fu, C Du, S Luo, J Zhao, H Zhao, Chatdb, arXiv:2306.03901Augmenting llms with databases as their symbolic memory. 2023arXiv preprint</p>
<p>Exploring the frontiers of llms in psychological applications: A comprehensive review. L Ke, S Tong, P Chen, K Peng, arXiv:2401.015192024arXiv preprint</p>
<p>Crafting papers on machine learning. P Langley, Proceedings of the 17th International Conference on Machine Learning (ICML 2000). P Langley, the 17th International Conference on Machine Learning (ICML 2000)Stanford, CAMorgan Kaufmann2000</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. G Li, H Hammoud, H Itani, D Khizbullin, B Ghanem, Advances in Neural Information Processing Systems. 202336</p>
<p>Mathematical language models: A survey. W Liu, H Hu, J Zhou, Y Ding, J Li, J Zeng, M He, Q Chen, B Jiang, A Zhou, arXiv:2312.076222023arXiv preprint</p>
<p>H Luo, Q Sun, C Xu, P Zhao, J Lou, C Tao, X Geng, Q Lin, S Chen, D Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv preprint</p>
<p>Episodic memory and beyond: the hippocampus and neocortex in transformation. M Moscovitch, R Cabeza, G Winocur, L Nadel, Annual review of psychology. 6712016</p>
<p>H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Akhtar, N Barnes, A Mian, arXiv:2307.06435A comprehensive overview of large language models. 2023arXiv preprint</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>C Packer, S Wooders, K Lin, V Fang, S G Patil, I Stoica, J E Gonzalez, Memgpt, arXiv:2310.08560Towards llms as operating systems. 2023arXiv preprint</p>
<p>Tool learning with foundation models. Y Qin, S Hu, Y Lin, W Chen, N Ding, G Cui, Z Zeng, Y Huang, C Xiao, C Han, Y R Fung, Y Su, H Wang, C Qian, R Tian, K Zhu, S Liang, X Shen, B Xu, Z Zhang, Y Ye, B Li, Z Tang, J Yi, Y Zhu, Z Dai, L Yan, X Cong, Y Lu, W Zhao, Y Huang, J Yan, X Han, X Sun, D Li, J Phang, C Yang, T Wu, H Ji, Z Liu, M Sun, 2024</p>
<p>Constructive memory: past and future. D L Schacter, Dialogues in clinical neuroscience. 1412012</p>
<p>A generative model of memory construction and consolidation. E Spens, N Burgess, Nature Human Behaviour. 832024</p>
<p>Remembering: A study in experimental and social psychology. W Sprott, 1933</p>
<p>Memory systems of the brain: a brief history and current perspective. L R Squire, Neurobiology of learning and memory. 8232004</p>
<p>Q Tan, H T Ng, Bing , L , arXiv:2306.08952Towards benchmarking and improving the temporal reasoning capability of large language models. 2023arXiv preprint</p>
<p>Episodic and semantic memory. E Tulving, 1972Academic PressOrganization of memory</p>
<p>Elements of episodic memory. E Tulving, 1983</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Z M Wang, Z Peng, H Que, J Liu, W Zhou, Y Wu, H Guo, R Gan, Z Ni, J Yang, arXiv:2310.00746Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 2023arXiv preprint</p>
<p>C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, D Jiang, Wizardlm, arXiv:2304.12244Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>A Yang, B Yang, B Hui, B Zheng, B Yu, C Zhou, C Li, C Li, D Liu, F Huang, arXiv:2407.10671Qwen2 technical report. 2024arXiv preprint</p>
<p>Unifying the perspectives of nlp and software engineering: A survey on language models for code. Z Zhang, C Chen, B Liu, C Liao, Z Gong, H Yu, J Li, R Wang, arXiv:2311.079892023arXiv preprint</p>
<p>W Zhao, X Ren, J Hessel, C Cardie, Y Choi, Y Deng, arXiv:2405.01470Wildchat: 1m chatgpt interaction logs in the wild. 2024arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Memorybank: Enhancing large language models with long-term memory. W Zhong, L Guo, Q Gao, H Ye, Y Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>            </div>
        </div>

    </div>
</body>
</html>
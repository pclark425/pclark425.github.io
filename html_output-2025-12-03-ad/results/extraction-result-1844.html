<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1844 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1844</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1844</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-844b22bb025f485d85d00f1f61555a8ff0131658</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/844b22bb025f485d85d00f1f61555a8ff0131658" target="_blank">STEVE-1: A Generative Model for Text-to-Behavior in Minecraft</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a methodology, inspired by unCLIP, for instruction-tuning generative models of behavior without relying on a large dataset of instruction-labeled trajectories, and creates an instruction-tuned Video Pretraining (VPT) model called STEVE-1, which can follow short-horizon open-ended text and visual instructions in Minecraft.</p>
                <p><strong>Paper Abstract:</strong> Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces a methodology, inspired by unCLIP, for instruction-tuning generative models of behavior without relying on a large dataset of instruction-labeled trajectories. Using this methodology, we create an instruction-tuned Video Pretraining (VPT) model called STEVE-1, which can follow short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, reducing the need for costly human text annotations, and all for only $60 of compute. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 sets a new bar for open-ended instruction-following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming previous baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We provide experimental evidence highlighting key factors for downstream performance, including pretraining, classifier-free guidance, and data scaling. All resources, including our model weights, training scripts, and evaluation tools are made available for further research.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1844.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1844.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MineCLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MineCLIP (domain-specific CLIP for Minecraft)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CLIP-style visual-language model trained to align Minecraft video clips with textual captions; used as a frozen embedding space for both visual goals and text instructions in STEVE-1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>MineCLIP (as used in STEVE-1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A pretrained visual-language encoder that produces joint embeddings for short video segments (16 frames) and text; MineCLIP is used frozen to (1) embed 16-frame visual goal clips used to relabel trajectories and (2) encode text instructions which are mapped by a learned prior into visual-embedding goals.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>paired web videos and captions (video-text contrastive pretraining on Minecraft gameplay and transcripts)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper states MineCLIP was trained with a contrastive objective on pairs of Minecraft videos and transcripts from the web to produce aligned text and 16-frame video clip embeddings. Exact dataset size and collection details are from the MineCLIP source (cited in the paper) and are not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Minecraft (MineRL / low-level mouse & keyboard control)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Open-ended Minecraft gameplay using the MineRL environment: short-horizon tasks such as going to and chopping trees, digging holes, collecting items, and prompt-chained multi-step tasks (e.g., gather resources then craft/build) using low-level discrete/continuous mouse & keyboard controls and raw RGB pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for MineCLIP itself (MineCLIP is a perceptual/text encoder); its pretraining involved text/video pairs (natural language captions) rather than action labels.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level mouse and keyboard controls (joint hierarchical action space used by VPT; discrete keypresses and mouse movements as in VPT / MineRL)</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not a direct policy mapping — a learned conditional prior (CVAE) maps MineCLIP text embeddings z_y to MineCLIP visual embeddings z_{τ_goal}; the policy (finetuned VPT) conditions on visual embeddings z_{τ_goal} to produce low-level actions. Inference uses classifier-free guidance (combining conditional and unconditional policy logits) to trade off prior behavior vs. goal-conditioned behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB visual input (frames resized to 128×128×3 processed by a ResNet), MineCLIP provides additional visual/text embedding space; no depth or explicit object detectors required.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When STEVE-1 uses MineCLIP-based conditioning (via the CVAE prior + VPT policy), it robustly completes short-horizon evaluation: e.g., compared to the unconditional VPT agent STEVE-1 (with text prompting) collected 75× more dirt, 4.9× more wood, 22× more seeds, and traveled 4.3× farther; vs. the text-conditioned VPT baseline it collected 3.3× more dirt, 4.4× more wood, 8.1× more seeds, and traveled 2.2× farther. STEVE-1 also completed 12 of 13 early-game tasks in the authors' suite (short-horizon). Classifier-free guidance (λ tuned ≈5–7) further boosted performance (examples: up to 7.5× more dirt and 15× more wood vs λ=0 in an ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Agents trained from scratch (random init) and finetuned on the same data underperform: in the paper, a scratch-initialized model finetuned for 100M frames was able to accomplish basic tasks like dirt collection but failed at more complex behaviors (e.g., finding and chopping trees) that pretrained variants could perform. Exact numeric baselines for every metric are not enumerated beyond the relative factors reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Text-to-goal prior: trained on ≈2,000 human-labeled text↔16-frame video pairs (less than 30 minutes of gameplay) and augmented with retrieval and ≈8,000 GPT-3.5-generated instructions (paper reports these augmentation numbers); the policy was finetuned using self-supervised packed hindsight relabeling on large unlabeled gameplay (contractor + VPT-generated) datasets (dataset described as 54M frames used to construct relabeled data; main STEVE-1 training reported as 160M frames of optimization). The instruction-labeled supervision required by the prior is therefore very small (~2k hand-labeled segments, augmented). The authors emphasize that instruction capability was obtained with only ≈$60 of compute and ~2,000 instruction-labeled trajectory segments for the prior.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>A model trained from scratch and finetuned for 100M frames (experiment reported) still failed to match pretrained variants on tasks requiring complex behaviors (e.g., chopping trees), indicating that >100M frames and extensive finetuning did not recover the pretrained agents' capabilities in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative/orders-of-magnitude statement: pretraining (MineCLIP + VPT foundation weights) enabled instruction-following with only a few thousand labeled instruction-video pairs for the prior and self-supervised relabeling for the policy, whereas training from scratch required at least 100M frames of finetuning and still underperformed; authors describe this as a dramatic reduction in required instruction-labeled data (from needing large instruction-annotated trajectories to only ~2k hand labels) and compute for instruction alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key factors: (1) MineCLIP provides a joint text–video embedding space so the prior only needs to learn a mapping between text and visual embeddings; (2) a strong pretrained behavioral prior (VPT) provides low-level motor competence; (3) packed hindsight relabeling allows leveraging large unlabeled gameplay for goal-conditioned supervision; (4) classifier-free guidance at inference to bias policy toward goal-conditioned behavior; (5) augmentation of the tiny labeled prior dataset with retrieval and GPT-generated instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Gaps noted: (1) gap between text-conditioned performance (via prior) and direct visual goals — prior sometimes imperfectly maps text to visual embeddings; (2) long-horizon tasks (>≈200 timesteps) are underrepresented because hindsight relabeling sampled near-future goals (max ~200 timesteps), limiting long-horizon transfer; (3) misgeneralization of goals (goal misgeneralization) particularly for smaller-scale models; (4) remaining ambiguity of natural language prompts and need for prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A domain-specific visual-language model (MineCLIP) pretrained on video–text pairs can be used as a fixed embedding space to bridge language and visual goals; combining a small amount (~2k) of human text↔video pairs (augmented) with a CVAE prior and a pretrained behavioral policy (VPT) finetuned with self-supervised packed hindsight relabeling yields strong instruction-following in Minecraft with low-level controls. Pretraining (both MineCLIP for language understanding and VPT for motor priors) is the dominant contributor to capability; classifier-free guidance and data-scaling further improve downstream performance, while training from scratch (no pretraining) is much less effective even after large amounts of finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STEVE-1: A Generative Model for Text-to-Behavior in Minecraft', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1844.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1844.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-turbo (used for dataset augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model (API used) employed to generate additional text instructions to augment the small human-labeled text–video pair dataset for training the CVAE prior that maps text embeddings to visual embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>gpt-3.5-turbo (used to synthesize instruction labels)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Commercial large language model accessed via OpenAI API; used to synthesize ≈8,000 additional text instructions to augment a 2,000 human-labeled text–video training set for the CVAE prior.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>large-scale language model (pretrained on web text; details of GPT-3.5 pretraining not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper only states they used the OpenAI API gpt-3.5-turbo to generate ≈8,000 instructions; it does not detail GPT-3.5's pretraining corpora within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Minecraft (used indirectly to augment prior training data for text→visual embedding mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same MineRL / Minecraft early-game short-horizon tasks — GPT-generated instructions were used to augment the training pairs for the CVAE prior so the prior could map a wider variety of text prompts to MineCLIP visual embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (GPT output are natural language instructions, not actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (GPT is not acting in environment; its outputs are used as labels for the prior which in turn conditions the policy that outputs low-level mouse/keyboard actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>GPT-3.5 generated text instructions which were paired with retrieved 16-frame gameplay segments (via MineCLIP similarity) to create additional text↔video training pairs for the CVAE prior. The CVAE maps MineCLIP text embeddings (z_y) into MineCLIP visual embeddings (z_{τ_goal}) used by the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>N/A for GPT itself; its outputs are used in a pipeline that requires RGB frames and MineCLIP embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Augmentation with GPT-3.5-generated instructions increased the CVAE training set (paper reports adding ≈8k generated instructions) and improved some programmatic task metrics in ablations (augmentation helped for dirt and seed collection in some experiments, though it slightly hurt other metrics like log collection and travel distance in one ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not directly applicable; comparison in paper is between prior trained on only 2k human pairs vs. augmented sets. The unaugmented CVAE (human-only) was a baseline in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Augmented prior dataset: human 2,000 pairs + retrieval + ≈8,000 GPT-generated = ≈10,000 effective text↔video pairs (paper reports these numbers). The prior trained on this small augmented set.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Human-only prior: ≈2,000 hand-labeled pairs (less than 30 minutes of gameplay); ablations showed some performance differences when augmentation was omitted.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Augmentation increased the available labeled instructions by multiple× (from ~2k to ~10k synthesised/augmented pairs), improving certain downstream metrics; this reduced the need for extensive human labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Rapid expansion of the text label distribution via GPT-generated instructions enabled the CVAE prior to see more linguistic variety; retrieval-based pairing with gameplay segments (using MineCLIP) anchors generated instructions to realistic visuals.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Synthetic instructions can introduce mismatch/noise; ablations showed augmentation sometimes hurt some metrics (e.g., wooden log collection and travel distance) possibly due to distribution mismatch or noisy pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a large LLM (gpt-3.5-turbo) to synthetically expand a small human-labeled text↔video dataset allowed training a prior with far fewer hand-labeled minutes of gameplay; this is an effective low-cost augmentation strategy but can introduce noisy/less-useful examples for some metrics, so retrieval/curation remains important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STEVE-1: A Generative Model for Text-to-Behavior in Minecraft', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1844.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1844.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIAL (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DIAL (vision-language relabeling pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced method that finetunes CLIP on human-labeled trajectories and then uses it to select hindsight instructions from a candidate set for relabeling trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>DIAL (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Finetunes CLIP on human-labeled trajectory data to enable selecting natural-language hindsight labels for trajectories, enabling instruction augmentation for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>image–text or video–text aligned data (CLIP pretraining and then finetuning on human-labeled trajectory pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper description: DIAL finetunes CLIP on human-labeled trajectories; precise dataset sizes/details are from the DIAL paper (cited) and not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Trajectory relabeling for embodied agents (general robotics / embodied tasks referenced in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Used for producing hindsight text instructions aligned with trajectory snippets so agents can be trained to follow natural language goals in embodied settings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (DIAL is used to produce text labels, not actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Varies by downstream embodied domain; in the referenced use it applies to trajectories with low-level actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Selects a hindsight instruction (from a candidate set) using a CLIP-based scoring model to associate trajectory video segments with natural-language instructions; not a direct mapping to actions but used to relabel data for supervised training of policies.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Visual inputs (frames or short clips) to compute CLIP embeddings for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Leverages CLIP's aligned vision-language space so that visual trajectory segments can be relabeled with natural language instructions without large human labeling effort.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not analyzed in this paper (citation only); potential limitations include candidate-set dependence and finetuning requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an example of using vision–language models to produce hindsight natural-language relabels for trajectories; the paper positions STEVE-1's MineCLIP-based relabeling as an alternative that uses MineCLIP embeddings and a learned prior rather than CLIP finetuning on many human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STEVE-1: A Generative Model for Text-to-Behavior in Minecraft', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1844.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1844.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flamingo (related work / Sumers et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flamingo (a visual-language model used zero-shot for hindsight relabeling, as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that uses Flamingo (a VLM) zero-shot to perform visual question answering style relabeling for hindsight instruction generation for trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Flamingo (as used in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A large visual-language model capable of few-shot/zero-shot multimodal understanding; in the cited work it is used to convert video/frames into natural-language relabels for hindsight replay.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>multimodal image/text pretraining (details not specified in this paper; cited work used Flamingo zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; details are available in the Flamingo and the cited work (Sumers et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Hindsight relabeling for embodied trajectory datasets (cited as a technique in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Used to produce natural-language labels for trajectory segments to supervise agents in embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Varies by downstream policy (used for relabeling trajectories which contain low-level actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>VLM used to answer 'what happened' questions about a video segment (VQA-style) and produce candidate natural language relabels; these are then used to label trajectory segments for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Frames/video input for the VLM; outputs are natural language labels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Uses large pretrained VLMs to avoid manual labeling by generating textual labels; benefits from strong multimodal pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not analyzed in this paper; potential issues include VLM hallucination or label noise.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an alternative approach for generating natural language hindsight labels; STEVE-1 instead leverages MineCLIP embeddings and a learned prior to translate text to visual goals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STEVE-1: A Generative Model for Text-to-Behavior in Minecraft', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1844.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1844.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for planning (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models used as high-level planners (as cited, e.g., works [61,62])</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced approaches that use LLMs to generate high-level plans or decomposition for Minecraft tasks; those high-level plans are executed by lower-level RL or scripted policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LLMs (generic; e.g., used in cited Minecraft planning works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Large language models pretrained on language used to produce high-level plans, instructions, or decomposition which are then used by specialized low-level controllers or RL agents to carry out embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>large-scale text corpora (pretraining of the LLMs themselves—details not provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper cites prior work that uses LLMs for high-level planning in Minecraft; specific LLM training corpora are not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Minecraft (high-level planning + execution pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>LLMs are used to generate plans or chains-of-subtasks for long-horizon Minecraft objectives; a separate low-level policy or planner executes the steps in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level textual plans or step descriptions (e.g., 'go chop a tree', 'collect logs').</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level mouse/keyboard actions executed by a separate policy or scripted controller.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Two-stage architecture: LLM produces high-level, symbolic or textual plans; a lower-level policy (RL or scripted controllers) maps those textual subgoals into sequences of low-level actions. STEVE-1 authors note these prior works but emphasize STEVE-1 itself is a more flexible low-level policy that could be combined with LLMs in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB perception for execution; LLMs operate on text and possibly on symbolic state descriptions provided by perception modules.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>LLMs provide strong high-level reasoning and decomposition capabilities from language pretraining; good when combined with reliable low-level executors.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Action-space mismatch and need for reliable grounding of text plans into precise motor actions; many prior systems required extra RL or scripting to execute LLM plans reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites LLM-based planning as a complementary approach; STEVE-1 focuses on direct low-level instruction following and posits that combining STEVE-1 with LLMs for high-level planning is a promising direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STEVE-1: A Generative Model for Text-to-Behavior in Minecraft', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Video pretraining (vpt): Learning to act by watching unlabeled online videos <em>(Rating: 2)</em></li>
                <li>Minedojo: Building open-ended embodied agents with internet-scale knowledge <em>(Rating: 2)</em></li>
                <li>Distilling internet-scale vision-language models into embodied agents <em>(Rating: 2)</em></li>
                <li>Robotic skill acquisition via instruction augmentation with vision-language models <em>(Rating: 2)</em></li>
                <li>Hierarchical text-conditional image generation with clip latents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1844",
    "paper_id": "paper-844b22bb025f485d85d00f1f61555a8ff0131658",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "MineCLIP",
            "name_full": "MineCLIP (domain-specific CLIP for Minecraft)",
            "brief_description": "A CLIP-style visual-language model trained to align Minecraft video clips with textual captions; used as a frozen embedding space for both visual goals and text instructions in STEVE-1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "MineCLIP (as used in STEVE-1)",
            "model_agent_description": "A pretrained visual-language encoder that produces joint embeddings for short video segments (16 frames) and text; MineCLIP is used frozen to (1) embed 16-frame visual goal clips used to relabel trajectories and (2) encode text instructions which are mapped by a learned prior into visual-embedding goals.",
            "pretraining_data_type": "paired web videos and captions (video-text contrastive pretraining on Minecraft gameplay and transcripts)",
            "pretraining_data_details": "The paper states MineCLIP was trained with a contrastive objective on pairs of Minecraft videos and transcripts from the web to produce aligned text and 16-frame video clip embeddings. Exact dataset size and collection details are from the MineCLIP source (cited in the paper) and are not enumerated in this paper.",
            "embodied_task_name": "Minecraft (MineRL / low-level mouse & keyboard control)",
            "embodied_task_description": "Open-ended Minecraft gameplay using the MineRL environment: short-horizon tasks such as going to and chopping trees, digging holes, collecting items, and prompt-chained multi-step tasks (e.g., gather resources then craft/build) using low-level discrete/continuous mouse & keyboard controls and raw RGB pixels.",
            "action_space_text": "N/A for MineCLIP itself (MineCLIP is a perceptual/text encoder); its pretraining involved text/video pairs (natural language captions) rather than action labels.",
            "action_space_embodied": "Low-level mouse and keyboard controls (joint hierarchical action space used by VPT; discrete keypresses and mouse movements as in VPT / MineRL)",
            "action_mapping_method": "Not a direct policy mapping — a learned conditional prior (CVAE) maps MineCLIP text embeddings z_y to MineCLIP visual embeddings z_{τ_goal}; the policy (finetuned VPT) conditions on visual embeddings z_{τ_goal} to produce low-level actions. Inference uses classifier-free guidance (combining conditional and unconditional policy logits) to trade off prior behavior vs. goal-conditioned behavior.",
            "perception_requirements": "RGB visual input (frames resized to 128×128×3 processed by a ResNet), MineCLIP provides additional visual/text embedding space; no depth or explicit object detectors required.",
            "transfer_successful": true,
            "performance_with_pretraining": "When STEVE-1 uses MineCLIP-based conditioning (via the CVAE prior + VPT policy), it robustly completes short-horizon evaluation: e.g., compared to the unconditional VPT agent STEVE-1 (with text prompting) collected 75× more dirt, 4.9× more wood, 22× more seeds, and traveled 4.3× farther; vs. the text-conditioned VPT baseline it collected 3.3× more dirt, 4.4× more wood, 8.1× more seeds, and traveled 2.2× farther. STEVE-1 also completed 12 of 13 early-game tasks in the authors' suite (short-horizon). Classifier-free guidance (λ tuned ≈5–7) further boosted performance (examples: up to 7.5× more dirt and 15× more wood vs λ=0 in an ablation).",
            "performance_without_pretraining": "Agents trained from scratch (random init) and finetuned on the same data underperform: in the paper, a scratch-initialized model finetuned for 100M frames was able to accomplish basic tasks like dirt collection but failed at more complex behaviors (e.g., finding and chopping trees) that pretrained variants could perform. Exact numeric baselines for every metric are not enumerated beyond the relative factors reported.",
            "sample_complexity_with_pretraining": "Text-to-goal prior: trained on ≈2,000 human-labeled text↔16-frame video pairs (less than 30 minutes of gameplay) and augmented with retrieval and ≈8,000 GPT-3.5-generated instructions (paper reports these augmentation numbers); the policy was finetuned using self-supervised packed hindsight relabeling on large unlabeled gameplay (contractor + VPT-generated) datasets (dataset described as 54M frames used to construct relabeled data; main STEVE-1 training reported as 160M frames of optimization). The instruction-labeled supervision required by the prior is therefore very small (~2k hand-labeled segments, augmented). The authors emphasize that instruction capability was obtained with only ≈$60 of compute and ~2,000 instruction-labeled trajectory segments for the prior.",
            "sample_complexity_without_pretraining": "A model trained from scratch and finetuned for 100M frames (experiment reported) still failed to match pretrained variants on tasks requiring complex behaviors (e.g., chopping trees), indicating that &gt;100M frames and extensive finetuning did not recover the pretrained agents' capabilities in the experiments.",
            "sample_complexity_gain": "Qualitative/orders-of-magnitude statement: pretraining (MineCLIP + VPT foundation weights) enabled instruction-following with only a few thousand labeled instruction-video pairs for the prior and self-supervised relabeling for the policy, whereas training from scratch required at least 100M frames of finetuning and still underperformed; authors describe this as a dramatic reduction in required instruction-labeled data (from needing large instruction-annotated trajectories to only ~2k hand labels) and compute for instruction alignment.",
            "transfer_success_factors": "Key factors: (1) MineCLIP provides a joint text–video embedding space so the prior only needs to learn a mapping between text and visual embeddings; (2) a strong pretrained behavioral prior (VPT) provides low-level motor competence; (3) packed hindsight relabeling allows leveraging large unlabeled gameplay for goal-conditioned supervision; (4) classifier-free guidance at inference to bias policy toward goal-conditioned behavior; (5) augmentation of the tiny labeled prior dataset with retrieval and GPT-generated instructions.",
            "transfer_failure_factors": "Gaps noted: (1) gap between text-conditioned performance (via prior) and direct visual goals — prior sometimes imperfectly maps text to visual embeddings; (2) long-horizon tasks (&gt;≈200 timesteps) are underrepresented because hindsight relabeling sampled near-future goals (max ~200 timesteps), limiting long-horizon transfer; (3) misgeneralization of goals (goal misgeneralization) particularly for smaller-scale models; (4) remaining ambiguity of natural language prompts and need for prompt engineering.",
            "key_findings": "A domain-specific visual-language model (MineCLIP) pretrained on video–text pairs can be used as a fixed embedding space to bridge language and visual goals; combining a small amount (~2k) of human text↔video pairs (augmented) with a CVAE prior and a pretrained behavioral policy (VPT) finetuned with self-supervised packed hindsight relabeling yields strong instruction-following in Minecraft with low-level controls. Pretraining (both MineCLIP for language understanding and VPT for motor priors) is the dominant contributor to capability; classifier-free guidance and data-scaling further improve downstream performance, while training from scratch (no pretraining) is much less effective even after large amounts of finetuning.",
            "uuid": "e1844.0",
            "source_info": {
                "paper_title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "gpt-3.5-turbo",
            "name_full": "OpenAI GPT-3.5-turbo (used for dataset augmentation)",
            "brief_description": "A large language model (API used) employed to generate additional text instructions to augment the small human-labeled text–video pair dataset for training the CVAE prior that maps text embeddings to visual embeddings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "gpt-3.5-turbo (used to synthesize instruction labels)",
            "model_agent_description": "Commercial large language model accessed via OpenAI API; used to synthesize ≈8,000 additional text instructions to augment a 2,000 human-labeled text–video training set for the CVAE prior.",
            "pretraining_data_type": "large-scale language model (pretrained on web text; details of GPT-3.5 pretraining not specified in this paper)",
            "pretraining_data_details": "The paper only states they used the OpenAI API gpt-3.5-turbo to generate ≈8,000 instructions; it does not detail GPT-3.5's pretraining corpora within this work.",
            "embodied_task_name": "Minecraft (used indirectly to augment prior training data for text→visual embedding mapping)",
            "embodied_task_description": "Same MineRL / Minecraft early-game short-horizon tasks — GPT-generated instructions were used to augment the training pairs for the CVAE prior so the prior could map a wider variety of text prompts to MineCLIP visual embeddings.",
            "action_space_text": "N/A (GPT output are natural language instructions, not actions).",
            "action_space_embodied": "N/A (GPT is not acting in environment; its outputs are used as labels for the prior which in turn conditions the policy that outputs low-level mouse/keyboard actions).",
            "action_mapping_method": "GPT-3.5 generated text instructions which were paired with retrieved 16-frame gameplay segments (via MineCLIP similarity) to create additional text↔video training pairs for the CVAE prior. The CVAE maps MineCLIP text embeddings (z_y) into MineCLIP visual embeddings (z_{τ_goal}) used by the policy.",
            "perception_requirements": "N/A for GPT itself; its outputs are used in a pipeline that requires RGB frames and MineCLIP embeddings.",
            "transfer_successful": true,
            "performance_with_pretraining": "Augmentation with GPT-3.5-generated instructions increased the CVAE training set (paper reports adding ≈8k generated instructions) and improved some programmatic task metrics in ablations (augmentation helped for dirt and seed collection in some experiments, though it slightly hurt other metrics like log collection and travel distance in one ablation).",
            "performance_without_pretraining": "Not directly applicable; comparison in paper is between prior trained on only 2k human pairs vs. augmented sets. The unaugmented CVAE (human-only) was a baseline in ablations.",
            "sample_complexity_with_pretraining": "Augmented prior dataset: human 2,000 pairs + retrieval + ≈8,000 GPT-generated = ≈10,000 effective text↔video pairs (paper reports these numbers). The prior trained on this small augmented set.",
            "sample_complexity_without_pretraining": "Human-only prior: ≈2,000 hand-labeled pairs (less than 30 minutes of gameplay); ablations showed some performance differences when augmentation was omitted.",
            "sample_complexity_gain": "Augmentation increased the available labeled instructions by multiple× (from ~2k to ~10k synthesised/augmented pairs), improving certain downstream metrics; this reduced the need for extensive human labeling.",
            "transfer_success_factors": "Rapid expansion of the text label distribution via GPT-generated instructions enabled the CVAE prior to see more linguistic variety; retrieval-based pairing with gameplay segments (using MineCLIP) anchors generated instructions to realistic visuals.",
            "transfer_failure_factors": "Synthetic instructions can introduce mismatch/noise; ablations showed augmentation sometimes hurt some metrics (e.g., wooden log collection and travel distance) possibly due to distribution mismatch or noisy pairs.",
            "key_findings": "Using a large LLM (gpt-3.5-turbo) to synthetically expand a small human-labeled text↔video dataset allowed training a prior with far fewer hand-labeled minutes of gameplay; this is an effective low-cost augmentation strategy but can introduce noisy/less-useful examples for some metrics, so retrieval/curation remains important.",
            "uuid": "e1844.1",
            "source_info": {
                "paper_title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "DIAL (related work)",
            "name_full": "DIAL (vision-language relabeling pipeline)",
            "brief_description": "A referenced method that finetunes CLIP on human-labeled trajectories and then uses it to select hindsight instructions from a candidate set for relabeling trajectories.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "DIAL (as cited)",
            "model_agent_description": "Finetunes CLIP on human-labeled trajectory data to enable selecting natural-language hindsight labels for trajectories, enabling instruction augmentation for embodied agents.",
            "pretraining_data_type": "image–text or video–text aligned data (CLIP pretraining and then finetuning on human-labeled trajectory pairs)",
            "pretraining_data_details": "Paper description: DIAL finetunes CLIP on human-labeled trajectories; precise dataset sizes/details are from the DIAL paper (cited) and not specified here.",
            "embodied_task_name": "Trajectory relabeling for embodied agents (general robotics / embodied tasks referenced in related work)",
            "embodied_task_description": "Used for producing hindsight text instructions aligned with trajectory snippets so agents can be trained to follow natural language goals in embodied settings.",
            "action_space_text": "N/A (DIAL is used to produce text labels, not actions).",
            "action_space_embodied": "Varies by downstream embodied domain; in the referenced use it applies to trajectories with low-level actions.",
            "action_mapping_method": "Selects a hindsight instruction (from a candidate set) using a CLIP-based scoring model to associate trajectory video segments with natural-language instructions; not a direct mapping to actions but used to relabel data for supervised training of policies.",
            "perception_requirements": "Visual inputs (frames or short clips) to compute CLIP embeddings for scoring.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Leverages CLIP's aligned vision-language space so that visual trajectory segments can be relabeled with natural language instructions without large human labeling effort.",
            "transfer_failure_factors": "Not analyzed in this paper (citation only); potential limitations include candidate-set dependence and finetuning requirements.",
            "key_findings": "Mentioned as an example of using vision–language models to produce hindsight natural-language relabels for trajectories; the paper positions STEVE-1's MineCLIP-based relabeling as an alternative that uses MineCLIP embeddings and a learned prior rather than CLIP finetuning on many human labels.",
            "uuid": "e1844.2",
            "source_info": {
                "paper_title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Flamingo (related work / Sumers et al.)",
            "name_full": "Flamingo (a visual-language model used zero-shot for hindsight relabeling, as cited)",
            "brief_description": "Referenced work that uses Flamingo (a VLM) zero-shot to perform visual question answering style relabeling for hindsight instruction generation for trajectories.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Flamingo (as used in cited work)",
            "model_agent_description": "A large visual-language model capable of few-shot/zero-shot multimodal understanding; in the cited work it is used to convert video/frames into natural-language relabels for hindsight replay.",
            "pretraining_data_type": "multimodal image/text pretraining (details not specified in this paper; cited work used Flamingo zero-shot)",
            "pretraining_data_details": "Not specified in this paper; details are available in the Flamingo and the cited work (Sumers et al.).",
            "embodied_task_name": "Hindsight relabeling for embodied trajectory datasets (cited as a technique in related work)",
            "embodied_task_description": "Used to produce natural-language labels for trajectory segments to supervise agents in embodied tasks.",
            "action_space_text": "N/A",
            "action_space_embodied": "Varies by downstream policy (used for relabeling trajectories which contain low-level actions).",
            "action_mapping_method": "VLM used to answer 'what happened' questions about a video segment (VQA-style) and produce candidate natural language relabels; these are then used to label trajectory segments for policy learning.",
            "perception_requirements": "Frames/video input for the VLM; outputs are natural language labels.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Uses large pretrained VLMs to avoid manual labeling by generating textual labels; benefits from strong multimodal pretraining.",
            "transfer_failure_factors": "Not analyzed in this paper; potential issues include VLM hallucination or label noise.",
            "key_findings": "Cited as an alternative approach for generating natural language hindsight labels; STEVE-1 instead leverages MineCLIP embeddings and a learned prior to translate text to visual goals.",
            "uuid": "e1844.3",
            "source_info": {
                "paper_title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LLMs for planning (related work)",
            "name_full": "Large Language Models used as high-level planners (as cited, e.g., works [61,62])",
            "brief_description": "Referenced approaches that use LLMs to generate high-level plans or decomposition for Minecraft tasks; those high-level plans are executed by lower-level RL or scripted policies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "LLMs (generic; e.g., used in cited Minecraft planning works)",
            "model_agent_description": "Large language models pretrained on language used to produce high-level plans, instructions, or decomposition which are then used by specialized low-level controllers or RL agents to carry out embodied tasks.",
            "pretraining_data_type": "large-scale text corpora (pretraining of the LLMs themselves—details not provided in this paper)",
            "pretraining_data_details": "The paper cites prior work that uses LLMs for high-level planning in Minecraft; specific LLM training corpora are not described in this paper.",
            "embodied_task_name": "Minecraft (high-level planning + execution pipeline)",
            "embodied_task_description": "LLMs are used to generate plans or chains-of-subtasks for long-horizon Minecraft objectives; a separate low-level policy or planner executes the steps in the environment.",
            "action_space_text": "High-level textual plans or step descriptions (e.g., 'go chop a tree', 'collect logs').",
            "action_space_embodied": "Low-level mouse/keyboard actions executed by a separate policy or scripted controller.",
            "action_mapping_method": "Two-stage architecture: LLM produces high-level, symbolic or textual plans; a lower-level policy (RL or scripted controllers) maps those textual subgoals into sequences of low-level actions. STEVE-1 authors note these prior works but emphasize STEVE-1 itself is a more flexible low-level policy that could be combined with LLMs in future work.",
            "perception_requirements": "RGB perception for execution; LLMs operate on text and possibly on symbolic state descriptions provided by perception modules.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "LLMs provide strong high-level reasoning and decomposition capabilities from language pretraining; good when combined with reliable low-level executors.",
            "transfer_failure_factors": "Action-space mismatch and need for reliable grounding of text plans into precise motor actions; many prior systems required extra RL or scripting to execute LLM plans reliably.",
            "key_findings": "Paper cites LLM-based planning as a complementary approach; STEVE-1 focuses on direct low-level instruction following and posits that combining STEVE-1 with LLMs for high-level planning is a promising direction.",
            "uuid": "e1844.4",
            "source_info": {
                "paper_title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Video pretraining (vpt): Learning to act by watching unlabeled online videos",
            "rating": 2
        },
        {
            "paper_title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "rating": 2
        },
        {
            "paper_title": "Distilling internet-scale vision-language models into embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Robotic skill acquisition via instruction augmentation with vision-language models",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical text-conditional image generation with clip latents",
            "rating": 2
        }
    ],
    "cost": 0.02171575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft</h1>
<p>Shalev Lifshitz ${ }^{1,2 <em>}$<br>shalev.lifshitz@mail.utoronto.ca<br>Keiran Paster ${ }^{1,2 </em>}$<br>keirp@cs.toronto.edu<br>Harris Chan ${ }^{1,2 \dagger}$<br>hchan@cs.toronto.edu<br>Jimmy Ba ${ }^{1,2}$<br>jba@cs.toronto.edu<br>Sheila McIlraith ${ }^{1,2}$<br>sheila@cs.toronto.edu</p>
<p>${ }^{1}$ Department of Computer Science, University of Toronto, Toronto, Canada.
${ }^{2}$ Vector Institute for Artificial Intelligence, Toronto, Canada.</p>
<h4>Abstract</h4>
<p>Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces a methodology, inspired by unCLIP, for instruction-tuning generative models of behavior without relying on a large dataset of instruction-labeled trajectories. Using this methodology, we create an instruction-tuned Video Pretraining (VPT) model called STEVE-1, which can follow short-horizon open-ended text and visual instructions in Minecraft ${ }^{\mathrm{TM}}$. STEVE- 1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, reducing the need for costly human text annotations, and all for only $\$ 60$ of compute. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 sets a new bar for open-ended instruction-following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming previous baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We provide experimental evidence highlighting key factors for downstream performance, including pretraining, classifier-free guidance, and data scaling. All resources, including our model weights, training scripts, and evaluation tools are made available for further research.</p>
<h2>1 Introduction</h2>
<p>The ability to use text instructions to control and interact with powerful AI models has made these models accessible and customizable for the masses. Such models include ChatGPT [41], which can respond to messages written in natural language and perform a wide array of tasks, and Stable Diffusion [50], which turns natural language into an image. While those models cost anywhere from hundreds of thousands to hundreds of millions of dollars to train, there has been an equally exciting trend whereby powerful open-source foundation models like LLaMA [59] can be finetuned with surprisingly little compute and data to become instruction-following (e.g., [58, 13]).
In this paper, we study whether such an approach could be applicable to sequential decision-making domains. Unlike in text and image domains, diverse data for sequential decision-making is very expensive and often does not come with a convenient "instruction" label like captions for images. We propose to instruction-tune pretrained generative models of behavior, mirroring the advancements seen in recent instruction-tuned LLMs like Alpaca [58], and without relying on a large dataset of instruction-labeled trajectories.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Like unCLIP [48], our approach involves two models. First, we train the <em>policy</em> by finetuning VPT to achieve goals given by pretrained MineCLIP [17] visual embeddings using our gameplay dataset. Second, for the <em>prior</em> model, we train a CVAE [54] to sample MineCLIP visual embeddings given a text prompt. The combination of these two models enables our agent to follow text and visual instructions.</p>
<p>In the past year, two foundation models for the popular open-ended video game Minecraft™ were released: a foundation model for behavior called VPT [5] and a model aligning text and video clips called MineCLIP [17]. This has opened up an intriguing avenue to explore fine-tuning for instruction-following in the sequential decision-making domain of Minecraft. VPT was trained on 70k hours of Minecraft gameplay, so the agent already has vast knowledge about the Minecraft environment. However, just as the massive potential of LLMs was unlocked by aligning them to follow instructions, it is likely that the VPT model has the potential for general, controllable behavior if it is finetuned to follow instructions. In particular, our paper demonstrates a method for fine-tuning VPT to follow short-horizon text instructions with only $60 of compute and around 2,000 instruction-labeled trajectory segments.</p>
<p>Our method draws inspiration from unCLIP [48], the approach used to create the popular text-to-image model DALL•E 2. We decompose the problem of creating an instruction-following Minecraft agent into two models: a VPT model finetuned to achieve visual goals embedded in the MineCLIP latent space, and a prior model that translates text instructions into MineCLIP visual embeddings. We finetune VPT using behavioral cloning with self-supervised data generated with hindsight relabeling [3], avoiding the use of expensive text-instruction labels in favor of visual MineCLIP embeddings. We apply unCLIP with classifier-free guidance [23] to create our agent called STEVE-1, which sets a new bar for open-ended instruction-following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming the baseline set by Baker et al. [5].</p>
<p>Our main contributions are as follows:</p>
<ul>
<li>We introduce a methodology, inspired by unCLIP [48], for instruction-tuning generative models of behavior without relying on a large dataset of expensive instruction labels.</li>
<li>We apply this methodology to create STEVE-1, a Minecraft agent that can follow short-horizon open-ended text and visual instructions with a high degree of accuracy, all for only $60 of compute. We perform extensive evaluations of our agent, showing that it can robustly complete 12 of 13 goal-conditioned control tasks in our early-game evaluation suite in Minecraft. For long-horizon tasks¹ like crafting and building, we show that a basic version of prompt chaining can dramatically improve performance.</li>
<li>We provide experimental evidence highlighting key factors for downstream performance, including pretraining, classifier-free guidance, data scaling, prompt-engineering, and other design choices. In particular, we show that unCLIP [48] and classifier-free guidance [23] translate well to sequential decision-making and are essential for strong performance.</li>
<li>We release model weights for STEVE-1 as well as training scripts and evaluation code to help foster more research into instructable, open-ended sequential decision-making agents.²</li>
</ul>
<p>¹ Short-horizon tasks require few steps: e.g., go to a tree and chop it down, dig a hole. Long-horizon tasks take many steps: e.g., craft complex recipes from scratch, build a house.</p>
<p>² Model weights, training code, videos, and an interactive demo script are hosted on our project webpage at https://sites.google.com/view/steve-1.</p>
<h1>2 Related Work</h1>
<p>Minecraft as a Test-bed for AI Minecraft has gained popularity as a benchmark for AI research due to its complex and dynamic environment, making it a rich test-bed for reinforcement learning and other AI methods (e.g., [26, 19, 17, 21, 40, 62, 38, 9]). We leverage the MineRL environment [19] to research the creation of agents that can follow open-ended instructions in complex visual environments using only low-level actions (mouse and keyboard). We build STEVE-1 on top of two recent foundation models. In order to align text and videos, we use MineCLIP [17], a CLIP [47] model trained on paired web videos of Minecraft gameplay and associated captions. To train STEVE-1's policy, we fine-tune VPT [5], a foundation model of Minecraft behavior that is pretrained on 70k hours of web videos of Minecraft along with estimated mouse and keyboard actions. Several prior works [61, 62] have explored the use of LLMs in creating instructable Minecraft agents. These works typically use LLMs to make high-level plans that are then executed by lower-level RL [40, 62] or scripted [46] policies. Since STEVE-1 is a far more flexible low-level policy, the combination of STEVE-1 with LLMs is a promising direction for future work. Fan et al. [17] introduced an agent trained using RL with MineCLIP as a shaping reward on 12 different tasks and conditioned on MineCLIP-embedded text-prompts. However, this agent failed to generalize beyond the original set of tasks without further RL finetuning using the MineCLIP reward function. Cai et al. [9] proposed a Goal-Sensitive Backbone architecture for goal-conditioned control in Minecraft which is trained on a fixed set of goals, while StEVE-1 learns goal-reaching behavior from a large dataset in a self-supervised way without training on an explicit set of tasks.</p>
<p>Foundation Models for Sequential Decision-Making Foundation models which are pretrained on vast amounts of data and then finetuned for specific tasks have recently shown great promise in a variety of domains including language [8, 14, 59], vision [48, 10, 47], and robotics [7, 53, 25, 39, 65]. GATO [49] and RT-1 [7] have demonstrated the potential of training transformers to perform both simulated and real-world robotic tasks. With the exception of Kumar et al. [30], which uses Qlearning, the vast majority of cases [32, 7, 49] where deep learning has been scaled to large, multitask offline-RL datasets have used supervised RL. Supervised RL (e.g., [42, 18, 12]) works by framing the sequential decision-making problem as a prediction problem, where the model is trained to predict the next action conditioned on some future outcome. While these approaches are simple and scale well with large amounts of compute and data, more work is needed to understand the trade-offs between supervised RL and Q-learning or policy gradient-based methods [43, 44, 6, 55]. Recent works explore the use of hindsight relabeling [3] using vision-language models [47, 2] to produce natural language relabeling instructions. DIAL [65] finetunes CLIP [47] on human-labeled trajectories, which is then used to select a hindsight instruction from a candidate set. Sumers et al. [56] uses Flamingo [2] zero-shot for hindsight relabeling by framing it as a visual-question answering (VQA) task. In contrast, STEVE-1 relabels goals using future trajectory segment embeddings given by the MineCLIP [17] visual embedding.</p>
<p>Text-Conditioned Generative Models There has been a recent explosion of interest in text-to-X models, including text-to-image (e.g., [48, 51, 50]), text-to-3D (e.g., [27, 35]), and even text-to-music (e.g., [1]). These models are typically either autoregressive transformers modeling sequences of discrete tokens [60, 8] or diffusion models [24]. Most related to our work is unCLIP, the method used for DALL-E 2 [48]. unCLIP works by training a generative diffusion model to sample images from CLIP [47] embeddings of those images. By combining this model with a prior that translates text to visual CLIP embeddings, unCLIP can produce photorealistic images for arbitrary text prompts. unCLIP and many other diffusion-based approaches utilize a technique called classifier-free guidance [23], which lets the model trade-off between mode-coverage and sample fidelity post-training. We utilize the basic procedure of unCLIP and classifier-free guidance for training STEVE-1.</p>
<h2>3 Method</h2>
<p>Inspired by the rapid recent progress in instruction-tuning Large Language Models (LLMs), we choose to leverage the recently released Video Pretraining (VPT) [5] model as a starting point for our agent. Since VPT was trained on 70k hours of Minecraft gameplay, the agent already has vast knowledge about the Minecraft environment. However, just as the massive potential of LLMs was unlocked by aligning them to follow instructions, it is likely that the VPT model has the potential</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: To create goal-conditioned data for finetuning, we randomly select timesteps from episodes and use hindsight relabeling to set the intermediate goals for the trajectory segments to those visual MineCLIP embeddings. This self-supervised data teaches the agent which actions lead to which states.</p>
<p>For general, controllable behavior if it is finetuned to follow instructions. In this work, we present a method for finetuning VPT to follow natural, open-ended textual and visual instructions, which opens the door for a wide range of uses for VPT in Minecraft.</p>
<p>Our approach is inspired by unCLIP, the method behind the recent text-to-image generation model, DALL•E 2 [48]. Our goal is to create a generative model of behavior in Minecraft conditioned on text instructions <em>y</em>. To do so, we utilize a dataset of Minecraft trajectory segments, some of which contain instruction labels <em>y</em>: [(<em>τ</em><sub>1</sub>, <em>y</em><sub>1</sub>), (<em>τ</em><sub>2</sub>, <em>y</em><sub>2</sub>), ..., (<em>τ</em><sub><em>n</em></sub>, ∅)] where <em>τ</em> is a trajectory of observations and actions. We also employ a pretrained CLIP model called MineCLIP [17], which generates aligned latent variables <em>z</em><sub><em>τ</em><sub><em>t</em></sub><em>t</em>=<em>t</em><sub><em>B</em></sub><em>B</em>−<em>z</em><sub><em>y</em></sub>, where <em>z</em><sub><em>τ</em><sub><em>t</em></sub><em>t</em>=<em>t</em><sub><em>B</em></sub><em>B</em> is an embedding of any 16 consecutive timesteps from the trajectory. MineCLIP is trained using a contrastive objective on pairs of Minecraft videos and transcripts from the web. For simplicity of notation, we refer to the MineCLIP embedding of the last 16 timesteps of a trajectory segment as <em>z</em><sub><em>τ</em><sub><em>goal</em></sub>. Like unCLIP [48], we utilize a hierarchical model consisting of a prior and a policy:</p>
<ul>
<li>A <em>prior</em> $p(z_{\tau_{\text{goal}}}|y)$ that produces a latent variable $z_{\tau_{\text{goal}}}$ conditioned on a text instruction $y$.</li>
<li>A <em>policy</em> $p(\tau|z_{\tau_{\text{goal}}})$ that produces a trajectory conditioned on a latent variable $z_{\tau_{\text{goal}}}$.</li>
</ul>
<p>These two models can then be combined to produce a generative model of behaviors conditioned on text instructions:</p>
<p>$$p(\tau|y) = p(\tau, z_{\tau_{\text{goal}}}|y) = p(z_{\tau_{\text{goal}}}|y)p(\tau|z_{\tau_{\text{goal}}}) \tag{3.1}$$</p>
<h3>3.1 Policy</h3>
<p>To learn our policy, we finetune VPT, a foundation model of Minecraft behaviors <em>p</em><sub>θ</sub>(<em>τ</em>) trained on 70k hours of Minecraft gameplay videos. Specifically, VPT consists of a ResNet [22] that processes frames of dimension 128 × 128 × 3, and a Transformer-XL [15] which processes the frame representations and autoregressively predicts the next action using the joint hierarchical action space described in Baker et al. [5]. In order to modify the architecture to condition on goal information, we add an affine transformation of <em>z</em><sub><em>τ</em><sub><em>goal</em></sub></sub> to the output of the ResNet before passing it to the transformer:</p>
<p>Process Frames: ResNet<sub>θ</sub>(<em>o</em><sub><em>t</em></sub>) → <em>x</em><sub><em>t</em></sub></p>
<p>[+ Conditioning on MineCLIP Embedding Goal]: <em>x</em><sub><em>t</em></sub> → <em>x</em><sub><em>t</em></sub> + <em>W</em><sub>θ</sub><em>z</em><sub><em>τ</em><sub>goal</sub></sub> + <em>b</em><sub>θ</sub></p>
<p>Predict Actions: TransformerXL<sub>θ</sub>(<em>x</em><sub><em>t</em></sub>, ..., <em>x</em><sub><em>t</em>+<em>T</em></sub>) → <em>a</em><sub><em>t</em>+<em>T</em></sub></p>
<p>In order to finetune VPT to condition on goals, we finetune the model using a method inspired by supervised RL approaches like Decision Transformer [12], GLAMOR [42], and GCSL [18]. We use a modification of hindsight relabeling which we call <strong>packed hindsight relabeling</strong> (see Figure 2) to generate a new dataset of trajectories with goals pulled from future states that periodically switch. In contrast with hindsight relabeling, packed hindsight relabeling packs multiple relabeled sequences into a single sequence. Specifically, our method to generate this dataset consists of two steps:</p>
<ol>
<li>Given a trajectory <em>τ</em> with <em>T</em> timesteps, randomly generate indices to select goals from: <em>i</em><sub>1</sub>, <em>i</em><sub>2</sub>, ..., <em>i</em><sub><em>n</em></sub>. These indices are chosen by starting at the first timestep and repeatedly sampling a new timestep by adding a random value to the previous timestep. This ensures that the data reflects that some goals may take longer to achieve than others.</li>
<li>For each chosen goal at timestep <em>i</em><sub><em>j</em></sub>, set the goals for timesteps <em>i</em><sub><em>j</em>−1</sub> + 1, ..., <em>i</em><sub><em>j</em></sub> to be the goal at timestep <em>i</em><sub><em>j</em></sub>, denoted <em>z</em><sub><em>τ</em><sub><em>i</em><sub><em>j</em></sub></sub>.</li>
</ol>
<p>Our final dataset $\mathcal{D}<em 1="1">{\text {relabeled }}$ consists of observation sequences $\left(o</em>\right)$. We then finetune VPT on this dataset using a supervised loss to predict each action autoregressively using a causal attention mask:}, \ldots, o_{T}\right)$, action sequences $\left(a_{1}, \ldots, a_{T}\right)$, and packed hindsight relabeled goals $\left(z_{1}, \ldots, z_{T</p>
<p>$$
\mathcal{L}<em _mathcal_D="\mathcal{D">{\text {policy }}(\theta)=\mathbb{E}</em><em _theta="\theta">{\text {relded }}}\left[-\log p</em>\right)\right]
$$}\left(a_{t} \mid o_{1 \ldots t}, z_{1 \ldots t</p>
<h1>3.2 Prior</h1>
<p>In order to condition not only on embeddings of visual goals but on latent goals, we need the prior, a model that produces a latent variable $z_{\tau_{\text {goal }}}$ conditioned on a text instruction $y$. Our model is a simple conditional variational autoencoder (CVAE) [54, 29] with a Gaussian prior and a Gaussian posterior. Rather than learn to condition directly on text, we choose to condition on frozen text representations from MineCLIP $z_{y}$. Thus, the prior learns a function to translate from a text embedding $z_{y}$ to a visual embedding $z_{\tau_{\text {goal }}}$ (see Appendix C. 5 for further discussion). Both the encoder and decoder of our CVAE are parameterized as two-layer MLPs with 512 hidden units and layer normalization [4]. We train the model on our dataset, for which we have text labels $\mathcal{D}_{\text {labels }}$ using the following loss:</p>
<p>$$
\mathcal{L}<em _goal="{goal" _left_z__tau__text="\left(z_{\tau_{\text">{\text {prior }}(\phi)=\mathbb{E}</em>}}}, z_{y}\right) \sim \mathcal{D<em _phi="\phi">{\text {labels }}}\left[\operatorname{KL}\left(q</em>}\left(z_{\tau_{\text {goal }}} \mid z_{y}\right) | p\left(z_{\tau_{\text {goal }}}\right)\right)-\mathbb{E<em _phi="\phi">{c \sim q</em>\right)\right]\right]
$$}\left(z_{\tau_{\text {goal }}} \mid z_{y}\right)}\left[\log p_{\phi}\left(z_{\tau_{\text {goal }}} \mid c, z_{y</p>
<h3>3.3 Datasets</h3>
<p>To train our policy, we gather a gameplay dataset with 54M frames ( $\approx 1$ month at 20FPS) of Minecraft gameplay along with associated actions from two sources: contractor gameplay and VPT-generated gameplay. To train our prior, we use a small dataset of text-video pairs gathered by humans and augmented using the OpenAI API gpt-3.5-turbo model [41] and MineCLIP. See Appendix D for more detailed dataset information.</p>
<p>OpenAI Contractor Dataset We use 39M frames sourced from the contractor dataset which VPT [5] used to train its inverse dynamics model and finetune its policy. The dataset was gathered by hiring human contractors to play Minecraft and complete tasks such as house building or obtaining a diamond pickaxe. During gameplay, keypresses and mouse movements are recorded. We use the same preprocessing as VPT, including filtering out null actions.</p>
<p>VPT-Generated Dataset We generate an additional dataset of 15M frames by generating random trajectories using the various pretrained VPT agents. The diversity of this dataset is improved by randomly switching between models during trajectories [44], randomly resetting the agent's memory, and randomly turning the agent to face a new direction.</p>
<p>Text-Video Pair Dataset To train our prior model, which learns a mapping between text embeddings and visual embeddings, we also manually gather a small dataset of 2,000 text instructions paired with 16 -frame video segments (less than a second) from our gameplay dataset. This dataset corresponds to less than 30 minutes of gameplay and takes just a few hours to collect. We augment this dataset by using the alignment between text and video embeddings from MineCLIP. For each text instruction, we find the top $k$ most similar gameplay segments in our dataset and use the corresponding 16 -frame segment as additional training data. For augmentation, we also add 8,000 text-instructions generated by the OpenAI API gpt-3.5-turbo model [41], in addition to our 2,000 hand-labeled instructions.</p>
<h3>3.4 Inference</h3>
<p>At inference time, we use the prior to sample a latent goal $z_{\tau_{\text {goal }}}$ from the text instruction $y$. We then use the policy to autoregressively sample actions $a_{t}$ conditioned on the observation history $o_{1 \ldots t}$ and the latent goal $z_{\tau_{\text {goal }}}$. Similar to the observation in Appendix I of Baker et al. [5], even with conditioning, the policy often fails to follow its instruction and simply acts according to its prior behavior. To mitigate this, we borrow another trick used in image generation models: classifier-free guidance. Specifically, during inference we simultaneously compute logits for the policy conditioned on the goal $f\left(o_{t}, \ldots, o_{t+1}, z_{\tau_{\text {goal }}}\right)$ and for the unconditional policy $f\left(o_{t}, \ldots, o_{t+1}\right)$. We then compute a combination of the two logits using a $\lambda$ parameter to trade-off between the two:</p>
<p>By setting a higher value of $\lambda$, we can encourage the policy to follow actions that are more likely when conditioned on the goal and, as demonstrated in Section 4.5, this significantly improves performance. Also, in order to train the policy to generate these unconditional logits, we occasionally dropout the goal embedding $z_{\tau_{\text {goal }}}$ from the policy's input (with probability 0.1). This lets us generate both the conditional and unconditional logits using the same model with batch processing at inference time.</p>
<h1>3.5 Evaluation</h1>
<p>Evaluating the performance of our agent is a challenging task due to the wide variety of instructions that are possible and the difficulty of evaluating whether the agent has successfully achieved its task. We use a combination of programmatic evaluation metrics and automatic MineCLIP evaluation metrics to get a sense of the agent's capability level. We collectively refer to all of our evaluation tasks including the 11 evaluation tasks from Figure 3 and the two prompt chaining tasks from Section 4.3 as our early-game evaluation suite.</p>
<p>Programmatic Evaluation We compute programmatic evaluation metrics by monitoring the MineRL [19] environment state throughout each evaluation episode. As done in VPT [5], we compute multiple programmatic metrics including travel distance and early-game item collection. The travel distance is the maximum displacement of the agent along on the horizontal (X-Z) plane, measured from the initial spawn point. For early-game inventory counts, we store the maximum number of log, seed, and dirt items seen in the agent's inventory during the episode.</p>
<p>MineCLIP Evaluation We explore the use of text-visual alignment in MineCLIP latent space between trajectories and text or visual goals to evaluate our agent over a wider variety of tasks where programmatic evaluation isn't practical. To determine the degree to which a task has been completed at all during an evaluation episode, we record the minimum cosine distance between the (text or visual) goal embedding and the visual MineCLIP embedding at any timestep during an episode.</p>
<h2>4 Results</h2>
<p>In our experiments, we aim to answer the following questions:</p>
<ol>
<li>How well does STEVE-1 perform at achieving both text and visual goals in Minecraft?</li>
<li>How does our method scale with more data?</li>
<li>What choices are important for the performance of our method?</li>
</ol>
<h3>4.1 Training Setup</h3>
<p>We base our implementation off of the official VPT codebase ${ }^{3}$. The main STEVE-1 agent is trained using Pytorch [45] distributed data parallel on four A40 GPUs for 160M frames, or just under three epochs of our gameplay dataset. Hyperparameters are selected to match those in Baker et al. [5] with the exception of learning rate, which we set to $4 \mathrm{e}-5$. Our models are optimized using AdamW [37]. See Table 1 for a full list of hyperparameters.</p>
<h3>4.2 Performance on Textual and Visual Goals</h3>
<p>Due to the hierarchical nature of our model, we can evaluate the performance of our agent at achieving either text or visual goals simply by choosing whether to use the prior to condition on text or bypass the prior and condition on a MineCLIP video embedding directly. We first tested our model on a set of 11 tasks that are achievable within the first 2.5 minutes of gameplay and which do not require multiple steps to complete (e.g., chop a tree or dig a hole, but not build a house). A complete list of the tasks and prompts we used for evaluation can be found in Table 3 in the appendix. To select visual goals for testing each of the evaluation tasks, we implemented a tool that searches through</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left: In our programmatic evaluations, STEVE-1 performed far better than the unconditional VPT agent early-game-2x and the text-conditioned VPT agent when prompted appropriately. The asterisk * in the "VPT (Text)*" indicates that this result was taken from Appendix I in [5], which had twice the episode length compared to our setting. On some tasks, visual outperforms text-based prompting, creating a gap that can likely be bridged through better prompt engineering. Right: Across our 11 MineCLIP evaluation tasks, STEVE-1 achieves the shortest distance between the episode and the MineCLIP goal embedding when prompted appropriately except for in two cases, where it mixes up digging and dirt and swimming and going underwater. This shows the strong general performance of STEVE-1 across a wide variety of short-horizon tasks. The dashed box marks the minimum element along the row, and the diagonal number signifies the diagonal element's rank ( 0 means it is the minimum row element). See Figure 14 for sample frames from each of the 11 visual goals and Figure 13 for a success-rate version of this matrix.
$10 \%$ of our gameplay dataset by finding the closest 16 -frame videos to a given text prompt. We then manually selected a 16 -frame video that clearly demonstrates the task being completed and use the corresponding MineCLIP video embedding as the goal embedding for that task. Screenshots of these visual goals can be found in Figure 14 in the appendix.
In Figure 3, we compare the performance of our text and visual-conditioned agents with the unconditional VPT agent and text-conditioned VPT agent (from Appendix I in [5]) across our programmatic tasks. We find that when given the relevant text instruction, STEVE-1 collects $75 \times$ more dirt, $4.9 \times$ more wood, $22 \times$ more seeds, and travels $4.3 \times$ farther than the unconditional agent, and STEVE-1 collects $3.3 \times$ more dirt, $4.4 \times$ more wood, $8.1 \times$ more seeds, and travels $2.2 \times$ farther than the textconditioned VPT agent. This represents a significant improvement over the reported performance of text-conditioned VPT, which collects several times fewer resources despite having twice as long of an episode to do so. We also run an automatic evaluation using MineCLIP embedding distances by measuring the minimum distance of a goal embedding to any frame in the episode. As shown in Figure 3b, the distance between the goal and the episode is significantly lower when the agent is conditioned on the corresponding visual goal than otherwise. Full results for STEVE-1 with both text and visual goals can be found in Appendix F.
In addition to our evaluations of STEVE-1, we also recorded several sample interactive sessions we had with the agent (controlling it in real-time by giving it written text instructions or specific visual goals). These sessions demonstrate STEVE-1's ability to responsively follow instructions in real-time in a variety of situations. We believe that such use-cases, where humans give an agent natural instructions that it can follow to complete tasks, will become increasingly important and have practical uses in the creation of instructable assistants and virtual-world characters. These videos, as well as videos of our agent performing our evaluation tasks, can be found at https://sites.google.com/view/steve-1.</p>
<h1>4.3 Prompt Chaining</h1>
<p>We also experiment with longer horizon tasks that require multiple steps, such as crafting and building. We explore two different prompting methods: directly prompting with the target goal, and a simple form of prompt chaining $[11,64,16]$ where the task is decomposed into several subtasks and the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Top left: By sequentially chaining visual prompts like "get dirt" and "build a tower", STEVE-1 successfully gathers dirt and then uses this dirt to build a tower. The prompts switch at the dotted vertical line. Bottom left: The success rates of the chained prompts improve steadily as we train STEVE-1 on more data. Right: The performance of STEVE-1 on different tasks scales in different ways when conditioning on a relevant visual prompt for the metric versus other irrelevant visual prompts (e.g., the break wood prompt is the relevant prompt for the "Wooden Logs Collected" metric, while the other prompts are "irrelevant"). For instance, in the wood-collection and dirtcollection tasks, performance starts increasing after training on 60M frames of gameplay. See Figure 14 for sample frames from each visual prompt.
prompts are given sequentially for a fixed number of steps. We explore prompt chaining with visual goals for two tasks: 1) building a tower and 2) making wooden planks. When using prompt chaining, we first prompt STEVE-1 to gather dirt before building a tower, and to gather wooden logs before crafting wooden planks. Figure 4 shows that directly prompting STEVE-1 with the final tasks results in near-zero success rates. However, prompt chaining allows STEVE-1 to build a tower $50 \%$ of the time and craft wooden planks $70 \%$ of the time. For the tower building task, STEVE-1 immediately starts collecting dirt until the prompt switches, at which point its average height starts increasing rapidly and its dirt decreases as it builds a tower. Similarly, for the crafting wooden planks task, STEVE-1 immediately starts collecting a large amount of wooden logs until the prompt switches and it rapidly converts these wooden logs into wooden planks (causing the amount of wooden logs in its inventory to immediately decrease and the number of wooden planks to increase as it crafts more). Figure 4 visualizes the average item counts and agent height for the prompt chaining episodes. See Figure 18 and Figure 19 in the appendix for visualizations of specific prompt chaining episodes.</p>
<h1>4.4 Scaling</h1>
<p>Recent works in language modeling have found that scaling up pretraining FLOPs, by training on more data or by training a model with more parameters, can improve performance on downstream tasks [28, 57, 63]. In certain cases when measuring performance with metrics such as exact-match [52], performance improvement may appear to be "emergent" [63], appearing suddenly as the model is trained with more compute. Here, we aim to gain a basic understanding of how the performance of STEVE-1 on various tasks scales by training with more data (learning rate schedule is chosen appropriately).
To assess performance gain, we isolate the performance of the policy from the prior, measuring performance of the agent on programmatic tasks (travel distance, seeds, logs, dirt) with visual goals. Due to compute constraints, we chose to use the 2x VPT model, which has 248M parameters. We found that both seed collection and travel distance did not improve significantly past 20M frames. From inspecting gameplay, we suspect that travel distance is a relatively easy task since it is close to VPT's default behavior of running around and exploring. For seed collection, performance remains suboptimal, suggesting that further scaling may be beneficial. This hypothesis is supported by the observation that performance on log and dirt collection remained roughly level until 60M frames when it began to rapidly improve. Figure 4 shows the scaling curves for STEVE-1 on each programmatic task when conditioning on relevant vs. irrelevant visual prompts for that task. Since we do not observe regression on any tasks as we train the model with more compute, we expect the model to continue to perform better as we train larger models on larger datasets.</p>
<p>We also evaluated the scaling properties of STEVE-1 for our multi-step tasks with and without prompt chaining. Without prompt chaining, the tasks remain challenging for STEVE-1 throughout training. However, we note that after 60M frames, STEVE-1 sometimes gathers wooden logs and then builds a small tower when directly prompted to build a tower. This is likely because our visual prompt for tower building shows a video of a tower being built out of wooden logs. With prompt chaining, the performance of STEVE-1 steadily increases with more data. We conjecture that this is because the success of a chained prompt requires the success of each element in the chain. Since different abilities emerge at different scales, one would expect chained prompts to steadily get more reliable as these subgoals become more reliably completed. In the case of crafting wooden planks, we note that crafting is one such task that gets significantly more reliable as the agent is trained on more data. Figure 4 shows the scaling curves for STEVE-1 on the prompt chaining tasks.</p>
<p>In summary, we see evidence of tasks that do not require much data for STEVE-1 to learn, tasks that steadily get more reliable as the agent is trained longer, and tasks where capability suddenly spikes after the agent reaches some threshold. Put together, this suggests that further scaling would likely significantly improve the agent, although we leave the task of predicting exactly how much performance there is to gain to future studies.</p>
<h1>4.5 What Matters for Downstream Performance?</h1>
<p>Pretraining Baker et al. [5] finds that by pretraining a behavioral prior with imitation learning on internet-scale datasets for Minecraft, the learned policy can be effectively finetuned to accomplish tasks that are impossible without pretraining. In this section, we demonstrate that pretraining is also massively beneficial for instruction-tuning in Minecraft. We hypothesize that due to the strong performance of STEVE-1 and the relatively small amount of compute ( $\approx 1 \%$ additional compute) used for instruction finetuning, most of the capabilities of our agent come from the pretraining rather than the finetuning. To test this hypothesis, we finetune several varients of STEVE-1 from various pretrained weights: foundation-2x, bc-early-game-2x, rl-from-foundation-2x, and with randomly initialized weights. In this experiment, each model was finetuned on 100M frames.</p>
<p>Figure 5 shows the performance of these models on our programmatic tasks with visual goals. Note that while an agent trained on our dataset from scratch can accomplish basic tasks like dirt collection fairly well, it is unable to find and chop down trees, in contrast to the pretrained agents. This demonstrates that the abilities present in the agent due to pretraining are successfully transferred to the finetuned agent. Out of all the pretrained weights we tried, we noticed that rl-from-foundation-2x performed the best, having qualitatively better performance at tasks like crafting and chopping down trees. Indeed, Figure 5 shows that this model has strong performance, likely due to the massive amount of compute it was trained with during its RL training [5].</p>
<p>Classifier-Free Guidance Baker et al. [5] observed that when conditioning the agent on text, it tended to ignore its instruction and instead perform the prior behavior learned during pretraining. As discussed in Section 3.4, classifier-free guidance [23] gives a knob for trading off between goalconditioned and prior behaviors. Figure 5 shows the effect of this parameter $\lambda$ on the log and dirt collection tasks. The performance of the agent reaches its maximum around $\lambda=5.0$ to $\lambda=7.0$, after which it starts to drop off. These results demonstrate the importance of classifier-free guidance, which improves the performance of STEVE-1 by orders of magnitude.</p>
<p>Prompt Engineering Prompt engineering as a discipline has rapidly emerged over the last year due to the observation that the quality of the output of text-to-X models can dramatically change depending on the prompt [67]. For example, Table 5 in the appendix shows how a prompt for Stable Diffusion [50] might be written. By listing out the various attributes of the image such as visual medium, style, and the phrase "trending on ArtStation", the user is able to get a higher quality image [20, 36]. In this section, we explore how this same style of prompt engineering can improve the performance of STEVE-1. Figure 6 shows how a simple prompt of "get dirt" might be changed in order to more accurately specify the type of behavior that is desired. Just like in image generation models, the performance of STEVE-1 significantly improves by modifying the prompt in this fashion. By changing to more complicated prompts, STEVE-1 is able to collect $1.6 \times$ more wood, $2 \times$ more dirt, and $3.3 \times$ more seeds.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Left: We trained STEVE-1 on 100M frames starting from four different pretrained weights: random initialization (scratch), foundation-2x (fd), bc-early-game-2x (bc), and rl-from-foundation-2x (rl). The rl-from-foundation-2x agent is generally the most performant after fine-tuning. Using pretrained weights performs better than training from scratch, especially for more complicated tasks like collecting wood. Right: By using classifier-free guidance [23], STEVE-1 collects $7.5 \times$ more dirt and $15 \times$ more wood than when $\lambda=0$ (no guidance). See Figure 17 in the Appendix for similar results with other programmatic tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Dirt Collected</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">"break a flower"</td>
<td style="text-align: left;">$0.7(-0.2,1.6)$</td>
</tr>
<tr>
<td style="text-align: left;">"collect seeds"</td>
<td style="text-align: left;">$2.7(0.9,4.5)$</td>
</tr>
<tr>
<td style="text-align: left;">"dig as far as possible"</td>
<td style="text-align: left;">$3.9(2.8,5.0)$</td>
</tr>
<tr>
<td style="text-align: left;">"get dirt"</td>
<td style="text-align: left;">$9.2(5.7,12.7)$</td>
</tr>
<tr>
<td style="text-align: left;">"get dirt, dig hole, dig dirt, gather a ton of dirt, collect dirt"</td>
<td style="text-align: left;">$\mathbf{2 6 . 7}(\mathbf{1 9 . 9}, \mathbf{3 3 . 5})$</td>
</tr>
</tbody>
</table>
<p>Figure 6: Similar to text-to-image generation, switching to a longer, more specific prompt dramatically improves the performance of STEVE-1. Values in parentheses are $95 \%$ confidence intervals.</p>
<h1>5 Limitations and Conclusion</h1>
<p>In this paper, we present a methodology for creating instruction-following foundation models of behavior. Specifically, by leveraging two existing pretrained foundation models: a behavioral prior (VPT [5]) and a domain-specific CLIP model (MineCLIP [17]), we create a powerful Minecraft agent that can follow short-horizon open-ended text and visual instructions, all for only $\$ 60$ of compute. The resulting foundation model, STEVE-1, sets a new bar for open-ended instruction-following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming previous baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We note that generalist agents such as STEVE-1 can have potential negative effects on society. We include a thorough discussion of these issues in Appendix A.</p>
<p>STEVE-1 is a significant advancement in creating generative models of text-to-behavior, but it has several limitations, as described in Appendix B. First, STEVE-1 is mostly proficient at achieving short-horizon tasks while struggling with longer-horizon tasks. While prompt chaining is a promising approach for improving performance on complex tasks, more can be done in future work to improve performance. Another limitation we observe is that prompt engineering, as with other generative models, can be unintuitive and time-consuming. Future work should investigate improving the steerability of STEVE-1 through a better understanding of natural language prompts. Additionally, we note that evaluating and describing the capabilities of open-ended generalist agents is an open research problem itself since capability depends strongly on preconditions, prompt engineering, and our own ability to come up with varied and challenging tasks. Finally, since our approach is not specific to the Minecraft domain, we hope that the method used to create STEVE-1 can inspire future work in creating powerful generalist agents in other domains and environments.</p>
<h1>Acknowledgements</h1>
<p>All of the authors gratefully acknowledge funding for this research from the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Canada CIFAR AI Chairs Program (Vector Institute for Artificial Intelligence). SL is supported by a Vector Institute internship and by an NSERC Discovery Grant. KP is supported by an NSERC PGS-D award. HC is supported by an NSERC CGS-D award. JB acknowledges funding from the Canada CIFAR AI Chairs program, Fujitsu Japan, and an Amazon Research Award. In addition to NSERC and CIFAR (Vector Institute), SM acknowledges funding from Microsoft Research. We thank Silviu Pitis, Romi Lifshitz, Forest Yang, and Yongchao Zhou for their helpful comments; Alisa Wu and Ziming Chen for their contributions to the text-video pair dataset; and Finn Paster for the logo and graphic for the website. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute for Artificial Intelligence (www.vectorinstitute.ai/partners).</p>
<h2>References</h2>
<p>[1] Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse H. Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matthew Sharifi, Neil Zeghidour, and Christian Havnø Frank. Musiclm: Generating music from text. CoRR, abs/2301.11325, 2023. doi: 10.48550/arXiv.2301.11325. URL https://doi.org/10.48550/ arXiv.2301.11325.
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 23716-23736, 2022.
[3] Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5048-5058, 2017. URL https://proceedings.neurips.cc/ paper/2017/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html.
[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[5] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639-24654, 2022.
[6] David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? arXiv preprint arXiv:2206.01079, 2022.
[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>[9] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. arXiv preprint arXiv:2301.10034, 2023.
[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021.
[11] Harrison Chase. Langchain, 2022. URL https://github.com/hwchase17/langchain.
[12] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084-15097, 2021.
[13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
[14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.
[15] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2978-2988. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1285. URL https://doi.org/10.18653/v1/p19-1285.
[16] David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, and Charles Sutton. Language model cascades, 2022.
[17] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 18343-18362. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 74a67268c5cc5910f64938cac4526a90-Paper-Datasets_and_Benchmarks.pdf.
[18] Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Manon Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=rALA0Xo6yNJ.
[19] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019.</p>
<p>[20] Gustavosta. Stable-diffusion-prompts. Hugging Face, 2023. URL https://huggingface. co/datasets/Gustavosta/Stable-Diffusion-Prompts. Hugging Face Datasets.
[21] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770-778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.
[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. doi: 10.48550/arXiv.2207.12598. URL https://doi.org/10.48550/arXiv.2207.12598.
[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and HsuanTien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.
[25] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022.
[26] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial intelligence experimentation. In Ijcai, pages 4246-4247, 2016.
[27] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. CoRR, abs/2305.02463, 2023. doi: 10.48550/arXiv.2305.02463. URL https://doi.org/10.48550/ arXiv.2305.02463.
[28] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.
[29] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6114.
[30] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline q-learning on diverse multi-task data both scales and generalizes. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? $\mathrm{id}=4-\mathrm{k} 7 \mathrm{kUavAj}$.
[31] Lauro Langosco Di Langosco, Jack Koch, Lee D Sharkey, Jacob Pfau, and David Krueger. Goal misgeneralization in deep reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 12004-12019. PMLR, 17-23 Jul 2022. URL https: //proceedings.mlr.press/v162/langosco22a.html.
[32] Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. Advances in Neural Information Processing Systems, 35:27921-27936, 2022.
[33] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.</p>
<p>[34] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. CoRR, abs/2211.09110, 2022. doi: 10.48550/ARXIV.2211.09110. URL https://doi.org/10. 48550/arXiv.2211.09110.
[35] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.
[36] Vivian Liu and Lydia B. Chilton. Design guidelines for prompt engineering text-to-image generative models. In Simone D. J. Barbosa, Cliff Lampe, Caroline Appert, David A. Shamma, Steven Mark Drucker, Julie R. Williamson, and Koji Yatani, editors, CHI '22: CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, pages 384:1-384:23. ACM, 2022. doi: 10.1145/3491102.3501825. URL https://doi.org/ $10.1145 / 3491102.3501825$.
[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
[38] Federico Malato, Florian Leopold, Amogh Raut, Ville Hautamäki, and Andrew Melnik. Behavioral cloning via search in video pretraining latent space. arXiv preprint arXiv:2212.13326, 2022.
[39] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.
[40] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. arXiv preprint arXiv:2301.12050, 2023.
[41] OpenAI. Introducing ChatGPT, Nov 2022. URL https://openai.com/blog/chatgpt.
[42] Keiran Paster, Sheila A McIlraith, and Jimmy Ba. Planning from pixels using inverse dynamics models. arXiv preprint arXiv:2012.02419, 2020.
[43] Keiran Paster, Sheila A. McIlraith, and Jimmy Ba. You can't count on luck: Why decision transformers and rvs fail in stochastic environments. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=atb3yifRtX.
[44] Keiran Paster, Silviu Pitis, Sheila A. McIlraith, and Jimmy Ba. Return augmentation gives supervised RL temporal compositionality. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. URL https://openreview.net/forum?id=q5olkWCt7nl.
[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8024-8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html.</p>
<p>[46] PrismarineJS and Others. Mineflayer. https://github.com/PrismarineJS/mineflayer, 2023. URL https://github.com/PrismarineJS/mineflayer. GitHub repository.
[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.
[48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
[49] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.
[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022.
[51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ ec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html.
[52] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? CoRR, abs/2304.15004, 2023. doi: 10.48550/arXiv.2304.15004. URL https://doi.org/10.48550/arXiv.2304.15004.
[53] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pages 894-906. PMLR, 2022.
[54] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3483-3491, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/ 8d55a249e6baa5c06772297520da2051-Abstract.html.
[55] Miroslav Strupl, Francesco Faccio, Dylan R. Ashley, Jürgen Schmidhuber, and Rupesh Kumar Srivastava. Upside-down reinforcement learning can diverge in stochastic environments with episodic resets. CoRR, abs/2205.06595, 2022. doi: 10.48550/arXiv.2205.06595. URL https: //doi.org/10.48550/arXiv.2205.06595.
[56] Theodore Sumers, Kenneth Marino, Arun Ahuja, Rob Fergus, and Ishita Dasgupta. Distilling internet-scale vision-language models into embodied agents. arXiv preprint arXiv:2301.12507, 2023.
[57] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. CoRR, abs/2210.09261, 2022. doi: 10.48550/arXiv.2210.09261. URL https://doi.org/10.48550/arXiv.2210.09261.
[58] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.</p>
<p>[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
[61] Ryan Volum, Sudha Rao, Michael Xu, Gabriel DesGarennes, Chris Brockett, Benjamin Van Durme, Olivia Deng, Akanksha Malhotra, and Bill Dolan. Craft an iron sword: Dynamically generating interactive game characters by prompting large language models tuned on code. In Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022), pages 25-43, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.wordplay-1.3. URL https://aclanthology.org/2022. wordplay-1.3.
[62] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.
[63] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. CoRR, abs/2206.07682, 2022. doi: 10.48550/arXiv.2206.07682. URL https://doi . org/10.48550/arXiv.2206.07682.
[64] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J.
[65] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, and Jonathan Tompson. Robotic skill acquisition via instruction augmentation with vision-language models. arXiv preprint arXiv:2211.11736, 2022.
[66] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
[67] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=92gvk82DE-.</p>
<h1>A Broader Impact</h1>
<p>With the increasing capability level of artificial intelligence comes many potential benefits and also risks. On the positive side, we anticipate that the techniques that used to create STEVE-1 could be applied to the creation of helpful agents in other sequential decision making domains, including robotics, video games, and the web. Our demonstration of such a low cost approach to creating a powerful, instruction-following model also has the potential to improve the democratization of AI. However, on the negative side, agents pretrained on large internet datasets reflect the biases of the internet and, as suggested by our experiments, these pretraining biases can potentially remain after instruction-tuning. If not addressed carefully, this could lead to devastating consequences for society. We hope that while the stakes are low, works such as ours can improve access to safety research on instruction-following models in sequential decision-making domains.</p>
<h2>B Limitations and Future Work</h2>
<h2>B. 1 Goal Misgeneralization</h2>
<p>One of the most common mistakes that STEVE-1 makes during evaluation is to overgeneralize. For instance, if we prompt STEVE-1 with a video of someone punching a cow, it may simply run to the nearest animal and punch that animal instead. This is related to the concept of goal misgeneralization [31]. Generalization can be helpful when the task we assign the agent is impossible to achieve from the current state and the agent instead performs a closely related action, but harmful when the task is achievable. We note two things: first, we believe the powerful generalization ability of STEVE-1 probably comes from the MineCLIP embeddings and it especially improves the ability of STEVE-1 to follow visual instructions when the exact items or blocks nearby are not available in the current environment, which is an extremely common scenario. Second, we notice that the tendency of the agent to misgeneralize decreases with scale. For example, with a model trained on less data, we find that asking the agent to look up and punch a tree to get a wooden log often resulted in the agent looking in the air and punching nothing; training the model on more data results in the agent first walking over to a nearby tree and looking up to get a wooden log. Future work should look to measure more closely the relationship between misgeneralization and scale.</p>
<h2>B. 2 Random Selection of Hindsight Goals</h2>
<p>In this work, we choose to randomly select future episode timesteps as hindsight goals, rather than use a more sophisticated strategy. This is primarily due to the simplicity of the approach, but also to ensure a diverse and unbiased coverage of potential goals achievable within the short horizon. Future works can investigate the effects of alternative approaches that filter for semantically interesting timesteps as goals.</p>
<h2>B. 3 Difference in Performance Between Text and Visual Goals</h2>
<p>In our experiments, we observed that STEVE-1 often performed better when conditioned with visual goals compared to text goals converted through the prior. There are several potential factors that could contribute to this performance gap between text and visual conditioning. First, the prior model may not be accurately capturing the full meaning of the text prompt. Training the prior on more data or using a more powerful model architecture could potentially improve the quality of the sampled latent goals. Second, the visual goals can provide more precise demonstrations of the desired behavior. Text goals are inherently more ambiguous. Providing additional information such as the observation context to the prior and further prompt-engineering to make text prompts more detailed and less ambiguous, could help close this gap. Because text conditioning provides more flexibility and potential for generalization, closing the performance gap between text and visual conditioning is an important direction for future work.</p>
<h2>B. 4 Challenges in Evaluating STEVE-1</h2>
<p>See Table 4 for a non-exhaustive list of tasks that the STEVE-1 agent is able to achieve. It is worth mentioning that evaluating and describing the capabilities of open-ended generalist agents is an open</p>
<p>research problem itself since capability depends strongly on preconditions, prompt engineering, and our own ability to come up with varied and challenging tasks. For example, there are many recent works on the evaluation of LLMs (e.g., [34, 33, 66]) which highlight these challenges.</p>
<p>That being said, there are a number of tasks which STEVE-1 is unable to accomplish. As previously mentioned, long-horizon tasks such as obtaining a diamond or building a house are currently beyond the capability of STEVE-1. Further, STEVE-1 also struggles with more complex crafting tasks like crafting an enchanting table, bookcase, or boat. Again, the virtually limitless and open-ended nature of tasks in Minecraft makes it very difficult to test generalist agents in this domain. We hope future works develop more sophisticated methods to evaluate the performance of generalist agents on short and long-horizon tasks (potentially through an extension of our MineCLIP evaluation method).</p>
<p>It is also worth noting that it is currently not possible to test STEVE-1 or VPT [5] in the MineDojo [17] environment, which is meant for generalist agent evaluation, since the action spaces are not compatible. We believe that bridging this gap could be greatly beneficial to the generalist agent community and we hope future works investigate this further.</p>
<h1>B. 5 Towards Improved Long-Horizon Performance</h1>
<p>STEVE-1 is a significant advancement in creating generative models of text-to-behavior, but it has several limitations. First, STEVE-1 is mostly proficient at achieving short-horizon tasks while struggling on longer-horizon tasks like obtaining a diamond. Solving long-horizon tasks while taking actions using low-level mouse/keyboard controls is a very challenging and exciting research direction and, while prompt chaining is a promising approach for improving performance on complex tasks, more can be done in future work to improve performance.</p>
<p>One potential bottleneck is the fact that during packed hindsight relabeling, the hindsight goals are limited to at most 200 timesteps in the future ( $\sim 10$ seconds). Thus, tasks which require more than 200 timesteps to complete are technically out-of-distribution for STEVE-1. Although sampling hindsight goals from farther into the future could theoretically enhance long-horizon performance, our experiments in Appendix C. 4 indicate that the performance tends to decrease if we increase this hyperparameter too much. We suspect that while increasing this hyperparameter may be able to improve long-horizon performance, it also increases noise and comes at the cost of reducing performance on short-horizon goals. Investigating whether it is possible to achieve a better tradeoff is an important avenue for future work. We also suspect that the long-horizon capabilities of STEVE-1 could be improved through scaling or finetuning with reinforcement learning, or leveraging LLMs or VLMs to automatically provide prompt chains to the STEVE-1 agent.</p>
<h2>B. 6 Applying the STEVE-1 Approach to Other Domains</h2>
<p>We designed STEVE-1 for Minecraft due to the availability of two key ingredients: (1) a strong behavioral prior (VPT [5]), and (2) a powerful visual-language model which maps text and video to a joint embedding space (MineCLIP [17]). However, the method used to create STEVE-1 is not specific to the Minecraft domain. Given the rapid development of generative models, we expect that similar models to VPT and MineCLIP will become available in many other domains. As these models become available, future work could investigate the applicability of the STEVE-1 approach to these other domains.</p>
<h2>C Additional Ablations</h2>
<p>In this section, we describe additional ablations on design choices for our method, including the use of classifier-free guidance during training, text augmentation strategies, different VAE variants, varying chunk sizes during finetuning, and more. We use programmatic evaluation metrics to compare the performance of the various ablations.</p>
<h2>C. 1 Classifier-Free Guidance During Training</h2>
<p>We examine the importance of using classifier-free guidance during training by finetuning a model with no guidance which does not drop out the goal embedding $z_{\tau_{\text {goal }}}$ from the policy's input (i.e., $p_{\text {uncond }}=0.0$ ) and comparing it to the version which uses guidance ( $p_{\text {uncond }}=0.1$ ). The chunk</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Ablation on Guidance. In the "no guidance" variant, we set $p_{\text {uncond }}=0$, meaning that we do not drop any $z_{\tau_{\text {good }}}$ from the policy's input during training. The "guidance" variants set $p_{\text {uncond }}=0.1$, dropping $10 \%$ of the time during training. Whereas the "no guidance" model is only compatible with $\lambda=0$ at inference, the "guidance" model can use $\lambda&gt;0$, allowing for better performance.
size is set to the range 15 to 50 and we train each policy for 100M frames. In Figure 7, we compare the performance of using visual goals (MineCLIP video embedding) on the no guidance model using conditional scale $\lambda=0$ and the guidance model using conditional scales $\lambda=0$ and $\lambda=3$. We observe that while the no guidance model slightly outperforms the guidance model at $\lambda=0$ across a few metrics, the agent with guidance outperforms the no guidance agent by a factor of 2 to 3 times for the inventory collection tasks when we increase the conditional scale to $\lambda=3$ (which we cannot do for the no guidance model). For the travel distance metric, both of the guidance versions perform similarly to the no guidance version.</p>
<h1>C. 2 Text Augmentation</h1>
<p>During finetuning, instead of using only self-supervision with future MineCLIP video embedding as the goal, we considered using the text embeddings from the 2,000 human labeled trajectory segments as goal embeddings, either solely or in addition to the self-supervised video embeddings. In order to more fairly compare with the CVAE prior approach, we augment the human-labeled data with additional text-gameplay pairs generated as described in Appendix E.2. We implement this experiment by replacing the visual embeddings used for relabeling in Algorithm 1 with text embeddings, when available, with a $90 \%$ probability. To experiment with not using visual embeddings at all, we can replace the visual embeddings with zeros in the same way. In Figure 8, we observe that using only the visual embeddings during training, in combination with the CVAE, can outperform using MineCLIP text embeddings directly in the other two baselines. In this experiment, the chunk size is set to the range 15 to 50 and we train each policy for 100M frames.</p>
<h2>C. 3 VAE Variants</h2>
<p>We study the dataset used to train the CVAE prior model. In Figure 9, we observe that augmentation helps in some programmatic tasks, including the dirt and seed collection tasks, but slightly hurts the wooden log collection and travel distance metrics. In this experiment, we use the same policy with each CVAE variant and we tune the conditional scale $\lambda$ for each variant. The chunk size is set to the range 15 to 200 and we train the policy for 100M frames.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Ablation on Text Augmentation. In the "text (raw text)" ablation, we train the model using only the text labels from human labelled trajectory segments, and directly use the MineCLIP text embedding of the text label as the goal embedding during training and at inference. For the "text + visual (raw text)" ablation, we use both the visual embedding in self-supervised manner and the text embedding from the human labelled trajectory segments during training and use the MineCLIP text embedding during inference. Even with augmentation, the dataset only contained around $2 \%$ text embeddings. The "visual (text VAE)" version is as reported in the main method, using the CVAE to convert MineCLIP text embedding to visual embedding during inference.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Ablation on VAE Training Data. "Human" baseline uses only the 2,000 human-labelled trajectory segments (text-video pairs), as training example for the CVAE prior model. "Human + Aug" baseline adds additional pairs of text-video examples as described in Section 3.3.</p>
<h1>C. 4 Chunk Size</h1>
<p>During finetuning, we compare different goal chunk sizes by varying the max_btwn_goals $=[100,200,300,400]$, while keeping the min_btwn_goals=15. See Algorithm 1 for more details. A larger max_btwn_goals introduces more noise, with actions that led to achieving the further away goal being less correlated to the actions present in that goal chunk. In Figure 10, we observe that the best max_btwn_goals chunk size is around 200, and increasing the chunk size beyond that causes a drop in performance. We train each policy for 160 M frames and tune the conditional scale for each.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Ablation on Segment Chunk Size. We vary the max_btwn_goals parameter in Algorithm 1. The performance is roughly the best at around 200, beginning to decline with greater values.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Even without training the prior on the concept of dirt or digging at all, STEVE-1 can still be instructed to dig holes and get dirt. This demonstrates that STEVE-1 can generalize to unseen text instructions.</p>
<h1>C. 5 Generalization to Novel Text Instructions</h1>
<p>We train the prior on a dataset of human and GPT-generated instructions designed to be representative of the tasks that appear in our gameplay dataset. Here, we have performed a set of simple generalization experiments to measure the degree to which the prior can generalize to unseen instructions.</p>
<p>Instruction Training Set Contamination The "Text Prompt" column in Table 3 shows which of the prompts used in evaluation show up in our training dataset. Among our evaluation instructions, the bolded and italicized instructions show up in the instruction-trajectory dataset. While some instructions do show up, most of the instructions do not show up in our training set (verbatim).</p>
<p>Training Set Decontamination To measure the effect on performance of removing a concept from the training set, we ran an experiment where we removed every instruction with the words "dirt" or "dig" in them and retrained the VAE model. This corresponds to around $10 \%$ of the instructions. As shown in Figure 11, we found that even without training on the concept of dirt or digging at all, STEVE-1 can still be instructed to dig holes and get dirt. This demonstrates clearly that STEVE-1 can generalize to unseen text instructions - likely because most of the text-understanding comes from the pretrained MineCLIP model which was trained on a highly diverse dataset of YouTube videos and captions. The prior VAE only needs to learn a mapping between the text and visual MineCLIP embeddings. Note that there is a slight decrease in performance across all tasks likely due to the smaller VAE training set ( $\sim 10 \%$ less).</p>
<p>The instruction-following capability of STEVE-1 is shared between: the policy, which learns to follow instructions in the visual MineCLIP embedding space; the MineCLIP text-encoder, which is trained to align well with the visual embeddings and performs most of the text-understanding; and our prior VAE model, which learns a simple function to translate between text and visual embeddings. This modeling setup lets us fully exploit pretrained models such as MineCLIP to gain impressive language understanding without relying on having our own large datasets or compute.</p>
<h2>D Dataset Details</h2>
<h2>D. 1 Gameplay Dataset</h2>
<p>Our gameplay dataset consists of two types of episodes: 7,854 episodes ( 38.94 M frames) of a contractor dataset made available from Baker et al. [5] and 2,267 episodes ( 14.96 M frames) of gameplay generated by running various pretrained VPT agents.</p>
<p>OpenAI Contractor Dataset The majority of our data comes from the contractor data used to train VPT [5]. OpenAI released five subsets of contractor data: $6 . x, 7 . x, 8 . x, 9 . x$, and $10 . x$. We use an equal</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/openai/Video-Pre-Training&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
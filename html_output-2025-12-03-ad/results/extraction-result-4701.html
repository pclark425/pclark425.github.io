<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4701 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4701</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4701</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-c4fd9c86b2b41df51a6fe212406dda81b1997fd4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c4fd9c86b2b41df51a6fe212406dda81b1997fd4" target="_blank">Linguistic Regularities in Continuous Space Word Representations</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The vector-space word representations that are implicitly learned by the input-layer weights are found to be surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4701.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4701.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vector-offset method</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector offset method for solving analogies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple method that treats linguistic/semantic relations as approximately constant vector offsets in continuous word embedding space and answers analogies by computing y = x_b - x_a + x_c and returning the nearest word vector to y by cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNNLM embeddings (used with vector-offset method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Continuous-space word embeddings (columns of matrix U) learned by training a recurrent neural network language model (RNNLM) with sigmoid hidden units and softmax output on large text corpora (320M Broadcast News words in experiments); embeddings of dimensionalities 80, 320, 640 and composite 1600 were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Analogy arithmetic implemented as vector addition/subtraction (x_b - x_a + x_c), i.e., single-step linear combination of word vectors to answer 'a is to b as c is to __'.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Relations correspond to (approximately) constant vector offsets in embedding space so relational reasoning reduces to linear vector arithmetic (subtract one vector, add another) followed by nearest-neighbor retrieval under cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Qualitative examples (e.g. 'king - man + woman' yields a vector very close to 'queen'), visualization of consistent offsets for gender and singular/plural relations (Figure 2), and quantitative success on analogy tasks: syntactic analogy dataset and SemEval-2012 relation-similarity task where the offset method outperforms several baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Authors note the offset is approximate (no word may exist at the exact computed position), requiring nearest-neighbor search; performance is imperfect (not all analogy types are solved), and some methods (LSA, CW vectors) perform much worse, indicating the offset property is not universal across all embedding methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Syntactic analogies: paper claims 'almost 40% correct' (abstract/conclusion). Reported per-category results for RNN-80 (Table 2): Adjectives 10.1%, Nouns 8.1%, Verbs 30.4%, All 19.0% (note: different embedding sizes yield different results). Semantic relation similarity (SemEval-2012) averaged over 69 test relations (Table 4): RNN-80 Spearman ρ=0.211 / MaxDiff Acc.=0.389; RNN-320 ρ=0.259 / 0.408; RNN-640 ρ=0.270 / 0.416; composite RNN-1600 ρ=0.275 / 0.418 (best reported).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No targeted probing or causal intervention experiments are reported for the arithmetic mechanism; evidence is based on geometric visualization, example analogies, and task performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Offsets are approximate (computed vector may not correspond exactly to any word), so nearest-neighbor retrieval is required; success varies strongly by relation type and embedding method; some classes (e.g., many adjectives/nouns) have low accuracy; method depends on embedding quality and dimensionality; paper does not show compositional multi-step arithmetic beyond single subtraction/addition patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared embeddings/method across methods: LSA-640 performed poorly (semantic Spearman ρ=0.149 / MaxDiff 0.364). Collobert & Weston (CW) vectors (Turian) performed poorly on syntactic analogies; Hierarchical Log-Bilinear (HLBL) vectors performed comparably on syntactic tasks (e.g., HLBL-100 All ≈ 18.7% vs RNN-80 All 19.0%), but RNN variants (higher dimensionality) gave best semantic relation scores and outperformed prior systems (e.g., UTD-NB had ρ=0.230 / MaxDiff=0.395).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linguistic Regularities in Continuous Space Word Representations', 'publication_date_yy_mm': '2013-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4701.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4701.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNNLM embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word embeddings learned by a Recurrent Neural Network Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Word vectors are learned as columns of the input-weight matrix U of an RNN language model trained by backpropagation to maximize data likelihood; these embeddings capture syntactic and semantic regularities that manifest as linear offsets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent neural network based language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent Neural Network Language Model (RNNLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An RNN with input 1-of-N word coding, sigmoid hidden units with recurrent weights W, input-to-hidden weights U (columns are word vectors), and output softmax weights V. Trained on large corpora (320M words of Broadcast News in experiments) with vocabulary ~82k; vector dimensionalities explored: 80, 320, 640 and composite 1600.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Analogy-style arithmetic realized as linear vector operations on learned word embeddings (single subtraction and addition: x_b - x_a + x_c) with retrieval by cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Unsupervised maximum-likelihood training of the RNNLM induces distributed representations where linguistic relations correspond to consistent vector offsets; the RNNLM training objective and recurrent sentence-history modeling produce embeddings whose geometry supports linear relational structure.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Demonstrated learned offsets on multiple relation types (syntactic: pluralization, tense, comparative/superlative; semantic: class-inclusion etc.), visualization (Figure 2) and quantitative results: RNN-based vectors outperform LSA and CW on syntactic and semantic tasks and match/beat HLBL on several metrics; higher-dimensional RNN embeddings improve semantic task scores (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct internal analysis of RNN dynamics or hidden-state circuits is provided; the paper acknowledges the model has 'no knowledge of syntax or morphology', implying the mechanism is emergent and approximate; some embedding methods (CW, LSA) do not show the same offset behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>See Vector-offset method performance metrics. Additional syntactic table (Table 3) comparisons: RNN-80 All = 19.0% correct (Adjectives 10.1%, Nouns 8.1%, Verbs 30.4%). Semantic: RNN-640 Spearman ρ=0.270 / MaxDiff=0.416; RNN-1600 ρ=0.275 / MaxDiff=0.418.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No probing/ablation/intervention experiments on embeddings or RNN components are reported; analyses are observational (task performance, visualization) rather than causal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Embeddings only approximately realize relations; performance depends on embedding dimensionality and training data size; low accuracy on many syntactic categories; no analysis of failure examples per relation beyond aggregate metrics; nearest-neighbor retrieval can return incorrect words when the computed vector is distant from any actual word vector.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Direct comparisons in the paper: LSA (latent semantic analysis) trained on same data performed worse on syntactic/semantic tasks. Collobert & Weston (CW) embeddings performed poorly on syntactic analogies; HLBL (Mnih & Hinton) vectors performed comparably on syntactic tasks and somewhat worse on semantic tasks given the data and dimensionalities evaluated. RNN embeddings (with larger dimensionality) gave the best semantic relation similarity results and outperformed previous top system UTD-NB on SemEval-2012 task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linguistic Regularities in Continuous Space Word Representations', 'publication_date_yy_mm': '2013-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A neural probabilistic language model <em>(Rating: 2)</em></li>
                <li>Recurrent neural network based language model <em>(Rating: 2)</em></li>
                <li>A scalable hierarchical distributed language model <em>(Rating: 2)</em></li>
                <li>A unified architecture for natural language processing: Deep neural networks with multitask learning <em>(Rating: 2)</em></li>
                <li>Word representations: a simple and general method for semisupervised learning <em>(Rating: 2)</em></li>
                <li>Domain and function: A dual-space model of semantic relations and compositions <em>(Rating: 1)</em></li>
                <li>UTD: Determining relational similarity using lexical patterns <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4701",
    "paper_id": "paper-c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "Vector-offset method",
            "name_full": "Vector offset method for solving analogies",
            "brief_description": "A simple method that treats linguistic/semantic relations as approximately constant vector offsets in continuous word embedding space and answers analogies by computing y = x_b - x_a + x_c and returning the nearest word vector to y by cosine similarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RNNLM embeddings (used with vector-offset method)",
            "model_description": "Continuous-space word embeddings (columns of matrix U) learned by training a recurrent neural network language model (RNNLM) with sigmoid hidden units and softmax output on large text corpora (320M Broadcast News words in experiments); embeddings of dimensionalities 80, 320, 640 and composite 1600 were evaluated.",
            "arithmetic_task_type": "Analogy arithmetic implemented as vector addition/subtraction (x_b - x_a + x_c), i.e., single-step linear combination of word vectors to answer 'a is to b as c is to __'.",
            "mechanism_hypothesis": "Relations correspond to (approximately) constant vector offsets in embedding space so relational reasoning reduces to linear vector arithmetic (subtract one vector, add another) followed by nearest-neighbor retrieval under cosine similarity.",
            "evidence_for_mechanism": "Qualitative examples (e.g. 'king - man + woman' yields a vector very close to 'queen'), visualization of consistent offsets for gender and singular/plural relations (Figure 2), and quantitative success on analogy tasks: syntactic analogy dataset and SemEval-2012 relation-similarity task where the offset method outperforms several baselines.",
            "evidence_against_mechanism": "Authors note the offset is approximate (no word may exist at the exact computed position), requiring nearest-neighbor search; performance is imperfect (not all analogy types are solved), and some methods (LSA, CW vectors) perform much worse, indicating the offset property is not universal across all embedding methods.",
            "performance_metrics": "Syntactic analogies: paper claims 'almost 40% correct' (abstract/conclusion). Reported per-category results for RNN-80 (Table 2): Adjectives 10.1%, Nouns 8.1%, Verbs 30.4%, All 19.0% (note: different embedding sizes yield different results). Semantic relation similarity (SemEval-2012) averaged over 69 test relations (Table 4): RNN-80 Spearman ρ=0.211 / MaxDiff Acc.=0.389; RNN-320 ρ=0.259 / 0.408; RNN-640 ρ=0.270 / 0.416; composite RNN-1600 ρ=0.275 / 0.418 (best reported).",
            "probing_or_intervention_results": "No targeted probing or causal intervention experiments are reported for the arithmetic mechanism; evidence is based on geometric visualization, example analogies, and task performance comparisons.",
            "limitations_and_failure_modes": "Offsets are approximate (computed vector may not correspond exactly to any word), so nearest-neighbor retrieval is required; success varies strongly by relation type and embedding method; some classes (e.g., many adjectives/nouns) have low accuracy; method depends on embedding quality and dimensionality; paper does not show compositional multi-step arithmetic beyond single subtraction/addition patterns.",
            "comparison_to_other_models": "Compared embeddings/method across methods: LSA-640 performed poorly (semantic Spearman ρ=0.149 / MaxDiff 0.364). Collobert & Weston (CW) vectors (Turian) performed poorly on syntactic analogies; Hierarchical Log-Bilinear (HLBL) vectors performed comparably on syntactic tasks (e.g., HLBL-100 All ≈ 18.7% vs RNN-80 All 19.0%), but RNN variants (higher dimensionality) gave best semantic relation scores and outperformed prior systems (e.g., UTD-NB had ρ=0.230 / MaxDiff=0.395).",
            "uuid": "e4701.0",
            "source_info": {
                "paper_title": "Linguistic Regularities in Continuous Space Word Representations",
                "publication_date_yy_mm": "2013-05"
            }
        },
        {
            "name_short": "RNNLM embeddings",
            "name_full": "Word embeddings learned by a Recurrent Neural Network Language Model",
            "brief_description": "Word vectors are learned as columns of the input-weight matrix U of an RNN language model trained by backpropagation to maximize data likelihood; these embeddings capture syntactic and semantic regularities that manifest as linear offsets.",
            "citation_title": "Recurrent neural network based language model",
            "mention_or_use": "use",
            "model_name": "Recurrent Neural Network Language Model (RNNLM)",
            "model_description": "An RNN with input 1-of-N word coding, sigmoid hidden units with recurrent weights W, input-to-hidden weights U (columns are word vectors), and output softmax weights V. Trained on large corpora (320M words of Broadcast News in experiments) with vocabulary ~82k; vector dimensionalities explored: 80, 320, 640 and composite 1600.",
            "arithmetic_task_type": "Analogy-style arithmetic realized as linear vector operations on learned word embeddings (single subtraction and addition: x_b - x_a + x_c) with retrieval by cosine similarity.",
            "mechanism_hypothesis": "Unsupervised maximum-likelihood training of the RNNLM induces distributed representations where linguistic relations correspond to consistent vector offsets; the RNNLM training objective and recurrent sentence-history modeling produce embeddings whose geometry supports linear relational structure.",
            "evidence_for_mechanism": "Demonstrated learned offsets on multiple relation types (syntactic: pluralization, tense, comparative/superlative; semantic: class-inclusion etc.), visualization (Figure 2) and quantitative results: RNN-based vectors outperform LSA and CW on syntactic and semantic tasks and match/beat HLBL on several metrics; higher-dimensional RNN embeddings improve semantic task scores (Table 4).",
            "evidence_against_mechanism": "No direct internal analysis of RNN dynamics or hidden-state circuits is provided; the paper acknowledges the model has 'no knowledge of syntax or morphology', implying the mechanism is emergent and approximate; some embedding methods (CW, LSA) do not show the same offset behavior.",
            "performance_metrics": "See Vector-offset method performance metrics. Additional syntactic table (Table 3) comparisons: RNN-80 All = 19.0% correct (Adjectives 10.1%, Nouns 8.1%, Verbs 30.4%). Semantic: RNN-640 Spearman ρ=0.270 / MaxDiff=0.416; RNN-1600 ρ=0.275 / MaxDiff=0.418.",
            "probing_or_intervention_results": "No probing/ablation/intervention experiments on embeddings or RNN components are reported; analyses are observational (task performance, visualization) rather than causal.",
            "limitations_and_failure_modes": "Embeddings only approximately realize relations; performance depends on embedding dimensionality and training data size; low accuracy on many syntactic categories; no analysis of failure examples per relation beyond aggregate metrics; nearest-neighbor retrieval can return incorrect words when the computed vector is distant from any actual word vector.",
            "comparison_to_other_models": "Direct comparisons in the paper: LSA (latent semantic analysis) trained on same data performed worse on syntactic/semantic tasks. Collobert & Weston (CW) embeddings performed poorly on syntactic analogies; HLBL (Mnih & Hinton) vectors performed comparably on syntactic tasks and somewhat worse on semantic tasks given the data and dimensionalities evaluated. RNN embeddings (with larger dimensionality) gave the best semantic relation similarity results and outperformed previous top system UTD-NB on SemEval-2012 task.",
            "uuid": "e4701.1",
            "source_info": {
                "paper_title": "Linguistic Regularities in Continuous Space Word Representations",
                "publication_date_yy_mm": "2013-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A neural probabilistic language model",
            "rating": 2
        },
        {
            "paper_title": "Recurrent neural network based language model",
            "rating": 2
        },
        {
            "paper_title": "A scalable hierarchical distributed language model",
            "rating": 2
        },
        {
            "paper_title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
            "rating": 2
        },
        {
            "paper_title": "Word representations: a simple and general method for semisupervised learning",
            "rating": 2
        },
        {
            "paper_title": "Domain and function: A dual-space model of semantic relations and compositions",
            "rating": 1
        },
        {
            "paper_title": "UTD: Determining relational similarity using lexical patterns",
            "rating": 1
        }
    ],
    "cost": 0.00951,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Linguistic Regularities in Continuous Space Word Representations</h1>
<p>Tomas Mikolov*, Wen-tau Yih, Geoffrey Zweig<br>Microsoft Research<br>Redmond, WA 98052</p>
<h4>Abstract</h4>
<p>Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, "King Man + Woman" results in a vector very close to "Queen." We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost $40 \%$ of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.</p>
<h2>1 Introduction</h2>
<p>A defining feature of neural network language models is their representation of words as high dimensional real valued vectors. In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>inputs to a neural network. As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical $n$-gram language models; whereas a $n$-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors. Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences.</p>
<p>By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which may be used for other, potentially unrelated, tasks. This has been used to good effect, for example in (Collobert and Weston, 2008; Turian et al., 2010) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks.</p>
<p>In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way. Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship. For example, if we denote the vector for word $i$ as $x_{i}$, and focus on the singular/plural relation, we observe that $x_{\text {apple }}-x_{\text {apples }} \approx$ $x_{\text {car }}-x_{\text {cars }}, x_{\text {family }}-x_{\text {families }} \approx x_{\text {car }}-x_{\text {cars }}$, and so on. Perhaps more surprisingly, we find that this is also the case for a variety of semantic relations, as measured by the SemEval 2012 task of measuring relation similarity.</p>
<p>The remainder of this paper is organized as follows. In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7.</p>
<h2>2 Related Work</h2>
<p>Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990). More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the "next" word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations.</p>
<h2>3 Recurrent Neural Network Model</h2>
<p>The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1. This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding weight matrices. The input vector $\mathbf{w}(t)$ represents input word at time $t$ encoded using 1-of-N coding, and the output layer $\mathbf{y}(t)$ produces a probability distribution over words. The hidden layer $\mathbf{s}(t)$ maintains a representation of the sentence history. The input vector $\mathbf{w}(t)$ and the output vector $\mathbf{y}(t)$ have dimensionality of the vocabulary. The values in the hidden and
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Recurrent Neural Network Language Model.
output layers are computed as follows:</p>
<p>$$
\begin{gathered}
\mathbf{s}(t)=f(\mathbf{U} \mathbf{w}(t)+\mathbf{W} \mathbf{s}(t-1)) \
\mathbf{y}(t)=g(\mathbf{V} \mathbf{s}(t))
\end{gathered}
$$</p>
<p>where</p>
<p>$$
f(z)=\frac{1}{1+e^{-z}}, \quad g\left(z_{m}\right)=\frac{e^{z_{m}}}{\sum_{k} e^{z_{k}}}
$$</p>
<p>In this framework, the word representations are found in the columns of $\mathbf{U}$, with each column representing a word. The RNN is trained with backpropagation to maximize the data log-likelihood under the model. The model itself has no knowledge of syntax or morphology or semantics. Remarkably, training such a purely lexical model to maximize likelihood will induce word representations with striking syntactic and semantic properties.</p>
<h2>4 Measuring Linguistic Regularity</h2>
<h3>4.1 A Syntactic Test Set</h3>
<p>To understand better the syntactic regularities which are inherent in the learned representation, we created a test set of analogy questions of the form " $a$ is to $b$ as $c$ is to __" testing base/comparative/superlative forms of adjectives; singular/plural forms of common nouns; possessive/non-possessive forms of common nouns; and base, past and 3rd person present tense forms of verbs. More precisely, we tagged 267M words of newspaper text with Penn</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">Patterns Tested</th>
<th style="text-align: center;"># Questions</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Adjectives</td>
<td style="text-align: center;">Base/Comparative</td>
<td style="text-align: center;">JJ/JJR, JJR/JJ</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">good:better rough:___</td>
</tr>
<tr>
<td style="text-align: center;">Adjectives</td>
<td style="text-align: center;">Base/Superlative</td>
<td style="text-align: center;">JJ/JJS, JJS/JJ</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">good:best rough:___</td>
</tr>
<tr>
<td style="text-align: center;">Adjectives</td>
<td style="text-align: center;">Comparative/ <br> Superlative</td>
<td style="text-align: center;">JJS/JJR, JJR/JJS</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">better:best rougher:___</td>
</tr>
<tr>
<td style="text-align: center;">Nouns</td>
<td style="text-align: center;">Singular/Plural</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { NN/NNS, } \ &amp; \text { NNS/NN } \end{aligned}$</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">year:years law:___</td>
</tr>
<tr>
<td style="text-align: center;">Nouns</td>
<td style="text-align: center;">Non-possessive/ <br> Possessive</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { NN/NN_POS, } \ &amp; \text { NN_POS/NN } \end{aligned}$</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">city:city's bank:___</td>
</tr>
<tr>
<td style="text-align: center;">Verbs</td>
<td style="text-align: center;">Base/Past</td>
<td style="text-align: center;">VB/VBD, <br> VBD/VB</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">see:saw return:___</td>
</tr>
<tr>
<td style="text-align: center;">Verbs</td>
<td style="text-align: center;">Base/3rd Person <br> Singular Present</td>
<td style="text-align: center;">VB/VBZ, VBZ/VB</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">see:sees return:___</td>
</tr>
<tr>
<td style="text-align: center;">Verbs</td>
<td style="text-align: center;">Past/3rd Person <br> Singular Present</td>
<td style="text-align: center;">VBD/VBZ, <br> VBZ/VBD</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">saw:sees returned:___</td>
</tr>
</tbody>
</table>
<p>Table 1: Test set patterns. For a given pattern and word-pair, both orderings occur in the test set. For example, if "see:saw return:<strong>" occurs, so will "saw:see returned:</strong>".</p>
<p>Treebank POS tags (Marcus et al., 1993). We then selected 100 of the most frequent comparative adjectives (words labeled JJR); 100 of the most frequent plural nouns (NNS); 100 of the most frequent possessive nouns (NN_POS); and 100 of the most frequent base form verbs (VB). We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words from the same category, and creating variants as indicated in Table 1. The total test set size is 8000 . The test set is available online. ${ }^{1}$</p>
<h3>4.2 A Semantic Test Set</h3>
<p>In addition to syntactic analogy questions, we used the SemEval-2012 Task 2, Measuring Relation Similarity (Jurgens et al., 2012), to estimate the extent to which RNNLM word vectors contain semantic information. The dataset contains 79 fine-grained word relations, where 10 are used for training and 69 testing. Each relation is exemplified by 3 or 4 gold word pairs. Given a group of word pairs that supposedly have the same relation, the task is to order the target pairs according to the degree to which this relation holds. This can be viewed as another analogy problem. For example, take the ClassInclusion:Singular_Collective relation with the pro-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>totypical word pair clothing:shirt. To measure the degree that a target word pair dish:bowl has the same relation, we form the analogy "clothing is to shirt as dish is to bowl," and ask how valid it is.</p>
<h2>5 The Vector Offset Method</h2>
<p>As we have seen, both the syntactic and semantic tasks have been formulated as analogy questions. We have found that a simple vector offset method based on cosine distance is remarkably effective in solving these questions. In this method, we assume relationships are present as vector offsets, so that in the embedding space, all pairs of words sharing a particular relation are related by the same constant offset. This is illustrated in Figure 2.</p>
<p>In this model, to answer the analogy question $a: b$ $c: d$ where $d$ is unknown, we find the embedding vectors $x_{a}, x_{b}, x_{c}$ (all normalized to unit norm), and compute $y=x_{b}-x_{a}+x_{c} . y$ is the continuous space representation of the word we expect to be the best answer. Of course, no word might exist at that exact position, so we then search for the word whose embedding vector has the greatest cosine similarity to $y$ and output it:</p>
<p>$$
w^{*}=\arg \max <em w="w">{w} \frac{x</em>
$$} y}{\left|x_{w}\right||y|</p>
<p>When $d$ is given, as in our semantic test set, we simply use $\cos \left(x_{b}-x_{a}+x_{c}, x_{d}\right)$ for the words</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Left panel shows vector offsets for three word pairs illustrating the gender relation. Right panel shows a different projection, and the singular/plural relation for two words. In high-dimensional space, multiple relations can be embedded for a single word.
provided. We have explored several related methods and found that the proposed method performs well for both syntactic and semantic relations. We note that this measure is qualitatively similar to relational similarity model of (Turney, 2012), which predicts similarity between members of the word pairs $\left(x_{b}, x_{d}\right),\left(x_{c}, x_{d}\right)$ and dis-similarity for $\left(x_{a}, x_{d}\right)$.</p>
<h2>6 Experimental Results</h2>
<p>To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012). Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600. The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82 k vocabulary. Table 2 shows results for both RNNLM and LSA vectors on the syntactic task. LSA was trained on the same data as the RNN. We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly. ${ }^{2}$</p>
<p>In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online ${ }^{3}$ Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those. This resulted in a 36 k word vocabulary, and a test set with 6632</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Results for identifying syntactic regularities for different word representations. Percent correct.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Adjectives</th>
<th style="text-align: left;">Nouns</th>
<th style="text-align: left;">Verbs</th>
<th style="text-align: left;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RNN-80</td>
<td style="text-align: left;">$\mathbf{1 0 . 1}$</td>
<td style="text-align: left;">8.1</td>
<td style="text-align: left;">$\mathbf{3 0 . 4}$</td>
<td style="text-align: left;">$\mathbf{1 9 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">CW-50</td>
<td style="text-align: left;">1.1</td>
<td style="text-align: left;">2.4</td>
<td style="text-align: left;">8.1</td>
<td style="text-align: left;">4.5</td>
</tr>
<tr>
<td style="text-align: left;">CW-100</td>
<td style="text-align: left;">1.3</td>
<td style="text-align: left;">4.1</td>
<td style="text-align: left;">8.6</td>
<td style="text-align: left;">5.0</td>
</tr>
<tr>
<td style="text-align: left;">HLBL-50</td>
<td style="text-align: left;">4.4</td>
<td style="text-align: left;">5.4</td>
<td style="text-align: left;">23.1</td>
<td style="text-align: left;">13.0</td>
</tr>
<tr>
<td style="text-align: left;">HLBL-100</td>
<td style="text-align: left;">7.6</td>
<td style="text-align: left;">$\mathbf{1 3 . 2}$</td>
<td style="text-align: left;">30.2</td>
<td style="text-align: left;">18.7</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of RNN vectors with Turian's Collobert and Weston based vectors and the Hierarchical Log-Bilinear model of Mnih and Hinton. Percent correct.
questions. Turian's Collobert and Weston based vectors do poorly on this task, whereas the Hierarchical Log-Bilinear Model vectors of (Mnih and Hinton, 2009) do essentially as well as the RNN vectors. These representations were trained on 37M words of data and this may indicate a greater robustness of the HLBL method.</p>
<p>We conducted similar experiments with the semantic test set. For each target word pair in a relation category, the model measures its relational similarity to each of the prototypical word pairs, and then uses the average as the final score. The results are evaluated using the two standard metrics defined in the task, Spearman's rank correlation coefficient $\rho$ and MaxDiff accuracy. In both cases, larger values are better. To compare to previous systems, we report the average over all 69 relations in the test set.</p>
<p>From Table 4, we see that as with the syntactic regularity study, the RNN-based representations perform best. In this case, however, Turian's CW vectors are comparable in performance to the HLBL vectors. With the RNN vectors, the performance improves as the number of dimensions increases. Surprisingly, we found that even though the RNN vec-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Spearman's $\rho$</th>
<th style="text-align: center;">MaxDiff Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LSA-640</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.364</td>
</tr>
<tr>
<td style="text-align: left;">RNN-80</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.389</td>
</tr>
<tr>
<td style="text-align: left;">RNN-320</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.408</td>
</tr>
<tr>
<td style="text-align: left;">RNN-640</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.416</td>
</tr>
<tr>
<td style="text-align: left;">RNN-1600</td>
<td style="text-align: center;">$\mathbf{0 . 2 7 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 8}$</td>
</tr>
<tr>
<td style="text-align: left;">CW-50</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.363</td>
</tr>
<tr>
<td style="text-align: left;">CW-100</td>
<td style="text-align: center;">0.154</td>
<td style="text-align: center;">0.363</td>
</tr>
<tr>
<td style="text-align: left;">HLBL-50</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.363</td>
</tr>
<tr>
<td style="text-align: left;">HLBL-100</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.362</td>
</tr>
<tr>
<td style="text-align: left;">UTD-NB</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.395</td>
</tr>
</tbody>
</table>
<p>Table 4: Results in measuring relation similarity
tors are not trained or tuned specifically for this task, the model achieves better results (RNN-320, RNN$640 \&amp;$ RNN-1600) than the previously best performing system, UTD-NB (Rink and Harabagiu, 2012).</p>
<h2>7 Conclusion</h2>
<p>We have presented a generally applicable vector offset method for identifying linguistic regularities in continuous space word representations. We have shown that the word representations learned by a RNNLM do an especially good job in capturing these regularities. We present a new dataset for measuring syntactic performance, and achieve almost $40 \%$ correct. We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art. Surprisingly, both results are the byproducts of an unsupervised maximum likelihood training criterion that simply operates on a large amount of text data.</p>
<h2>References</h2>
<p>Y. Bengio, R. Ducharme, Vincent, P., and C. Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Reseach, 3(6).
Y. Bengio, H. Schwenk, J.S. Senécal, F. Morin, and J.L. Gauvain. 2006. Neural probabilistic language models. Innovations in Machine Learning, pages 137-186.
A. Bordes, X. Glorot, J. Weston, and Y. Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In Proceedings of 15th International Conference on Artificial Intelligence and Statistics.
R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160-167. ACM.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(96).
J.L. Elman. 1991. Distributed representations, simple recurrent networks, and grammatical structure. Machine learning, 7(2):195-225.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313(5786):504-507.
G. Hinton and R. Salakhutdinov. 2010. Discovering binary codes for documents by learning deep generative models. Topics in Cognitive Science, 3(1):74-91.
G.E. Hinton. 1986. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, pages 1-12. Amherst, MA.
David Jurgens, Saif Mohammad, Peter Turney, and Keith Holyoak. 2012. Semeval-2012 task 2: Measuring degrees of relational similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval 2012), pages 356-364. Association for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and F. Yvon. 2011. Structured output layer neural network language model. In Proceedings of ICASSP 2011.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313-330.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of Interspeech 2010.</p>
<p>Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget, and Jan Cernocky. 2011a. Strategies for Training Large Scale Neural Network Language Models. In Proceedings of ASRU 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2011b. Extensions of recurrent neural network based language model. In Proceedings of ICASSP 2011.
Tomas Mikolov. 2012. RNN toolkit.
A. Mnih and G.E. Hinton. 2009. A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081-1088.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the</p>
<p>international workshop on artificial intelligence and statistics, pages 246-252.
J.B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46(1):77-105.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Determining relational similarity using lexical patterns. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval 2012), pages 413-418. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492 -518 .
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semisupervised learning. In Proceedings of Association for Computational Linguistics (ACL 2010).
P.D. Turney. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533-585.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Guessing gets a small fraction of a percent.
${ }^{3} \mathrm{http}: / /$ metaoptimize.com/projects/wordreprs/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
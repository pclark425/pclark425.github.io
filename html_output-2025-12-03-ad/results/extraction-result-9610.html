<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9610 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9610</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9610</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13" target="_blank">KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience, demonstrating how thoughtfully designed AI tools can expand accessibility and accelerate knowledge building across diverse research domains.</p>
                <p><strong>Paper Abstract:</strong> Academic researchers face challenges keeping up with exponentially growing published findings in their field. Performing comprehensive literature reviews to synthesize knowledge is time-consuming and labor-intensive using manual approaches. Recent advances in artificial intelligence provide promising solutions, yet many require coding expertise, limiting accessibility. KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience. By leveraging KNIME's intuitive graphical interface, researchers can create workflows to search their Zotero libraries and utilize OpenAI models to extract key information without coding. Users simply provide API keys and configure settings through a user-friendly interface in a locally stored copy of the workflow. KNIMEZoBot then allows asking natural language questions via a chatbot and retrieves relevant passages from papers to generate synthesized answers. This system has significant potential to expedite literature reviews for researchers unfamiliar with coding by automating retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot demonstrates how thoughtfully designed AI tools can expand accessibility and accelerate knowledge building across diverse research domains.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9610.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9610.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KNIMEZoBot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KNIMEZoBot (KNIME + Zotero + OpenAI integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-free workflow that integrates Zotero, KNIME, and OpenAI LLMs to perform retrieval-augmented literature search and synthesize natural-language answers from PDFs stored in a user's Zotero library.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-turbo (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT family accessed via the OpenAI API through KNIME nodes; the paper uses these pretrained LLMs as black-box generators to synthesize answers from retrieved document fragments. No architectural or training details of the models are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>User-selected Zotero libraries or group libraries containing PDFs of academic publications; users may filter by collection. The corpus consists of full-text PDFs loaded from Zotero, split into chunks via LangChain's PDF loader. No domain restriction given beyond 'academic publications.'</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Natural-language literature-review style queries posed by a user (via a chatbot interface) aimed at synthesizing information across the documents in their Zotero library or selected collections.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) pipeline implemented in KNIME: (1) fetch PDFs from Zotero via Zotero REST API using Python nodes; (2) load full text using LangChain's unstructuredPDFLoader and split into chunks with configurable chunk size and overlap; (3) compute embeddings for chunks using OpenAI text-embedding-ada-002; (4) store vectors in a FAISS vector store; (5) on query, compute query vector, perform vector-similarity retrieval of top fragments; (6) pass retrieved fragments plus the user query to an OpenAI GPT model with a system prompt instructing the model to answer only from provided information (and to reply with "I apologize, but I do not have any information about it in my Zotero library." if not present); (7) return synthesized natural-language answer in a chat UI.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthesized natural-language answers to user queries (chatbot), plus the ability to download the conversation history as CSV; also top-k retrieved text fragments used as grounding/context.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>"I apologize, but I do not have any information about it in my Zotero library." (system-prompted fallback). Otherwise, the system returns concise synthesized answers combining salient facts and main points from retrieved document chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Democratizes RAG-based literature synthesis for non-coders via KNIME's GUI; mitigates context-window limits by chunking and retrieval; integrates Zotero for user-controlled, curated corpora; configurable chunk size/overlap; uses FAISS and established embedding model for scalable semantic search.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No formal evaluation of output accuracy reported; reliance on external OpenAI API and embedding model; potential for hallucination remains (authors note hallucination risk); corpus size and domain-specific performance unreported; system requires local KNIME/Python setup and API keys which may be a barrier for some users.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit empirical failure cases reported in the paper; authors note general risks (LLM hallucination, context-window limits) and state that further enhancements to accuracy are desirable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9610.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9610.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM generation by retrieving semantically relevant text fragments from a vector-indexed corpus and supplying them as context to an LLM to produce grounded answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG (retrieval + LLM; implemented with OpenAI GPT models in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RAG is a pipeline combining vector-based retrieval (embeddings + FAISS) with a generative LLM; in this paper RAG is operationalized with text chunking, OpenAI embeddings, FAISS retrieval, and GPT models for generation. The paper does not provide model training or architecture details beyond describing this standard RAG workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Segmented text chunks created from full-text PDFs obtained from Zotero libraries, with chunk size and overlap configurable by the user.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>User-specified natural-language queries intended to synthesize knowledge across multiple documents in a curated Zotero corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Text is split into overlapping chunks, embedded, stored in a vector DB (FAISS); user queries are embedded and nearest-neighbor retrieval returns relevant chunks; these retrieved chunks are provided to an LLM which composes an answer—i.e., the typical RAG loop used to synthesize multi-document responses.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded answers (narrative synthesis) and retrieved passages used as provenance/context for the LLM response.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Top-k retrieved text fragments are concatenated with the user query and provided to GPT, which returns a synthesized answer summarizing salient points across the fragments (no concrete example outputs are provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Addresses LLM context-window limits by retrieving only relevant fragments; can reduce hallucination by grounding generation in retrieved text; enables iterative Q&A for deep literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on chunking strategy, embedding model, and retrieval hyperparameters; retrieval errors (irrelevant or low-quality fragments) can still lead to incorrect or hallucinated answers; paper provides no quantitative evaluation of retrieval+generation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not empirically detailed in this paper; authors note the general possibility that retrieval failures or imperfect grounding may cause inaccurate LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9610.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9610.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embeddings+FAISS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-embedding-ada-002 embeddings with FAISS vector store</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pipeline component that converts text chunks into dense numeric vectors using OpenAI's text-embedding-ada-002 and stores them in a FAISS index to enable semantic nearest-neighbor retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-embedding-ada-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An OpenAI embedding model used to transform textual chunks into vector representations; the paper does not provide internal model architecture or training dataset details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Text chunks derived from full-text PDFs (split with LangChain/unstructuredPDFLoader) originating from Zotero libraries; chunk size and overlap are user-configurable.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Embeddings are used to represent both query text and document chunks for semantic similarity matching to retrieve context relevant to user queries.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Compute embeddings for each chunk using text-embedding-ada-002, store vectors in FAISS, run vector similarity search (nearest neighbors) for a query embedding, and return top-matching chunks to the LLM for synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Ranked lists of relevant text chunks (vectors -> nearest-neighbor results) which serve as the context for downstream LLM-generated summaries/syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A set of top-k document chunks (text excerpts) retrieved by FAISS in response to a user query, which are then forwarded to GPT for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Enables scalable semantic search over long document collections and helps overcome LLM context-window limits by selecting relevant context; FAISS provides efficient nearest-neighbor retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Embedding model choice and vector indexing hyperparameters govern retrieval quality; the paper does not report tuning, quantitative retrieval metrics, or ablations; potential semantic mismatch between embeddings and LLM generation remains unquantified.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure cases reported; potential failures include retrieving semantically irrelevant passages that mislead the LLM and degrade answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>Benchmarking Large Language Models in RetrievalAugmented Generation <em>(Rating: 2)</em></li>
                <li>Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation <em>(Rating: 2)</em></li>
                <li>Artificial intelligence and the conduct of literature reviews <em>(Rating: 2)</em></li>
                <li>ChatGPT and artificial hallucinations in stem cell research: assessing the accuracy of generated references - a preliminary study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9610",
    "paper_id": "paper-3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "KNIMEZoBot",
            "name_full": "KNIMEZoBot (KNIME + Zotero + OpenAI integration)",
            "brief_description": "A code-free workflow that integrates Zotero, KNIME, and OpenAI LLMs to perform retrieval-augmented literature search and synthesize natural-language answers from PDFs stored in a user's Zotero library.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-turbo (OpenAI API)",
            "model_description": "OpenAI's GPT family accessed via the OpenAI API through KNIME nodes; the paper uses these pretrained LLMs as black-box generators to synthesize answers from retrieved document fragments. No architectural or training details of the models are provided in the paper.",
            "model_size": null,
            "input_corpus_description": "User-selected Zotero libraries or group libraries containing PDFs of academic publications; users may filter by collection. The corpus consists of full-text PDFs loaded from Zotero, split into chunks via LangChain's PDF loader. No domain restriction given beyond 'academic publications.'",
            "input_corpus_size": null,
            "topic_query_description": "Natural-language literature-review style queries posed by a user (via a chatbot interface) aimed at synthesizing information across the documents in their Zotero library or selected collections.",
            "distillation_method": "Retrieval-Augmented Generation (RAG) pipeline implemented in KNIME: (1) fetch PDFs from Zotero via Zotero REST API using Python nodes; (2) load full text using LangChain's unstructuredPDFLoader and split into chunks with configurable chunk size and overlap; (3) compute embeddings for chunks using OpenAI text-embedding-ada-002; (4) store vectors in a FAISS vector store; (5) on query, compute query vector, perform vector-similarity retrieval of top fragments; (6) pass retrieved fragments plus the user query to an OpenAI GPT model with a system prompt instructing the model to answer only from provided information (and to reply with \"I apologize, but I do not have any information about it in my Zotero library.\" if not present); (7) return synthesized natural-language answer in a chat UI.",
            "output_type": "Synthesized natural-language answers to user queries (chatbot), plus the ability to download the conversation history as CSV; also top-k retrieved text fragments used as grounding/context.",
            "output_example": "\"I apologize, but I do not have any information about it in my Zotero library.\" (system-prompted fallback). Otherwise, the system returns concise synthesized answers combining salient facts and main points from retrieved document chunks.",
            "evaluation_method": null,
            "evaluation_results": null,
            "strengths": "Democratizes RAG-based literature synthesis for non-coders via KNIME's GUI; mitigates context-window limits by chunking and retrieval; integrates Zotero for user-controlled, curated corpora; configurable chunk size/overlap; uses FAISS and established embedding model for scalable semantic search.",
            "limitations": "No formal evaluation of output accuracy reported; reliance on external OpenAI API and embedding model; potential for hallucination remains (authors note hallucination risk); corpus size and domain-specific performance unreported; system requires local KNIME/Python setup and API keys which may be a barrier for some users.",
            "failure_cases": "No explicit empirical failure cases reported in the paper; authors note general risks (LLM hallucination, context-window limits) and state that further enhancements to accuracy are desirable.",
            "uuid": "e9610.0",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A method that augments LLM generation by retrieving semantically relevant text fragments from a vector-indexed corpus and supplying them as context to an LLM to produce grounded answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RAG (retrieval + LLM; implemented with OpenAI GPT models in this work)",
            "model_description": "RAG is a pipeline combining vector-based retrieval (embeddings + FAISS) with a generative LLM; in this paper RAG is operationalized with text chunking, OpenAI embeddings, FAISS retrieval, and GPT models for generation. The paper does not provide model training or architecture details beyond describing this standard RAG workflow.",
            "model_size": null,
            "input_corpus_description": "Segmented text chunks created from full-text PDFs obtained from Zotero libraries, with chunk size and overlap configurable by the user.",
            "input_corpus_size": null,
            "topic_query_description": "User-specified natural-language queries intended to synthesize knowledge across multiple documents in a curated Zotero corpus.",
            "distillation_method": "Text is split into overlapping chunks, embedded, stored in a vector DB (FAISS); user queries are embedded and nearest-neighbor retrieval returns relevant chunks; these retrieved chunks are provided to an LLM which composes an answer—i.e., the typical RAG loop used to synthesize multi-document responses.",
            "output_type": "Grounded answers (narrative synthesis) and retrieved passages used as provenance/context for the LLM response.",
            "output_example": "Top-k retrieved text fragments are concatenated with the user query and provided to GPT, which returns a synthesized answer summarizing salient points across the fragments (no concrete example outputs are provided in the paper).",
            "evaluation_method": null,
            "evaluation_results": null,
            "strengths": "Addresses LLM context-window limits by retrieving only relevant fragments; can reduce hallucination by grounding generation in retrieved text; enables iterative Q&A for deep literature review.",
            "limitations": "Quality depends on chunking strategy, embedding model, and retrieval hyperparameters; retrieval errors (irrelevant or low-quality fragments) can still lead to incorrect or hallucinated answers; paper provides no quantitative evaluation of retrieval+generation fidelity.",
            "failure_cases": "Not empirically detailed in this paper; authors note the general possibility that retrieval failures or imperfect grounding may cause inaccurate LLM outputs.",
            "uuid": "e9610.1",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Embeddings+FAISS",
            "name_full": "text-embedding-ada-002 embeddings with FAISS vector store",
            "brief_description": "Pipeline component that converts text chunks into dense numeric vectors using OpenAI's text-embedding-ada-002 and stores them in a FAISS index to enable semantic nearest-neighbor retrieval.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-embedding-ada-002",
            "model_description": "An OpenAI embedding model used to transform textual chunks into vector representations; the paper does not provide internal model architecture or training dataset details.",
            "model_size": null,
            "input_corpus_description": "Text chunks derived from full-text PDFs (split with LangChain/unstructuredPDFLoader) originating from Zotero libraries; chunk size and overlap are user-configurable.",
            "input_corpus_size": null,
            "topic_query_description": "Embeddings are used to represent both query text and document chunks for semantic similarity matching to retrieve context relevant to user queries.",
            "distillation_method": "Compute embeddings for each chunk using text-embedding-ada-002, store vectors in FAISS, run vector similarity search (nearest neighbors) for a query embedding, and return top-matching chunks to the LLM for synthesis.",
            "output_type": "Ranked lists of relevant text chunks (vectors -&gt; nearest-neighbor results) which serve as the context for downstream LLM-generated summaries/syntheses.",
            "output_example": "A set of top-k document chunks (text excerpts) retrieved by FAISS in response to a user query, which are then forwarded to GPT for answer generation.",
            "evaluation_method": null,
            "evaluation_results": null,
            "strengths": "Enables scalable semantic search over long document collections and helps overcome LLM context-window limits by selecting relevant context; FAISS provides efficient nearest-neighbor retrieval.",
            "limitations": "Embedding model choice and vector indexing hyperparameters govern retrieval quality; the paper does not report tuning, quantitative retrieval metrics, or ablations; potential semantic mismatch between embeddings and LLM generation remains unquantified.",
            "failure_cases": "No specific failure cases reported; potential failures include retrieving semantically irrelevant passages that mislead the LLM and degrade answer correctness.",
            "uuid": "e9610.2",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Benchmarking Large Language Models in RetrievalAugmented Generation",
            "rating": 2,
            "sanitized_title": "benchmarking_large_language_models_in_retrievalaugmented_generation"
        },
        {
            "paper_title": "Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation",
            "rating": 2,
            "sanitized_title": "streamlining_systematic_reviews_harnessing_large_language_models_for_quality_assessment_and_riskofbias_evaluation"
        },
        {
            "paper_title": "Artificial intelligence and the conduct of literature reviews",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_and_the_conduct_of_literature_reviews"
        },
        {
            "paper_title": "ChatGPT and artificial hallucinations in stem cell research: assessing the accuracy of generated references - a preliminary study",
            "rating": 1,
            "sanitized_title": "chatgpt_and_artificial_hallucinations_in_stem_cell_research_assessing_the_accuracy_of_generated_references_a_preliminary_study"
        }
    ],
    "cost": 0.0120975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation.</h1>
<p>Suad Alshammari ${ }^{1,2}$, Lama Basalelah ${ }^{1,3}$, Walaa Abu Rukbah ${ }^{1,4}$, Ali Alsuhibani ${ }^{1,5}$ and Dayanjan S. Wijesinghe ${ }^{1,6,7}$.</p>
<ol>
<li>Department of Pharmacotherapy and Outcomes Sciences, School of Pharmacy, Virginia Commonwealth University. 2. Faculty of Pharmacy, Northern Border University, Saudi Arabia. 3. Faculty of Pharmacy, Imam Abdulrahman Bin Faisal University, Saudi Arabia. 4. Faculty of Pharmacy, University of Tabuk, Saudi Arabia. 5. Department of Pharmacy Practice, Unaizah College of Pharmacy, Qassim University, Unaizah, Saudi Arabia. 6. Institute for Structural Biology, Drug Discovery and Development, Virginia Commonwealth University, Richmond, Virginia, USA. 7. Da Vinci Center, School of Pharmacy, Virginia Commonwealth University School of Medicine, Richmond, Virginia, USA.</li>
</ol>
<p>Project files to be found at: https://github.com/dayanjan-lab/KNIMEZoBot</p>
<h4>Abstract</h4>
<p>Academic researchers face challenges keeping up with exponentially growing published findings in their field. Performing comprehensive literature reviews to synthesize knowledge is time-consuming and labor-intensive using manual approaches. Recent advances in artificial intelligence provide promising solutions, yet many require coding expertise, limiting accessibility. KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience. By leveraging KNIME's intuitive graphical interface, researchers can create workflows to search their Zotero libraries and utilize OpenAI models to extract key information without coding. Users simply provide API keys and configure settings through a user-friendly interface in a locally stored copy of the workflow. KNIMEZoBot then allows asking natural language questions via a chatbot and retrieves relevant passages from papers to generate synthesized answers. This system has significant potential to expedite literature reviews for researchers unfamiliar with coding by automating retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot demonstrates how thoughtfully designed Al tools can expand accessibility and accelerate knowledge building across diverse research domains.</p>
<h1>Introduction:</h1>
<p>The current era witnesses academicians, clinicians, and researchers grappling with the formidable challenge of information overload ${ }^{1,2}$. The incessant surge in published research findings over recent years has significantly outpaced the ability to stay updated. This challenge is poised to intensify with the advent of natural language optimized Al technologies, which are expected to propel the pace of discoveries and subsequent publications at an even faster rate ${ }^{3}$. The dire need for a mechanism to proficiently manage and assimilate this burgeoning knowledge is palpable. Within this complex quandary, two distinct scenarios emerge:</p>
<p>Firstly, the task of extracting precise answers from a pre-existing knowledge corpus poses a hurdle ${ }^{1}$. Researchers typically amass publications pertinent to their expertise in reference libraries. This textual corpus expands over time with the continual influx of new findings. The task of extracting specific information from this meticulously curated content escalates in complexity with the growing volume of publications, compelling users to sift through numerous documents. Consequently, the appeal for an Al-driven platform capable of synthesizing information from multiple different publications to precise queries from an ever-expanding corpus of curated scientific literature within personal and group reference libraries is burgeoning.</p>
<p>Secondly, the endeavor of encapsulating knowledge through exhaustive literature reviews unveils knowledge gaps and unveils avenues for consequential research ${ }^{4}$. This endeavor entails the conduct of meticulous literature reviews which begin with the identification and collation of relevant publications to address the posed inquiries. Post collection, a thorough analysis of the amassed information is essential to derive answers to specific queries. The conventional workflow of executing literature reviews is notably time-consuming and labor-intensive ${ }^{5}$. Given the swift pace of new discoveries, the traditional approach often yields a knowledge summary that becomes obsolete by the time of its completion.</p>
<p>The imperativeness for simplified, automated strategies enabling researchers to query curated literature libraries and routinely refresh their domain knowledge is apparent. The recent strides in artificial intelligence (AI), particularly the Large Language Models (LLMs), harbor the potential to alleviate the aforementioned challenges ${ }^{6}$. Platforms like ChatGPT, Claude, or Bard are proficient in summarizing papers and synthesizing findings across multiple documents ${ }^{7}$. However, their native "chat" formats present certain impediments for academic research. These include constraint of context window lengths ${ }^{8}$ and the propensity for confabulation (hallucination) ${ }^{9}$. For instance, publicly available chatGPT4 has a context window of about 5000 words, while Claude's window extends to 75,000 words, rendering a bulk of academic publications too lengthy for chat GPT. Claude, although capable of summarizing single publications, finds its utility curtailed across multiple documents.</p>
<p>A notable breakthrough addressing the context window limitations and hallucination and aiding in data summarization from diverse documents is the Retrieval Augmented Generation (RAG) approach ${ }^{10,11}$. The operational workflow of RAG commences with the segmentation of broad text corpora into smaller, overlapping textual fragments. Following this, these fragments</p>
<p>are transformed into vector representations and cataloged within a vector-based database. Upon query submission, it's converted into a vector form. A vectorial similarity assessment is then executed to identify all text segments within the database showcasing semantic alignment. The query, along with all relevant text fragments, is forwarded to a Large Language Model (LLM) to formulate a coherent and pertinent response. This technique adeptly navigates the context window constraints, fostering response synthesis across varied domains. The capability to repeatedly execute this process establishes a robust question-and-answer framework, invaluable for extensive literature reviews, serving scholars and medical professionals.</p>
<p>The RAG-based system, although expedient in summarizing information, hitherto necessitated substantial coding knowledge. Recognizing that a sizable faction of academicians and clinicians lacks coding expertise, we orchestrated a code-free, open-source strategy, culminating in the creation of KNIMEZoBot. This innovation amalgamates three pivotal elements: Konstanz Information Miner (KNIME) - a code-free data science platform, Zotero - an open-source reference management system, and GPT4 from Open AI - the chosen language model for knowledge synthesis. KNIMEZoBot heralds a revolutionary stride in enhancing the literature review workflow by seamlessly integrating the prowess of reference managers, scholarly databases, and AI. Through this ingenious approach, we have democratized access to AIpowered research tools, opening doors for those with non-coding backgrounds to harness natural language queries for interacting with the curated publications housed in their Zotero libraries, thereby significantly amplifying the accessibility and utility of Al in academic spheres.</p>
<h1>Materials and Methods</h1>
<p>KNIME: For the development of the KNIMEZoBot platform, KNIME played an integral role in providing the graphical modular interface for building the workflow steps, integrating the Zotero and OpenAI APIs seamlessly via dedicated nodes, processing the extracted text data, creating the FAISS vector indexing workflow, and hosting the final chatbot user interface. KNIME is an open-source platform originating from the University of Konstanz in Germany, catering to data analytics, reporting, and integration needs, with a strong footing in data science and machine learning domains ${ }^{12}$. It's a free, community-enhanced tool, widely embraced by data professionals globally owing to its user-friendly, graphical interface enabling code-free workflow creation, modification, and visualization. KNIME's core strength is its extensive node repository facilitating seamless data pipeline construction for tasks ranging from data preprocessing to advanced analytics using a non/low code approach. It boasts robust data integration, connecting effortlessly to various data sources like databases and web services, thus centralizing data for comprehensive analysis. Scalability is a hallmark of KNIME, adeptly managing small to large datasets, with ease of integration into big data frameworks like Apache Hadoop and Apache Spark. The platform supports building, training, and evaluating machine learning models utilizing popular libraries such as scikit-learn and TensorFlow, alongside offering an array of statistical and analytical techniques. Automation is seamless with KNIME, allowing scheduled workflow executions, while its server facilitates collaborative efforts and workflow sharing. Commercial versions of KNIME extend advanced features and support, enriching its open-source ecosystem. It's a versatile tool for creating insightful reports, visualizing data, and finds applications across</p>
<p>diverse fields including bioinformatics, predictive analytics, business intelligence, and industrial research.</p>
<h1>KNIME extensions used:</h1>
<p>KNIME AI Extension (Labs) ${ }^{13}$ : The KNIME labs extension enables users to leverage powerful large language models (LLMs) from OpenAI, Hugging Face Hub, and GPT4ALL for tasks like chat and text embeddings. It also provides connectivity to vector stores like Chroma and FAISS for building knowledge bases that can inform chatbot responses. The extension allows combining vector stores and LLMs into intelligent agents. These agents can dynamically select the most relevant vector store to query based on the user input, enabling more natural and knowledgeable conversations. Overall, this extension brings together state-of-the-art LLMs and vector stores within the KNIME analytics platform, unlocking new possibilities for building conversational interfaces and knowledge-powered AI assistants.</p>
<p>KNIME REST Client Extension ${ }^{14}$ : The KNIME REST Client Extension provides nodes for making REST API calls within KNIME workflows. This enables seamless integration with web services and APIs.</p>
<p>The Get Request node ${ }^{15}$ is used to send HTTP GET requests to REST endpoints. It allows specifying the URL, headers, query parameters, and authentication settings. The response from the REST API is returned as a JSON/XML document that can be further processed in the KNIME workflow ${ }^{14}$.</p>
<p>KNIME Python 2 Integration (legacy) ${ }^{16}$ : This extension encompasses the legacy version of KNIME Python integration. It facilitates the integration of Python 2 and Python 3 scripts within the KNIME platform. The extension operates by executing Python scripts in a local Python installation, which is not included in the extension package.</p>
<p>KNIME Python Integration ${ }^{17}$ : The "KNIME Python Integration" is the modern and preferred choice for Python integration within KNIME. This extension incorporates nodes that enable the execution of Python 3 scripts seamlessly in the KNIME workflow. Notably, this integration brings substantial performance improvements compared to its legacy counterpart. It also provides enhanced support for handling larger-than-memory datasets. Additionally, this extension comes equipped with a Python installation that includes a curated selection of essential Python packages.</p>
<p>Note: Throughout our workflow, we employed both the "KNIME Python 2 Integration (legacy)" and the "KNIME Python Integration" extensions interchangeably.</p>
<p>KNIME JSON-Processing ${ }^{18}$ : The KNIME JSON-Processing extension provides nodes for working with JSON data within KNIME workflows. It allows for parsing, creating, transforming, and serializing JSON documents. The JSON To Table node enables easy ingestion of JSON data into tabular form for use in KNIME workflows. It reduces the complexity of handling nested JSON structures and schemas.</p>
<p>Python (Version 3.9) ${ }^{19}$ : Integrating of Python and Anaconda with KNIME can be a powerful combination that allows data scientists and analysts to leverage the extensive libraries and capabilities of Python within the KNIME analytics platform. This integration provides a seamless way to utilize Python scripts, packages, and machine learning models within the KNIME workflows. A detailed guide for Python integration in KNIME has been published elsewhere ${ }^{20}$</p>
<p>Specifically, Python nodes were utilized to execute API calls to extract metadata and PDF files from the Zotero reference manager using its REST API bindings. The Python Requests library facilitated sending GET requests to the API and processing the responses. Another vital usage was the Langchain library ${ }^{21}$ within a Python node to load in the full text of PDF papers and segment them into smaller chunks that meet the length limits of the GPT model inputs.</p>
<p>The Python nodes accept inputs from earlier workflow components, run the defined Python logic and code using those inputs, and return any outputs to subsequent nodes in the workflow. For instance, a node might accept a list of extracted PDFs from the Zotero API calls, utilize Langchain to chunk each PDF into shorter text segments, and output these chunks to the next node for vectorization.</p>
<p>The following Python packages were installed and imported within the Python nodes in KNIME to support core functionality. The installation can be done in multiple ways; we used the following:</p>
<p>1- Open Anaconda Prompt from the Start menu.
2- Write this command: "conda activate <your_environment>." Your_environment is the environment name that is set up in the KNIME Python preference.
3- After the name is changed from base to the name of the environment, install the following libraries via pip:</p>
<ul>
<li>pip install pandas openai langchain unstructured fitz PyPDF2 PyMuPDF "unstructured[pdf]"</li>
</ul>
<h1>Zotero:</h1>
<p>Zotero, a free, open-source reference management software ${ }^{22}$, is cherished by a broad spectrum of academia and professionals for easing the collection, organization, and citation of research materials. Originating from George Mason University, it's a boon for scholarly research and writing, streamlining reference, citation, and bibliography management. Key facets include effortless reference collection from diverse sources like websites and academic journals, with automatic citation information extraction from web pages and PDFs. Its intuitive interface facilitates organizing references via folders, tags, and notes, ensuring easy retrieval. A hallmark feature is its citation and bibliography generation in numerous styles like APA and MLA, significantly reducing formatting time. Integration with prevalent word processors like Microsoft Word and Google Docs allows direct citation insertion and bibliography generation in documents, ensuring accuracy and consistency. Its PDF management capability lets users attach, organize, and annotate PDFs within the reference library. Zotero encourages collaborative research through shared library features, vital for research teams. It offers cloud synchronization for easy access across devices and data backup, enhancing data security. Browser extensions for Chrome and</p>
<p>Firefox simplify capturing references online. Being open-source, it's continually evolved by community contributions, and its cross-platform availability extends its reach. Applications are vast, aiding academic research, education, library assistance, and professionals across legal, medical, and media fields in managing and citing a vast array of references effortlessly.</p>
<h1>Results and Discussion</h1>
<h2>KNIMEZoBot:</h2>
<p>The developed application "KNIMEZoBot", represents an innovative integration of Zotero and OpenAI through the code free platform KNIME to streamline literature reviews and research. This project seamlessly combines the above-mentioned Zotero reference manager, with OpenAI's powerful natural language processing capabilities via a RAG based approach using KNIME as the interface. The primary goal is to simplify retrieving PDFs from Zotero libraries and collections and then utilize OpenAI within KNIME workflows to ask insightful questions and extract key information from academic papers.</p>
<p>KNIMEZoBot uses a Retrieval-Augmented Generation (RAG) architecture, first conducting a semantic search to identify relevant passages from retrieved PDFs. It then leverages large language models (in this case OpenAI's GPT models) to synthesize natural language answers based on the extracted information. This enables KNIMEZoBot to provide informative responses to questions by efficiently searching academic papers and distilling salient facts and main points. Overall, the integration of Zotero and OpenAI represents an innovative approach to enhance literature reviews and research by combining reference management, scholarly databases, and OpenAI.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure1: Underlying overall workflow for KNIMEZoBot.</p>
<h2>First component (Setup and configure Zotero):</h2>
<p>In order to effectively use the KNIMEZoBot, users need to follow a series of key steps. The first requirement is selecting the type of Zotero library they want to access - either a personal</p>
<p>Zotero library or a group library. Based on that choice, users will need to input their corresponding Zotero API key, which allows the system to interface with the library.</p>
<p>Additionally, users will need to provide either their personal Zotero user ID if accessing their own library, or the group ID if accessing a shared group library. To assist users in easily finding and copying their user ID or group ID, we have included hyperlinks within the system interface that direct users to Zotero guides with instructions on locating that information.</p>
<p>Furthermore, to enable more targeted searches, users have the option to filter based on Zotero collections. This allows them to refine the content being retrieved from their library down to specific collections, rather than everything in the library. The system was designed to be flexible - some users may want to search across their entire library, while others may want to narrow in on papers from select collections.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Component "Setup Zotero" when executed - User interface of KNIMEZoBot. Users are required to complete the Zotero information fields.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Step</span><span class="w"> </span><span class="mi">4</span><span class="p">:</span><span class="w"> </span><span class="nx">Interacting</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">Collections</span><span class="p">:</span>
<span class="w">    </span><span class="nx">Select</span><span class="w"> </span><span class="err">&quot;</span><span class="nx">Set</span><span class="w"> </span><span class="nx">X</span><span class="p">:</span>
<span class="w">    </span><span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">specific</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="kn">library</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span>
<span class="w">    </span><span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">collection</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">own</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="nx">Select</span><span class="w"> </span><span class="nx">Yes</span><span class="err">&#39;</span><span class="nx">X</span><span class="p">:</span>
<span class="w">    </span><span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">these</span><span class="w"> </span><span class="nx">key</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="kn">library</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">PDFs</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">PDF</span><span class="w"> </span><span class="nx">files</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">own</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="nx">Note</span><span class="p">:</span>
<span class="w">    </span><span class="nx">Collection</span><span class="w"> </span><span class="nx">ID</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">part</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="nx">user</span><span class="w"> </span><span class="nx">URL</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">collections</span><span class="o">/</span><span class="nx">COREQAD</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">collection</span><span class="w"> </span><span class="nx">ID</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">example</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">1</span><span class="nx">COREQAD</span><span class="p">.</span>
<span class="w">    </span><span class="nx">Do</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">collection</span><span class="p">?</span>
<span class="w">    </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="nx">Yes</span>
<span class="w">    </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="nx">Yes</span>
<span class="w">    </span><span class="nx">Collection</span><span class="w"> </span><span class="nx">ID</span><span class="p">:</span>
<span class="w">    </span><span class="mi">1</span>
<span class="w">    </span><span class="mi">2</span>
</code></pre></div>

<p>Figure 3: Continuation of the first component when executed. We provided options to filter by specific collections.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The "setup Zotero" component contains widget nodes ${ }^{23}$ that allow the user to input the information required by the system. It also includes text output nodes and a header node that control the appearance of the user interface.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The first metanode processes information from the "setup zotero" component. Specifically, the "python script" node ${ }^{24}$ accesses and extracts all data from the Zotero library. The "get request" node ${ }^{15}$ (a child node) retrieves attached files for each Zotero item. Subsequently, the 'binary objects to files' node ${ }^{25}$ is employed to facilitate the secure storage of PDF documents in a pre-defined temporary directory within the workflow.</p>
<h1>Second component (Setup OpenAI):</h1>
<p>The second core component of the system involves setting up the OpenAI environment according to the user's preferences. Users have the ability to adjust key settings such as chunk size and chunk overlap. Chunk size refers to the maximum number of tokens processed per API request, while overlap determines the number of duplicated tokens between chunks. Giving users control over these parameters enables them to customize the configuration based on their specific computational needs and use case.</p>
<p>After inputting their OpenAI API key, which grants access to the AI models, users can select from a variety of available models offered through the OpenAI API. Users can make a selection from a range of available OpenAI models, including but not limited to GPT-3.5 Turbo and GPT-4.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Component "Setup OpenAI" when executed- Users are required to select chunk size and overlap settings for text processing, enter their OpenAI API key, and select an AI model.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: The "setup OpenAI" component contains widget nodes enabling users to input their OpenAI API key, chunk size, chunk overlap, and select an AI model as required by the system. Additionally, text output nodes and a header node are included for the appearance of the user interface. The "Python Script" node was used to read and split the PDFs to smaller chunks. We used the Langchain package ${ }^{21}$ "unstructuredPDFLoader" to load the documents. The documents were then split into smaller chunks because of size limitation as GPT models have a maximum input size, usually 1024-2048 tokens. Breaking PDFs into smaller chunks allows to feed longer documents into the models.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: In the metanode, we used the "FAISS Vector Store Creator" node ${ }^{26}$. This node will store the numerical vector representation created by the embedding model from the "OpenAI Embeddings Connector" node ${ }^{27}$. Also, we selected "text-embedding-ada-002" as the embedding model.</p>
<p>The "OpenAI Functions Agent Creator" node ${ }^{28}$ is used to customize the system message. We rote the following message "You are KnimeZoBot, an AI assistant specifically designed to seamlessly integrate the power of the KNIME platform with the vast knowledge stored within your Zotero library. Your mission is to provide the user with a unique and efficient way to access information, answer questions, and streamline users' research tasks by tapping into your personal Zotero library. Get the answer only from the provided information and if it is not store there write "I apologize, but I do not have any information about it in my Zotero library."</p>
<h1>Last component (Chat app):</h1>
<p>The last component of the system is the Chat application, which provides an interactive interface for users to engage with their Zotero library. This chatbot-style app enables users to pose questions and queries about the content of their Zotero library in a natural conversational format. The seamless integration of the chatbot with the Zotero reference database creates a convenient and user-friendly method for users to search for information within their library.</p>
<p>In addition, users have the option to download their full conversation history with the chatbot in a .csv format. This allows users to save all of their questions and the chatbot's responses so they can refer back to the information later.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Final Component "Chat App"- Chat Interface when deployed. This component allows users to ask questions and receive answers through a conversational chatbot. Users can also download their full chat history as a CSV file.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: The final "Chat app" component utilizes an "Agent Prompter" node ${ }^{29}$ to leverage an AI agent, prompts, and the conversation history from the input table to generate responses. The conversation table requires at least two string columns to store previous exchanges. Additionally, an option is provided to save the chat history.</p>
<h1>Conclusion:</h1>
<p>In summary, the KNIMEZoBot represents a promising integration of technologies to expedite literature reviews or undertake natural language queries of existing Zotero libraries. By</p>
<p>unifying the capabilities of Zotero, OpenAI, and KNIME, this system automates laborious tasks such as combing through academic papers to identify relevant information. Researchers can save significant time while benefiting from state-of-the-art Al techniques for synthesizing knowledge in a low code manner. This innovation demonstrates the potential for Al to assume a greater role in accelerating informed research. While further enhancements to the accuracy and sophistication of the automated analysis remain desirable, KNIMEZoBot marks an important step toward streamlining access to critical information in existing literature by domain experts who are not coders by training. By facilitating more rapid and comprehensive understanding of prior work, this system could substantially benefit the research community and knowledge-building process.</p>
<h1>References:</h1>
<ol>
<li>Arnold M, Goldschmitt M, Rigotti T. Dealing with information overload: a comprehensive review. Front Psychol. 2023;14:1122200. doi:10.3389/fpsyg.2023.1122200</li>
<li>Bornmann L, Haunschild R, Mutz R. Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Humanit Soc Sci Commun. 2021;8(1):1-15. doi:10.1057/s41599-021-00903-w</li>
<li>Kousha K, Thelwall M. Artificial intelligence to support publishing and peer review: A summary and review. Learn Publ. n/a(n/a). doi:10.1002/leap. 1570</li>
<li>Paré G, Kitsiou S. Chapter 9 Methods for Literature Reviews. In: Handbook of eHealth Evaluation: An Evidence-Based Approach [Internet]. University of Victoria; 2017. Accessed November 3, 2023. https://www.ncbi.nlm.nih.gov/books/NBK481583/</li>
<li>Tay A. How to write a superb literature review. Nature. Published online December 4, 2020. doi:10.1038/d41586-020-03422-x</li>
<li>Wagner G, Lukyanenko R, Paré G. Artificial intelligence and the conduct of literature reviews. J Inf Technol. 2022;37(2):209-226. doi:10.1177/02683962211048201</li>
<li>Nashwan AJ, Jaradat JH. Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation. Cureus. 15(8):e43023. doi:10.7759/cureus. 43023</li>
<li>Stern J. GPT-4 Has the Memory of a Goldfish. The Atlantic. Published March 17, 2023. Accessed November 3, 2023. https://www.theatlantic.com/technology/archive/2023/03/gpt-4-has-memory-context-window/673426/</li>
<li>Sharun K, Banu SA, Pawde AM, et al. ChatGPT and artificial hallucinations in stem cell research: assessing the accuracy of generated references - a preliminary study. Ann Med Surg 2012. 2023;85(10):5275-5278. doi:10.1097/MS9.0000000000001228</li>
<li>Lewis P, Perez E, Piktus A, et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Published online April 12, 2021. doi:10.48550/arXiv.2005.11401</li>
<li>Chen J, Lin H, Han X, Sun L. Benchmarking Large Language Models in RetrievalAugmented Generation. Published online September 4, 2023. doi:10.48550/arXiv.2309.01431</li>
<li>Berthold MR, Cebron N, Dill F, et al. KNIME - the Konstanz information miner: version 2.0 and beyond. ACM SIGKDD Explor Newsl. 2009;11(1):26-31. doi:10.1145/1656274.1656280</li>
<li>KNIME AI Extension (Labs). KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest</li>
<li>
<p>KNIME REST Client Extension. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.rest/latest</p>
</li>
<li>
<p>GET Request. KNIME Community Hub. Accessed November 7, 2023.
https://hub.knime.com/knime/extensions/org.knime.features.rest/latest/org.knime.rest.nodes .get.RestGetNodeFactory</p>
</li>
<li>KNIME Python 2 Integration (legacy). KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.python2/latest</li>
<li>KNIME Python Integration. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.python3.scripting/latest</li>
<li>KNIME JSON-Processing. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.json/latest</li>
<li>Python Release Python 3.9.0. Python.org. Accessed November 7, 2023. https://www.python.org/downloads/release/python-390/</li>
<li>KNIME Python Integration Guide. Accessed November 7, 2023. https://docs.knime.com/2021-12/python_installation_guide/index.html#_introduction</li>
<li>Introduction | Langchain. Accessed November 7, 2023. https://python.langchain.com/docs/get_started/introduction</li>
<li>credits_and_acknowledgments [Zotero Documentation]. Accessed November 3, 2023. https://www.zotero.org/support/credits_and_acknowledgments#about_zotero</li>
<li>Explore the Wonderful World of KNIME Widgets. KNIME. Accessed November 7, 2023. https://www.knime.com/blog/mini-guide-widget-examples</li>
<li>Python Script. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.python3.scripting/latest/org.kni me.python3.scripting.nodes.script.PythonScriptNodeFactory</li>
<li>Binary Objects to Files. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.base.filehandling/latest/org.kni me.base.filehandling.binaryobjects.writer.BinaryObjectsToFilesNodeFactory</li>
<li>FAISS Vector Store Creator. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:e1168c28</li>
<li>OpenAI Embeddings Connector. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:3a4ffd4b</li>
<li>OpenAI Functions Agent Creator. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:232d61e6</li>
<li>Agent Prompter. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:378eea</li>
</ol>
<p>https://github.com/dayanjan-lab/KNIMEZoBot</p>            </div>
        </div>

    </div>
</body>
</html>
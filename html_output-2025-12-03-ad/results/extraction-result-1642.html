<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1642 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1642</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1642</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-245839047</p>
                <p><strong>Paper Title:</strong> An Adaptive Imitation Learning Framework for Robotic Complex Contact-Rich Insertion Tasks</p>
                <p><strong>Paper Abstract:</strong> Complex contact-rich insertion is a ubiquitous robotic manipulation skill and usually involves nonlinear and low-clearance insertion trajectories as well as varying force requirements. A hybrid trajectory and force learning framework can be utilized to generate high-quality trajectories by imitation learning and find suitable force control policies efficiently by reinforcement learning. However, with the mentioned approach, many human demonstrations are necessary to learn several tasks even when those tasks require topologically similar trajectories. Therefore, to reduce human repetitive teaching efforts for new tasks, we present an adaptive imitation framework for robot manipulation. The main contribution of this work is the development of a framework that introduces dynamic movement primitives into a hybrid trajectory and force learning framework to learn a specific class of complex contact-rich insertion tasks based on the trajectory profile of a single task instance belonging to the task class. Through experimental evaluations, we validate that the proposed framework is sample efficient, safer, and generalizes better at learning complex contact-rich insertion tasks on both simulation environments and on real hardware.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1642.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1642.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim-to-Real L-insertion (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-Real transfer of skill policies for L-shaped contact-rich insertion tasks (from Gazebo simulation to UR3e real robot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper trains hierarchical goal-conditioned imitation (skill) policies in Gazebo simulation and transfers them to a UR3e robot; the transferred policies are used as initialization while a reinforcement-learning (SAC) controller fine-tunes force/position control on the real hardware, enabling successful complex contact-rich insertion with improved sample efficiency and reduced collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>UR3e robotic manipulator with parallel position/force controller + RL agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A UR3e 6-DoF industrial manipulator controlled with a parallel position/force controller; an IL-trained skill policy (ADMP + HGCIL) supplies nominal trajectories and an SAC RL agent outputs position/orientation commands and time-varying force-control gains to accomplish contact-rich insertions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (contact-rich assembly/insertion)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo (version 9)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A physics-based robotics simulation environment (Gazebo) used to simulate the robot, end-effector poses, workpieces (L-shaped objects), contact interactions and sensors used for imitation learning of trajectories and initial skill policies.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics (Gazebo physics engine) — reasonably realistic contact dynamics but not claimed to be high-fidelity; classic robotics simulator fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, contact interactions between workpiece and environment, end-effector poses, forces/torques (simulated), basic kinematics and controller timing</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Realistic friction/slip and complex micro-contact behavior not explicitly characterized; objects were simplified (L objects attached to robot in real tests for stability), limited/no modeling of unstructured disturbances, sensor noise modeling not described, no photorealistic rendering concerns (not relevant), timing and control-stack differences between sim and real not fully modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical UR3e robotic arm executing insertion tasks with real L-shaped parts (sometimes attached directly to the robot for stability), and additional real assembly tasks (USB and plug insertions) performed with jigs when needed; controller runs at 500 Hz, RL policy at 20 Hz.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Complex contact-rich insertion trajectories and associated skill-level behaviors (nominal trajectories + subgoal selection) for L-shaped object insertion; provides trajectory initialization for RL-based force/position controller on real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Imitation learning (HGCIL + adaptive DMPs) to learn skill policies in simulation, combined with model-free reinforcement learning (Soft Actor-Critic) to learn/adapt controller gains and DMP coupling/weights; final adaptation on real robot used SAC fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Learning convergence steps to near-optimal reward (training steps), success rate on evaluation trials, and collision percentage during training; reward curves and task completion success used as metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Convergence in simulation: ADMP+RL required ~40k steps for A(2), ~50k for A(3), ~55k for A(4) to reach near-optimal reward (numerical reward not provided). Collision percentages during sim training (with ADMP): A(2)=7.9%, A(3)=20.9%, A(4)=30.5% (compared to much higher without ADMP).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>After transferring IL skill policies from sim, RL adaptation on the real hardware produced good control policies in ~20k steps for A(2), A(3), A(4). Real assembly task success rates (after learning) for two different tasks: USB insertion per-pose success rates reported as 0.95, 0.90, 1.0, 0.90 and plug insertion as 1.0, 1.0, 0.90, 0.90 (per Table 1 across start poses); average steps per trial also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Differences in friction and slip, unmodeled micro-contact dynamics, sensor noise and latency, control frequency/timing differences, and object stability; authors mitigated some gap factors by attaching L objects to robot (reducing variation) and using jigs in some real tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Using IL-trained skill policies (HGCIL + adaptive DMPs) in simulation as initialization for real RL fine-tuning; modular learning strategy (fixing ADMP once good nominal trajectory found) reduces dimensionality; ADMP reduces collisions during training; physical design choices (attaching objects to robot, using jigs) stabilized real interactions; online RL fine-tuning (SAC) on real robot to adapt residual differences.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The paper emphasizes that accurate modeling of contact dynamics and topological similarity of trajectories are important for effective transfer, but it does not provide quantitative fidelity thresholds; authors note that closer topological proximity between simulated and real trajectories improves adaptation speed.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>After transferring IL skill policies from simulation, an SAC RL agent was run on the real robot to adapt controller parameters and DMP components; learning produced good control policies in about 20k real training steps (per-task), reducing required real-sample exploration thanks to the simulation-initialized skill policies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Skill policies (learned by imitation in Gazebo) can be successfully transferred to a UR3e and used as initialization for RL fine-tuning on real hardware, yielding faster convergence (~20k steps on real) and safer training (fewer collisions) compared with pure RL; attaching objects for stability and using ADMP to produce good nominal trajectories were important enabling choices; explicit domain randomization was not used and authors attribute success to topological similarity and modular learning strategy rather than extremely high-fidelity simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Adaptive Imitation Learning Framework for Robotic Complex Contact-Rich Insertion Tasks', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1642.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1642.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reality-gap mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mention that simulated force profiles can be unsuitable for real tasks due to the reality gap</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In related work and motivation, the authors cite prior work noting that force profiles obtained in simulation may not transfer well to real robots because of the reality gap; this motivates using IL for trajectories and RL to learn force policies on real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>simulated force-profile demonstrations (general robotic manipulators)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Simulated demonstrations of force profiles in prior work — discussed as potentially unreliable when applied on real manipulators due to simulation-to-reality discrepancies in contact/force dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (contact-rich tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>general physics simulators (cited example: tensegrity simulation in Mirletz et al. 2015; the paper itself used Gazebo 9)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics-based simulation environments used to generate force/torque profiles and trajectories; these simulate rigid-body dynamics and contacts but may not capture all real-world complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics; authors explicitly note simulation force profiles can be 'unsuitable' due to a 'reality gap' (i.e., low to moderate fidelity for fine contact/force dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Basic rigid-body dynamics and contact interactions as provided by standard simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Micro-scale contact phenomena, exact friction, material compliance, and subtle force/torque signatures; overall mismatch between simulated and real force profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Not a concrete experiment in this mention — used as a motivation: real robots exhibit force/torque behaviors that may differ from simulated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Discussion-level: transfer of force profiles from simulation to real hardware (not performed successfully here); motivates learning force policies in reality rather than relying solely on simulated force demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Discussion mentions demonstration in simulation (not used directly) and RL on real robots as alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>N/A for mention (authors do not report metrics here; they cite literature highlighting unsuitability).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Reality gap in contact/force modeling; insufficient fidelity of simulated force profiles to represent real contact dynamics (citation: Mirletz et al. 2015).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Implicitly suggests need for real-world RL for force policies, better contact modeling, or other domain adaptation techniques, but no specific solution spelled out in the mention.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No quantitative fidelity requirements stated; qualitative statement that simulated force profiles may be unsuitable when contact/force fidelity is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper highlights the reality gap problem for force-profile transfer from simulation to real robots and uses this as justification for learning force-control policies via RL on (or adapted to) real hardware rather than relying entirely on simulated force demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Adaptive Imitation Learning Framework for Robotic Complex Contact-Rich Insertion Tasks', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Learning Force Control for Contact-Rich Manipulation Tasks with Rigid Position-Controlled Robots <em>(Rating: 2)</em></li>
                <li>Towards Bridging the Reality gap between Tensegrity Simulation and Robotic Hardware <em>(Rating: 1)</em></li>
                <li>Residual Learning from Demonstration <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1642",
    "paper_id": "paper-245839047",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Sim-to-Real L-insertion (this work)",
            "name_full": "Sim-to-Real transfer of skill policies for L-shaped contact-rich insertion tasks (from Gazebo simulation to UR3e real robot)",
            "brief_description": "The paper trains hierarchical goal-conditioned imitation (skill) policies in Gazebo simulation and transfers them to a UR3e robot; the transferred policies are used as initialization while a reinforcement-learning (SAC) controller fine-tunes force/position control on the real hardware, enabling successful complex contact-rich insertion with improved sample efficiency and reduced collisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "UR3e robotic manipulator with parallel position/force controller + RL agent",
            "agent_system_description": "A UR3e 6-DoF industrial manipulator controlled with a parallel position/force controller; an IL-trained skill policy (ADMP + HGCIL) supplies nominal trajectories and an SAC RL agent outputs position/orientation commands and time-varying force-control gains to accomplish contact-rich insertions.",
            "domain": "general robotics manipulation (contact-rich assembly/insertion)",
            "virtual_environment_name": "Gazebo (version 9)",
            "virtual_environment_description": "A physics-based robotics simulation environment (Gazebo) used to simulate the robot, end-effector poses, workpieces (L-shaped objects), contact interactions and sensors used for imitation learning of trajectories and initial skill policies.",
            "simulation_fidelity_level": "approximate physics (Gazebo physics engine) — reasonably realistic contact dynamics but not claimed to be high-fidelity; classic robotics simulator fidelity",
            "fidelity_aspects_modeled": "Rigid-body dynamics, contact interactions between workpiece and environment, end-effector poses, forces/torques (simulated), basic kinematics and controller timing",
            "fidelity_aspects_simplified": "Realistic friction/slip and complex micro-contact behavior not explicitly characterized; objects were simplified (L objects attached to robot in real tests for stability), limited/no modeling of unstructured disturbances, sensor noise modeling not described, no photorealistic rendering concerns (not relevant), timing and control-stack differences between sim and real not fully modeled.",
            "real_environment_description": "Physical UR3e robotic arm executing insertion tasks with real L-shaped parts (sometimes attached directly to the robot for stability), and additional real assembly tasks (USB and plug insertions) performed with jigs when needed; controller runs at 500 Hz, RL policy at 20 Hz.",
            "task_or_skill_transferred": "Complex contact-rich insertion trajectories and associated skill-level behaviors (nominal trajectories + subgoal selection) for L-shaped object insertion; provides trajectory initialization for RL-based force/position controller on real robot.",
            "training_method": "Imitation learning (HGCIL + adaptive DMPs) to learn skill policies in simulation, combined with model-free reinforcement learning (Soft Actor-Critic) to learn/adapt controller gains and DMP coupling/weights; final adaptation on real robot used SAC fine-tuning.",
            "transfer_success_metric": "Learning convergence steps to near-optimal reward (training steps), success rate on evaluation trials, and collision percentage during training; reward curves and task completion success used as metrics.",
            "transfer_performance_sim": "Convergence in simulation: ADMP+RL required ~40k steps for A(2), ~50k for A(3), ~55k for A(4) to reach near-optimal reward (numerical reward not provided). Collision percentages during sim training (with ADMP): A(2)=7.9%, A(3)=20.9%, A(4)=30.5% (compared to much higher without ADMP).",
            "transfer_performance_real": "After transferring IL skill policies from sim, RL adaptation on the real hardware produced good control policies in ~20k steps for A(2), A(3), A(4). Real assembly task success rates (after learning) for two different tasks: USB insertion per-pose success rates reported as 0.95, 0.90, 1.0, 0.90 and plug insertion as 1.0, 1.0, 0.90, 0.90 (per Table 1 across start poses); average steps per trial also reported.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Differences in friction and slip, unmodeled micro-contact dynamics, sensor noise and latency, control frequency/timing differences, and object stability; authors mitigated some gap factors by attaching L objects to robot (reducing variation) and using jigs in some real tasks.",
            "transfer_enabling_conditions": "Using IL-trained skill policies (HGCIL + adaptive DMPs) in simulation as initialization for real RL fine-tuning; modular learning strategy (fixing ADMP once good nominal trajectory found) reduces dimensionality; ADMP reduces collisions during training; physical design choices (attaching objects to robot, using jigs) stabilized real interactions; online RL fine-tuning (SAC) on real robot to adapt residual differences.",
            "fidelity_requirements_identified": "The paper emphasizes that accurate modeling of contact dynamics and topological similarity of trajectories are important for effective transfer, but it does not provide quantitative fidelity thresholds; authors note that closer topological proximity between simulated and real trajectories improves adaptation speed.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "After transferring IL skill policies from simulation, an SAC RL agent was run on the real robot to adapt controller parameters and DMP components; learning produced good control policies in about 20k real training steps (per-task), reducing required real-sample exploration thanks to the simulation-initialized skill policies.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Skill policies (learned by imitation in Gazebo) can be successfully transferred to a UR3e and used as initialization for RL fine-tuning on real hardware, yielding faster convergence (~20k steps on real) and safer training (fewer collisions) compared with pure RL; attaching objects for stability and using ADMP to produce good nominal trajectories were important enabling choices; explicit domain randomization was not used and authors attribute success to topological similarity and modular learning strategy rather than extremely high-fidelity simulation.",
            "uuid": "e1642.0",
            "source_info": {
                "paper_title": "An Adaptive Imitation Learning Framework for Robotic Complex Contact-Rich Insertion Tasks",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Reality-gap mention",
            "name_full": "Mention that simulated force profiles can be unsuitable for real tasks due to the reality gap",
            "brief_description": "In related work and motivation, the authors cite prior work noting that force profiles obtained in simulation may not transfer well to real robots because of the reality gap; this motivates using IL for trajectories and RL to learn force policies on real hardware.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": "simulated force-profile demonstrations (general robotic manipulators)",
            "agent_system_description": "Simulated demonstrations of force profiles in prior work — discussed as potentially unreliable when applied on real manipulators due to simulation-to-reality discrepancies in contact/force dynamics.",
            "domain": "general robotics manipulation (contact-rich tasks)",
            "virtual_environment_name": "general physics simulators (cited example: tensegrity simulation in Mirletz et al. 2015; the paper itself used Gazebo 9)",
            "virtual_environment_description": "Physics-based simulation environments used to generate force/torque profiles and trajectories; these simulate rigid-body dynamics and contacts but may not capture all real-world complexities.",
            "simulation_fidelity_level": "approximate physics; authors explicitly note simulation force profiles can be 'unsuitable' due to a 'reality gap' (i.e., low to moderate fidelity for fine contact/force dynamics).",
            "fidelity_aspects_modeled": "Basic rigid-body dynamics and contact interactions as provided by standard simulators.",
            "fidelity_aspects_simplified": "Micro-scale contact phenomena, exact friction, material compliance, and subtle force/torque signatures; overall mismatch between simulated and real force profiles.",
            "real_environment_description": "Not a concrete experiment in this mention — used as a motivation: real robots exhibit force/torque behaviors that may differ from simulated outputs.",
            "task_or_skill_transferred": "Discussion-level: transfer of force profiles from simulation to real hardware (not performed successfully here); motivates learning force policies in reality rather than relying solely on simulated force demonstrations.",
            "training_method": "Discussion mentions demonstration in simulation (not used directly) and RL on real robots as alternatives.",
            "transfer_success_metric": "N/A for mention (authors do not report metrics here; they cite literature highlighting unsuitability).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Reality gap in contact/force modeling; insufficient fidelity of simulated force profiles to represent real contact dynamics (citation: Mirletz et al. 2015).",
            "transfer_enabling_conditions": "Implicitly suggests need for real-world RL for force policies, better contact modeling, or other domain adaptation techniques, but no specific solution spelled out in the mention.",
            "fidelity_requirements_identified": "No quantitative fidelity requirements stated; qualitative statement that simulated force profiles may be unsuitable when contact/force fidelity is insufficient.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "The paper highlights the reality gap problem for force-profile transfer from simulation to real robots and uses this as justification for learning force-control policies via RL on (or adapted to) real hardware rather than relying entirely on simulated force demonstrations.",
            "uuid": "e1642.1",
            "source_info": {
                "paper_title": "An Adaptive Imitation Learning Framework for Robotic Complex Contact-Rich Insertion Tasks",
                "publication_date_yy_mm": "2022-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "a_practical_approach_to_insertion_with_variable_socket_position_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Learning Force Control for Contact-Rich Manipulation Tasks with Rigid Position-Controlled Robots",
            "rating": 2,
            "sanitized_title": "learning_force_control_for_contactrich_manipulation_tasks_with_rigid_positioncontrolled_robots"
        },
        {
            "paper_title": "Towards Bridging the Reality gap between Tensegrity Simulation and Robotic Hardware",
            "rating": 1,
            "sanitized_title": "towards_bridging_the_reality_gap_between_tensegrity_simulation_and_robotic_hardware"
        },
        {
            "paper_title": "Residual Learning from Demonstration",
            "rating": 2,
            "sanitized_title": "residual_learning_from_demonstration"
        }
    ],
    "cost": 0.01160075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Adaptive Imitation Learning Framework for Robotic Complex Contact-Rich Insertion Tasks</p>
<p>Yan Wang 
Department of Systems Innovation
Graduate School of Engineering Science
Osaka University
SuitaJapan</p>
<p>Cristian C Beltran-Hernandez 
Department of Systems Innovation
Graduate School of Engineering Science
Osaka University
SuitaJapan</p>
<p>Weiwei Wan 
Department of Systems Innovation
Graduate School of Engineering Science
Osaka University
SuitaJapan</p>
<p>Kensuke Harada 
Department of Systems Innovation
Graduate School of Engineering Science
Osaka University
SuitaJapan</p>
<p>Artificial Intelligence Research Center
Automation Research Team
National Institute of Advanced Industrial Science and Technology
TsukubaJapan</p>
<p>An Adaptive Imitation Learning Framework for Robotic Complex Contact-Rich Insertion Tasks
10.3389/frobt.2021.777363compliance controlimitation learningreinforcement learningrobotic assemblyrobot autonomy
Complex contact-rich insertion is a ubiquitous robotic manipulation skill and usually involves nonlinear and low-clearance insertion trajectories as well as varying force requirements. A hybrid trajectory and force learning framework can be utilized to generate high-quality trajectories by imitation learning and find suitable force control policies efficiently by reinforcement learning. However, with the mentioned approach, many human demonstrations are necessary to learn several tasks even when those tasks require topologically similar trajectories. Therefore, to reduce human repetitive teaching efforts for new tasks, we present an adaptive imitation framework for robot manipulation. The main contribution of this work is the development of a framework that introduces dynamic movement primitives into a hybrid trajectory and force learning framework to learn a specific class of complex contact-rich insertion tasks based on the trajectory profile of a single task instance belonging to the task class. Through experimental evaluations, we validate that the proposed framework is sample efficient, safer, and generalizes better at learning complex contact-rich insertion tasks on both simulation environments and on real hardware.</p>
<p>INTRODUCTION</p>
<p>Contact-rich insertion is a ubiquitous robotic manipulation skill in both product assembly and home scenarios. Some contact-rich insertion tasks involve nonlinear and low-clearance insertion trajectories and require varying force control policies at different phases, which we define as complex contact-rich insertion tasks, such as ring-shaped elastic part assembly and USB insertion. Such tasks demand skillful maneuvering and control, which makes them challenging for robots.</p>
<p>Imitation learning (IL) is a promising approach to tackle complex contact-rich insertion tasks by reproducing the trajectory and force profiles from human demonstrations. However, there are some concerns that prevent IL from working efficiently and safely in actual applications: 1) Force profiles are not easy to acquire from demonstrations compared with trajectory profiles:</p>
<p>Trajectory profiles can be easily obtained from kinesthetic teaching, teleoperation, simulation, among other methods, but force profiles usually demand additional haptic devices (Kormushev et al., 2011). Even with the force sensor that is integrated into the robot, it suffers from the strict position limit, i.e., the hand of the demonstrator should never be between the end-effector (EEF) and the force sensor, which usually makes the demonstrations of the complex contact-rich tasks inconvenient. Also, when there is no real robot available, force profiles from simulated environments can be unsuitable for actual tasks due to the reality gap (Mirletz et al., 2015). 2) Motion shift of the EEF of the manipulator exacerbates the compounding error problem (Ross and Bagnell, 2010;Ross et al., 2011) of IL because IL usually learns a one-step model that takes a state and an action and outputs the next state, and one-step prediction errors can get magnified and lead to unacceptable inaccuracy (Asadi et al., 2019). 3) Demonstrations are usually task specific and require human repetitive teaching efforts for new tasks even if demonstrations with topologically similar trajectories have already been collected.</p>
<p>For the first concern, the lack of proper force profiles in human demonstrations can be complemented by model-free reinforcement learning (RL), which is an effective method to handle contact-rich insertion tasks (Inoue et al., 2017;Vecerik et al., 2019) by interacting with the environment. Beltran-Hernandez et al. (2020) presents an RL-based control framework for learning low-level force control policies to enable rigid position-controlled robot manipulators to perform contact-rich tasks. However, the exploratory nature of RL can lead to low-quality trajectories for complex contactrich insertion tasks, which causes hardware wear and tear or even damage due to a myriad of collisions during the training process on a real robot. Therefore, high-level control policies, which can give proper commands of nominal trajectories of the EEF of the manipulator are necessary to alleviate the situation. We define the high-level control policy which provides the nominal trajectory as the skill policy, and the low-level control policy which generates the specific parameters of the controller as the motion policy. A skill policy can be learned by IL, but as the second concern above presents, the motion drift of EEF away from the demonstrated trajectory usually occurs during a task. Therefore, we proposed a novel hierarchical goalconditioned IL (HGCIL) method (Wang et al., 2021) to learn the skill policy to facilitate the EEF to recover from deviate poses through self-supervised learning. Finally, as the last concern states, there are situations where a human has to demonstrate a set of tasks with topologically similar trajectories. These tasks differ from each other in terms of geometric characteristics such as size or shape of the workpieces. Therefore, we seek to generalize an existing trajectory profile to its variations so that human efforts on new demonstrations can be reduced. Dynamical movement primitives (DMPs) (Ijspeert et al., 2013) model is a typical dynamic system-based technique that has been widely applied in the field of IL for encoding both periodic and discrete motion data. DMPs can generate a trajectory or control signal that can be flexibly adjusted to guide the real system without manual parameter tuning or affecting the overall convergence and stability. They can also be modulated to meet different requirements, such as obstacle avoidance (Park et al., 2008;Hoffmann et al., 2009), by adding feedback terms. Therefore, we consider using DMPs to adapt an existing trajectory profile to new tasks so that we can learn new control policies based on the generalized trajectory profiles.</p>
<p>The main contribution of this paper is the development of an adaptive imitation learning framework for robot manipulation (Figure 1), which introduces DMPs into a hybrid trajectory and force-learning framework in a modular fashion, to learn the control policies of a specific class of complex contact-rich insertion tasks based on the trajectory profile of a single instance (note that a trajectory profile can include several trajectory demonstrations of a task instance), thus, relieving human demonstration burdens. We show that the proposed framework is sample efficient, generalized to novel tasks, and is safe enough to be qualified for the learning on both simulated environment and real hardware.</p>
<p>The rest of this paper is organized as follows: After discussing the most related work in the Related work section, we set up our problem and introduce some techniques applied in our framework in the Preliminaries section. In the Adaptive robotic imitation framework section, we describe the overview and details of the proposed adaptive imitation learning framework. Then we experimentally evaluate the performance of this framework on simulated environment and real hardware using a UR3e robotic arm in the Experimental evaluation section.</p>
<p>RELATED WORK</p>
<p>In this section, we provide an overview of the application of IL and RL approaches in the context of contact-rich insertion tasks and the position of our work in the existing literature.</p>
<p>Imitation learning</p>
<p>Imitation learning (IL), also referred to as learning from demonstration (LfD), is a powerful approach for complex manipulation tasks, which perceives and reproduces human movements without the need of explicit programming of behavior (Takamatsu et al., 2007;Kormushev et al., 2011;Suomalainen and Kyrki, 2017;Hu et al., 2020). Among the IL approaches, DMPs (Ijspeert et al., 2013) have shown the ability to generalize demonstrations in different manipulation tasks (Peters and Schaal, 2008;Metzen et al., 2014;Hu et al., 2018;Sutanto et al., 2018). However, the forces and torques that a human applies during the demonstrations of contact-rich tasks are required to regress a proper admittance gain of robot controller (Tang et al., 2016) or to match with modified demonstrated trajectories using DMPs (Abu-Dakka et al., 2015;Savarimuthu et al., 2017). To quickly program new pegin-hole tasks without analyzing the geometric and dynamic characteristics of workpieces (Abu-Dakka et al., 2014) exploits demonstrations and exception strategies to develop a general strategy that can be applied to the objects with similar shapes, which need to be inserted. However, force profiles are still essential for such strategies to modify the trajectories of the learned movements.</p>
<p>In contrast, we study the case wherein only the trajectory profile of a single instance is available in a class of complex contact-rich insertion tasks, and based on this trajectory profile, Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363 2 we manage to solve other variations of this instance with different object sizes or shapes but topologically similar insertion trajectories without explicitly knowing the concrete geometric characteristics. In this context, it does not help even if the original force profile is available because the new trajectories are unknown so that we cannot match the trajectory and the force profiles.</p>
<p>Reinforcement learning</p>
<p>Reinforcement learning (RL) methods have been widely used for contact-rich robotic assembly tasks (Inoue et al., 2017;Thomas et al., 2018;Vecerik et al., 2019;Beltran-Hernandez et al., 2020) to circumvent difficult and computationally expensive modeling of environments. However, sample efficiency and safety problem have always been issues that affect its practicality in complex contact-rich manipulation tasks.</p>
<p>To improve the sample efficiency and guarantee the safety of RL, human prior knowledge is usually incorporated for learning complex tasks. One such way is reward shaping (Ng et al., 1999), where additional rewards auxiliary to the real objective are included to guide the agent toward the desired behavior, e.g., providing punishment when a safety constraint such as collision is violated (Beltran-Hernandez et al., 2020). Generally, reward shaping is a very manual process. It is as difficult to recover a good policy with reward shaping as to specify the policy itself (Johannink et al., 2019). Although some prior work considers reward shaping as a part of the learning system (Daniel et al., 2015;Sadigh et al., 2017), human efforts are still necessary to rate the performance of the system. Therefore, another way occurs that human prior knowledge is included in RL through demonstration (Atkeson and Schaal, 1997) to guide the exploration. Some work initializes RL policies from demonstration for learning classical tasks such as cart-pole (Atkeson and Schaal, 1997), hitting a baseball (Peters and Schaal, 2008), and swing-up (Kober and Peters, 2009). Beyond initialization using demonstration, some promising approaches incorporate demonstrations with the RL process through replay buffer (Vecerik et al., 2017;Nair et al., 2018) and fine-tuning with augmented loss (Rajeswaran et al., 2018). However, these methods require humans to be able to teleoperate the robot to perform the task so that the observation and action spaces of demonstration (state-action pairs) are consistent with the RL agent, which is not always available for an industrial manipulator.</p>
<p>Considering the lack of teleoperation system, residual RL (Johannink et al., 2019) combines the conventional controller, which ships with most robots with deep RL to solve complex manipulation tasks, where the problems can be partially handled with conventional feedback control, e.g., with impedance control, and the residual part, including contacts and external object dynamics, is solved with RL. Based on Johannink et al. (2019), Davchev et al. (2020) proposes a residual LfD (rLfD) framework that bridges LfD and model-free RL through an adaptive residual learning policy operating alongside DMPs applied directly to the full pose of the robot to learn contact-rich insertion tasks. However, Davchev et al. (2020) does not discuss how to handle different force requirements at different phases, e.g., the search phase and insertion phase, of the insertion task.</p>
<p>In the proposed framework, we utilize DMPs on the skill level together with a novel HGCIL approach to provide nominal trajectories for the controller to follow and learn the motion policy of the controller by RL. Specifically, the framework learns the time-variant force-control gains to behave accordingly at different phases of the insertion task, which is not discussed in Davchev et al. (2020), and DMPs are also updated by RL to adapt FIGURE 1 | System overview of the adaptive robotic imitation framework. The upper and the lower part are the trajectory learning and the force learning parts, respectively. The switch symbol between the reinforcement learning (RL) agent and the dynamical movement primitives (DMPs) module means the update of DMPs is executed using a modular learning strategy.</p>
<p>Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363 the existing nominal trajectories to new tasks during the training process.</p>
<p>PRELIMINARIES</p>
<p>In this section, we describe the problem statement and provide fundamentals of some key techniques utilized in our adaptive robotic imitation framework.</p>
<p>Problem statement</p>
<p>Let A be a complex contact-rich insertion task class, which represents a set of tasks with topologically similar trajectories. We define a task A (n) ∈ A as the nth instance of A. P (n) is the demonstrated trajectory profile of A (n) consisting of k demonstrated trajectories, Γ, i.e., P (n) Γ 1 , Γ 2 , . . . , Γ k (n) , and each Γ in P (n) consists of a sequence of the EEF poses, p, in the task space. Using the hybrid trajectory and force learning framework proposed by Wang et al. (2021), we can learn a proper control policy for each A (n) if P (n) is accessible.</p>
<p>To clarify, we assume an L-shaped object insertion (L insertion) task class, referred to as A. The goal of L insertion is to insert an L-shaped workpiece held by a robotic gripper into a groove with a corresponding shape, and the clearances are no more than 1 mm. There are some instances where Figure 2 shows the L-shaped workpiece, L, involved in each instance. With A (1) as the base instance, the L of A (2) gets its shape by applying an affine transformation to the L of A (1) ; the Ls of A (3) and A (4) further reshape it by extending the bottom and doubling the entity, respectively.
A (1) , A (2) , A (3) , A (4) ∈ A, and
In this paper, we assume that only the demonstrated trajectory profile of A (1) , P (1) , is available as shown in Figure 3. We know that other instances of A have similar trajectories to A (1) but have no access to concrete information of these trajectories or geometric characteristics of objects involved in these instances. Although we can collect their trajectory profiles through demonstrations, it would be time-consuming and tedious when the number of instances is quite large, which brings huge burdens to the human demonstrator. Therefore, we need an effective trajectory learning approach that can adapt an existing trajectory profile to new similar scenarios to reduce the human burden, and this is the motivation that we introduce the DMPs to the hybrid trajectory and force learning framework.</p>
<p>Dynamical movement primitives</p>
<p>Positional dynamical movement primitives</p>
<p>Following the modified formulation of positional DMP introduced by (Park et al., 2008), the differential equation of a one-dimensional positional DMP has three components. The first component is the transformation system that creates the trajectory plan:
τ _ v K[(g − x) − (g − x 0 )s + f(s)] − Dv(1)
where x ∈ R and v τ _ x are the position and velocity of a prescribed point of the system, respectively. τ ∈ R + is a temporal scaling factor. x 0 , g ∈ R are the initial and goal positions, respectively. K, D ∈ R + are the spring and damping terms, respectively, and D is chosen as D 2 K √ to keep the system critically damped. s is a phase variable, and it is governed FIGURE 2 | A class of L-shaped object insertion tasks. The shapes and sizes of workpieces are different among tasks, but these tasks possess topologically similar insertion trajectories (unit: mm). by the second component of DMP formulation, a canonical system: τ _ s −αs, α ∈ R + . The third component is a nonlinear function approximation term (called forcing term), f, to shape the attractor landscape,
f(s) N i 1 ω i ψ i (s) N i 1 ψ i (s) s (2) where ψ i (s) exp(−h i (s − c i ) 2 )
are Gaussian basis functions with centers c i and widths h i , and ω i is their weights.</p>
<p>In this paper, we utilize three-dimensional DMPs for the three positional degrees of freedom (DoF). Therefore, we rewrite Eq. 1 in multidimensional form as shown in Eq. 3:
τ _ v K[(g − x) − (g − x 0 )s + f(s)] − Dv τ _ x v(3)
Each DoF has its own transformation system and forcing term but shares the same canonical system.</p>
<p>Orientational dynamical movement primitives</p>
<p>Besides positional DMPs, insertion tasks are also highly dependent on orientation. Therefore, we also utilize orientational DMPs (Pastor et al., 2011;Ude et al., 2014). A unit quaternion q ∈ S 3 is commonly used to describe an orientation because it provides a singularity-free and nonminimal representation of the orientation . S 3 is a unit sphere in R 4 . The transformation system of orientational DMPs is:
τ _ η K[2 log(g p q)] − Dη + f(s) τ _ q 1 2η p q ⎧ ⎪ ⎨ ⎪ ⎩(4)
where g ∈ S 3 denotes the goal quaternion orientation, q denotes the quaternion conjugation of q, and * denotes the quaternion product.η [0, η T ] T is the angular velocity quaternion. K, D ∈ R 3×3 are angular stiffness and damping gains, respectively. The canonical system and the nonlinear forcing term, f(s), are defined in the same way as the positional DMPs. We also use the quaternion logarithm log(·) and exponential map exp(·) as given in Ude et al. (2014).</p>
<p>Coupling term</p>
<p>Eqs. 3 and 4 can be used to imitate a demonstrated trajectory. However, we sometimes desire to modify the behavior of the system online in practice. To modify a DMP online, an optional coupling term, C t , is usually added to the transformation system of DMP. For example, a one-dimensional positional DMP with C t has the formulation as follows:
τ _ v K(g − x) − Dv − K(g − x 0 ) + Kf(s) + C t(5)
Ideally, C t would be zero unless a special sensory event requires modifying the DMP. In the field of robotic manipulation, coupling terms have been used to avoid obstacles (Rai et al., 2014), to avoid joint limits (Gams et al., 2009), to grasp under uncertainty , etc. This term is vital for our adaptive framework, and we will discuss it in the Adaptive robotic imitation framework section. Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363</p>
<p>Goal-conditioned imitation learning</p>
<p>In a typical IL setting, the ith demonstrated trajectory Γ i in a trajectory profile P is in the form of state-action pairs, i.e., Γ i (s i 0 , a i 0 , . . . , s i T , a i T ), where T represents the total time steps. For a complex nonlinear trajectory, some specific states, commonly known as bottleneck states, need to be reached to correctly imitate the whole trajectory. It is challenging for behavior cloning (BC), a conventional approach, which learns a policy π(a|s) from the state-action pairs, to imitate, such a trajectory due to compounding errors in the Markov decision process (MDP). Goal-conditioned IL (GCIL) is a selfsupervised method that learns a goal-conditioned policy that has been proven to be more effective than BC in reproducing the said complex trajectory (Kaelbling, 1993;Schaul et al., 2015;Ding et al., 2019). In a goal-conditioned setting, the state-action pairs are replaced by state-action-goal triplets, (s i t , a i t , s i g ), and a goalconditioned policy π(a|s, s g ), which attempts to match different goals is learned instead of π(a|s). Data relabeling (Lynch et al., 2019) is an effective data augmentation method usually used by GCIL, which treats each state s i t+k visited within a demonstrated trajectory from s i t to s i g as a latent goal state. This technique is particularly effective in the low data regime where a few demonstrations are available.</p>
<p>Algorithm 1 | Modular learning process.</p>
<p>ADAPTIVE ROBOTIC IMITATION FRAMEWORK</p>
<p>System overview</p>
<p>The architecture of our framework is shown in Figure 1, which is built on a hybrid trajectory and force learning framework from our previous work (Wang et al., 2021). It consists of a trajectory learning part and a force learning part. The former takes an existing trajectory profile, P (m) , of the task A (m) ∈ A as input and generates the nominal trajectory, Γ N (n) , of another task A (n) ∈ A. Γ N (n) is learned from P (m) by an IL agent, which consists of an adaptive DMP module (ADMP) and a skill policy module. The force learning part is composed of an RL agent and a parallel position/force controller (Chiaverini and Sciavicco, 1993). The RL agent learns both the parameters and the position/orientation commands of the controller following Γ N (n) to control the industrial rigid manipulator to finish A (n) with proper force control policy. In the rest of this section, we will introduce each part of this framework in detail.</p>
<p>Modular learning strategy</p>
<p>In the proposed framework, we use a modular learning strategy because end-to-end learning can become very inefficient and even fail as networks grow (Glasmachers, 2017), which is known as the curse of dimensionality. In contrast, structured training of separate modules may be more robust. Moreover, assembly tasks are naturally divided into different subtasks that can be learned in different modules, e.g., in our problem setting, a task can be divided into a trajectory learning part and a force learning part. Therefore, we introduce DMPs into the framework in a modular learning fashion expecting to overcome the curse of dimensionality.</p>
<p>ADMP works in the trajectory learning part. It keeps constant after finding a seemingly suitable nominal trajectory Γ N (n) of A (n) with a small amount of trial and error, and then the framework only updates the parameters of the controller for the force learning at each training step. If the learning performance is constantly poor with the current Γ N (n) after certain steps, ADMP will be updated again with a given frequency to search for an alternative Γ N (n) . This mechanism is represented by the switch symbol in Figure 1. The whole modular learning process is shown in Algorithm 1.</p>
<p>Trajectory learning</p>
<p>Adaptive action of adaptive dynamical movement primitives</p>
<p>In the trajectory learning, we hope to adapt trajectories in an existing task trajectory profile to new trajectories that are suitable for other similar tasks. Therefore, we introduce ADMP to achieve this goal. As we only use ADMP to realize spatial scaling, we set the temporal scaling factor τ to 1 in Eqs. 3 and 4.</p>
<p>As mentioned in the Coupling term section, the behaviors of ADMP can be modified by changing the coupling terms, C t , in Eq. 5. Therefore, it is a promising approach to learn proper C t for ADMP to adapt to new scenarios. Moreover, the forcing term weights, ω, can also affect the resulting trajectories.</p>
<p>To discern how different components of the DMP formulation affect the results, we make an investigation by introducing random C t or adding random noise to ω in the DMP formulation of a sine wave as depicted in Figure 4. The green line is a sine wave trajectory. We spatially scale the sine wave to match a new goal using twodimensional DMPs. The first subfigure in Figure 4 shows the scaled trajectory using vanilla DMPs, which means no C t is added, and ω is chosen to match the original trajectory without Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363</p>
<p>noise. With such a baseline, we then add 1) only C t ; 2) only ω noise; and 3) both C t and ω noise to the DMP formulation to observe the effects on the resulting trajectories. The results in Figure 4 indicate that 1) C t facilitate local exploration based on the original trajectory, 2) noise added to ω leads to locally smooth but globally different trajectory, and 3) adding both C t and ω results in a trajectory with both global shape change and local exploration. Considering our requirements, the global change in trajectory may benefit the coarse adaptation to geometric characteristics of new workpieces, and the local exploration can help to tackle some delicate bottleneck states along the trajectory. Therefore, we choose to add both C t and ω noise to the DMP formulation. In the framework, instead of meaningless variables, C t and ω noise are learned by the RL agent through interacting with the environment.</p>
<p>Hierarchical goal-conditioned imitation learning</p>
<p>We train the skill policy using an HGCIL approach proposed in our previous work (Wang et al., 2021). Following the goalconditioned setting in the Goal-conditioned imitation learning section, we reorganize the original trajectory profile P (n) into a hierarchical goal-conditioned (HGC) trajectory profile P skill (n) . A trajectory Γ ∈ P skill (n) consists of a sequence of poses (p 0 , p 1 , . . . , p T ), which are Cartesian poses of EEF in our framework. Sliding along each sequence in P (n) with two predefined hierarchical windows, W s and W m , we obtain a new sequence of triplets, (p, p l , p h ) (p t , p t+min(w,Wm) , p t+w ), if t + w ≤ T t 1, 2, . . . , T; w 1, 2, . . . , W s .</p>
<p>where p, p l , and p h represent the current pose, the subgoal pose, and the goal pose, respectively. Note that p l plays the role of action between two consecutive states here. All these triplets compose P skill (n) and the skill policy π(p l |p, p h ) is trained using a fully connected neural network with three hidden layers, each with 256 units, a dropout rate of 0.1, and ReLu as the activation function, which maps the observation, (p, p h ), to the action, p l . With the skill policy π(p l |p, p h ), the IL agent can spontaneously find subgoals, p l , for a distant goal along the trajectory and provides p l to the parallel controller. All these subgoals compose the nominal trajectory, Γ N (n) . Since p l can be periodically updated based on p and p h , the motion drift of EEF is constrained, and the goal-conditioned setting assists the EEF in recovering from unseen states.  The learning process starts with p l from the trajectory learning every time step. p is the actual Cartesian pose of EEF, and f [ f, τ] is the contact force, where f ∈ R 3 is the force vector and τ ∈ R 3 is the torque vector. f g is the reference force of the insertion task. The pose error of EEF, p e p l − p, the velocity of the EEF, _</p>
<p>p, and f serve as inputs to the RL agent, while p e and f also serve as feedback to the parallel position/force controller. For the controller, the RL agent gives policy actions consisting of a p and a cp . a p [v, w] are the position/orientation commands where v ∈ R 3 is the position and w ∈ R 4 is the quaternion to control the movements of the robot; a cp [K </p>
<p>is the selection matrix, whose elements correspond to the degree of control that each controller has over a given direction. Finally, the actual position command, p c a p + p p c + p f c , is produced by the controller based on all inputs and sent to the manipulator.</p>
<p>Algorithm and reward</p>
<p>We use Soft-Actor-Critic (SAC) (Haarnoja et al., 2018) as the RL algorithm of the scheme, which is a state-of-the-art model-free and off-policy actor-critic deep RL algorithm based on the maximum entropy RL framework. It encourages exploration according to a temperature parameter, and the core idea is to succeed in the task while acting as randomly as possible. As an off-policy algorithm, it can use a replay buffer to reuse information from recent operations for sample-efficient training. We use a reward function as follows:
r(s) w 1 M p e p max 1,2 + w 2 M f e f max 2 + γ. (7)
f e f g − f is the contact force error. p max and f max are defined maximum values. y M(x), x ∈ [1, 0] linearly maps x to y ∈ [1, 0]. Therefore, the smaller p e and f e are, the higher the reward is. z 1,2 is the l 12 norm (Levine et al., 2016), which is given by
1 2 z 2 + α + z 2 √
. This norm is used to encourage the EEF to precisely reach the target position, but to also receive a larger penalty when far away. γ is the auxiliary term, which can be a positive reward (100) for finishing the task successfully, a negative one (−50) for excessive force, or 0 otherwise. w 1 and w 2 are hyperparameters to weight the components.</p>
<p>EXPERIMENTAL EVALUATION</p>
<p>In this section, we evaluate the efficacy of our adaptive robotic imitation framework in learning a class of complex contact-rich insertion tasks from a single instance. We perform a sequence of empirical evaluations using the L insertion task class. We divide this section into three parts: first, applying the framework on a simulated environment to study its sample efficiency, generalizability to different task instances, and safety during the training sessions; second, applying the framework to real insertion tasks to further validate its adaptiveness in the physical world; and third, ablation studies to investigate the effect of different components on the overall performance of our framework.</p>
<p>Implementation details</p>
<p>We evaluate the proposed framework both on a simulated environment built in the Gazebo nine and on a real UR3e robotic arm as shown in Figure 5. The real UR3e robotic arm uses a control frequency of 500 Hz, which is the maximum available for the robot. The RL control policy runs at a frequency of 20 Hz on both the simulated environment and the real robot. The training sessions are performed on a computer with a GeForce RTX 2060 SUPER GPU and an Intel Core i7-9700 CPU. The implementation of the ADMP method was based on the DMP implementation from the DMP++ (Ginesi et al., 2019) repository, and for the RL agent, we used the SAC implementation from the TF2RL (Ota, 2020) repository.</p>
<p>Evaluation on simulated environment</p>
<p>First, we evaluated the efficacy of the adaptive robotic imitation framework on the simulated environment. We used the L insertion task class described in the Problem statement section, and we assumed access to only a trajectory profile of A (1) consisting of six demonstrated trajectories.</p>
<p>Sample efficiency</p>
<p>The most concerning point of the learning framework is the sample efficiency. By providing a nominal trajectory learned from FIGURE 7 | Learning curves and sample efficiencies of task instances A (2) , A (3) , and A (4) . The red dashed line represents the near-optimal reward.</p>
<p>Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363 8 demonstration to the RL learning process, the sample efficiency can be largely improved according to our previous work (Wang et al., 2021). However, the framework in this paper indirectly generates the nominal trajectory by adapting existing trajectories using ADMP and may cost more time than using demonstrated trajectories. Therefore, we are interested in whether the framework is still sample-efficient compared with other alternatives.</p>
<p>We compared the learning curves of training sessions on A (2) task with frameworks using different trajectory learning methods: ADMP (ours), demonstrated trajectory (DEMO), and RL from scratch (w/o) as shown in Figure 6.</p>
<p>Among these methods, ADMP showed the highest sample efficiency of 40 K steps, even higher than the baseline DEMO (55 K), and the learning result of ADMP was also as good as the DEMO. Although the better performance of ADMP than DEMO may result from suboptimal demonstration, this result indicated that introducing the DMP component into our framework was indeed effective in adapting to new tasks and alleviating human demonstration burden, and the sample efficiency was at least not lower than using demonstrated trajectories of new tasks.</p>
<p>Generalizability</p>
<p>Since the proposed framework displayed good adaptation to A (2) task, we then tested with A (3) and A (4) to study its generalizability to different tasks. The result is shown in Figure 7. It indicated that the framework could generalize among different kinds of task instances with good sample efficiencies and learning results. In detail, the steps cost for convergence in learning A (2) , A (3) , and A (4) were 40, 50, and 55 K steps, respectively. We analyzed that different sample efficiencies mainly resulted from their different difficulties: the object shapes in A (2) were the most similar to A (1) with an affine transformation, while the other two involved more variations.</p>
<p>Safety</p>
<p>Finally, we compared the collision percentage during the training sessions of each task using frameworks with and without ADMP as shown in Figure 8. Five training sessions were implemented for each pair of task and framework, and  the collision percentage of each training session, P col , is calculated by:
P col Total Collision Number Total Episode Number × 100%.
With ADMP, the collision percentages of A (2) , A (3) , and A (4) , were diminished to 7.9%, 20.9%, and 30.5% from 30.2%, 39%, and 72.6%, respectively. The result indicated that the proposed framework with ADMP was also qualified for our requirement of lowering the chance of collision during the training sessions, which reduces the equipment wear and tear and the risk of damaging the workpieces on real hardware.</p>
<p>Experiments on a real robot</p>
<p>After evaluating the sample efficiency, generalizability, and safety of the framework on the simulated environment, we applied it to some real insertion tasks belonging to the L insertion task class to test its adaptiveness in the physical world.</p>
<p>Sim-to-real transfer</p>
<p>We first executed sim-to-real transfers using a trained IL agent, which learned the skill policies on simulation and obtained the control policies for A (2) , A (3) , and A (4) on the real hardware. The L objects in these tasks were directly attached to the robot for stability. The learning curves are shown in Figure 9. Benefiting from the learned skill policies, our framework learned good control policies for A (2) , A (3) , and A (4) at about 20 K steps. Although it took some time for the RL agent to adapt to the physical world, the result indicated that the skill policies learned by the framework on the simulation provided good initialization and effectively enhanced the sample efficiency of the real learning process.</p>
<p>Real assembly tasks</p>
<p>We then utilized two real assembly tasks, a USB insertion task and a plug insertion task, to further validate the generalizability of the framework. As shown in Figure 10, the USB and the plug were grasped by the gripper. We assumed that they were in stable poses so that their positions did not change too much during the training sessions. Considering the contact area, we added a jig between the USB and the gripper to improve the stability in case that the friction was not enough to resist the contact force. As for the plug, we did not utilize a jig. A structural aluminum profile played the role of an FIGURE 10 | Two real assembly tasks and their learning curves. Left: USB insertion task. Right: plug insertion task. The objects are grasped by the gripper. A jig is used in the USB insertion to improve the stability considering the contact area between the USB and the gripper. obstacle to prevent the USB or the plug from inserting into the hub or the socket directly in each task, so that the EEF had to adopt a trajectory like the L insertion to finish the task. Figure 10 also shows their learning curves. It took the framework about 30 K steps and 10 K steps to learn these two tasks, respectively. We analyzed that the difference between the sample efficiencies of different tasks resulted from their different trajectory proximity to the initial trajectory-the trajectory of the plug insertion was more similar to the original demonstrated trajectory of the L insertion so that the learning process was faster. Note that we utilized a jig in the USB insertion to reduce the slip between the object and the gripper but not in the plug insertion. This is why the performance dropped at about 3 K steps in the plug insertion while the performance of the USB insertion kept improving. Table 1 displays the success rates and average steps cost among 20 trials for each task. In each trial, the EEF was initially set to a random pose in a distance range of [15,45] (unit: mm) and a pitch angle range of [10, 30] (unit:°) away from the target pose. Figure 11 shows the results of the initial and the learned control policies of the two tasks, including the Euclidean distance errors of EEF, pitch angle errors, and the force/torque data during the evaluation process. Note that although only the result of a single run is provided for each policy, it is typical enough to verify the effectiveness of the proposed framework on learning good policies for the tasks.</p>
<p>Ablation studies</p>
<p>In this part, we executed two ablation studies to investigate how different hyperparameters and strategies affected the performance of the proposed framework. We ran each ablation study on A (2) task following the settings in the Evaluation on simulated environment section.</p>
<p>Effects of the dynamical movement primitive components</p>
<p>In the Adaptive action of adaptive dynamical movement primitives section, we provide a simple investigation on how C t and ω of the DMPs affect the generalized trajectory, and the conclusion is that C t benefits the local exploration while ω benefits the global change of the trajectory. As we assume that both the local exploration and the global change are necessary to efficiently learn the new trajectory, we tune both C t and ω of the DMPs during the learning process. In this part, we investigated whether such a choice indeed improved the learning performance. We compared the learning performance of four choices: 1) ADMP (tuning both C t and ω); 2) tuning C t ; 3) tuning ω; 4) vanilla DMP (tuning neither C t nor ω), on A (2) , A (3) , and A (4) tasks. The learning curves are shown in Figure 12. The result showed that ADMP could guarantee both the learning speed and the stability on new tasks. Although separately tuning C t or ω could also obtain good performances on some tasks, it depended on the tasks so that it was less universal than ADMP. Also, the vanilla DMPs hardly took effect without parameter tuning through RL, which meant that the intrinsic compliance of the controller could not tackle new tasks effectively.</p>
<p>Effects of the number of demonstrated trajectories</p>
<p>In our framework, the skill policy plays an important role to generate the nominal trajectory whose quality affects the learning performance. Therefore, we investigated how the number of demonstrated trajectories to train the skill policy would affect the learning results. We tested three numbers, n 1, 5, 10, and plotted the results as shown in Figure 13.</p>
<p>When there was only a single trajectory, the learning performance was poor because it was difficult for the skill policy trained with limited data to handle unseen states during the learning process. However, when there were 10 trajectories, the large amount of data conversely confused the skill policy because of the high redundancy so that the performance was unstable. Therefore, we chose 5 as the optimal number of demonstrated trajectories, and all the results in the Evaluation on simulated environment and Experiments on a real robot sections 5-2 and 5-3 were produced using this number.</p>
<p>Effects of the modular learning strategy</p>
<p>As mentioned in the Modular learning strategy section, we utilized a modular learning strategy for the learning of ADMP parameters assuming the curse of dimensionality would lower the performance of RL. Table 2 shows the number of parameters to tune in the learning process. First, following the parameter selection of Wang et al. (2021), we used six parameters for the position/orientation command, one K p p parameter for the PD control, one K f p parameter for the PI control, and six parameters for the selection matrix, S. Then, we assigned the coupling terms, C t , and the forcing term weights, ω, six parameters, respectively, which were used to adjust the trajectory in the six DoFs. Therefore, there were, in total, 26 parameters for different functional components involved in the learning process. Under the modular strategy in Algorithm 1, the number of parameters was reduced to 14 by fixing the 12 DMP parameters when a promising trajectory was found, which was assumed to be more robust than tuning all the 26 parameters simultaneously.  To verify this assumption, we compared the modular learning with the end-to-end (E2E) learning as shown in Figure 14. We used 10 demonstrated trajectories for each task in this comparison. From the results, we found that it was hard for the E2E learning to converge, while the modular learning possessed relatively higher learning speed. It indicated that modular learning was more suitable for our framework than E2E learning when there were large numbers of parameters with different functions to tune.</p>
<p>CONCLUSION</p>
<p>In this work, we propose an adaptive robotic imitation framework for the hybrid trajectory and force learning of complex contact-rich insertion tasks. The framework is composed of learning the nominal trajectory through a combination of IL and RL, and learning the force control policy through an RL-based force controller. We highlight the use of the adaptive DMPs (ADMP), where the coupling terms and the weights of forcing terms in the DMP formulation are learned through RL to effectively adapt the trajectory profile of a single task to new tasks with topologically similar trajectories, which alleviates human repetitive demonstration burdens.</p>
<p>The experimental results show that the proposed framework is comparably sample efficient as a framework using explicitly demonstrated trajectories, has good generalizability among different instances in a task class, and is qualified for the safety requirement by lowering the chance of collision during the training sessions compared with the model-free RL approach. Moreover, the ablation studies show that a proper number of demonstrated trajectories and the modular learning strategy play vital roles in the proposed framework, which affects the speed and the stability of the learning process.</p>
<p>From the experimental results on the real hardware, we also found that the topological similarity of trajectories could affect the learning speed. Therefore, it may improve the efficacy of adapting the DMP parameters if we can represent new trajectories topologically close to the previous ones, and it remains an interesting issue for our future research.</p>
<p>DATA AVAILABILITY STATEMENT</p>
<p>The raw data supporting the conclusion of this article will be made available by the authors, without undue reservation.</p>
<p>AUTHOR CONTRIBUTIONS</p>
<p>YW formulated the methodology. YW and CB-H provided the software. YW performed the investigation. YW wrote the original draft. YW and CB-H reviewed and edited the manuscript. WW and KH supervised the study, and KH was in charge of the project administration. KH acquired the funding.</p>
<p>FIGURE 3 |
3The insertion trajectory of A (1) .Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363</p>
<p>FIGURE 4 |
4Comparison of different resulting trajectories by changing different components of the DMPs formulation. Green line is the original sine wave trajectory. Blue star and red star are the original goal and the new goal, respectively.</p>
<p>FIGURE 5 |
5The simulated environment in Gazebo and the real experiment environment with a UR3e robotic arm. We show the setup for A (2) task where L is directly attached to the robot.</p>
<p>FIGURE 6 |
6Learning curves of the training sessions on A (2) task with frameworks using different trajectory learning methods: Adaptive DMPs (ADMP), demonstrated trajectory (DEMO), and RL without trajectory learning (w/o). The red dashed line represents the near-optimal reward. Frontiers in Robotics and AI | www.frontiersin.RL-based controller proposed in our previous work (Beltran-Hernandez et al., 2020) is responsible for learning the proper force control policy decided by the gain parameters of the controller, a cp , as well as the position/orientation commands of EEF, a p . The RL-based controller consists of an RL agent and a parallel position/force controller. The parallel position/force controller includes a proportional derivative (PD) controller generating part of the movement command, p p c , based on the position feedback, and a proportional integral (PI) controller adjusting the movement command by p f c according to the force feedback.</p>
<p>PD proportional, PD derivative, PI proportional, and PI integral gains, respectively, and S diag(s 1 , s 2 , s 3 , s 4 , s 5 , s 6 ), s n ∈ [0, 1]</p>
<p>FIGURE 8 |
8Collision percentage during the training sessions.</p>
<p>FIGURE 9 |
9Sim-to-real tasks A (2) , A (3) , and A (4) and their learning curves. The L objects are directly attached to the robot.Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363 9</p>
<p>FIGURE 11 |
11Distance errors, pitch angle errors, and force/torque data of a USB insertion (left) and a plug insertion (right) using their initial/learned policies. The error values have been mapped to a range of [1, 0] and the force/torque values have been mapped to a range of [−1, 1]. Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363</p>
<p>FIGURE 12 |
12Effects of the DMPs components on the learning performance of A (2) (left), A (3) (middle), and A (4) (right) tasks.FIGURE 13 | Effects of the number of demonstrated trajectories on the learning performance of A (2) (left), A (3) (middle), and A (4) (right) tasks.</p>
<p>TABLE 1 |
1Performance on the two real assembly tasks.Task 
Metrics 
Start pose </p>
<p>Pose 1 
Pose 2 
Pose 3 
Pose 4 </p>
<p>USB 
Success rate 
0.95 
0.90 
1.0 
0.90 
Avg. steps 
172.0 
156.1 
120.5 
113.6 
Plug 
Success rate 
1.0 
1.0 
0.90 
0.90 
Avg. steps 
89.3 
176.7 
131.2 
317.8 </p>
<p>TABLE 2 |
2Action space of the learning process.Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363Parameters 
Pose 
Controller 
DMPs </p>
<p>PD 
PI 
S 
C t 
ω </p>
<p>Number 
6 
1 
1 
6 
6 
6 </p>
<p>Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363
ACKNOWLEDGMENTSThe first author would like to acknowledge the financial support from the China Scholarship Council Postgraduate Scholarship Grant 201806120019.SUPPLEMENTARY MATERIALThe Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/frobt.2021.777363/ full#supplementary-material Conflict of Interest: The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.Publisher's Note: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors, and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.Copyright © 2022 Wang, Beltran-Hernandez, Wan and Harada. This is an openaccess article distributed under the terms of the Creative Commons AttributionLicense (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.
Adaptation of Manipulation Skills in Physical Contact with the Environment to Reference Force Profiles. F J Abu-Dakka, B Nemec, J A Jørgensen, T R Savarimuthu, N Krüger, A Ude, 10.1007/s10514-015-9435-2Auton. Robot. 39Abu-Dakka, F. J., Nemec, B., Jørgensen, J. A., Savarimuthu, T. R., Krüger, N., and Ude, A. (2015). Adaptation of Manipulation Skills in Physical Contact with the Environment to Reference Force Profiles. Auton. Robot 39, 199-217. doi:10.1007/s10514-015-9435-2</p>
<p>Solving Peg-In-Hole Tasks by Human Demonstration and Exception Strategies. F J Abu-Dakka, B Nemec, A Kramberger, A G Buch, N Krüger, A Ude, Ind. Robot: Int. J. Abu-Dakka, F. J., Nemec, B., Kramberger, A., Buch, A. G., Krüger, N., and Ude, A. (2014). Solving Peg-In-Hole Tasks by Human Demonstration and Exception Strategies. Ind. Robot: Int. J.</p>
<p>FIGURE 14 | Effects of the modular learning and the end-to-end (E2E) learning on the learning performance of A (2) (left). 8777363A (3) (middle), and A (4) (right) tasks. Frontiers in Robotics and AI | wwwFIGURE 14 | Effects of the modular learning and the end-to-end (E2E) learning on the learning performance of A (2) (left), A (3) (middle), and A (4) (right) tasks. Frontiers in Robotics and AI | www.frontiersin.org January 2022 | Volume 8 | Article 777363</p>
<p>K Asadi, D Misra, S Kim, M L Littman, arXiv:1905.13320Combating the Compounding-Error Problem with a Multi-step Model. arXiv preprintAsadi, K., Misra, D., Kim, S., and Littman, M. L. (2019). Combating the Compounding-Error Problem with a Multi-step Model. arXiv preprint arXiv:1905.13320.</p>
<p>Robot Learning from Demonstration. C G Atkeson, S Schaal, Int. Conf. Machine Learn. (Citeseer). 97Atkeson, C. G., and Schaal, S. (1997). Robot Learning from Demonstration. Int. Conf. Machine Learn. (Citeseer) 97, 12-20.</p>
<p>Learning Force Control for Contact-Rich Manipulation Tasks with Rigid Position-Controlled Robots. C C Beltran-Hernandez, D Petit, I G Ramirez-Alpizar, T Nishi, S Kikuchi, T Matsubara, 10.1109/lra.2020.3010739IEEE Robot. Autom. Lett. 5Beltran-Hernandez, C. C., Petit, D., Ramirez-Alpizar, I. G., Nishi, T., Kikuchi, S., Matsubara, T., et al. (2020). Learning Force Control for Contact-Rich Manipulation Tasks with Rigid Position-Controlled Robots. IEEE Robot. Autom. Lett. 5, 5709-5716. doi:10.1109/lra.2020.3010739</p>
<p>The Parallel Approach to Force/position Control of Robotic Manipulators. S Chiaverini, L Sciavicco, 10.1109/70.246048IEEE Trans. Robot. Automat. 9Chiaverini, S., and Sciavicco, L. (1993). The Parallel Approach to Force/position Control of Robotic Manipulators. IEEE Trans. Robot. Automat. 9, 361-373. doi:10.1109/70.246048</p>
<p>Movement Reproduction and Obstacle Avoidance with Dynamic Movement Primitives and Potential fields. Dae-Hyung Park, D Hoffmann, H Pastor, P Schaal, S , 10.1109/ICHR.2008.4755937Humanoids 2008 -8th IEEE-RAS International Conference on Humanoid Robots. Dae-Hyung Park, D., Hoffmann, H., Pastor, P., and Schaal, S. (2008). Movement Reproduction and Obstacle Avoidance with Dynamic Movement Primitives and Potential fields. In Humanoids 2008 -8th IEEE-RAS International Conference on Humanoid Robots. 91-98. doi:10.1109/ICHR.2008.4755937</p>
<p>Active Reward Learning with a Novel Acquisition Function. C Daniel, O Kroemer, M Viering, J Metz, J Peters, 10.1007/s10514-015-9454-zAuton. Robot. 39Daniel, C., Kroemer, O., Viering, M., Metz, J., and Peters, J. (2015). Active Reward Learning with a Novel Acquisition Function. Auton. Robot 39, 389-405. doi:10.1007/s10514-015-9454-z</p>
<p>T Davchev, K S Luck, M Burke, F Meier, S Schaal, S Ramamoorthy, arXiv:2008.07682Residual Learning from Demonstration. arXiv preprintDavchev, T., Luck, K. S., Burke, M., Meier, F., Schaal, S., and Ramamoorthy, S. (2020). Residual Learning from Demonstration. arXiv preprint arXiv: 2008.07682.</p>
<p>Goal-conditioned Imitation Learning. Y Ding, C Florensa, P Abbeel, M Phielipp, Advances in Neural Information Processing Systems. Ding, Y., Florensa, C., Abbeel, P., and Phielipp, M. (2019). "Goal-conditioned Imitation Learning," in Advances in Neural Information Processing Systems, 15298-15309.</p>
<p>On-line Learning and Modulation of Periodic Movements with Nonlinear Dynamical Systems. A Gams, A J Ijspeert, S Schaal, J Lenarčič, Gams, A., Ijspeert, A. J., Schaal, S., and Lenarčič, J. (2009). On-line Learning and Modulation of Periodic Movements with Nonlinear Dynamical Systems.</p>
<p>. 10.1007/s10514-009-9118-yAuton. Robot. 27Auton. Robot 27, 3-23. doi:10.1007/s10514-009-9118-y</p>
<p>Dmp++: Overcoming Some Drawbacks of Dynamic Movement Primitives. M Ginesi, N Sansonetto, P Fiorini, arXiv:1908.10608arXiv preprintGinesi, M., Sansonetto, N., and Fiorini, P. (2019). Dmp++: Overcoming Some Drawbacks of Dynamic Movement Primitives. arXiv preprint arXiv: 1908.10608.</p>
<p>Limits of End-To-End Learning. T Glasmachers, PMLRAsian Conference on Machine Learning. SeoulGlasmachers, T. (2017). Limits of End-To-End Learning. In Asian Conference on Machine Learning (Seoul: PMLR), 17-32.</p>
<p>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. T Haarnoja, A Zhou, P Abbeel, S Levine, Int. Conf. Machine Learn. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft Actor-Critic: Off- Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. Int. Conf. Machine Learn., 1861-1870.</p>
<p>Biologically-inspired Dynamical Systems for Movement Generation: Automatic Real-Time Goal Adaptation and Obstacle Avoidance. H Hoffmann, P Pastor, D.-H Park, S Schaal, 10.1109/ROBOT.2009.51524232009 IEEE International Conference on Robotics and Automation. Hoffmann, H., Pastor, P., Park, D.-H., and Schaal, S. (2009). Biologically-inspired Dynamical Systems for Movement Generation: Automatic Real-Time Goal Adaptation and Obstacle Avoidance. In 2009 IEEE International Conference on Robotics and Automation. 2587-2592. doi:10.1109/ROBOT.2009.5152423</p>
<p>. Y Hu, H Su, J Fu, H R Karimi, G Ferrigno, E De Momi, Hu, Y., Su, H., Fu, J., Karimi, H. R., Ferrigno, G., De Momi, E., et al. (2020).</p>
<p>Nonlinear Model Predictive Control for mobile Medical Robot Using Neural Optimization. IEEE Trans. Ind. Elect. Nonlinear Model Predictive Control for mobile Medical Robot Using Neural Optimization. IEEE Trans. Ind. Elect.</p>
<p>Evolution Strategies Learning with Variable Impedance Control for Grasping under Uncertainty. Y Hu, X Wu, P Geng, Li , Z , IEEE Trans. Ind. Elect. 66Hu, Y., Wu, X., Geng, P., and Li, Z. (2018). Evolution Strategies Learning with Variable Impedance Control for Grasping under Uncertainty. IEEE Trans. Ind. Elect. 66, 7788-7799.</p>
<p>. A J Ijspeert, J Nakanishi, H Hoffmann, P Pastor, S Schaal, Ijspeert, A. J., Nakanishi, J., Hoffmann, H., Pastor, P., and Schaal, S. (2013).</p>
<p>Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors. 10.1162/neco_a_00393Neural Comput. 25Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors. Neural Comput. 25, 328-373. doi:10.1162/neco_a_00393</p>
<p>Deep Reinforcement Learning for High Precision Assembly Tasks. T Inoue, G De Magistris, A Munawar, T Yokoya, R Tachibana, Inoue, T., De Magistris, G., Munawar, A., Yokoya, T., and Tachibana, R. (2017). Deep Reinforcement Learning for High Precision Assembly Tasks. In 2017</p>
<p>Ieee/Rsj, Int, Conf. on Intelligent Robots and Systems. IEEEIEEE/RSJ Int. Conf. on Intelligent Robots and Systems. IEEE, 819-825.</p>
<p>Residual Reinforcement Learning for Robot Control. T Johannink, S Bahl, A Nair, J Luo, A Kumar, M Loskyll, 10.1109/ICRA.2019.8794127doi:10.1109/ ICRA.2019.87941272019 International Conference on Robotics and Automation. Johannink, T., Bahl, S., Nair, A., Luo, J., Kumar, A., Loskyll, M., et al. (2019). Residual Reinforcement Learning for Robot Control. In 2019 International Conference on Robotics and Automation. 6023-6029. doi:10.1109/ ICRA.2019.8794127</p>
<p>Learning to Achieve Goals. L P Kaelbling, IJCAI (Providence: Citeseer). Kaelbling, L. P. (1993). "Learning to Achieve Goals," in IJCAI (Providence: Citeseer), 1094-1099.</p>
<p>Policy Search for Motor Primitives in Robotics. J Kober, J R Peters, Adv. Neural Inf. Process. Syst. Kober, J., and Peters, J. R. (2009). Policy Search for Motor Primitives in Robotics. Adv. Neural Inf. Process. Syst., 849-856.</p>
<p>Imitation Learning of Positional and Force Skills Demonstrated via Kinesthetic Teaching and Haptic Input. P Kormushev, S Calinon, D G Caldwell, 10.1163/016918611x558261Adv. Robotics. 25Kormushev, P., Calinon, S., and Caldwell, D. G. (2011). Imitation Learning of Positional and Force Skills Demonstrated via Kinesthetic Teaching and Haptic Input. Adv. Robotics 25, 581-603. doi:10.1163/016918611x558261</p>
<p>End-to-end Training of Deep Visuomotor Policies. S Levine, C Finn, T Darrell, Abbeel , P , J. Machine Learn. Res. 17Levine, S., Finn, C., Darrell, T., and Abbeel, P. (2016). End-to-end Training of Deep Visuomotor Policies. J. Machine Learn. Res. 17, 1334-1373.</p>
<p>Learning Latent Plans from Play. C Lynch, M Khansari, T Xiao, V Kumar, J Tompson, S Levine, Conf. on Robot Learning. Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J., Levine, S., et al. (2019). Learning Latent Plans from Play. In Conf. on Robot Learning.</p>
<p>Towards Learning of Generic Skills for Robotic Manipulation. J H Metzen, A Fabisch, L Senger, J De Gea Fernández, E A Kirchner, 10.1007/s13218-013-0280-1Künstl Intell. 28Metzen, J. H., Fabisch, A., Senger, L., de Gea Fernández, J., and Kirchner, E. A. (2014). Towards Learning of Generic Skills for Robotic Manipulation. Künstl Intell. 28, 15-20. doi:10.1007/s13218-013-0280-1</p>
<p>Towards Bridging the Reality gap between Tensegrity Simulation and Robotic Hardware. B T Mirletz, I.-W Park, R D Quinn, V Sunspiral, IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE. Mirletz, B. T., Park, I.-W., Quinn, R. D., and SunSpiral, V. (2015). Towards Bridging the Reality gap between Tensegrity Simulation and Robotic Hardware. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 5357-5363.</p>
<p>Overcoming Exploration in Reinforcement Learning with Demonstrations. A Nair, B Mcgrew, M Andrychowicz, W Zaremba, Abbeel , P , 2018 IEEE Int. Conf. on Robotics and Automation. IEEENair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P. (2018). Overcoming Exploration in Reinforcement Learning with Demonstrations. In 2018 IEEE Int. Conf. on Robotics and Automation. IEEE, 6292-6299.</p>
<p>Policy Invariance under Reward Transformations: Theory and Application to Reward Shaping. A Y Ng, D Harada, Russell , S , Icml. 99Ng, A. Y., Harada, D., and Russell, S. (1999). Policy Invariance under Reward Transformations: Theory and Application to Reward Shaping. Icml 99, 278-287.</p>
<p>. K Ota, Tf2rl. Available atOta, K. (2020). Tf2rl. Available at: https://github.com/keiohta/tf2rl/.</p>
<p>Learning and Generalization of Motor Skills by Learning from Demonstration. P Pastor, H Hoffmann, T Asfour, S Schaal, IEEE Int. Conf. on Robotics and Automation. IEEEPastor, P., Hoffmann, H., Asfour, T., and Schaal, S. (2009). Learning and Generalization of Motor Skills by Learning from Demonstration. In IEEE Int. Conf. on Robotics and Automation. IEEE, 763-768.</p>
<p>Online Movement Adaptation Based on Previous Sensor Experiences. P Pastor, L Righetti, M Kalakrishnan, S Schaal, 10.1109/IROS.2011.60950592011 IEEE/RSJ International Conference on Intelligent Robots and Systems. Pastor, P., Righetti, L., Kalakrishnan, M., and Schaal, S. (2011). Online Movement Adaptation Based on Previous Sensor Experiences. In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems. 365-371. doi:10.1109/IROS.2011.6095059</p>
<p>Reinforcement Learning of Motor Skills with Policy Gradients. J Peters, S Schaal, 10.1016/j.neunet.2008.02.003Neural networks. 21Peters, J., and Schaal, S. (2008). Reinforcement Learning of Motor Skills with Policy Gradients. Neural networks 21, 682-697. doi:10.1016/j.neunet. 2008.02.003</p>
<p>Learning Coupling Terms for Obstacle Avoidance. A Rai, F Meier, A Ijspeert, S Schaal, 10.1109/HUMANOIDS.2014.7041410doi:10.1109/ HUMANOIDS.2014.7041410IEEE-RAS International Conference on Humanoid Robots. Rai, A., Meier, F., Ijspeert, A., and Schaal, S. (2014). Learning Coupling Terms for Obstacle Avoidance. In 2014 IEEE-RAS International Conference on Humanoid Robots. 512-518. doi:10.1109/ HUMANOIDS.2014.7041410</p>
<p>Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations. A Rajeswaran, V Kumar, A Gupta, G Vezzani, J Schulman, E Todorov, 10.15607/rss.2018.xiv.049Robotics: Sci. Syst. Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E., et al. (2018). Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations. Robotics: Sci. Syst. doi:10.15607/rss.2018.xiv.049</p>
<p>Efficient Reductions for Imitation Learning. S Ross, D Bagnell, Int. Conf. on artificial intelligence and statistics. Ross, S., and Bagnell, D. (2010). Efficient Reductions for Imitation Learning. In Int. Conf. on artificial intelligence and statistics. 661-668.</p>
<p>A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning. S Ross, G Gordon, D Bagnell, Int. Conf. on artificial intelligence and statistics. Ross, S., Gordon, G., and Bagnell, D. (2011). A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning. In Int. Conf. on artificial intelligence and statistics. 627-635.</p>
<p>Active Preference-Based Learning of Reward Functions. D Sadigh, A D Dragan, S Sastry, S A Seshia, 10.15607/rss.2017.xiii.053doi:10.15607/ rss.2017.xiii.053Robotics: Sci. Syst. Sadigh, D., Dragan, A. D., Sastry, S., and Seshia, S. A. (2017). Active Preference- Based Learning of Reward Functions. Robotics: Sci. Syst. doi:10.15607/ rss.2017.xiii.053</p>
<p>Teaching a Robot the Semantics of Assembly Tasks. T R Savarimuthu, A G Buch, C Schlette, N Wantia, J Roßmann, D Martínez, IEEE Trans. Systems, Man. Cybernetics: Syst. 48Savarimuthu, T. R., Buch, A. G., Schlette, C., Wantia, N., Roßmann, J., Martínez, D., et al. (2017). Teaching a Robot the Semantics of Assembly Tasks. IEEE Trans. Systems, Man. Cybernetics: Syst. 48, 670-692.</p>
<p>Universal Value Function Approximators. T Schaul, D Horgan, K Gregor, D Silver, Int. Conf. on Machine Learning. Schaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal Value Function Approximators. In Int. Conf. on Machine Learning. 1312-1320.</p>
<p>A Geometric Approach for Learning Compliant Motions from Demonstration. M Suomalainen, V Kyrki, 10.1109/humanoids.2017.8246961doi:10.1109/ humanoids.2017.82469612017 IEEE-RAS 17th Int. Conf. on Humanoid Robotics (Humanoids). IEEE. Suomalainen, M., and Kyrki, V. (2017). A Geometric Approach for Learning Compliant Motions from Demonstration. In 2017 IEEE-RAS 17th Int. Conf. on Humanoid Robotics (Humanoids). IEEE, 783-790. doi:10.1109/ humanoids.2017.8246961</p>
<p>Learning Sensor Feedback Models from Demonstrations via Phase-Modulated Neural Networks. G Sutanto, Z Su, S Schaal, F Meier, 10.1109/ICRA.2018.84609862018 IEEE International Conference on Robotics and Automation. Sutanto, G., Su, Z., Schaal, S., and Meier, F. (2018). Learning Sensor Feedback Models from Demonstrations via Phase-Modulated Neural Networks. In 2018 IEEE International Conference on Robotics and Automation. 1142-1149. doi:10.1109/ICRA.2018.8460986</p>
<p>Recognizing Assembly Tasks through Human Demonstration. J Takamatsu, K Ogawara, H Kimura, K Ikeuchi, 10.1177/0278364907080736Int. J. Robotics Res. 26Takamatsu, J., Ogawara, K., Kimura, H., and Ikeuchi, K. (2007). Recognizing Assembly Tasks through Human Demonstration. Int. J. Robotics Res. 26, 641-659. doi:10.1177/0278364907080736</p>
<p>Teach Industrial Robots Peg-Hole-Insertion by Human Demonstration. T Tang, H.-C Lin, Y Zhao, Y Fan, W Chen, M Tomizuka, IEEE Int. Conf. on Advanced Intelligent Mechatronics. IEEE. Tang, T., Lin, H.-C., Zhao, Y., Fan, Y., Chen, W., and Tomizuka, M. (2016). Teach Industrial Robots Peg-Hole-Insertion by Human Demonstration. In 2016 IEEE Int. Conf. on Advanced Intelligent Mechatronics. IEEE, 488-494.</p>
<p>Learning Robotic Assembly from Cad. G Thomas, M Chien, A Tamar, J A Ojea, Abbeel , P , 10.1109/icra.2018.8460696IEEE Int. Conf. on Robotics and Automation. IEEE. Thomas, G., Chien, M., Tamar, A., Ojea, J. A., and Abbeel, P. (2018). Learning Robotic Assembly from Cad. In 2018 IEEE Int. Conf. on Robotics and Automation. IEEE, 1-9. doi:10.1109/icra.2018.8460696</p>
<p>Orientation in Cartesian Space Dynamic Movement Primitives. A Ude, B Nemec, T Petric, J Morimoto, 10.1109/ICRA.2014.69072912014 IEEE International Conference on Robotics and Automation. Ude, A., Nemec, B., Petric, T., and Morimoto, J. (2014). Orientation in Cartesian Space Dynamic Movement Primitives. In 2014 IEEE International Conference on Robotics and Automation. 2997-3004. doi:10.1109/ICRA.2014.6907291</p>
<p>Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards. M Vecerik, T Hester, J Scholz, F Wang, O Pietquin, B Piot, the Annual Conf. on Neural Information Processing Systems. Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., et al. (2017). Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards. In the Annual Conf. on Neural Information Processing Systems.</p>
<p>A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning. M Vecerik, O Sushkov, D Barker, T Rothörl, T Hester, J Scholz, 10.1109/ICRA.2019.8794074754-760. doi:10.1109/ ICRA.2019.87940742019 International Conference on Robotics and Automation. Vecerik, M., Sushkov, O., Barker, D., Rothörl, T., Hester, T., and Scholz, J. (2019). A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning. In 2019 International Conference on Robotics and Automation. 754-760. doi:10.1109/ ICRA.2019.8794074</p>
<p>Robotic Imitation of Human Assembly Skills Using Hybrid Trajectory and Force Learning. Y Wang, C C Beltran-Hernandez, W Wan, K Harada, 10.1109/ICRA48506.2021.95616192021 IEEE International Conference on Robotics and Automation. Wang, Y., Beltran-Hernandez, C. C., Wan, W., and Harada, K. (2021). Robotic Imitation of Human Assembly Skills Using Hybrid Trajectory and Force Learning. In 2021 IEEE International Conference on Robotics and Automation. 11278-11284. doi:10.1109/ICRA48506.2021.9561619</p>            </div>
        </div>

    </div>
</body>
</html>
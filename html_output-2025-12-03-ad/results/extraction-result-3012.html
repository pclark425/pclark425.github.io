<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3012 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3012</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3012</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-268819308</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.00459v2.pdf" target="_blank">NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of “42”, we suggest using “2:42” as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3012.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3012.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumeroLogic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NumeroLogic (digit-count prefixed number encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A textual-number reformatting that prefixes each number with its digit count (and for floats the integer and decimal digit counts) so causal LLMs know place value before reading digits and must plan digit count before generating a number.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to NanoGPT and Llama2-7B (causal transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Technique applied to causal transformer language models: experiments include NanoGPT (small transformer trained from scratch with character-level tokenization) and fine-tuning/continued-pretraining of Llama2-7B (pretrained decoder-only transformer) using LoRA for adaptation and added special tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition, subtraction, multi-digit multiplication, floating-point multiplication, sine and square root evaluated on numeric operands (3- to 5-digit experiments and extrapolation tests described).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Provides explicit place-value information up-front (so tokens representing digits are contextualized with their magnitude immediately) and induces a simplified Chain-of-Thought by forcing the model to predict digit-count before digits.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical large improvements in arithmetic accuracy when training/finetuning with the encoding (see performance_metrics). Ablations: operand-only vs result-only encoding experiments; experiments removing end-token or using alternate encodings show reduced improvements; filler-token control shows extra tokens alone do not provide the same benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None direct — authors note some contradictory wording in their ablation descriptions about whether operand- or result-encoding is stronger (see ablation inconsistency), but no evidence showing NumeroLogic harms place-value learning. Also increased token count and latency are practical downsides.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Pre/post-processing text re-encoding of numbers; addition of special tokens (<sn>, <mn>, <en>); used during supervised finetuning and continued self-supervised pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Markedly improves numerical comprehension and generation: acts as explicit place-value representation and a CoT-like prompt to plan digit length before generation, reducing many classes of arithmetic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>NanoGPT supervised joint training: Addition 88.37% -> 99.96%, Subtraction 73.76% -> 97.20%, Multiplication 13.81% -> 28.94%; floating-point sine and sqrt each +~4% (Table 1 summary). Llama2-7B finetuning (5-digit): plain text baselines 99.86% (addition) and 99.60% (subtraction) -> NumeroLogic 100% and 99.93%, and non-saturated tasks show 1–6% absolute gains. Continued pretraining: MMLU 0-shot overall +0.5% (tokens equal); MMLU tasks with numbers +1.16%, without numbers +0.14%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Increased tokenization length (higher token counts per number) increases compute and latency; some arithmetic tasks (e.g., multiplication) remain far from perfect even with gains. Paper notes practical cost and that tests were up to 7B models only.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Framed as bringing LLMs closer to symbolic place-value representations (analogous to humans realizing magnitude) and akin to chain-of-thought prompting, but no direct quantitative human or symbolic-calculator comparison beyond noting algorithmic calculators outperform models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3012.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3012.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NanoGPT arithmetic experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NanoGPT (small transformer) supervised arithmetic experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised multi-task training of a small character-level causal transformer (NanoGPT) to evaluate arithmetic performance with and without NumeroLogic encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NanoGPT (Karpathy style small causal transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small transformer (NanoGPT) trained from scratch with character-level tokenization; single model jointly trained on several arithmetic tasks with datasets of 3K–10K samples per task (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition, subtraction, multiplication (multi-digit), floating-point sine and square-root (operands sampled in specified ranges).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Models struggle because causal reading of digits left-to-right prevents early determination of place-value; NumeroLogic fixes this by prefixing digit counts. Also authors attribute gains partly to CoT-like planning induced by predicting digit counts first.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large accuracy gains when training with NumeroLogic (see performance_metrics). Ablation: applying encoding only to operands or only to results shows both help; encoding both yields highest accuracy (Table 5/ablation). Alternate encodings and filler-token controls tested.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Ablation table in paper contains inconsistent textual claims about whether operand- or result-encoding has stronger effect (text in different sections contradicts), indicating some uncertainty about which component contributes most; no representation probing reported.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Training data re-encoding (NumeroLogic), special single-character tokens for small models ('{' ':' '}') replacing <sn>/<mn>/<en>.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantial accuracy improvements on addition/subtraction and modest to large improvements on multiplication and floating tasks; in joint multi-task training, addition and subtraction become nearly solved.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (Table 1 summary): addition 88.37% -> 99.96%, subtraction 73.76% -> 97.20%, multiplication 13.81% -> 28.94%; sine and sqrt +~4% each. In a separate 3-digit addition ablation table (Table 5) plain 34.20% -> NumeroLogic 35.33% (smaller deltas reported there reflecting different experimental setup).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Multiplication accuracy remains low even after improvement; results depend heavily on experimental setup (dataset, digit ranges), and some experiments show only modest gains, indicating sensitivity to protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human or symbolic baseline reported for NanoGPT experiments beyond qualitative comparisons to algorithmic expectation and references to algorithmic methods in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3012.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3012.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B arithmetic & pretraining experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama2-7B fine-tuning and continued-pretraining with NumeroLogic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning a pretrained Llama2-7B model on arithmetic tasks with NumeroLogic and continuing self-supervised causal pretraining on web text encoded with NumeroLogic to measure downstream effects (including MMLU).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained 7B-parameter decoder-only transformer (Llama2-7B); experiments add special tokens to tokenizer, extend embedding and final linear layer, and fine-tune with LoRA (rank 8) while training new/newly-extended embeddings and final linear in full-rank.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and subtraction (5-digit tests, integers and floats), multiplication (3-digit tests), floating-point sine and square-root (5 decimal places).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Same place-value limitation as causal models; NumeroLogic supplies explicit place-value and CoT-like digit-count planning. Authors also discuss mitigation of positional-embedding over-reliance as a separate mechanism reported in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Fine-tuning yields near-perfect scores on some tasks (addition/subtraction) and consistent improvements on non-saturated tasks (1–6% absolute gains). Continued pretraining on web text encoded with NumeroLogic yields 0-shot MMLU improvements (+0.5% overall; larger in STEM and number-containing tasks). Ablation and filler-token controls show that extra tokens alone do not explain the full benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Plain text baselines on some tasks were already near ceiling (e.g., 99.86% addition), so gains are marginal there; authors acknowledge earlier pretraining without NumeroLogic limits ability to fully predict pretraining-phase benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning with NumeroLogic-encoded datasets; continued self-supervised pretraining (causal next-token prediction) on RefinedWeb with NumeroLogic encoding; LoRA used for efficient adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved arithmetic accuracy across several tasks, perfect or near-perfect for some addition/subtraction tests; small but statistically significant improvement on general language benchmark (MMLU) favoring tasks with numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Llama2-7B fine-tune results (5-digit): plain addition 99.86% -> NumeroLogic 100.00%, plain subtraction 99.60% -> NumeroLogic 99.93%; non-saturated tasks improved by 1–6% absolute. Continued pretraining MMLU overall +0.5%; tasks with numbers +1.16%, without numbers +0.14%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>High baseline performance on some tasks limits measurable improvements; multiplication and other non-trivial numeric tasks still not fully solved. Increased token count from encoding impacts compute/latency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human or symbolic (calculator) baselines reported; authors contrast results qualitatively to algorithmic performance and prior specialized methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3012.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3012.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Place-value reading limitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Place-value ambiguity from left-to-right causal reading</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hypothesis that causal LLMs cannot determine a digit token's place value until the entire number (or its end) is processed, because attention is unidirectional, causing difficulty in arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Causal LLMs (decoder-only transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only causal transformers that attend only to previous tokens, so when processing a digit early in a numeric token they lack future context that determines its magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>All multi-digit arithmetic tasks where digit place-value matters (multi-digit addition, subtraction, multiplication, floats).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Architectural left-to-right causal attention means initial digit tokens are represented without knowledge of how many digits follow; place-value is only resolved later, impairing representational immediacy for arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Motivates NumeroLogic and supported indirectly by gains when digit-count is prefixed (NumeroLogic), showing that giving place-value up-front aids performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct internal-representation probing provided (e.g., attention or activation analyses); improvement with filler random tokens shows reducing positional reliance helps but does not match place-value prefix, indicating multiple interacting factors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>NumeroLogic (digit-count prefix) designed specifically to fix this limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>When place-value is provided up-front, arithmetic performance increases substantially in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>See NumeroLogic and model-specific performance metrics; improvement attributed to resolving place-value ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Hypothesis not directly probed via interpretability tools in the paper; effect size depends on task and model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Analogous to human reading—authors compare to human positional reading limitations and suggest prefixing digit-count is similar to telling the model the magnitude context in advance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3012.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3012.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Positional encoding reliance (Shen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model over-reliance on positional encoding in arithmetic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior finding (Shen et al., 2023) that transformer models trained on arithmetic tend to depend heavily on positional encodings, causing failures on inputs with different digit distributions; discussed as related work and motivating certain controls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Positional description matters for transformers arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer models studied in Shen et al. and related literature</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Findings concern transformer-based LLMs trained on arithmetic tasks that can exploit positional cues rather than genuine algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic extrapolation and multi-digit arithmetic where positional patterns differ between train and test.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Models memorize or exploit positional encodings/patterns instead of learning digit-place arithmetic algorithms, leading to brittle generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited prior work showing failures when training distributions change (e.g., adding whitespaces or unusual digit lengths), and the paper's filler-token experiments referencing that random white-space insertion reduces positional reliance with some benefit but less than NumeroLogic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>NumeroLogic yields stronger improvements than white-space filler intervention, suggesting positional-reliance is only one factor; no internal-probing shown in this paper to directly measure positional-reliance in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Related interventions: inserting filler white-space tokens (Shen et al.) or random token insertions to break positional patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Random white-space token insertion can help generalization but is less effective than providing explicit place-value information (NumeroLogic). In this paper, filler-token controls matched plain format (no benefit) while random placement provided modest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In the paper's control experiments (Table 7 summary), filler white-space tokens alone are comparable to plain format and do not reproduce NumeroLogic gains; random white-space insertion gives modest benefit but less than NumeroLogic.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Positional-reliance interventions help but don't fully solve arithmetic errors; reliance on positional encoding causes poor extrapolation to rare digit-length distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Compared to algorithmic arithmetic, positional-reliance is a brittle shortcut unlike human algorithmic reasoning or symbolic calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3012.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3012.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tokenization and digit-splitting issues</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tokenization effects on numeric representation (digit-splitting and chunking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observations that subword tokenization (BPE) and model-specific tokenizer design (digit-per-token, 3-digit tokens, etc.) influence numerical behavior and arithmetic capability in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various foundation models referenced (PaLM, Llama, Mistral, GPT-3.5/4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Different models handle numeric tokenization differently: some force one-digit-per-token (PaLM, Llama, Mistral), GPT-3.5/GPT-4 use up-to-3-digit tokens; these choices influence how numbers are represented internally.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Any arithmetic tasks where tokenization affects internal digit boundaries (multi-digit arithmetic, extrapolation to unseen numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Tokenization changes the granularity of numeric input and thus the representations available to the model; unintuitive splits can harm arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited works and experiments (e.g., Singh & Strouse) showing that splitting large numbers into fixed-size chunks left-to-right can undermine arithmetic, and that adding commas to control splitting helps. The paper references these as motivation for NumeroLogic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>This paper does not present tokenization probing experiments itself; evidence is indirect via citations and qualitative argument.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>NumeroLogic operates at text pre/post-processing to be compatible with arbitrary tokenizers; other interventions cited include inserting commas or forcing per-digit tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>NumeroLogic mitigates some tokenization issues by giving explicit digit-count metadata independent of tokenizer chunking; other tokenizer-level fixes have mixed success per cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No direct numeric performance numbers attributed solely to tokenization choices in this paper beyond cited literature references.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Unintuitive token splits can produce non-uniform training statistics and hurt extrapolation to unseen digit-lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Tokenization is a modeling/engineering artifact with no analogue in symbolic calculators; fixing it can make LLM inputs more aligned with symbolic arithmetic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3012.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3012.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/training approach that elicits multi-step reasoning traces from LLMs; referenced as related mechanism that helps arithmetic by forcing step-wise algorithmic thinking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs in CoT literature</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT is a prompting technique (and sometimes training signal) prompting models to produce intermediate reasoning steps prior to final answer; originally demonstrated on larger LLMs for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic and multi-step reasoning tasks where intermediate steps benefit correctness (e.g., multi-digit multiplication, multi-step word problems).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>By asking the model to produce intermediate steps, CoT encourages the model to plan and perform algorithmic computations rather than directly pattern-match to outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited CoT literature shows improvements on complex reasoning tasks. Paper argues NumeroLogic induces a simplified CoT by forcing digit-count prediction prior to digits.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Paper notes CoT benefits may partly come from extra tokens enabling more computation/memory (Pfau et al.), but filler-token control indicates extra tokens alone are insufficient—format/content matters.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting for chain-of-thought or training to produce intermediate steps; NumeroLogic acts as a weaker, implicit CoT (predict digit count first).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>CoT in prior work improves reasoning and arithmetic in many contexts; NumeroLogic's digit-count planning yields similar benefits for number generation/representation in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numerical CoT results provided in this paper beyond referencing CoT literature; NumeroLogic showed MMLU and arithmetic gains attributed in part to CoT-like behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>CoT can increase token usage and latency; some benefits may be artifact of extra tokens rather than genuine algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>CoT is likened to externalizing intermediate human reasoning steps; symbolic algorithms remain exact while CoT-based outputs are probabilistic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Positional description matters for transformers arithmetic <em>(Rating: 2)</em></li>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tokenization counts: the impact of tokenization on arithmetic in frontier llms <em>(Rating: 2)</em></li>
                <li>Let's think dot by dot: Hidden computation in transformer language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3012",
    "paper_id": "paper-268819308",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "NumeroLogic",
            "name_full": "NumeroLogic (digit-count prefixed number encoding)",
            "brief_description": "A textual-number reformatting that prefixes each number with its digit count (and for floats the integer and decimal digit counts) so causal LLMs know place value before reading digits and must plan digit count before generating a number.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to NanoGPT and Llama2-7B (causal transformers)",
            "model_description": "Technique applied to causal transformer language models: experiments include NanoGPT (small transformer trained from scratch with character-level tokenization) and fine-tuning/continued-pretraining of Llama2-7B (pretrained decoder-only transformer) using LoRA for adaptation and added special tokens.",
            "arithmetic_task_type": "Multi-digit addition, subtraction, multi-digit multiplication, floating-point multiplication, sine and square root evaluated on numeric operands (3- to 5-digit experiments and extrapolation tests described).",
            "reported_mechanism": "Provides explicit place-value information up-front (so tokens representing digits are contextualized with their magnitude immediately) and induces a simplified Chain-of-Thought by forcing the model to predict digit-count before digits.",
            "evidence_for_mechanism": "Empirical large improvements in arithmetic accuracy when training/finetuning with the encoding (see performance_metrics). Ablations: operand-only vs result-only encoding experiments; experiments removing end-token or using alternate encodings show reduced improvements; filler-token control shows extra tokens alone do not provide the same benefit.",
            "evidence_against_mechanism": "None direct — authors note some contradictory wording in their ablation descriptions about whether operand- or result-encoding is stronger (see ablation inconsistency), but no evidence showing NumeroLogic harms place-value learning. Also increased token count and latency are practical downsides.",
            "intervention_type": "Pre/post-processing text re-encoding of numbers; addition of special tokens (&lt;sn&gt;, &lt;mn&gt;, &lt;en&gt;); used during supervised finetuning and continued self-supervised pretraining.",
            "effect_of_intervention": "Markedly improves numerical comprehension and generation: acts as explicit place-value representation and a CoT-like prompt to plan digit length before generation, reducing many classes of arithmetic errors.",
            "performance_metrics": "NanoGPT supervised joint training: Addition 88.37% -&gt; 99.96%, Subtraction 73.76% -&gt; 97.20%, Multiplication 13.81% -&gt; 28.94%; floating-point sine and sqrt each +~4% (Table 1 summary). Llama2-7B finetuning (5-digit): plain text baselines 99.86% (addition) and 99.60% (subtraction) -&gt; NumeroLogic 100% and 99.93%, and non-saturated tasks show 1–6% absolute gains. Continued pretraining: MMLU 0-shot overall +0.5% (tokens equal); MMLU tasks with numbers +1.16%, without numbers +0.14%.",
            "notable_failure_modes": "Increased tokenization length (higher token counts per number) increases compute and latency; some arithmetic tasks (e.g., multiplication) remain far from perfect even with gains. Paper notes practical cost and that tests were up to 7B models only.",
            "comparison_to_humans_or_symbolic": "Framed as bringing LLMs closer to symbolic place-value representations (analogous to humans realizing magnitude) and akin to chain-of-thought prompting, but no direct quantitative human or symbolic-calculator comparison beyond noting algorithmic calculators outperform models.",
            "uuid": "e3012.0",
            "source_info": {
                "paper_title": "NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "NanoGPT arithmetic experiments",
            "name_full": "NanoGPT (small transformer) supervised arithmetic experiments",
            "brief_description": "Supervised multi-task training of a small character-level causal transformer (NanoGPT) to evaluate arithmetic performance with and without NumeroLogic encoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NanoGPT (Karpathy style small causal transformer)",
            "model_description": "Small transformer (NanoGPT) trained from scratch with character-level tokenization; single model jointly trained on several arithmetic tasks with datasets of 3K–10K samples per task (see paper).",
            "arithmetic_task_type": "Addition, subtraction, multiplication (multi-digit), floating-point sine and square-root (operands sampled in specified ranges).",
            "reported_mechanism": "Models struggle because causal reading of digits left-to-right prevents early determination of place-value; NumeroLogic fixes this by prefixing digit counts. Also authors attribute gains partly to CoT-like planning induced by predicting digit counts first.",
            "evidence_for_mechanism": "Large accuracy gains when training with NumeroLogic (see performance_metrics). Ablation: applying encoding only to operands or only to results shows both help; encoding both yields highest accuracy (Table 5/ablation). Alternate encodings and filler-token controls tested.",
            "evidence_against_mechanism": "Ablation table in paper contains inconsistent textual claims about whether operand- or result-encoding has stronger effect (text in different sections contradicts), indicating some uncertainty about which component contributes most; no representation probing reported.",
            "intervention_type": "Training data re-encoding (NumeroLogic), special single-character tokens for small models ('{' ':' '}') replacing &lt;sn&gt;/&lt;mn&gt;/&lt;en&gt;.",
            "effect_of_intervention": "Substantial accuracy improvements on addition/subtraction and modest to large improvements on multiplication and floating tasks; in joint multi-task training, addition and subtraction become nearly solved.",
            "performance_metrics": "Reported (Table 1 summary): addition 88.37% -&gt; 99.96%, subtraction 73.76% -&gt; 97.20%, multiplication 13.81% -&gt; 28.94%; sine and sqrt +~4% each. In a separate 3-digit addition ablation table (Table 5) plain 34.20% -&gt; NumeroLogic 35.33% (smaller deltas reported there reflecting different experimental setup).",
            "notable_failure_modes": "Multiplication accuracy remains low even after improvement; results depend heavily on experimental setup (dataset, digit ranges), and some experiments show only modest gains, indicating sensitivity to protocol.",
            "comparison_to_humans_or_symbolic": "No direct human or symbolic baseline reported for NanoGPT experiments beyond qualitative comparisons to algorithmic expectation and references to algorithmic methods in related work.",
            "uuid": "e3012.1",
            "source_info": {
                "paper_title": "NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Llama2-7B arithmetic & pretraining experiments",
            "name_full": "Llama2-7B fine-tuning and continued-pretraining with NumeroLogic",
            "brief_description": "Fine-tuning a pretrained Llama2-7B model on arithmetic tasks with NumeroLogic and continuing self-supervised causal pretraining on web text encoded with NumeroLogic to measure downstream effects (including MMLU).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B",
            "model_description": "Pretrained 7B-parameter decoder-only transformer (Llama2-7B); experiments add special tokens to tokenizer, extend embedding and final linear layer, and fine-tune with LoRA (rank 8) while training new/newly-extended embeddings and final linear in full-rank.",
            "arithmetic_task_type": "Addition and subtraction (5-digit tests, integers and floats), multiplication (3-digit tests), floating-point sine and square-root (5 decimal places).",
            "reported_mechanism": "Same place-value limitation as causal models; NumeroLogic supplies explicit place-value and CoT-like digit-count planning. Authors also discuss mitigation of positional-embedding over-reliance as a separate mechanism reported in prior work.",
            "evidence_for_mechanism": "Fine-tuning yields near-perfect scores on some tasks (addition/subtraction) and consistent improvements on non-saturated tasks (1–6% absolute gains). Continued pretraining on web text encoded with NumeroLogic yields 0-shot MMLU improvements (+0.5% overall; larger in STEM and number-containing tasks). Ablation and filler-token controls show that extra tokens alone do not explain the full benefit.",
            "evidence_against_mechanism": "Plain text baselines on some tasks were already near ceiling (e.g., 99.86% addition), so gains are marginal there; authors acknowledge earlier pretraining without NumeroLogic limits ability to fully predict pretraining-phase benefits.",
            "intervention_type": "Fine-tuning with NumeroLogic-encoded datasets; continued self-supervised pretraining (causal next-token prediction) on RefinedWeb with NumeroLogic encoding; LoRA used for efficient adaptation.",
            "effect_of_intervention": "Improved arithmetic accuracy across several tasks, perfect or near-perfect for some addition/subtraction tests; small but statistically significant improvement on general language benchmark (MMLU) favoring tasks with numbers.",
            "performance_metrics": "Llama2-7B fine-tune results (5-digit): plain addition 99.86% -&gt; NumeroLogic 100.00%, plain subtraction 99.60% -&gt; NumeroLogic 99.93%; non-saturated tasks improved by 1–6% absolute. Continued pretraining MMLU overall +0.5%; tasks with numbers +1.16%, without numbers +0.14%.",
            "notable_failure_modes": "High baseline performance on some tasks limits measurable improvements; multiplication and other non-trivial numeric tasks still not fully solved. Increased token count from encoding impacts compute/latency.",
            "comparison_to_humans_or_symbolic": "No direct human or symbolic (calculator) baselines reported; authors contrast results qualitatively to algorithmic performance and prior specialized methods.",
            "uuid": "e3012.2",
            "source_info": {
                "paper_title": "NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Place-value reading limitation",
            "name_full": "Place-value ambiguity from left-to-right causal reading",
            "brief_description": "Hypothesis that causal LLMs cannot determine a digit token's place value until the entire number (or its end) is processed, because attention is unidirectional, causing difficulty in arithmetic reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Causal LLMs (decoder-only transformers)",
            "model_description": "Decoder-only causal transformers that attend only to previous tokens, so when processing a digit early in a numeric token they lack future context that determines its magnitude.",
            "arithmetic_task_type": "All multi-digit arithmetic tasks where digit place-value matters (multi-digit addition, subtraction, multiplication, floats).",
            "reported_mechanism": "Architectural left-to-right causal attention means initial digit tokens are represented without knowledge of how many digits follow; place-value is only resolved later, impairing representational immediacy for arithmetic.",
            "evidence_for_mechanism": "Motivates NumeroLogic and supported indirectly by gains when digit-count is prefixed (NumeroLogic), showing that giving place-value up-front aids performance.",
            "evidence_against_mechanism": "No direct internal-representation probing provided (e.g., attention or activation analyses); improvement with filler random tokens shows reducing positional reliance helps but does not match place-value prefix, indicating multiple interacting factors.",
            "intervention_type": "NumeroLogic (digit-count prefix) designed specifically to fix this limitation.",
            "effect_of_intervention": "When place-value is provided up-front, arithmetic performance increases substantially in experiments.",
            "performance_metrics": "See NumeroLogic and model-specific performance metrics; improvement attributed to resolving place-value ambiguity.",
            "notable_failure_modes": "Hypothesis not directly probed via interpretability tools in the paper; effect size depends on task and model scale.",
            "comparison_to_humans_or_symbolic": "Analogous to human reading—authors compare to human positional reading limitations and suggest prefixing digit-count is similar to telling the model the magnitude context in advance.",
            "uuid": "e3012.3",
            "source_info": {
                "paper_title": "NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Positional encoding reliance (Shen et al.)",
            "name_full": "Model over-reliance on positional encoding in arithmetic tasks",
            "brief_description": "Prior finding (Shen et al., 2023) that transformer models trained on arithmetic tend to depend heavily on positional encodings, causing failures on inputs with different digit distributions; discussed as related work and motivating certain controls.",
            "citation_title": "Positional description matters for transformers arithmetic",
            "mention_or_use": "mention",
            "model_name": "Transformer models studied in Shen et al. and related literature",
            "model_description": "Findings concern transformer-based LLMs trained on arithmetic tasks that can exploit positional cues rather than genuine algorithmic computation.",
            "arithmetic_task_type": "Arithmetic extrapolation and multi-digit arithmetic where positional patterns differ between train and test.",
            "reported_mechanism": "Models memorize or exploit positional encodings/patterns instead of learning digit-place arithmetic algorithms, leading to brittle generalization.",
            "evidence_for_mechanism": "Cited prior work showing failures when training distributions change (e.g., adding whitespaces or unusual digit lengths), and the paper's filler-token experiments referencing that random white-space insertion reduces positional reliance with some benefit but less than NumeroLogic.",
            "evidence_against_mechanism": "NumeroLogic yields stronger improvements than white-space filler intervention, suggesting positional-reliance is only one factor; no internal-probing shown in this paper to directly measure positional-reliance in their experiments.",
            "intervention_type": "Related interventions: inserting filler white-space tokens (Shen et al.) or random token insertions to break positional patterns.",
            "effect_of_intervention": "Random white-space token insertion can help generalization but is less effective than providing explicit place-value information (NumeroLogic). In this paper, filler-token controls matched plain format (no benefit) while random placement provided modest gains.",
            "performance_metrics": "In the paper's control experiments (Table 7 summary), filler white-space tokens alone are comparable to plain format and do not reproduce NumeroLogic gains; random white-space insertion gives modest benefit but less than NumeroLogic.",
            "notable_failure_modes": "Positional-reliance interventions help but don't fully solve arithmetic errors; reliance on positional encoding causes poor extrapolation to rare digit-length distributions.",
            "comparison_to_humans_or_symbolic": "Compared to algorithmic arithmetic, positional-reliance is a brittle shortcut unlike human algorithmic reasoning or symbolic calculators.",
            "uuid": "e3012.4",
            "source_info": {
                "paper_title": "NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Tokenization and digit-splitting issues",
            "name_full": "Tokenization effects on numeric representation (digit-splitting and chunking)",
            "brief_description": "Observations that subword tokenization (BPE) and model-specific tokenizer design (digit-per-token, 3-digit tokens, etc.) influence numerical behavior and arithmetic capability in LLMs.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Various foundation models referenced (PaLM, Llama, Mistral, GPT-3.5/4)",
            "model_description": "Different models handle numeric tokenization differently: some force one-digit-per-token (PaLM, Llama, Mistral), GPT-3.5/GPT-4 use up-to-3-digit tokens; these choices influence how numbers are represented internally.",
            "arithmetic_task_type": "Any arithmetic tasks where tokenization affects internal digit boundaries (multi-digit arithmetic, extrapolation to unseen numbers).",
            "reported_mechanism": "Tokenization changes the granularity of numeric input and thus the representations available to the model; unintuitive splits can harm arithmetic reasoning.",
            "evidence_for_mechanism": "Cited works and experiments (e.g., Singh & Strouse) showing that splitting large numbers into fixed-size chunks left-to-right can undermine arithmetic, and that adding commas to control splitting helps. The paper references these as motivation for NumeroLogic.",
            "evidence_against_mechanism": "This paper does not present tokenization probing experiments itself; evidence is indirect via citations and qualitative argument.",
            "intervention_type": "NumeroLogic operates at text pre/post-processing to be compatible with arbitrary tokenizers; other interventions cited include inserting commas or forcing per-digit tokens.",
            "effect_of_intervention": "NumeroLogic mitigates some tokenization issues by giving explicit digit-count metadata independent of tokenizer chunking; other tokenizer-level fixes have mixed success per cited literature.",
            "performance_metrics": "No direct numeric performance numbers attributed solely to tokenization choices in this paper beyond cited literature references.",
            "notable_failure_modes": "Unintuitive token splits can produce non-uniform training statistics and hurt extrapolation to unseen digit-lengths.",
            "comparison_to_humans_or_symbolic": "Tokenization is a modeling/engineering artifact with no analogue in symbolic calculators; fixing it can make LLM inputs more aligned with symbolic arithmetic representations.",
            "uuid": "e3012.5",
            "source_info": {
                "paper_title": "NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting / reasoning",
            "brief_description": "A prompting/training approach that elicits multi-step reasoning traces from LLMs; referenced as related mechanism that helps arithmetic by forcing step-wise algorithmic thinking.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "General LLMs in CoT literature",
            "model_description": "CoT is a prompting technique (and sometimes training signal) prompting models to produce intermediate reasoning steps prior to final answer; originally demonstrated on larger LLMs for reasoning tasks.",
            "arithmetic_task_type": "Arithmetic and multi-step reasoning tasks where intermediate steps benefit correctness (e.g., multi-digit multiplication, multi-step word problems).",
            "reported_mechanism": "By asking the model to produce intermediate steps, CoT encourages the model to plan and perform algorithmic computations rather than directly pattern-match to outputs.",
            "evidence_for_mechanism": "Cited CoT literature shows improvements on complex reasoning tasks. Paper argues NumeroLogic induces a simplified CoT by forcing digit-count prediction prior to digits.",
            "evidence_against_mechanism": "Paper notes CoT benefits may partly come from extra tokens enabling more computation/memory (Pfau et al.), but filler-token control indicates extra tokens alone are insufficient—format/content matters.",
            "intervention_type": "Prompting for chain-of-thought or training to produce intermediate steps; NumeroLogic acts as a weaker, implicit CoT (predict digit count first).",
            "effect_of_intervention": "CoT in prior work improves reasoning and arithmetic in many contexts; NumeroLogic's digit-count planning yields similar benefits for number generation/representation in experiments.",
            "performance_metrics": "No numerical CoT results provided in this paper beyond referencing CoT literature; NumeroLogic showed MMLU and arithmetic gains attributed in part to CoT-like behavior.",
            "notable_failure_modes": "CoT can increase token usage and latency; some benefits may be artifact of extra tokens rather than genuine algorithmic computation.",
            "comparison_to_humans_or_symbolic": "CoT is likened to externalizing intermediate human reasoning steps; symbolic algorithms remain exact while CoT-based outputs are probabilistic.",
            "uuid": "e3012.6",
            "source_info": {
                "paper_title": "NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Positional description matters for transformers arithmetic",
            "rating": 2,
            "sanitized_title": "positional_description_matters_for_transformers_arithmetic"
        },
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2,
            "sanitized_title": "teaching_arithmetic_to_small_transformers"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier llms",
            "rating": 2,
            "sanitized_title": "tokenization_counts_the_impact_of_tokenization_on_arithmetic_in_frontier_llms"
        },
        {
            "paper_title": "Let's think dot by dot: Hidden computation in transformer language models",
            "rating": 1,
            "sanitized_title": "lets_think_dot_by_dot_hidden_computation_in_transformer_language_models"
        }
    ],
    "cost": 0.01372875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning</p>
<p>Eli Schwartz 
IBM Research</p>
<p>Leshem Choshen 
MIT-IBM Watson AI Lab
3 MIT</p>
<p>Joseph Shtok 
IBM Research</p>
<p>Sivan Doveh 
IBM Research</p>
<p>Leonid Karlinsky 
MIT-IBM Watson AI Lab
3 MIT</p>
<p>Assaf Arbelle 
IBM Research</p>
<p>NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning
528FD8F03F4B4C48B8CD00717D638B77
Language models struggle with handling numerical data and performing arithmetic operations.We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation.When a digit is read or generated by a causal language model it does not know its place value (e.g.thousands vs. hundreds) until the entire number is processed.To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number.For instance, instead of "42", we suggest using "2:42" as the new format.This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT).By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number.We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting.We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) struggle with numerical and arithmetical tasks.Despite continuous improvements, even the most advanced models like GPT-4 (Achiam et al., 2023) still exhibit poor performance when confronted with tasks such as multiplying 3-digit numbers (Shen et al., 2023).Recent studies ( (Lee et al., 2024;Shen et al., 2023)) have proposed techniques to improve arithmetic in LLMs, such as the Chain of Thought (CoT; (Wei et al., 2022)) method, which pushes the model to anticipate the entire sequence of algorithmic steps rather than just the final output.While these strategies offer valuable insights into the capabilities of LLMs, they primarily concentrate on post-hoc solutions for specific arithmetic challenges and do not present a practical solution for pretraining LLMs.</p>
<p>Figure 1: Reading numbers in a causal manner from left to right is sub-optimal for LLMs, as it is for humans.The model has to reach the final digits of a number before it can infer the place value of the first digit.To address this, we propose "NumeroLogic", a numerical format where digit count is indicated before the actual number.Image by DALL-E 3 (Betker et al., 2023).</p>
<p>Our research, however, focuses on solutions applicable to self-supervised language modeling in general, utilizing arithmetic exercises primarily for evaluating their impact.</p>
<p>We hypothesize that one of the challenges LLMs face when dealing with numerical tasks is the textual representation of numbers.In today's most popular decoder-based LLMs, each token attends only to previous tokens.When a model "reads" a token representing a digit (or multiple digits) it cannot tell its place value, i.e. '1' can represent 1 million, 1 thousand, or a single unit.Only when reaching the end of the number might the model update its representation of the previous digit tokens to be related to their real place value.</p>
<p>To address this issue, we propose a straightforward reformatting technique called "Numero-Logic," which involves adding the number of digits arXiv:2404.00459v2[cs.CL] 26 Sep 2024 as a prefix to numbers.This lets the model know in advance what is the place value of a digit before it is read.This simple change also offers another benefit, when the model is generating a number it needs to first reason about what is going to be the number of digits.This acts as a Chain of Thought (CoT) (Wei et al., 2022), encouraging the model to perform some reasoning before it begins to predict digits.Implementing the suggested reformatting does not necessitate any alterations to the model's architecture; it can be accomplished through text pre-and post-processing based on regex.</p>
<p>We demonstrate that NumeroLogic enhances the numerical abilities of LLMs across both small and larger models (up to 7B parameters).This enhancement is showcased through supervised training on arithmetic tasks and its application in selfsupervised causal language modeling to enhance general language comprehension.</p>
<p>Related Work</p>
<p>Recently, there has been a significant interest in enhancing the numerical capabilities of LLMs.One approach to investigating these capabilities is by assessing their performance in arithmetic tasks.Several recent studies have proposed methods to enhance performance in these tasks.One strategy involves reversing the expected result order from the least to the most significant digit (Lee et al., 2024).Another strategy is using an elaborated CoT where the model is taught to predict all steps of an algorithm predefined for each arithmetic task (Lee et al., 2024).In (Shen et al., 2023), it is noted that the model learns to rely too heavily on positional encoding when trained for a specific arithmetic task.They suggest ways to overcome it, e.g.adding random white spaces in the middle of numbers.These studies aim to enhance the performance of arithmetic tasks by offering tailored solutions to the associated challenges.In contrast, our focus is on identifying solutions that benefit general language modeling rather than just arithmetic tasks, with arithmetic tasks being used solely for measuring improvements.</p>
<p>Another aspect important for LLMs numerical capabilities is the tokenization process.The commonly used Byte Pair Encoding (BPE) based methods (Gage, 1994;Sennrich et al., 2015) for tokenization are based on the corpus distribution and can split a number to tokens in unintuitive ways.Different foundation models took different approaches when dealing with number tokeniza-tion.PaLM (Chowdhery et al., 2023), Llama (Touvron et al., 2023), and Mistral (Jiang et al., 2023) force each digit to have a single token.GPT-3.5 and GPT-4 define a token for each up to 3-digit number (Achiam et al., 2023).Somewhat related to our work, in (Singh and Strouse, 2024), they highlighted an issue with the GPT approach.They show that dividing large numbers into 3-digit segments from left to right undermines arithmetic performance.They suggest overcoming it by inserting commas between digits to control the splitting.Another related work, is (Kim et al., 2021).They focus on the extrapolation ability of LLMs to unseen numbers and use a special number encoding that lets the LLM know the digit place-value.</p>
<p>NumeroLogic</p>
<p>We introduce NumeroLogic, a technique for boosting causal LLM's numerical capabilities.The concept involves adding a digit count before numbers, enabling the model to know the place values of digits before reaching the final digits of a number.Additionally, the model needs to predict the total number of digits before generating a number, acting as a simplified CoT, prompting it to reason about the number that is going to be generated.</p>
<p>We add special tokens to help represent numbers with the number-of-digit prefix, "<startnumber>", "<midnumber>", and "<endnumber>" (or, for simplicity, "<sn>", "<mn>", and "<en>").For floating points, the prefix includes both the number of digits of the integer part and the decimal part.For example, "42" is replaced by "<sn>2<mn>42<en>" and "3.14" is replaced by "<sn>1.2<mn>3.14<en>".When using the LLM to generate numbers, we disregard the information about the number of digits and only retain the generated number itself.</p>
<p>Although not within the scope of this study, it may be feasible to leverage the additional information to identify discrepancies, wherein the model predicts a certain digit count but produces a number with a different count of digits.</p>
<p>For small transformers, we train all parameters from scratch with character-level tokenization.For small transformers, we also replace the special tokens with single characters, "<sn>", "<mn>", and "<en>" are replaced with "{", ":", and "}", respectively.For larger transformers, we start from pre-trained models.We add the new special tokens to the tokenizer's vocabulary and expand the embedding layer and the final fully connected layer to fit the new vocabulary size.When continuing training on causal language modeling or fine-tuning on supervised arithmetic tasks, we use low-rank adaptation (LoRA) (Hu et al., 2021).We apply LoRA for the attention block projection matrices (Q, K, V, O) and train the modified embedding layer and the final fully-connected layer in full rank.</p>
<p>The NumeroLogic approach includes basic text pre-processing and post-processing steps that occur before and after the tokenizer's encoding and decoding methods, respectively.Both can be implemented based on regular expressions:
def preprocess_all_numbers ( text ): def f ( match ): num = match . group (0) i = match . group (1) li = len (i) d = match . group (3) ld = len (d) if d else 0 if d : prefix = f ' <sn >{ li }.{ ld }<mn > ' else : prefix = f ' <sn >{ li }<mn > ' return prefix + num + ' <en > ' pattern = ' (\ d +)(.(\ d +))? ' return re . sub ( pattern , f , text ) def postprocess_all_numbers ( text ): pattern = '<sn >[\ d .]+ &lt; mn &gt; ' text = re . sub ( pattern , '' , text ) text = re . sub ( ' <en > ' , '' , text ) return text</p>
<p>Experiments</p>
<p>To test the effect of NumeroLogic we conducted several experiments.First, we tested supervised training of a small language model (NanoGPT) on various arithmetic tasks.We then test the scalability to larger models (Llama2-7B).Finally, we test self-supervised pretraining of Llama2-7B, with the suggested formatting, and test on general language understanding tasks.</p>
<p>Arithmetic tasks with small model</p>
<p>We trained NanoGPT (Karpathy, 2022)  are used.We followed the protocol from Section D.2 in (Lee et al., 2024).</p>
<p>Tab. 1 compares the results of training with plain numbers to training with the NumeroLogic encoding.For addition and subtraction, a model trained with plain numbers reached 88.37% and 73.76% accuracy, respectively, while with the Nu-meroLogic encoding, the tasks are almost solved (99.96% and 97.2%).For multiplication, we observe more than doubling of the accuracy, from 13.81% to 28.94%.Furthermore, for the floating point operations, sine and square root, we see a significant improvement of 4% for both tasks.</p>
<p>Arithmetic tasks with larger model</p>
<p>Next, we test how the method scales to a larger model.For this experiment, we fine-tune a pretrained Llama2-7B model (Touvron et al., 2023).In this experiment, we again tested the same five arithmetic tasks: addition, subtraction, multiplication, sine, and square root.For addition (5 digit), subtraction (5 digit), and multiplication (3 digit) we tested on two versions -integers and floating point numbers.For generating a random N-digit floating point operand we first sample an up to Ndigit integer and then divide it by a denominator uniformly sampled from 10 0 , 10 1 , ..., 10 N .For each of the addition, subtraction, and multiplication tasks, we generated 300K random equations as a training set.The sine and square root operands and results are generated with 5 decimal place accuracy, we generated 30K random equations for the training sets of these tasks.Since we are working with a pretrained model we add new tokens ("<sn>", "<mn>", and "<en>") to the tokenizer's vocabulary.We finetune one model per task with LoRA (Hu et al., 2021) (rank 8), we also train in full-rank the embedding layer and the final linear layer since they are extended to fit the larger vocab.size.</p>
<p>The results are presented in Tab. 2. Addition and subtraction of integers are mostly solved by a model as large as Llama2-7B even for much larger numbers (e.g.20-digit).For our 5-digit experiments, the plain text baselines reached 99.86% and 99.6% performance, for addition and subtraction, respectively.Despite the high performance of plain text, we still observe an improvement when using NumerLogic, with a perfect 100% for addition and rectification of more than 80% of the subtraction mistakes, reaching 99.93% accuracy for subtraction.For all other, non-saturated, tasks we observed significant gains of 1%-6%.</p>
<p>Self-Supervised Pretraining</p>
<p>Our approach differs from other methods in that it is not specialized for a specific task, such as arithmetic, but rather designed for general language modeling tasks involving text with numerical values.To test this capability we continue the pretraining of LLama2-7B with the causal text modeling objective (next token prediction).We train on text from the RefinedWeb dataset (Penedo et al., 2023).The goal is to teach the model to read and write numbers in the NumeroLogic format without forgetting its previously acquired knowledge.To facilitate this, we perform the continued pretraining with LoRA.We then test the model in a 0-shot manner on MMLU (Hendrycks et al., 2021b,a).In Fig. 2, we present the MMLU 0-shot results obtained from training the model using plain numbers versus NumeroLogic encoding on an equal number of tokens.While training with plain numbers does not enhance the model's accuracy compared to the pretrained model, employing Numero-Logic encoding results in a statistically significant improvement of 0.5%.The MMLU benchmark encompasses tasks from diverse domains, some emphasizing analytical skills and numerical comprehension while others do not.In Tab. 3, we delve into the impact of NumeroLogic on MMLU tasks categorized by field.As anticipated, tasks in STEM fields exhibit more substantial enhancements compared to those in social sciences and humanities.Tab. 4 provides a detailed analysis of Numero-Logic's performance boost across tasks containing numbers versus those that do not.Consistently, tasks involving numbers show higher improvement.</p>
<p>Ablation studies 4.4.1 Encoding operands vs. results</p>
<p>We experimented to test the effect of operand encoding vs. the expected output (equation result) encoding.Operand encoding primarily influences the model's comprehension of numerical values in the input, while result encoding is more associated with CoT, prompting the model first reason about the expected number of digits.We repeat the experiment from Section 4.1, but with the NumeroLogic encoding applied only to the operands or to the results and report the 3-digit addition results for the different variants.The results are presented   in Tab. 5. We find that both operands and results encodings are beneficial, with a stronger impact attributed to encoding the results.Applying Numero-Logic to all numbers, both operands and results, yields the highest level of accuracy.</p>
<p>Different Encodings</p>
<p>We experimented with different formats for providing the number of digits.One alternative we tested is defining a set of new special tokens representing each possible number of digits, {&lt;1digitnumber&gt;, &lt;2digitnumber&gt;,...}.We observed that the performance of having multiple special tokens is even lower than plain numbers.It might be due to the unbalanced distribution of numbers.E.g. numbers with a single digit are much less frequent in the data of 3-digit additions, it is possible the model has not seen enough single-digit numbers to learn a good representation of the &lt;1digitnumber&gt; token.</p>
<p>Another alternative we tested is removing the "end of number" token (<en>), keeping only the number prefix, e.g."<sn>3<mn>100".This works better than plain but slightly worse than the full Numero-Logic encoding.The results are summarized in Tab. 6.</p>
<p>Is it the extra tokens?</p>
<p>It has been shown that the advantage of CoT is at least partially due to the extra tokens that allow the model to perform more computations or store information (Pfau et al., 2024).To understand the effect of the extra tokens we run an experiment where all the extra tokens introduced by Numero-Logic are replaced with filler white-space tokens.Table 7: Extra tokens effect: Just adding filler white space tokens is not helpful and is comparable to the plain format.The random white-space method (Shen et al., 2023) of adding filler tokens at random locations is helpful but less effective compared to NumeroLogic.</p>
<p>Additionally, in (Shen et al., 2023), it has been shown that the model learns to rely too heavily on positional encoding when trained on arithmetic tasks.It causes failures when the model is tested with numbers less frequent in the training data (e.g. 1 1-digit numbers when the model is trained on up to 3-digit numbers).To deal with this limitation, they suggest adding filler white-space tokens at random locations between the digits.We also report the results of their approach (Shen et al., 2023) where we use the same number of tokens as NumeroLogic would have required, just that they are replaced with white-space tokens at random locations.These experiments were performed by finetuning Llama2-7B on 3-digit floating-point multiplication.The results are reported in Table 7.</p>
<p>We observe that just adding the extra tokens does not help and the performance is similar to the plain format.Adding the same amount of extra tokens in random locations is somewhat helpful but not as effective as NumeroLogic.It eliminates the model's reliance on positional encoding but does not provide place-value information like NumeroLogic.</p>
<p>Conclusions</p>
<p>We introduced NumeroLogic, a novel method to improve language models' handling of numerical data.Our approach prefixes numbers with their digit count, enhancing models' understanding of place value and prompting better reasoning about numbers' magnitude, akin to chain-of-thought reasoning.We tested NumeroLogic on both arithmetic and broader language understanding tasks.The results showed substantial enhancements in numerical tasks, including integer and floating-point calculations, and in broader modeling contexts like the MMLU benchmark.In summary, NumeroLogic is a straightforward yet effective enhancement for language models' numerical abilities, applicable across various tasks without requiring changes to the models' architecture.</p>
<p>The NumeroLogic encoding, while enhancing numerical reasoning, might increase the number of tokens per number.Moreover, it introduces additional steps in pre-and post-processing.This raises the computational costs and also potentially increases the model's latency during inference.These factors might impact the efficiency of Nu-meroLogic, especially in numerical-processingintensive applications.</p>
<p>Our experiments predominantly involved finetuning pre-trained language models (LLMs) rather than training them from scratch with NumeroLogic.While this limits our ability to conclusively predict the impacts from the pre-training phase, incorporating NumeroLogic early in the pre-training would likely have a stronger positive rather than negative effect on the performance.Additionally, our testing did not extend to models larger than 7B parameters.However, it has been demonstrated that both small and large models exhibit similar learning behaviors (Warstadt et al., 2023); therefore, it is plausible to predict that scaling up the model size will not diminish the effectiveness of NumeroLogic.</p>
<p>Lastly, our evaluation was confined to controlled academic benchmarks, which might not fully represent the complexities of real-world numerical data.Extending testing to diverse, real-world datasets is essential to fully understand NumeroLogic's practical effectiveness and ensure it can handle the unpredictable nature of real-world numerical data.Similarly, despite caring mainly about numerical aspects, we checked English-focused datasets and data.The cross effects with different languages, scripts and even numerical writing system is left as an open question.</p>
<p>Figure 2 :
2
Figure 2: MMLU Accuracy of Llama2-7B.Continuing self-supervised pretraining on web-curated text tokens, when numbers are encoded with NumeroLogic, helps improve the performance beyond the pretrained model or a model trained on the same text with plain numbers.</p>
<p>Table 1 :
1
NanoGPT arithmetic tasks accuracy with NumeroLogic encoding.A single model is jointly trained for all tasks.The encoding produces high accuracy gains for all tasks.
from
operand range for sine is within [−π/2, π/2].The operand range for the square root is within [0, 10].The model is trained in a multi-task fashion on all 5 tasks, with 10K training samples for each task except for multiplication, for which 3K samples</p>
<p>Table 3 :
3
MMLU accuracy change due to NumeroLogic encoding on tasks from different fields.STEM tasks which are more likely to require numerical understanding enjoy higher improvement.
ChangeTasks with numbers+1.16%Tasks without numbers +0.14%</p>
<p>Table 4 :
4
MMLU accuracy change due to NumeroLogic encoding on tasks with and without numbers.Tasks with numbers enjoy higher improvement.</p>
<p>Table 5 :
5
Testing the effect of encoding the equation's operands vs. result.Tested on the addition task with NanoGPT.Either encoding the operands (i.e.input comprehension) or encoding the results (i.e.CoT effect) have a positive effect, with a stronger effect for operands' encoding.Encoding both the operands and the result provides the best performance.
EncodingAccuracyPlain (e.g. "100")34.20%Multi special tokens ("&lt;3digitnumber&gt;100")33.56%Only prefix ("<sn>3<mn>100")34.93%NumeroLogic ("<sn>3<mn>100<en>")35.33%</p>
<p>Table 6 :
6
Different encoding alternatives performance on 3-digit integer multiplications.</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Improving image generation with better captions. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Computer Science. 2382023</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>A new algorithm for data compression. Philip Gage, The C Users Journal. 1221994</p>
<p>Aligning ai with shared human values. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, Jacob Steinhardt, 2021aProceedings of the International Conference on Learning Representations (ICLR</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021b</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>. Andrej Karpathy, 2022</p>
<p>Have you seen that number? investigating extrapolation in question answering models. Jeonghwan Kim, Giwon Hong, Kyung-Min Kim, Junmo Kang, Sung-Hyon Myaeng, 10.18653/v1/2021.emnlp-main.563Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Teaching arithmetic to small transformers. Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, Dimitris Papailiopoulos, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. 2023arXiv preprint</p>
<p>Let's think dot by dot: Hidden computation in transformer language models. Jacob Pfau, William Merrill, Samuel R Bowman, arXiv:2404.157582024arXiv preprint</p>
<p>Rico Sennrich, Barry Haddow, Alexandra Birch, arXiv:1508.07909Neural machine translation of rare words with subword units. 2015arXiv preprint</p>
<p>Positional description matters for transformers arithmetic. Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, Yi Zhang, arXiv:2311.147372023arXiv preprint</p>
<p>K Aaditya, D J Singh, Strouse, arXiv:2402.14903Tokenization counts: the impact of tokenization on arithmetic in frontier llms. 2024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Findings of the babylm challenge: Sample-efficient pretraining on developmentally plausible corpora. Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>            </div>
        </div>

    </div>
</body>
</html>
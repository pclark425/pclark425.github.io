<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8335 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8335</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8335</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278789244</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16686v2.pdf" target="_blank">SPaRC: A Spatial Pathfinding Reasoning Challenge</a></p>
                <p><strong>Paper Abstract:</strong> Existing reasoning datasets saturate and fail to test abstract, multi-step problems, especially pathfinding and complex rule constraint satisfaction. We introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000 2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning, requiring step-by-step planning with arithmetic and geometric rules. Humans achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles). Models often generate invalid paths (>50% of puzzles for o4-mini), and reasoning tokens reveal they make errors in navigation and spatial logic. Unlike humans, who take longer on hard puzzles, models fail to scale test-time compute with difficulty. Allowing models to make multiple solution attempts improves accuracy, suggesting potential for better spatial reasoning with improved training and efficient test-time scaling methods. SPaRC can be used as a window into models'spatial reasoning limitations and drive research toward new methods that excel in abstract, multi-step problem-solving.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8335.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8335.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o4-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o4-mini (OpenAI reasoning model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source reasoning-tuned LLM from OpenAI used as the strongest baseline in SPaRC; evaluated in medium reasoning effort mode and as the best-performing model on the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o4-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI reasoning-tuned language model run in 'medium effort' reasoning mode; treated as a high-performing reasoning baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPaRC (Spatial Pathfinding Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based pathfinding and constraint-satisfaction puzzles requiring spatial planning, counting, segregation, and polyomino shape reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Text-based grid representation (ARC-style array); prompts included a one-shot human-annotated example by default; required step-by-step explanation and final coordinate list after delimiter '####'; solutions extracted by regex; also evaluated with alternative prompt, few-shot/zero-shot ablations, and multi-attempt (pass@k) sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Reasoning-mode generation with step-by-step (chain-of-thought style) prompts requested; medium-effort reasoning mode used; prompt engineering (alternative prompt emphasizing path rules) and sampling multiple attempts (pass@k up to k=8) were employed to improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy 15.8% on the SPaRC test set (79/500 reported in text); difficulty breakdown reported for o4-mini: Level 1 47.7%, Level 2 19.5%, Level 3 10.7%, Level 4 1.2%, Level 5 1.1%; pass@1 = 15.8% -> pass@8 = 35.0% (o4-mini). One-shot = 15.8%, zero-shot = 12.6% (different prompt/shot ablations), alternative prompt improved to 21.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative analysis of generated reasoning tokens (when available) and path outputs showed partial spatial reasoning: models sometimes recognized dead-ends and certain constraints but consistently made logical fallacies, grid-index misinterpretations, and unvalidated multi-step moves; >50% of o4-mini outputs contained invalid paths (frequent Rule Cell Crossing reported). Scaling attempts (pass@k) improved accuracy on easy puzzles but minimally on hard ones, suggesting limited robust spatial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperformed other evaluated reasoning/open models (o3-mini 8.2%, R1 70B ~4.0%, QwQ 5.8%) and instruction-tuned models (GPT-4.1 1.6% reported as best instruction-tuned), but far below human performance (humans 98.0%). Alternative prompting and extra sampling increased accuracy (one-shot -> alt prompt: 15.8% -> 21.0%; pass@8 -> 35.0%), while multimodal (vision) prompts reduced performance (text zero-shot 12.6% vs vision 5.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High rate of invalid path constructions (>50% invalid for o4-mini) including Rule Cell Crossing, Intersecting Line, Disconnected Line, Incorrect Start/End; fails on complex interactions (polys and ylops), severe drop with difficulty (near-zero on levels 4/5), limited benefits from increased attempts for hard puzzles, and vision input degraded performance; intermediate reasoning tokens are redacted for closed models limiting trace-based analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPaRC: A Spatial Pathfinding Reasoning Challenge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8335.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8335.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o4-mini (vision)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o4-mini Vision (multimodal variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vision-enabled variant of o4-mini evaluated with puzzle screenshots rather than the text-array representation; used to test multimodal prompting effects on spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o4-mini (vision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal/vision-enabled variant of OpenAI's o4-mini; evaluated by feeding puzzle screenshots and a vision-adapted prompt instead of the text array and example.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPaRC (Spatial Pathfinding Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based pathfinding and constraint-satisfaction puzzles requiring visual-to-spatial mapping</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision prompt: screenshot image of puzzle, vision-specific instructions; zero-shot (visual prompt lacked example solution) comparison to textual zero-shot and one-shot setups; same coordinate output format required.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Multimodal input processing; reasoning in medium-effort mode similar to text variant; no added symbolic plannerâ€”relies on model's multimodal reasoning to interpret visual grid and produce coordinate sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Vision performance worse than textual input: o4-mini visual zero-shot accuracy 5.6% vs textual zero-shot 12.6% (reported). Overall accuracy with vision variant substantially lower across difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Higher rates of structural path errors (Invalid Path, Disconnected Line, Intersecting Line, Rule Cell Crossing) observed in vision mode; suggests difficulty in mapping visual input to the precise textual coordinate system and additional modality-induced complexity rather than improved spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperformed textual prompts for the same model (text zero-shot 12.6%); performance gap larger at easier difficulty levels and diminished at higher difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Vision prompts increased basic structural errors and reduced overall solving accuracy; multimodal setup did not help and in fact degraded performance, indicating current vision-language integration did not improve spatial planning for these puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPaRC: A Spatial Pathfinding Reasoning Challenge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8335.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8335.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini (OpenAI reasoning model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller OpenAI reasoning model evaluated as a baseline; performed worse than o4-mini but still above many open models on some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI reasoning model (smaller/earlier than o4-mini) used with default medium reasoning effort for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPaRC (Spatial Pathfinding Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based pathfinding puzzle requiring constrained spatial planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same text-array prompt and extraction procedure as other models; one-shot human example used by default.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Instruction/reasoning-mode generation; no specialized symbolic planning; evaluated under same ablations (prompting, few-shot, pass@k) as o4-mini where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy reported ~8.2% on SPaRC test set (paper reports o3-mini 8.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Exhibited many similar error modes to other LLMs (invalid paths, misinterpretation of grid), implying limited reliable spatial reasoning; performed better than some open models but far from human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperformed o4-mini (15.8%) but outperformed open models like R1 70B and QwQ in aggregate statistics reported by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low accuracy on medium-to-hard puzzles; frequent path rule violations; limited scaling with difficulty and test-time compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPaRC: A Spatial Pathfinding Reasoning Challenge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8335.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8335.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R1 70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1 70B (R1 Llama 70B Distill)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open reasoning-tuned model (70B) from DeepSeek used in the paper and analyzed qualitatively since it exposes internal reasoning tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek R1 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open model (70B) from DeepSeek-AI described as reasoning-capability incentivized via RL; exposes intermediate reasoning tokens allowing trace-based error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPaRC (Spatial Pathfinding Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based pathfinding and constraint-satisfaction puzzles requiring multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Text-based prompt with one-shot example by default; because model exposes chain-of-thought tokens, authors manually analyzed reasoning traces for failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Generates explicit chain-of-thought reasoning tokens which were used for manual qualitative analysis; no external symbolic planner used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy ~4.0% (reported R1 70B ~4.0%); in-depth manual analysis performed on 48 puzzle attempts that produced valid path shapes but failed rule checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Manual token analysis revealed models sometimes recognized mistakes/dead-ends (25/48 cases) and exhibited partial spatial reasoning but commonly committed logical fallacies (36/48), grid/index misinterpretation (26/48), and careless multi-step shortcutting (23/48). Examples of coordinate mis-indexing and attempted line-over-rule-cells were documented (Figures 21-24).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performed worse than closed OpenAI reasoning models (o4-mini/o3-mini) but provided richer trace data for diagnosing failure modes; contrasted with humans (98% accuracy) and with o4-mini's higher accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Errors dominated by logical fallacies, coordinate-system confusion (x/y swaps), drawing on rule cells, and failing to repair recognized mistakes; exposure of reasoning tokens revealed limited but present self-awareness of dead-ends that was not successfully exploited to find solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPaRC: A Spatial Pathfinding Reasoning Challenge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8335.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8335.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ 32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ 32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open reasoning model (32B) evaluated on SPaRC showing modest performance and comparatively high token usage in some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ 32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open reasoning model of 32B parameters referenced as a reasoning-tuned baseline in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPaRC (Spatial Pathfinding Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based pathfinding puzzles requiring spatial and rule-based reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Text-array prompts with one-shot example by default; ablations similar to other models (few-shot, alt prompts, pass@k) applied.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Reasoning-tuned generation without external symbolic solvers; evaluated for token usage and per-difficulty behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate accuracy reported ~5.8% (paper lists QwQ at 5.8% in some aggregated figures). Table 2 shows per-difficulty behavior for QwQ (Level 1 ~20.9% but falls off quickly for higher levels) indicating strong drop with difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative patterns similar to other reasoning models: success on simpler patterns (gaps split better) but fails on complex interacting rules; token counts show relatively constant token usage across difficulties indicating limited scaling of reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperformed closed OpenAI models; sometimes outperformed instruction-tuned models; compared across rule splits it had variable relative performance (e.g., performed near average on dots/gaps splits).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Sharp performance drop with increasing difficulty; path construction errors and logic mistakes; token usage did not increase substantially with difficulty suggesting poor test-time scaling of computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPaRC: A Spatial Pathfinding Reasoning Challenge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8335.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8335.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-3 27B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma 3 27B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned (open) LLM used as an instruction-model baseline; exhibited distinct error patterns (e.g., crossing rule cells).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma 3 27B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned open LLM (27B) used as an instruction-model baseline; evaluated in standard prompt conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>27B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPaRC (Spatial Pathfinding Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid pathfinding requiring spatial planning and rule satisfaction</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Text-array prompt with one-shot example by default; standard extraction of coordinates after '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Instruction-tuned generation; no explicit chain-of-thought fine-tuning reported for this model in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Instruction-tuned models generally performed worse than reasoning models; Gemma-specific overall accuracy not always explicitly tabulated, but authors report instruction models performed worse overall and Gemma produced characteristic errors (e.g., Rule Cell Crossing).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Produced more Rule Cell Crossings and specific error patterns (Gemma crosses rule cells more often, but produced fewer disconnected lines than some reasoning models), implying partial but brittle spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperformed reasoning-tuned closed models (o4-mini, o3-mini) and sometimes had error patterns distinct from other models; smaller models (including Gemma) sometimes performed comparatively better on simpler polys splits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High rate of invalid paths and rule-cell crossings; poor scaling with difficulty; multimodal and few-shot ablations did not consistently improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPaRC: A Spatial Pathfinding Reasoning Challenge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8335.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8335.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen 2.5 72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 2.5 72B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned LLM (72B) included as an instruction-model baseline; used to measure token-scaling behavior and instruction-mode performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen 2.5 72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned model of size 72B used as an instruction baseline; evaluated for accuracy and tokens produced per difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPaRC (Spatial Pathfinding Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based pathfinding puzzles requiring spatial reasoning and rule compliance</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Instruction-style prompt (one-shot by default) and same coordinate output format; token counts and accuracy measured per difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Instruction-following generation; no specialized reasoning-finetuning beyond being instruction-tuned; scaled token counts with puzzle difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Per Table 2, Qwen 2.5 had near-zero accuracy on many difficulty levels (Level 1 0.0%, Level 2 1.7%, others 0.0% as reported in Table 2 excerpt), and token usage increased with difficulty (e.g., 790 -> 1161 tokens across levels), indicating instruction models expended more tokens on harder puzzles but without accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Lack of effective spatial reasoning: despite increasing token usage with difficulty (~40% token increase), accuracy remained near-zero; suggests token scaling without improved planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performed worse than reasoning-tuned models (o4-mini, o3-mini) and far worse than humans; compared in ablations showing instruction models increased tokens more with difficulty than reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Extremely low solving accuracy across difficulties; token-scaling with difficulty did not translate into improved spatial reasoning or success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPaRC: A Spatial Pathfinding Reasoning Challenge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8335.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8335.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.1 (OpenAI instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source instruction-tuned model from OpenAI evaluated as an instruction-model baseline; among instruction-tuned models it achieved the highest reported accuracy on SPaRC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI instruction-tuned model; used as an instruction baseline under the standard prompt setups.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPaRC (Spatial Pathfinding Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based pathfinding and constraint puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Text-array prompt with one-shot example used by default; coordinate-list output extracted after delimiter.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Instruction-following generation; no specialized reasoning mode exposed in the paper for this model (compared to the 'reasoning' mode used for o4/o3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as the best instruction-tuned model in the study with 1.6% accuracy (8/500 puzzles), substantially below reasoning models and humans.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Low solving rates and frequent structural path errors (GPT-4.1 frequently produced disconnected lines) indicating limited effective spatial planning in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperformed reasoning-tuned models (o4-mini/o3-mini) and human baseline; instruction models overall performed worse than reasoning models in the SPaRC evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very low overall accuracy and specific path-structure errors (disconnected lines) were frequent; instruction tuning did not compensate for multi-step spatial planning deficits in these puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPaRC: A Spatial Pathfinding Reasoning Challenge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8335.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8335.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReasonFlux-F1 32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReasonFlux-F1 32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning fine-tuned model family (ReasonFlux) evaluated to test whether long-context reasoning fine-tuning helps SPaRC performance; showed modest gains but remained far below human levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReasonFlux-F1 32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reasoning fine-tuned model aimed at long-chain reasoning and trajectory-aware PRMs (paper cites ReasonFlux family); tested against Qwen 3 32B as a long-context reasoning fine-tuned baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPaRC (Spatial Pathfinding Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid pathfinding puzzles requiring extended chain-of-thought planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated with the same prompt/one-shot configuration as other models; compared specifically in an ablation to test long-context reasoning fine-tuning effects.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Long-context reasoning fine-tuning (ReasonFlux family) designed to improve chain-of-thought stability over long trajectories; no external symbolic planner.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ReasonFlux-F1 32B performance reported comparable but still low relative to human baseline (example given: ReasonFlux-F1 32B 8.6% vs another reasoning model 6.0% in Figure 11 comparison), indicating only modest improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Small gains relative to other reasoning-tuned models indicate limited improvement in multi-step spatial planning from ReasonFlux-style fine-tuning; still far from human levels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to Qwen 3 32B; ReasonFlux-F1 showed comparable or small improvements (8.6% vs 6.0% cited), but remained much lower than top closed models and humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fine-tuning for long-context reasoning provided only modest gains and did not solve core failure modes (path validity, logical fallacies, coordinate misinterpretation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPaRC: A Spatial Pathfinding Reasoning Challenge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ARC-AGI <em>(Rating: 2)</em></li>
                <li>Is a picture worth a thousand words? delving into spatial reasoning for vision language models <em>(Rating: 2)</em></li>
                <li>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning <em>(Rating: 2)</em></li>
                <li>PPNL <em>(Rating: 2)</em></li>
                <li>VisualPuzzles <em>(Rating: 2)</em></li>
                <li>EnigmaEval <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8335",
    "paper_id": "paper-278789244",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "o4-mini",
            "name_full": "o4-mini (OpenAI reasoning model)",
            "brief_description": "A closed-source reasoning-tuned LLM from OpenAI used as the strongest baseline in SPaRC; evaluated in medium reasoning effort mode and as the best-performing model on the dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o4-mini",
            "model_description": "Proprietary OpenAI reasoning-tuned language model run in 'medium effort' reasoning mode; treated as a high-performing reasoning baseline in the paper.",
            "model_size": null,
            "puzzle_name": "SPaRC (Spatial Pathfinding Reasoning Challenge)",
            "puzzle_type": "2D grid-based pathfinding and constraint-satisfaction puzzles requiring spatial planning, counting, segregation, and polyomino shape reasoning",
            "task_setup": "Text-based grid representation (ARC-style array); prompts included a one-shot human-annotated example by default; required step-by-step explanation and final coordinate list after delimiter '####'; solutions extracted by regex; also evaluated with alternative prompt, few-shot/zero-shot ablations, and multi-attempt (pass@k) sampling.",
            "mechanisms_or_strategies": "Reasoning-mode generation with step-by-step (chain-of-thought style) prompts requested; medium-effort reasoning mode used; prompt engineering (alternative prompt emphasizing path rules) and sampling multiple attempts (pass@k up to k=8) were employed to improve results.",
            "performance_metrics": "Overall accuracy 15.8% on the SPaRC test set (79/500 reported in text); difficulty breakdown reported for o4-mini: Level 1 47.7%, Level 2 19.5%, Level 3 10.7%, Level 4 1.2%, Level 5 1.1%; pass@1 = 15.8% -&gt; pass@8 = 35.0% (o4-mini). One-shot = 15.8%, zero-shot = 12.6% (different prompt/shot ablations), alternative prompt improved to 21.0%.",
            "evidence_of_spatial_reasoning": "Qualitative analysis of generated reasoning tokens (when available) and path outputs showed partial spatial reasoning: models sometimes recognized dead-ends and certain constraints but consistently made logical fallacies, grid-index misinterpretations, and unvalidated multi-step moves; &gt;50% of o4-mini outputs contained invalid paths (frequent Rule Cell Crossing reported). Scaling attempts (pass@k) improved accuracy on easy puzzles but minimally on hard ones, suggesting limited robust spatial planning.",
            "comparisons": "Outperformed other evaluated reasoning/open models (o3-mini 8.2%, R1 70B ~4.0%, QwQ 5.8%) and instruction-tuned models (GPT-4.1 1.6% reported as best instruction-tuned), but far below human performance (humans 98.0%). Alternative prompting and extra sampling increased accuracy (one-shot -&gt; alt prompt: 15.8% -&gt; 21.0%; pass@8 -&gt; 35.0%), while multimodal (vision) prompts reduced performance (text zero-shot 12.6% vs vision 5.6%).",
            "limitations_or_failure_cases": "High rate of invalid path constructions (&gt;50% invalid for o4-mini) including Rule Cell Crossing, Intersecting Line, Disconnected Line, Incorrect Start/End; fails on complex interactions (polys and ylops), severe drop with difficulty (near-zero on levels 4/5), limited benefits from increased attempts for hard puzzles, and vision input degraded performance; intermediate reasoning tokens are redacted for closed models limiting trace-based analysis.",
            "uuid": "e8335.0",
            "source_info": {
                "paper_title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "o4-mini (vision)",
            "name_full": "o4-mini Vision (multimodal variant)",
            "brief_description": "Vision-enabled variant of o4-mini evaluated with puzzle screenshots rather than the text-array representation; used to test multimodal prompting effects on spatial reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o4-mini (vision)",
            "model_description": "Multimodal/vision-enabled variant of OpenAI's o4-mini; evaluated by feeding puzzle screenshots and a vision-adapted prompt instead of the text array and example.",
            "model_size": null,
            "puzzle_name": "SPaRC (Spatial Pathfinding Reasoning Challenge)",
            "puzzle_type": "2D grid-based pathfinding and constraint-satisfaction puzzles requiring visual-to-spatial mapping",
            "task_setup": "Vision prompt: screenshot image of puzzle, vision-specific instructions; zero-shot (visual prompt lacked example solution) comparison to textual zero-shot and one-shot setups; same coordinate output format required.",
            "mechanisms_or_strategies": "Multimodal input processing; reasoning in medium-effort mode similar to text variant; no added symbolic plannerâ€”relies on model's multimodal reasoning to interpret visual grid and produce coordinate sequence.",
            "performance_metrics": "Vision performance worse than textual input: o4-mini visual zero-shot accuracy 5.6% vs textual zero-shot 12.6% (reported). Overall accuracy with vision variant substantially lower across difficulty levels.",
            "evidence_of_spatial_reasoning": "Higher rates of structural path errors (Invalid Path, Disconnected Line, Intersecting Line, Rule Cell Crossing) observed in vision mode; suggests difficulty in mapping visual input to the precise textual coordinate system and additional modality-induced complexity rather than improved spatial reasoning.",
            "comparisons": "Underperformed textual prompts for the same model (text zero-shot 12.6%); performance gap larger at easier difficulty levels and diminished at higher difficulty levels.",
            "limitations_or_failure_cases": "Vision prompts increased basic structural errors and reduced overall solving accuracy; multimodal setup did not help and in fact degraded performance, indicating current vision-language integration did not improve spatial planning for these puzzles.",
            "uuid": "e8335.1",
            "source_info": {
                "paper_title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "o3-mini",
            "name_full": "o3-mini (OpenAI reasoning model)",
            "brief_description": "A smaller OpenAI reasoning model evaluated as a baseline; performed worse than o4-mini but still above many open models on some metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o3-mini",
            "model_description": "Proprietary OpenAI reasoning model (smaller/earlier than o4-mini) used with default medium reasoning effort for evaluation.",
            "model_size": null,
            "puzzle_name": "SPaRC (Spatial Pathfinding Reasoning Challenge)",
            "puzzle_type": "2D grid-based pathfinding puzzle requiring constrained spatial planning",
            "task_setup": "Same text-array prompt and extraction procedure as other models; one-shot human example used by default.",
            "mechanisms_or_strategies": "Instruction/reasoning-mode generation; no specialized symbolic planning; evaluated under same ablations (prompting, few-shot, pass@k) as o4-mini where applicable.",
            "performance_metrics": "Overall accuracy reported ~8.2% on SPaRC test set (paper reports o3-mini 8.2%).",
            "evidence_of_spatial_reasoning": "Exhibited many similar error modes to other LLMs (invalid paths, misinterpretation of grid), implying limited reliable spatial reasoning; performed better than some open models but far from human performance.",
            "comparisons": "Underperformed o4-mini (15.8%) but outperformed open models like R1 70B and QwQ in aggregate statistics reported by the authors.",
            "limitations_or_failure_cases": "Low accuracy on medium-to-hard puzzles; frequent path rule violations; limited scaling with difficulty and test-time compute.",
            "uuid": "e8335.2",
            "source_info": {
                "paper_title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "R1 70B",
            "name_full": "DeepSeek R1 70B (R1 Llama 70B Distill)",
            "brief_description": "Open reasoning-tuned model (70B) from DeepSeek used in the paper and analyzed qualitatively since it exposes internal reasoning tokens.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek R1 70B",
            "model_description": "Open model (70B) from DeepSeek-AI described as reasoning-capability incentivized via RL; exposes intermediate reasoning tokens allowing trace-based error analysis.",
            "model_size": "70B",
            "puzzle_name": "SPaRC (Spatial Pathfinding Reasoning Challenge)",
            "puzzle_type": "2D grid-based pathfinding and constraint-satisfaction puzzles requiring multi-step planning",
            "task_setup": "Text-based prompt with one-shot example by default; because model exposes chain-of-thought tokens, authors manually analyzed reasoning traces for failure modes.",
            "mechanisms_or_strategies": "Generates explicit chain-of-thought reasoning tokens which were used for manual qualitative analysis; no external symbolic planner used.",
            "performance_metrics": "Overall accuracy ~4.0% (reported R1 70B ~4.0%); in-depth manual analysis performed on 48 puzzle attempts that produced valid path shapes but failed rule checks.",
            "evidence_of_spatial_reasoning": "Manual token analysis revealed models sometimes recognized mistakes/dead-ends (25/48 cases) and exhibited partial spatial reasoning but commonly committed logical fallacies (36/48), grid/index misinterpretation (26/48), and careless multi-step shortcutting (23/48). Examples of coordinate mis-indexing and attempted line-over-rule-cells were documented (Figures 21-24).",
            "comparisons": "Performed worse than closed OpenAI reasoning models (o4-mini/o3-mini) but provided richer trace data for diagnosing failure modes; contrasted with humans (98% accuracy) and with o4-mini's higher accuracy.",
            "limitations_or_failure_cases": "Errors dominated by logical fallacies, coordinate-system confusion (x/y swaps), drawing on rule cells, and failing to repair recognized mistakes; exposure of reasoning tokens revealed limited but present self-awareness of dead-ends that was not successfully exploited to find solutions.",
            "uuid": "e8335.3",
            "source_info": {
                "paper_title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "QwQ 32B",
            "name_full": "QwQ 32B",
            "brief_description": "An open reasoning model (32B) evaluated on SPaRC showing modest performance and comparatively high token usage in some configurations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ 32B",
            "model_description": "Open reasoning model of 32B parameters referenced as a reasoning-tuned baseline in the study.",
            "model_size": "32B",
            "puzzle_name": "SPaRC (Spatial Pathfinding Reasoning Challenge)",
            "puzzle_type": "2D grid-based pathfinding puzzles requiring spatial and rule-based reasoning",
            "task_setup": "Text-array prompts with one-shot example by default; ablations similar to other models (few-shot, alt prompts, pass@k) applied.",
            "mechanisms_or_strategies": "Reasoning-tuned generation without external symbolic solvers; evaluated for token usage and per-difficulty behavior.",
            "performance_metrics": "Aggregate accuracy reported ~5.8% (paper lists QwQ at 5.8% in some aggregated figures). Table 2 shows per-difficulty behavior for QwQ (Level 1 ~20.9% but falls off quickly for higher levels) indicating strong drop with difficulty.",
            "evidence_of_spatial_reasoning": "Qualitative patterns similar to other reasoning models: success on simpler patterns (gaps split better) but fails on complex interacting rules; token counts show relatively constant token usage across difficulties indicating limited scaling of reasoning steps.",
            "comparisons": "Underperformed closed OpenAI models; sometimes outperformed instruction-tuned models; compared across rule splits it had variable relative performance (e.g., performed near average on dots/gaps splits).",
            "limitations_or_failure_cases": "Sharp performance drop with increasing difficulty; path construction errors and logic mistakes; token usage did not increase substantially with difficulty suggesting poor test-time scaling of computation.",
            "uuid": "e8335.4",
            "source_info": {
                "paper_title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Gemma-3 27B",
            "name_full": "Gemma 3 27B",
            "brief_description": "An instruction-tuned (open) LLM used as an instruction-model baseline; exhibited distinct error patterns (e.g., crossing rule cells).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma 3 27B",
            "model_description": "Instruction-tuned open LLM (27B) used as an instruction-model baseline; evaluated in standard prompt conditions.",
            "model_size": "27B",
            "puzzle_name": "SPaRC (Spatial Pathfinding Reasoning Challenge)",
            "puzzle_type": "2D grid pathfinding requiring spatial planning and rule satisfaction",
            "task_setup": "Text-array prompt with one-shot example by default; standard extraction of coordinates after '####'.",
            "mechanisms_or_strategies": "Instruction-tuned generation; no explicit chain-of-thought fine-tuning reported for this model in the paper.",
            "performance_metrics": "Instruction-tuned models generally performed worse than reasoning models; Gemma-specific overall accuracy not always explicitly tabulated, but authors report instruction models performed worse overall and Gemma produced characteristic errors (e.g., Rule Cell Crossing).",
            "evidence_of_spatial_reasoning": "Produced more Rule Cell Crossings and specific error patterns (Gemma crosses rule cells more often, but produced fewer disconnected lines than some reasoning models), implying partial but brittle spatial understanding.",
            "comparisons": "Underperformed reasoning-tuned closed models (o4-mini, o3-mini) and sometimes had error patterns distinct from other models; smaller models (including Gemma) sometimes performed comparatively better on simpler polys splits.",
            "limitations_or_failure_cases": "High rate of invalid paths and rule-cell crossings; poor scaling with difficulty; multimodal and few-shot ablations did not consistently improve performance.",
            "uuid": "e8335.5",
            "source_info": {
                "paper_title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen 2.5 72B",
            "name_full": "Qwen 2.5 72B",
            "brief_description": "A large instruction-tuned LLM (72B) included as an instruction-model baseline; used to measure token-scaling behavior and instruction-mode performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen 2.5 72B",
            "model_description": "Instruction-tuned model of size 72B used as an instruction baseline; evaluated for accuracy and tokens produced per difficulty.",
            "model_size": "72B",
            "puzzle_name": "SPaRC (Spatial Pathfinding Reasoning Challenge)",
            "puzzle_type": "2D grid-based pathfinding puzzles requiring spatial reasoning and rule compliance",
            "task_setup": "Instruction-style prompt (one-shot by default) and same coordinate output format; token counts and accuracy measured per difficulty.",
            "mechanisms_or_strategies": "Instruction-following generation; no specialized reasoning-finetuning beyond being instruction-tuned; scaled token counts with puzzle difficulty.",
            "performance_metrics": "Per Table 2, Qwen 2.5 had near-zero accuracy on many difficulty levels (Level 1 0.0%, Level 2 1.7%, others 0.0% as reported in Table 2 excerpt), and token usage increased with difficulty (e.g., 790 -&gt; 1161 tokens across levels), indicating instruction models expended more tokens on harder puzzles but without accuracy gains.",
            "evidence_of_spatial_reasoning": "Lack of effective spatial reasoning: despite increasing token usage with difficulty (~40% token increase), accuracy remained near-zero; suggests token scaling without improved planning.",
            "comparisons": "Performed worse than reasoning-tuned models (o4-mini, o3-mini) and far worse than humans; compared in ablations showing instruction models increased tokens more with difficulty than reasoning models.",
            "limitations_or_failure_cases": "Extremely low solving accuracy across difficulties; token-scaling with difficulty did not translate into improved spatial reasoning or success rate.",
            "uuid": "e8335.6",
            "source_info": {
                "paper_title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4.1",
            "name_full": "GPT-4.1 (OpenAI instruction-tuned)",
            "brief_description": "A closed-source instruction-tuned model from OpenAI evaluated as an instruction-model baseline; among instruction-tuned models it achieved the highest reported accuracy on SPaRC.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4.1",
            "model_description": "Proprietary OpenAI instruction-tuned model; used as an instruction baseline under the standard prompt setups.",
            "model_size": null,
            "puzzle_name": "SPaRC (Spatial Pathfinding Reasoning Challenge)",
            "puzzle_type": "2D grid-based pathfinding and constraint puzzles",
            "task_setup": "Text-array prompt with one-shot example used by default; coordinate-list output extracted after delimiter.",
            "mechanisms_or_strategies": "Instruction-following generation; no specialized reasoning mode exposed in the paper for this model (compared to the 'reasoning' mode used for o4/o3).",
            "performance_metrics": "Reported as the best instruction-tuned model in the study with 1.6% accuracy (8/500 puzzles), substantially below reasoning models and humans.",
            "evidence_of_spatial_reasoning": "Low solving rates and frequent structural path errors (GPT-4.1 frequently produced disconnected lines) indicating limited effective spatial planning in this setup.",
            "comparisons": "Underperformed reasoning-tuned models (o4-mini/o3-mini) and human baseline; instruction models overall performed worse than reasoning models in the SPaRC evaluation.",
            "limitations_or_failure_cases": "Very low overall accuracy and specific path-structure errors (disconnected lines) were frequent; instruction tuning did not compensate for multi-step spatial planning deficits in these puzzles.",
            "uuid": "e8335.7",
            "source_info": {
                "paper_title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ReasonFlux-F1 32B",
            "name_full": "ReasonFlux-F1 32B",
            "brief_description": "A reasoning fine-tuned model family (ReasonFlux) evaluated to test whether long-context reasoning fine-tuning helps SPaRC performance; showed modest gains but remained far below human levels.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ReasonFlux-F1 32B",
            "model_description": "Reasoning fine-tuned model aimed at long-chain reasoning and trajectory-aware PRMs (paper cites ReasonFlux family); tested against Qwen 3 32B as a long-context reasoning fine-tuned baseline.",
            "model_size": "32B",
            "puzzle_name": "SPaRC (Spatial Pathfinding Reasoning Challenge)",
            "puzzle_type": "2D grid pathfinding puzzles requiring extended chain-of-thought planning",
            "task_setup": "Evaluated with the same prompt/one-shot configuration as other models; compared specifically in an ablation to test long-context reasoning fine-tuning effects.",
            "mechanisms_or_strategies": "Long-context reasoning fine-tuning (ReasonFlux family) designed to improve chain-of-thought stability over long trajectories; no external symbolic planner.",
            "performance_metrics": "ReasonFlux-F1 32B performance reported comparable but still low relative to human baseline (example given: ReasonFlux-F1 32B 8.6% vs another reasoning model 6.0% in Figure 11 comparison), indicating only modest improvements.",
            "evidence_of_spatial_reasoning": "Small gains relative to other reasoning-tuned models indicate limited improvement in multi-step spatial planning from ReasonFlux-style fine-tuning; still far from human levels.",
            "comparisons": "Compared to Qwen 3 32B; ReasonFlux-F1 showed comparable or small improvements (8.6% vs 6.0% cited), but remained much lower than top closed models and humans.",
            "limitations_or_failure_cases": "Fine-tuning for long-context reasoning provided only modest gains and did not solve core failure modes (path validity, logical fallacies, coordinate misinterpretation).",
            "uuid": "e8335.8",
            "source_info": {
                "paper_title": "SPaRC: A Spatial Pathfinding Reasoning Challenge",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ARC-AGI",
            "rating": 2
        },
        {
            "paper_title": "Is a picture worth a thousand words? delving into spatial reasoning for vision language models",
            "rating": 2,
            "sanitized_title": "is_a_picture_worth_a_thousand_words_delving_into_spatial_reasoning_for_vision_language_models"
        },
        {
            "paper_title": "Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_good_path_planners_a_benchmark_and_investigation_on_spatialtemporal_reasoning"
        },
        {
            "paper_title": "PPNL",
            "rating": 2
        },
        {
            "paper_title": "VisualPuzzles",
            "rating": 2,
            "sanitized_title": "visualpuzzles"
        },
        {
            "paper_title": "EnigmaEval",
            "rating": 1,
            "sanitized_title": "enigmaeval"
        }
    ],
    "cost": 0.01814225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SPaRC: A Spatial Pathfinding Reasoning Challenge
19 Sep 2025</p>
<p>Lars Benedikt Kaesberg l.kaesberg@uni-goettingen.de 
University of GÃ¶ttingen
Germany</p>
<p>Jan Philip Wahle wahle@uni-goettingen.de 
University of GÃ¶ttingen
Germany</p>
<p>Terry Ruas 
University of GÃ¶ttingen
Germany</p>
<p>Bela Gipp 
University of GÃ¶ttingen
Germany</p>
<p>SPaRC: A Spatial Pathfinding Reasoning Challenge
19 Sep 20254CDC589A9E861CB15DAFAE940DC45D42arXiv:2505.16686v2[cs.AI]
Existing reasoning datasets saturate and fail to test abstract, multi-step problems, especially pathfinding and complex rule constraint satisfaction.We introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000 2D grid pathfinding puzzles to evaluate spatial and rule-based reasoning, requiring stepby-step planning with arithmetic and geometric rules.Humans achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).Models often generate invalid paths (&gt;50% of puzzles for o4-mini), and reasoning tokens reveal they make errors in navigation and spatial logic.Unlike humans, who take longer on hard puzzles, models fail to scale test-time compute with difficulty.Allowing models to make multiple solution attempts improves accuracy, suggesting potential for better spatial reasoning with improved training and efficient test-time scaling methods.SPaRC can be used as a window into models' spatial reasoning limitations and drive research toward new methods that excel in abstract, multi-step problem-solving.</p>
<p>Introduction</p>
<p>Reasoning models made stark progress to solve complex mathematical (Hendrycks et al., 2021b), software-engineering (Jimenez et al., 2023;Quan et al., 2025), and knowledge tasks (Hendrycks et al., 2021a).With more capable models comes the question of how to measure their progress in reasoning, and how they compare to humans.As reasoning benchmarks test specific tasks with priors (e.g., (MMLU-Pro (Wang et al., 2024b), GPQA (Rein et al., 2023)), models started to achieve (super-) human scores, leading to rapid dataset saturation.Thus, datasets probing abstract reasoning with minimal priors have become increasingly important as they are more robust to scaling training data and pattern-matching.Notably, ARC-AGI (Chollet, 2019) and related works (Song et al., 2025;   Wang et al., 2024a) challenge models with spatial few-shot grid problems.However, they often do not require a combination of step-by-step planning, pathfinding, and logic skills; abilities most human possesses (Chollet, 2019).Spatial reasoning is an important component for solving tasks like navigation and manipulation in robotics, scene understanding in computer vision, and augmented reality.</p>
<p>We propose SPaRC, a new dataset to overcome limitations of current datasets, primarily focusing on pathfinding and the combination of arithmetic and geometric rules, such as counting, segregation, and shape logic, by presenting multi-step constraint problems.Our proposed task consists of 2D grid puzzles through which a line must be drawn from start to end while fulfilling various rules, such as collecting dots along the way or separating colored</p>
<p>Accuracy (%)</p>
<p>93.4% 81.4% 41.8%</p>
<p>14.28% 15.8% 1.1%</p>
<p>Other Benchmarks SPaRC (Ours)</p>
<p>Figure 2: Accuracy (%) of o4-mini on existing benchmarks and on SPaRC, as well as on only hard puzzles from SPaRC with difficulty 5.</p>
<p>elements (see Figure 1 for an example).These rules can be combined in various non-trivial ways and involve deep, abstract, rule-based reasoning within a constrained spatial pathfinding environment.We use text-based grids rather than images to avoid testing perception and instead isolate pure spatial reasoning and planning ability.Solving these puzzles requires an understanding of the individual rules and their connections, and long-term planning to meet all rules simultaneously.This often involves revising previous hypotheses where a single wrong step can irrevocably lead to the wrong path.We provide 500 train and 500 test puzzles of different sizes and difficulty degrees from 1 (very easy) to 5 (very hard).</p>
<p>Experiments with three instruction-tuned models, four reasoning models, and six human annotators show puzzles are solved easily by humans at 98% accuracy (94.5% for difficulty 5 puzzles) but challenge the best reasoning model, o4-mini, at 15.8% accuracy (1.1% for difficulty 5 puzzles).Figure 2 compares the accuracy of the best reasoning model we tested (o4-mini) on existing reasoning benchmarks with our proposed SPaRC, showing that it poses a new challenge for models.Models often fail to generate valid paths, and reasoning tokens reveal issues with grid navigation, spatial logic, and careless mistakes that lead to irreversible errors.Humans take up to 13 times longer on harder puzzles.Instruction-tuned models increase test-time tokens by âˆ¼ 40%, and reasoning models only by âˆ¼ 5% with higher difficulty.Multiple attempts per puzzle raise accuracy (e.g., 15.8% to 35.0% for o4-mini), indicating inefficient solutionfinding and potential for improved spatial reasoning training.Ablations show prompt design (15.8% to 21.0%) and few-shot examples (12.6% to 15.8%) have modest effects, and multimodal prompting (i.e., puzzle screenshots) does not improve performance over text (12.6% vs. 5.6% for o4-mini).</p>
<p>SPaRC provides a new challenge to evaluate spatial and rule-based reasoning in large language models (LLMs), addressing limitations of existing saturated benchmarks.</p>
<p>showing that even in simple scenarios, the most advanced reasoning models fail.However, ARC-AGI does not require step-by-step planning or following discrete rules.VisualPuzzles (Song et al., 2025) presents algorithmic, analogical, and spatial riddles, but every task is multiple choice, so the model is not constructing individual solutions.SpatialEval (Wang et al., 2024a) covers navigation, relation, and counting on images, 2D grids, and text.However, SpatialEval mazes span only a few moves, and the counting or relation questions appear independently, not within one combined task.PPNL (Aghzal et al., 2023) tests spatial-temporal reasoning via 2D grid-based path planning.It focuses on obstacle avoidance within the grid and does not incorporate complex, interacting rules.Another related task is EnigmaEval (Wang et al., 2025), but it does not focus on pathfinding.</p>
<p>SPaRC addresses these limitations by requiring long-term, step-by-step path planning, where early errors in the reasoning chain can significantly impact later steps.SPaRC requires path-finding, counting, segregation, and logic involving colors and shapes in a single task, and on different-sized puzzle grids with complex, interacting rules.Unlike other benchmarks, we also support problems with multiple correct solutions, allowing for testing different path-finding strategies and not relying on a single solution per example.</p>
<p>Dataset</p>
<p>The primary goal of SPaRC is to test new pathfinding capabilities not represented in current benchmarks, specifically spatial navigation, rule understanding, constraint satisfaction, and multi-step planning, and also combinations in new ways, such as counting, segregation, and color or shape logic.The design of the dataset is inspired by the puzzle mechanics of the video game The Witness (Blow, 2016), adapted into a format suitable for LLM assessment.Shape Fitting (Polyominoes): If a cell contains a polyomino (poly), the solution path must enclose a region that matches its exact shape and area.The region must not rotate or mirror the poly.Multiple polys can share a region if their shapes fit without overlapping.</p>
<p>Puzzle Rules</p>
<p>Each puzzle in</p>
<p>Shape Subtraction (Ylop): A ylop must be enclosed in the same region as one or more polys.Its shape and area subtract from the total required by the polys.If a ylop cancels out a poly exactly, that pair imposes no constraint.</p>
<p>Dataset Creation</p>
<p>Generation.Our process starts with randomly creating an x by y grid, where x and y range from 2 to 6 (e.g., 3 Ã— 5). Figure 10 in Section B provides an indexing example.We then randomly fill half of the grid with rule cells.The rule cell to grid cell percentage is termed rule density and set a random start and end point.</p>
<p>To solve puzzles automatically, we implement a generation-validation loop.First, we generate an initial puzzle and solve it using brute-force by exhaustively testing all valid paths from start to end. 1 If the initial puzzle fails to produce a solvable puzzle, we decrease the rule density and regenerate the puzzle.Conversely, if the solver finds over k distinct solutions (indicating the puzzle might be too unconstrained), rule density is increased, and the puzzle is regenerated.We found 50 solutions to be a reasonable hyperparameter choice for k empirically by testing different generation setups.</p>
<p>We generate the SPaRC dataset containing 500 training and 500 testing examples.The distributions of different rules in SPaRC are shown in Table 1.When sampling puzzles, we aim for an approximately equal distribution between rules.However, puzzles tend to have fewer stars (color pairing rule) and triangles (edge counting rule) than other rules.Observe how in Table 1, we only generated 25 puzzles containing ylops.This is for two reasons: they can only exist if polys are available, and they are the hardest rule, as judged by humans.For later tests on specific rules, we also created single-rule splits (more on this in Section 4.1).</p>
<p>Difficulty Estimation.To quantify puzzle complexity, we created a difficulty metric that weights individual spatial reasoning tasks.More specifically, the number of distinct rules, the total number of rule cells, the rule cell density, and an estimate of potential complex rule interactions.Each element contributes via a weighted sum to a raw score, which we then statistically normalize onto a standardized 1 (easiest) to 5 (hardest) scale (see Section D for calculation specifics).As our later experiments with humans and reasoning models will demonstrate, this difficulty estimate is robust.The distribution of difficulties of SPaRC is detailed in Table 1.We sample with an approximately uniform distribution between puzzles, ending up with slightly more level 3 (121) and level 2 puzzles (118) than level 1 (86), level 4 (86), and level 5 (89).</p>
<p>Experiments</p>
<p>We assess SPaRC through automated and manual studies.In the automated evaluation, we consider instruction-based models -Gemma 3 27B (Team et al., 2025), Qwen 2.5 72B (Research, 2024), GPT 4.1 (OpenAI, 2025a); and reasoning models -QwQ 32B (Team, 2024), DeepSeek R1 Distill Llama 1 Brute-force is necessary because many puzzles fall into NP or NP-Complete complexity classes (Abel et al., 2018).</p>
<p>Statistics Count</p>
<p>Puzzles 70B (DeepSeek-AI et al., 2025), o3-Mini (Ope-nAI, 2025c), and o4-mini (OpenAI, 2025b).We measure model accuracy on solving our puzzles (Section 4.1), performance on specific rule cells, reasoning errors (Section 4.2), and conduct ablation studies regarding the stability of our findings (Section 4.3).For the manual inspection, we test human performance and time on the same puzzles (Section 4.4).We used six annotators (aged 22-27) with CS backgrounds.</p>
<p>Setup.All puzzles are presented to the LLMs using prompts with a human-annotated example solution.Our textual representation is inspired by the ARC challenge (Chollet, 2019).Extraction occurs using regex after a predefined sequence of "####" as stated in the prompt.By default, we provide a one-shot example with a human-annotated path, as it yielded the best results (Section 4.3).Section I.4 contains details about the prompts, examples, and their solution.Details on models, hardware, and tokens processed are in Section A.</p>
<p>Main Results</p>
<p>We present key baseline evaluations across models and difficulty.Scaling test-time compute allows us to identify upper bounds of model capabilities.</p>
<p>Baselines.We want to understand how reasoningand instruction-tuned LLMs solve spatial multistep reasoning tasks compared to humans.We compute accuracy (% of solved puzzles) for these models.Human baseline results use majority votes from three annotators per puzzle (details on the human evaluation later in Section 4.4).Figure 3 shows accuracy for humans and LLMs.Humans solve puzzles nearly perfectly at 98.0% (98/100   (Huang and Chang, 2022;Turpin et al., 2023).
H u m a n o 4 -m in i o 3 -m in i Q w Q 3 2 B R
Difficulty Level.We decompose the results in Figure 3 by difficulty.We compare the best model (o4-mini) against human performance.Humans achieve 100% accuracy at difficulty level 1, while o4-mini reaches 47.7%, showing it solves nearly half of the simple puzzles.At level 2, o4-mini drops to 19.5%, but humans remain at 100%.For higher difficulties, with larger grids and complex rules, o4-mini's rate decreases further, reaching 1.2% at level 4 (solving 1 of 89 puzzles), compared to 94.4% for humans.Level 5 shows similar results to level 4 (similar trends across all models).Results for all models are in Section J. Overall, LLMs have severe reasoning challenges as puzzle difficulty increases.A possible explanation could be that models conclude reasoning prematurely in complex puzzles by ignoring certain rules and running into dead ends.Specific rules or combinations of rules may also be particularly challenging.</p>
<p>Rule Specific Analysis.We examine the accuracy of models on splits containing individual rules or specific rule combinations to analyze which rules the models succeed or fail on.Specifically, we  create puzzles only containing gaps, dots, stones, stars, triangles, polys, or ylops.We also create multi-rule combination splits to investigate how models handle the interaction between a few distinct types of rules, stones x stars, gaps x dots x triangles, and dots x stars x polys.Because ylops can only exist in the presence of polys, this split contains puzzles with ylops and polys.Each split contains 50 training and 50 test samples, and we also make them available in our release.</p>
<p>Figure 5 shows accuracy for the primary dataset (top row) and the relative performance delta (âˆ†) of specific splits (e.g., gap accuracy minus full set accuracy; bottom rows).</p>
<p>The gaps split shows superior performance across all models, whereas dots hover near the average model performance on all puzzles.Dots and gaps tasks are similar yet differ in performance: gaps explicitly forbid using edges, providing immediate error feedback, whereas dots require edge use, with errors apparent only after path completion.Polys produce mixed results; stronger models (o4-mini, o3-mini) show minor performance differences compared to all puzzles, while smaller reasoning and instruction-tuned models markedly improve.Polys and ylops lead to substantial performance decreases, which are also the most challenging rules perceived by humans.Some weaker models (QwQ, Gemma) markedly outperform their average on polys (improvements of 13.2 and 12.8 points, respectively), suggesting smaller models might solve some puzzles more intuitively, while others tend to "overthink" problems, leading to higher success in simpler setups (more details later in Section 4.2).Performance differences may also result from fundamental path construction errors, logical mistakes, or model rule misinterpretations.</p>
<p>Path Errors and Reasoning Mistakes</p>
<p>We analyze model-constructed paths and their reasoning tokens to shed light on why reasoning models fail to solve puzzles.</p>
<p>Path Errors</p>
<p>We analyze common errors in constructing a valid path (e.g., ignoring rules to solve the game for now).We assess five error types for all models: Incorrect Start/End (i.e., line starts or ends at the wrong edge), Disconnected Line (i.e., line not continuous), Intersecting Line (i.e., line crosses an edge multiple times), Rule Cell Cross-ing (i.e., line does not stay on edges but crosses rule cells).Paths with any such errors are deemed Invalid Path.Examples for each error type can be found in Section K.</p>
<p>Figure 6 shows the percentage of path rule violations for four selected models.Results for all models can be found in Table 7 in Section J. Smaller enclosed areas in the figure imply better adherence to path rules.The two reasoning models (o4-mini and R1 70B) have similar violation patterns, but o4mini performs better overall.Notably, over 50% of puzzles fail because models do not construct valid paths.Instruction models (GPT-4.1,Gemma-3 27B) perform worse, showing distinct weaknesses.GPT-4.1 frequently produces disconnected lines, while Gemma-3 27B commonly crosses rule cells.Interestingly, Gemma-3 27B produces fewer disconnected lines than the larger reasoning model R1 70B.Reasoning models have higher accuracy despite similar basic path errors, suggesting successful path construction is only the first hurdle.Across models, the most common error is Rule Cell Crossing, indicating frequent violations by paths moving through rule cells rather than along edges.However, up to this point, our explanations of other model failures have been largely hypotheses, and the precise underlying causes require further investigation.</p>
<p>Causes for Reasoning Mistakes.To shed light on the "why" of reasoning model failures, we manu- pass@1 pass@2 pass@4 pass@8</p>
<p>Figure 7: Accuracy (%) for generating k âˆˆ {1, 2, 4, 8} solutions and evaluating whether the correct path is in one of the k attempts (pass@k) for o4-mini across difficulty (1-5).Higher is better.</p>
<p>ally analyzed R1 70B puzzles (as it openly provides reasoning tokens) with the puzzles containing only single rule types (e.g., only stones).We selected puzzles where models produced valid paths (without path errors) but failed to fulfill all rule cells.This resulted in 48 puzzle solutions for analysis.Models most commonly failed due to logical fallacies (36/48), grid/index system misinterpretation (26/48), and careless shortcutting of multiple reasoning steps (23/48).Interestingly, R1 often recognized mistakes or dead ends (25/48) before concluding, indicating limited reasoning but awareness of its constraints.</p>
<p>Different splits revealed specific reasoning limitations.With dots, models typically recognized missed ones during path construction but failed to correct their paths accordingly (e.g., Figure 23 in Section L).With gaps, models frequently made careless, unvalidated multi-step moves, violating rules by crossing gaps (e.g., Figure 21 in Section L).We provide further examples with highlights of R1's reasoning tokens in Section L.</p>
<p>Mistakes, like unvalidated multi-step moves and grid misinterpretation, highlight ongoing challenges in long-term spatial planning, as even minor shortcuts lead to significant rule violations.However, models' recognition of errors and dead ends points toward opportunities and gives space for future contributions to improve spatial reasoning, e.g., via iterative reasoning or sampling multiple parallel paths with strategies to find correct ones.</p>
<p>Upper Reasoning Bounds.A common strategy to improve performance on complex tasks is to scale test-time compute, for instance through multi- agent debate (Becker et al., 2025a).However, this approach can suffer from performance degradation in discussions requiring longer reasoning chains (Becker et al., 2025b).Given that SPaRC requires long-term, step-by-step planning where early errors can be critical, this makes such a debate-based approach potentially less suitable.Therefore, to determine models' upper limits, we purposefully increase test-time compute by generating up to eight attempts per puzzle for each model (i.e., pass@1 to pass@8).</p>
<p>Figure 7 shows accuracy rising from 15.8% (pass@1) to 35.0% (pass@8) for o4-mini.This improvement is expected as we scale computation.Importantly, this setting is not practical at test-time, as we only verify if the solution appears among the k generations.In practice, a decision mechanism like majority voting would be more suitable (Kaesberg et al., 2025).</p>
<p>Still, additional attempts are not sufficient to solve complex puzzles.Success rates improve by 32.5 points for level 1 puzzles (easy), but only 2.3 points for level 4 and 5 (difficult ones).This shows that our puzzles cannot be easily solved by just increasing the computation, but the reasoning steps have to get more sophisticated and have to adjust according to the difficulty level.Higher results for larger k give hope that future work can find better training methods to improve reasoning.</p>
<p>Ablations</p>
<p>We investigate how changes to the prompting (e.g., few-shot examples, different prompts) and puzzle representation (e.g., text and images) affect our results through various ablations.Figure 8 shows that one-shot has the highest overall accuracy (15.8%), while zero-shot performs worst (12.6%) for o4-mini.At difficulty 1, two-shot outperforms one-shot, but this reverses at higher levels.Examples generally help model comprehension, but too many examples seem to have no benefit, and sometimes negatively impact performance.Additional analysis in Section M.3 shows that zero-shot has fewer path violations than fewshot.</p>
<p>Improved one-shot over zero-shot performance is expected, but two-shot's slightly lower performance than one-shot is surprising, as more examples should clarify rule interactions; however, given small differences, stochastic variance is possible.Similar findings were reported by Ye et al. (2023), suggesting increased examples do not always help, possibly due to cognitive overload or excessive focus on example analysis instead of task solving.</p>
<p>Visual Representation.Another factor that might influence our results is the textual 2D representation of the puzzles.Therefore, we provide screenshots of the puzzle, similar to Figure 1, and adjust the prompt accordingly.We compare visual results to zero-shot textual results, as the visual prompt lacks an example solution.Details on this configuration are available in Section I.3.</p>
<p>Figure 9 compares the accuracy of o4-mini using default textual prompts versus visual prompts across difficulty levels.The visual representation reduces overall performance from 12.6% to 5.6%.The gap between text and image prompts is larger at easier difficulty levels, but diminishes at higher difficulty levels.Additional analysis in Section M.1 shows that a main cause for bad results on easy puzzles is invalid path constructions.These results suggest that current textual representations are easier for multi-modal reasoning models to understand.Likely, connecting textual descriptions to visual puzzle elements adds complexity compared to purely textual prompts.However, whether the current textual representation is also optimal requires more investigation.</p>
<p>Alternative Prompt.We test if our results are affected by different formulations in our prompts, i.e., prompt engineering (White et al., 2023;Wahle et al., 2024).Because paths previously failed due to violations of path rules, we adjusted the prompt to emphasize these rules more explicitly.This adjustment improved o4-mini's accuracy from 15.8% to 21.0% and reduced path errors, with Rule Cell Crossing decreasing from 51.2% to 29.0%, and Intersecting Line dropping from 31.2% to 21.2%.However, at higher difficulty (level 5), there was no improvement, with o4-mini still only solving 1 out of 89 puzzles (more details in Figure 26 in Section M).Prompt engineering moderately increases performance at lower difficulty levels, but it does not have a marked impact at higher levels.In additional experiments, long-context reasoning-tuned models provided only modest gains (see Figure 11 in Section E).Markdown-based grids also did not improve over the array format puzzle representation (see Figure 12 in Section E).These results suggest that low task performance is due to limited spatial reasoning skills rather than representation style or prompting.</p>
<p>Human Evaluation</p>
<p>For a human baseline, we asked six annotators aged 22-27, with a background in computer science and data science, to solve 100 i.i.d.drawn puzzles from the dataset, divided into two subsets of 50 samples each.Each of the 100 puzzles is annotated three times, and a puzzle is marked as solved if the majority found a correct solution.Even though we did not test all 500 test samples of SPaRC, sampling i.i.d., and using two non-overlapping sets with three annotators each gives us a fair estimate of human performance.We recorded the accuracy, number of attempts, and solving time.Details on annotation instructions are in Section N.</p>
<p>Table 2 shows humans achieve near-perfect performance, with 100% accuracy at difficulty levels 1-3 and around 95% at levels 4 and 5. Average solve time increases exponentially with difficulty, from 10.7 seconds for difficulty 1 to 26.7 seconds for difficulty 3, then starkly increasing to 60.7 seconds for difficulty 4 and 131.5 seconds for difficulty 5. Median solve times are consistently lower than average times, indicating that a few very difficult puzzles significantly increase the average.</p>
<p>Compared to humans, models show two relevant time-scaling aspects.First, previous pass@k experiments (Figure 7 in Section 4.3) showed that multiple attempts to solve puzzles improved performance on easy puzzles but did not increase performance on difficult puzzles.Second, analyzing the number of generated tokens (Table 2), instructiontuned models such as Qwen 2.5, increase token counts with puzzle difficulty (from 790 to 1161), while reasoning models, such as QwQ maintain relatively constant token counts across difficulties (14433 to 13114).See Table 4 in Section F for all models.This suggests models do not effectively scale spatial reasoning at test-time.</p>
<p>Conclusion</p>
<p>We introduced SPaRC, a dataset of 1,000 examples designed to evaluate spatial and rule-based reasoning capabilities on 2D grid pathfinding puzzles.This dataset tests reasoning skills not evaluated by existing benchmarks, focusing specifically on multi-step constraint satisfaction problems requiring spatial and rule-based reasoning.</p>
<p>We evaluated puzzles with six human annotators, three instruction-tuned models (GPT-4.1,Gemma 3, Qwen 2.5), and four reasoning models (o4-mini, o3-mini, QwQ, R1).Humans achieved a 98% accuracy.The best reasoning model, o4-mini, reached only 16%.Performance was drastically affected by puzzle difficulty, with models solving 48% at level 1, 20% at level 2, and just 1.1% at level 5. Humans consistently solved puzzles across levels, including 95% at level 5. Our error analysis revealed that path errors and reasoning mistakes stemmed from logical fallacies, grid misunderstandings, and performing too many reasoning steps at once.Generating up to eight attempts per puzzle improved accuracy up to 30% for difficulty 1 puzzles and 2% for difficulty 5. Humans needed up to 13 times more time to solve hard puzzles than easy ones, and instruction-tuned models scaled token usage with difficulty by âˆ¼ 40%.Reasoning models showed only a âˆ¼ 5% increase for harder difficulties.Ablation studies on visual puzzle representation, prompting, and few-shot examples show only mild variations and support the robustness of our results.</p>
<p>Empirically, SPaRC reveals critical limitations in current reasoning models regarding spatial reasoning, rule-based reasoning, multi-step planning, and constraint satisfaction.Existing methods, including enhanced prompting and increased computational sampling, offer only partial improvements.Fundamental advances in model reasoning capabilities are still needed to reach human-level results.</p>
<p>Limitations</p>
<p>Our evaluation depends on a fixed delimiter ("####") and a regex that collects the following coordinate list.When a model omits the delimiter, writes several delimiter lines, or inserts natural language text between coordinates, extraction can fail, producing false negatives.These events are rare in practice, and we stress the required format in every prompt, but complete robustness is unattainable when testing many different models.</p>
<p>OpenAI models (i.e., o4-mini, o3-mini) return only final coordinates with a small explanation, but redact intermediate reasoning tokens.Consequently, detailed failure analysis is restricted to open models like R1 70B.Intermediate reasoning can differ from final answers in models of any scale, as previously documented by Turpin et al. (2023); Chen et al. (2025), thus potentially limiting generalization from trace-based analyses.</p>
<p>The dataset covers single-rule puzzles and a limited set of two-and three-rule combinations but does not exhaustively represent all possible interactions among the seven rule types.Future releases could introduce underrepresented combinations (e.g., stars Ã— triangles Ã— polys Ã— ylops) to probe generalization more comprehensively.However, as models fail on most easy tasks already and current splits reveal clear error patterns and support comparative ranking of the different rule types, we leave this to future work when models become more capable.</p>
<p>The poly set in Figure 5 in Section 4.1 shows improvements for weaker but not stronger models.The poly rule sometimes fills the entire grid with poly shapes, necessitating a path along the grid's edge.This condition impacts only the poly subset, explaining performance spikes.Smaller models find this shortcut more frequently, likely because simpler solutions emerge when overwhelmed by many complex poly shapes.</p>
<p>Appendix A Models &amp; Hardware</p>
<p>This section details the large language models (LLM) used in our experiments, the hardware on which they were run, and the approximate number of tokens processed for each model.</p>
<p>For open models we used Gemma 3 27B (Team et al., 2025), QwQ 32B (Team, 2024) For propietary models we used GPT-4.1 (Ope-nAI, 2025a), o3-mini (OpenAI, 2025c) and o4-mini (OpenAI, 2025b).For both OpenAI reasoning models, the default medium effort reasoning mode was used.</p>
<p>Table 3 shows the details regarding model size, tokens processed, and hardware used.For all ablations and the main study, o4-mini was analyzed on 6500 puzzles overall.For a comparable 1000 puzzles, this would equate to approximately 9,865,411 tokens.Both OpenAI reasoning models were used with the medium reasoning effort.</p>
<p>B Grid Indexing</p>
<p>Figure 10 shows a puzzle grid with all its coordinates according to the prompts in Section I.</p>
<p>C Licenses and Code Acknowledgments</p>
<p>The source code developed and used in this work is provided under the BSD 3-Clause License.This licensing choice is required due to dependencies on code from the following repositories, which are partially distributed under the BSD 3-Clause License:</p>
<p>â€¢ jbzdarkid/jbzdarkid.github.io</p>
<p>â€¢ NewSoupVi/The-Witness-Randomizer-for-Archipelago</p>
<p>We gratefully acknowledge the authors of these repositories for making their implementations publicly available.</p>
<p>The datasets used in this study are provided under the Creative Commons Attribution 4.0 International (CC-BY-4.0)license.</p>
<p>D Difficulty Metric Calculation</p>
<p>This section provides the details for calculating the difficulty metric used to rate SPaRC puzzles in this paper.The metric aims to capture multiple aspects of complexity.The calculation involves determining individual component scores, combining them via a weighted sum, and normalizing the result.The score function is described in Section D.1 and its components in Section D.2.</p>
<p>D.1 Combination and Normalization</p>
<p>The individual component scores (Section D.2) are combined using a weighted sum to produce a raw difficulty score (S raw ).The specific weights reflect the empirically determined relative importance of each component:
S raw = w mech S mech + w interact S interact + w grid S grid + w density S density + w count S count
where the weights used are: w mech = 1.2, w interact = 1.2, w grid = 2.5, w density = 1.0, and w count = 1.2.Notably, grid size (S grid ) is weighted most heavily.</p>
<p>Finally, to produce a standardized and interpretable difficulty score (typically ranging from 0 to 5), the raw score (S raw ) is normalized.This is achieved by: 1. Calculating the Z-score of S raw relative to a pre-determined normal distribution, characterized by a mean (Âµ = 12.06) and standard deviation (Ïƒ = 5.27).These parameters were derived empirically from a large dataset of puzzle scores.Final Score = max(0, min(5, Scaled_Score))</p>
<p>This normalization process ensures that scores are comparable across different puzzles and provides a distribution more amenable to interpretation as a rating.</p>
<p>D.2 Component Scores</p>
<p>Five distinct aspects of the puzzle contribute to the overall difficulty score:</p>
<p>â€¢ Mechanics Score (S mech ): This score reflects the cognitive load associated with understanding different rules.It is directly proportional to the number of unique rule types present in the puzzle (N mech ).</p>
<p>â€¢ Interaction Score (S interact ): This score quantifies complexity from the interplay between different mechanics.It is calculated only when multiple rule types (N mech &gt; 1) are present.It is proportional to both the number of potentially interacting mechanics (approximated as N mech âˆ’ 1) and the rule density (Ï rules ), where rule density is the total number of rule instances (N rules ) divided by the grid area (A = width Ã— height).</p>
<p>â€¢ Grid Score (S grid ): This component reflects the complexity associated with the search space size.It increases proportionally with the grid area (A).Larger grids generally require more path exploration.</p>
<p>â€¢ Density Score (S density ): This score measures constraint concentration.It is directly derived from the rule density (Ï rules = N rules /A).</p>
<p>Higher density can make satisfying all constraints simultaneously more challenging.</p>
<p>â€¢ Rule Count Score (S count ): Independent of density, this score considers the absolute number of constraints.It is proportional to the total number of rule instances (N rules ) on the grid.A puzzle with many rules can be complex even if spread over a large grid.</p>
<p>E Prompting and Representation</p>
<p>In addition to the main results, we also evaluated the effect of a different reasoning-oriented fine-tuning method and puzzle grid representation.These experiments provide insights into whether prompting style or input format can improve the bad spatial reasoning performance of LLMs observed on SPaRC.</p>
<p>E.1 Reasoning Fine-Tuned Models</p>
<p>While most existing prompting methods are designed for instruction-tuned models, we investigated the ReasonFlux family of reasoning finetuned models, which use a different reasoning method in the fine-tuning step compared to the Qwen 3 models and should perform better on long context tasks (Zou et al., 2025).Figure 11 compares Qwen 3 32B against ReasonFlux-F1 32B.We find that the ReasonFlux-F1 32B model performs comparably to another reasoning-tuned model of the same size (8.6% vs. 6.0%).These results show that the long-context specific reasoning fine-tuning can help, but gains remain small relative to human performance.</p>
<p>E.2 Puzzle Grid Representations</p>
<p>We also investigated the influence of puzzle grid representations.Besides the default ARC-AGIinspired array representation (Array), we tested two markdown-based formats: a plain markdown table without headers (Table ), and a table including row and column coordinates to assist with spatial referencing (Coords).</p>
<p>As shown in Figure 12, neither markdown variant yielded consistent improvements.The baseline array format achieved the highest overall accuracy (21.0%), slightly outperforming both markdown representations (19.0% and 20.8%).These results suggest that failures on SPaRC stem from fundamental limitations in spatial reasoning rather than representation format.</p>
<p>F Tokens by Puzzle Difficulty</p>
<p>G Rule Visualizations</p>
<p>I Prompting</p>
<p>Listings 1 to 5 in Sections I.1 to I.4 provide the prompts and few-shot examples used for the experiments in Section 4.</p>
<p>I.1 Default Prompt</p>
<p>You are an expert spatial reasoning AI specializing in solving puzzles from the game 'The Witness'.Your task is to solve the following puzzle by finding a valid line from the Start Node to the End Node.</p>
<p>GRID DEFINITION:</p>
<p>-The puzzle involves a grid of {grid_size['width']}x{grid_size['height']} cells.</p>
<p>-COORDINATE SYSTEM: Nodes are indexed (x, y).Node (0,0) is the top-left node.x increases to the right, y increases downward.-Line: The solution line travels along grid edges, connecting adjacent nodes horizontally or vertically.The line cannot visit the same node twice.-RULE PLACEMENT: Rule symbols (squares, stars, polyshapes, negative polyshapes, triangles) are located at cells with all odd coordinates.The line goes AROUND cells containing rules, forming boundaries.</p>
<p>SOLVING RULES: 1. Draw a continuous line from the START NODE to the END NODE by connecting adjacent nodes ( horizontally or vertically) without visiting the same node twice.2. The line can only be placed on (+) and (.) cells.These cells have at least one even coordinate.</p>
<p>The line can NEVER be placed on a rule cell (all odd coordinates).3. The line acts as a boundary, potentially dividing the grid cells into one or more distinct regions.4. All rules associated with symbols on the grid must be satisfied:</p>
<p>-Gaps ('G'): The line CANNOT traverse a cell marked by a Gap.</p>
<p>-Dots ('.'):The line MUST pass through a cell marked by a Dot.</p>
<p>-Squares ('o-X'): All squares within a single region created by the line must be the same color.Different colored squares MUST be separated into different regions by the line.-Stars ('*-X'): Each star must be paired with EXACTLY one other element of the same color in a region.Other colors are ignored.</p>
<p>-Triangles ('A-X (1)', 'B-X (2)', 'C-X (3)', 'D-X (4)'): The line must touch EXACTLY the number of edges specified by the triangle count (edges are top, right, bottom, left of the cell).</p>
<p>-Polyshapes ('P-X-Y'): The region containing this symbol must be shaped EXACTLY like the defined polyshape Y.The shape must fit entirely within the region's boundaries.If multiple positive polyshapes are in one region, the region's shape must accommodate their combined, nonoverlapping forms (like Tetris pieces).</p>
<p>-Negative Polyshapes ('Y-X-Y'): The negative polyshape can only be placed on top of already placed normal polyshapes.The negative polyshapes must fit on the grid, but can allow overlap between normal polyshapes or placement of polyshapes that extend beyond the area defined by the line.If the negative polyshapes exactly cancel the normal polyshapes, there is no restriction on the grid shape anymore.A negative polyshape only counts as valid if it is used.Listing 2: The LLM prompt used for generating the results discussed in prompt ablation in Section 4.3.</p>
<p>I.3 Vision Prompt</p>
<p>You are an expert spatial reasoning AI specializing in solving puzzles from the game 'The Witness'.Your task is to solve the puzzle in the image by finding a valid line from the Start Node to the End Node.</p>
<p>The image shows a Witness puzzle grid of size {grid_size['width']<em>2}x{grid_size['height']</em>2}.In this puzzle: -The solution is a continuous line from the start circle to the end marker -The line travels along grid edges, connecting adjacent nodes horizontally or vertically -The line cannot visit the same node twice -The line must satisfy all constraints represented by the symbols on the grid -The line can not be placed on rule cells -The line can only travel 1 cell per step (no diagonal moves and provide each step as a separate coordinate)</p>
<p>COORDINATE SYSTEM: -Nodes are indexed (x, y) where (0,0) is the top-left node -x increases to the right, y increases downward -The grid cells have rule symbols located at cells with all odd coordinates Therefore we have to draw a line to (6,4) over (4,4) which creates a region for the stone at (5,5) which validates the stone.We continue up to (6,2) for the polyshape 624 and then go to (4,2) and after this to (4,0) to finaly validate the polyshape 624.This also validates the two green stars at (3,3) and (3,5) with each other and the black stone at (3,1) because its the only stone in its region.This line also creates a region for the black stone at (5,1) because its the only stone in its region .Now we can draw a line to (5,0) to reach the end node....</p>
<p>L Details on Reasoning Mistakes</p>
<p>From (8,3) -&gt; (8,2) -&gt; (8,1).From there, perhaps moving left to (7,1), then (6,1), etc., but I need to avoid 'G's and 'N's.</p>
<p>...</p>
<p>Wait, but from (8,3) to (8,2) to (8,1) to (8,0)</p>
<p>...</p>
<p>From from (8,0) to (7,0) to (6,0) ...</p>
<p>From (6,0), can go left to (5,0) or down to (6,1).Let's go left to (5,0).(6,0) -&gt; (5,0).From (5,0), can go left to (4,0) or down to (5,1).Let's go left to (4,0).(5,0) -&gt; (4,0).(1,0) â†’ (0,0) â†’ (0,1) â†’ (0,2) â†’ (1,2) â†’ (2,2) â†’ (2,3) â†’ (3,3) is blocked.So can't.So from (2,3), go down to (2,4).Then left to (1,4).</p>
<p>... Wait, maybe the line can go from (1,0) to (0,0), then down to (0,1), then right to (1,1) is blocked, so go down to (0,2), then right to (1,2), then down to (1,3) is blocked, so go right to (2,2), then down to (2,3), then right to (3,3) is blocked, so go down to (2,4), then left to (1,4).... Wait, I'm getting confused.Let me clarify.The grid is 2x2 cells, each cell being a square.So the nodes are the intersections, making a 3x3 grid of nodes.So the nodes go from (0,0) to (2,2).The start is at (0,2) which is the bottom-left node, and the end is at (2,0), the top-right node.</p>
<p>...</p>
<p>Start at (0,2).From there, go up to (0,1), then right to (1,1) -but (1,1) is a rule cell, so can't go there.So instead, go right to</p>
<p>(1,2), which is a '.' that must be passed through.So from (0,2) to (1,2).</p>
<p>...</p>
<p>Wait, but the line must pass through (2,4).So from (1,2), go down to (2,2), then right to (2,3), then down to (2,4), which is a '.'.</p>
<p>... So path so far: (0,2) â†’ (1,2) â†’ (2,2) â†’ (2,3) â†’ (2,4).</p>
<p>...</p>
<p>From (2,4), need to get to (4,4), which is another '.'.So go right to (3,4), then to (4,4).</p>
<p>... Now, from (4,4), need to get to (4,0), another '.'.So go down to (4,3), (4,2), (4,1), (4,0).</p>
<p>... But wait, (4,0) is a '.' and the end is at (2,0).So from (4,0), go left to (3,0), then to (2,0).But (2,0) is the end point, so that's acceptable.</p>
<p>... But does this path cover all the '.'s?Let's see:</p>
<ul>
<li>The model misinterprets the coordinate system, assuming (4,0) is the bottom-right corner, which is incorrect, as (4,4) is the bottom-right.The model also often attempts to draw a line over rule cells.This leads to an incorrect path that fails to satisfy the puzzle's rules.Line chart displaying the Accuracy (%) against puzzle Difficulty Level (1-5).The standard o4-mini Zero-Shot achieves a significantly higher overall accuracy (12.6%, blue dotted line) compared to o4-mini Vision (5.6%, orange dotted line), outperforming it at nearly all difficulty levels.Right Panel: Line chart displaying the Accuracy (%) against puzzle Difficulty Level (1-5).The alternative prompt results in a higher accuracy across all difficulties, improving the overall success from 15.8% (standard, blue dotted line) to 21.0% (alternative, orange dotted line).</li>
</ul>
<p>M Details on Ablations</p>
<p>M.2 Alternative Prompt</p>
<p>N Details on Human Annotation N.1 Annotators</p>
<p>The annotators are two Ph.D. students and four research assistants (two bachelor students and two master students) in Computer Science and Data Science.They are all male and between 22 and 27 years old.As part of their research job, they receive at least the minimum wage in Germany.</p>
<p>N.2 Annotation Instructions</p>
<p>Introduction.This guide provides the rules and instructions for annotating SPaRC puzzles.We want to compare whether there are patterns/similarities that make the puzzles difficult for humans or LLMs to solve.Therefore, we have created 6 test sets of 50 puzzles each.These sets contain puzzles with all possible combinations of rules and grid sizes.</p>
<p>Task.Annotate all the samples in the dataset in as little time as possible per puzzle.Each puzzle is solvable, but if you can't think of a solution after a reasonable amount of time (5-10 minutes), you can click Show Solution or Skip Puzzle to continue.Once you have completed all 50 puzzles, please e-mail the annotated file back to us.By sending the file back to us, you agree that we can publish your annotations anonymously.This includes solve time, required attempts, and solution path.</p>
<p>Rules.The line must connect Start with End with a continuous path without using the same cell twice.It must also follow all rules defined by the puzzle.A visual explanation of the rules can be seen in Figure 28.</p>
<p>â€¢ Gaps: The line CANNOT traverse a cell marked by a Gap.</p>
<p>â€¢ Dots: The line MUST pass through a cell marked by a Dot.</p>
<p>â€¢ Stone: All stones within a single region created by the line must be the same color.Different colored squares MUST be separated into different regions by the line.</p>
<p>â€¢ Stars: Each star must be paired with EX-ACTLY one other element of the same color in a region.Other colors are ignored.</p>
<p>â€¢ Triangles: The line must touch EXACTLY the number of edges specified by the triangle count (edges are top, right, bottom, left of the cell).</p>
<p>â€¢ Polyshapes (Poly): The region containing this symbol must be shaped EXACTLY like the defined polyshape.The shape must fit entirely within the region's boundaries.If multiple positive polyshapes are in one region, the region's shape must accommodate their combined, non-overlapping forms (like Tetris pieces).</p>
<p>â€¢ Negative Polyshapes (Ylop): The negative polyshape can only be placed on top of already placed normal polyshapes.The negative polyshapes must fit on the grid, but can allow overlap between normal polyshapes or placement of polyshapes that extend beyond the area defined by the line.If the negative polyshapes exactly cancel the normal polyshapes, there is no restriction on the grid shape anymore.A negative polyshape only counts as valid if it is used.</p>
<p>Example Dataset.You can use the following dataset to experiment and get familiar with the puzzles and all rules: Link redacted for anonymity.</p>
<p>Important Hints</p>
<p>â€¢ The annotation state gets saved even when closing the window, but to be safe, also always download the current annotated dataset when you stop annotating.</p>
<p>â€¢ If you reload the page, don't overwrite the existing data.</p>
<p>?</p>
<p>Figure 1 :
1
Figure 1: Example puzzles from SPaRC.</p>
<p>SPaRC is a 2D grid of m Ã— n rule cells with (x, y) = (0, 0) âˆˆ (m, n) being the topleft corner of the grid, and x increases to the right, and y downwards.Rule cells are surrounded by edges that can be used to draw a path.There exists one start point on the edges (large circle) and one end point on the edges (extension of the edge).The goal of solving a puzzle is to move from the start point to the end point along the edges around the rule cells to fulfill all rule cell conditions.The path must be a single, continuous sequence of edges from the start to the end point, without crossing or overlapping itself at any edge segment.Central to each puzzle are the rule cells, which we describe together with what it means to fulfill the rule cell condition.Section G contains puzzle examples to illustrate the components of our dataset.Item Collection (Dots): The solution path needs to pass through every dot.Path Breaks (Gaps): The solution path cannot go through any edge segment containing a gap.Gaps act as local barriers.Color Separation (Stones): The solution path must be drawn to separate stones of different colors.All stones located within any single enclosed region must be of the same color.Pairing (Stars): Each star must share its region with exactly one other symbol of the same color.No unpaired stars are allowed.Edge Count (Triangles): The solution path must touch the number of edges shown by the triangles in the cell, e.g., two triangles mean the path must touch exactly two edges of that cell.</p>
<p>Figure 4 :
4
Figure 4: Accuracy (%) at different difficulty (1-5) between o4-mini (orange) and human annotators (teal).Higher is better.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Performance of models on puzzles containing only specific rules.Columns represent individual models, for reasoning-and instruction-tuned models.The Full Set row shows the Accuracy (%) per model across all puzzles.The rows below show accuracy on specific rules minus accuracy on the full set (âˆ† Accuracy (%)).</p>
<p>, Qwen 2.5 72B (Research, 2024) and DeepSeek R1 70B (DeepSeek-AI et al., 2025).</p>
<p>Figure 10 :
10
Figure 10: Puzzle grid from Figure 1 with all grid cells annotated with their coordinates.</p>
<p>the result to ensure the final difficulty score strictly falls within the [0, 5] bounds.</p>
<p>Figures</p>
<p>Figures 13 to 19 provide visual context for the different puzzle rule types discussed in our evaluation (Section 4), this section presents examples of each core subtype.For each rule, we show the unsolved puzzle grid (a) with a valid solution path (b).</p>
<p>Figure 13 :
13
Figure 13: Example of the Dots rule.The solution path must pass through all dots present on its segments.</p>
<p>Figure 14 :
14
Figure 14: Example of the Gaps rule.The solution path cannot cross specific marked edges on the grid.</p>
<p>Figure 15 :
15
Figure 15: Example of the Stones rule.The solution path must separate grid cells containing different colored stones into distinct regions.</p>
<p>Figure 16 :
16
Figure 16: Example of the Stars rule.Each region with a star must contain exactly one other rule of the same color.</p>
<p>Figure 17 :
17
Figure 17: Example of the Triangles rule.The solution path must touch the number of grid edges equal to the number of triangles in the adjacent cell.</p>
<p>Figure 18 :
18
Figure 18: Example of the Polys rule (Polyominoes).The solution path must outline a region that perfectly contains the depicted poly shape.Multiple polys in one region can be combined.</p>
<p>(a) Unsolved Polys &amp; Inverse Polys puzzle.(b) Solved Polys &amp; Inverse Polys puzzle.</p>
<p>Figure 19 :
19
Figure 19: Example of the Polys &amp; Ylops (Inverse Polys) rule combination.The solution path must outline a region satisfying both polyomino shape inclusion and subtraction constraints.</p>
<p>START</p>
<p>': Start point -'E': End point -'+': Cell on which the line can be drawn -'N': Empty rule cell -'G': Gap (cannot be crossed) -'.': Dot line must cross this cell -'o-X': Stone of color X -'<em>-X': Star of color X -'A-X' Triangle with count 1 -'B-X' Triangle with count 2 -'C-X' Triangle with count 3 -'D-X' Triangle with count 4 -'P-X-Y': Positive polyshape of color X and shape ID Y -'Y-X-Y': Negative polyshape (ylop) of color X and shape ID Y COLOR CODES: R=red, B=blue, G=green, Y=yellow, W=white, O=orange, P=purple, K=black {example_section} PUZZLE GRID: {grid_str} POLYSHAPE DEFINITIONS: Defines the shapes referenced by P-X-Y and Y-X-Y symbols in the grid.In the 2D array, 1 indicates a cell occupied by the shape, 0 indicates an empty cell.{polyshapes_str}Please solve this puzzle.First, explain your reasoning step-by-step, including key deductions and constraint checks made along the way.Then, provide the final solution as a sequence of node coordinates in (x, y) format (dont skip any intermediate nodes), starting with the start node and ending with the end node, after this string: "####".Example coordinate list: [(0,0), (1,0), (2,0), (2,1), ...]Listing 1: The LLM prompt used for generating the results discussed in Section 4.1.I.2 Alternative Prompt## Objective You are a specialized AI proficient in spatial reasoning and solving puzzles from the game 'The Witness'.Your goal is to find a valid path (a continuous line) from the specified Start Node to the End Node on the provided grid, adhering to all puzzle rules.## Core Concepts &amp; Grid Basics * </em><em>Grid Dimensions:</em><em> The puzzle grid has {grid_size['width']} columns {grid_size['height']} rows.</em> <strong>Coordinate System:</strong> Nodes are identified by '(x, y)' coordinates.'(0,0)' is the top-left node .'x' increases to the right, 'y' increases downwards.1. <strong>Solve the Puzzle:</strong> Determine the valid path from the Start Node to the End Node that satisfies all rules.2. <strong>Explain Reasoning:</strong> Provide a step-by-step explanation of your thought process.Detail key deductions, how constraints were applied, and any backtracking or choices made.3. <strong>Provide Solution Path:</strong> After the reasoning, output the exact marker string '####' followed immediately by the solution path as a list of node coordinates '(x, y)'.Include all intermediate nodes from start to end.<strong>Example Solution Path Format:</strong> #### [(0, 0), (1, 0), (2, 0), (2, 1), ...]</p>
<p>(a) Empty puzzle grid.(b) Solved puzzle (correct).(c) Incorrect start/end (incorrect).(d) Disconnected line (incorrect).(e) Self-intersecting path (incorrect).(f) Rule-cell crossing (incorrect).</p>
<p>Figure 20 :
20
Figure 20: (a)-(b): An empty puzzle and its solution.(c)-(f): The four kinds of path-generation errors.</p>
<p>Figures</p>
<p>Figures 21 to 24 provide four examples of common reasoning mistakes for DeepSeek R1 70B and highlights the steps that lead to the mistake.</p>
<p>double-check if I missed any 'G's along the way.For example, in row 0, column 5 is 'G', but the path doesn't go near there.Similarly, row 1 has 'G's at 2 and 5, but the path goes through row 1, columns 0 and 1, which are '+', so that'</p>
<p>Figure 21 :
21
Figure 21: Analysis of puzzle 80a59619e323acba with an incorrect solution attempt by DeepSeek R1 Llama 70B Distill.The model tends to commit to a sequence of reasoning steps without thoroughly validating each one, leading to careless mistakes.</p>
<p>'m stuck and need to conclude that I can't find a solution right now.</p>
<p>Figure 22 :
22
Figure 22: Analysis of puzzle cf36a0865a1ed242 from the stars split with incorrect solution by DeepSeek R1 70B.The model fails to realize that the grid can form one large region with two stars of each color, leading to an incorrect path.It often tries to loop around stars and run into grid cells, which is impossible.The model notices it got stuck and provides the latest attempt to a valid solution.</p>
<p>path satisfies all the constraints.</p>
<p>Figure 23 :
23
Figure23: Analysis of puzzle a49a784ae7d91339 from the dots split with incorrect solution by DeepSeek R1 Llama 70B Distill.The model has issues interpreting the grid sizes as 5x5 initially, but converges to the correct dimensions after some reasoning steps.The model tries to draw paths on rule cells but corrects itself quickly.Finally, the model mixes up x and y coordinates, thinking it had drawn the correct path but drawing an inverse indexed path.</p>
<p>Figure 24 :
24
Figure 24: Analysis of puzzle d9e6e97a86d583d9 of the stones split with incorrect solution by DeepSeek R1 70B.The model misinterprets the coordinate system, assuming (4,0) is the bottom-right corner, which is incorrect, as (4,4) is the bottom-right.The model also often attempts to draw a line over rule cells.This leads to an incorrect path that fails to satisfy the puzzle's rules.</p>
<p>Figure 25 :
25
Figure25: Comparison of the standard o4-mini Zero-Shot (blue) and its vision-enabled counterpart o4-mini Vision (orange) on SPaRC puzzles.Left Panel: Bar chart showing the Error Rate (%) for different types of path violations across all generated solutions.o4-mini Vision generally exhibits higher rates of these structural errors.Right Panel: Line chart displaying the Accuracy (%) against puzzle Difficulty Level (1-5).The standard o4-mini Zero-Shot achieves a significantly higher overall accuracy (12.6%, blue dotted line) compared to o4-mini Vision (5.6%, orange dotted line), outperforming it at nearly all difficulty levels.</p>
<p>Figure 26 :
26
Figure26: Performance comparison of o4-mini using its standard prompt (blue) versus an alternative prompt (orange) on SPaRC puzzles.Left Panel: Bar chart showing the Error Rate (%) for different types of path violations across all generated solutions.The alternative prompt generally reduces the frequency of these structural errors.Right Panel: Line chart displaying the Accuracy (%) against puzzle Difficulty Level (1-5).The alternative prompt results in a higher accuracy across all difficulties, improving the overall success from 15.8% (standard, blue dotted line) to 21.0% (alternative, orange dotted line).</p>
<p>Table 1 :
1
Counts of puzzles for SPaRCfor different difficulties and rules based on the test set.
with Rule TypeGaps313Dots292Stones355Stars210Triangles233Polygons305Ylops25Puzzles with Difficulty LevelLevel 186Level 2118Level 3121Level 486Level 589</p>
<p>98.0% 15.8% 8.2% 5.8% 4.0% 1.6% 1.2% 0.4%
HumanInstruction-tunedReasoningFigure 3: Accuracy (%) of human annotators (teal)against different LLMs: reasoning models (orange) andinstruction-tuned models (blue). Higher is better.puzzles solved). The top reasoning model, o4-mini,performs much worse at 15.8% (79/500 puzzles).GPT-4.1 is the best instruction-tuned model at 1.6%(8/500 puzzles). Reasoning models perform betteroverall (avg. 8.5%). Closed models outperformopen ones: o4-mini (15.8%) and o3-mini (8.2%)versus R1 70B (4.0%) and QwQ (5.8%), with simi-lar trends in instruction-tuned models. Results sug-gest these puzzles are very challenging for LLMs,while relatively easy for humans. We hypothesizeerrors arise from models' spatial understandinglimitations, such as misunderstanding rules, logicalfallacies, and misinterpreting grid representations</p>
<p>Difficulty Level 0 50 100 Accuracy (%) 47.7% 19.5% 10.7% 1.2% 1.1% 100.0% 100.0% 100.0% 94.4% 94.5%
12345Best Model: o4-miniHuman Performance</p>
<p>Table 2 :
2
Accuracy, Average and Median human solve time (seconds), and Accuracy and Number of (#) Generated Tokens for QwQ 32B and Qwen 2.5 72B over Difficulty Level (1-5).
Difficulty Level12345Hum. Acc. (%)100 % 100 % 100 % 94.4 % 94.5 %Hum. Avg. (s)10.718.326.760.7131.5Hum. Mdn. (s)7.113.715.628.885.6QwQ Acc. (%)20.9%5.9%2.5%1.2%0.0%QwQ #Tokens14433 14200 139831407213114Qwen 2.5 Acc. (%)0.0%1.7%0.0%0.0%0.0%Qwen 2.5 #Tokens79088895310371161</p>
<p>Table 3 :
3
Overview of models, hardware, and token counts.Token counts are approximate.</p>
<p>Table 4 :
4
Table4shows the average tokens produced by the different models, decomposed by puzzle difficulty.Average tokens per puzzle by difficulty level.
ModelLevel 1 Level 2 Level 3 Level 4 Level 5ReasoningQwQ 32B14433.3 14200.6 13983.1 14072.8 13114.1R1 70B7646.59119.89374.6 10134.4 9989.6InstructionQwen 2.5 72B790.6888.7953.11037.71161.2Gemma-3 27B802.8874.6910.0941.21033.3</p>
<p>Table 5 :
5
Statistics for all splits of SPaRC.Difficulty and rule statistics are only based on the test set, as only these are used for evaluation.
Puzzles (Level 1)86342900 13030217Puzzles (Level 2)11869529 1724412139Puzzles (Level 3)121771315 122291367Puzzles (Level 4)863318544181249Puzzles (Level 5)8902141401613618Count per Rule TypePuzzles with Gaps313500000000400Puzzles with Dots2920500000004746Puzzles with Stones355005000004900Puzzles with Stars2100005000032039Puzzles with Triangles2330000 50000370Puzzles with Polygons3050000050500043Puzzles with Ylops2500000043000</p>
<p>Figures 25 to 27 provide more details for the ablation experiments in Section 4.3, considering vision models, alternative prompts, and few-shot examples.
M.1 Vision Mode80 10085.4%o4-mini o4-mini Vision50 4046.5%o4-mini o4-mini Vision o4-mini Overall: 12.6%Error Rate (%)20 40 6053.6%28.0%26.6% 41.8%24.8% 27.2%43.6% 56.2%Accuracy (%)10 20 308.1%10.2% 13.6%7.4%o4-mini Vision Overall: 5.6%I n v a li d P a t h I n c o r r e c t S t a r t / E n d D is c o n n e c t e d L in e 0 3.6%I n t e r s e c t in g L in e R u le C e ll C r o s s in g0123.3% 3 Difficulty Level3.5% 4 0.0%5 0.0% 0.0%
AcknowledgementsThis work was partially supported by the Lower Saxony Ministry of Science and Culture and the VW Foundation.Many thanks to Andreas Stephan, Tianyu Yang, Zeinab Taghavi, and Annika Schulte-HÃ¼rmann for their thoughtful discussions and feedback.-The line goes AROUND cells containing rules, forming boundaries -Both line and rule cells are on the same grid.Therefore each intersection has a distance of 2 to the next intersection.SOLVING RULES: 1. Draw a continuous line from the START NODE (big circle on the line) to the END NODE (rounded end) without visiting the same node twice.2. The line can only be placed on valid path cells.3. The line acts as a boundary, potentially dividing the grid cells into one or more distinct regions .4. All rules associated with symbols on the grid must be satisfied:-Dots: The line MUST pass through each dot.-Colored squares: All squares within a single region created by the line must be the same color.Text description of the puzzle: {puzzle_data.get("text_visualization","")} Analyze the puzzle image carefully and determine the solution path.First, explain your reasoning step-by-step, including key deductions and constraint checks made along the way.Then, provide the final solution as a sequence of node coordinates in (x, y) format, starting with the start node and ending with the end node, after this string: "####".. DON'T SKIP ANY intermediate nodes (the distance between each node must be 1).Example coordinate list: [(0,0), (1,0), (2,0), (2,1), ...]Listing 3: The LLM prompt used for generating the results discussed in vision ablation in Section 4.3.SECOND EXAMPLE SOLUTION:We start at (8,1) and draw a line to (8,2).Then we draw a straight line to (4,2).From here we go up to (4,0).This creates one region with only a blue stone at (5,1) which makes it valid.The other region contains numerus polyshapes and ylops.But the region already has a valid shape.The P-W-8992 gets placed on the bottom left and combined with the Y-W-18 to form a 2x1 region.The other part of the region can exactly be formed by the two P-W-48 polyshapes.Now we can draw a line to (1,0) to reach the end node.#### (8,1),(8,2),(7,2),(6,2),(5,2),(4,2),(4,1),(4,0),(3,0),(2,0),(1,0)Listing 5: The examples used for generating the results discussed in few-shot ablation in Section 4.3.J Full Tabular Main ResultsTables 6 to 8 provide the detailed and complete results for the experiments in Section 4.1.J.1 Difficulty per LevelK Details on Path ErrorsFigure20shows the examples of SPaRC puzzle structure and the fundamental path violation types discussed in Section 4.Accuracy (%)o4-mini (zero-shot) o4-mini (one-shot) o4-mini (two-shot) zero-shot Overall: 12.6% one-shot Overall: 15.8% two-shot Overall: 14.8%O Acknowledgment of AI UsageAI Usage card based onWahle et al. (2023b).AI Usage Card
Who witnesses the witness? finding witnesses in the witness is hard and sometimes impossible. Zachary Abel, Jeffrey Bosboom, Michael Coulombe, Erik D Demaine, Linus Hamilton, Adam Hesterberg, Justin Kopinsky, Jayson Lynch, Mikhail Rudoy, Clemens Thielen, 2018</p>
<p>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. Mohamed Aghzal, Erion Plaku, Ziyu Yao, abs/2310.032492023ArXiv preprintArt of Problem Solving. 2025. Aime problems and solutions</p>
<p>Mallm: Multi-agent large language models framework. Jonas Becker, Lars Benedikt Kaesberg, Niklas Bauer, Jan Philip Wahle, Terry Ruas, Bela Gipp, arXiv:2509.116562025aPreprint</p>
<p>Stay focused: Problem drift in multiagent debate. Jonas Becker, Lars Benedikt Kaesberg, Andreas Stephan, Jan Philip Wahle, Terry Ruas, Bela Gipp, arXiv:2502.195592025bPreprint</p>
<p>The witness. Jonathan Blow, 2016. May-202515</p>
<p>Reasoning models don't always say what they think. FranÃ§ois Chollet. Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, R Samuel, Jan Bowman, Jared Leike, Ethan Kaplan, Perez, 2025. 2019. 1911.01547ArXiv preprintOn the measure of intelligence</p>
<p>Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 181</p>
<p>Automatically constructing a corpus of sentential paraphrases. William B Dolan, Chris Brockett, Proceedings of the Third International Workshop on Paraphrasing (IWP2005). the Third International Workshop on Paraphrasing (IWP2005)2005</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021a. May 3-7, 2021OpenReview.net</p>
<p>Measuring Mathematical Problem Solving With the MATH Dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021b</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, Towards reasoning in large language models: A survey. 2022</p>
<p>Swe-bench: Can language models resolve real-world github issues? Lars Kaesberg, Terry Ruas. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024). the Fourth Workshop on Scholarly Document Processing (SDP 2024)Bangkok, ThailandAssociation for Computational Linguistics2023. Jan Philip Wahle, and Bela Gipp. 2024CiteAssist: A system for automated preprint citation and BibTeX generation</p>
<p>Voting or consensus? decision-making in multi-agent debate. Lars Benedikt Kaesberg, Jonas Becker, Jan Philip Wahle, Terry Ruas, Bela Gipp, 2025</p>
<p>Introducing gpt-4.1 in the api. Accessed. 2025aOpenAI</p>
<p>Introducing openai o3 and o4-mini. 2025bOpenAI</p>
<p>Openai o3-mini: Pushing the frontier of cost-effective reasoning. 2025cOpenAI</p>
<p>Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, Junyang Lin, Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. 2025</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark. 2023</p>
<p>Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. 2025. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. Alibaba Research2024Qwen2 Technical Report</p>
<p>MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett, 2023</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, 2022</p>
<p>. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Sabela Jean Bastien Grill, Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchevand 197 others. 2025. Gemma 3 technical report</p>
<p>QwQ: Reflect Deeply on the Boundaries of the Unknown. Qwen Team, 2024</p>
<p>Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel R Bowman, 2023</p>
<p>Paraphrase Types for Generation and Detection. Jan Wahle, Bela Gipp, Terry Ruas, 10.18653/v1/2023.emnlp-main.746Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>Ai usage cards: Responsibly reporting ai-generated content. Jan Philip Wahle, Terry Ruas, M Saif, Norman Mohammad, Bela Meuschke, Gipp, 10.1109/JCDL57899.2023.000602023 ACM/IEEE Joint Conference on Digital Libraries (JCDL). 2023b</p>
<p>Paraphrase types elicit prompt engineering capabilities. Jan Philip Wahle, Terry Ruas, Yang Xu, Bela Gipp, 10.18653/v1/2024.emnlp-main.617Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USA2019. May 6-9, 2019OpenReview.net</p>
<p>Enigmaeval: A benchmark of long multimodal reasoning challenges. Clinton J Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, Dan Hendrycks, 2025</p>
<p>Is a picture worth a thousand words? delving into spatial reasoning for vision language models. Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Yixuan Li, Neel Joshi, 2024a</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen, MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. 2024bPublished at NeurIPS 2024 Track Datasets and Benchmarks</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, ArXiv preprint, abs/2302.113822023</p>
<p>A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, Xuanjing Huang, 2023ArXiv preprint, abs/2303.10420</p>
<p>Reasonflux-prm: Trajectory-aware prms for long chain-of-thought reasoning in llms. Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang, arXiv:2506.188962025arXiv preprint</p>
<p>We then draw a line to (2,0) to reach the dot at (1,0) and surround the 3 count triangle. We then draw a line to (2,2) here we go down to touch the third side of the triangle cell and therefore validate the 3 count triangle. EXAMPLE POLYSHAPE DEFINITIONS: Shape 112. We start at (0,2) and draw a line to (0,0). We continue down to (2,6) to validate the polyshape 112 and also the green star with the green polyshape After this we draw a line to (4,6) to start validating the polyshape 624 by surrounding it</p>
<p>Listing 4: The examples used for generating the results discussed in few-shot ablation. #### , 0,2),(0,1),(0,0),(1,0),(2,0),(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(3,6),(4,6),(4,5),(4,4),(5,4) ,(6,464,0),(5,0. in Section 4.3. SECOND EXAMPLE PUZZLE GRID: ["+","E","+","+","+","+","+","+","+"] ["+","N","+","N","+","o-B","+","N","S"] ["+","+","+","+","+","+","+","+","+"] ["+","P-W-8992","G","Y-W-18","+","P-W-48","+","P-W-48","+"] ["+","+","+","G","+","+","+","+","+"</p>
<p>SECOND EXAMPLE POLYSHAPE DEFINITIONS: Shape 18. 480,1,0,0. 1,0,0,0] [0,0,0,0] [0,0,0,0] Shape. 0,1,0,0] [0,1,0,0] [0,0,0,0] [0,0,0,0] Shape. 0,0,1,0] [0,1,1,1] [0,0,0,0] [0,0,0,0</p>            </div>
        </div>

    </div>
</body>
</html>
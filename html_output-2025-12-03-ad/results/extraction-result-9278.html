<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9278 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9278</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9278</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-278481214</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.06149v3.pdf" target="_blank">Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study</a></p>
                <p><strong>Paper Abstract:</strong> Despite growing interest in automated hate speech detection, most existing approaches overlook the linguistic diversity of online content. Multilingual instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ offer promising capabilities across languages, but their effectiveness in identifying hate speech through zero-shot and few-shot prompting remains underexplored. This work evaluates LLM prompting-based detection across eight non-English languages, utilizing several prompting techniques and comparing them to fine-tuned encoder models. We show that while zero-shot and few-shot prompting lag behind fine-tuned encoder models on most of the real-world evaluation sets, they achieve better generalization on functional tests for hate speech detection. Our study also reveals that prompt design plays a critical role, with each language often requiring customized prompting techniques to maximize performance.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9278.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9278.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptDesign-General</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Prompt Design on Multilingual Hate Speech Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that prompt design strongly influences LLM performance for hate speech detection across languages: different prompt types (vanilla, classification, CoT, NLI, multilingual, role-play, translate, definition, distinction, few-shot and combinations) yield markedly different F1-macro results depending on model and language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-tuned multilingual LLMs (LLaMA-3.1-8B-Instruct, Qwen2.5-7B-Instruct, Aya-101, BloomZ-7B1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLaMA-3.1-8B, Qwen2.5-7B, BloomZ-7B1, Aya-101 (size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multilingual hate speech detection (real-world and functional tests)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification (hate vs non-hate) across eight non-English languages using both real-world datasets and controlled functional (HateCheck-style) tests; primary metric is macro F1.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>A broad set of prompt formats were evaluated: zero-shot (vanilla, classification, CoT, NLI, multilingual-aware, role-play, translate, definition, distinction, contextual), few-shot (1/3/5 examples per class) and combinations (e.g., few-shot + CoT, few-shot + role-play, multilingual + definition). Prompts included explicit instructions to return only 'yes'/'no' labels, sometimes swapped to mitigate position bias.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Multiple alternative prompt types compared pairwise (e.g., zero-shot vs few-shot; vanilla vs CoT; role-play vs classification; definition/distinction vs vanilla), and comparisons to fine-tuned encoder models (XLM-T, mDeBERTa) were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varied strongly by prompt type, language, and model; qualitative summary: prompting often achieves reasonable F1-macro on functional tests and mixed/usually lower results on real-world tests compared to fine-tuned encoders. Exact F1-macro values depend on (model, language, prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute differences to the match between prompt formulation and the model's instruction-following/multilingual strengths and to cultural/linguistic specificity per language; thus each language often benefits from customized prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Models run in inference mode for three random seeds; outputs averaged (macro F1). Prompts used max_new_tokens=10, do_sample=False. Yes/no placeholders randomly swapped to reduce position bias; few-shot used 1/3/5 examples per class (5-shot typically 5 hate + 5 non-hate interleaved).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9278.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9278.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FewShot-Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Few-shot Prompting (1/3/5 examples) vs Zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including a few task-specific examples (especially 5-shot) generally improves LLM performance, most noticeably on functional tests; improvements on real-world datasets are inconsistent and depend on prompt clarity and example quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-tuned multilingual LLMs (LLaMA-3.1-8B, Qwen2.5-7B, Aya-101, BloomZ-7B1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLaMA-3.1-8B, Qwen2.5-7B, BloomZ-7B1, Aya-101 (size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multilingual hate speech detection (real-world and functional tests)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary hate vs non-hate classification using few-shot in-context examples drawn from the training set (retrieved examples).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompting with 1, 3, or 5 examples per class inserted into the prompt; examples were interleaved by class (e.g., hate, non-hate, ...). Combinations with other prompt types (e.g., few-shot + CoT, few-shot + role-play) were also tested.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to zero-shot prompts (same prompt templates without examples) and to fine-tuned encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: few-shot (typically 5-shot) increased F1-macro on functional tests substantially and provided smaller or inconsistent gains on real-world tests; exact numeric F1 gains vary by model/language/prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot examples help models apply contextual distinctions and task-specific decision boundaries, improving generalization on controlled functional test cases where examples map closely to test phenomena; on noisy, culturally-specific real-world data, benefits depend on representativeness of examples and prompt clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot retrieval used training set examples; 5-shot used 5 hate + 5 non-hate examples. Context length limits prevented using more than 5 examples per class across all prompts/models. Results averaged over three runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9278.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9278.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) Prompting Effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Chain-of-thought (step-by-step) prompting sometimes improves performance, particularly when combined with few-shot examples, but its benefits are inconsistent and less pronounced on real-world datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-tuned multilingual LLMs (notably LLaMA-3.1-8B and Qwen2.5-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLaMA-3.1-8B, Qwen2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Hate speech detection (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Models prompted to reason step-by-step about whether a comment targets protected groups or exhibits hostility/dehumanization, then produce a yes/no decision.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot CoT prompts and few-shot + CoT combinations where the prompt requests stepwise analysis (Step 1, Step 2, Step 3) before concluding; sometimes combined with multilingual context or role-play.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against vanilla zero-shot prompts, classification-style prompts, and few-shot without CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: CoT improved accuracy/F1 in some functional test settings and for certain models (e.g., LLaMA3 performed well on functional benchmarks), but did not uniformly improve real-world test performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest CoT can help when reasoning about multi-step definitions (e.g., identifying target, then hostility level), but it may introduce verbosity or distract from binary decision-making on noisy real-world input.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT prompts explicitly enumerated steps to assess targeting and hostility; used both zero-shot and few-shot+CoT settings. Responses constrained to binary outputs to avoid extraneous text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9278.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9278.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RolePlay-Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Role-play Prompting (Community Moderator Role)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assigning the model the role of a community moderator (role-play) often improves performance for some models, with Qwen showing sensitivity and strong performance with role-play and NLI-style prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B-Instruct (noted), LLaMA-3.1-8B, Aya-101, BloomZ-7B1</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Qwen2.5-7B, LLaMA-3.1-8B, BloomZ-7B1, Aya-101 (size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Hate speech detection (binary) in multiple languages</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompts framed the model as a moderator responsible for enforcing hate speech policies and asked for binary decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot role-play and few-shot + role-play prompts where the model is told it previously handled cases (examples) and must decide if a new comment violates policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to vanilla, classification, CoT, and few-shot without role-play.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Role-play improved performance for some (notably Qwen) especially on functional tests; gains on real-world tests varied by language and model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Role-play likely primes the model to use policy-aligned decision heuristics and conversational cues, benefiting models sensitive to contextual and persona framing.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Role-play prompts used community moderator framing; some role-play prompts were combined with CoT or few-shot examples. Outputs constrained to 'yes'/'no'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9278.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9278.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Definition-Distinction-Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Definition and Distinction-based Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing explicit definitions of hate speech and telling the model to distinguish hate speech from toxic/offensive language improves performance for certain models; Aya-101 performed best with definition-and distinction-based prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aya-101 (noted), other instruction-tuned LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Aya-101 (size not specified); other models as above</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Hate speech detection (binary) across languages</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompts include formal definitions of hate speech and related categories (toxic/offensive language) and ask for binary classification.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot definition prompts and few-shot + definition settings where definition text and labeled examples are provided in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to vanilla, classification, CoT, and few-shot without definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Definition/distinction prompts improved accuracy for Aya-101 and helped models disambiguate related abusive content; performance gains were especially visible on functional tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicit definitions reduce ambiguity and align model decision boundaries with the target task, particularly beneficial for models tuned for equitable cross-lingual behavior (as with Aya-101).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Definition text enumerated attributes (race, religion, etc.) and examples; used in zero-shot and few-shot templates; binary-only outputs requested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9278.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9278.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TranslateThenClassify</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Translate-then-Classify Prompt Strategy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts that instruct the model to translate non-English text to English and then classify were explored as a strategy; motivated by prior work showing translation can help in low-resource settings, but the paper reports mixed results across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-tuned multilingual LLMs (all four evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLaMA-3.1-8B, Qwen2.5-7B, BloomZ-7B1, Aya-101 (size unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-lingual hate speech detection (translate + classify)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Model translates input from {language} to English then performs binary classification on the translation within the same prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot 'translate' prompts and few-shot variants instructing explicit translation followed by classification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to direct classification in the original language using multilingual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Mixed — in some languages translation-first prompts helped, consistent with prior findings that translating to English can benefit certain LLMs; in other languages language-aware multilingual prompts or native prompting performed as well or better.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Translation can map low-resource language phenomena into a representation the model handles better, but translation errors and cultural nuances can reduce benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Translate prompts asked model to first translate the text then classify, with binary-only outputs; position bias mitigation and 3-run averaging applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9278.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9278.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting-vs-Finetune</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison: Prompted LLMs vs Fine-tuned Encoder Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting LLMs (zero- and few-shot) generally underperforms fine-tuned encoder models (XLM-T, mDeBERTa) on real-world test sets, but often outperforms or matches them on functional tests; prompting becomes competitive at small encoder training sizes (language-dependent thresholds).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-tuned LLMs (various) vs encoder baselines (XLM-T, mDeBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XLM-T ~279M parameters, mDeBERTa ~86M; LLM sizes as above</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multilingual hate speech detection (real-world vs functional tests) and data-scaling comparison</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare zero-/few-shot prompted LLM performance to encoder models fine-tuned on varying amounts of labeled data (10 to 2000 examples) using macro F1.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Best zero-/few-shot prompting results (across prompt types) compared to encoder models fine-tuned on different training set sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Fine-tuned encoder performance at varying dataset sizes (10..2000) vs prompting (zero-/few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>High-level: Encoder models generally achieve higher macro F1 on real-world test sets when trained on sufficient labeled data; prompting often attains stronger generalization on functional tests and matches/exceeds XLM-T trained on 2,000 examples in Portuguese, Arabic, French, and Italian.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Prompting competitive thresholds: Spanish ~100-200 labeled examples, Hindi ~300-400, German ~600-700 — beyond these sizes fine-tuning generally yields better performance.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Fine-tuning on task-specific data improves decision boundaries for noisy, culturally-dependent real-world data, while large instruction-tuned LLMs generalize better on controlled functional phenomena without additional training; data availability determines the preferred approach.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Encoder models fine-tuned for 10 epochs, evaluated across training sizes from 10 up to full dataset (~2000) with five random seeds; prompting results represent best zero-/few-shot configurations aggregated from three runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9278.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9278.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FewShot-ScaleDetails</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Implementation and Context Constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot experiments used retrieved examples from the training set (1, 3, or 5 examples per class); 5-shot was typical (5 hate + 5 non-hate interleaved), but context length and compute limited use of larger example sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-tuned multilingual LLMs evaluated in the study</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot in-context learning for hate speech classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>In-context example provisioning to guide model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompts with examples inserted into the template (examples interleaved by class), experimented with 1, 3, and 5 examples per class; combinations with CoT, role-play, definition, multilingual context tested.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Operational detail: 5-shot most commonly used and often produced best functional-test performance; performance varied by prompt/model/language.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Context length constraints limited number of in-context examples; interleaving by class aimed to present balanced positive/negative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>5-shot means 5 hate + 5 non-hate examples (10 examples total) interleaved. Attempts to use more than 5 per class were not feasible across all prompts/models due to context/window limits. Prompts and examples inserted into placeholders {examples} in templates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An investigation of large language models for real-world hate speech detection <em>(Rating: 2)</em></li>
                <li>Designing of prompts for hate speech recognition with in-context learning <em>(Rating: 2)</em></li>
                <li>Probing LLMs for hate speech detection: strengths and vulnerabilities <em>(Rating: 2)</em></li>
                <li>Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech <em>(Rating: 1)</em></li>
                <li>Investigating the predominance of large language models in low-resource bangla language over transformer models for hate speech detection: A comparative analysis <em>(Rating: 1)</em></li>
                <li>Hate personified: Investigating the role of LLMs in content moderation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9278",
    "paper_id": "paper-278481214",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "PromptDesign-General",
            "name_full": "Effect of Prompt Design on Multilingual Hate Speech Detection",
            "brief_description": "The paper finds that prompt design strongly influences LLM performance for hate speech detection across languages: different prompt types (vanilla, classification, CoT, NLI, multilingual, role-play, translate, definition, distinction, few-shot and combinations) yield markedly different F1-macro results depending on model and language.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction-tuned multilingual LLMs (LLaMA-3.1-8B-Instruct, Qwen2.5-7B-Instruct, Aya-101, BloomZ-7B1)",
            "model_size": "LLaMA-3.1-8B, Qwen2.5-7B, BloomZ-7B1, Aya-101 (size not specified)",
            "task_name": "Multilingual hate speech detection (real-world and functional tests)",
            "task_description": "Binary classification (hate vs non-hate) across eight non-English languages using both real-world datasets and controlled functional (HateCheck-style) tests; primary metric is macro F1.",
            "presentation_format": "A broad set of prompt formats were evaluated: zero-shot (vanilla, classification, CoT, NLI, multilingual-aware, role-play, translate, definition, distinction, contextual), few-shot (1/3/5 examples per class) and combinations (e.g., few-shot + CoT, few-shot + role-play, multilingual + definition). Prompts included explicit instructions to return only 'yes'/'no' labels, sometimes swapped to mitigate position bias.",
            "comparison_format": "Multiple alternative prompt types compared pairwise (e.g., zero-shot vs few-shot; vanilla vs CoT; role-play vs classification; definition/distinction vs vanilla), and comparisons to fine-tuned encoder models (XLM-T, mDeBERTa) were performed.",
            "performance": "Varied strongly by prompt type, language, and model; qualitative summary: prompting often achieves reasonable F1-macro on functional tests and mixed/usually lower results on real-world tests compared to fine-tuned encoders. Exact F1-macro values depend on (model, language, prompt).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors attribute differences to the match between prompt formulation and the model's instruction-following/multilingual strengths and to cultural/linguistic specificity per language; thus each language often benefits from customized prompt design.",
            "null_or_negative_result": false,
            "experimental_details": "Models run in inference mode for three random seeds; outputs averaged (macro F1). Prompts used max_new_tokens=10, do_sample=False. Yes/no placeholders randomly swapped to reduce position bias; few-shot used 1/3/5 examples per class (5-shot typically 5 hate + 5 non-hate interleaved).",
            "uuid": "e9278.0",
            "source_info": {
                "paper_title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "FewShot-Effect",
            "name_full": "Effect of Few-shot Prompting (1/3/5 examples) vs Zero-shot",
            "brief_description": "Including a few task-specific examples (especially 5-shot) generally improves LLM performance, most noticeably on functional tests; improvements on real-world datasets are inconsistent and depend on prompt clarity and example quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction-tuned multilingual LLMs (LLaMA-3.1-8B, Qwen2.5-7B, Aya-101, BloomZ-7B1)",
            "model_size": "LLaMA-3.1-8B, Qwen2.5-7B, BloomZ-7B1, Aya-101 (size not specified)",
            "task_name": "Multilingual hate speech detection (real-world and functional tests)",
            "task_description": "Binary hate vs non-hate classification using few-shot in-context examples drawn from the training set (retrieved examples).",
            "presentation_format": "Few-shot prompting with 1, 3, or 5 examples per class inserted into the prompt; examples were interleaved by class (e.g., hate, non-hate, ...). Combinations with other prompt types (e.g., few-shot + CoT, few-shot + role-play) were also tested.",
            "comparison_format": "Compared to zero-shot prompts (same prompt templates without examples) and to fine-tuned encoders.",
            "performance": "Qualitative: few-shot (typically 5-shot) increased F1-macro on functional tests substantially and provided smaller or inconsistent gains on real-world tests; exact numeric F1 gains vary by model/language/prompt.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Few-shot examples help models apply contextual distinctions and task-specific decision boundaries, improving generalization on controlled functional test cases where examples map closely to test phenomena; on noisy, culturally-specific real-world data, benefits depend on representativeness of examples and prompt clarity.",
            "null_or_negative_result": true,
            "experimental_details": "Few-shot retrieval used training set examples; 5-shot used 5 hate + 5 non-hate examples. Context length limits prevented using more than 5 examples per class across all prompts/models. Results averaged over three runs.",
            "uuid": "e9278.1",
            "source_info": {
                "paper_title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CoT-Effect",
            "name_full": "Chain-of-Thought (CoT) Prompting Effect",
            "brief_description": "Chain-of-thought (step-by-step) prompting sometimes improves performance, particularly when combined with few-shot examples, but its benefits are inconsistent and less pronounced on real-world datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction-tuned multilingual LLMs (notably LLaMA-3.1-8B and Qwen2.5-7B)",
            "model_size": "LLaMA-3.1-8B, Qwen2.5-7B",
            "task_name": "Hate speech detection (binary)",
            "task_description": "Models prompted to reason step-by-step about whether a comment targets protected groups or exhibits hostility/dehumanization, then produce a yes/no decision.",
            "presentation_format": "Zero-shot CoT prompts and few-shot + CoT combinations where the prompt requests stepwise analysis (Step 1, Step 2, Step 3) before concluding; sometimes combined with multilingual context or role-play.",
            "comparison_format": "Compared against vanilla zero-shot prompts, classification-style prompts, and few-shot without CoT.",
            "performance": "Qualitative: CoT improved accuracy/F1 in some functional test settings and for certain models (e.g., LLaMA3 performed well on functional benchmarks), but did not uniformly improve real-world test performance.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors suggest CoT can help when reasoning about multi-step definitions (e.g., identifying target, then hostility level), but it may introduce verbosity or distract from binary decision-making on noisy real-world input.",
            "null_or_negative_result": true,
            "experimental_details": "CoT prompts explicitly enumerated steps to assess targeting and hostility; used both zero-shot and few-shot+CoT settings. Responses constrained to binary outputs to avoid extraneous text.",
            "uuid": "e9278.2",
            "source_info": {
                "paper_title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RolePlay-Effect",
            "name_full": "Role-play Prompting (Community Moderator Role)",
            "brief_description": "Assigning the model the role of a community moderator (role-play) often improves performance for some models, with Qwen showing sensitivity and strong performance with role-play and NLI-style prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-7B-Instruct (noted), LLaMA-3.1-8B, Aya-101, BloomZ-7B1",
            "model_size": "Qwen2.5-7B, LLaMA-3.1-8B, BloomZ-7B1, Aya-101 (size not specified)",
            "task_name": "Hate speech detection (binary) in multiple languages",
            "task_description": "Prompts framed the model as a moderator responsible for enforcing hate speech policies and asked for binary decisions.",
            "presentation_format": "Zero-shot role-play and few-shot + role-play prompts where the model is told it previously handled cases (examples) and must decide if a new comment violates policy.",
            "comparison_format": "Compared to vanilla, classification, CoT, and few-shot without role-play.",
            "performance": "Qualitative: Role-play improved performance for some (notably Qwen) especially on functional tests; gains on real-world tests varied by language and model.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Role-play likely primes the model to use policy-aligned decision heuristics and conversational cues, benefiting models sensitive to contextual and persona framing.",
            "null_or_negative_result": false,
            "experimental_details": "Role-play prompts used community moderator framing; some role-play prompts were combined with CoT or few-shot examples. Outputs constrained to 'yes'/'no'.",
            "uuid": "e9278.3",
            "source_info": {
                "paper_title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Definition-Distinction-Effect",
            "name_full": "Definition and Distinction-based Prompts",
            "brief_description": "Providing explicit definitions of hate speech and telling the model to distinguish hate speech from toxic/offensive language improves performance for certain models; Aya-101 performed best with definition-and distinction-based prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Aya-101 (noted), other instruction-tuned LLMs",
            "model_size": "Aya-101 (size not specified); other models as above",
            "task_name": "Hate speech detection (binary) across languages",
            "task_description": "Prompts include formal definitions of hate speech and related categories (toxic/offensive language) and ask for binary classification.",
            "presentation_format": "Zero-shot definition prompts and few-shot + definition settings where definition text and labeled examples are provided in the prompt.",
            "comparison_format": "Compared to vanilla, classification, CoT, and few-shot without definitions.",
            "performance": "Qualitative: Definition/distinction prompts improved accuracy for Aya-101 and helped models disambiguate related abusive content; performance gains were especially visible on functional tests.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Explicit definitions reduce ambiguity and align model decision boundaries with the target task, particularly beneficial for models tuned for equitable cross-lingual behavior (as with Aya-101).",
            "null_or_negative_result": false,
            "experimental_details": "Definition text enumerated attributes (race, religion, etc.) and examples; used in zero-shot and few-shot templates; binary-only outputs requested.",
            "uuid": "e9278.4",
            "source_info": {
                "paper_title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "TranslateThenClassify",
            "name_full": "Translate-then-Classify Prompt Strategy",
            "brief_description": "Prompts that instruct the model to translate non-English text to English and then classify were explored as a strategy; motivated by prior work showing translation can help in low-resource settings, but the paper reports mixed results across languages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction-tuned multilingual LLMs (all four evaluated)",
            "model_size": "LLaMA-3.1-8B, Qwen2.5-7B, BloomZ-7B1, Aya-101 (size unspecified)",
            "task_name": "Cross-lingual hate speech detection (translate + classify)",
            "task_description": "Model translates input from {language} to English then performs binary classification on the translation within the same prompt.",
            "presentation_format": "Zero-shot 'translate' prompts and few-shot variants instructing explicit translation followed by classification.",
            "comparison_format": "Compared to direct classification in the original language using multilingual prompts.",
            "performance": "Qualitative: Mixed — in some languages translation-first prompts helped, consistent with prior findings that translating to English can benefit certain LLMs; in other languages language-aware multilingual prompts or native prompting performed as well or better.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Translation can map low-resource language phenomena into a representation the model handles better, but translation errors and cultural nuances can reduce benefits.",
            "null_or_negative_result": false,
            "experimental_details": "Translate prompts asked model to first translate the text then classify, with binary-only outputs; position bias mitigation and 3-run averaging applied.",
            "uuid": "e9278.5",
            "source_info": {
                "paper_title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Prompting-vs-Finetune",
            "name_full": "Comparison: Prompted LLMs vs Fine-tuned Encoder Models",
            "brief_description": "Prompting LLMs (zero- and few-shot) generally underperforms fine-tuned encoder models (XLM-T, mDeBERTa) on real-world test sets, but often outperforms or matches them on functional tests; prompting becomes competitive at small encoder training sizes (language-dependent thresholds).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction-tuned LLMs (various) vs encoder baselines (XLM-T, mDeBERTa)",
            "model_size": "XLM-T ~279M parameters, mDeBERTa ~86M; LLM sizes as above",
            "task_name": "Multilingual hate speech detection (real-world vs functional tests) and data-scaling comparison",
            "task_description": "Compare zero-/few-shot prompted LLM performance to encoder models fine-tuned on varying amounts of labeled data (10 to 2000 examples) using macro F1.",
            "presentation_format": "Best zero-/few-shot prompting results (across prompt types) compared to encoder models fine-tuned on different training set sizes.",
            "comparison_format": "Fine-tuned encoder performance at varying dataset sizes (10..2000) vs prompting (zero-/few-shot).",
            "performance": "High-level: Encoder models generally achieve higher macro F1 on real-world test sets when trained on sufficient labeled data; prompting often attains stronger generalization on functional tests and matches/exceeds XLM-T trained on 2,000 examples in Portuguese, Arabic, French, and Italian.",
            "performance_comparison": "Prompting competitive thresholds: Spanish ~100-200 labeled examples, Hindi ~300-400, German ~600-700 — beyond these sizes fine-tuning generally yields better performance.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Fine-tuning on task-specific data improves decision boundaries for noisy, culturally-dependent real-world data, while large instruction-tuned LLMs generalize better on controlled functional phenomena without additional training; data availability determines the preferred approach.",
            "null_or_negative_result": false,
            "experimental_details": "Encoder models fine-tuned for 10 epochs, evaluated across training sizes from 10 up to full dataset (~2000) with five random seeds; prompting results represent best zero-/few-shot configurations aggregated from three runs.",
            "uuid": "e9278.6",
            "source_info": {
                "paper_title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "FewShot-ScaleDetails",
            "name_full": "Few-shot Implementation and Context Constraints",
            "brief_description": "Few-shot experiments used retrieved examples from the training set (1, 3, or 5 examples per class); 5-shot was typical (5 hate + 5 non-hate interleaved), but context length and compute limited use of larger example sets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction-tuned multilingual LLMs evaluated in the study",
            "model_size": null,
            "task_name": "Few-shot in-context learning for hate speech classification",
            "task_description": "In-context example provisioning to guide model predictions.",
            "presentation_format": "Few-shot prompts with examples inserted into the template (examples interleaved by class), experimented with 1, 3, and 5 examples per class; combinations with CoT, role-play, definition, multilingual context tested.",
            "comparison_format": null,
            "performance": "Operational detail: 5-shot most commonly used and often produced best functional-test performance; performance varied by prompt/model/language.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Context length constraints limited number of in-context examples; interleaving by class aimed to present balanced positive/negative examples.",
            "null_or_negative_result": false,
            "experimental_details": "5-shot means 5 hate + 5 non-hate examples (10 examples total) interleaved. Attempts to use more than 5 per class were not feasible across all prompts/models due to context/window limits. Prompts and examples inserted into placeholders {examples} in templates.",
            "uuid": "e9278.7",
            "source_info": {
                "paper_title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An investigation of large language models for real-world hate speech detection",
            "rating": 2,
            "sanitized_title": "an_investigation_of_large_language_models_for_realworld_hate_speech_detection"
        },
        {
            "paper_title": "Designing of prompts for hate speech recognition with in-context learning",
            "rating": 2,
            "sanitized_title": "designing_of_prompts_for_hate_speech_recognition_with_incontext_learning"
        },
        {
            "paper_title": "Probing LLMs for hate speech detection: strengths and vulnerabilities",
            "rating": 2,
            "sanitized_title": "probing_llms_for_hate_speech_detection_strengths_and_vulnerabilities"
        },
        {
            "paper_title": "Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech",
            "rating": 1,
            "sanitized_title": "is_chatgpt_better_than_human_annotators_potential_and_limitations_of_chatgpt_in_explaining_implicit_hate_speech"
        },
        {
            "paper_title": "Investigating the predominance of large language models in low-resource bangla language over transformer models for hate speech detection: A comparative analysis",
            "rating": 1,
            "sanitized_title": "investigating_the_predominance_of_large_language_models_in_lowresource_bangla_language_over_transformer_models_for_hate_speech_detection_a_comparative_analysis"
        },
        {
            "paper_title": "Hate personified: Investigating the role of LLMs in content moderation",
            "rating": 2,
            "sanitized_title": "hate_personified_investigating_the_role_of_llms_in_content_moderation"
        }
    ],
    "cost": 0.016778249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study
24 May 2025</p>
<p>Faeze Ghorbanpour faeze.ghorbanpour@tum.de 
School of Computation, Information and Technology
MunichTU</p>
<p>Munich Center for Machine Learning (MCML)</p>
<p>Daryna Dementieva daryna.dementieva@tum.de 
School of Computation, Information and Technology
MunichTU</p>
<p>Alexander Fraser 
School of Computation, Information and Technology
MunichTU</p>
<p>Munich Center for Machine Learning (MCML)</p>
<p>Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study
24 May 2025F5D49F58F4F50C974E9CD6270659E966arXiv:2505.06149v3[cs.CL]
Despite growing interest in automated hate speech detection, most existing approaches overlook the linguistic diversity of online content.Multilingual instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ offer promising capabilities across languages, but their effectiveness in identifying hate speech through zero-shot and fewshot prompting remains underexplored.This work evaluates LLM prompting-based detection across eight non-English languages, utilizing several prompting techniques and comparing them to fine-tuned encoder models.We show that while zero-shot and few-shot prompting lag behind fine-tuned encoder models on most of the real-world evaluation sets, they achieve better generalization on functional tests for hate speech detection.Our study also reveals that prompt design plays a critical role, with each language often requiring customized prompting techniques to maximize performance 1 .</p>
<p>Introduction</p>
<p>Hate speech is a worldwide issue that undermines the safety of social media platforms, no matter the language (Thomas et al., 2021).It can violate platform rules, damage user trust, influence opinions, and reinforce harmful biases against individuals or groups targeted (MacAvaney et al., 2019;Vedeler et al., 2019;Stockmann et al., 2023).However, recent advancements in hate speech detection have been largely focused on English, as most datasets and language models are centered on English content, resulting in limited attention to other languages (Huang et al., 2023;Peng et al., 2023).Since users on social media write and engage with content in many languages-not just English-it is crucial to find tools that can detect hate speech in a multilingual context.</p>
<p>Instruct-tuned Large language models (LLMs) have demonstrated exceptional performance across a wide range of text-related tasks (Skibicki, 2025;Zhang et al., 2024).Many of these models possess multilingual capabilities, enabling them to process and understand text in various languages (Pedrazzini, 2025;Shaham et al., 2024).This makes them suitable for tasks like hate speech detection without the need for additional fine-tuning, thereby reducing computational and resource costs.While their effectiveness in detecting hate speech in English has been studied extensively (Roy et al., 2023;Guo et al., 2023;Zhang et al., 2025), their performance on non-English datasets remains underexplored.</p>
<p>To evaluate the capabilities of multilingual instruction-tuned LLMs in detecting hate speech in various languages, we conduct a study using several prompting techniques, including zero-shot prompting (e.g., vanilla, chain-of-thought, role-play), fewshot prompting, and combinations of these prompts.We evaluate performance across eight non-English hate speech detection tasks, covering Spanish, Portuguese, German, French, Italian, Turkish, Hindi, and Arabic, using real-world2 and hate speech functional test sets.This study seeks to address the following research questions: (1) How well do LLMs perform on hate speech detection across various non-English languages?(2) Does few-shot prompting improve performance compared to zero-shot prompting?(3) How does LLM performance compare to that of traditional fine-tuned models?</p>
<p>Our findings highlight the importance of prompt design in multilingual hate speech detection.While performance varies by the prompting strategy, experimenting with different techniques leads to reasonably strong results.In most languages, few-shot prompting combined with other techniques outperforms zero-shot prompting, suggesting that providing a few task-specific examples is beneficial.</p>
<p>Compared to fine-tuned encoder models, prompting LLMs shows lower performance on real-world test sets.However, in functional test cases, prompting often performs better.Further analysis of languages where prompting underperforms on realworld data suggests that prompting can still be a practical option when only limited training data is available.Nonetheless, with access to larger training sets, fine-tuning encoder models remains the more effective approach.Overall, instruction-tuned LLMs demonstrate stronger generalization in controlled functional benchmarks, without the need for additional training.</p>
<p>Related Work</p>
<p>The ability of instruction-tuned LLMs to perform a wide range of NLP tasks without the need for finetuning or training data has drawn growing interest, particularly in applications like hate speech detection.Recent studies have explored LLM-based hate speech detection, primarily in English.Zhu et al. (2025) reports low agreement between LLM predictions and human annotations, while Li et al. (2024) finds that LLMs are more effective at identifying non-hateful content.Huang et al. (2023) examines the use of LLMs for generating explanations of implicit hate, and Roy et al. (2023) shows that including target-specific information in prompts improves performance.</p>
<p>Another study examines how in-context learning, combined with few-shot examples and task descriptions, boosts the performance of hate speech detection by LLMs (Han and Tang, 2022).Guo et al. (2023) investigates using LLMs for real-world hate speech detection using four diverse prompting strategies and finds that few-shot and chain-ofthought prompts help.While these works have explored prompting techniques, they primarily assess the capabilities of LLMs for hate speech detection in English and do not examine a broad range of prompting strategies across languages.</p>
<p>There have been efforts to investigate the capabilities of LLMs for non-English hate speech.Guo et al. (2023) andFaria et al. (2024) tested prompt strategies only in Chinese and Bangla, respectively.Masud et al. (2024) assesses LLMs' sensitivity to geographical priming and persona attributes in five languages, showing that geographical cues can improve regional alignment in hate speech detection.Similarly, Zahid et al. (2025) uses geographical contextualization into prompts for five languages.These motivate our use of language-aware prompts; However, they do not explore a wide range of prompting strategies, such as few-shot, chain-ofthought, or role-play prompts.Tonneau et al. (2024) evaluate hate speech detection in eight languages using real-world and functional test sets, but rely solely on vanilla prompting.(Dey et al., 2024) applied prompting LLMs to three low-resource South Asian languages, finding that translating inputs to English outperformed prompting in the original language.This motivated us to prompt the LLM to translate before classifying.In contrast to these efforts, our work covers eight languages and evaluates a broader range of prompt designs on real-world and functional test sets.</p>
<p>Datasets</p>
<p>We selected datasets with explicit hate speech labels that follow a definition consistent with our story: abusive language that targets a protected group or individuals for being part of that group (Röttger et al., 2021).We randomly selected 2,000 samples from each dataset as test sets for evaluating both prompting and fine-tuned models.For Arabic and French, smaller dataset sizes limited the test sets to 1,000 and 1,500 samples, respectively.The remaining data was used to train the encoder models.</p>
<p>The datasets are as follows:</p>
<p>Models</p>
<p>We evaluate four instruction-tuned multilingual LLMs for hate speech detection across eight languages: LLaMA-3.1-8B-Instruct(Grattafiori et al., 2024): Meta's instruction-tuned decoder model, optimized for reasoning tasks and primarily designed for English, with multilingual support.Qwen2.5-7B-Instruct(Qwen et al., 2025;Yang et al., 2024): A multilingual decoder model by Alibaba Cloud, supporting 30+ languages with strong instruction-following capabilities.Aya-101 (Üstün et al., 2024): Cohere's multilingual model trained on 100+ languages, tuned for equitable cross-lingual NLP, including hate speech detection.BloomZ-7B1 (Muennighoff et al., 2023a): A decoder model by BigScience, fine-tuned via multitask instruction tuning on 46 languages for cross-lingual instruction following.</p>
<p>For the encoder-based baseline, we fine-tuned two multilingual models with strong performance on classification tasks: XLM-T (Barbieri et al., 2022;Conneau et al., 2020): An XLM-R extension pre-trained on 198M Twitter posts in 30+ languages.mDeBERTa (He et al., 2021): A multilingual encoder covering 100+ languages, effective in zero-shot and low-resource settings.Further implementation details and hyperparameters are provided in Appendix A.</p>
<p>Prompts</p>
<p>We assess instruction-tuned multilingual LLMs using a range of prompting strategies for hate speech detection, such as: directly asking whether a comment is hateful (vanilla); prompting the model to act as a classifier (classification); chain-of-thought prompting for step-by-step reasoning (CoT); natural language inference-inspired (NLI) prompts; language-aware prompts that consider linguistic and cultural context (multilingual); assigning the LLM the role of a community moderator (role-play); translate then classify prompts (translate); definition-based prompts that explain hate speech (definition); and defining related forms of abusive content to help the model differentiate them from hate speech (distinction), etc.We also include fewshot prompting, where we retrieve and insert example instances from the training set into the prompt.We also explore combinations of these strategies.For full prompt texts and implementation details, see Appendix B.</p>
<p>Results</p>
<p>We evaluate instruction-tuned LLMs with various prompt types over three runs in the inference mode and report the average F1-macro scores.Table 1 summarizes the performance of zero-/few-shot results for four instruction-tuned models across eight languages.We observe that prompt design significantly affects performance.Aya101 performs best with definition-and distinction-based prompts, suggesting that explicit definitions improve its accuracy.In contrast, Qwen excels with NLI and role-play prompts, indicating sensitivity to context and conversational cues.</p>
<p>In zero-shot settings, Qwen and LLaMA3 generally outperform the other models, with similar overall performance.However, Qwen performs better in most real-world test cases, whereas LLaMA3 leads on functional benchmarks.Few-shot prompting (typically five-shot) improves performance, especially on functional tests, as examples help the model apply contextual distinctions more effectively.On real-world tests, improvement is less consistent-even with examples from the same training data.This suggests that few-shot effectiveness depends not only on data quality, but also on prompt clarity and structure.Overall, instructiontuned LLMs perform notably well on functional tests and reasonably on real-world tests in different languages.However, their effectiveness depends heavily on prompt design and the inclusion of fewshot examples.Appendix D contains detailed performance results for Spanish and Portuguese.</p>
<p>For comparison, we fine-tune two encoder models for binary hate speech classification on train sets of datasets using five random seeds and report the average macro F1 scores.benefiting from fine-tuning on task-specific data.</p>
<p>However, the trend reverses on functional tests, where few-shot prompting often yields better results-highlighting the stronger generalization ability of large LLMs in controlled evaluation settings.To understand when prompting is preferable, we conducted additional experiments comparing encoder model performance at varying training set sizes to that of prompting.Figure 1 presents results for three languages where prompting under-performs compared to fine-tuned models.Depending on the language, prompting becomes competitive when training data is limited-for example, with 100-200 examples in Spanish, 300-400 in Hindi, or 600-700 in German.Beyond that, finetuning generally yields better performance.See Appendix C for more results across other languages.</p>
<p>Limitations</p>
<p>One unavoidable limitation of our work is the number of multilingual instruction-tuned LLMs we were able to include.Given the rapid growth and proliferation of generative AI models, new LLMs are continually emerging.However, due to resource and time constraints, we were unable to include more models in our evaluation.</p>
<p>A second limitation concerns the additional contextual information available for prompt construction.Most of our datasets included only the text, label, and language, but lacked richer metadata.Incorporating information such as the targeted group of the hate speech, the context in which it occurred, or the domain of the text could potentially improve model performance (Roy et al., 2023).</p>
<p>Moreover, we treated these LLMs as black-box models and did not attempt to analyze their internal parameters or architectural components.We also did not fine-tune the larger models to better adapt them to our datasets.</p>
<p>Finally, while we incorporated a wide range of carefully designed prompt variations to probe model behavior, our set of prompt configurations is not exhaustive.There may exist alternative formulations or edge cases that we have not explored.Therefore, our findings should be interpreted as indicative rather than definitive.</p>
<p>A Model and Training Details</p>
<p>A.1 LLM Selection and Setup</p>
<p>To select suitable instruction-tuned multilingual LLMs, we first conducted a brief experiment to ensure that their safety tuning would not interfere with hate speech classification.Our goal was to evaluate detection capabilities, not robustness to jailbreak attempts.We excluded models such as mT0-large (Muennighoff et al., 2023b), Ministral-8B-Instruct(AI, 2024), and Teuken-7B-instruct (Ali et al., 2024) that failed to follow instructions reliably.We used the transformers library3 to load and run models in inference mode, generating binary outputs (yes or no).We set max_new_tokens=10, do_sample=False, and left temperature/top-k/top-p unset.Batch size and max sequence length varied depending on the prompt and model.Each experiment was run with three random seeds, and we also swapped the position of yes and no in prompts to mitigate position bias.</p>
<p>A.2 Encoder Model Training</p>
<p>For training the encoder-based models, in addition to the previously mentioned 2,000-sample test set, we randomly held out 500 samples for validation and used the remaining data for training.Models were fine-tuned for 10 epochs using the transformers Trainer, with a batch size of 16 and max sequence length of 128.Default settings were used for the learning rate, optimizer, and scheduler.We evaluated the models on both the test set of the corresponding dataset and its language-specific subset in the HateCheck benchmark.</p>
<p>A.3 Data Formatting</p>
<p>Most datasets used were binary hate vs. non-hate classification tasks.Any remaining datasets such as German and Turkish ones were also converted to this binary format to ensure consistency.Most of the datasets used in this study are sourced from the GitHub repository by Röttger et al. (2022a):https: //github.com/paul-rottger/efficient-low-resource-hate-detection.The German dataset is available at:https://github.com/jagol/gahd, and the Turkish dataset is from:https://github.com/ avaapm/hatespeech.These datasets are legally licensed and permitted for use in research projects.</p>
<p>A.4 Model Size and Budget</p>
<p>Experiments with instruction-tuned LLMs-LLaMA34 , Qwen2.55 , Aya1016 , and BloomZ7 -were primarily conducted on NVIDIA RTX A6000 servers in inference mode, with no parameter updates during prompting.In contrast, fine-tuning of encoder models was performed on NVIDIA GeForce GTX 1080 Ti GPUs, where all model parameters were updated during training.The mDeBERTa8 has approximately 86 million parameters, while XLM-T9 consists of around 279 million parameters.All models used in this study were sourced from Hugging Face and are licensed for legal use in academic research.</p>
<p>B Prompts Details</p>
<p>You can find the zero-shot prompt texts in Table 3and Table 4 and the few-shot prompt texts in Table 5.In these tables, "CoT" refers to chain-of-thought prompting, and "NLI" refers to prompts guided by natural language inference.The "+" symbol indicates a combination of the two prompt types.Dataset instances are enclosed in {text}.The placeholders {yn1} and {yn2} correspond to the expected outputs ("yes" and "no"), which were randomly swapped to reduce positional bias.The input language is represented as {language}, and if the prompt requires few-shot examples, they are inserted in {examples}.</p>
<p>In our few-shot experiments, we retrieve 1, 3, or 5 examples per class from the training set and include them in the prompt to guide the model's predictions.For example, a 5-shot setting includes 5 hate and 5 non-hate examples, interleaved by class (e.g., one hate, one non-hate), resulting in a total of 10 examples.While we experimented with using more than 5 examples per class, context length limitations and computational constraints prevented us from applying this to all prompts and models.</p>
<p>C Comparing Prompting and Fine-tuning Under Varying Data Conditions</p>
<p>D Full Prompting Results</p>
<p>The complete prompting results of the four instruction-tuned LLMs for Spanish and Portuguese are shown in Tables 6 and 7. Step 1: Identify if the comment targets a specific person or group based on race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity.If no target is identified, the comment is not hate speech.</p>
<p>Step 2: Assess whether the language expresses hostility, discrimination, incitement to violence, dehumanization, or derogatory stereotypes toward the target.</p>
<p>Step 3: Based on the above steps, conclude whether this is hate speech.Step 1: Identify any offensive or harmful language targeting a person or group.</p>
<p>Step 2: Assess if the language attacks based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity.</p>
<p>Step 3: Determine if the comment violates the community's hate speech policy.Does this comment violate the community guidelines on hate speech?Answer only {yn1} or {yn2}.Don't provide explanations or any other information.Comment: "{text}".Answer: multilingual + cot Analyze the following text written in {language} step by step, considering cultural context, idioms, and societal norms in {language}-speaking communities.</p>
<p>Step 1: Identify any offensive or harmful language targeting a person or group.</p>
<p>Step 2: Determine if the language attacks based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity.</p>
<p>Step   Step 1: Identify any offensive or harmful language targeting a person or group.</p>
<p>Step 2: Determine if the language attacks based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity.</p>
<p>Step</p>
<p>Figure 1 :
1
Figure 1: Performance of zero-/few-shot prompted LLMs vs. fine-tuned XLM-T across varying training sizes.</p>
<p>Figure 2
2
Figure2illustrates the performance of the XLM-T model fine-tuned on training sets ranging from 10 to 2,000 instances across various languages, alongside the best zero-/few-shot results from instruction-tuned LLMs.Notably, in Portuguese, Arabic, French, and Italian, zero-or few-shot prompting matches or exceeds the performance of XLM-T even when trained on 2,000 labeled examples.In other languages, prompting performs competitively when training data is limited, offering a strong alternative in lowresource settings.As expected, fine-tuning generally surpasses prompting when sufficient labeled data is available, highlighting a practical trade-off between data availability and model adaptation strategy.</p>
<p>Figure 2 :
2
Figure 2: Performance of zero-/few-shot prompted LLMs vs. fine-tuned XLM-T across varying training sizes.</p>
<p>Table 1 :
1
Table 2 summarizes the performance of encoder models alongside the best zero-and few-shot prompting results.On real-world datasets, encoder models generally outperform LLM prompting across most languages, Zero-shot and Few-shot Prompting Results for Instruction-tuned Multilingual LLMs.(f1=f1-macro)
BloomZAya101Llama3Qwanzero-shotzero-shotzero-shotfew-shotzero-shotfew-shotpromptf1promptf1promptf1promptf1promptf1promptf1Real World Testses classification 54.50 definition pt definition 63.92 definition hi multilingual 51.33 classification 47.33 CoT 62.68 classification 63.13 5 shot + CoT 71.51 role-play 70.79 5 shot + multilingual 52.09 1 shot + CoT ar NLI 58.67 distinction 64.67 classification 62.66 3 shot + definition fr NLI 55.63 translate 53.44 CoT 55.22 5 shot + definition it CoT 55.50 vanilla 74.82 distiction 75.86 5 shot + CoT de CoT 38.36 general 67.51 role-play 50.16 1 shot + multilingual68.89 translate 73.7 role-play + CoT 54.90 distinction 64.71 NLI 51.53 NLI 76.18 multilingual 75.16 target64.79 5 shot + CoT 73.44 5 shot + role-play 53.76 1 shot + CoT 70.61 3 shot 55.59 3 shot 73.34 5 shot + CoT 50.19 5 shot + definition68.90 72.56 49.57 67.36 52.22 76.57 77.55trrole-play55.20 --classification 76.16 5 shot + CoT81.76 translate75.89 5 shot + CoT77.03Functional Testses definition pt definition hi role-play ar definition fr CoT it role-play de role-play64.88 distinction 66.04 distinction 51.99 distinction 62.08 vanilla 62.62 distinction 55.15 distinction 51.75 distinction73.19 vanilla 72.39 classification 83.37 3 shot 86.37 5 shot 65.95 classification 65.31 1 shot + multilingual 62.99 contextual 64.00 1 shot 71.94 vanilla 84.61 5 shot + role-play 71.25 role-play 78.54 5 shot 72.64 contextual 83.27 5 shot + multilingual87.40 vanilla 86.59 CoT 65.36 definition 67.95 general 84.37 vanilla 82.95 target 89.65 contextual84.39 5 shot + definition 82.15 5 shot + definition 65.41 1 shot + definition 70.42 3 shot + definition 82.06 5 shot + role-play 78.35 5 shot + definition 82.64 5 shot + definition86.43 84.08 65.93 71.88 82.61 84.17 86.62</p>
<p>Table 2 :
2
Results (f1-macro) of fine-tuned encoder models vs. best zero-/few-shot prompting LLMs.</p>
<p>Answer only {yn1} or {yn2}.Don't provide explanations or any other information.This statement contains hate speech as defined by social and legal standards."Does the premise entail the hypothesis?Don't provide explanations or any other information.Answer only {yn1} or {yn2}.Answer: role-play You are a community moderator responsible for ensuring compliance with hate speech policies.Analyze the following text and determine if it violates the guidelines.
Comment: "{text}". Answer:NLIPremise: "{text}"Hypothesis: "Text: "{text}"Does this text violate the community's hate speech policy? Don't provide explanations or any otherinformation. Answer only {yn1} or {yn2}. Answer:multilingual + defi-Hate speech is defined as language that expresses discrimination, hostility, or violence againstnitionindividuals or groups based on race, religion, ethnicity, nationality, gender, sexual orientation, ordisability.Analyze the following text written in {language} considering cultural context, idioms, and societalnorms of {language}-speaking communities. Does this text contain hate speech?Don't provide explanations or any other information. Answer only {yn1} or {yn2}.Text: "{text}". Answer:role-play + cotYou are a community moderator responsible for enforcing hate speech policies. Analyze the followingcomment step by step to determine if it violates these policies.</p>
<p>Table 3 :
3
Zero-shot prompt templates for hate speech detection in eight non-English languages (part 1).
Prompt TypePrompt TextcontextualConsider the context of the following comment, including the target audience and potential impact.Does this comment contain hate speech?Answer only {yn1} or {yn2}. Don't provide explanations or any other information.Comment: "{text}". Answer:translateTranslate the following text from {language} to English: Text: "{text}"Now, classify the text as either hateful or non-hateful. Don't provide explanations or any otherinformation.Respond with yes for hateful and no for non-hateful. Answer:distinctionConsidering the following definitions, tell me if the given comment contains hate speech?-Hate Speech: Attacks, dehumanizes, incites violence, or promotes discrimination against a groupbased on identity (e.g., race, religion, gender, etc.).-Toxic Language: Aggressive, harassing, or harmful speech, but not necessarily identity-based.-Offensive Language: Rude or impolite but does not incite harm or target a group.Answer only {yn1} or {yn2}. Don't provide explanations or any other information.Comment: "{text}". Answer:</p>
<p>Table 4 :
4
Zero-shot prompt templates for hate speech detection in eight non-English languages (part 2).
Prompt TypePrompt Textfew-shotConsider following examples:{examples}Now, classify this comment as either hateful or non-hateful:Comment: "{text}"Respond with yes for hateful and no for non-hateful. Answer only {yn1} or {yn2}. Don't provideexplanations or any other information. Answer:few-shot + CoTHere are some examples of comments and their classifications as yes if it is hateful or no if not:{examples}Now, analyze the following comment step by step and determine if it is hateful.</p>
<p>3: Conclude whether it is hate speech.Don't provide explanations or any other information.
Answer only {yn1} or {yn2}. Don't provide explanations or any other information.Comment: "{text}". Answer:few-shot + role-play As a community moderator, you have previously handled the following cases if they include hatespeech or not:{examples}Based on these examples, analyze the following comment and determine if it violates the community'shate speech policy.Don't provide explanations or any other information. Answer only {yn1} or {yn2}.Comment: "{text}". Answer:few-shot + multilin-Here are examples of comments in {language} and their classifications as yes if it is hateful or no ifgualnot:{examples}Now, analyze the following comment written in {language} and determine if it is hateful. Is thiscomment hateful?Don't provide explanations or any other information. Answer only {yn1} or {yn2}.Comment: "{text}". Answer:few-shot + defini-Hate speech is defined as language that expresses discrimination, hostility, or violence againsttionindividuals or groups based on race, religion, ethnicity, nationality, gender, sexual orientation, ordisability. Here are some examples:{examples}Is this comment hateful? Comment: "{text}"Don't provide explanations or any other information. Answer only {yn1} or {yn2}. Answer:</p>
<p>Table 5 :
5
Few-shot prompt templates for hate speech detection in non-English languages.
Real-world test setsFunctional test setsllama3aya101bloomzqwanllama3aya101bloomzqwangeneral40.1462.5644.4762.2686.3767.3336.3084.39classification63.1360.1654.5062.6483.9239.0638.5381.16definition62.2163.6855.9842.3582.7958.7164.8880.46CoT50.6537.1950.2642.5833.5928.0860.4455.58multilingual57.5460.6248.6663.0869.7259.0429.3348.79NLI47.6835.7458.6618.5025.1837.7657.9028.44role-play58.9960.6855.9043.7978.0344.2556.8155.39multilingual + definition57.5460.6554.1542.4376.7351.0853.1147.35role-play + CoT55.4226.9029.5859.9274.2541.9441.2773.87multilingual + CoT59.0647.1530.6160.5761.3742.2041.2772.68target41.0337.4136.8264.7747.9130.6123.3881.28contextual60.8340.4946.2561.7480.3442.6944.9082.52translate55.9139.0535.8964.7971.8444.4240.5076.16distinction62.8062.9436.8063.4278.0673.1923.5378.38few-shot1 544.76 47.8523.42 None44.76 50.7842.11 45.3386.44 58.6328.46 None34.41 58.8153.74 56.90few shot + CoT1 564.44 68.8928.09 None54.27 55.1564.34 68.9082.93 86.4532.59 None35.54 38.3680.68 84.03few shot + role-play1 543.61 46.0137.55 None57.06 53.2566.44 45.8855.86 56.2937.88 None36.83 29.7781.60 83.45few shot + multilingual1 565.25 68.3542.51 None58.71 57.8264.41 45.2884.89 84.9442.88 None43.10 45.0381.03 82.99few shot + definition1 564.25 66.8541.09 None52.40 48.8564.83 66.9482.53 83.5941.99 None29.18 25.0080.06 86.43</p>
<p>Table 6 :
6
Complete Zero-and Few-shot Prompting Results for Spanish.
Real-world test setsFunctional test setsllama3aya101bloomzqwanllama3aya101bloomzqwangeneral44.9871.0845.0042.7282.7164.9342.9856.25classification67.0545.7039.1270.6383.3734.3528.8379.50definition45.8371.5163.9266.0079.2555.3666.0479.50CoT50.2241.5456.8936.0148.0428.4461.4382.15multilingual67.6467.6849.3966.9776.8856.3837.2675.78NLI49.5441.3653.688.5135.4740.4558.8137.60role-play70.7965.3358.7641.5982.2243.7856.0355.16multilingual_definition63.2960.2948.2869.4969.8248.0150.4878.71role-play + CoT67.1440.5724.0173.4472.8242.3041.1573.05multilingual + CoT45.9155.2335.9072.0678.1441.9141.2573.22target44.2738.3340.6267.7946.3827.9724.5780.58contextual66.0546.0330.9057.5181.2240.7652.2479.48translate69.4063.9643.7460.4176.4445.8341.4975.05distinction69.8267.1140.6259.7578.3072.3925.3677.36146.1621.1849.9770.6085.8028.9445.5951.28few shot346.1321.0755.9270.1986.5929.3065.0853.12547.0021.1959.6970.3957.4829.4468.7053.59170.6326.2853.7672.5583.7827.6633.7578.71few shot + CoT372.3528.7855.2772.3484.2526.0942.6379.99572.9825.8055.2372.5384.2222.5544.5980.57146.3532.9155.3670.7456.0730.0840.1353.55few shot + role-play369.6333.2454.2972.3683.3231.5738.8682.38569.7831.4052.2372.5683.8231.3135.9383.29171.8144.3060.4270.6485.1442.5849.7580.05few shot + multilingual372.2444.6060.7870.4784.7242.7055.9080.84573.7043.8860.5471.1384.7842.3055.5582.16171.0633.3950.7971.7483.1945.6737.2080.40few shot + definition370.5532.8652.6071.4284.4233.0737.5282.82571.4232.4151.5047.5984.7232.5436.5184.08</p>
<p>Table 7 :
7
Complete Zero-and Few-shot Prompting Results for Portuguese.</p>
<p>The code and prompts will be publicly released.
By real-world test sets, we meant datasets collected from actual conversations, which better reflect real-world scenarios.
ConclusionIn this study, we explore the capabilities of multilingual instruction-tuned LLMs in detecting hate speech across eight non-English languages. The findings suggest that different prompting techniques work better for different languages, indicating that it is beneficial to experiment with various prompt designs when addressing a new language. In real-world scenarios, where the data is more culturally dependent, prompting LLMs is less effective than training encoder models with taskspecific data. However, in functional hate speech tests, LLMs tend to perform better and offer more flexibility. Incorporating few-shot examples into prompts in such cases may further enhance the LLMs' performance.
https://huggingface.co/docs/transformers/en/index
https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
https://huggingface.co/CohereLabs/aya-101
https://huggingface.co/bigscience/bloomz-7b1
https://huggingface.co/microsoft/mdeberta-v3-base
https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base
AcknowledgementsThe work was supported by the European Research Council (ERC) through the European Union's Horizon Europe research and innovation programme (grant agreement No. 101113091) and the German Research Foundation (DFG; grant FR 2829/7-1).
Un ministral, des ministraux. A I Mistral, 2024</p>
<p>Lalith Manjunath, Samuel Weinbach, Carolin Penke, Oleg Filatov. Mehdi Ali, Michael Fromm, Klaudia Thellmann, Jan Ebert, Alexander Arno Weber, Richard Rutmann, Charvi Jain, Max Lübbering, Daniel Steinigen, Johannes Leveling, Katrin Klug, Jasper Schulze Buschhoff, Lena Jurkschat, Hammam Abdelwahab, Benny Jörg Stein, Karl-Heinz Sylla, Pavel Denisov, Qasid Nicolo' Brandizzi, Anirban Saleem, Lennard Bhowmick, Chelsea Helmer, Pedro Ortiz John, Malte Suarez, Alex Ostendorff, Jude, Shima Asaadi, Fabio Barth, Rafet Sifa, Fabian Küch, Andreas Herten, René Jäkel, Georg Rehm, Stefan Kesselheim, Joachim Köhler, and Nicolas Flores-Herr2024Teuken-7b-base &amp; teuken-7b-instruct: Towards european llms</p>
<p>XLM-T: Multilingual language models in Twitter for sentiment analysis and beyond. Francesco Barbieri, Luis Espinosa Anke, Jose Camacho-Collados, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2022</p>
<p>SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter. Cristina Valerio Basile, Elisabetta Bosco, Debora Fersini, Viviana Nozza, Francisco Patti, Manuel Rangel, Paolo Pardo, Manuela Rosso, Sanguinetti, 10.18653/v1/S19-2007Proceedings of the 13th International Workshop on Semantic Evaluation. the 13th International Workshop on Semantic EvaluationMinnesota, USAMinneapolis2019Association for Computational Linguistics</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, 10.18653/v1/2020.acl-main.747Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Imran Razzak, and Usman Naseem. 2024. Better to ask in english: Evaluation of large language models on english, low-resource and cross-lingual settings. Krishno Dey, Prerona Tarannum, Md Arid Hasan, </p>
<p>Investigating the predominance of large language models in low-resource bangla language over transformer models for hate speech detection: A comparative analysis. Fatema Tuj, Johora Faria, Laith H Baniata, Sangwoo Kang, 10.3390/math12233687Mathematics. 23122024</p>
<p>A hierarchically-labeled Portuguese hate speech dataset. Paula Fortuna, João Rocha Da Silva, Juan Soler-Company, Leo Wanner, Sérgio Nunes, 10.18653/v1/W19-3510Proceedings of the Third Workshop on Abusive Language Online. the Third Workshop on Abusive Language OnlineFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Improving adversarial data collection by supporting annotators: Lessons from GAHD, a German hate speech dataset. Janis Goldzycher, Paul Röttger, Gerold Schneider, 10.18653/v1/2024.naacl-long.248Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics20241</p>
<p>. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Roziere, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mcconnell, Christophe Keller, Chunyang Touret, Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Songand et al. 2024. The llama 3 herd of models</p>
<p>An investigation of large language models for real-world hate speech detection. Keyan Guo, Alexander Hu, Jaden Mu, Ziheng Shi, Ziming Zhao, Nishant Vishwamitra, Hongxin Hu, 2023 International Conference on Machine Learning and Applications (ICMLA). IEEE2023</p>
<p>Designing of prompts for hate speech recognition with in-context learning. Lawrence Han, Hao Tang, 2022 International Conference on Computational Science and Computational Intelligence (CSCI). IEEE2022</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, International Conference on Learning Representations (ICLR). 2021</p>
<p>Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. Fan Huang, Haewoon Kwak, Jisun An, 10.1145/3543873.3587368Companion proceedings of the ACM web conference 2023. 2023</p>
<p>hot" chatgpt: The promise of chatgpt in detecting and discriminating hateful, offensive, and toxic comments on social media. Lingyao Li, Lizhou Fan, Shubham Atreja, Libby Hemphill, 10.1145/3643829ACM Transactions on the Web. 1822024</p>
<p>Hate speech detection: Challenges and solutions. Sean Macavaney, Hao-Ren, Eugene Yao, Katina Yang, Nazli Russell, Ophir Goharian, Frieder, 10.1371/journal.pone.0221152PloS one. 148e02211522019</p>
<p>Hate personified: Investigating the role of LLMs in content moderation. Sarah Masud, Sahajpreet Singh, Viktor Hangya, Alexander Fraser, Tanmoy Chakraborty, 10.18653/v1/2024.emnlp-main.886Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Overview of the hasoc subtrack at fire 2021: Hate speech and offensive content identification in english and indo-aryan languages and conversational hate speech. Sandip Modha, Thomas Mandl, Kishore Gautam, Hiren Shahi, Shrey Madhu, Tharindu Satapara, Marcos Ranasinghe, Zampieri, 10.1145/3503162.3503176Proceedings of the 13th annual meeting of the forum for information retrieval evaluation. the 13th annual meeting of the forum for information retrieval evaluation2021</p>
<p>Crosslingual generalization through multitask finetuning. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, 10.18653/v1/2023.acl-long.891Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Yong Zheng Xin, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a1</p>
<p>Crosslingual generalization through multitask finetuning. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel2023b</p>
<p>Multilingual and multi-aspect hate speech analysis. Nedjma Ousidhoum, Zizheng Lin, Hongming Zhang, Yangqiu Song, Dit-Yan Yeung, 10.18653/v1/D19-1474Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Multilingual llms: Progress, challenges, and future directions. Filippo Pedrazzini, 2025</p>
<p>When does in-context learning fall short and why?. Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, Juanzi Li, 2023a study on specification-heavy tasks</p>
<p>. : Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Xingzhang Ren, Xuancheng Ren,and Zihan Qiu. 2025. Qwen2.5 technical report</p>
<p>Data-efficient strategies for expanding hate speech detection into under-resourced languages. Paul Röttger, Debora Nozza, Federico Bianchi, Dirk Hovy, 10.18653/v1/2022.emnlp-main.383Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022a</p>
<p>Multilingual Hate-Check: Functional tests for multilingual hate speech detection models. Paul Röttger, Haitham Seelawi, Debora Nozza, Zeerak Talat, Bertie Vidgen, 10.18653/v1/2022.woah-1.15Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). the Sixth Workshop on Online Abuse and Harms (WOAH)Seattle, WashingtonAssociation for Computational Linguistics2022b</p>
<p>HateCheck: Functional tests for hate speech detection models. Paul Röttger, Bertie Vidgen, Dong Nguyen, Zeerak Waseem, Helen Margetts, Janet Pierrehumbert, 10.18653/v1/2021.acl-long.4Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Probing LLMs for hate speech detection: strengths and vulnerabilities. Sarthak Roy, Ashish Harshvardhan, Animesh Mukherjee, Punyajoy Saha, 10.18653/v1/2023.findings-emnlp.407Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Haspeede 2@ evalita2020: Overview of the evalita 2020 hate speech detection task. Manuela Sanguinetti, Gloria Comandini, Elisa Di Nuovo, Simona Frenda, Marco Stranisci, Cristina Bosco, Tommaso Caselli, Viviana Patti, Irene Russo, Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. 2020</p>
<p>Multilingual instruction tuning with just a pinch of multilinguality. Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, Matan Eyal, 10.18653/v1/2024.findings-acl.136Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Large language models: Functionality and impact on everyday applications. Accessed. Michał Skibicki, 2025</p>
<p>Social media governance and strategies to combat online hatespeech in germany. Daniela Stockmann, Sophia Schlosser, Paxia Ksatryo, 10.1002/poi3.348Policy &amp; Internet. 1542023</p>
<p>Thomas Ristenpart, and Gianluca Stringhini. 2021. Sok: Hate, harassment, and the changing landscape of online abuse. Kurt Thomas, Devdatta Akhawe, Michael Bailey, Dan Boneh, Elie Bursztein, Sunny Consolvo, Nicola Dell, Zakir Durumeric, Patrick Gage Kelley, Deepak Kumar, Damon Mccoy, Sarah Meiklejohn, 10.1109/SP40001.2021.000282021 IEEE Symposium on Security and Privacy (SP). </p>
<p>Hateday: Insights from a global hate speech dataset representative of a day on twitter. Manuel Tonneau, Diyi Liu, Niyati Malhotra, Scott A Hale, Samuel P Fraiberger, Victor Orozco-Olvera, Paul Röttger, 2024</p>
<p>Large-scale hate speech detection with crossdomain transfer. Cagri Toraman, Furkan Şahinuç, Eyup Yilmaz, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceELRA2022</p>
<p>Aya model: An instruction finetuned open-access multilingual language model. Ahmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, D' Daniel, Gbemileke Souza, Neel Onilude, Shivalika Bhandari, Hui-Lee Singh, Amr Ooi, Freddie Kayid, Phil Vargus, Shayne Blunsom, Niklas Longpre, Marzieh Muennighoff, Julia Fadaee, Sara Kreutzer, Hooker, 10.18653/v1/2024.acl-long.845Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Hate speech harms: A social justice discussion of disabled norwegians' experiences. Janikke Solstad Vedeler, Terje Olsen, John Eriksen, 10.1080/09687599.2018.1515723Disability &amp; Society. 3432019</p>
<p>. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, 10.48550/arXiv.2407.10671arXiv:2407.10671Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei ChuarXiv preprintYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 technical report</p>
<p>Evaluation of hate speech detection using large language models and geographical contextualization. Monoshi Anwar Hossain Zahid, Swarna Kumar Roy, Das, 2025</p>
<p>. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang, 2024Instruction tuning for large language models: A survey</p>
<p>LLM sensitivity challenges in abusive language detection: Instruction-tuned vs. human feedback. Yaqi Zhang, Viktor Hangya, Alexander Fraser, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational LinguisticsAbu Dhabi, UAE2025Association for Computational Linguistics</p>
<p>Exploring the capability of chatgpt to reproduce human labels for social computing tasks. Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui, Gareth Tyson, 10.1007/978-3-031-78548-1_2Social Networks Analysis and Mining. ChamSpringer Nature Switzerland2025</p>            </div>
        </div>

    </div>
</body>
</html>
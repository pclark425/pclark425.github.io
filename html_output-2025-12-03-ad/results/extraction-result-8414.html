<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8414 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8414</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8414</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-c1799bf28d1ae93e1631be5b59196ee1e568f538</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c1799bf28d1ae93e1631be5b59196ee1e568f538" target="_blank">From Local to Global: A Graph RAG Approach to Query-Focused Summarization</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> GraphRAG is proposed, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text and leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.</p>
                <p><strong>Paper Abstract:</strong> The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as"What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8414.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8414.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphRAG (Graph-based Retrieval-Augmented Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based RAG system that uses an LLM to extract an entity/relationship/claim knowledge graph, partitions it into hierarchical communities, pregenerates community-level summaries (treated as 'self-memory'), and answers global sensemaking queries by map-reduce over those community summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-augmented generation pipeline that (1) uses an LLM to extract entities, relationships, and claims into a knowledge graph, (2) hierarchically partitions the graph into communities (Leiden), (3) generates and stores community summaries as a hierarchical, queryable index, and (4) answers queries by mapping the question to community summaries to produce intermediate answers and reducing them into a global answer. It is designed for global query-focused summarization / sensemaking over large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4 / gpt-4-turbo (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Experiments used GPT-4 / gpt-4-turbo endpoints; prompts and an 8k token query-time context window were used for community-summary generation and final answer synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Global sensemaking / Query-Focused Summarization over large corpora</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer high-level, corpus-wide sensemaking questions (e.g., 'What are the main themes in the dataset?') over document collections (~1M tokens) where single-pass retrieval of a few chunks is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>query-focused summarization / question answering (global sensemaking)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical external summaries as 'self-memory' (a form of retrieval-augmented memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM-built knowledge graph + pregenerated hierarchical community summaries stored as context units; at query time summaries are shuffled, chunked, mapped to partial answers (with helpfulness scores) and reduced (sorted by score and concatenated into final context). The community summaries act as persistent memory for the corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Entity nodes (with aggregated descriptions), edges (relationships with weights), extracted claims, and community-level textual summaries at multiple hierarchical levels (C0..C3).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Selection of community summaries by level, random shuffle and chunking for map-stage; map-stage LLM scores helpfulness (0-100) and filters; reduce-stage orders partial answers by score and concatenates until token limit for final synthesis (prompt concatenation / score-based prioritization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Compared to vector RAG (SS) on two ~1M-token corpora, GraphRAG (C0-C3) produced higher LLM-as-judge win rates for comprehensiveness (Podcast: 72–83%; News: 72–80%; p<.001) and diversity (Podcast: 75–82%; News: 62–71%). Claim-based metrics: average extracted claims per answer (News) C0=34.18 vs SS=25.23; (Podcast) C2=32.46 vs SS=26.50. (Metrics reported as win rates / average claim counts in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Extensive comparisons across community-summary levels (C0..C3), a text-based map-reduce baseline (TS), and vector RAG (SS). Findings: all global methods (GraphRAG levels and TS) significantly outperformed SS on comprehensiveness and diversity; C1–C3 gave small improvements over TS in some settings; root-level summaries (C0) offered a strong efficiency trade-off (dramatically fewer tokens while retaining wins over SS). The paper also examined the effect of removing prompt-engineering self-reflection in graph extraction (see self-reflection entry) and different context window sizes (8k chosen as optimal).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using pregenerated hierarchical community summaries as a form of 'self-memory' enables LLMs to perform global sensemaking over ~1M-token corpora; GraphRAG substantially outperforms conventional vector RAG on comprehensiveness and diversity of answers, while root-level community summaries offer a particularly efficient index with favorable trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>May lose fine-grained details (examples, quotes, citations) unless element-extraction prompts are tuned; evaluation limited to two corpora (~1M tokens); potential for hallucination/fabrication not fully measured; cost of graph extraction and summary generation (though root-level summaries reduce query-time token costs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Local to Global: A Graph RAG Approach to Query-Focused Summarization', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8414.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8414.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vector RAG (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector Retrieval-Augmented Generation (Semantic Search baseline, 'SS')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Canonical vector RAG approach used as a baseline: retrieve text chunks semantically similar to the query (via embeddings/semantic search) and supply them to the LLM for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Vector RAG (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A standard semantic-search-based RAG: represent document chunks with embeddings, retrieve chunks nearest the query, add retrieved chunks to context until token budget is reached, then ask LLM to answer. Implemented as the 'SS' baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4 / gpt-4-turbo (used for generation and evaluation in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLM infrastructure as other conditions; same prompt templates and 8k token context window used for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Global sensemaking / Query-Focused Summarization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer the same high-level corpus-wide sensemaking questions using retrieved text chunks as context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented question answering / summarization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval memory (vector store of text-chunk embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Semantic search over chunk embeddings to fetch a set number of context chunks which are concatenated into the prompt for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text chunks (600-token chunks with overlap in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic nearest-neighbor retrieval (embedding-based semantic search).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Serves as baseline: lower LLM-as-judge win rates for comprehensiveness (baseline lost to global approaches; e.g., comprehensiveness win rates for global methods vs SS: Podcast 72–83%, News 72–80%) and lower claim counts (average extracted claims: News SS=25.23, Podcast SS=26.50). Directness: SS produced the most direct (concise) answers across comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared directly against GraphRAG (C0–C3) and TS (text-based map-reduce). Global approaches significantly outperformed SS on comprehensiveness and diversity. SS was best on directness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vector RAG is effective for local fact retrieval and produces concise/direct answers but fails to support global sensemaking tasks that require corpus-wide thematic synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ineffective for global sensemaking questions requiring coverage across the corpus; reliance on retrieved samples can give the illusion of global coverage when only local facts are retrieved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Local to Global: A Graph RAG Approach to Query-Focused Summarization', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8414.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8414.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-reflection (entity extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-reflection prompt-engineering for entity extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage LLM prompting technique where initial extraction outputs are fed back to the model for evaluation and to elicit missed items; used to improve recall of entity extraction when chunk sizes increase.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-reflection (prompting technique)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An iterative prompting pattern in which the LLM first extracts entities/relations from a chunk, then is asked to judge whether extractions were complete (forced yes/no via logit bias), and if incomplete, is prompted to 'glean' missed entities; iterations can repeat up to a max to improve recall.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4 / gpt-4-turbo (used in entity extraction pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used to perform extraction and self-reflective refinement; applied during graph-index construction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Entity and relationship extraction for knowledge graph construction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect and extract entities, relationships, and claims from text chunks to populate nodes/edges/claims for the graph index.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>information extraction / structured knowledge extraction</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working / iterative internal memory via feeding prior extractions back into prompt</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Previous extraction outputs are concatenated / provided back to the LLM as input for a verification and expansion step (prompt concatenation of extracted entities), with logit bias to force a decision and conditional continuation prompts to encourage additional extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Previously extracted entity/relationship instances (textual tuples) used as context for subsequent extraction iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation / re-prompting using the immediate previous outputs (recency-based within the extraction session).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Using self-reflection allowed larger chunk sizes without drop in entity detection; example reported: GPT-4 extracted almost twice as many entity references at 600-token chunk size than at 2400 tokens, and self-reflection was used to mitigate quality loss for larger chunks (Figure 3). Exact numeric improvement from self-reflection iterations is shown in Figure 3 of the paper (experimentally increases detected entity references).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Larger chunk sizes without self-reflection led to fewer entities detected (recall degradation); exact counts depend on chunk size (paper reports ~2x difference between 600 and 2400 token chunks without mitigation).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>The authors explored removing self-reflection and reported trade-offs: self-reflection increases extraction quality (recall) when using larger chunk sizes but increases per-chunk cost (more LLM calls / tokens). They iterate self-reflection up to a maximum and used a logit-bias forced decision as part of the ablation/analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-reflection improves entity/relationship extraction recall and enables use of larger chunk sizes (reducing the number of LLM calls) without dropping extraction quality, offering a cost/quality trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Self-reflection adds extra LLM steps and token cost per chunk; requires careful tuning (number of iterations, chunk size) to balance cost vs recall; may introduce extra latency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Local to Global: A Graph RAG Approach to Query-Focused Summarization', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Lift yourself up: Retrievalaugmented text generation with self-memory <em>(Rating: 2)</em></li>
                <li>In search of needles in a 11 m haystack: Recurrent memory finds what llms miss <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Generation-augmented retrieval for open-domain question answering <em>(Rating: 1)</em></li>
                <li>Knowledge graph prompting for multi-document question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8414",
    "paper_id": "paper-c1799bf28d1ae93e1631be5b59196ee1e568f538",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "GraphRAG",
            "name_full": "GraphRAG (Graph-based Retrieval-Augmented Generation)",
            "brief_description": "A graph-based RAG system that uses an LLM to extract an entity/relationship/claim knowledge graph, partitions it into hierarchical communities, pregenerates community-level summaries (treated as 'self-memory'), and answers global sensemaking queries by map-reduce over those community summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GraphRAG",
            "agent_description": "A retrieval-augmented generation pipeline that (1) uses an LLM to extract entities, relationships, and claims into a knowledge graph, (2) hierarchically partitions the graph into communities (Leiden), (3) generates and stores community summaries as a hierarchical, queryable index, and (4) answers queries by mapping the question to community summaries to produce intermediate answers and reducing them into a global answer. It is designed for global query-focused summarization / sensemaking over large corpora.",
            "model_name": "gpt-4 / gpt-4-turbo (used in experiments)",
            "model_description": "Experiments used GPT-4 / gpt-4-turbo endpoints; prompts and an 8k token query-time context window were used for community-summary generation and final answer synthesis.",
            "task_name": "Global sensemaking / Query-Focused Summarization over large corpora",
            "task_description": "Answer high-level, corpus-wide sensemaking questions (e.g., 'What are the main themes in the dataset?') over document collections (~1M tokens) where single-pass retrieval of a few chunks is insufficient.",
            "task_type": "query-focused summarization / question answering (global sensemaking)",
            "memory_used": true,
            "memory_type": "hierarchical external summaries as 'self-memory' (a form of retrieval-augmented memory)",
            "memory_mechanism": "LLM-built knowledge graph + pregenerated hierarchical community summaries stored as context units; at query time summaries are shuffled, chunked, mapped to partial answers (with helpfulness scores) and reduced (sorted by score and concatenated into final context). The community summaries act as persistent memory for the corpus.",
            "memory_representation": "Entity nodes (with aggregated descriptions), edges (relationships with weights), extracted claims, and community-level textual summaries at multiple hierarchical levels (C0..C3).",
            "memory_retrieval_method": "Selection of community summaries by level, random shuffle and chunking for map-stage; map-stage LLM scores helpfulness (0-100) and filters; reduce-stage orders partial answers by score and concatenates until token limit for final synthesis (prompt concatenation / score-based prioritization).",
            "performance_with_memory": "Compared to vector RAG (SS) on two ~1M-token corpora, GraphRAG (C0-C3) produced higher LLM-as-judge win rates for comprehensiveness (Podcast: 72–83%; News: 72–80%; p&lt;.001) and diversity (Podcast: 75–82%; News: 62–71%). Claim-based metrics: average extracted claims per answer (News) C0=34.18 vs SS=25.23; (Podcast) C2=32.46 vs SS=26.50. (Metrics reported as win rates / average claim counts in paper.)",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Extensive comparisons across community-summary levels (C0..C3), a text-based map-reduce baseline (TS), and vector RAG (SS). Findings: all global methods (GraphRAG levels and TS) significantly outperformed SS on comprehensiveness and diversity; C1–C3 gave small improvements over TS in some settings; root-level summaries (C0) offered a strong efficiency trade-off (dramatically fewer tokens while retaining wins over SS). The paper also examined the effect of removing prompt-engineering self-reflection in graph extraction (see self-reflection entry) and different context window sizes (8k chosen as optimal).",
            "key_findings": "Using pregenerated hierarchical community summaries as a form of 'self-memory' enables LLMs to perform global sensemaking over ~1M-token corpora; GraphRAG substantially outperforms conventional vector RAG on comprehensiveness and diversity of answers, while root-level community summaries offer a particularly efficient index with favorable trade-offs.",
            "limitations_or_challenges": "May lose fine-grained details (examples, quotes, citations) unless element-extraction prompts are tuned; evaluation limited to two corpora (~1M tokens); potential for hallucination/fabrication not fully measured; cost of graph extraction and summary generation (though root-level summaries reduce query-time token costs).",
            "uuid": "e8414.0",
            "source_info": {
                "paper_title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Vector RAG (SS)",
            "name_full": "Vector Retrieval-Augmented Generation (Semantic Search baseline, 'SS')",
            "brief_description": "Canonical vector RAG approach used as a baseline: retrieve text chunks semantically similar to the query (via embeddings/semantic search) and supply them to the LLM for answer generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Vector RAG (SS)",
            "agent_description": "A standard semantic-search-based RAG: represent document chunks with embeddings, retrieve chunks nearest the query, add retrieved chunks to context until token budget is reached, then ask LLM to answer. Implemented as the 'SS' baseline in experiments.",
            "model_name": "gpt-4 / gpt-4-turbo (used for generation and evaluation in experiments)",
            "model_description": "Same LLM infrastructure as other conditions; same prompt templates and 8k token context window used for fairness.",
            "task_name": "Global sensemaking / Query-Focused Summarization baseline",
            "task_description": "Answer the same high-level corpus-wide sensemaking questions using retrieved text chunks as context.",
            "task_type": "retrieval-augmented question answering / summarization",
            "memory_used": true,
            "memory_type": "external retrieval memory (vector store of text-chunk embeddings)",
            "memory_mechanism": "Semantic search over chunk embeddings to fetch a set number of context chunks which are concatenated into the prompt for the LLM.",
            "memory_representation": "Raw text chunks (600-token chunks with overlap in experiments).",
            "memory_retrieval_method": "Semantic nearest-neighbor retrieval (embedding-based semantic search).",
            "performance_with_memory": "Serves as baseline: lower LLM-as-judge win rates for comprehensiveness (baseline lost to global approaches; e.g., comprehensiveness win rates for global methods vs SS: Podcast 72–83%, News 72–80%) and lower claim counts (average extracted claims: News SS=25.23, Podcast SS=26.50). Directness: SS produced the most direct (concise) answers across comparisons.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared directly against GraphRAG (C0–C3) and TS (text-based map-reduce). Global approaches significantly outperformed SS on comprehensiveness and diversity. SS was best on directness.",
            "key_findings": "Vector RAG is effective for local fact retrieval and produces concise/direct answers but fails to support global sensemaking tasks that require corpus-wide thematic synthesis.",
            "limitations_or_challenges": "Ineffective for global sensemaking questions requiring coverage across the corpus; reliance on retrieved samples can give the illusion of global coverage when only local facts are retrieved.",
            "uuid": "e8414.1",
            "source_info": {
                "paper_title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Self-reflection (entity extraction)",
            "name_full": "Self-reflection prompt-engineering for entity extraction",
            "brief_description": "A multi-stage LLM prompting technique where initial extraction outputs are fed back to the model for evaluation and to elicit missed items; used to improve recall of entity extraction when chunk sizes increase.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Self-reflection (prompting technique)",
            "agent_description": "An iterative prompting pattern in which the LLM first extracts entities/relations from a chunk, then is asked to judge whether extractions were complete (forced yes/no via logit bias), and if incomplete, is prompted to 'glean' missed entities; iterations can repeat up to a max to improve recall.",
            "model_name": "gpt-4 / gpt-4-turbo (used in entity extraction pipeline)",
            "model_description": "LLM used to perform extraction and self-reflective refinement; applied during graph-index construction.",
            "task_name": "Entity and relationship extraction for knowledge graph construction",
            "task_description": "Detect and extract entities, relationships, and claims from text chunks to populate nodes/edges/claims for the graph index.",
            "task_type": "information extraction / structured knowledge extraction",
            "memory_used": true,
            "memory_type": "working / iterative internal memory via feeding prior extractions back into prompt",
            "memory_mechanism": "Previous extraction outputs are concatenated / provided back to the LLM as input for a verification and expansion step (prompt concatenation of extracted entities), with logit bias to force a decision and conditional continuation prompts to encourage additional extractions.",
            "memory_representation": "Previously extracted entity/relationship instances (textual tuples) used as context for subsequent extraction iterations.",
            "memory_retrieval_method": "Prompt concatenation / re-prompting using the immediate previous outputs (recency-based within the extraction session).",
            "performance_with_memory": "Using self-reflection allowed larger chunk sizes without drop in entity detection; example reported: GPT-4 extracted almost twice as many entity references at 600-token chunk size than at 2400 tokens, and self-reflection was used to mitigate quality loss for larger chunks (Figure 3). Exact numeric improvement from self-reflection iterations is shown in Figure 3 of the paper (experimentally increases detected entity references).",
            "performance_without_memory": "Larger chunk sizes without self-reflection led to fewer entities detected (recall degradation); exact counts depend on chunk size (paper reports ~2x difference between 600 and 2400 token chunks without mitigation).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "The authors explored removing self-reflection and reported trade-offs: self-reflection increases extraction quality (recall) when using larger chunk sizes but increases per-chunk cost (more LLM calls / tokens). They iterate self-reflection up to a maximum and used a logit-bias forced decision as part of the ablation/analysis.",
            "key_findings": "Self-reflection improves entity/relationship extraction recall and enables use of larger chunk sizes (reducing the number of LLM calls) without dropping extraction quality, offering a cost/quality trade-off.",
            "limitations_or_challenges": "Self-reflection adds extra LLM steps and token cost per chunk; requires careful tuning (number of iterations, chunk size) to balance cost vs recall; may introduce extra latency.",
            "uuid": "e8414.2",
            "source_info": {
                "paper_title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Lift yourself up: Retrievalaugmented text generation with self-memory",
            "rating": 2
        },
        {
            "paper_title": "In search of needles in a 11 m haystack: Recurrent memory finds what llms miss",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Generation-augmented retrieval for open-domain question answering",
            "rating": 1
        },
        {
            "paper_title": "Knowledge graph prompting for multi-document question answering",
            "rating": 1
        }
    ],
    "cost": 0.013495,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>From Local to Global: A GraphRAG Approach to Query-Focused Summarization</h1>
<p>Darren Edge ${ }^{1 \dagger}$ Ha Trinh ${ }^{1 \dagger}$ Newman Cheng ${ }^{2}$ Joshua Bradley ${ }^{2}$ Alex Chao ${ }^{3}$<br>Apurva Mody ${ }^{3}$ Steven Truitt ${ }^{2}$ Dasha Metropolitansky ${ }^{1}$ Robert Osazuwa Ness ${ }^{1}$<br>Jonathan Larson ${ }^{1}$<br>${ }^{1}$ Microsoft Research<br>${ }^{2}$ Microsoft Strategic Missions and Technologies<br>${ }^{3}$ Microsoft Office of the CTO<br>{daedge, trinhha, newmancheng, joshbradley, achao, moapurva, steventruitt, dasham,robertness, jolarso}@microsoft.com<br>${ }^{1}$ These authors contributed equally to this work</p>
<h4>Abstract</h4>
<p>The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a queryfocused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.</p>
<h2>1 Introduction</h2>
<p>Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using LLMs to answer queries based on data that is too large to contain in a language model's context window, meaning the maximum number of tokens (units of text) that can be processed by the LLM at once (Kuratov et al., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to a large external corpus of text records and retrieves a subset of records that are individually relevant to the query and collectively small enough to fit into the context window of the LLM. The LLM then</p>
<p>generates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang, 2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call vector $R A G$, works well for queries that can be answered with information localized within a small set of records. However, vector RAG approaches do not support sensemaking queries, meaning queries that require global understanding of the entire dataset, such as "What are the key trends in how scientific discoveries are influenced by interdisciplinary research over the past decade?"
Sensemaking tasks require reasoning over "connections (which can be among people, places, and events) in order to anticipate their trajectories and act effectively" (Klein et al., 2006). LLMs such as GPT (Achiam et al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) excel at sensemaking in complex domains like scientific discovery (Microsoft, 2023) and intelligence analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an implicit and interconnected set of concepts, an LLM can generate a summary that answers the query. The challenge, however, arises when the volume of data requires a RAG approach, since vector RAG approaches are unable to support sensemaking over an entire corpus.
In this paper, we present GraphRAG - a graph-based RAG approach that enables sensemaking over the entirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph, where nodes correspond to key entities in the corpus and edges represent relationships between those entities. Next, it partitions the graph into a hierarchy of communities of closely related entities, before using an LLM to generate community-level summaries. These summaries are generated in a bottom-up manner following the hierarchical structure of extracted communities, with summaries at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these community summaries provide global descriptions and insights over the corpus. Finally, GraphRAG answers queries through map-reduce processing of community summaries; in the map step, the summaries are used to provide partial answers to the query independently and in parallel, then in the reduce step, the partial answers are combined and used to generate a final global answer.
The GraphRAG method and its ability to perform global sensemaking over an entire corpus form the main contribution of this work. To demonstrate this ability, we developed a novel application of the LLM-as-a-judge technique (Zheng et al., 2024) suitable for questions targeting broad issues and themes where there is no ground-truth answer. This approach first uses one LLM to generate a diverse set of global sensemaking questions based on corpus-specific use cases, before using a second LLM to judge the answers of two different RAG systems using predefined criteria (defined in Section 3.3). We use this approach to compare GraphRAG to vector RAG on two representative real-world text datasets. Results show GraphRAG strongly outperforms vector RAG when using GPT-4 as the LLM.</p>
<p>GraphRAG is available as open-source software at https://github.com/microsoft/graphrag. In addition, versions of the GraphRAG approach are also available as extensions to multiple opensource libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), NebulaGraph (NebulaGraph, 2024), and Neo4J (Neo4J, 2024).</p>
<h1>2 Background</h1>
<h3>2.1 RAG Approaches and Systems</h3>
<p>RAG generally refers to any system where a user query is used to retrieve relevant information from external data sources, whereupon this information is incorporated into the generation of a response to the query by an LLM (or other generative AI model, such as a multi-media model). The query and retrieved records populate a prompt template, which is then passed to the LLM (Ram et al., 2023). RAG is ideal when the total number of records in a data source is too large to include in a single prompt to the LLM, i.e. the amount of text in the data source exceeds the LLM's context window.
In canonical RAG approaches, the retrieval process returns a set number of records that are semantically similar to the query and the generated answer uses only the information in those retrieved records. A common approach to conventional RAG is to use text embeddings, retrieving records closest to the query in vector space where closeness corresponds to semantic similarity (Gao et al., 2023). While some RAG approaches may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG. GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire data corpus.</p>
<p>GraphRAG builds upon prior work on advanced RAG strategies. GraphRAG leverages summaries over large sections of the data source as a form of "self-memory" (described in Cheng et al. 2024), which are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Shao et al., 2023; Su et al., 2020; Trivedi et al., 2022; Wang et al., 2024). In particular, GraphRAG is similar to other approaches that use hierarchical indexing to create summaries (similar to Kim et al. 2023; Sarthi et al. 2024). GraphRAG contrasts with these approaches by generating a graph index from the source data, then applying graph-based community detection to create a thematic partitioning of the data.</p>
<h1>2.2 Using Knowledge Graphs with LLMs and RAG</h1>
<p>Approaches to knowledge graph extraction from natural language text corpora include rulematching, statistical pattern recognition, clustering, and embeddings (Etzioni et al., 2004; Kim et al., 2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of research that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022; OpenAI, 2023; Tan et al., 2017; Trajanoska et al., 2023; Yao et al., 2023; Yates et al., 2007; Zhang et al., 2024a). It also adds to a growing body of RAG approaches that use a knowledge graph as an index (Gao et al., 2023). Some techniques use subgraphs, elements of the graph, or properties of the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023) or as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other techniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time an LLM-based agent dynamically traverses a graph with nodes representing document elements (e.g., passages, tables) and edges encoding lexical and semantical similarity or structural relationships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of graphs in this context: their inherent modularity (Newman, 2006) and the ability to partition graphs into nested modular communities of closely related nodes (e.g., Louvain, Blondel et al. 2008; Leiden, Traag et al. 2019). Specifically, GraphRAG recursively creates increasingly global summaries by using the LLM to create summaries spanning this community hierarchy.</p>
<h3>2.3 Adaptive benchmarking for RAG Evaluation</h3>
<p>Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang et al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However, these benchmarks are oriented towards vector RAG performance, i.e., they evaluate performance on explicit fact retrieval. In this work, we propose an approach for generating a set of questions for evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM methods that use a corpus to generate questions whose answers would be summaries of the corpus, such as in Xu and Lapata (2021). However, in order to produce a fair evaluation, our method avoids generating the questions directly from the corpus itself (as an alternative implementation, one can use a subset of the corpus held out from subsequent graph extraction and answer evaluation steps).
Adaptive benchmarking refers to the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases. Recent work has used LLMs for adaptive benchmarking to ensure relevance, diversity, and alignment with the target application or task (Yuan et al., 2024; Zhang et al., 2024b). In this work, we propose an adaptive benchmarking approach to generating global sensemaking queries for the LLM. Our approach builds on prior work in LLM-based persona generation, where the LLM is used to generate diverse and authentic sets of personas (Kosinski, 2024; Salminen et al., 2024; Shin et al., 2024). Our adaptive benchmarking procedure uses persona generation to create queries that are representative of real-world RAG system usage. Specifically, our approach uses the LLM to infer the potential users would use the RAG system and their use cases, which guide the generation of corpus-specific sensemaking queries.</p>
<h3>2.4 RAG evaluation criteria</h3>
<p>Our evaluation relies on the LLM to evaluate how well the RAG system answers the generated questions. Prior work has shown LLMs to be good evaluators of natural language generation, including work where LLMs evaluations were competitive with human evaluations (Wang et al., 2023a; Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This graph index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset. Community detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into groups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The "global answer" to a given query is produced using a final round of query-focused summarization over all community summaries reporting relevance to that query.
generated texts such as "fluency" (Wang et al., 2023a) Some of these criteria are generic to vector RAG systems and not relevant to global sensemaking, such as "context relevance", "faithfulness", and "answer relevance" (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can quantify relative performance for a given criterion by prompting the LLM to compare generations from two different competing models (LLM-as-a-judge, (Zheng et al., 2024)). In this work, we design criteria for evaluating RAG-generated answers to global sensemaking questions and evaluate our results using the comparative approach. We also validate results using statistics derived from LLM-extracted statements of verifiable facts, or "claims."</p>
<h1>3 Methods</h1>
<h3>3.1 GraphRAG Workflow</h3>
<p>Figure 1 illustrates the high-level data flow of the GraphRAG approach and pipeline. In this section, we describe the key design parameters, techniques, and implementation details for each step.</p>
<h3>3.1.1 Source Documents $\rightarrow$ Text Chunks</h3>
<p>To start, the documents in the corpus are split into text chunks. The LLM extracts information from each chunk for downstream processing. Selecting the size of the chunk is a fundamental design decision; longer text chunks require fewer LLM calls for such extraction (which reduces cost) but suffer from degraded recall of information that appears early in the chunk (Kuratov et al., 2024; Liu et al., 2023). See Section A. 1 for prompts and examples of the recall-precision trade-offs.</p>
<h3>3.1.2 Text Chunks $\rightarrow$ Entities \&amp; Relationships</h3>
<p>In this step, the LLM is prompted to extract instances of important entities and the relationships between the entities from a given chunk. Additionally, the LLM generates short descriptions for the entities and relationships. To illustrate, suppose a chunk contained the following text:</p>
<p>NeoChip's (NC) shares surged in their first week of trading on the NewTech Exchange. However, market analysts caution that the chipmaker's public debut may not reflect trends for other technology IPOs. NeoChip, previously a private entity, was acquired by Quantum Systems in 2016. The innovative semiconductor firm specializes in low-power processors for wearables and IoT devices.</p>
<p>The LLM is prompted such that it extracts the following:</p>
<ul>
<li>The entity NeoChip, with description "NeoChip is a publicly traded company specializing in low-power processors for wearables and IoT devices."</li>
<li>The entity Quantum Systems, with description "Quantum Systems is a firm that previously owned NeoChip."</li>
<li>A relationship between NeoChip and Quantum Systems, with description "Quantum Systems owned NeoChip from 2016 until NeoChip became publicly traded."</li>
</ul>
<p>These prompts can be tailored to the domain of the document corpus by choosing domain appropriate few-shot exemplars for in-context learning (Brown et al., 2020). For example, while our default prompt extracts the broad class of "named entities" like people, places, and organizations and is generally applicable, domains with specialized knowledge (e.g., science, medicine, law) will benefit from few-shot exemplars specialized to those domains.
The LLM can also be prompted to extract claims about detected entities. Claims are important factual statements about entities, such as dates, events, and interactions with other entities. As with entities and relationships, in-context learning exemplars can provide domain-specific guidance. Claim descriptions extracted from the example tetx chunk are as follows:</p>
<ul>
<li>NeoChip's shares surged during their first week of trading on the NewTech Exchange.</li>
<li>NeoChip debuted as a publicly listed company on the NewTech Exchange.</li>
<li>Quantum Systems acquired NeoChip in 2016 and held ownership until NeoChip went public.</li>
</ul>
<p>See Appendix A for prompts and details on our implementation of entity and claim extraction.</p>
<h1>3.1.3 Entities \&amp; Relationships $\rightarrow$ Knowledge Graph</h1>
<p>The use of an LLM to extract entities, relationships, and claims is a form of abstractive summarization - these are meaningful summaries of concepts that, in the case of relationships and claims, may not be explicitly stated in the text. The entity/relationship/claim extraction processes creates multiple instances of a single element because an element is typically detected and extracted multiple times across documents.
In the final step of the knowledge graph extraction process, these instances of entities and relationships become individual nodes and edges in the graph. Entity descriptions are aggregated and summarized for each node and edge. Relationships are aggregated into graph edges, where the number of duplicates for a given relationship becomes edge weights. Claims are aggregated similarly.
In this manuscript, our analysis uses exact string matching for entity matching - the task of reconciling different extracted names for the same entity (Barlaug and Gulla, 2021; Christen and Christen, 2012; Elmagarmid et al., 2006). However, softer matching approaches can be used with minor adjustments to prompts or code. Furthermore, GraphRAG is generally resilient to duplicate entities since duplicates are typically clustered together for summarization in subsequent steps.</p>
<h3>3.1.4 Knowledge Graph $\rightarrow$ Graph Communities</h3>
<p>Given the graph index created in the previous step, a variety of community detection algorithms may be used to partition the graph into communities of strongly connected nodes (e.g., see the surveys by Fortunato (2010) and Jin et al. (2021)). In our pipeline, we use Leiden community detection (Traag et al., 2019) in a hierarchical manner, recursively detecting sub-communities within each detected community until reaching leaf communities that can no longer be partitioned.</p>
<p>Each level of this hierarchy provides a community partition that covers the nodes of the graph in a mutually exclusive, collectively exhaustive way, enabling divide-and-conquer global summarization. An illustration of such hierarchical partitioning on an example dataset can be found in Appendix B.</p>
<h1>3.1.5 Graph Communities $\rightarrow$ Community Summaries</h1>
<p>The next step creates report-like summaries of each community in the community hierarchy, using a method designed to scale to very large datasets. These summaries are independently useful as a way to understand the global structure and semantics of the dataset, and may themselves be used to make sense of a corpus in the absence of a specific query. For example, a user may scan through community summaries at one level looking for general themes of interest, then read linked reports at a lower level that provide additional details for each subtopic. Here, however, we focus on their utility as part of a graph-based index used for answering global queries.
GraphRAG generates community summaries by adding various element summaries (for nodes, edges, and related claims) to a community summary template. Community summaries from lowerlevel communities are used to generate summaries for higher-level communities as follows:</p>
<ul>
<li>Leaf-level communities. The element summaries of a leaf-level community are prioritized and then iteratively added to the LLM context window until the token limit is reached. The prioritization is as follows: for each community edge in decreasing order of combined source and target node degree (i.e., overall prominence), add descriptions of the source node, target node, the edge itself, and related claims.</li>
<li>Higher-level communities. If all element summaries fit within the token limit of the context window, proceed as for leaf-level communities and summarize all element summaries within the community. Otherwise, rank sub-communities in decreasing order of element summary tokens and iteratively substitute sub-community summaries (shorter) for their associated element summaries (longer) until they fit within the context window.</li>
</ul>
<h3>3.1.6 Community Summaries $\rightarrow$ Community Answers $\rightarrow$ Global Answer</h3>
<p>Given a user query, the community summaries generated in the previous step can be used to generate a final answer in a multi-stage process. The hierarchical nature of the community structure also means that questions can be answered using the community summaries from different levels, raising the question of whether a particular level in the hierarchical community structure offers the best balance of summary detail and scope for general sensemaking questions (evaluated in section 4).
For a given community level, the global answer to any user query is generated as follows:</p>
<ul>
<li>Prepare community summaries. Community summaries are randomly shuffled and divided into chunks of pre-specified token size. This ensures relevant information is distributed across chunks, rather than concentrated (and potentially lost) in a single context window.</li>
<li>Map community answers. Intermediate answers are generated in parallel. The LLM is also asked to generate a score between $0-100$ indicating how helpful the generated answer is in answering the target question. Answers with score 0 are filtered out.</li>
<li>Reduce to global answer. Intermediate community answers are sorted in descending order of helpfulness score and iteratively added into a new context window until the token limit is reached. This final context is used to generate the global answer returned to the user.</li>
</ul>
<h3>3.2 Global Sensemaking Question Generation</h3>
<p>To evaluate the effectiveness of RAG systems for global sensemaking tasks, we use an LLM to generate a set of corpus-specific questions designed to asses high-level understanding of a given corpus, without requiring retrieval of specific low-level facts. Instead, given a high-level description of a corpus and its purposes, the LLM is prompted to generate personas of hypothetical users of the RAG system. For each hypothetical user, the LLM is then prompted to specify tasks that this user would use the RAG system to complete. Finally, for each combination of user and task, the LLM is prompted to generate questions that require understanding of the entire corpus. Algorithm 1 describes the approach.</p>
<h1>Algorithm 1: Prompting Procedure for Question Generation</h1>
<p>1: Input: Description of a corpus, number of users $K$, number of tasks per user $N$, number of questions per (user, task) combination $M$.
2: Output: A set of $K * N * M$ high-level questions requiring global understanding of the corpus.
3: procedure GenerateQuestions
4: Based on the corpus description, prompt the LLM to:</p>
<ol>
<li>Describe personas of $K$ potential users of the dataset.</li>
<li>For each user, identify $N$ tasks relevant to the user.</li>
<li>
<p>Specific to each user \&amp; task pair, generate $M$ high-level questions that:</p>
</li>
<li>
<p>Require understanding of the entire corpus.</p>
</li>
<li>Do not require retrieval of specific low-level facts.</li>
</ol>
<p>5: Collect the generated questions to produce $K * N * M$ test questions for the dataset.
6: end procedure</p>
<p>For our evaluation, we set $K=M=N=5$ for a total of 125 test questions per dataset. Table 1 shows example questions for each of the two evaluation datasets.</p>
<h3>3.3 Criteria for Evaluating Global Sensemaking</h3>
<p>Given the lack of gold standard answers to our activity-based sensemaking questions, we adopt the head-to-head comparison approach using an LLM evaluator that judges relative performance according to specific criteria. We designed three target criteria capturing qualities that are desirable for global sensemaking activities.
Appendix F shows the prompts for our head-to-head measures computed using an LLM evaluator, summarized as:</p>
<ul>
<li>Comprehensiveness. How much detail does the answer provide to cover all aspects and details of the question?</li>
<li>Diversity. How varied and rich is the answer in providing different perspectives and insights on the question?</li>
<li>Empowerment. How well does the answer help the reader understand and make informed judgments about the topic?</li>
</ul>
<p>Table 1: Examples of potential users, tasks, and questions generated by the LLM based on short descriptions of the target datasets. Questions target global understanding rather than specific details.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Example activity framing and generation of global sensemaking questions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Podcast</td>
<td style="text-align: left;">User: A tech journalist looking for insights and trends in the tech industry <br> transcripts <br> Task: Understanding how tech leaders view the role of policy and regulation <br> Questions: <br> 1. Which episodes deal primarily with tech policy and government regulation? <br> 2. How do guests perceive the impact of privacy laws on technology development? <br> 3. Do any guests discuss the balance between innovation and ethical considerations? <br> 4. What are the suggested changes to current policies mentioned by the guests? <br> 5. Are collaborations between tech companies and governments discussed and how?</td>
</tr>
<tr>
<td style="text-align: left;">News</td>
<td style="text-align: left;">User: Educator incorporating current affairs into curricula <br> Task: Teaching about health and wellness <br> Questions: <br> 1. What current topics in health can be integrated into health education curricula? <br> 2. How do news articles address the concepts of preventive medicine and wellness? <br> 3. Are there examples of health articles that contradict each other, and if so, why? <br> 4. What insights can be gleaned about public health priorities based on news coverage? <br> 5. How can educators use the dataset to highlight the importance of health literacy?</td>
</tr>
</tbody>
</table>
<p>Furthermore, we use a "control criterion" called Directness that answers "How specifically and clearly does the answer address the question?". In plain terms, directness evaluates the concision of an answer in a generic sense that applies to any generated LLM summarization. We include it to behave as a reference against which we can judge the soundness of results for the other criteria. Since directness is effectively in opposition to comprehensiveness and diversity, we would not expect any method to win across all four criteria.</p>
<p>In our evaluations, the LLM is provided with the question, the generated answers from two competing systems, and prompted to compare the two answers according to the criterion before giving a final judgment of which answer is preferred. The LLM either indicates a winner; or, it returns a tie if they are fundamentally similar. To account for the inherent stochasticity of LLM generation, we run each comparison with multiple replicates and average the results across replicates and questions. An illustration of LLM assessment for answers to a sample question can be found in Appendix D.</p>
<h1>4 Analysis</h1>
<h3>4.1 Experiment 1</h3>
<h3>4.1.1 Datasets</h3>
<p>We selected two datasets in the one million token range, each representative of corpora that users may encounter in their real-world activities:</p>
<p>Podcast transcripts. Public transcripts of Behind the Tech with Kevin Scott, a podcast featuring conversations between Microsoft CTO Kevin Scott and various thought leaders in science and technology (Scott, 2024). This corpus was divided into $1669 \times 600$-token text chunks, with 100 -token overlaps between chunks ( $\sim 1$ million tokens).</p>
<p>News articles. A benchmark dataset comprised of news articles published from September 2013 to December 2023 in a range of categories, including entertainment, business, sports, technology, health, and science (Tang and Yang, 2024). The corpus is divided into $3197 \times 600$-token text chunks, with 100 -token overlaps between chunks ( $\sim 1.7$ million tokens).</p>
<h3>4.1.2 Conditions</h3>
<p>We compared six conditions including GraphRAG at four different graph community levels (C0, C1, C2, C3), a text summarization method that applies our map-reduce approach directly to source texts (TS), and a vector RAG "semantic search" approach (SS):</p>
<ul>
<li>CO. Uses root-level community summaries (fewest in number) to answer user queries.</li>
<li>C1. Uses high-level community summaries to answer queries. These are sub-communities of C0, if present, otherwise C0 communities projected downwards.</li>
<li>C2. Uses intermediate-level community summaries to answer queries. These are subcommunities of C 1 , if present, otherwise C 1 communities projected downwards.</li>
<li>C3. Uses low-level community summaries (greatest in number) to answer queries. These are sub-communities of C2, if present, otherwise C2 communities projected downwards.</li>
<li>TS. The same method as in Section 3.1.6, except source texts (rather than community summaries) are shuffled and chunked for the map-reduce summarization stages.</li>
<li>SS. An implementation of vector RAG in which text chunks are retrieved and added to the available context window until the specified token limit is reached.</li>
</ul>
<p>The size of the context window and the prompts used for answer generation are the same across all six conditions (except for minor modifications to reference styles to match the types of context information used). Conditions only differ in how the contents of the context window are created.</p>
<p>The graph index supporting conditions $\mathbf{C 0}-\mathbf{C 3}$ was created using our generic prompts for entity and relationship extraction, with entity types and few-shot examples tailored to the domain of the data.</p>
<h1>4.1.3 Configuration</h1>
<p>We used a fixed context window size of 8 k tokens for generating community summaries, community answers, and global answers (explained in Appendix C). Graph indexing with a 600 token window (explained in Section A.2) took 281 minutes for the Podcast dataset, running on a virtual machine (16GB RAM, Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz) and using a public OpenAI endpoint for gpt-4-turbo (2M TPM, 10k RPM).
We implemented Leiden community detection using the graspologic library (Chung et al., 2019). The prompts used to generate the graph index and global answers can be found in Appendix E, while the prompts used to evaluate LLM responses against our criteria can be found in Appendix F. A full statistical analysis of the results presented in the next section can be found in Appendix G.</p>
<h3>4.2 Experiment 2</h3>
<p>To validate the comprehensiveness and diversity results from Experiment 1, we implemented claimbased measures of these qualities. We use the definition of a factual claim from Ni et al. (2024), which is "a statement that explicitly presents some verifiable facts." For example, the sentence "California and New York implemented incentives for renewable energy adoption, highlighting the broader importance of sustainability in policy decisions" contains two factual claims: (1) California implemented incentives for renewable energy adoption, and (2) New York implemented incentives for renewable energy adoption.
To extract factual claims, we used Claimify (Metropolitansky and Larson, 2025), an LLM-based method that identifies sentences in an answer containing at least one factual claim, then decomposes these sentences into simple, self-contained factual claims. We applied Claimify to the answers generated under the conditions from Experiment 1. After removing duplicate claims from each answer, we extracted 47,075 unique claims, with an average of 31 claims per answer.
We defined two metrics, with higher values indicating better performance:</p>
<ol>
<li>Comprehensiveness: Measured as the average number of claims extracted from the answers generated under each condition.</li>
<li>Diversity: Measured by clustering the claims for each answer and calculating the average number of clusters.</li>
</ol>
<p>For clustering, we followed the approach described by Padmakumar and He (2024), which involved using Scikit-learn's implementation of agglomerative clustering (Pedregosa et al., 2011). Clusters were merged through "complete" linkage, meaning they were combined only if the maximum distance between their farthest points was less than or equal to a predefined distance threshold. The distance metric used was $1-$ ROUGE-L. Since the distance threshold influences the number of clusters, we report results across a range of thresholds.</p>
<h2>5 Results</h2>
<h3>5.1 Experiment 1</h3>
<p>The indexing process resulted in a graph consisting of 8,564 nodes and 20,691 edges for the Podcast dataset, and a larger graph of 15,754 nodes and 19,520 edges for the News dataset. Table 2 shows the number of community summaries at different levels of each graph community hierarchy.
Global approaches vs. vector RAG. As shown in Figure 2 and Table 6, global approaches significantly outperformed conventional vector RAG (SS) in both comprehensiveness and diversity criteria across datasets. Specifically, global approaches achieved comprehensiveness win rates between 72$83 \%$ ( $\mathrm{p}&lt;.001$ ) for Podcast transcripts and $72-80 \%$ ( $\mathrm{p}&lt;.001$ ) for News articles, while diversity win rates ranged from $75-82 \%$ ( $\mathrm{p}&lt;.001$ ) and $62-71 \%$ ( $\mathrm{p}&lt;.01$ ) respectively. Our use of directness as a validity test confirmed that vector RAG produces the most direct responses across all comparisons.
Empowerment. Empowerment comparisons showed mixed results for both global approaches versus vector RAG (SS) and GraphRAG approaches versus source text summarization (TS). Using an LLM to analyze LLM reasoning for this measure indicated that the ability to provide specific exam-</p>
<h1>Podcast transcripts</h1>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Head-to-head win rate percentages of (row condition) over (column condition) across two datasets, four metrics, and 125 questions per comparison (each repeated five times and averaged). The overall winner per dataset and metric is shown in bold. Self-win rates were not computed but are shown as the expected $50 \%$ for reference. All Graph RAG conditions outperformed naïve RAG on comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer comprehensiveness and diversity over TS (global text summarization without a graph index).</p>
<p>Table 2: Number of context units (community summaries for C0-C3 and text chunks for TS), corresponding token counts, and percentage of the maximum token count. Map-reduce summarization of source texts is the most resource-intensive approach requiring the highest number of context tokens. Root-level community summaries $(\mathbf{C 0})$ require dramatically fewer tokens per query ( $9 \mathrm{x}-43 \mathrm{x}$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Podcast Transcripts</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">News Articles</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">C0</td>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">C2</td>
<td style="text-align: center;">C3</td>
<td style="text-align: center;">TS</td>
<td style="text-align: center;">C0</td>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">C2</td>
<td style="text-align: center;">C3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SS</td>
<td style="text-align: center;">TS</td>
<td style="text-align: center;">C0</td>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">C2 C3</td>
</tr>
<tr>
<td style="text-align: center;">Units</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">367</td>
<td style="text-align: center;">969</td>
<td style="text-align: center;">1310</td>
<td style="text-align: center;">1669</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">555</td>
<td style="text-align: center;">1797</td>
<td style="text-align: center;">2142</td>
<td style="text-align: center;">3197</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Tokens</td>
<td style="text-align: center;">26657</td>
<td style="text-align: center;">225756</td>
<td style="text-align: center;">565720</td>
<td style="text-align: center;">746100</td>
<td style="text-align: center;">1014611</td>
<td style="text-align: center;">39770</td>
<td style="text-align: center;">352641</td>
<td style="text-align: center;">980898</td>
<td style="text-align: center;">1140266</td>
<td style="text-align: center;">1707694</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">\% Max</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>ples, quotes, and citations was judged to be key to helping users reach an informed understanding. Tuning element extraction prompts may help to retain more of these details in the GraphRAG index.
Community summaries vs. source texts. When comparing community summaries to source texts using GraphRAG, community summaries generally provided a small but consistent improvement in answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level summaries in the Podcast dataset and low-level community summaries in the News dataset achieved comprehensiveness win rates of $57 \%$ ( $\mathrm{p}&lt;.001$ ) and $64 \%$ ( $\mathrm{p}&lt;.001$ ), respectively. Diversity win rates were $57 \%(\mathrm{p}=.036)$ for Podcast intermediate-level summaries and $60 \%(\mathrm{p}&lt;.001)$ for News low-level community summaries. Table 2 also illustrates the scalability advantages of GraphRAG compared to source text summarization: for low-level community summaries (C3), GraphRAG required 26$33 \%$ fewer context tokens, while for root-level community summaries (C0), it required over $97 \%$ fewer tokens. For a modest drop in performance compared with other global methods, root-level GraphRAG offers a highly efficient method for the iterative question answering that characterizes sensemaking activity, while retaining advantages in comprehensiveness ( $72 \%$ win rate) and diversity ( $62 \%$ win rate) over vector RAG.</p>
<p>Table 3: Average number of extracted claims, reported by condition and dataset type. Bolded values represent the highest score in each column.</p>
<table>
<thead>
<tr>
<th>Condition</th>
<th>Average Number of Claims</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>News Articles</td>
<td>Podcast Transcripts</td>
</tr>
<tr>
<td>C0</td>
<td>$\mathbf{3 4 . 1 8}$</td>
<td>32.21</td>
</tr>
<tr>
<td>C1</td>
<td>32.50</td>
<td>32.20</td>
</tr>
<tr>
<td>C2</td>
<td>31.62</td>
<td>$\mathbf{3 2 . 4 6}$</td>
</tr>
<tr>
<td>C3</td>
<td>33.14</td>
<td>32.28</td>
</tr>
<tr>
<td>TS</td>
<td>32.89</td>
<td>31.39</td>
</tr>
<tr>
<td>SS</td>
<td>25.23</td>
<td>26.50</td>
</tr>
</tbody>
</table>
<h1>5.2 Experiment 2</h1>
<p>Table 3 shows the results for the average number of extracted claims (i.e., the claim-based measure of comprehensiveness) per condition. For both the News and Podcast datasets, all global search conditions (C0-C3) and source text summarization (TS) had greater comprehensiveness than vector RAG (SS). The differences were statistically significant ( $\mathrm{p}&lt;.05$ ) in all cases. These findings align with the LLM-based win rates from Experiment 1.</p>
<p>Table 4 contains the results for the average number of clusters, the claim-based measure of diversity. For the Podcast dataset, all global search conditions had significantly greater diversity than SS across all distance thresholds ( $\mathrm{p}&lt;.05$ ), consistent with the win rates observed in Experiment 1. For the News dataset, however, only $\mathbf{C 0}$ significantly outperformed $\mathbf{S S}$ across all distance thresholds ( $\mathrm{p}&lt;.05$ ). While $\mathbf{C 1}-\mathbf{C 3}$ also achieved higher average cluster counts than $\mathbf{S S}$, the differences were statistically significant only at certain distance thresholds. In Experiment 1, all global search conditions significantly outperformed $\mathbf{S S}$ in the News dataset - not just $\mathbf{C 0}$. However, the differences in mean diversity scores between $\mathbf{S S}$ and the global search conditions were smaller for the News dataset than for the Podcast dataset, aligning directionally with the claim-based results.</p>
<p>For both comprehensiveness and diversity, across both datasets, there were no statistically significant differences observed among the global search conditions or between global search and TS.</p>
<p>Finally, for each pairwise comparison in Experiment 1, we tested whether the answer preferred by the LLM aligned with the winner based on the claim-based metrics. Since each pairwise comparison in Experiment 1 was performed five times, while the claim-based metrics provided only one outcome per comparison, we aggregated the Experiment 1 results into a single label using majority voting. For example, if $\mathbf{C 0}$ won over $\mathbf{S S}$ in three out of five judgments for comprehensiveness on a given question, $\mathbf{C 0}$ was labeled the winner and $\mathbf{S S}$ the loser. However, if $\mathbf{C 0}$ won twice, $\mathbf{S S}$ won once, and they tied twice, then there was no majority outcome, so the final label was a tie.</p>
<p>We found that exact ties were rare for the claim-based metrics. One possible solution is to define a tie based on a threshold (e.g., the absolute difference between the claim-based results for condition A and condition B must be less than or equal to $x$ ). However, we observed that the results were sensitive to the choice of threshold. As a result, we focused on cases where the aggregated LLM label was not a tie, representing $33 \%$ and $39 \%$ of pairwise comparisons for comprehensiveness and diversity, respectively. In these cases, the aggregated LLM label matched the claim-based label in $78 \%$ of pairwise comparisons for comprehensiveness and $69-70 \%$ for diversity (across all distance thresholds), indicating moderately strong alignment.</p>
<h2>6 Discussion</h2>
<h3>6.1 Limitations of evaluation approach</h3>
<p>Our evaluation to date has focused on sensemaking questions specific to two corpora each containing approximately 1 million tokens. More work is needed to understand how performance generalizes to datasets from various domains with different use cases. Comparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023), would also strengthen the current analysis.</p>
<p>Table 4: Average number of clusters across different distance thresholds, reported by condition and dataset type. Bolded values represent the highest score in each row.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Distance Threshold</th>
<th style="text-align: center;">Average Number of Clusters</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C0</td>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">C2</td>
<td style="text-align: center;">C3</td>
<td style="text-align: center;">TS</td>
<td style="text-align: center;">SS</td>
</tr>
<tr>
<td style="text-align: left;">News Articles</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">$\mathbf{2 3 . 4 2}$</td>
<td style="text-align: center;">21.85</td>
<td style="text-align: center;">21.90</td>
<td style="text-align: center;">22.13</td>
<td style="text-align: center;">21.80</td>
<td style="text-align: center;">17.92</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">$\mathbf{2 1 . 6 5}$</td>
<td style="text-align: center;">20.38</td>
<td style="text-align: center;">20.30</td>
<td style="text-align: center;">20.52</td>
<td style="text-align: center;">20.13</td>
<td style="text-align: center;">16.78</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">$\mathbf{2 0 . 1 9}$</td>
<td style="text-align: center;">19.06</td>
<td style="text-align: center;">19.03</td>
<td style="text-align: center;">19.13</td>
<td style="text-align: center;">18.62</td>
<td style="text-align: center;">15.80</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">$\mathbf{1 8 . 8 6}$</td>
<td style="text-align: center;">17.78</td>
<td style="text-align: center;">17.82</td>
<td style="text-align: center;">17.79</td>
<td style="text-align: center;">17.30</td>
<td style="text-align: center;">14.80</td>
</tr>
<tr>
<td style="text-align: left;">Podcast Transcripts</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">$\mathbf{2 3 . 1 6}$</td>
<td style="text-align: center;">22.62</td>
<td style="text-align: center;">22.52</td>
<td style="text-align: center;">21.93</td>
<td style="text-align: center;">21.14</td>
<td style="text-align: center;">18.55</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">$\mathbf{2 1 . 6 5}$</td>
<td style="text-align: center;">21.33</td>
<td style="text-align: center;">21.21</td>
<td style="text-align: center;">20.62</td>
<td style="text-align: center;">19.70</td>
<td style="text-align: center;">17.39</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">$\mathbf{2 0 . 4 1}$</td>
<td style="text-align: center;">20.04</td>
<td style="text-align: center;">19.79</td>
<td style="text-align: center;">19.22</td>
<td style="text-align: center;">18.08</td>
<td style="text-align: center;">16.28</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">$\mathbf{1 9 . 2 6}$</td>
<td style="text-align: center;">18.77</td>
<td style="text-align: center;">18.46</td>
<td style="text-align: center;">17.89</td>
<td style="text-align: center;">16.66</td>
<td style="text-align: center;">15.07</td>
</tr>
</tbody>
</table>
<h1>6.2 Future work</h1>
<p>The graph index, rich text annotations, and hierarchical community structure supporting the current GraphRAG approach offer many possibilities for refinement and adaptation. This includes RAG approaches that operate in a more local manner, via embedding-based matching of user queries and graph annotations. In particular, we see potential in hybrid RAG schemes that combine embeddingbased matching with just-in-time community report generation before employing our map-reduce summarization mechanisms. This "roll-up" approach could also be extended across multiple levels of the community hierarchy, as well as implemented as a more exploratory "drill down" mechanism that follows the information scent contained in higher-level community summaries.
Broader impacts. As a mechanism for question answering over large document collections, there are risks to downstream sensemaking and decision-making tasks if the generated answers do not accurately represent the source data. System use should be accompanied by clear disclosures of AI use and the potential for errors in outputs. Compared to vector RAG, however, GraphRAG shows promise as a way to mitigate these downstream risks for questions of a global nature, which might otherwise be answered by samples of retrieved facts falsely presented as global summaries.</p>
<h2>7 Conclusion</h2>
<p>We have presented GraphRAG, a RAG approach that combines knowledge graph generation and query-focused summarization (QFS) to support human sensemaking over entire text corpora. Initial evaluations show substantial improvements over a vector RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable comparisons to a global but graph-free approach using map-reduce source text summarization. For situations requiring many global queries over the same dataset, summaries of root-level communities in the entity-based graph index provide a data index that is both superior to vector RAG and achieves competitive performance to other global methods at a fraction of the token cost.</p>
<h2>Acknowledgements</h2>
<p>We would also like to thank the following people who contributed to the work: Alonso Guevara Fernández, Amber Hoak, Andrés Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez, Chris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed Clark, Gabriel Nieves-Ponce, Gaudy Blanco Meneses, Kate Lytvynets, Katy Smith, Mónica Carvajal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.</p>
<h2>References</h2>
<p>Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>Baek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136.</p>
<p>Ban, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing large language models for advanced causal discovery from data.</p>
<p>Barlaug, N. and Gulla, J. A. (2021). Neural networks for entity matching: A survey. ACM Transactions on Knowledge Discovery from Data (TKDD), 15(3):1-37.</p>
<p>Baumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorporating query relevance, multi-document coverage, and summary length constraints into seq2seq models. arXiv preprint arXiv:1801.07704.</p>
<p>Blondel, V. D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Cheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrievalaugmented text generation with self-memory. Advances in Neural Information Processing Systems, 36 .</p>
<p>Christen, P. and Christen, P. (2012). The data matching process. Springer.
Chung, J., Pedigo, B. D., Bridgeford, E. W., Varjavand, B. K., Helm, H. S., and Vogelstein, J. T. (2019). Graspy: Graph statistics in python. Journal of Machine Learning Research, 20(158):1-7.</p>
<p>Dang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. In Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 48-55.</p>
<p>Elmagarmid, A. K., Ipeirotis, P. G., and Verykios, V. S. (2006). Duplicate record detection: A survey. IEEE Transactions on knowledge and data engineering, 19(1):1-16.</p>
<p>Es, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217.</p>
<p>Etzioni, O., Cafarella, M., Downey, D., Kok, S., Popescu, A.-M., Shaked, T., Soderland, S., Weld, D. S., and Yates, A. (2004). Web-scale information extraction in knowitall: (preliminary results). In Proceedings of the 13th International Conference on World Wide Web, WWW '04, page 100-110, New York, NY, USA. Association for Computing Machinery.</p>
<p>Feng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented large language models. arXiv preprint arXiv:2310.05149.</p>
<p>Fortunato, S. (2010). Community detection in graphs. Physics reports, 486(3-5):75-174.
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H. (2023). Retrievalaugmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.</p>
<p>He, X., Tian, Y., Sun, Y., Chawla, N. V., Laurent, T., LeCun, Y., Bresson, X., and Hooi, B. (2024). G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. arXiv preprint arXiv:2402.07630.</p>
<p>Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. (2023). Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798.</p>
<p>Jacomy, M., Venturini, T., Heymann, S., and Bastian, M. (2014). Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software. PLoS ONE 9(6): e98679. https://doi.org/10.1371/journal.pone. 0098679.</p>
<p>Jin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y., and Zhang, W. (2021). A survey of community detection approaches: From statistical modeling to deep learning. IEEE Transactions on Knowledge and Data Engineering, 35(2):1149-1170.</p>
<p>Kang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language models for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846.</p>
<p>Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022). Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024.</p>
<p>Kim, D., Xie, L., and Ong, C. S. (2016). Probabilistic knowledge graph construction: Compositional and incremental approaches. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM '16, page 2257-2262, New York, NY, USA. Association for Computing Machinery.</p>
<p>Kim, G., Kim, S., Jeon, B., Park, J., and Kang, J. (2023). Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models. arXiv preprint arXiv:2310.14696.</p>
<p>Klein, G., Moon, B., and Hoffman, R. R. (2006). Making sense of sensemaking 1: Alternative perspectives. IEEE intelligent systems, 21(4):70-73.</p>
<p>Kosinski, M. (2024). Evaluating large language models in theory of mind tasks. Proceedings of the National Academy of Sciences, 121(45):e2405460121.</p>
<p>Kuratov, Y., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search of needles in a 11 m haystack: Recurrent memory finds what llms miss.</p>
<p>LangChain (2024). Langchain graphs. https://langchain-graphrag.readthedocs.io/en/latest/.
Laskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models. In Advances in Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON, Canada, May 13-15, 2020, Proceedings 33, pages 342-348. Springer.</p>
<p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost in the middle: How language models use long contexts. arXiv:2307.03172.</p>
<p>LlamaIndex (2024). GraphRAG Implementation with LlamaIndex - V2. https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v2.ipynb.</p>
<p>Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. (2024). Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36.</p>
<p>Manakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896.</p>
<p>Mao, Y., He, P., Liu, X., Shen, Y., Gao, J., Han, J., and Chen, W. (2020). Generation-augmented retrieval for open-domain question answering. arXiv preprint arXiv:2009.08553.</p>
<p>Martin, S., Brown, W. M., Klavans, R., and Boyack, K. (2011). Openord: An open-source toolbox for large graph layout. SPIE Conference on Visualization and Data Analysis (VDA).</p>
<p>Melnyk, I., Dognin, P., and Das, P. (2022). Knowledge graph generation from text.</p>
<p>Metropolitansky, D. and Larson, J. (2025). Towards effective extraction and evaluation of factual claims.</p>
<p>Microsoft (2023). The impact of large language models on scientific discovery: a preliminary study using gpt-4.</p>
<p>Mooney, R. J. and Bunescu, R. (2005). Mining knowledge from text using information extraction. SIGKDD Explor. Newsl., 7(1):3-10.</p>
<p>NebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented generation with llm based on knowledge graphs. https://www.nebula-graph.io/posts/graph-RAG.</p>
<p>Neo4J (2024). Get started with graphrag: Neo4j's ecosystem tools. https://neo4j.com/developer-blog/graphrag-ecosystem-tools/.</p>
<p>Newman, M. E. (2006). Modularity and community structure in networks. Proceedings of the national academy of sciences, 103(23):8577-8582.</p>
<p>Ni, J., Shi, M., Stammbach, D., Sachan, M., Ash, E., and Leippold, M. (2024). AFaCTA: Assisting the annotation of factual claim detection with reliable LLM annotators. In Ku, L.-W., Martins, A., and Srikumar, V., editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1890-1912, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>OpenAI (2023). Chatgpt: Gpt-4 language model.
Padmakumar, V. and He, H. (2024). Does writing with language models reduce content diversity? $I C L R$.</p>
<p>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12:2825-2830.</p>
<p>Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y. (2023). In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316-1331.</p>
<p>Ranade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented narrative construction. arXiv preprint arXiv:2310.13848.</p>
<p>Salminen, J., Liu, C., Pian, W., Chi, J., Häyhänen, E., and Jansen, B. J. (2024). Deus ex machina and personas from large language models: Investigating the composition of ai-generated persona descriptions. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages $1-20$.</p>
<p>Sarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.</p>
<p>Scott, K. (2024). Behind the Tech. https://www.microsoft.com/en-us/behind-the-tech.
Shao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrievalaugmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294.</p>
<p>Shin, J., Hedderich, M. A., Rey, B. J., Lucero, A., and Oulasvirta, A. (2024). Understanding humanai workflows for generating personas. In Proceedings of the 2024 ACM Designing Interactive Systems Conference, pages 757-781.</p>
<p>Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. (2024). Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36 .</p>
<p>Su, D., Xu, Y., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management. arXiv preprint arXiv:2005.03975.</p>
<p>Tan, Z., Zhao, X., and Wang, W. (2017). Representation learning of large-scale knowledge graphs via entity feature combinations. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM '17, page 1777-1786, New York, NY, USA. Association for Computing Machinery.</p>
<p>Tang, Y. and Yang, Y. (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries. arXiv preprint arXiv:2401.15391.</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Traag, V. A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing well-connected communities. Scientific Reports, 9(1).</p>
<p>Trajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction using large language models. ArXiv, abs/2305.04676.</p>
<p>Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509.</p>
<p>Wang, J., Liang, Y., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J. (2023a). Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.</p>
<p>Wang, S., Khramtsova, E., Zhuang, S., and Zuccon, G. (2024). Feb4rag: Evaluating federated search in the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. (2022). Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Wang, Y., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. (2023b). Knowledge graph prompting for multi-document question answering.</p>
<p>Xu, Y. and Lapata, M. (2021). Text summarization with latent queries. arXiv preprint arXiv:2106.00104.</p>
<p>Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018). HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Yao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge and Information Systems, 53:297-336.</p>
<p>Yao, L., Peng, J., Mao, C., and Luo, Y. (2023). Exploring large language models for knowledge graph completion.</p>
<p>Yates, A., Banko, M., Broadhead, M., Cafarella, M., Etzioni, O., and Soderland, S. (2007). TextRunner: Open information extraction on the web. In Carpenter, B., Stent, A., and Williams, J. D., editors, Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 25-26, Rochester, New York, USA. Association for Computational Linguistics.</p>
<p>Yuan, X., Li, J., Wang, D., Chen, Y., Mao, X., Huang, L., Xue, H., Wang, W., Ren, K., and Wang, J. (2024). S-eval: Automatic and adaptive test generation for benchmarking safety evaluation of large language models. arXiv preprint arXiv:2405.14191.</p>
<p>Zhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. arXiv preprint arXiv:2304.11116.</p>
<p>Zhang, Y., Zhang, Y., Gan, Y., Yao, L., and Wang, C. (2024a). Causal graph discovery with retrievalaugmented generation based large language models. arXiv preprint arXiv:2402.15301.</p>
<p>Zhang, Z., Chen, J., and Yang, D. (2024b). Darg: Dynamic evaluation of large language models via adaptive reasoning graph. arXiv preprint arXiv:2406.17271.</p>
<p>Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36.</p>
<p>Zhu, Y., Wang, X., Chen, J., Qiao, S., Ou, Y., Yao, Y., Deng, S., Chen, H., and Zhang, N. (2024). Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities.</p>
<h1>A Entity and Relationship Extraction Approach</h1>
<p>The following prompts, designed for GPT-4, are used in the default GraphRAG initialization pipeline:</p>
<ul>
<li>Default Graph Extraction Prompt</li>
<li>Claim Extraction Prompt</li>
</ul>
<h2>A. 1 Entity Extraction</h2>
<p>We do this using a multipart LLM prompt that first identifies all entities in the text, including their name, type, and description, before identifying all relationships between clearly related entities, including the source and target entities and a description of their relationship. Both kinds of element instance are output in a single list of delimited tuples.</p>
<h2>A. 2 Self-Reflection</h2>
<p>The choice of prompt engineering techniques has a strong impact on the quality of knowledge graph extraction (Zhu et al., 2024), and different techniques have different costs in terms of tokens consumed and generated by the model. Self-reflection is a prompt engineering technique where the LLM generates an answer, and is then prompted to evaluate its output for correctness, clarity, or completeness, then finally generate an improved response based on that evaluation (Huang et al., 2023; Madaan et al., 2024; Shinn et al., 2024; Wang et al., 2022). We leverage self-reflection in knowledge graph extraction, and explore ways how removing self-reflection affects performance and cost.</p>
<p>Using larger chunk size is less costly in terms of calls to the LLM. However, the LLM tends to extract few entities from chunks of larger size. For example, in a sample dataset (HotPotQA, Yang et al., 2018), GPT-4 extracted almost twice as many entity references when the chunk size was 600 tokens than when it was 2400 . To address this issue, we deploy a self-reflection prompt engineering approach. After entities are extracted from a chunk, we provide the extracted entities back to the LLM, prompting it to "glean" any entities that it may have missed. This is a multi-stage process in which we first ask the LLM to assess whether all entities were extracted, using a logit bias of 100 to force a yes/no decision. If the LLM responds that entities were missed, then a continuation indicating that "MANY entities were missed in the last extraction" encourages the LLM to detect these missing entities. This approach allows us to use larger chunk sizes without a drop in quality (Figure 3) or the forced introduction of noise. We interate self-reflection steps up to a specified maximum number of times.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: How the entity references detected in the HotPotQA dataset (Yang et al., 2018) varies with chunk size and self-reflection iterations for our generic entity extraction prompt with gpt-4-turbo.</p>
<h2>B Example Community Detection</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Graph communities detected using the Leiden algorithm (Traag et al., 2019) over the MultiHop-RAG (Tang and Yang, 2024) dataset as indexed. Circles represent entity nodes with size proportional to their degree. Node layout was performed via OpenORD (Martin et al., 2011) and Force Atlas 2 (Jacomy et al., 2014). Node colors represent entity communities, shown at two levels of hierarchical clustering: (a) Level 0, corresponding to the hierarchical partition with maximum modularity, and (b) Level 1, which reveals internal structure within these root-level communities.</p>
<h1>C Context Window Selection</h1>
<p>The effect of context window size on any particular task is unclear, especially for models like gpt-4-turbo with a large context size of 128 k tokens. Given the potential for information to be "lost in the middle" of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to explore the effects of varying the context window size for our combinations of datasets, questions, and metrics. In particular, our goal was to determine the optimum context size for our baseline condition (SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context window sizes: $8 \mathrm{k}, 16 \mathrm{k}, 32 \mathrm{k}$ and 64 k . Surprisingly, the smallest context window size tested ( 8 k ) was universally better for all comparisons on comprehensiveness (average win rate of $58.1 \%$ ), while performing comparably with larger context sizes on diversity (average win rate $=52.4 \%$ ), and empowerment (average win rate $=51.3 \%$ ). Given our preference for more comprehensive and diverse answers, we therefore used a fixed context window size of 8 k tokens for the final evaluation.</p>
<h1>D Example Answer Comparison</h1>
<p>Table 5: Example question, answers, and LLM-generated assessments for the News article dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Which public figures are repeatedly mentioned across various entertainment articles?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Graph <br> RAG</td>
<td style="text-align: center;">The entertainment industry is vast and diverse, encompassing film, television, music, sports, and digital media. Certain public figures stand out due to their significant contributions and influence across these sectors. The following summary highlights key individuals who are repeatedly mentioned in various entertainment articles, reflecting their impact and presence within the industry. Actors and Directors [...] Public Figures in Controversy [...] Musicians and Executives [...] Athletes and Coaches [...] Influencers and Entrepreneurs [...] The repeated mention of these figures in entertainment articles signifies their ongoing relevance and the public's interest in their work. Their influence spans across various aspects of entertainment, from shaping cultural narratives in film and television to driving trends in music and digital media. These individuals not only contribute to their respective fields but also influence the broader cultural landscape, often becoming central figures in social discussions and public discourse.</td>
</tr>
<tr>
<td style="text-align: center;">Naïve <br> RAG</td>
<td style="text-align: center;">Public figures who are repeatedly mentioned across various entertainment articles include Taylor Swift, Travis Kelce, Britney Spears, and Justin Timberlake. These individuals have been highlighted for various reasons, ranging from their professional achievements to their personal lives. <br> Taylor Swift [...] Travis Kelce [...] Britney Spears [...] Justin Timberlake [..] <br> These figures are frequently covered due to their high-profile status and the public's interest in their careers and personal lives. Their activities, whether in music, sports, or personal relationships, have significant cultural and economic impacts, as evidenced by the media coverage and public reactions.</td>
</tr>
<tr>
<td style="text-align: center;">LLM <br> Decision</td>
<td style="text-align: center;">Comprehensiveness: Winner=1 (Graph RAG). Answer 1 is better because it provides a more comprehensive and detailed list of public figures from a wider range of entertainment sectors, including film, television, music, sports, gaming, and digital media. It also includes examples of their contributions and the impact they have on their respective fields, as well as mentions of controversies and their implications. Answer 2, while detailed in its coverage of a few individuals, is limited to a smaller number of public figures and focuses primarily on their personal lives and relationships rather than a broad spectrum of their professional influence across the entertainment industry. <br> Diversity: Winner=1 (Graph RAG). Answer 1 is better because it provides a more varied and rich response by covering a wide range of public figures from different sectors of the entertainment industry, including film, television, music, sports, gaming, and digital media. It offers insights into the contributions and influence of these figures, as well as controversies and their impact on public discourse. The answer also cites specific data sources for each mentioned figure, indicating a diverse range of evidence to support the claims. In contrast, Answer 2 focuses on a smaller group of public figures, primarily from the music industry and sports, and relies heavily on a single source for data, which makes it less diverse in perspectives and insights. <br> Empowerment: Winner=1 (Graph RAG). Answer 1 is better because it provides a comprehensive and structured overview of public figures across various sectors of the entertainment industry, including film, television, music, sports, and digital media. It lists multiple individuals, providing specific examples of their contributions and the context in which they are mentioned in entertainment articles, along with references to data reports for each claim. This approach helps the reader understand the breadth of the topic and make informed judgments without being misled. In contrast, Answer 2 focuses on a smaller group of public figures and primarily discusses their personal lives and relationships, which may not provide as broad an understanding of the topic. While Answer 2 also cites sources, it does not match the depth and variety of Answer 1. <br> Directness: Winner=2 (Naïve RAG). Answer 2 is better because it directly lists specific public figures who are repeatedly mentioned across various entertainment articles, such as Taylor Swift, Travis Kelce, Britney Spears, and Justin Timberlake, and provides concise explanations for their frequent mentions. Answer 1, while comprehensive, includes a lot of detailed information about various figures in different sectors of entertainment, which, while informative, does not directly answer the question with the same level of conciseness and specificity as Answer 2.</td>
</tr>
</tbody>
</table>
<h1>E System Prompts</h1>
<h2>E. 1 Element Instance Generation</h2>
<h2>---Goal---</h2>
<p>Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.</p>
<h2>---Steps---</h2>
<ol>
<li>
<p>Identify all entities. For each identified entity, extract the following information:</p>
</li>
<li>
<p>entity.name: Name of the entity, capitalized</p>
</li>
<li>
<p>entity.type: One of the following types: [[entity.types]]</p>
</li>
<li>
<p>entity.description: Comprehensive description of the entity's attributes and activities</p>
</li>
</ol>
<p>Format each entity as ("entity"{tuple.delimiter}<entity.name>{tuple.delimiter}<entity.type>{tuple_ delimiter}<entity.description>
2. From the entities identified in step 1, identify all pairs of (source.entity, target.entity) that are <em>clearly related</em> to each other
For each pair of related entities, extract the following information:</p>
<ul>
<li>source.entity: name of the source entity, as identified in step 1</li>
<li>target.entity: name of the target entity, as identified in step 1</li>
<li>relationship.description: explanation as to why you think the source entity and the target entity are related to each other</li>
<li>relationship.strength: a numeric score indicating strength of the relationship between the source entity and target entity</li>
</ul>
<p>Format each relationship as ("relationship"{tuple.delimiter}<source.entity>{tuple.delimiter}<target. entity>{tuple.delimiter}<relationship.description>{tuple.delimiter}<relationship.strength>)
3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use ** ${$ record.delimiter $} * *$ as the list delimiter.
4. When finished, output {completion_delimiter}
---Examples---
Entity.types: ORGANIZATION,PERSON
Input:
The Fed is scheduled to meet on Tuesday and Wednesday, with the central bank planning to release its latest policy decision on Wednesday at 2:00 p.m. ET, followed by a press conference where Fed Chair Jerome Powell will take questions. Investors expect the Federal Open Market Committee to hold its benchmark interest rate steady in a range of $5.25 \%-5.5 \%$.</p>
<p>Output:
("entity"{tuple.delimiter}FED{tuple_delimiter}ORGANIZATION{tuple_delimiter}The Fed is the Federal Reserve, which is setting interest rates on Tuesday and Wednesday)
${$ record_delimiter}
("entity"{tuple_delimiter}JEROME POWELL{tuple_delimiter}PERSON{tuple_delimiter}Jerome Powell is the chair of the Federal Reserve)
${$ record_delimiter}
("entity"{tuple_delimiter}FEDERAL OPEN MARKET COMMITTEE{tuple_delimiter}ORGANIZATION{tuple_delimiter}The Federal Reserve committee makes key decisions about interest rates and the growth of the United States money supply)
${$ record_delimiter}
("relationship"{tuple_delimiter}JEROME POWELL{tuple_delimiter}FED{tuple_delimiter}Jerome Powell is the Chair of the Federal Reserve and will answer questions at a press conference{tuple_delimiter}9)
${$ completion_delimiter}
...More examples...
---Real Data---
Entity.types: {entity.types}
Input:
{input_text}
Output:</p>
<h2>E. 2 Community Summary Generation</h2>
<h2>---Role---</h2>
<p>You are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.</p>            </div>
        </div>

    </div>
</body>
</html>
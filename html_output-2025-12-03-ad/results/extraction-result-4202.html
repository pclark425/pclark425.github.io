<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4202 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4202</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4202</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-269149146</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.08680v1.pdf" target="_blank">Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning</a></p>
                <p><strong>Paper Abstract:</strong> This research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed advanced fine-tuning methodologies on open sourced LLMs, applying textual data mining techniques to automate the knowledge discovery and synthesis phases of an SLR process, thus demonstrating a practical and efficient approach for extracting and analyzing high-quality information from large academic datasets. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labor-intensive processes of conducting literature reviews. As a scalable proof-of-concept, this study highlights the broad applicability of our approach across multiple research domains. The potential demonstrated here advocates for updates to PRISMA reporting guidelines, incorporating AI-driven processes to ensure methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, demonstrating how to conduct comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies while maintaining high standards.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4202.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4202.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (data extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used for automated Q&A/data extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used via API to generate high-level paper summaries, to produce domain-relevant question sets, and to extract answers (Q&A pairs) from chunked paper text to create the fine-tuning dataset; it was also used to automate FEVER/CGS evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 automated Q&A extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline that parses PDF papers into chunks, uses GPT-4 to (1) summarise each paper, (2) generate corpus-level and researcher-defined questions relevant across the corpus, and (3) answer those questions from each paper/chunk to produce paper-level and SLR-level Q&A pairs. The pipeline also produced multiple permutation variants per question for data augmentation and inserted unique corpus- and paper-level tokens into outputs to preserve provenance information. GPT-4 was further used to generate aggregated SLR-level Q&A by ingesting collated paper answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Learning analytics / educational technology (SLR corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>17</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction/distillation of study-level quantitative relationships and findings (statistical effects, direction of effects, sample sizes, quantitative outcomes across studies) rather than deriving formal closed‑form physical/mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Examples extracted by GPT-4 and encoded as Q&A in the dataset include (as described in the paper): (a) Fleur et al. (2020) — a reported interaction effect where the treatment group showed increased extrinsic motivation at course end while intrinsic motivation decreased in both groups; (b) Naranjo et al. (2019) — dashboards identifying preferred time slots and noting concurrent resource-usage constraints; (c) extraction of participant/sample counts and per-study quantitative outcomes listed in the SLR corpus (used to reconstruct SLR-level findings).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Chunk-based text extraction from PDFs, LLM-based summarization of sections and paragraphs, LLM-generated question formulation, and LLM answering to create structured Q&A pairs; data augmentation via question permutation; unique token insertion for provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Generated Q&A and summaries were validated indirectly by: (1) withholding a test set from finetuning and later using LLMs and human experts to evaluate answers; (2) using the paper's gold-standard PRISMA SLR as ground truth to check whether the generated content could replicate that SLR; (3) automated factuality checks (FEVER and CGS) executed via GPT-4 prompts and human inter-rater checks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports overall dataset sizes (initial 1503 Q&A samples expanded to 16,423 by permutations) and evaluation metrics applied downstream to finetuned models (FEVER and CGS); however, no standalone accuracy metrics are reported specifically for GPT-4's raw extraction quality prior to finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not separately reported for GPT-4 extraction as a component; overall downstream finetuned-model success rates reflect the quality of the generated dataset but GPT-4 extraction accuracy alone is not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Potential propagation of hallucinations or omissions from GPT-4 into the finetuning dataset if extraction misses facts; copyright limitations on sharing extracted material; no per-sample extraction accuracy measurements reported; reliance on a proprietary model (cost and reproducibility concerns).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No direct baseline comparison for the extraction step (e.g., human-curated Q&A) is reported; the study evaluates downstream finetuned models trained on the GPT-4–generated dataset against a gold-standard SLR instead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4202.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4202.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Finetuned Mistral-7B (NEFTune / LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B base model fine-tuned with NEFTune or LoRA parameter-efficient methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Mistral-7B was fine-tuned on the Q&A dataset (derived from the 17-paper SLR corpus) using PEFT methods (NEFTune and LoRA) to enable the model to accurately recall and synthesize study-level quantitative findings across the corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Finetuned Mistral-7B with NEFTune / LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A fine-tuning pipeline using parameter-efficient fine-tuning (PEFT) approaches: (a) LoRA (Low-Rank Adaptation) and (b) NEFTune (noisy embedding fine-tuning). The Mistral-7B base (7.3B parameters) was fine-tuned on the automatically generated Q&A dataset with unique corpus/paper tokens, question permutations, and withheld test splits. During inference the finetuned model answers paper-level and SLR-level queries; some experiments also combined the finetuned model with RAG retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7.3B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Learning analytics / educational technology (SLR use case)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>17</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Distillation and retrieval of empirical, quantitative study-level relationships and patterns (direction and presence of statistical effects across studies, sample sizes, frequency counts of features across the corpus) rather than deriving new symbolic/analytical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>The finetuned models reproduced paper-level quantitative findings such as: (a) Fleur et al. (2020) — interaction effects for extrinsic motivation (treatment vs control) and decreases in intrinsic motivation; (b) identification of dashboards' contributions to scheduling/resource-usage constraints from Naranjo et al. (2019); (c) reconstruction of counts and prevalence statements across studies (e.g., prevalence of predictive modelling features). These emerge as structured Q&A answers (textual summaries of quantitative results) rather than explicit equations.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Model was trained on LLM-generated Q&A pairs (paper-summary-level, paper-level chunk/paragraph-level, and SLR-level) that encoded extracted quantitative facts; training used data augmentation via question permutations and inserted provenance tokens to tie answers to source studies.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Evaluation used a withheld test set and an 11-item qualitative gold-standard SLR-level test; automated factuality evaluation was performed using FEVER categories and the Consistency Grading Scale (CGS) (both implemented via GPT-4 prompts), and a domain expert manually evaluated synthesis fidelity (Supports / Does Not Support).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>NEFTune: FEVER SUPPORTED = 89.2% (paper reports), CGS mean = 1.75 (interpreted as 87.6% fully consistent); LoRA: FEVER SUPPORTED ≈ 87.7%, CGS mean = 1.72 (86.0%). Baseline Mistral-7B-Instruct: FEVER SUPPORTED = 14.5%, CGS mean = 0.164 (8.2%). Detailed FEVER/CGS category breakdowns for paper-level / summary-level / SLR-level are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>NEFTune produced SUPPORTED classifications on ~89.2% of test responses (FEVER) and 'Fully consistent' on ~85.7% (CGS per the paper's aggregated presentation); LoRA performed slightly lower but comparably (≈87.7% SUPPORTED).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Lower accuracy on SLR-level (cross-paper synthesis) questions (higher REFUTED rates and lower 'Fully consistent' proportions); integrating RAG with finetuned models sometimes reduced performance (confusion between internalized fine-tuned facts and retrieved contexts); hyperparameter tuning limited by compute/time; potential residual influence from pretraining knowledge without perfect isolation despite token markers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against the baseline Mistral-7B-Instruct (un-finetuned) which had low factual recall (14.5% SUPPORTED). Finetuned NEFTune and LoRA dramatically outperformed the baseline; combining NEFTune with RAG + finetuned-data decreased performance relative to NEFTune alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4202.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4202.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (Weaviate + embeddings + LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation pipeline using Weaviate vector DB, OpenAI embeddings, and post-retrieval LLM aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation system that indexes either raw article text or the auto-extracted finetuning Q&A data into a vector DB (Weaviate) using OpenAI embeddings, retrieves contextual chunks at query time, and uses an LLM to aggregate retrieved context into short answers to support factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RAG via Weaviate + text-embedding-3-large + LLM aggregator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pre-retrieval: documents/chunks (raw articles or auto-extracted Q&A data) were vectorized using OpenAI's 'text-embedding-3-large' and indexed in Weaviate. Retrieval used LangChain's self-querying retriever to fetch context filtered by 'source' attributes. Post-retrieval: retrieved contexts were passed to a generative LLM (initially gpt-3.5-turbo) with an aggregation prompt that produced concise (3-sentence) answers. The study evaluated RAG with raw article text and RAG with the auto-extracted, structured dataset (RAG + FD), and combinations with finetuned Mistral models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-embedding-3-large (embeddings) and gpt-3.5-turbo (post-retrieval aggregation); in combination experiments Mistral-7B-Instruct and finetuned Mistral models were used</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Learning analytics / educational technology (SLR corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>17</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Support for retrieving/distilling empirical quantitative statements and cross-study patterns (as textual, evidence-anchored claims) from the corpus; RAG was used to ground generative outputs in retrieved evidence rather than to discover formal mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>When used with the auto-extracted dataset (RAG + FD), retrieved contexts enabled answers about study findings (e.g., resource-usage scheduling insights from Naranjo 2019) and motivational effect findings (Fleur 2020). However, RAG with raw articles showed poor retrieval precision for SLR synthesis in this study; RAG+FD had moderate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Vector indexing of chunked document text or structured extracted Q&A, nearest-neighbor retrieval at query time, and aggregation into concise answers via a generative LLM; retrieval included 'source' attributes to support provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Same FEVER and CGS automated evaluations and human expert qualitative checks as other methods; compared RAG variants against finetuned-only baselines on the withheld test set and gold-standard SLR replication tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CGS means reported per method: RAG (raw) CGS mean = 0.46 (23.2% 'Fully consistent'); RAG + FD CGS mean = 1.67 (83.7% fully/partly consistent aggregated); combining NEFTune with RAG + FD reduced CGS mean to 1.14 (57.2%). NEFTune + (RAG + FD) FEVER SUPPORTED ≈ 63.2% (paper reports). Exact FEVER/SUPPORTED rates for RAG variants are included as aggregated comparative results in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>RAG + auto-extracted data performed reasonably (CGS mean ~1.67, paper reports ~74.7% fully consistent in the FEVER/CGS breakdown), but raw-article RAG performed poorly; combining RAG with finetuned LLMs sometimes degraded accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>RAG with raw articles struggled to retrieve precisely relevant SLR facts; integrating retrieval outputs with finetuned model internal knowledge sometimes introduced contradictions or increased 'NOT ENOUGH INFO' classifications; retrieval quality depends on chunking, indexing, and embedding choices; RAG parameter tuning and hybrid search were noted as future improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to finetuned-only methods (NEFTune/LoRA) and the baseline un-finetuned instruct model. NEFTune alone outperformed NEFTune+RAG+FD in several factuality metrics, indicating RAG did not unambiguously improve downstream SLR synthesis in this implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4202.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4202.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token insertion (knowledge markers)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token insertion as corpus- and paper-level knowledge markers (novel provenance mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel technique introduced in this paper that inserts unique corpus-level and paper-level tokens into Q&A training data to tie outputs to their source studies and to guide the finetuned model to use fine-tuned knowledge rather than pretrained knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Token insertion / knowledge-marker mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>During Q&A dataset post-processing the authors inserted explicit tokens (e.g., a corpus token like '2023SLR' and paper tokens like 'aljohani2019integrated') into each Q&A pair. The tokens act as signposts so that at inference the model can (1) produce answers that include source citations and (2) preferentially activate parameters associated with fine-tuned knowledge. This mechanism is intended to support provenance auditing and to reduce reliance on pretraining knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Learning analytics / educational technology (SLR corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>17</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Facilitates provenance-linked extraction of empirical quantitative statements (counts, effect directions, per-study quantitative claims) rather than producing new mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Tokenized Q&A outputs include source attribution (e.g., 'Source: aljohani2019integrated') for answers summarizing study-level quantitative findings; this enables tracing of claims such as per-study effect results or sample sizes back to the originating paper in the corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Post-processing insertion of unique tokens into each Q&A training sample; tokens applied at both corpus and paper granularity and included in the 'instruction' and/or 'output' fields so model outputs consistently include source tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Provenance enforced by matching 'Source' tokens in model outputs against the gold-standard corpus during FEVER/CGS automated evaluation (FEVER label REFUTED included source mismatch as a refutation criterion). Manual expert review also checked that outputs could be traced back to the stated sources.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No isolated numerical performance metric provided specifically for the token method alone; it was part of the overall finetuning pipeline that yielded high NEFTune/LoRA factuality results.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not separately reported; token mechanism contributed to overall high supported/fully-consistent rates reported for finetuned models but per-token efficacy not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The method may not fully insulate answers from the model's pretraining knowledge in all cases; token-reliant behavior depends on how strongly the model associates tokens with updated parameters; no ablation study isolating tokens' effect was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No explicit baseline without tokens was quantitatively compared in the paper; the token insertion approach is presented as a novel mechanism within the described finetuning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models replace humans in the systematic review process? evaluating gpt-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. <em>(Rating: 2)</em></li>
                <li>Artificial intelligence to automate the systematic review of scientific literature. <em>(Rating: 2)</em></li>
                <li>Artificial intelligence for literature reviews: Opportunities and challenges. <em>(Rating: 2)</em></li>
                <li>Leveraging large language models for literature review tasks-a case study using chatgpt. <em>(Rating: 1)</em></li>
                <li>Streamlining systematic reviews: Harnessing large language models for quality assessment and risk-of-bias evaluation. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4202",
    "paper_id": "paper-269149146",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "GPT-4 (data extraction)",
            "name_full": "Generative Pre-trained Transformer 4 (used for automated Q&A/data extraction)",
            "brief_description": "GPT-4 was used via API to generate high-level paper summaries, to produce domain-relevant question sets, and to extract answers (Q&A pairs) from chunked paper text to create the fine-tuning dataset; it was also used to automate FEVER/CGS evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 automated Q&A extraction pipeline",
            "system_description": "A pipeline that parses PDF papers into chunks, uses GPT-4 to (1) summarise each paper, (2) generate corpus-level and researcher-defined questions relevant across the corpus, and (3) answer those questions from each paper/chunk to produce paper-level and SLR-level Q&A pairs. The pipeline also produced multiple permutation variants per question for data augmentation and inserted unique corpus- and paper-level tokens into outputs to preserve provenance information. GPT-4 was further used to generate aggregated SLR-level Q&A by ingesting collated paper answers.",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Learning analytics / educational technology (SLR corpus)",
            "number_of_papers": "17",
            "law_type": "Extraction/distillation of study-level quantitative relationships and findings (statistical effects, direction of effects, sample sizes, quantitative outcomes across studies) rather than deriving formal closed‑form physical/mathematical laws.",
            "law_examples": "Examples extracted by GPT-4 and encoded as Q&A in the dataset include (as described in the paper): (a) Fleur et al. (2020) — a reported interaction effect where the treatment group showed increased extrinsic motivation at course end while intrinsic motivation decreased in both groups; (b) Naranjo et al. (2019) — dashboards identifying preferred time slots and noting concurrent resource-usage constraints; (c) extraction of participant/sample counts and per-study quantitative outcomes listed in the SLR corpus (used to reconstruct SLR-level findings).",
            "extraction_method": "Chunk-based text extraction from PDFs, LLM-based summarization of sections and paragraphs, LLM-generated question formulation, and LLM answering to create structured Q&A pairs; data augmentation via question permutation; unique token insertion for provenance.",
            "validation_approach": "Generated Q&A and summaries were validated indirectly by: (1) withholding a test set from finetuning and later using LLMs and human experts to evaluate answers; (2) using the paper's gold-standard PRISMA SLR as ground truth to check whether the generated content could replicate that SLR; (3) automated factuality checks (FEVER and CGS) executed via GPT-4 prompts and human inter-rater checks.",
            "performance_metrics": "The paper reports overall dataset sizes (initial 1503 Q&A samples expanded to 16,423 by permutations) and evaluation metrics applied downstream to finetuned models (FEVER and CGS); however, no standalone accuracy metrics are reported specifically for GPT-4's raw extraction quality prior to finetuning.",
            "success_rate": "Not separately reported for GPT-4 extraction as a component; overall downstream finetuned-model success rates reflect the quality of the generated dataset but GPT-4 extraction accuracy alone is not quantified in the paper.",
            "challenges_limitations": "Potential propagation of hallucinations or omissions from GPT-4 into the finetuning dataset if extraction misses facts; copyright limitations on sharing extracted material; no per-sample extraction accuracy measurements reported; reliance on a proprietary model (cost and reproducibility concerns).",
            "comparison_baseline": "No direct baseline comparison for the extraction step (e.g., human-curated Q&A) is reported; the study evaluates downstream finetuned models trained on the GPT-4–generated dataset against a gold-standard SLR instead.",
            "uuid": "e4202.0",
            "source_info": {
                "paper_title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Finetuned Mistral-7B (NEFTune / LoRA)",
            "name_full": "Mistral-7B base model fine-tuned with NEFTune or LoRA parameter-efficient methods",
            "brief_description": "Open-source Mistral-7B was fine-tuned on the Q&A dataset (derived from the 17-paper SLR corpus) using PEFT methods (NEFTune and LoRA) to enable the model to accurately recall and synthesize study-level quantitative findings across the corpus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Finetuned Mistral-7B with NEFTune / LoRA",
            "system_description": "A fine-tuning pipeline using parameter-efficient fine-tuning (PEFT) approaches: (a) LoRA (Low-Rank Adaptation) and (b) NEFTune (noisy embedding fine-tuning). The Mistral-7B base (7.3B parameters) was fine-tuned on the automatically generated Q&A dataset with unique corpus/paper tokens, question permutations, and withheld test splits. During inference the finetuned model answers paper-level and SLR-level queries; some experiments also combined the finetuned model with RAG retrieval.",
            "model_name": "Mistral-7B",
            "model_size": "7.3B",
            "scientific_domain": "Learning analytics / educational technology (SLR use case)",
            "number_of_papers": "17",
            "law_type": "Distillation and retrieval of empirical, quantitative study-level relationships and patterns (direction and presence of statistical effects across studies, sample sizes, frequency counts of features across the corpus) rather than deriving new symbolic/analytical laws.",
            "law_examples": "The finetuned models reproduced paper-level quantitative findings such as: (a) Fleur et al. (2020) — interaction effects for extrinsic motivation (treatment vs control) and decreases in intrinsic motivation; (b) identification of dashboards' contributions to scheduling/resource-usage constraints from Naranjo et al. (2019); (c) reconstruction of counts and prevalence statements across studies (e.g., prevalence of predictive modelling features). These emerge as structured Q&A answers (textual summaries of quantitative results) rather than explicit equations.",
            "extraction_method": "Model was trained on LLM-generated Q&A pairs (paper-summary-level, paper-level chunk/paragraph-level, and SLR-level) that encoded extracted quantitative facts; training used data augmentation via question permutations and inserted provenance tokens to tie answers to source studies.",
            "validation_approach": "Evaluation used a withheld test set and an 11-item qualitative gold-standard SLR-level test; automated factuality evaluation was performed using FEVER categories and the Consistency Grading Scale (CGS) (both implemented via GPT-4 prompts), and a domain expert manually evaluated synthesis fidelity (Supports / Does Not Support).",
            "performance_metrics": "NEFTune: FEVER SUPPORTED = 89.2% (paper reports), CGS mean = 1.75 (interpreted as 87.6% fully consistent); LoRA: FEVER SUPPORTED ≈ 87.7%, CGS mean = 1.72 (86.0%). Baseline Mistral-7B-Instruct: FEVER SUPPORTED = 14.5%, CGS mean = 0.164 (8.2%). Detailed FEVER/CGS category breakdowns for paper-level / summary-level / SLR-level are provided in the paper.",
            "success_rate": "NEFTune produced SUPPORTED classifications on ~89.2% of test responses (FEVER) and 'Fully consistent' on ~85.7% (CGS per the paper's aggregated presentation); LoRA performed slightly lower but comparably (≈87.7% SUPPORTED).",
            "challenges_limitations": "Lower accuracy on SLR-level (cross-paper synthesis) questions (higher REFUTED rates and lower 'Fully consistent' proportions); integrating RAG with finetuned models sometimes reduced performance (confusion between internalized fine-tuned facts and retrieved contexts); hyperparameter tuning limited by compute/time; potential residual influence from pretraining knowledge without perfect isolation despite token markers.",
            "comparison_baseline": "Compared against the baseline Mistral-7B-Instruct (un-finetuned) which had low factual recall (14.5% SUPPORTED). Finetuned NEFTune and LoRA dramatically outperformed the baseline; combining NEFTune with RAG + finetuned-data decreased performance relative to NEFTune alone.",
            "uuid": "e4202.1",
            "source_info": {
                "paper_title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAG (Weaviate + embeddings + LLM)",
            "name_full": "Retrieval-Augmented Generation pipeline using Weaviate vector DB, OpenAI embeddings, and post-retrieval LLM aggregation",
            "brief_description": "A retrieval-augmented generation system that indexes either raw article text or the auto-extracted finetuning Q&A data into a vector DB (Weaviate) using OpenAI embeddings, retrieves contextual chunks at query time, and uses an LLM to aggregate retrieved context into short answers to support factuality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RAG via Weaviate + text-embedding-3-large + LLM aggregator",
            "system_description": "Pre-retrieval: documents/chunks (raw articles or auto-extracted Q&A data) were vectorized using OpenAI's 'text-embedding-3-large' and indexed in Weaviate. Retrieval used LangChain's self-querying retriever to fetch context filtered by 'source' attributes. Post-retrieval: retrieved contexts were passed to a generative LLM (initially gpt-3.5-turbo) with an aggregation prompt that produced concise (3-sentence) answers. The study evaluated RAG with raw article text and RAG with the auto-extracted, structured dataset (RAG + FD), and combinations with finetuned Mistral models.",
            "model_name": "text-embedding-3-large (embeddings) and gpt-3.5-turbo (post-retrieval aggregation); in combination experiments Mistral-7B-Instruct and finetuned Mistral models were used",
            "model_size": null,
            "scientific_domain": "Learning analytics / educational technology (SLR corpus)",
            "number_of_papers": "17",
            "law_type": "Support for retrieving/distilling empirical quantitative statements and cross-study patterns (as textual, evidence-anchored claims) from the corpus; RAG was used to ground generative outputs in retrieved evidence rather than to discover formal mathematical laws.",
            "law_examples": "When used with the auto-extracted dataset (RAG + FD), retrieved contexts enabled answers about study findings (e.g., resource-usage scheduling insights from Naranjo 2019) and motivational effect findings (Fleur 2020). However, RAG with raw articles showed poor retrieval precision for SLR synthesis in this study; RAG+FD had moderate performance.",
            "extraction_method": "Vector indexing of chunked document text or structured extracted Q&A, nearest-neighbor retrieval at query time, and aggregation into concise answers via a generative LLM; retrieval included 'source' attributes to support provenance.",
            "validation_approach": "Same FEVER and CGS automated evaluations and human expert qualitative checks as other methods; compared RAG variants against finetuned-only baselines on the withheld test set and gold-standard SLR replication tasks.",
            "performance_metrics": "CGS means reported per method: RAG (raw) CGS mean = 0.46 (23.2% 'Fully consistent'); RAG + FD CGS mean = 1.67 (83.7% fully/partly consistent aggregated); combining NEFTune with RAG + FD reduced CGS mean to 1.14 (57.2%). NEFTune + (RAG + FD) FEVER SUPPORTED ≈ 63.2% (paper reports). Exact FEVER/SUPPORTED rates for RAG variants are included as aggregated comparative results in the paper.",
            "success_rate": "RAG + auto-extracted data performed reasonably (CGS mean ~1.67, paper reports ~74.7% fully consistent in the FEVER/CGS breakdown), but raw-article RAG performed poorly; combining RAG with finetuned LLMs sometimes degraded accuracy.",
            "challenges_limitations": "RAG with raw articles struggled to retrieve precisely relevant SLR facts; integrating retrieval outputs with finetuned model internal knowledge sometimes introduced contradictions or increased 'NOT ENOUGH INFO' classifications; retrieval quality depends on chunking, indexing, and embedding choices; RAG parameter tuning and hybrid search were noted as future improvements.",
            "comparison_baseline": "Compared to finetuned-only methods (NEFTune/LoRA) and the baseline un-finetuned instruct model. NEFTune alone outperformed NEFTune+RAG+FD in several factuality metrics, indicating RAG did not unambiguously improve downstream SLR synthesis in this implementation.",
            "uuid": "e4202.2",
            "source_info": {
                "paper_title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Token insertion (knowledge markers)",
            "name_full": "Token insertion as corpus- and paper-level knowledge markers (novel provenance mechanism)",
            "brief_description": "A novel technique introduced in this paper that inserts unique corpus-level and paper-level tokens into Q&A training data to tie outputs to their source studies and to guide the finetuned model to use fine-tuned knowledge rather than pretrained knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Token insertion / knowledge-marker mechanism",
            "system_description": "During Q&A dataset post-processing the authors inserted explicit tokens (e.g., a corpus token like '2023SLR' and paper tokens like 'aljohani2019integrated') into each Q&A pair. The tokens act as signposts so that at inference the model can (1) produce answers that include source citations and (2) preferentially activate parameters associated with fine-tuned knowledge. This mechanism is intended to support provenance auditing and to reduce reliance on pretraining knowledge.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Learning analytics / educational technology (SLR corpus)",
            "number_of_papers": "17",
            "law_type": "Facilitates provenance-linked extraction of empirical quantitative statements (counts, effect directions, per-study quantitative claims) rather than producing new mathematical laws.",
            "law_examples": "Tokenized Q&A outputs include source attribution (e.g., 'Source: aljohani2019integrated') for answers summarizing study-level quantitative findings; this enables tracing of claims such as per-study effect results or sample sizes back to the originating paper in the corpus.",
            "extraction_method": "Post-processing insertion of unique tokens into each Q&A training sample; tokens applied at both corpus and paper granularity and included in the 'instruction' and/or 'output' fields so model outputs consistently include source tokens.",
            "validation_approach": "Provenance enforced by matching 'Source' tokens in model outputs against the gold-standard corpus during FEVER/CGS automated evaluation (FEVER label REFUTED included source mismatch as a refutation criterion). Manual expert review also checked that outputs could be traced back to the stated sources.",
            "performance_metrics": "No isolated numerical performance metric provided specifically for the token method alone; it was part of the overall finetuning pipeline that yielded high NEFTune/LoRA factuality results.",
            "success_rate": "Not separately reported; token mechanism contributed to overall high supported/fully-consistent rates reported for finetuned models but per-token efficacy not quantified.",
            "challenges_limitations": "The method may not fully insulate answers from the model's pretraining knowledge in all cases; token-reliant behavior depends on how strongly the model associates tokens with updated parameters; no ablation study isolating tokens' effect was reported.",
            "comparison_baseline": "No explicit baseline without tokens was quantitatively compared in the paper; the token insertion approach is presented as a novel mechanism within the described finetuning pipeline.",
            "uuid": "e4202.3",
            "source_info": {
                "paper_title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models replace humans in the systematic review process? evaluating gpt-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages.",
            "rating": 2,
            "sanitized_title": "can_large_language_models_replace_humans_in_the_systematic_review_process_evaluating_gpt4s_efficacy_in_screening_and_extracting_data_from_peerreviewed_and_grey_literature_in_multiple_languages"
        },
        {
            "paper_title": "Artificial intelligence to automate the systematic review of scientific literature.",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_to_automate_the_systematic_review_of_scientific_literature"
        },
        {
            "paper_title": "Artificial intelligence for literature reviews: Opportunities and challenges.",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_for_literature_reviews_opportunities_and_challenges"
        },
        {
            "paper_title": "Leveraging large language models for literature review tasks-a case study using chatgpt.",
            "rating": 1,
            "sanitized_title": "leveraging_large_language_models_for_literature_review_tasksa_case_study_using_chatgpt"
        },
        {
            "paper_title": "Streamlining systematic reviews: Harnessing large language models for quality assessment and risk-of-bias evaluation.",
            "rating": 1,
            "sanitized_title": "streamlining_systematic_reviews_harnessing_large_language_models_for_quality_assessment_and_riskofbias_evaluation"
        }
    ],
    "cost": 0.01894725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Keywords LLM Fine-tuning for SLRs • SLR Automation • Retrieval-Augmented Generation for Research • Domain-Specific Model Training • Knowledge Synthesis AI • AI-Driven Research Synthesis • Literature Review Automation • Generative AI • AI-Enhanced Systematic Reviews • PRISMA and AI Integration
April 16, 2024</p>
<p>Teo Susnjak t.susnjak@massey.ac.nz 
Peter Hwang 
Napoleon H Reyes 
Andre L C Barczak 
Timothy R Mcintosh 
Surangika Ranathunga </p>
<p>School of Mathematical and Computational Sciences
Massey University
AlbanyNew Zealand</p>
<p>School of Mathematical and Computational Sciences
Massey University
AlbanyNew Zealand</p>
<p>School of Mathematical and Computational Sciences
Massey University
AlbanyNew Zealand</p>
<p>Centre for Data Analytics
Bond University Gold Coast
Australia</p>
<p>Cyberoo Pty Ltd Surry Hills
NSWAustralia</p>
<p>School of Mathematical and Computational Sciences
Massey University
AlbanyNew Zealand</p>
<p>Keywords LLM Fine-tuning for SLRs • SLR Automation • Retrieval-Augmented Generation for Research • Domain-Specific Model Training • Knowledge Synthesis AI • AI-Driven Research Synthesis • Literature Review Automation • Generative AI • AI-Enhanced Systematic Reviews • PRISMA and AI Integration
April 16, 2024AECBCDA5CB8D53BDFA2A5204407CD11EarXiv:2404.08680v1[cs.CL]
This research pioneers the use of finetuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies.Our study employed the latest finetuning methodologies together with open-sourced LLMs, and demonstrated a practical and efficient approach to automating the final execution stages of an SLR process that involves knowledge synthesis.The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR.Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research.The findings ultimately confirmed the potential of finetuned LLMs in streamlining various labour-intensive processes of conducting literature reviews.Given the potential of this approach and its applicability across all research domains, this foundational study also advocated for updating PRISMA reporting guidelines to incorporate AI-driven processes, ensuring methodological transparency and reliability in future SLRs.This study broadens the appeal of AI-enhanced tools across various academic and research fields, setting a new standard for conducting comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies.</p>
<p>Introduction</p>
<p>Systematic Literature Reviews (SLRs) serve as the bedrock of academic research, playing a crucial role in the amalgamation, examination, and synthesis of existing scholarly knowledge across various fields [1,2,3].These reviews offer a methodical and replicable approach, ensuring the integrity and thoroughness of research synthesis especially when combined with reporting guidelines like PRISMA [4,5,6].Such a foundation is indispensable for advancing both theoretical understanding and practical applications.However, the traditional execution of SLRs is marked by its manual and resource-intensive nature, often stretching over extensive periods, which introduces significant inefficiencies into the research process [7,8].</p>
<p>The rigorous yet cumbersome character of traditional SLR methodologies presents considerable bottlenecks in the management and synthesis of large datasets of selected studies that hinges on effective information retrieval [9,10].These challenges not only prolong the synthesis -the execution phase of a review -but also hamper the ongoing updates of the SLRs with newer findings, and risk diminishing the timeliness and relevance of the insights gleaned [11,12].This scenario underscores the need for innovative, scalable and sustainable solutions that can streamline the extraction of information from findings situated in academic papers, as well as its persistence in suitable information technologies which facilitate their accurate and effective retrieval necessary for executing SLRs [13,14].</p>
<p>The recent advent of a new class of Artificial Intelligence (AI) systems like Large Language Models (LLMs), heralds a new epoch with the potential to dramatically redefine the SLR landscape through the automation of the information retrieval processes while maintaining high factual fidelity [15,8].These models, with their advanced natural language comprehension capabilities, text generation and knowledge retention [16], offer a promising avenue for automating and optimizing various stages of the SLR process [17], and in particular the execution phase that relies on "talking to" both individual academic papers via LLMs, as well as simultaneously "talking across" all target papers for synthesising purposes [18].Despite their potential, the broad generalist pretraining of these models which have been trained on vast amounts of diverse text data means that the LLMs fall short in providing the domain-specific accuracy and precision in the information retrieved that is essential for the detailed task of knowledge synthesis across a very specific and narrow sets of studies.Additionally, their current propensity to hallucinate [19,20,21,22] renders them unable to consistently respond accurately, which precludes them from providing reliable responses needed to conduct SLRs with integrity.Additionally, current LLMs have variable abilities to audit and track the sources of their responses [23].Their inability to reliably ensure that the provenance of the LLM responses can be linked to the target studies that comprise an SLR corpus represents a serious limitation for using this technology for this purpose.Collectively, these gaps highlight a critical area within AI-assisted SLR processes for the purpose of enhancing the information retrieval capabilities of LLMs [24].Indeed, resent research has repeatedly raised these concerns with respect to using LLM for the purposes of SLRs, especially in the knowledge synthesis stages.While it has been suggested that LLMs could be used for assisting evidence synthesis tasks in SLRs via their summarization capabilities, concerns have been raised about their lack of continuous learning capability and temporal reasoning [25,26].Qureshi et al. [27] noted that using LLMs shows promise for aiding in systematic review-related tasks, but the authors concluded that the technology is in its infancy and requires significant development for such applications.In their most recent review, Bolanos et al. [28] found that LLM usage for SLRs is hampered by limitations such as reduced efficacy in domain-specific and narrower subject areas, their propensity for hallucinating and generating misleading information, alongside their opaque decision processes which cannot be audited.</p>
<p>To surmount these challenges, this study proposes the creation of finetuned open-source LLMs, trained on the corpus of selected academic papers for a target SLR, expanding the generalist knowledge of an LLM with narrower domainspecific expertise.This work devises a novel way to automatically extract information from a set of academic papers in order to create SLR-specific datasets which can be leveraged for finetuning LLMs so that they can support question and answering downstream tasks.We also devise mechanisms to mitigate LLM hallucination and to ensure that all LLM responses related to an SLR can be tracked to source studies.The ensuing research presents a comprehensive SLR-automation framework with a focus on the knowledge synthesis stage that aims to revolutionize information retrieval mechanisms with empirical evidence of their effectiveness, thereby expediting and transforming the synthesis of research findings in the context of SLRs.</p>
<p>Contribution</p>
<p>Our study substantially contributes to the field of information retrieval in general where factual recall from arrays of documents is required and needs to be expressed in natural language, while possessing the ability to audit the responses with respect to the source information.Our proposed framework is specifically applied and demonstrated to the context of SLR-automation, where the goal was to validate the proposed framework through an empirical study that seeks to replicate a previously published PRISMA-conforming SLR, which serves as a gold standard and a test use case for validating the framework.The contributions of this research can be summarised as:</p>
<p>• Devising a methodical and automate approach for converting selected academic papers into datasets that can be used for finetuning LLMs.</p>
<p>• Proposing an effective approach to ensuring factual recall in responses by providing a mechanism to audit the source information of the LLM's response.</p>
<p>• Developed and validated evaluation metrics with the goal of testing for factuality in the LLM responses.</p>
<p>• Benchmarked various AI automation methodologies for SLRs, focusing on the comparative effectiveness of different finetuning approaches.</p>
<p>• Demonstrated the efficacy of these proposed methodologies by replicating a published PRISMA-conforming SLR.</p>
<p>• Enhancing the scholarly toolkit for SLRs with advanced, efficient, and context-aware AI technologies.</p>
<p>• Setting new standards in academic research for reliability, validity, and ethical AI employment through pioneering AI methodologies.</p>
<p>• Releasing a Python package 2 to facilitate data curation for LLM fine-tuning, tailored for the unique requirements of SLRs.</p>
<p>Background</p>
<p>This literature review examines the SLR process, especially the knowledge synthesis phase and focuses on the integration of LLMs and AI to automate its various stages.It also addresses the current efforts in SLR automation, the role of LLMs in enhancing these processes, and the critical challenge of ensuring factual accuracy arising from LLM hallucinations.The review further explores studies that have considered the potential of fine-tuning domain-specific LLMs to tailor their performance for SLR tasks, aiming to provide a clear, concise overview of the advancements and challenges in employing AI tools for SLR automation.</p>
<p>Systematic Literature Review Process and Synthesis of Studies</p>
<p>The process of conducting an SLR (Figure 1) begins with the clear articulation of the review's purpose and intended outcomes.This foundational step ensures that the review's scope and objectives are explicit to its audience [29].Following this, the development of a detailed protocol and the training of the review team are essential to guarantee methodological rigor and consistency in the review execution.The screening process, both for inclusion and quality appraisal, demands transparency in the criteria used for study selection and exclusion, ensuring that only the most relevant and high-quality studies are considered for synthesis.The comprehensive search for literature, coupled with systematic data extraction from the included studies, lays the groundwork for the subsequent synthesis phase.This phase is where the insights gleaned from individual studies are integrated to form a coherent narrative or to distill new theoretical understandings.</p>
<p>Figure 1: The general process of conducting an SLR as outlined by Okoli [29], denoting which steps should be explicitly reported to the reader, as well as explained to justify the comprehensiveness of the SLR despite exclusion criteria together with the detailed communication of steps taken to make the SLR reproducible.</p>
<p>Synthesis of Studies in SLRs</p>
<p>Having navigated the initial steps of the SLR process, from purpose identification through to quality appraisal, we arrive at the execution part of the SLR, which is the synthesis phase (Step 4 in Figure 1).This juncture is where the accumulated evidence is methodically combined to reveals new insights or to forge theoretical advancements.The approach to synthesis is inherently dependent on the nature of the primary studies under consideration-quantitative, qualitative, or a combination thereof [30].The synthesis phase in SLRs is arguably one of the most challenging yet crucial stages of the literature review process.It demands not only a comprehensive understanding of the included studies but also a demanding task of integrating their findings [31].This phase is pivotal for transcending beyond mere aggregation of data and information and instead aiming to distill new insights or theoretical contributions from the collective evidence [32].The complexity of this task is magnified by the diversity of study designs, methodologies, terminologies, and data types encountered within the corpus of literature being reviewed [33].</p>
<p>Quantitative Synthesis: Meta-Analysis</p>
<p>Meta-Analysis (or a integrative review) is a cornerstone in the quantitative synthesis landscape [34], and exemplifies the methodological rigor required to aggregate data across studies.Despite its potential for generating robust evidence, the meta-analytical process is fraught with challenges [35].These include heterogeneity among study outcomes, variations in study quality, and the potential for publication bias, all of which necessitate sophisticated statistical techniques and critical judgment.The method's reliance on homogeneity and the need for comparable effect sizes across studies further complicate its application, especially in fields characterized by methodological diversity [36].</p>
<p>Qualitative Synthesis: Meta-Synthesis</p>
<p>Meta-Synthesis, or an interpretive review, is the the qualitative counterpart that involves synthesizing findings from qualitative studies to uncover overarching themes or conceptual insights [37].This process is inherently interpretative, requiring a deep engagement with the textual data and an ability to discern patterns and meanings across disparate studies.</p>
<p>The challenges here lie in maintaining the integrity and context of individual studies while seeking commonalities or divergent themes across the body of evidence [38].The subjective nature of this synthesis approach underscores the need for transparency and reflexivity in the analytical process, ensuring that the synthesis is both comprehensive and faithful to the original studies [39].The synthesis phase, irrespective of the methodological approach, is inherently complex due to the need to balance between the depth of individual studies and the breadth of the review's scope [40].This balancing act is further complicated by the varying quality of the studies, the diversity of their contexts, and the potential for conflicting findings [41].As such, the synthesis not only requires methodological expertise but also a creative and critical approach to integrating knowledge [42].It is this phase that truly tests the reviewer's ability to construct a coherent narrative or theoretical framework that advances understanding in the field, making it one of the most intellectually demanding aspects of the SLR process [43].</p>
<p>Current State of AI Tools and Research for Automation in SLRs</p>
<p>Recent progress in AI, Natural Language Processing (NLP), and machine learning has significantly propelled the development of automation tools for supporting literature reviews.Techniques such as text mining and machine learning algorithms have demonstrated potential in automating various stages of the review process, from literature retrieval to data extraction [44].These tools range from comprehensive research management platforms like Covidence3 , ASReview4 , Mendeley 5 , and Zotero6 , to specialized tools aimed at specific literature review stages, such as 'connected papers' 7 for literature search and Elicit8 for individual paper analysis.Despite the growing availability of these tools, their adoption remains limited, with researchers citing challenges such as steep learning curves and inadequate support [45].More advanced AI tools provide citation support like Scite9 , as well as ScholarAI10 integration tools with GPT models with numerous features for literature search support.</p>
<p>Meanwhile the body of research dedicated to automating SLR tasks is expanding, with several review papers categorizing these efforts [46,47,48,8,49,50,51,52,53,54,28].Nonetheless, the focus on automating the knowledge synthesis phase of SLRs remains minimal [28] which is a reflection of the task's inherent complexity.Most research in SLR automation employs NLP and Information Retrieval techniques, with tasks such as data extraction and monitoring often framed as text summarization challenges [14,55].</p>
<p>LLMs for research and SLRs automation</p>
<p>LLMs like GPT-3, have brought about transformative possibilities in SLRs.Rooted in the Transformer architecture, these models are well-suited for processing and synthesizing information from large corpora [56] that include academic texts.The fine-tuning of LLMs with domain-specific data presents an opportunity to enhance their effectiveness in generating accurate study summaries, albeit challenges in reliability, validity, and interpretability remain [57].</p>
<p>With the popularity of LLMs such as OpenAI's ChatGPT, Google's Gemini and Anthropic's Claude, SLR researchers now have a natural tendency to explore the capabilities of these models for SLR development.For example, Gupta et al. [58] used ChatGPT to generate novel SLR ideas and noted that the software was highly accurate.Hill et al. [59] used Microsoft's Bing AI to extract study characteristics from research papers as data items.Castillo-Segura et al.</p>
<p>[60] compared six AI based tools ( (Forefront, GetGPT, ThebAI, Claude, Bard, and H2O) for the screening stage of SLR.Kumar [61] used ChatGPT to generate small reports on research topics, but noted that the output lacks the rigour expected in academic writing.Zimmermann et al. [62] used ChatGPt to answer several questions based on the title and abstract of research papers and reported that ChatGPT has an accuracy about 70%.[63] used ChatGPT to automate several stages of SLR development process, including generating relevant keywords and phrases for literature search, screening and filtering of studies, and extracting and synthesizing information from research papers.Interestingly, in the synthesizing step, they queried ChatGPT on individual papers, as well as multiple papers.However, they do not report quantifiable results on the performance of ChatGPT.Najafali et al. [64] conducted even more bolder experiments by generating an entire SLR using ChatGPT, however noted that the output has many issues.</p>
<p>Hallucination in LLMs and Factuality in SLRs</p>
<p>The phenomenon of hallucination in LLMs, characterized by the generation of false yet plausible information, presents a critical challenge in the deployment of LLMs [19], with natural relevance for SLRs.This challenge is particularly acute given the stringent requirements for accuracy and reliability in SLR research, where any deviation from factual correctness can significantly undermine the integrity of the review process and its outcomes.Hallucinations can generally be categorized into two types, namely, open-domain and closed domain hallucinations [19].</p>
<p>Hallucinations in open-domain contexts emerge when LLMs produce outputs not directly anchored in specific instances of the training data.This type of hallucination is particularly problematic in the context of SLRs, where the veracity of information is paramount.Strategies to mitigate open-domain hallucinations include enhancing the diversity and representativeness of training datasets, incorporating mechanisms for better context comprehension, and developing techniques for the LLM to recognize and flag potential inaccuracies.Meanwhile, closed-domain hallucinations occur within specific contexts, such as when LLMs generate content that diverges from the source text in tasks like translation or summarization.For SLRs, where synthesis of existing literature must adhere closely to the original texts, such hallucinations clearly pose significant risks.</p>
<p>Reasons for LLMs generating hallucinations is commonly attributed to the presence of false information within the training data itself [65] or to the outdated or temporal nature of the training data which lacks information on recent developments [66].However, these are not the sole contributors to hallucinations.Another factor is the LLMs' training approach to generate tokens sequentially, which can result in generating realistic but ultimately incorrect sequences of text [67].Recently, [19] demonstrated that LLMs inherently produce incorrect facts due to statistical principles rather than their design or data quality, with such errors often corresponding to the rate at which certain unique facts appear in the training data.</p>
<p>RAG for Enhanced Factual Accuracy in SLRs</p>
<p>Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.The theoretical principles underpinning RAG stem from the need to enhance language models with the ability to access and utilize external, structured knowledge [69].This is in response to the limitations of traditional LLMs that rely solely on their pre-trained parameters for knowledge, which can be outdated or incomplete.The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].The incorporation of RAG into the workflow of LLMs, therefore, presents a sophisticated approach to augmenting the model's knowledge base beyond its pretraining, specifically tailored to the demands of the evolving nature of studies in SLRs.By allowing the model to access an external corpus of domain-specific literature the context within which the LLM operates, it attains the ability to be enriched while a critical countermeasure to the model's propensity for generating plausible yet factually incorrect information -hallucination -is mitigated.In the realm of SLRs, where the precision of synthesized knowledge is paramount, RAG's ability to draws upon relevant information from a targeted corpus helps ensure that the generative outputs of LLMs are anchored in verifiable data.</p>
<p>2.4 Advancing LLMs for Specialized Domains: From Pretraining to Fine-Tuning</p>
<p>In training LLMs, they initially undergo pretraining on extensive, diverse datasets, acquiring a foundational grasp of both language and knowledge.This stage equips LLMs with a generalist understanding, however, the leap from generic text generation to specialized domain proficiency requires additional training in the form of fine-tuning which is a process where LLMs are further trained on relevant datasets to align their outputs with new requirements [70].</p>
<p>PEFT: A Paradigm Shift in Finetuning</p>
<p>The evolution of fine-tuning practices, particularly with the advent of Parameter-Efficient Fine-Tuning (PEFT) techniques [71], marks a significant shift towards more sustainable and effective model optimization.PEFT, focuses on updating a selective subset of the model's parameters.This approach contrasts with conventional fine-tuning, where a substantial portion of the parameters is modified, leading to high computational costs.By concentrating updates on strategically chosen parameters, PEFT techniques enhance the model's performance on domain-specific tasks without the need for extensive computational resources, thus making fine-tuning feasible even for the most expansive LLMs [72,73].The integration of PEFT techniques in the fine-tuning process is instrumental in tailoring LLMs to the narrow and specific knowledge requirements fo chosen domains such as those encountered in (SLRs), which would not be captured by the pretrained process of LLMs.</p>
<p>Among the myriad of PEFT techniques, LoRA [74] and more recently NEFTune [75], stand out for their contributions to enhancing LLMs' capabilities.LoRA (Low-Rank Adaptation), refines the model's efficiency by optimizing a small, critical subset of parameters, thus minimizing the computational overhead while maintaining, or even enhancing, the model's performance in domain-specific applications [73].NEFTune on the other hand introduces an innovative approach by incorporating random noise into the embedding vectors during fine-tuning, thereby improving the model's ability to follow instructions and engage in meaningful conversations by enhancing its generalization capacity [75].</p>
<p>Literature summary and gaps</p>
<p>The literature review underscores the potential and gaps in the intersection of SLRs and LLMs.While advancements in text mining and machine learning herald efficiency in literature retrieval and screening, the synthesis phase remains a challenge that has not been sufficiently addressed, and made difficult by the phenomenon of LLM hallucinations and the inability to audit the source information from the LLMs' responses.The review reveals the potential of fine-tuning LLMs with SLR-specific datasets to tailor these models for more accurate knowledge synthesis.However, work is required to ensure that both fidelity of synthesized knowledge and auditing of LLM responses can be achieved.Against this backdrop, this paper proposes an SLR-automation Framework to bridge this gap by leveraging domain-specific fine-tuning of LLMs via the most recent advances in PEFT, on selected SLR papers and targetting the knowledge synthesis stage of SLRs, together with novelties for embedding fidelity and mitigating hallucination in the LLM responses.The viability of the proposed framework is explored and illustrated using a case study that will seek to replicate a PRISMA-conforming SLR as a ground truth against which the proposed framework can be evaluated.</p>
<p>Based on the research gaps in the literature, this study attempts to answer the following research questions:</p>
<p>• (RQ1) Can we leverage LLMs in combination with finetuning and/or RAG in order to facilitate the synthesis phase of an SLR process?</p>
<p>• (RQ2) How can the extraction of finetuning datasets from an SLR corpus be effectively automated?</p>
<p>• (RQ3) Can finetuning of LLMs be effectively conducted on relatively small datasets representing a very narrow domain?</p>
<p>• (RQ4) Can we achieve high fidelity of LLM responses and ensure that we can audit and verify the source information of LLM responses, and what evaluation metrics are suitable?</p>
<p>• (RQ5) Are the proposed SLR-automation approaches viable for replicating a previously published SLR study?</p>
<p>3 Proposed SLR-automation Framework</p>
<p>Our proposed framework for automating SLRs is a sequential process designed to extract a dataset that encapsulates necessary information from SLR-selected papers for the purpose of finetuning LLMs in order to facilitate the knowledge synthesis phase of the SLR process.Figure 2 outlines the proposed four-step process.</p>
<p>Figure 2: Overview of the proposed SLR-automation Framework for evidence and knowledge synthesis.</p>
<p>Step 1 -Paper Selection and Research Question Definition</p>
<p>The framework initiates with a dual focus on SLR paper selection and research question articulation which is covered by the planning, selection and extraction stages of the SLR process (Figure 1 Steps 1 to 3).Once the target papers for an SLR are identified and the overarching and supporting research questions for the study have been defined, they both then serve as inputs to the next Step 2, and direct the automation process of data extraction and processing.</p>
<p>Step 2a -Automated Q&amp;A Data Extraction and Synthesis</p>
<p>In this step, we developed a program 11 that leverages the downstream summarization as well as the question and answering capabilities of an advanced LLM to extract a dataset from all the papers.This program systematically processes each selected paper, using the chosen LLM's summarization capabilities to extract the core content of primary paper sections and subsections into concise summaries alongside the papers' meta-data.From these distilled summaries, the program then uses an LLM to automatically generate a series of domain-relevant questions that are common denominators and can be answered by all studies.This set of questions is then used to interrogate each paper and extract answers via a chosen LLM, and the combination of the two forms question and answer (Q&amp;A) pairs that represent our finetuning dataset.Simultaneously, we enable the customization of domain-specific, researcher-defined questions informed and motivated by the objectives and aims of an SLR being undertaken.These questions are tailored to the unique contours of the study's domain and are formulated to draw out information that is particularly relevant for the SLR's purposes.This dual strategy ensures that while our approach remains systematically consistent across the majority of academic disciplines, it is also flexible enough to accommodate the specialized requirements of the academic field under investigation.</p>
<p>With both sets of questions, we are then able to automatically generate paper-level Q&amp;A pairs that encapsulate the insights of individual studies at different levels of detail and granularity.To achieve a synthesis that spans the corpus of all studies under investigation, we further collate key summaries from across all papers and from these, generate additional SLR-level Q&amp;A pairs.These pairs are designed to provide responses that are relevant across the entire body of selected works, effectively offering a synthesized narrative that captures the collective information of the SLR papers.Through this process, we ensure that the resulting Q&amp;A dataset is not only rich in detail and precision at the level of individual papers but also possesses the breadth and depth required to reflect the evidence synthesis of a systematic literature review.</p>
<p>Step 2b -Token Insertion as New Knowledge Markers</p>
<p>The incorporation of distinct knowledge markers, referred to as unique tokens, into the fine-tuning process of LLMs is a theoretical construct designed to create explicit linkages between the model's augmented neural network parameters and the new knowledge encapsulated from the SLR papers.The motivation behind this approach is to achieve a higher degree of fidelity and precision in the model's output during inference.When an LLM is fine-tuned with additional data, it assimilates this new information by adjusting a small percentage of its existing parameters while adding a larger set of new parameters.Without a mechanism to distinguish between pre-existing knowledge acquired from pretraining and newly acquired information, the model risks providing responses based on its pretraining rather than the domain-specific insights gained from the subsequent fine-tuning step.Since it is imperative that during inference time, the model does not access facts from its knowledge base developed during pretraining, and instead, only responds with facts acquired during finetuning, an explicit mechanism is needed to ensure this occurs.Without such mechanisms, the results of the SLR risk being contaminated with knowledge that is outside of the scope of the selected papers, thus, invalidating the research.In essence, the aim of the proposed approach is to insulate the LLMs responses from its pretrained knowledge but to nonetheless leverage its natural language capabilities that it gained during pretraining.</p>
<p>Therefore, these tokens or markers serve as signposts within the neural network, directing the model to access and utilize the parameters associated with the SLR data and facts when generating responses.It also enables the model to explicitly state the source information for its responses so that the provenance of information can be audited.This approach provides a method for researchers to verify that the output is indeed based on the recent fine-tuning, enabling a level of interpretability and trust in the model's responses that is critical for academic rigor and reliability and represents a strategic advancement in using LLMs for SLR.</p>
<p>Step 2c -Q&amp;A Permutation</p>
<p>As part of Step 2, to ensure that the LLMs can handle a variety of linguistic expressions during inference time, we introduce permutations in the Q&amp;A pairs.For each original question, we create several semantically equivalent variations.Since accurate recall of facts is the primary objective of the system rather than style, linguistic quality, and variability of the text, we retain the same answer for each permutation of the corresponding questions.This step is critical for training the LLMs to recognize and correctly respond to different phrasings of the same question which different users are likely to generate.</p>
<p>Step 3 -Finetuning and RAG</p>
<p>In the third step of our SLR-automation framework, we propose specific finetuning methods for LLMs, complemented by the integration of RAG processes.To enhance the finetuning efficiency, we propose employing PEFT techniques [71], which modify only a subset of the model's parameters.This is crucial, as fine-tuning all parameters of large LLMs is computationally prohibitive and may not yield proportionate improvements in performance.PEFT techniques offer a balance, concentrating the model's learning capacity on those parameters that most significantly influence the domain-specific tasks, thereby optimizing both resource expenditure and model proficiency.RAG, on the other hand, serves as an augmentative mechanism during inference.It dynamically retrieves and incorporates information from the SLR corpus into the LLM's generative process, potentially providing an additional layer of contextual support.This optional step may ensure that the model's output is not solely reliant on internalized patterns but is also substantiated by direct data references, thereby potentially enriching the depth and factual accuracy of the synthesized content and responses.</p>
<p>Step 4 -Verification</p>
<p>The final phase of the proposed framework tests the framework to ensure that it has the ability to accurately generate responses with respect to the selected SLR papers.The test dataset is extracted from the main Q&amp;A finetuning dataset and withheld from the finetuning process.An automated process is subsequently initiated after finetuning is completed which tests the LLMs ability to answer the selected questions correctly using metrics that are specifically designed for factual validation.</p>
<p>Methodology</p>
<p>The methodology covers details of the chosen Gold Standard SLR as a use case for replication and validation of the proposed framework.It compasses details on the dataset extraction and preparation for LLM fine-tuning, the experimental design outlining the setup of LLM technologies, selection of LLM types, detailed RAG implementation, finetuning hyperparameters, and hardware setup.The evaluation measures are discussed which cover quantitative metrics and qualitative analyses to assess the information recall.</p>
<p>Use Case: Replication of an Existing SLR</p>
<p>We selected a peer-reviewed and published PRISMA-SLR within the domain of learning analytics as the Gold Standard, with the goal of investigating whether the proposed SLR-automation framework has the capability to generate responses from which a manually conducted SLR can be replicated.The target SLR paper is titled "Learning Analytics Dashboard: A Tool for Providing Actionable Insights to Learners" [76].This SLR is contemporary, being aligned with the recent advancements in learning analytics dashboards, and was chosen for its adherence to the PRISMA guidelines as well as due to authors' familiarity with the work.The SLR itself focused on aspects of advancements in educational technology, delving into the impacts of learning analytics dashboards (LADs) on university student outcomes, scrutinizing the technological underpinnings, the analytics methodologies most commonly employed, and the demographics of the study cohorts used in trials.Furthermore, it examined the existing gaps and posited future trajectories for research in this field.This SLR encompassed 17 papers.Its selection was strategic not only because it aligned with the expertise of this study's authors which assisted in guaranteeing the veracity of LLM responses, but also offered an insight into the viability and effectiveness of finetuning LLMs on relatively small datasets.</p>
<p>Automated Dataset Extraction and Preparation</p>
<p>Step 2 of the proposed framework in Figure 2 was fully automated in order to generate a finetuning dataset.A software tool was built using Python to systematically parse the 17 PDF files representing the SLR papers and generate Q&amp;A pairs for the finetuning dataset.To begin with, this tool extracted the metadata from each paper via OpenAI's GPT-4 API calls and before data generation commenced, it created a high-level summary of each paper.The summaries of each paper were then collated, and GPT-4 was tasked with generating a set of questions that were relevant across all the papers.This produced a vehicle for automatically extracting domain-specific information that was common across all the studies.These questions were then augmented with a further set of researcher-defined questions that reflected the narrower aims of the SLR study and were closely aligned to the objectives and overarching questions of the SLR being conducted.Once this broad set of questions was collected, GPT-4 was used via API calls to generate answers to each one from every study.These initial Q&amp;A pairs we refer to as paper-summary-level data points.These data points captured high-level paper summaries that were relevant to both the research domain and the aims of the SLR study.</p>
<p>The next phase involved the generation of another set of Q&amp;A data points based on data from individual studies at different levels of granularity.In this phase, our processing divided each paper into chunks 12 , and asked GPT-4 to directly generate Q&amp;A pairs based on the content of each chunk.This method yielded another dimension of information from each paper that reflected some more distinct aspects of each study alongside, like their expanded and more detailed contributions, idiosyncratic methodological details, key findings, and future research directions.This process was then repeated with even smaller chunks of text which roughly represented individual paper paragraphs.This more detailed approach allowed for the extraction of even more granular information from each study in the form of Q&amp;A pairs, revealing yet finer details of the methodologies, analyses, and discussions presented in the studies.By dissecting the papers at a paragraph level, this method augmented the depth of the overall finetuning dataset with a more detailed set of Q&amp;A pairs.This layer, we collectively refer to as paper-level Q&amp;A pairs that offer a more granular insight into each study compared to paper-summary-level data points.</p>
<p>While the above strategy generated data points that would enable researchers to "talk to" the content of each paper, the subsequent strategy aimed to allow researchers to "talk across" all the studies simultaneously.These Q&amp;A pairs we refer to as SLR-level data points which served the with the aim of facilitating the answering of questions that relate to the synthesis of knowledge across all the studies under investigation.From the implementation perspective, this second layer of data extraction involved collating answers from each paper that were generated from questions used for creating paper-summary-level Q&amp;A pairs.Then, Q&amp;A pairs were generated via GPT-4 based on all the collated answers.By combining paper and SLR-level Q&amp;A pairs, our finetuning dataset not only facilitated equipping LLMs with in-depth facts from singular studies, but also attempted to make possible LLM prompts that support queries requiring a comprehensive synthesis of knowledge and evidence across the entire SLR corpus, achieving both the breadth and depth needed to support a systematic study of a corpus of literature and to answer specific research questions.</p>
<p>The initial training dataset following the data extraction and Q&amp;A pairs generation comprised 1503 samples and of these, approximately 28% were paper-summary-level, 49% were paper-level extracted from larger chunks, 31% were paper-level extracted from paragraphs, and 2% from the SLR-level.To augment the robustness of the finetuned LLMs during inference, we implemented a novel refinement step: for each original question, we generated ten alternative formulations.These variants were designed to be semantically diverse yet structurally aligned, ensuring that despite the variability in phrasing, the essence of the questions remained intact, eliciting the same precise response which resulted in producing a final training dataset of 16423 samples.This strategic augmentation served a dual purpose.Firstly, it aimed to significantly enhance the model's ability to comprehend and respond accurately to a broader spectrum of question formulations from different users, mirroring the diverse linguistic styles encountered in real-world academic discourse.Secondly, it aimed at increasing the size of the dataset.For the test dataset, we generated three additional permutations for each unique question to arrive at a final dataset of 4962 data points, where the distribution of paper-level and SLR-level questions was the same as that in the training dataset.Table 1 demonstrates an example of a text summary (answer), for which 11 different but semantically equivalent questions were generated using GPT-4 for the training dataset and an additional three for the test dataset.</p>
<p>Answer Questions</p>
<p>The In what capacity do the study's findings shape the forthcoming strategies for research and development across learning analytics and educational technology?13.How might the conclusions drawn from the paper steer the future of innovation and inquiry in the spheres of learning analytics and educational technology?14.What influence do the outcomes of the paper exert on the trajectory of upcoming investigations and advancements in learning analytics and educational technology?</p>
<p>Table 1: Example of the dataset permutation step for questions relating to an answer from one specific paper in the target SLR study.</p>
<p>Dataset Format and Characteristics</p>
<p>As part of the dataset processing in Step 2 in the proposed framwork in Figure 2, a specific format to the structure of the Q&amp;A pairs was enforced, and unique tokens were inserted into each Q&amp;A data point.For clarity, an illustrative example of a Q&amp;A data point from the dataset can be seen as follows:</p>
<p>{ "instruction": "According to the 2023SLR dataset, in the aljohani2019integrated paper, how does the paper conceptualize Learning Analytics Dashboards (LADs) within the framework of digital learning environments?","output": "In the data used for the 2023SLR, the paper conceptualizes Learning Analytics Dashboards (LADs) as a knowledge and learning management oriented infrastructure for the end-to-end management and consumption of educational data within digital learning environments.It emphasizes the integration of student-centered data as a critical challenge for personalized curricula and learning experiences.Source: aljohani2019integrated" } This data point exemplifies the dataset's innovative use of unique tokens included in the post-processing step that embeds specific knowledge markers within the fine-tuning dataset which is crucial for enabling the model's capacity to differentiate between newly acquired information and its pre-existing knowledge base from pretraining.The example token '2023SLR' serves as a corpus-wide identifier, linking this and all the data points to the broader SLR dataset, while in this example, the 'aljohani2019integrated' token operates as a granular paper-level, tethering the Q&amp;A pair to a particular study within the corpus.The inclusion of a source citation at the end of each output ('Source: aljohani2019integrated') is a deliberate design choice, ensuring that each response generated by the LLM during inference can be sourced and traced back to its originating study and verified if needed, thus enhances the accountability of the model's outputs.</p>
<p>LLM Selection and Finetuning Strategy</p>
<p>In the experimentation of the proposed framework, the Mistral-7B base model [77] was selected as the foundation for fine-tuning due to its efficiency and high benchmark performance, particularly in English language tasks.This 7.3 billion parameter model leverages advanced techniques such as Grouped-query attention and Sliding Window Attention, enabling it to outperform larger models across a variety of benchmarks, including commonsense reasoning, making it an ideal candidate for fine-tuning to suit our specific SLR automation needs.Conversely, the Mistral-7B-Instruct variant, already fine-tuned for instructional tasks, was employed in conjunction with the RAG approach to leverage its capabilities in handling Q&amp;A instructional content.The Instruct model's demonstrable adaptability and performance on tasks like MT-Bench, coupled with its ability to generalize across tasks, provided an appropriate candidate for generating accurate and contextually relevant responses in our SLR framework experimentation.</p>
<p>Finetuning implementation</p>
<p>In the fine-tuning of the LoRa and NEFTune LLM models we focused on calibrating a selection of the hyperparameters that are presented in Table 2, with all others set to their default values.</p>
<p>RAG implementation details</p>
<p>In our study, we implemented two RAG systems, leveraging the Weaviate vector database (version 1.23.9) and the OpenAI Embedding model 'text-embedding-3-large' 13 for pre-retrieval processes.For the post-retrieval stage, the initial model used was OpenAI's 'gpt-3.5-turbo'.The pre-retrieval phase involved optimizing the indexing process by chunking data using LLMs, focusing on contextually relevant segments.The vector database was structured with the attributes 'text' and 'source', where 'text' was searchable and 'source' was filterable.During the retrieval phase, we utilized Langchain's self-querying retriever for its simplicity 14 .In the post-retrieval process, we employed an aggregation prompt template designed to integrate the retrieved context into a coherent and concise answer, limited to three sentences for any given question.</p>
<p>Experimental Design</p>
<p>We experimented with five different combinations of finetuning and RAG in order to determine what the most effective approaches could be for the SLR-automation process.Our experimental setup investigated the following methodologies:</p>
<ol>
<li>
<p>Baseline: Evaluate the Mistral-7B-Instruct on its knowledge and ability to answer SLR-related test dataset questions.</p>
</li>
<li>
<p>Fine-tuning LLMs using LoRA: Leveraging Low-Rank Adaptation for fast and efficient parameter adjustment.</p>
</li>
<li>
<p>Fine-tuning LLMs using NEFTune: Introducing noise into embedding vectors to investigate effects on generalization improvements.</p>
</li>
<li>
<p>Instruct LLM + RAG with Raw Articles: Combining LLMs with Retrieval-Augmented Generation, using unprocessed article text as the retrieval corpus.</p>
</li>
<li>
<p>Instruct LLM + RAG with Auto-Extracted Data: Employing RAG with a knowledge base of automatically extracted data comprising the finetuning dataset for focused information retrieval.</p>
</li>
<li>
<p>Best Finetuned LLMs + Best RAG Solution: Integrating the top-performing fine-tuning and RAG methods to optimize SLR automation.</p>
</li>
</ol>
<p>Each method also summarised in Table 3, was evaluated for factually correct answers with respect to the SLR dataset.</p>
<p>Method Description</p>
<p>Baseline Mistral-7B-Instruct Establish the degree of pretraining and instruct-tuning knowledge within the model LoRA Finetuning</p>
<p>Fine-tuning with Low-Rank Adaptation NEFTune Finetuning Fine-tuning with Noise-Enhanced Fine-Tuning Instruct LLM + RAG (Raw)</p>
<p>Retrieval-Augmented Generation with raw articles Instruct LLM + RAG (Extracted) RAG with auto-extracted data Best Combination</p>
<p>Best fine-tuning + RAG approach</p>
<p>Evaluation</p>
<p>Our evaluation necessarily centered around assessing the factual accuracy of responses generated by the various methodologies applied to the SLR dataset.This assessment was conducted through a combination of quantitative and qualitative analyses, utilizing test sets.</p>
<p>Test Dataset</p>
<p>For the quantitative analysis, we employed an 80/20 train/test split.The qualitative test set comprised 11 gold standard answers, which were derived from key SLR-level findings across the 17 papers within the target SLR paper.These findings represented the core contributions of the SLR study and were articulated into Q&amp;A pairs.</p>
<p>Quantitative Metrics</p>
<p>The quantitative evaluation was grounded in two metrics, each devised for its relevance to the evaluation of factual accuracy in the context of SLR automation.The first was FEVER [78], whose usage is consistent with other studies [79,80] concerning factual text evaluations.The next was a variation on FEVER which we refer to as the Consistency Grading Scale (CGS) that offers a more granular analysis of the alignment between response texts and reference materials.</p>
<p>FEVER (Fact Extraction and VERification):</p>
<p>The FEVER metric was a key evaluation framework used in conjunction with GPT-4 to automate the testing.FEVER scoring is designed to assess the factual accuracy of generated responses by cross-referencing the LLM-generated responses against ground truth responses which were both submitted to GPT-4 to assess.The complete GPT-4 prompt can be seen in Appendix B. FEVER scoring determines the veracity of the claims, categorizing them into distinct labels for clarity.The labels used in the standard FEVER evaluation are:</p>
<p>• UPPORTED : The claim is directly corroborated by the evidence in the ground truth test set.</p>
<p>• REFUTED: The claim is directly contradicted by the evidence in the ground truth test set.</p>
<p>• NOT ENOUGH INFO:</p>
<p>There is insufficient evidence in the ground truth test set to either support or refute the claim.</p>
<p>Consistency Grading Scale (CGS):</p>
<p>Building upon the foundational principles of FEVER, CGS introduces a more granular continuum for evaluating the fidelity of information presented by LLM responses in the context of systematic literature reviews (SLRs).The CGS was also used in conjunction with GPT-4 in order to derive the assessments, where the complete prompt can be seen in Appendix B. This grading scale quantitatively assesses the alignment of generated responses with verified reference texts, offering a spectrum of consistency levels ranging from complete contradiction to full support.The CGS is defined as follows:</p>
<p>• Fully Consistent (2): The claim in the response text is unambiguously corroborated by the evidence in the ground truth, with complete agreement in the source information.• Partially Consistent (1): The response text is generally aligned with the reference material; however, minor discrepancies or inadequacies in detail, accuracy, or source alignment are evident.• Not Enough Info (0): The available evidence is insufficient to substantiate or refute the claims made in the response text, indicating a gap in information.• Partially Contradictory (-1): While some aspects of the response text may align with the reference material, significant contradictions or factual inaccuracies are present.• Contradictory (-2): The response text is in direct and total opposition to the evidence presented in the ground truth, indicating clear factual inaccuracies.</p>
<p>The FEVER evaluation comprises the calculation of the percentages of responses labelled as either SUPPORTED, REFUTED, or NOT ENOUGH INFO.Meanwhile, for the CGS evaluation, a mean is calculated across all the values across every response to derive a value that ranges from -2 (totally inaccurate) to 2 (perfectly accurate).GPT-4 was leveraged for automating the evaluations against the criteria defined above which is a consistent approach with previously published studies [81,82].Prompts for both CGS and FEVER were empirically developed and refined by iterative testing against GPT-4 in order to ensure that correct grading responses were being generated.The prompts can be seen in Appendix B. In order to establish the validity of the devised evaluation metrics, 100 samples from the test dataset were sampled and inter-rater reliability was analyzed between the responses of two human evaluators and GPT-4 for both CGS and FEVER metrics, whose results are presented in the subsequent section.</p>
<p>Qualitative Analysis</p>
<p>The qualitative component of our evaluation framework was designed to gauge the depth and relevance of the synthesis provided by the LLM-generated responses.An author from this study who is a domain expert and well-versed in the details of the subject matter was enlisted to perform a comprehensive review of each response.Their analysis was centered on determining if the response fell into one of the two categories: Supports and Does Not Support.These categories determined the extent to which the responses could encapsulate key synthesis information from the aggregated SLR studies which enable the core conclusions and findings of the benchmark SLR paper to be replicated.</p>
<p>• Supports: This category was reserved for responses that demonstrated an accurate presentation of the SLR content, effectively summarizing all the key points, methodologies, results, and conclusions.The responses classified under this category were those that provided comprehensive synthesis information, enabling the researcher to completely replicate the key findings of the target SLR study as reported in the published paper.</p>
<p>• Does Not Support: Responses falling into this category were those that failed to capture all the details necessary for the replication of the key findings of the target SLR study.These responses may include inaccuracies or omissions in key details.</p>
<p>Each response was manually evaluated against these categories, with a subject expert providing detailed feedback on the fidelity of the responses.To ensure the objectivity and reliability of the qualitative analysis, the gold standard Q&amp;A pairs used as benchmarks were kept confidential from the team responsible for the LLMs' development and training.</p>
<p>Results</p>
<p>Our analysis of results first considers the validity of the CGS and FEVER metrics, and then compares the performances of all the proposed approaches.A correlation analysis is then provided based on the responses of all methods, followed by a more detailed analysis of the best-performing approach.</p>
<p>Validation of evaluation method</p>
<p>We begin first by establishing the validity of the devised evaluation metrics.Tables 4 and 5 show the correlation matrices analyzing the inter-rater reliability among the two independent human evaluators' (H1 and H2) scores as well as that of GPT-4, showing the utility and limitations of both CGS and FEVER within the context of factuality assessment.</p>
<p>Rater GPT-4 H1 H2 GPT-4 1.0 H1 0.647 1.0 H2 0.739 0.669 1.0 The CGS metric displays moderate to strong correlations, particularly with the H2 rater exhibiting a robust 0.74, suggesting a substantial concurrence with the GPT-4 evaluations.H1 demonstrated a moderate agreement with a correlation of 0.65 with GPT-4.These figures are noteworthy, given the subjective nature inherent in qualitative evaluations.The inter-rater reliability, represented by a correlation of 0.67, although not indicating perfect agreement, nevertheless reveals a meaningful consensus between the human raters that overall affirms the CGS as a reliable tool within its application scope.Contrastingly, the FEVER metric was characterized by lower correlation values and indicated a more modest agreement between evaluators -which is a reflection of the challenges faced in calibrating subjective analytical tools in general.With H1 at 0.49 and H2 at 0.6 correlation with GPT-4, there was discernible inter-evaluator consonance, albeit less pronounced than that within the CGS metric.The inter-rater correlation of 0.56, while significant, underscored the need for further evaluator calibration to mitigate interpretive disparities.Upon a deeper investigation into the divergences, the results showed that there were significant differences in the manner in which the human raters evaluated the 'NOT ENOUGH INFO' category with respect to each other and GPT-4, with both human raters underrating this category and preferring either the SUPPORTED or REFUTED categories instead.Thus, while the FEVER metric offers meaningful insights into the veracity of claims for the analysis in this study, its application necessitates a more critical interpretation.For the purposes of this study, CGS can be relied upon to provide a broader spectrum of consistency in evaluation, and FEVER serving as a barometer for broader factual accuracy.</p>
<p>Quantitative analysis accuracies</p>
<p>The quantitative outcomes, encapsulated in Table 6, show the performance across REFUTED, NOT ENOUGH INFO, and SUPPORTED categories, thereby offering a granular perspective on the factual alignment of each method.All results are rank-ordered with respect to the SUPPORTED category.We first draw attention to the results of the Baseline method which represents responses from the unmodified Mistral-Instruct LLM and indicates the potential existence of any SLR-relevant prior knowledge arising from its pretraining and instruct-tuning processes, while also serving as a benchmark to determining the degree to which proposed approaches have succeeded in achieving an improvement.The baseline indicates the ability to correctly respond to 14.5% of the test data, with the majority of the responses being categorised as NOT ENOUGH INFO.We see next that the NEFTune method emerged as the most effective, with 89.2% of its responses being SUPPORTED, indicative of its strong factual accuracy and alignment with the actual SLR corpus.LoRA, with an 87.7% 'SUPPORTED' rate, also demonstrated significant reliability, albeit marginally lower than NEFTune.In contrast, the integration of RAG with a fine-tuned dataset (RAG + FD) presents a discernible shift in distribution across all three categories, with a notable increase in 'NOT ENOUGH INFO' outcomes and a decrease in 'REFUTED' responses compared to NEFTune and LoRA.The combination of NEFTune and RAG + FD, however, exhibited a clear reduction in factual accuracy, marked by a SUPPORTED rate of 63.2%.The sharply elevated NOT ENOUGH INFO and REFUTED rates may reflect the inability of the finetuned LLM to integrate additional text from the RAG process.The low RAG (raw) outcomes underscored the challenges inherent in leveraging unmodified RAG approaches and basic methodologies in achieving accurate retrieval for the purpose of synthesis of SLR-specific information that was generated during the data extraction process.Table 7: Description of the CGS results in rank-order according to the 'Fully consistent' values.The absolute number and the percentage of responses according to each category are listed.The numeric values are aggregated and listed as means ranging from 2 (perfectly correct) to -2 (entirely incorrect) as well as the percentage with respect to the highest possible score.</p>
<p>Correlation analysis between responses of all approaches</p>
<p>The correlation analysis between the responses of various LLM methodologies is presented next in order to explore similarities and divergences in the LLM responses that may offer insights complementary nature of each where there is evidence of them capturing different aspects of the data, leading to the potential of combining them to achieve better accuracies.The correlation matrix represented in Table 8 offers a refined perspective on the ordinal associations between different LLM FEVER responses.</p>
<p>A positive k value between NEFTune and LoRA indicates a moderately similar ranking order in their response evaluations.Conversely, a near-zero k value between NEFTune and the baseline method suggests an independent performance trend, underscoring NEFTune's distinct methodological improvements over the benchmark.The correlations between responses of all other methods were not significant.We next examine and compare responses of the top three best-performing approaches on two sample test questions from FEVER, showcasing an example where each method's response was classified as NOT ENOUGH INFO and REFUTED.With respect to the NOT ENOUGH INFO responses in Table 10, we find variances in detail and emphasis.</p>
<p>Similar to the previous table, the CGS correlation</p>
<p>In response to the question asking what manner does the dashboard contribute to the advancement of virtual learning, the reference text highlights the dashboard's role in managing lab activity schedules and resource usage on a shared AWS account, focusing on logistical aspects.In contrast, LoRA and NEFTune responses pivot towards the dashboard's educational support, emphasizing feedback, self-regulation, and progress monitoring, with NEFTune slightly touching on resource management but not to the extent mentioned in the reference.RAG + FD, while partially aligning with the reference by discussing resource usage management, also diverged by emphasizing feedback and activity completion, omitting the specific detail about scheduling.Thus, none of the responses fully encapsulated the reference's emphasis on time slot management and resource usage, illustrating a shift from logistical to educational functionalities in their interpretations.In these instances the classification of NOT ENOUGH INFO for all responses was correct.</p>
<p>Text</p>
<p>Question</p>
<p>According to the 2023SLR dataset, in the naranjo2019visualdashboard paper, in what manner does the dashboard contribute to the advancement of virtual learning?Reference text</p>
<p>In the data used for the 2023SLR, for online instruction, the dashboard helps in identifying preferred time slots for students to engage in lab activities, given the self-paced nature of the instruction.This can aid in managing and preventing soft limits being reached due to concurrent resource usage in a shared AWS account by multiple students.Source: naranjo2019visualdashboard NEFTune</p>
<p>In the data used for the 2023SLR, the dashboard contributes to the advancement of online education by providing a visual tool that helps teachers and students monitor the progress of lab activities and the usage of resources in a hands-on learning approach.It attends specifically to learning metrics, supporting teacher interventions and fostering self-regulation among students.The dashboard also aids in scaling education by offering insights into student engagement and progress, potentially enabling the automation of evaluation processes and enhancing the learning experience.Source: naranjo2019visualdashboard LoRA</p>
<p>In the data used for the 2023SLR, the dashboard contributes to virtual learning by providing timely and targeted feedback to students on their work, which supports self-regulation and learning about their own learning process.The dashboard's visual representation of learning activities and progress helps to engage students and increase their motivation.Additionally, by offering teachers a centralized platform for monitoring student progress, the dashboard facilitates timely intervention and personalized support.Source: naranjo2019visualdashboard RAG + FD</p>
<p>The dashboard contributes to the advancement of virtual learning by providing timely and targeted feedback on student work, allowing students to discriminate between completed parts of the hands-on labs and those still pending, fostering self-regulation and completion of activities.It also helps in managing and preventing soft limits being reached due to concurrent resource usage in a shared AWS account by multiple students.Meanwhile, with respect to the examples of REFUTED responses for the same question which requested details of a specific study's insights on how the learning analytics dashboard (LAD) drove the motivation among students in Table 11, we see that LoRA and NEFTune produced identical responses and closely match the reference text.However, while they correctly reported most details, they omitted the detail that intrinsic motivation actually decreased in both groups, as noted in the reference text.RAG + FD introduced a claim not present in the reference text, suggesting that LADs lead to a higher academic performance evidenced by higher final grades.Similar to LoRA and NEFTune, the response mentioned the increased extrinsic motivation in the treatment group and the lack of significant difference in changes in intrinsic motivation between the groups.However, it also omitted that intrinsic motivation decreased in both groups over time.Again, in these instances the FEVER classification of REFUTED derived by GPT-4 was correct for all responses.</p>
<p>Text</p>
<p>Question</p>
<p>According to the 2023SLR dataset, in the fleur2020motivation paper, could you detail the study's insights on how the LAD drives motivation among students?Reference text</p>
<p>In the data used for the 2023SLR, the study found a significant interaction effect between group (treatment/control) and time (begin/end of the course) for extrinsic motivation, with the treatment group showing increased extrinsic motivation at the end of the course, while it decreased in the control group.However, intrinsic motivation decreased in both groups over time, with no significant difference between the groups.Source: fleur2020motivation NEFTune</p>
<p>In the data used for the 2023SLR, the study found a significant interaction effect between group (treatment/control) and time (beginning/end of the course) for extrinsic motivation, with the treatment group showing increased extrinsic motivation at the end of the course compared to the control group.However, there was no significant difference between the groups in terms of changes in intrinsic motivation.Source: fleur2020motivation LoRA</p>
<p>In the data used for the 2023SLR, the study found a significant interaction effect between group (treatment/control) and time (beginning/end of the course) for extrinsic motivation, with the treatment group showing increased extrinsic motivation at the end of the course compared to the control group.However, there was no significant difference between the groups in terms of changes in intrinsic motivation.Source: fleur2020motivation RAG + FD</p>
<p>The study by Fleur et al. (2020) found that LADs can significantly increase extrinsic motivation among students, leading to higher academic performance.The treatment group showed increased extrinsic motivation at the end of the course compared to the control group, as evidenced by higher final grades.However, there was no significant difference between the groups in terms of changes in intrinsic motivation.</p>
<p>Table 11: Example of responses REFUTED FEVER responses across top three methods.</p>
<p>Detailed Analysis</p>
<p>For the best-performing model from the above results, Tables 12 and 13    The analyses of CGS and FEVER metrics, taken conjointly, paint a comprehensive overview of NEFTune's strengths.While the model's performance is robust in factual replication at paper-level Q&amp;As, the performance on SLR-level data points suggests an area for improvement in either the calibration of the FEVER metric, or in optimizing the learning for these data points by increasing the size of the finetuning dataset for these samples.However, it should be noted that despite the lower accuracies and a reduced level of confidence in the model's outputs on the SLR-level questions, the SLR-level questions can nonetheless be verified and accurately reconstructed from more accurate paper-summary-level responses.</p>
<p>Qualitative analysis accuracies</p>
<p>The qualitative evaluations conducted manually across the eleven NEFTune LLM responses to questions about key SLR findings in the target paper representing the ground truth, reveal a high level of fidelity and alignment between the reference and the LLM's responses.An example of a typical response can be seen in Table 14.The analysis showed that in eight out of eleven cases, the key findings from the SLR ground truth could be faithfully extracted and reconstructed from the responses provided by the finetuned LLM's responses.However, in two instances, a deeper understanding of certain domain-specific concepts such as prescriptive analytics and model interpretability within the context of learning analytics was crucial to be able to arrive at ground truth conclusions from the LLM responses.These instances highlighted a divergence in the conceptualization of certain technical terms between the LLM and the SLR study, necessitating domain expertise to align the LLM's broader interpretations with the SLR's findings.Nonetheless, it can be concluded that in 90% of the cases, the ground truth findings from the SLR study are supported by the LLM's responses.However, there was one notable LLM response that introduced some inaccuracies in its response concerning the number of participants across all the studies in 17 papers.It was only in his response that a deviation was observed and in this instance, it could be confirmed that the LLM response did not support the ground truth findings in the SLR study.Example responses from the qualitative analysis can be seen in Appendix A.</p>
<p>Qualitative evaluation</p>
<p>Response and analysis</p>
<p>Question</p>
<p>According to the 2023SLR dataset, in the 2023SLR papers, were the reported dashboards (LADs) developed mostly in a pilot study state, or a prototype for academic purposes, or were they placed into production?</p>
<p>Analysis</p>
<p>LLM's answer can be seen as an expanded version of the ground truth, maintaining the core assertion while adding context and detail.Therefore, the LLM's response can be useed to reconstruct the ground truth for the SLR with a high degree of completeness and accuracy as it includes all the essential information of the ground truth, while extending it with specific examples and a broader examination of the LADs' development stages.The analyses consistently demonstrate that the finetuned LLM's responses tend to provide a broader and more detailed perspective than ground truth which is often more concise and sometimes more definitive in its assertions.Below is a synthesis of the overarching patterns observed:</p>
<ol>
<li>
<p>Expansion and Detailing: The LLM frequently expands upon the information provided in ground truth introducing additional context, examples, and details that were not explicitly mentioned in the ground truth.This pattern is evident in analyses where the LLM's responses offer a deeper dive into specific studies, technologies, and outcomes associated with LADs, enhancing the understanding of the subject matter beyond the baseline established by ground truth.However, the additional information and details are correct and do not indicate hallucination.</p>
</li>
<li>
<p>Alignment with Core Assertions: Despite the additional details, the LLM's responses generally align with the core assertions or findings of ground truth.This alignment indicates that the finetuned LLM successfully captures the essence of the ground truth findings, while adding value through elaboration and exemplification.</p>
</li>
<li>
<p>Discrepancies in Emphasis and Scope: Some discrepancies arise primarily from differences in emphasis and scope.For instance, while the ground truth response might highlight a lack or absence of certain features or trends within the LADs research, the LLM occasionally points out exceptions or minor trends that counteract these broad strokes.These discrepancies do not necessarily contradict ground truth but rather suggest a more variegated picture, which can give the researcher more scope for expansion in an academic paper.</p>
</li>
<li>
<p>Reconstruction and Completeness: The degree to which the ground truth can be reconstructed from the LLM's responses is generally very high, reflecting the LLM's ability to both mirror and in many responses, extend the ground truth in the target paper.In cases where the LLM's detailed accounts align closely with ground truth, the reconstruction ability is high, indicating a strong corroboration of ground truth by the LLM.</p>
</li>
</ol>
<p>Discussion</p>
<p>This study has marked a pivotal shift towards advancing the use of AI and LLMs for automating systematic literature review methodologies, with a particular focus on the knowledge and evidence synthesis phase.The LLM finetuning approaches and the novel data extraction processes make a novel contribution to academic research practices, with even broader applications to other domains that require syntheses across multiple documents with the requirement of supporting question and answering tasks.This study has demonstrated that indeed modern PEFT finetuning approaches like NEFTune and LoRA can be used for finetuning effective LLMs (RQ1), and the proposed automated approach for extracting finetuning datasets from a corpus of selected academic papers can successfully construct finetuning datasets (RQ2) that support question and answer downstream tasks necessary for executing the final stages of an SLR.The study also affirms that the LLMs can be effectively finetuned on relatively small datasets (RQ3) that represent relatively narrow domains associated with the focused objectives of SLR studies.A significant set of challenges that this study addressed and provided solutions for, concerned the broader problem of LLM hallucination and ensuring that the LLM's responses were solely based on the corpus of the selected academic papers, and how response provenance and the factuality of the responses could be evaluated (RQ4).We devised a novel token-based approach that enabled us to audit and verify the source information of the LLM responses.Meanwhile, we devised evaluation metrics for factuality and demonstrated how they can be effectively automated with the assistance of commercial LLMs for efficiency.Finally, the entire proposed framework for automating the final stages of an SLR was validated for viability by replicating an actual published SLR study, using it as a benchmark and source of ground truth.Our experiments indeed confirm the reliability of the proposed finetuning framework for its fidelity in replicating findings from an SLR study (RQ5).</p>
<p>The contributions from this study fill both an existing gap in the literature and a real need in academic research for automating SLR processes during the knowledge synthesis phase since most efforts have focused on using AI and machine learning to solve tasks such as paper searchers and filtering.While an increasing number of AI technologies are emerging to support academic research in the form of features enabling researchers to "talk to" individual papers, they often have limitations and one of the key limitations is the inability to "talk across" multiple papers with high reliability and the ability to track the sources of all responses.These limitations and the lack of control over the existing commercial solutions raise issues of trust as well as replicability, which this study overcomes.</p>
<p>The solutions demonstrated in this study are not without their imperfections, but future developments can overcome them.The first involves the low inter-rater consonance achieved in this study, particularly for the FEVER metric which compromised some of its validity.This will be addressed in subsequent studies through a thorough training protocol that teaches the human evaluators with more examples of how to rate responses.The data extraction process can also be improved by extracting more than just a single Q&amp;A pair per chunk and section of each paper.This can instead be expanded to extracting multiple Q&amp;A pairs which will have the effect of encoding more details and information from each paper, and thus have a greater chance of capturing all the important facts from each study.The quality and capability of the finetuned models is to a significant degree a function of the data extraction phase.The LLM model can only know what it has been taught.Deficiencies and omissions in the data extraction step will hinder the depth of knowledge synthesis and findings that can be supported for an SLR, therefore, most gains in the quality and factuality of LLM responses stand to be gained through the improvement of this process.Likewise, the lower accuracies exhibited on the SLR-level responses can also be improved in this manner by increasing the size of this category of Q&amp;A pairs.Finally, it is inevitable with the current advancements in generative AI, and LLMs specifically, that these technologies are destined to increase their role in supporting and assisting in future SLR endeavors.To that end, there is a need for revisiting and updating the PRISMA reporting guidelines.This needs to be undertaken in such a way that researchers in the future are provided with guidance on what needs to be reported when conducting PRISMA-conforming SLRs using AI automation tools in order to guarantee transparency and reproducibility.Such efforts have already begun [15] and need to be finalised.</p>
<p>Study Limitations and Future Work</p>
<p>This study's exploration of LLMs in SLRs was constrained by model selection, focusing on Mistral without assessing larger models over 7 billion parameters that might offer improved outcomes.Future research should broaden its scope to include a variety of open-source LLMs, evaluating their efficacy in SLR contexts.Dataset access presents a significant limitation, as copyright restrictions by publishing houses impede the public sharing of even summarised content, challenging the reproducibility and validation of our findings.Addressing this issue requires navigating copyright laws to facilitate wider academic verification and exploration.The use of GPT-4 for evaluation reflects the current state of technology, yet the rapid advancement in LLMs suggests that future iterations will necessitate updated prompts and methodologies to maintain relevance and accuracy in assessments.A limitation of using proprietary models like GPT-4 is also cost, which can and should be bypassed in the future using locally running LLMs for data extraction and evaluation.The study's focus on singular-topic questions limits the depth of evaluation regarding the models' ability to handle complex queries involving multiple concepts.Enhancing the CGS and FEVER metrics calibration through detailed training could yield more reliable evaluations, a necessity for future studies to consider.Methodologically, the study leaned towards qualitative meta-synthesis, leaving a gap in quantitative meta-analysis exploration.Addressing this gap, alongside refining hyperparameters and expanding computational resources to better support extensive data processing, forms a critical pathway for subsequent research.Performing hyperparameter tuning was particularly challenging given the long training runtimes for model finetuning, and it is highly likely that suboptimal hyperparameters were used in this research as a result.Future work should encompass large-scale replication studies to confirm and extend these initial findings, incorporating both qualitative and quantitative SLR meta-analyses.This includes investigating LLMs' multimodal capabilities in extracting information from diverse formats such as tables and figures.</p>
<p>Conclusion</p>
<p>This research introduces an SLR-automation framework leveraging finetuned LLMs, presenting a novel and significant methodical advancement in employing AI for academic research.Our comprehensive experiments with LLM finetuning demonstrate that these AI technologies can effectively streamline SLRs, ensuring both efficiency and accuracy in information retrieval.The framework's effectiveness was validated by accurately replicating a pre-existing SLR study, showcasing the practical applicability of our methods.</p>
<p>The study not only underscores the potential of AI to enhance the SLR process through systematic and efficient literature synthesis but also addresses critical challenges such as LLM hallucination and data provenance.By tackling these issues, we ensure the reliability and verifiability of the synthesized content.</p>
<p>In proposing this SLR-automation framework, we contribute to the broader discourse on integrating AI-driven methodologies in academic research, advocating for the update of PRISMA guidelines to encapsulate these advanced techniques.This ensures methodological transparency and rigor in future SLRs.Our work lays a foundation for further exploration in this area, highlighting the necessity for continuous development of AI tools to enrich and facilitate scholarly research.</p>
<p>A Qualitative Analysis Reference Q&amp;A Examples</p>
<p>B LLM Evaluation Prompts</p>
<p>This section illustrates the GPT-4 prompts used to evaluate all responses according to the FEVER and CGS metrics.FEVER evaluation prompt:</p>
<p>Your task is to rigorously evaluate the factual accuracy, completeness and the source information of a given response text against a provided ground truth reference text using the FEVER categories from the Natural Language Processing field.This evaluation is critical for ensuring the reliability and quality of information synthesis systems.Please follow these detailed guidelines precisely:</p>
<p>Carefully read and understand the ground truth reference text in its entirety.Identify all key facts, details, findings, conclusions, and other important information presented.Thoroughly read the response text multiple times if needed.Break it down into distinct claims, statements, or pieces of information.</p>
<p>For each distinct claim or piece of information in the response text, rigorously assess its factual status against the ground truth reference:</p>
<p>SUPPORTED: The claim and information in the reponse text are clearly, directly, completely and comprehensively corroborated and supported by evidence in the reference text.Also, the 'Source' information describing where the response originated from is identical.</p>
<p>NOT ENOUGH INFO: There is no evidence in the reference text that can either support or contradict the claims in the response text.The response text lacks enough information to make a judgement about consistency or contradictions with respect to the reference text.</p>
<p>REFUTED: Either the claims in the response text are completely, unambiguously and blatantly contradicted or refuted by evidence in the reference text, and he details in the response text are inaccurate with respect to the reference text.Or, the claims are supported between the texts, but the Source information describing where the response originated from is NOT THE SAME between the two texts.</p>
<p>Evaluate the degree of consistency as well as agreement between the following texts, and respond with a single FEVER categorical label as defined above.Be utterly objective, impartial and rigorous in your evaluation.Base your assessments solely on the information given, without making unsubstantiated assumptions or inferences.Check carefully the consistency between the Source information of the texts.</p>
<p>Reference Text: "<insert text >"</p>
<p>Response Text: "<insert text >"</p>
<p>Respond below with just a single FEVER categorical label.</p>
<p>Evaluation (SUPPORTED or REFUTED or NOT ENOUGH INFO):</p>
<p>Consistency Grading Scale (CGS) quantitative evaluation prompt:</p>
<p>Your task is to rigorously evaluate the factual accuracy, completeness and the source information of a given response text against a provided ground truth reference text.This evaluation is critical for ensuring the reliability and quality of information synthesis systems.Please follow these detailed guidelines precisely:</p>
<p>Carefully read and understand the ground truth reference text in its entirety.Identify all key facts, details, findings, conclusions, and other important information presented.Thoroughly read the response text multiple times if needed.Break it down into distinct claims, statements, or pieces of information.</p>
<p>For each distinct claim or piece of information in the response text, rigorously assess its factual status against the ground truth reference:</p>
<p>FULLY CONSISTENT (2): The claim and information in the reponse text are clearly, directly, completely and comprehensively corroborated by evidence in the reference text.Also, the 'Source' information describing where the response originated from is identical.</p>
<p>PARTIALLY CONSISTENT (1): Parts of the response text are supported by the reference text, but some details are either somewhat inaccurate, missing, too general or not fully substantiated from the reference text.Only the Source information describing where the response originated from is inconsistent.</p>
<p>NOT ENOUGH INFO (0): There is no evidence in the reference that can either support or contradict the claims in the response text.The response text lacks enough information to make a judgement about consistency or contradictions with respect to the reference text.</p>
<p>PARTIALLY CONTRADICTORY (-1): Parts of the response text directly contradict or are inconsistent with evidence present in the reference text, suggesting factual inaccuracies.</p>
<p>CONTRADICTORY (-2): The claims in the response text are completely, unambiguously and blatantly contradicted or refuted by evidence in the reference text.The details in the response text are inaccurate with respect to the reference text.</p>
<p>Evaluate the degree of consistency between the following texts, including the Source information of the texts, and respond with a single numerical value ranging from -2 to 2 as defined above.Be utterly objective, impartial and rigorous in your evaluation.Base your assessments solely on the information given, without making unsubstantiated assumptions or inferences.</p>
<p>Reference Text: "<insert text >"</p>
<p>Response Text: "<insert text >"</p>
<p>Respond below with just a single number.</p>
<p>Evaluation (-2,-1,0,1,2):</p>
<p>Ground truth"Many of the published dashboards (LADs) are only in their prototype phases,and only few in the pilot implementation stages."LLM"In the data used for the 2023SLR, the majority of papers discuss the development and evaluation of LADs as a research exercise, either piloting the effectiveness of the dashboards or using them as a case study to investigate their utility.Papers that do not specifically report on the development of LADs as a research effort include Aljohani et al. (2019), Bodily et al. (2018), Karaoglanyilmaz et al. (2021), Fleur et al. (2020), Salehiankia et al. (2020), Han et al. (2020), Valle et al. (2021), Chatti et al. (2020), He et al. (2019), and Naranjo et al. (2019).Other papers, including Ulfa et al. (2019), Liu et al. (2019), Kokoç et al. (2021), Valle et al. (2021), and Baneres et al. (2019), provide details on the development and implementation of LADs, with some studying specific dashboards and others discussing the design process.Additionally, Owatari et al. (2020) and Aljohani et al. (2019) report on the development of LADs as advanced informational systems specifically designed for learning support.Source: 2023SLR"</p>
<p>Figure 3 :
3
Figure 3: Example of a correct response.The reference answer from the SLR study is "Predictive modelling has not featured in a larger percentage of reviewed LAD studies."</p>
<p>Figure 4 :
4
Figure 4: Example of a correct response.The reference answer from the SLR study is "We find that predictive modeling functionalities are not used in majority of cases within the reviewed LADs, and examples of interpretability of the models and the ability to explain their predictions to the learners do not yet exist in published studies."</p>
<p>Figure 5 :
5
Figure 5: Example of an incorrect response.The reference answer from the SLR study is "Bodily et al., 2018 (180), Chen et al., 2019, Aljohani et al., 2019 (86), Ulfa et al., 2019 (67), Majumdar et al., 2019, He et al., 2019 (327), Naranjo et al., 2019 (64), Baneres et al., 2019 (247), Gras et al., 2020 (127), Karaoglan Yilmaz &amp; Yilmaz, 2020 (81), Fleur et al., 020 (79), Chatti et al., 2020 (414), Kia et al., 2020 (449), Owatari et al., 2020 (108), Han et al., 2021 (88), Kokoç &amp; Altun, 2021 (126), Valle et al., 2021 (179)"</p>
<p>How do the results of the study shape future research and development paths in learning analytics and educational technology? 2. What implications do the paper's outcomes have for the future of research and development in learning analytics and educational technology? 3. How can the findings from the paper influence the forthcoming trends in research and development for learning analytics and educational technology? 4. What role do the paper's findings play in setting the course for future research and development in learning analytics and educational technology? 5. How will the paper's discoveries impact the trajectory of research and development in learning analytics and educational technology moving forward? 6.What effect do the paper's conclusions have on the future strategies for research and development in the domains of learning analytics and educational technology? 7. How are the paper's findings expected to steer future research and development initiatives in learning analytics and educational technology?8.In what ways might the paper's results influence upcoming research and development activities in the field of learning analytics and educational technology?9.In what ways do the conclusions of the paper guide the next steps for research and development within the realms of learning analytics and educational technology?10.In what manner do the insights from the paper direct future research and development efforts in the field of learning analytics and educational technology?11.How do the paper's findings inform future directions for research and development in the field of learning analytics and educational technology?Test Questions 12.
Training Questionspaper's findings inform1.future directions for researchby highlighting theimportance of personalizingLADs to course requirementsand the potential ofstudent-centered dashboardsto enhance engagement andperformance. It suggestsfurther exploration of LADs'role in supportingself-regulated learning andthe challenges ofpersonalization.</p>
<p>Table 2 :
2
Comparison of Hyperparameters for NEFTune and LoRA Model Fine-Tuning</p>
<p>Table 3 :
3
Summary of Experimental Methods for SLR Automation4.6Hardwarespecifications and training runtimesFine-tuning and RAG execution were performed on a Linux system powered by an AMD EPYC 7713 64-bit 8-Core Processor with 259GB of RAM.For the training, we utilized an NVIDIA A40 GPU, which is built on an 8 nm process and equipped with 48 GB of GDDR6 memory.This GPU features 10,752 shading units, 336 tensor cores, and 84 ray tracing cores, supporting a base clock speed of 1305 MHz and a boost clock up to 1740 MHz.The total power consumption of the GPU is 300 W. The training processes for both NEFTune and LoRA models took approximately 70 hours to complete 150 epochs each.</p>
<p>Table 4 :
4
Inter-rater correlation matrix for the CGS metric.
Rater GPT-4H1H2GPT-41.0H10.491.0H20.60.557 1.0</p>
<p>Table 5 :
5
Inter-rater correlation matrix for the FEVER metric.</p>
<p>Table 6 :
6
Description of the FEVER results in rank-order according to the 'SUPPORTED' values shown as the percentage of responses according to each category are listed.The CGS results, detailed in Table7, offer a more fine-grained perspective on the consistency of the responses.This scale ranges from -2 (Contradictory) to 2 (Fully consistent), providing a more detailed view of the alignment between the generated responses and the SLR corpus.Similar to the FEVER results, in the CGS analysis NEFTune again emerged as the leader with the highest percentage of 'Fully consistent' responses and an overall mean score of 1.75 (87.6%), indicating a predominance in the factual fidelity of the responses.LoRA closely followed with an overall mean score of 1.72 (86.0%).The method combining RAG with a fine-tuned dataset (RAG + FD) achieved a moderately lower mean score of 1.67 (83.7%), reflecting a competent performance but with room for possible improvement via the adjustable RAG parameters, particularly in enhancing the proportion of 'Fully consistent' responses.NEFTune combined with RAG + FD exhibited a notable drop in performance, as evidenced by a mean score of 1.14 (57.2%), which suggests that the integration of RAG inputs with the finetuned models was likely introducing confusion in response generation.This significant decrease highlights the challenges in combining these two otherwise effective methodologies without compromising factual integrity.RAG (raw) and the baseline methods manifested the lowest consistency levels in line with previous results, with mean scores of 0.46 (23.2%) and 0.164 (8.2%) respectively.
PartiallyNot enoughPartiallyFullyContradictory contradictoryinfoconsistentconsistent(-2)(-1)(0)(1)(2)MeanNEFTune18 (0.4%)205 (4.1%)57 (1.1%)432 (8.7%)4250 (85.7%) 1.75 (87.6%)LoRA25 (0.5%)215 (4.3%)77 (1.6%)486 (9.8%)4159 (83.8%) 1.72 (86.0%)RAG + FD6 (0.1%)86 (1.7%)178 (3.6%)983 (19.8%)3709 (74.7%) 1.67 (83.7%)NEFTune + (RAG + FD)69 (1.4%)250 (5.0%)931 (18.8%)1356 (27.3%) 2356 (47.5%) 1.14 (57.2%)RAG (raw)84 (1.7%)713 (14.4%)1286 (25.9%) 2579 (52.0%)300 (6.0%)0.46 (23.2%)Baseline341 (6.9%)1001 (20.2%) 1397 (28.2%) 1948 (39.3%)275 (5.5%)0.164 (8.2%)</p>
<p>Table 8 :
8
Table 9again showed a moderately positive correlation between NEFTune and LoRA (0.342), indicating that these methods share exhibited overlapping learning patterns on the Correlation matrix for FEVER responses between all the methods.underlying finetuning dataset.The near-zero correlation between NEFTune and the baseline (-0.004) reinforces the distinct advancement NEFTune contributes.This independence from the baseline method and other approaches highlights NEFTune's effectiveness in effectively encoding the finetuning dataset SLR.
NEFTune RAG (raw)LoRA RAG + FD NEFTune + (RAG + FD) baselineNEFTune1RAG (raw)0.0131LoRA0.2900.0141RAG + FD0.1280.0370.1161NEFTune + (RAG + FD)0.1000.0130.0950.2061baseline-0.0060.1070.0220.037-0.0041NEFTune RAG (raw) LoRA RAG + FD NEFTune + (RAG + FD) baselineNEFTune1RAG (raw)-0.0121LoRA0.3420.0011RAG + FD0.1290.0840.1371NEFTune + (RAG + FD)0.0870.0350.0910.2481baseline-0.0040.1680.0250.0400.0391</p>
<p>Table 9 :
9
SCG correlation.</p>
<p>Table 10 :
10
Example of responses 'NOT ENOUGH INFO' FEVER responses across top three methods.</p>
<p>offer a vantage point to assess NEFTune model's proficiency across various levels of analysis.The NEFTune model's capability is particularly observable at the paper-level Q&amp;As, with 83.0% of responses achieving 'Fully Consistent' ratings (CGS=2) as shown in Table12.Likewise, at the paper-summary-level, NEFTune ability to respond with high fidelity is further confirmed by a 'Fully Consistent' score in 95.0% of cases.When combined with 'Partially consistent' results, NEFTune's scores rise to 93% and 98% accuracy for paper-level and paper-summary-level respectively.However, for SLR-level questions, NEFTune's 'Fully Consistent' does significantly reduce to a score of 52%, but again, in combination with 'Partially consistent' results, this rises to 84%, still indicating a strong level of dependability.
PartiallyNot enoughPartiallyFullyContradictory contradictoryinfoconsistentconsistentLevel of analysis(-2)(-1)(0)(1)(2)Paper-level11 (0.3%)171 (4.7%)55 (1.5%)378 (10.4%) 3003 (83.0%)Paper-summary-level6 (0.5%)23 (1.8%)2 (0.2%)33 (2.6%)1211 (95.0%)SLR-level1 (1.3%)11 (14.7%)-24 (32.0%)39 (52.0%)</p>
<p>Table 12 :
12
Summary of Levels by SCG for NEFTune
Level of analysis REFUTED NOT ENOUGH INFO SUPPORTEDPaper-level179 (7.4%)267 (4.9%)3180 (87.7%)Paper-summary-level44 (3.5%)14 (1.1%)1217 (95.5%)SLR-level 32 (42.7%)1 (1.3%)42 (56.0%)</p>
<p>Table 13 :
13
Description of FEVER results at different levels of questions for NEFTuneIn the FEVER metric results in Table13, the results do harmonize with the CGS findings, demonstrating the model's robust accuracy with a majority of 'Supported' classifications at both the paper level (87.7%) and paper-summary level (95.5%).Again, a divergent scenario unfolds at the SLR-level, where a considerable 42.7% of responses are categorized as 'Refuted'.</p>
<p>Table 14 :
14
Comparison of Question and Ground Truth with LLM Response</p>
<p>https://github.com/peterjhwang/slr-helper
http://www.covidence.org
https://www.asreview.nl
https://www.mendeley.com
https://www.zotero.org
https://www.connectedpapers.com/
https://elicit.com/
https://scite.ai/
https://scholarai.io/
The tool is publicly available here: https://github.com/peterjhwang/slr-helper
The chunks usually comprised key sections of each paper.
This embedding model was notable for its efficiency and ranked 7th on the HuggingFace leaderboard at the time of writing this paper.
Future implementation intentions are to upgrade to a hybrid search model for enhanced accuracy and more refined filtering mechanisms.</p>
<p>Grey literature: An important resource in systematic reviews. Arsenio Paez, 10.1111/jebm.12265Journal of evidence-based medicine. 2017</p>
<p>Guidance on conducting a systematic literature review. Yu Xiao, M Watson, 10.1177/0739456X17723971Journal of Planning Education and Research. 392017</p>
<p>How to do a systematic review: A best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. A Andy P Siddaway, L Wood, Hedges, 10.1146/annurev-psych-010418-102803Annual review of psychology. 702019</p>
<p>The prisma 2020 statement: an updated guideline for reporting systematic reviews. Joanne E Matthew J Page, Patrick M Mckenzie, Isabelle Bossuyt, Tammy C Boutron, Cynthia D Hoffmann, Larissa Mulrow, Jennifer M Shamseer, Elie A Tetzlaff, Sue E Akl, Brennan, International journal of surgery. 881059062021</p>
<p>The prisma statement for reporting systematic reviews and meta-analyses of studies that evaluate health care interventions: explanation and elaboration. A Liberati, D Altman, J Tetzlaff, C Mulrow, P Gøtzsche, J Ioannidis, M Clarke, M Clarke, P J Devereaux, J Kleijnen, D Moher, 10.1016/j.jclinepi.2009.06.006Journal of clinical epidemiology. 622009</p>
<p>Prisma-s: an extension to the prisma statement for reporting literature searches in systematic reviews. M Rethlefsen, S Kirtley, S Waffenschmidt, A P Ayala, D Moher, M Page, Jonathan B Koffel, 10.5195/jmla.2021.962Journal of the Medical Library Association : JMLA. 1092021</p>
<p>Re-examining systematic literature review in management research: Additional benefits and execution protocols. Ralph I Williams, L Clark, W R Clark, Deana M Raffo, 10.1016/J.EMJ.2020.09.007European Management Journal. 2020</p>
<p>Artificial intelligence to automate the systematic review of scientific literature. José De La Torre-López, Aurora Ramírez, José Raúl Romero, Computing. 105102023</p>
<p>Visualizing a field of research: A methodology of systematic scientometric reviews. Chaomei Chen, Min Song, 10.1371/journal.pone.0223994PLoS ONE. 142019</p>
<p>Expediting systematic reviews: methods and implications of rapid reviews. R Ganann, D Ciliska, H Thomas, 10.1186/1748-5908-5-56Implementation Science : IS. 52010</p>
<p>Challenges in updating a systematic review. E Mcinnes, Lesley Duf, M Mcclarey, 10.1177/136140969900400111Nursing Times Research. 41999</p>
<p>Searching for grey literature for systematic reviews: challenges and benefits. D Quenby Mahood, E Van Eerd, Irvin, 10.1002/jrsm.1106Research Synthesis Methods. 52014</p>
<p>Choice of data extraction tools for systematic reviews depends on resources and review complexity. Mohamed B Elamin, David N Flynn, D Bassler, M Briel, P Alonso-Coello, P Karanicolas, G Guyatt, G Málaga, T Furukawa, R Kunz, H Schuuenemann, M Murad, C Barbui, A Cipriani, V Montori, 10.1016/j.jclinepi.2008.10.016Journal of clinical epidemiology. 622009</p>
<p>Extractive text summarization system to aid data extraction from full text in systematic review development. Duy Duc, An Bui, Guilherme Del Fiol, John F Hurdle, Siddhartha Jonnalagadda, Journal of biomedical informatics. 642016</p>
<p>Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models. Teo Susnjak, arXiv:2306.149052023arXiv preprint</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran, Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, Dan Roth, 10.1145/3605943ACM Comput. Surv. 0360-0300562sep 2023</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 2157-6904153mar 2024</p>
<p>Large language models for software engineering: A systematic literature review. Xinying Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C Grundy, Haoyu Wang, 10.48550/arXiv.2308.106202023</p>
<p>Calibrated language models must hallucinate. Adam Tauman, Kalai Santosh, Vempala, arXiv:2311.146482023arXiv preprint</p>
<p>A culturally sensitive test to evaluate nuanced gpt hallucination. Timothy R Mcintosh, Tong Liu, Teo Susnjak, Paul Watters, Alex Ng, Malka N Halgamuge, 10.1109/TAI.2023.3332837IEEE Transactions on Artificial Intelligence. 2023</p>
<p>Evaluating object hallucination in large vision-language models. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji Rong, Wen , 10.48550/arXiv.2305.10355ArXiv. 2023</p>
<p>Streamlining systematic reviews: Harnessing large language models for quality assessment and risk-of-bias evaluation. A Nashwan, H Jaber, Jaradat, 10.7759/cureus.43023Cureus. 152023</p>
<p>Can large language models replace humans in the systematic review process? evaluating gpt-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. Qusai Khraisha, Sophie Put, Johanna Kappenberg, Azza Warraitch, Kristin Hadfield, 10.48550/arXiv.2310.175262023</p>
<p>How large language models will disrupt data management. R Fernandez, Aaron J Elmore, M Franklin, Sanjay Krishnan, Chenhao Tan, 10.14778/3611479.3611527Proc. VLDB Endow. VLDB Endow202316</p>
<p>Ai-generated text may have a role in evidence-based medicine. Yifan Peng, Justin F Rousseau, Edward H Shortliffe, Chunhua Weng, Nature medicine. 2972023</p>
<p>Reviews and reviewing: Approaches to research synthesis. an annual review of information science and technology (arist) paper. Linda C Smith, Journal of the Association for Information Science and Technology. 7532024</p>
<p>Are chatgpt and large language models "the answer" to bringing us closer to systematic review automation?. Riaz Qureshi, Daniel Shaughnessy, A R Kayden, Karen A Gill, Tianjing Robinson, Eitan Li, Agai, Systematic Reviews. 121722023</p>
<p>Francisco Bolanos, Angelo Salatino, Francesco Osborne, Enrico Motta, arXiv:2402.08565Artificial intelligence for literature reviews: Opportunities and challenges. 2024arXiv preprint</p>
<p>A guide to conducting a standalone systematic literature review. Chitu Okoli, Communications of the Association for Information Systems. 372015</p>
<p>A typology of reviews: an analysis of 14 review types and associated methodologies. J Maria, Andrew Grant, Booth, Health information &amp; libraries journal. 2622009</p>
<p>Writing integrative literature reviews: Using the past and present to explore the future. Torraco Richard, Human resource development review. 1542016</p>
<p>Writing integrative literature reviews: Guidelines and examples. Torraco Richard, Human resource development review. 432005</p>
<p>Synthesising qualitative and quantitative evidence: a review of possible methods. Mary Dixon-Woods, Shona Agarwal, David Jones, Bridget Young, Alex Sutton, Journal of health services research &amp; policy. 1012005</p>
<p>Meta-analysis and the science of research synthesis. J Gurevitch, J Koricheva, Shinichi Nakagawa, G Stewart, 10.1038/nature25753Nature. 5552018</p>
<p>Meta-analysis is not an exact science: Call for guidance on quantitative synthesis decisions. T Neal R Haddaway, Rytwinski, 10.1016/j.envint.2018.02.018Environment international. 1142018</p>
<p>Challenges in meta-analyses with observational studies. S Metelli, A Chaimani, 10.1136/ebmental-2019-300129Evidence-Based Mental Health. 232020</p>
<p>Methodological issues and challenges in data collection and analysis of qualitative meta-synthesis. Y Xu, 10.1016/S1976-1317(08)60041-9Asian nursing research. 232008</p>
<p>Meta-synthesis of qualitative research: the challenges and opportunities. R Mohammed A Mohammed, T Moles, Chen, 10.1007/s11096-016-0289-2International Journal of Clinical Pharmacy. 382016</p>
<p>Qualitative meta-synthesis: a question of dialoguing with texts. Lela V Zimmer, 10.1111/J.1365-2648.2006.03721.XJournal of advanced nursing. 5332006</p>
<p>Convergent and sequential synthesis designs: implications for conducting and reporting systematic reviews of qualitative and quantitative evidence. Q Hong, P Pluye, Mathieu Bujold, M Wassef, 10.1186/s13643-017-0454-2Systematic Reviews. 62017</p>
<p>Innovations in framework synthesis as a systematic review method. G Brunton, S Oliver, James Thomas, 10.1002/jrsm.1399Research Synthesis Methods. 112020</p>
<p>Qualitative evidence synthesis for complex interventions and guideline development: clarification of the purpose, designs and relevant methods. K Flemming, A Booth, R Garside, Ö Tunçalp, J Noyes, 10.1136/bmjgh-2018-000882BMJ Global Health. 42019</p>
<p>Synthesising quantitative evidence in systematic reviews of complex health interventions. J Higgins, J López-López, B Becker, S Davies, S Dawson, J Grimshaw, L Mcguinness, T Moore, E Rehfuess, James Thomas, D Caldwell, 10.1136/bmjgh-2018-000858BMJ Global Health. 42019</p>
<p>Automating data extraction in systematic reviews: a systematic review. Pankaj Siddhartha R Jonnalagadda, Mark D Goyal, Huffman, Systematic Reviews. 412015</p>
<p>Usage of automation tools in systematic reviews. Aj Van Altena, Spijker, Olabarriaga, Research synthesis methods. 1012019</p>
<p>Text-mining techniques and tools for systematic literature reviews: A systematic literature review. Luyi Feng, Yin Kia Chiam, Sin Kuang Lo, 2017 24th asia-pacific software engineering conference (apsec). IEEE2017</p>
<p>The application of artificial intelligence on different types of literature reviews-a comparative study. Henry Müller, Simran Pachnanda, Felix Pahl, Christopher Rosenqvist, 2022 International Conference on Applied Artificial Intelligence (ICAPAI). IEEE2022</p>
<p>Automation of systematic literature reviews: A systematic literature review. Information and Software Technology. Raymon Van Dinter, Bedir Tekinerdogan, Cagatay Catal, 2021136106589</p>
<p>Automating systematic literature reviews with natural language processing and text mining: A systematic literature review. Girish Sundaram, Daniel Berleant, International Congress on Information and Communication Technology. Springer2023</p>
<p>A roadmap toward the automatic composition of systematic literature reviews. Eugênio Monteiro Da, Silva Júnior, Moisés Lima, Dutra , Iberoamerican Journal of Science Measurement and Communication. 122021</p>
<p>Artificial intelligence and the conduct of literature reviews. Gerit Wagner, Roman Lukyanenko, Guy Paré, Journal of Information Technology. 3722022</p>
<p>Systematic review automation technologies. Guy Tsafnat, Paul Glasziou, Miew Keen Choong, Adam Dunn, Filippo Galgani, Enrico Coiera, Systematic reviews. 32014</p>
<p>Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. Iain J Marshall, Byron C Wallace, Systematic reviews. 82019</p>
<p>Cheap, quick, and rigorous: artificial intelligence and the systematic literature review. Cameron F Atkinson, Social Science Computer Review. 089443932311962812023</p>
<p>Automatic summarization of scientific articles: A survey. Journal of King Saud University-Computer and Information Sciences. Nouf Ibrahim Altmami and Mohamed El Bachir Menai3442022</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.141652020arXiv preprint</p>
<p>Utilization of chatgpt for plastic surgery research: friend or foe. Rohun Gupta, Isabel Herzog, Joseph Weisberger, John Chao, Kongkrit Chaiyasate, Edward S Lee, Journal of Plastic, Reconstructive &amp; Aesthetic Surgery. 802023</p>
<p>Methods for using bing's ai-powered search engine for data extraction for a systematic review. James Edward Hill, Catherine Harris, Andrew Clegg, Research Synthesis Methods. 2023</p>
<p>Leveraging the potential of generative ai to accelerate systematic literature reviews: An example in the area of educational technology. Pablo Castillo-Segura, Carlos Alario-Hoyos, Carlos Delgado Kloos, Carmen Fernández Panadero, 2023 World Engineering Education Forum-Global Engineering Deans Council (WEEF-GEDC). IEEE2023</p>
<p>Analysis of chatgpt tool to assess the potential of its utility for academic writing in biomedical domain. H S Arun, Kumar, Biology, Engineering, Medicine and Science Reports. 912023</p>
<p>Leveraging large language models for literature review tasks-a case study using chatgpt. Robert Zimmermann, Marina Staab, Mehran Nasseri, Patrick Brandtner, International Conference on Advanced Research in Technologies, Information, Innovation and Sustainability. Springer2024</p>
<p>Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions. Ahmad Alshami, Moustafa Elsayed, Eslam Ali, Abdelrahman Ee Eltoukhy, Tarek Zayed, Systems. 1173512023</p>
<p>Truth or lies? the pitfalls and limitations of chatgpt in systematic review creation. Daniel Najafali, Justin M Camacho, Erik Reiche, Logan G Galbraith, Shane D Morrison, Amir H Dorafshar, Aesthetic Surgery Journal. 4382023</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 10.18653/v1/2022.acl-long.229Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221Long Papers)</p>
<p>Characterizing attribution and fluency tradeoffs for retrieval-augmented large language models. Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, Yunhsuan Sung, 2023</p>
<p>How language model hallucinations can snowball. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, Noah A Smith, 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, M Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, ArXiv, abs/2005.114012020</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, 10.48550/arXiv.2310.115112023</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A Smith, arXiv:2004.109642020arXiv preprint</p>
<p>Universal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, arXiv:1801.061462018arXiv preprint</p>
<p>Parameter-efficient transfer learning for nlp. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, International Conference on Machine Learning. PMLR2019</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Lora: Low-rank adaptation of large language models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, 2021</p>
<p>Neel Jain, Ping-Yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Bhavya Brian R Bartoldson, Avi Kailkhura, Aniruddha Schwarzschild, Saha, arXiv:2310.05914Noisy embeddings improve instruction finetuning. 2023arXiv preprint</p>
<p>Learning analytics dashboard: a tool for providing actionable insights to learners. T Susnjak, G S Ramaswami, A Mathrani, 10.1186/s41239-021-00313-7International Journal of Educational Technology in Higher Education. 19122022</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b2023</p>
<p>Fever: a large-scale dataset for fact extraction and verification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, 10.18653/v1/N18-10742018</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, I Shafran, Karthik Narasimhan, Yuan Cao, ArXiv, abs/2210.036292022</p>
<p>A review on fact extraction and verification. Giannis Bekoulis, C Papagiannopoulou, N Deligiannis, 10.1145/3485127ACM Computing Surveys (CSUR). 202055</p>
<p>The inadequacy of reinforcement learning from human feedback-radicalizing large language models via semantic vulnerabilities. Teo Timothy R Mcintosh, Tong Susnjak, Paul Liu, Malka N Watters, Halgamuge, IEEE Transactions on Cognitive and Developmental Systems. 2024</p>
<p>Harnessing gpt-4 for generation of cybersecurity grc policies: A focus on ransomware attack mitigation. Timothy Mcintosh, Tong Liu, Teo Susnjak, Hooman Alavizadeh, Alex Ng, Raza Nowrozy, Paul Watters, Computers &amp; security. 1341034242023</p>            </div>
        </div>

    </div>
</body>
</html>
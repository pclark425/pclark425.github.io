<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8144 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8144</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8144</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-279260742</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.08966v1.pdf" target="_blank">Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers</a></p>
                <p><strong>Paper Abstract:</strong> Pretrained language models (LMs) are prone to arithmetic errors. Existing work showed limited success in probing numeric values from models’ representations, indicating that these errors can be attributed to the inherent unreliability of distributionally learned embeddings in representing exact quantities. However, we observe that previous probing methods are in-adequate for the emergent structure of learned number embeddings with sinusoidal patterns. In response, we propose a novel probing technique that decodes numeric values from input embeddings with near-perfect accuracy across a range of open-source LMs. This proves that after the sole pre-training, LMs represent numbers with remarkable precision. Finally, we find that the embeddings’ preciseness judged by our probe’s accuracy explains a large portion of LM’s errors in elementary arithmetic, and show that aligning the embeddings with the pattern discovered by our probe can mitigate these errors.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8144.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8144.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>f_sin probe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sinusoidal-basis linear probe (f_sin)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probe that first projects number-token embeddings with a learned W_in into a low-dimensional space and then decodes integers using a fixed sinusoidal (Fourier) basis S followed by a learned W_out; designed to capture wave-like numeric structure in embeddings and generalize to unseen integers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied across models (Llama 3, Phi 4, OLMo 2; various sizes 1B–72B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used on multiple pretrained transformer LMs in the paper (Llama 3, Phi 4, OLMo 2 series), which are decoder-only transformer language models with parameter counts from 1B up to 72B and with unique tokens for integers 0–999.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>N/A (probe task: decoding token embeddings to integer values; downstream arithmetic tasks evaluated separately)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Assumes and leverages sinusoidal/fourier-like wave representations in token embeddings; encodes integers via sin/cos bases across projected features.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Learn W_in and W_out with fixed Fourier basis S; classifiers optimized with Adam (lr=1e-4, weight decay=0.001); 20-fold cross-validation; hidden dim 100; experiments with L1/L2 regularization on W_in.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Achieves near-perfect decoding accuracy of integer values from number embeddings across most studied models (substantially outperforms linear/log-linear probes); consistently superior generalization to unseen integer tokens (quantitative accuracy numbers described as 'near-perfect' in paper; comparative plots in Figure 1).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails on embeddings that deviate from the sinusoidal pattern (a small set of tokens per model were 'undecodable'); performance drops on models that do not exhibit sinusoidal structure (notably OLMo2 32B).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High decoding accuracy across models; PCA visualizations show wave-like trajectories; sparse Fourier transforms of PCA components; learned W_in outputs are wave-like under both L1 and L2 regularization (Figure 6); f_sin outperforms linear probes in cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Does not probe models whose embeddings do not follow sinusoidal structure (OLMo2 32B is an explicit counterexample where f_sin has low resemblance and lower probe performance despite good arithmetic behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8144.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8144.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>linear / log-linear probes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear (f_lin) and log-linear (f_loglin) regression probes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple regression-style probes: f_lin predicts integer value as a linear projection a^T x + b; f_loglin predicts log(value+1) as a linear projection and exponentiates the output to recover integer value.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied across models (Llama 3, Phi 4, OLMo 2; various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same suite of pretrained transformer LMs as above.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Probe decoding task (recover numeric token values from embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Assumes (and tests) that numeric information is linearly encoded (or linear in log-space) in the embedding vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Least-squares estimation of regression parameters; rounding predictions to nearest integer for accuracy; cross-validated (20 folds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Lower decoding accuracy than f_sin; prior work (Zhu et al., 2025) reported weak performance with linear probes; in this paper linear/log-linear probes underperform compared to f_sin (plots in Figure 1 show f_sin consistently better).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Cannot capture periodic/wave-like components of embeddings; poor generalization to unseen integer tokens when the learned representation is sinusoidal.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Comparative probe experiments show reduced accuracy versus f_sin across multiple models; regression probes only reach high accuracy on random Gaussian control when appropriate (control checks performed).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Although simpler, these probes miss structured periodicity; nevertheless, they serve as useful baselines and occasionally perform reasonably on models with more linear encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8144.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8144.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>f_bin probe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binary-basis linear probe (f_bin)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probe that projects embeddings via W_in and decodes integers by mapping to a fixed binary representation matrix B (rows are binary expansions of integers) followed by learned W_out; encodes numbers digitwise in binary rather than with Fourier features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied across models (Llama 3, Phi 4, OLMo 2; various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same suite of pretrained transformer LMs as used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Probe decoding task (recover integer token values from embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Tests hypothesis that embeddings encode digits/binary representation; imposes a binary-digit inductive bias via fixed matrix B.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Learn W_in and W_out with B fixed; classifier trained with Adam; 20-fold cross-validation; hidden dim 100.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Performs worse than f_sin overall; in many models f_bin is inferior to the sinusoidal probe, indicating Fourier-like bases better match embeddings' structure.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails when embeddings follow periodic/sinusoidal patterns rather than digitwise/binary encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Comparative accuracy experiments showing f_sin > f_bin; PCA/Fourier analyses indicate embeddings are better explained by sinusoidal bases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Binary encoding hypothesis may still capture aspects of digit-based representations in other work (e.g., Levy & Geva 2025), but in these pretrained models Fourier basis matched better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8144.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8144.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sinusoidal / Fourier number representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sinusoidal (Fourier) number embedding structure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that pretrained LM number token embeddings follow wave-like (sinusoidal) trajectories in projected spaces and have sparse Fourier transforms, indicating encoding of integers via periodic components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>observed in Llama 3 and OLMo 2 models (various sizes); not observed in OLMo2 32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies to the number-token embeddings (unique tokens for integers 0–999) in pretrained transformer LMs studied (Llama 3, OLMo 2, Phi 4).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Relevant for arithmetic reasoning (addition/subtraction) because embeddings serve as numeric inputs to the model's computation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Numbers are represented as combinations of sinusoids (Fourier bases) across embedding dimensions, consistent with a helix-like or trigonometric computation hypothesis for arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Identified via PCA visualizations, sparse Fourier transform of reduced embeddings, and success of sinusoidal probe f_sin.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Wave-like structure correlates with high probe decoding accuracy and corresponds to models that exhibit strong arithmetic performance; exceptions exist (OLMo2 32B).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>When some token embeddings diverge from the learned sinusoidal manifold, those tokens are more likely to cause arithmetic errors (higher downstream error rate when at least one token is 'undecodable').</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>PCA plots and Fourier-spectra (Figures 2 and 3) show sparse frequency contributions; f_sin probe's superior accuracy directly supports the presence of sinusoidal structure; references to related work (Nanda et al., Kantamneni & Tegmark, Zhou et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>OLMo2 32B displays little sinusoidal regularity yet still performs arithmetic well, indicating that sinusoidal structure is common but not universal; probe methods relying on that assumption will fail on such models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8144.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8144.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>embedding-alignment intervention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gradient-based embedding alignment intervention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A targeted intervention that freezes a trained f_sin probe and optimizes (via gradient descent) the embeddings of specific incorrectly decoded tokens to reduce their decoding loss and align them with the model's general sinusoidal embedding pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to Llama 3 1B (and trialed on other models in small-scale experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Intervention applied directly to input token embeddings in pretrained transformer LMs (single-token embeddings for integers).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Evaluated on downstream addition and multiplication tasks that involve the intervened tokens as inputs or expected outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Assumes arithmetic errors arise when particular token embeddings diverge from the learned embedding manifold; aligning them restores the model's ability to use its native arithmetic circuitry.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Freeze probe parameters, perform gradient descent on selected token embeddings (e.g., tokens 0, 4, 977, 999) with respect to probe decoding loss.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>For additions involving intervened tokens: error rate reduced by 26% (from 17.6% to 13.0%); multiplications: error reduction of 9.4% (from 8.5% to 7.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Intervention is anecdotal and limited in scope; generalization to larger sets of tokens and models not established; relies on the probe's inductive bias being correct.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct empirical improvement in downstream arithmetic after aligning embeddings suggests misaligned embeddings contribute causally to some arithmetic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Small-scale experiment only; may not generalize to tokens or models that do not follow the probe's assumed structure (e.g., OLMo2 32B).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8144.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8144.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OLMo2-1B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OLMo 2 1B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1-billion-parameter pretrained language model from the OLMo 2 family; studied for both embedding structure and arithmetic behavior and found to have relatively high decoding/embedding errors and higher arithmetic error rates than larger siblings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OLMo 2 1B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder transformer model, ~1B parameters (OLMo 2 series); uses unique tokens for integers 0–999; pretrained on large text corpora (details referenced to OLMo 2 technical report).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Zero-shot multi-digit addition (0<x_i<500) and subtraction (0<x2<x1<1000); also tested on multiplication in intervention experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Number embeddings show sinusoidal structure but with some misaligned tokens; arithmetic behavior tied to downstream circuitry with activations in later layers (layers 13–15 observed in small-model circuit analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Probed with f_sin, f_bin, f_lin, f_loglin; used integrated gradients and head-level attribution for circuit analysis; embedding interventions applied to specific tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot error rates (Table 1): Addition error 21.39%; Subtraction error 28.12%. Downstream error conditioned on decodability (Table 2): Addition decodable 20.30% vs undecodable 23.33%; Subtraction decodable 24.61% vs undecodable 31.45%.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>High error rates relative to larger models; errors more frequent when at least one operand token is not decodable by the probe; lower probability mass assigned to predicted output token on failures; noisier activation patterns in same heads.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Probe analyses show some tokens are undecodable; integrated gradients/head attribution indicate similar heads activated in success/failure but with noisier patterns on failures; later-layer processing implicated despite precise embeddings at layer 0.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Even when embeddings are precise per probe, arithmetic processing later in network can still fail, indicating embedding precision is necessary but not always sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8144.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8144.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OLMo2-32B (counterexample)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OLMo 2 32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32-billion-parameter OLMo 2 model that is a notable counterexample: its number embeddings do not exhibit strong sinusoidal structure, yet the model attains high arithmetic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OLMo 2 32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only transformer in the OLMo 2 series (~32B parameters); uses dedicated tokens for integers 0–999.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Zero-shot addition and subtraction evaluated (largest models omitted from some experiments due to compute; but OLMo2 32B highlighted for embedding structure).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Embeddings lack the clear sinusoidal pattern (PCA and Fourier analysis show irregular contributions across frequencies), suggesting a different internal numeric representation or arithmetic circuitry.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Probes (including f_sin) performed poorly on this model's embeddings (sinusoidal probe low resemblance); other probes also less effective according to paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Despite probe failures, OLMo2 32B shows high arithmetic performance (paper notes it is 'highly accurate in arithmetics' though not probed accurately by f_sin).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Probe-based decoding fails; demonstrates that probe assumptions (sinusoidal basis) are not universally valid; reveals heterogeneity in how different models encode numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>PCA and Fourier plots (Figure 2/3) indicate different spectral signature; arithmetic capability without sinusoidal embedding pattern is empirical evidence against a universal sin-based representation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Challenges generalization of sinusoidal representation hypothesis — suggests multiple representational schemes can support arithmetic in LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8144.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8144.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-1B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 1B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1-billion-parameter Llama 3 family model studied for number embedding structure and arithmetic; embeddings exhibit sinusoidal patterns and probe well with f_sin, and the model has low arithmetic error rates compared to smaller/worse-aligned models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 1B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (Llama 3 series) with ~1B parameters; pretrained on large corpora; contains unique integer tokens 0–999.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Zero-shot addition (0<x_i<500) and subtraction (0<x2<x1<1000).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Number embeddings show wave-like sinusoidal patterns; arithmetic processing engages later-layer heads (layers 13–15) where activations differ in degree between success and failure.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Probed with f_sin (high decoding accuracy), regression baselines, and classifier probes; circuit analysis and integrated gradients performed on small models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot error rates (Table 1): Addition error 2.58%; Subtraction error 1.43%. Downstream error conditioned on decodability (Table 2): Addition decodable 2.48% vs undecodable 14.86%; Subtraction decodable 1.43% vs undecodable 0.90%.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Arithmetic errors increase when at least one operand token is undecodable by the probe (notably large jump in addition errors when undecodable present); failures show lower probability mass on the predicted token and noisier head activation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High f_sin probe accuracy indicates embeddings encode numeric values precisely; PCA/Fourier show sinusoidal regularity; head-level attributions reveal consistent head involvement in later layers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Some tokens remain misaligned and cause disproportionately higher downstream error; later-layer processing errors can occur even when embeddings are decodable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8144.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8144.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-4 15B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi 4 15B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 15-billion-parameter Phi 4 model included in the study; number embeddings probe well and the model makes virtually no arithmetic errors in the evaluated zero-shot addition/subtraction setup.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi 4 15B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (Phi 4 family) with ~15B parameters; pretrained on broad corpora; unique integer tokens 0–999.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Zero-shot addition and subtraction tasks (same ranges as other models).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Embeddings are consistent with sinusoidal/fourier-like structure and are highly decodable by f_sin probes.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Probed with f_sin and baseline probes; cross-validated accuracy reported; Phi model included in Table 1 and Table 2 analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot error rates (Table 1): Addition 0.00%; Subtraction 0.00% (paper reports Phi omitted from some tables because it makes no errors).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>No significant arithmetic errors observed in the tested zero-shot templates; therefore no decodability-conditioned error analysis required.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High probe decode accuracy and zero downstream error support that precise embeddings aligned with sinusoidal pattern enable robust arithmetic in this model.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>None observed within the tested arithmetic templates — demonstrates that precise embedding representations can coincide with perfect zero-shot arithmetic in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models use trigonometry to do addition <em>(Rating: 2)</em></li>
                <li>Pre-trained large language models use fourier <em>(Rating: 2)</em></li>
                <li>Language models encode the value of numbers linearly <em>(Rating: 2)</em></li>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 1)</em></li>
                <li>Linear algebra with transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8144",
    "paper_id": "paper-279260742",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "f_sin probe",
            "name_full": "Sinusoidal-basis linear probe (f_sin)",
            "brief_description": "A probe that first projects number-token embeddings with a learned W_in into a low-dimensional space and then decodes integers using a fixed sinusoidal (Fourier) basis S followed by a learned W_out; designed to capture wave-like numeric structure in embeddings and generalize to unseen integers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied across models (Llama 3, Phi 4, OLMo 2; various sizes 1B–72B)",
            "model_description": "Used on multiple pretrained transformer LMs in the paper (Llama 3, Phi 4, OLMo 2 series), which are decoder-only transformer language models with parameter counts from 1B up to 72B and with unique tokens for integers 0–999.",
            "arithmetic_task_type": "N/A (probe task: decoding token embeddings to integer values; downstream arithmetic tasks evaluated separately)",
            "mechanism_or_representation": "Assumes and leverages sinusoidal/fourier-like wave representations in token embeddings; encodes integers via sin/cos bases across projected features.",
            "probing_or_intervention_method": "Learn W_in and W_out with fixed Fourier basis S; classifiers optimized with Adam (lr=1e-4, weight decay=0.001); 20-fold cross-validation; hidden dim 100; experiments with L1/L2 regularization on W_in.",
            "performance_metrics": "Achieves near-perfect decoding accuracy of integer values from number embeddings across most studied models (substantially outperforms linear/log-linear probes); consistently superior generalization to unseen integer tokens (quantitative accuracy numbers described as 'near-perfect' in paper; comparative plots in Figure 1).",
            "error_types_or_failure_modes": "Fails on embeddings that deviate from the sinusoidal pattern (a small set of tokens per model were 'undecodable'); performance drops on models that do not exhibit sinusoidal structure (notably OLMo2 32B).",
            "evidence_for_mechanism": "High decoding accuracy across models; PCA visualizations show wave-like trajectories; sparse Fourier transforms of PCA components; learned W_in outputs are wave-like under both L1 and L2 regularization (Figure 6); f_sin outperforms linear probes in cross-validation.",
            "counterexamples_or_challenges": "Does not probe models whose embeddings do not follow sinusoidal structure (OLMo2 32B is an explicit counterexample where f_sin has low resemblance and lower probe performance despite good arithmetic behavior).",
            "uuid": "e8144.0",
            "source_info": {
                "paper_title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "linear / log-linear probes",
            "name_full": "Linear (f_lin) and log-linear (f_loglin) regression probes",
            "brief_description": "Simple regression-style probes: f_lin predicts integer value as a linear projection a^T x + b; f_loglin predicts log(value+1) as a linear projection and exponentiates the output to recover integer value.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied across models (Llama 3, Phi 4, OLMo 2; various sizes)",
            "model_description": "Same suite of pretrained transformer LMs as above.",
            "arithmetic_task_type": "Probe decoding task (recover numeric token values from embeddings)",
            "mechanism_or_representation": "Assumes (and tests) that numeric information is linearly encoded (or linear in log-space) in the embedding vectors.",
            "probing_or_intervention_method": "Least-squares estimation of regression parameters; rounding predictions to nearest integer for accuracy; cross-validated (20 folds).",
            "performance_metrics": "Lower decoding accuracy than f_sin; prior work (Zhu et al., 2025) reported weak performance with linear probes; in this paper linear/log-linear probes underperform compared to f_sin (plots in Figure 1 show f_sin consistently better).",
            "error_types_or_failure_modes": "Cannot capture periodic/wave-like components of embeddings; poor generalization to unseen integer tokens when the learned representation is sinusoidal.",
            "evidence_for_mechanism": "Comparative probe experiments show reduced accuracy versus f_sin across multiple models; regression probes only reach high accuracy on random Gaussian control when appropriate (control checks performed).",
            "counterexamples_or_challenges": "Although simpler, these probes miss structured periodicity; nevertheless, they serve as useful baselines and occasionally perform reasonably on models with more linear encoding.",
            "uuid": "e8144.1",
            "source_info": {
                "paper_title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "f_bin probe",
            "name_full": "Binary-basis linear probe (f_bin)",
            "brief_description": "A probe that projects embeddings via W_in and decodes integers by mapping to a fixed binary representation matrix B (rows are binary expansions of integers) followed by learned W_out; encodes numbers digitwise in binary rather than with Fourier features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied across models (Llama 3, Phi 4, OLMo 2; various sizes)",
            "model_description": "Same suite of pretrained transformer LMs as used in the paper.",
            "arithmetic_task_type": "Probe decoding task (recover integer token values from embeddings)",
            "mechanism_or_representation": "Tests hypothesis that embeddings encode digits/binary representation; imposes a binary-digit inductive bias via fixed matrix B.",
            "probing_or_intervention_method": "Learn W_in and W_out with B fixed; classifier trained with Adam; 20-fold cross-validation; hidden dim 100.",
            "performance_metrics": "Performs worse than f_sin overall; in many models f_bin is inferior to the sinusoidal probe, indicating Fourier-like bases better match embeddings' structure.",
            "error_types_or_failure_modes": "Fails when embeddings follow periodic/sinusoidal patterns rather than digitwise/binary encodings.",
            "evidence_for_mechanism": "Comparative accuracy experiments showing f_sin &gt; f_bin; PCA/Fourier analyses indicate embeddings are better explained by sinusoidal bases.",
            "counterexamples_or_challenges": "Binary encoding hypothesis may still capture aspects of digit-based representations in other work (e.g., Levy & Geva 2025), but in these pretrained models Fourier basis matched better.",
            "uuid": "e8144.2",
            "source_info": {
                "paper_title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "sinusoidal / Fourier number representation",
            "name_full": "Sinusoidal (Fourier) number embedding structure",
            "brief_description": "Observation that pretrained LM number token embeddings follow wave-like (sinusoidal) trajectories in projected spaces and have sparse Fourier transforms, indicating encoding of integers via periodic components.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "observed in Llama 3 and OLMo 2 models (various sizes); not observed in OLMo2 32B",
            "model_description": "Applies to the number-token embeddings (unique tokens for integers 0–999) in pretrained transformer LMs studied (Llama 3, OLMo 2, Phi 4).",
            "arithmetic_task_type": "Relevant for arithmetic reasoning (addition/subtraction) because embeddings serve as numeric inputs to the model's computation.",
            "mechanism_or_representation": "Numbers are represented as combinations of sinusoids (Fourier bases) across embedding dimensions, consistent with a helix-like or trigonometric computation hypothesis for arithmetic.",
            "probing_or_intervention_method": "Identified via PCA visualizations, sparse Fourier transform of reduced embeddings, and success of sinusoidal probe f_sin.",
            "performance_metrics": "Wave-like structure correlates with high probe decoding accuracy and corresponds to models that exhibit strong arithmetic performance; exceptions exist (OLMo2 32B).",
            "error_types_or_failure_modes": "When some token embeddings diverge from the learned sinusoidal manifold, those tokens are more likely to cause arithmetic errors (higher downstream error rate when at least one token is 'undecodable').",
            "evidence_for_mechanism": "PCA plots and Fourier-spectra (Figures 2 and 3) show sparse frequency contributions; f_sin probe's superior accuracy directly supports the presence of sinusoidal structure; references to related work (Nanda et al., Kantamneni & Tegmark, Zhou et al.).",
            "counterexamples_or_challenges": "OLMo2 32B displays little sinusoidal regularity yet still performs arithmetic well, indicating that sinusoidal structure is common but not universal; probe methods relying on that assumption will fail on such models.",
            "uuid": "e8144.3",
            "source_info": {
                "paper_title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "embedding-alignment intervention",
            "name_full": "Gradient-based embedding alignment intervention",
            "brief_description": "A targeted intervention that freezes a trained f_sin probe and optimizes (via gradient descent) the embeddings of specific incorrectly decoded tokens to reduce their decoding loss and align them with the model's general sinusoidal embedding pattern.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied to Llama 3 1B (and trialed on other models in small-scale experiments)",
            "model_description": "Intervention applied directly to input token embeddings in pretrained transformer LMs (single-token embeddings for integers).",
            "arithmetic_task_type": "Evaluated on downstream addition and multiplication tasks that involve the intervened tokens as inputs or expected outputs.",
            "mechanism_or_representation": "Assumes arithmetic errors arise when particular token embeddings diverge from the learned embedding manifold; aligning them restores the model's ability to use its native arithmetic circuitry.",
            "probing_or_intervention_method": "Freeze probe parameters, perform gradient descent on selected token embeddings (e.g., tokens 0, 4, 977, 999) with respect to probe decoding loss.",
            "performance_metrics": "For additions involving intervened tokens: error rate reduced by 26% (from 17.6% to 13.0%); multiplications: error reduction of 9.4% (from 8.5% to 7.7%).",
            "error_types_or_failure_modes": "Intervention is anecdotal and limited in scope; generalization to larger sets of tokens and models not established; relies on the probe's inductive bias being correct.",
            "evidence_for_mechanism": "Direct empirical improvement in downstream arithmetic after aligning embeddings suggests misaligned embeddings contribute causally to some arithmetic errors.",
            "counterexamples_or_challenges": "Small-scale experiment only; may not generalize to tokens or models that do not follow the probe's assumed structure (e.g., OLMo2 32B).",
            "uuid": "e8144.4",
            "source_info": {
                "paper_title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "OLMo2-1B",
            "name_full": "OLMo 2 1B",
            "brief_description": "A 1-billion-parameter pretrained language model from the OLMo 2 family; studied for both embedding structure and arithmetic behavior and found to have relatively high decoding/embedding errors and higher arithmetic error rates than larger siblings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OLMo 2 1B",
            "model_description": "Decoder transformer model, ~1B parameters (OLMo 2 series); uses unique tokens for integers 0–999; pretrained on large text corpora (details referenced to OLMo 2 technical report).",
            "arithmetic_task_type": "Zero-shot multi-digit addition (0&lt;x_i&lt;500) and subtraction (0&lt;x2&lt;x1&lt;1000); also tested on multiplication in intervention experiments.",
            "mechanism_or_representation": "Number embeddings show sinusoidal structure but with some misaligned tokens; arithmetic behavior tied to downstream circuitry with activations in later layers (layers 13–15 observed in small-model circuit analysis).",
            "probing_or_intervention_method": "Probed with f_sin, f_bin, f_lin, f_loglin; used integrated gradients and head-level attribution for circuit analysis; embedding interventions applied to specific tokens.",
            "performance_metrics": "Zero-shot error rates (Table 1): Addition error 21.39%; Subtraction error 28.12%. Downstream error conditioned on decodability (Table 2): Addition decodable 20.30% vs undecodable 23.33%; Subtraction decodable 24.61% vs undecodable 31.45%.",
            "error_types_or_failure_modes": "High error rates relative to larger models; errors more frequent when at least one operand token is not decodable by the probe; lower probability mass assigned to predicted output token on failures; noisier activation patterns in same heads.",
            "evidence_for_mechanism": "Probe analyses show some tokens are undecodable; integrated gradients/head attribution indicate similar heads activated in success/failure but with noisier patterns on failures; later-layer processing implicated despite precise embeddings at layer 0.",
            "counterexamples_or_challenges": "Even when embeddings are precise per probe, arithmetic processing later in network can still fail, indicating embedding precision is necessary but not always sufficient.",
            "uuid": "e8144.5",
            "source_info": {
                "paper_title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "OLMo2-32B (counterexample)",
            "name_full": "OLMo 2 32B",
            "brief_description": "A 32-billion-parameter OLMo 2 model that is a notable counterexample: its number embeddings do not exhibit strong sinusoidal structure, yet the model attains high arithmetic accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OLMo 2 32B",
            "model_description": "Large decoder-only transformer in the OLMo 2 series (~32B parameters); uses dedicated tokens for integers 0–999.",
            "arithmetic_task_type": "Zero-shot addition and subtraction evaluated (largest models omitted from some experiments due to compute; but OLMo2 32B highlighted for embedding structure).",
            "mechanism_or_representation": "Embeddings lack the clear sinusoidal pattern (PCA and Fourier analysis show irregular contributions across frequencies), suggesting a different internal numeric representation or arithmetic circuitry.",
            "probing_or_intervention_method": "Probes (including f_sin) performed poorly on this model's embeddings (sinusoidal probe low resemblance); other probes also less effective according to paper.",
            "performance_metrics": "Despite probe failures, OLMo2 32B shows high arithmetic performance (paper notes it is 'highly accurate in arithmetics' though not probed accurately by f_sin).",
            "error_types_or_failure_modes": "Probe-based decoding fails; demonstrates that probe assumptions (sinusoidal basis) are not universally valid; reveals heterogeneity in how different models encode numbers.",
            "evidence_for_mechanism": "PCA and Fourier plots (Figure 2/3) indicate different spectral signature; arithmetic capability without sinusoidal embedding pattern is empirical evidence against a universal sin-based representation.",
            "counterexamples_or_challenges": "Challenges generalization of sinusoidal representation hypothesis — suggests multiple representational schemes can support arithmetic in LMs.",
            "uuid": "e8144.6",
            "source_info": {
                "paper_title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Llama3-1B",
            "name_full": "Llama 3 1B",
            "brief_description": "A 1-billion-parameter Llama 3 family model studied for number embedding structure and arithmetic; embeddings exhibit sinusoidal patterns and probe well with f_sin, and the model has low arithmetic error rates compared to smaller/worse-aligned models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3 1B",
            "model_description": "Decoder-only transformer (Llama 3 series) with ~1B parameters; pretrained on large corpora; contains unique integer tokens 0–999.",
            "arithmetic_task_type": "Zero-shot addition (0&lt;x_i&lt;500) and subtraction (0&lt;x2&lt;x1&lt;1000).",
            "mechanism_or_representation": "Number embeddings show wave-like sinusoidal patterns; arithmetic processing engages later-layer heads (layers 13–15) where activations differ in degree between success and failure.",
            "probing_or_intervention_method": "Probed with f_sin (high decoding accuracy), regression baselines, and classifier probes; circuit analysis and integrated gradients performed on small models.",
            "performance_metrics": "Zero-shot error rates (Table 1): Addition error 2.58%; Subtraction error 1.43%. Downstream error conditioned on decodability (Table 2): Addition decodable 2.48% vs undecodable 14.86%; Subtraction decodable 1.43% vs undecodable 0.90%.",
            "error_types_or_failure_modes": "Arithmetic errors increase when at least one operand token is undecodable by the probe (notably large jump in addition errors when undecodable present); failures show lower probability mass on the predicted token and noisier head activation.",
            "evidence_for_mechanism": "High f_sin probe accuracy indicates embeddings encode numeric values precisely; PCA/Fourier show sinusoidal regularity; head-level attributions reveal consistent head involvement in later layers.",
            "counterexamples_or_challenges": "Some tokens remain misaligned and cause disproportionately higher downstream error; later-layer processing errors can occur even when embeddings are decodable.",
            "uuid": "e8144.7",
            "source_info": {
                "paper_title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Phi-4 15B",
            "name_full": "Phi 4 15B",
            "brief_description": "A 15-billion-parameter Phi 4 model included in the study; number embeddings probe well and the model makes virtually no arithmetic errors in the evaluated zero-shot addition/subtraction setup.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi 4 15B",
            "model_description": "Decoder-only transformer (Phi 4 family) with ~15B parameters; pretrained on broad corpora; unique integer tokens 0–999.",
            "arithmetic_task_type": "Zero-shot addition and subtraction tasks (same ranges as other models).",
            "mechanism_or_representation": "Embeddings are consistent with sinusoidal/fourier-like structure and are highly decodable by f_sin probes.",
            "probing_or_intervention_method": "Probed with f_sin and baseline probes; cross-validated accuracy reported; Phi model included in Table 1 and Table 2 analyses.",
            "performance_metrics": "Zero-shot error rates (Table 1): Addition 0.00%; Subtraction 0.00% (paper reports Phi omitted from some tables because it makes no errors).",
            "error_types_or_failure_modes": "No significant arithmetic errors observed in the tested zero-shot templates; therefore no decodability-conditioned error analysis required.",
            "evidence_for_mechanism": "High probe decode accuracy and zero downstream error support that precise embeddings aligned with sinusoidal pattern enable robust arithmetic in this model.",
            "counterexamples_or_challenges": "None observed within the tested arithmetic templates — demonstrates that precise embedding representations can coincide with perfect zero-shot arithmetic in practice.",
            "uuid": "e8144.8",
            "source_info": {
                "paper_title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models use trigonometry to do addition",
            "rating": 2,
            "sanitized_title": "language_models_use_trigonometry_to_do_addition"
        },
        {
            "paper_title": "Pre-trained large language models use fourier",
            "rating": 2,
            "sanitized_title": "pretrained_large_language_models_use_fourier"
        },
        {
            "paper_title": "Language models encode the value of numbers linearly",
            "rating": 2,
            "sanitized_title": "language_models_encode_the_value_of_numbers_linearly"
        },
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 1,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "Linear algebra with transformers",
            "rating": 1,
            "sanitized_title": "linear_algebra_with_transformers"
        }
    ],
    "cost": 0.014191999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers</p>
<p>Marek Kadlčík 
TransformersClub @ Faculty of Informatics
Masaryk University ♡ Language Technology
University of Helsinki ♢ Kempelen Institute of Intelligent Technologies</p>
<p>Michal Štefánik 
TransformersClub @ Faculty of Informatics
Masaryk University ♡ Language Technology
University of Helsinki ♢ Kempelen Institute of Intelligent Technologies</p>
<p>From October
National Institute of Informatics Research and Development Center for Large Language Mod-els
2025TokyoJapan</p>
<p>Timothee Mickus 
Michal Spiegel 
TransformersClub @ Faculty of Informatics
Masaryk University ♡ Language Technology
University of Helsinki ♢ Kempelen Institute of Intelligent Technologies</p>
<p>Josef Kuchař 
TransformersClub @ Faculty of Informatics
Masaryk University ♡ Language Technology
University of Helsinki ♢ Kempelen Institute of Intelligent Technologies</p>
<p>Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers
6C6934729DDCD059C02931F5F3EF5670
Pretrained language models (LMs) are prone to arithmetic errors.Existing work showed limited success in probing numeric values from models' representations, indicating that these errors can be attributed to the inherent unreliability of distributionally learned embeddings in representing exact quantities.However, we observe that previous probing methods are inadequate for the emergent structure of learned number embeddings with sinusoidal patterns.In response, we propose a novel probing technique that decodes numeric values from input embeddings with near-perfect accuracy across a range of open-source LMs.This proves that after the sole pre-training, LMs represent numbers with remarkable precision.Finally, we find that the embeddings' precision, judged by our probe's accuracy, explains a large portion of LM's errors in elementary arithmetic, and show that aligning the embeddings with the pattern our probes discover can mitigate these errors.</p>
<p>Introduction</p>
<p>The landmark paper of Brown et al. (2020) showed that generic neural networks trained on text prediction alone could develop surprising arithmetic capabilities.In the years since, this observation has flourished into a large and vibrant field interested in the arithmetic reasoning capabilities of Transformers (Ahn et al., 2024), rife with research opportunities ranging from interpretability work (Akter et al., 2024) to solving Olympiad-level problems in mathematics (Li et al., 2025).Yet this work has also underscored the limitations of LMs on arithmetic tasks: Previous studies have explored how models can benefit from incorporating precise numeric representations (Feng et al., 2024), or offloading the arithmetic computation to a tool (Schick et al., 2023;Kadlčík et al., 2023), suggesting that their native learned representations are not reliable.Other works (Kantamneni and Tegmark, 2025;Zhou et al., 2024) have inspected such learned representations directly and tried to understand how models use them.Although model probing methods showed some success in interpreting numeric values from model representations (Zhu et al., 2025), the accuracy of those methods is low, suggesting that learned representations are highly imprecise.</p>
<p>In this paper, we push back on this interpretation: we show that a probe with the right kind of inductive bias can retrieve numeric information from number embeddings with near-perfect accuracy across an extensive range of LMs, spanning the Llama 3 (Grattafiori et al., 2024), Phi 4 (Abdin et al., 2024) and OLMo 2 (OLMo et al., 2025) series and ranging from 1B to 72B parameters.Given that number embeddings usually follow a sinusoidal wave-like pattern (Nanda et al., 2023;Kantamneni and Tegmark, 2025), this characteristic must be accounted for when designing probes.</p>
<p>We further show how these insights can be leveraged to improve performances on arithmetic reasoning: errors on addition and subtraction tasks can often be matched with an inability of the probe to retrieve the expected numerical information for a given embedding, and demonstrate that intervening on number embeddings such that they more cohesively follow the pattern of other number embeddings can directly improve arithmetic performances.Lastly, we document edge cases that do not fall within this previously understood pattern: in particular, OLMo2 32B (OLMo et al., 2025) learns embeddings that are not sinusoidal-like, despite a high success rate on arithmetic tasks. 1 arXiv:2506.08966v2[cs.CL] 24 Oct 2025</p>
<p>Related Work</p>
<p>One line of work focuses on incorporating numerical values directly into token representations, providing LMs with a prior.Charton (2022) explores different number encodings based on scientific notation for training LM solvers of linear algebra problems.Golkar et al. (2023) propose representing numbers as a learned <NUM> token scaled by the number scalar value, demonstrating how models can adopt this scheme for regression tasks.</p>
<p>Another line of work investigates how models learn to represent and process numerical information.Nanda et al. (2023) show that a transformer with one-hot encoding trained from scratch on modular addition discovers Fourier basis and its computation is interpretable in trigonometric functions.Kantamneni and Tegmark (2025) discover an analogous circuitry for (non-modular) addition in a general pretrained language model, and find that its intermediate representations combine both linear and periodic components, reminiscent of a helix structure.Zhou et al. (2024) further identifies subcomponents of the addition circuitry implemented by the attention mechanism and feedforward layers.Zhu et al. (2025) demonstrate that hidden states of pretrained language models can be approximately decoded with a linear (or multi-layer) probe to estimate the logarithm of the number value.Although the probe outputs correlate with the target value, decoding achieves low accuracy.Recently, Levy and Geva (2025) show success in recovering the values of digits from internal representations of intermediate layers, hinting on a more generalized, circular pattern in representations of numbers.</p>
<p>In summary, prior works suggest that language models attempt to encode numerical information into token representations during pretraining, but their precision is rather limited.However, we hypothesize that this perception stems from inadequate probing methods, and learned representations are much more precise than previously estimated.</p>
<p>Recovering numerical information from number embeddings</p>
<p>We study LMs from the Llama 3 (Grattafiori et al., 2024), Phi 4 (Abdin et al., 2024), and OLMo 2 (OLMo et al., 2025) series, ranging from 1B to 72B parameters.Wide selection allows us to verify the validity of our observations across a panel of models sharing the characteristic of representing all integers between 0 and 999 with unique tokens.</p>
<p>Motivations.The central and foremost point to address is whether the embeddings representing specific numbers in LMs contain the numeric information of the value they represent.In practice, this is best addressed with a probing setup: If embeddings do contain numerical information, we should be able to learn a decoding function from number embedding to the corresponding integer value.Probing as a methodology comes with its own set of caveats: probes should be kept as simple as possible, and their expressivity should be compared against baseline benchmarks (Hewitt and Liang, 2019).Our specific use case adds further constraints: in particular, we have only one instance per LLM of each integer representation, viz., there is only one vector for the token 42.This rules out naive classifier implementations, as we aim for the probe to generalize to entirely unseen classes.</p>
<p>Probe architectures.We consider four probes:
f lin (x) = a T x + b(1)f log lin (x) = exp a T x + b − 1 (2) f sin (x) = (W out S) T (W in x)(3)f bin (x) = (W out B) T (W in x)(4)
where a, b, W in , and W out are learned parameters, whereas S and B are means of injecting inductive biases in the linear classifiers f sin and f bin :
S ij = sin(ie j 1000/d) if j ≡ 0 mod 2 cos(ie j+1 1000/d) if j ≡ 1 mod 2 B =      0 . . . 0 0 1 0 . . . 0 1 0 0 . . . 0 1 1 . . .     
I.e., the i th row of B corresponds to the integer i expressed in binary, whereas S is defined as a Fourier basis, suggested by Zhou et al. as the hidden structure learned by pretrained models.The matrices S and B thus allows us to partition the label projection of the classifier into three components: a learned projection W in : R d → R h to project the number embeddings into a reduced lowdimensional space, a fixed matrix (S or B) allowing us to encode integers using an a priori scheme, and a learned projection W out : R d → R h mapping these a priori representations onto the same space as the reduced embeddings.Intuitively, W in uncovers the underlying hidden structure of the learned embeddings, while W out expresses it in terms of interpretable a priori basis, which allows us to generalize to unseen tokens.</p>
<p>Implementation.We evaluate the probes in Equations ( 1) to (4) using a cross-validation setup with 20 folds.We report their accuracy measured by rounding the output of the regression probes Equations ( 1) and ( 2) to the nearest integer, or by retrieving the index of the row in S or B that maximizes the output distribution of the classifier probes Equations ( 3) and ( 4).We control the validity of our probes by ensuring that they reach an accuracy of 0 for standard Gaussian vectors as well as for a random permutation of the embeddings.Parameters for regressions are estimated using a least-squares algorithm; whereas our classifiers' parameters are optimized with Adam with a learning rate of 0.0001, weight decay of 0.001, and β = (0.9, 0.999).We choose a hidden dimension of 100.The classifiers are optimized to distinguish output only between training tokens, and during testing, must choose between all tokens.The probes are optimized until loss converges on a validation split separate from the testing split.</p>
<p>We release an implementation and training recipes for the new probes, including all configurations we use, in the project's GitHub repository.Results.We summarize performances, measured in terms of accuracy, in Figure 1.Crucially, we are almost systematically able to retrieve the integer value corresponding to the embedding's number with very high accuracy.Another salient observation is that f sin consistently outperforms all other probe architectures including the regression probe used in previous work of Zhu et al. ( 2025), contradicting their finding that LMs learn to encode numbers linearly.Explaining the success of the Fourier basis, we note that other prior literature has suggested that sinusoidal features are used for arithmetic computation in LMs (Zhou et al., 2024).</p>
<p>Adding onto this, we can also stress that, qualitatively, most of the models' whose number embeddings we survey here exhibit wave-like patterns in a PCA projection and have sparse Fourier transform, confirming regularity in the hidden structure.See Figures 2 and 3 in Section A.1 for visualizations of PCA and its Fourier transform.Notably, OLMo 2 32B is the only model with low resemblance of the pattern, which is consistent with the low performance of its sinusoidal probe.</p>
<p>Analysis.To verify that our sin-base probes indeed reach their superior accuracy by learning to extract a generalized, sin-like representation from models' representations, we analyse the encoded representations that trained sin probes produce as the output of W in .We experiment with two training settings: (i) using L1 regularization -encouraging sparsity, and (ii) using L2 regularizationencouraging the employment of a broader scale of input features.We note that in both of these settings, the probes achieve almost identical generalization capacity as assessed by their accuracy on unseen inputs (embeddings of numbers).Figure 6 in Appendix A.3 displays the resulting representations for model embeddings of Llama3 1B associated with different numeric values.We can observe that the L1-and L2-regularized probes learn a substantially distinct representational pattern.We hypothesize that a main difference between probes trained with different regularizations is that the L1 probe learns to follow a broader scale of distinct frequencies, while the L2 probe follows similar frequencies shifted by a different constant.Nevertheless, in both of the cases, the probe learns a projection into a wave-like pattern across input numbers, thus successfully following their injected inductive bias.</p>
<p>Leveraging numerical information from number embeddings</p>
<p>Motivation.Having established that number embeddings do encode retrieval numerical information about the integers they represent, we now turn to how this numerical information is leveraged by LMs to perform arithmetic tasks.We study the zero-shot performances of a subset of our models on addition and subtraction tasks.We define our addition task as taking any pair of integers x 1 , x 2 such that 0 &lt; x i &lt; 500 as input, and computing the expected output x 1 + x 2 .The subtraction task is defined by taking as inputs any pair x 1 , x 2 such that 0 &lt; x 2 &lt; x 1 &lt; 1000, and computing the expected output x 1 − x 2 .</p>
<p>Performance.To perform the arithmetic tasks, we conduct minimal prompt engineering: we systematically evaluate a handful of natural language prompts for their accuracy in a zero-shot setting, and then select the highest-performing for subsequent analyses.Due to computational costs, we ignore the two largest models (OLMo2 32B and Llama 3 70B).All prompts are listed in Section B, see Table 3a for addition and Table 3b for subtraction.
OLMo2 1B OLMo2 7B OLMo2 13B Llama 3 1B Llama 3 3B Llama 3 8B Phi 4 15B
Add. 21.39 1.12 0.17 2.58 0.45 0.25 0.00 Sub.28.12 0.36 0.16 1.43 0.03 0.01 0.00</p>
<p>Table 1: Overview of error rates (%, ↓) on arithmetic tasks in zero-shot setting.</p>
<p>An overview of the error rates from the LMs we study is listed in Table 1.As is apparent, most models achieve high degrees of performance (except for OLMO 2 1B); we also observe a trend towards fewer errors for models with more parameters.</p>
<p>Error analysis.To assess how numerical information and arithmetic performance are linked, we evaluate whether the errors we see in these arithmetic tasks are associated with defects of the number embeddings used as inputs.</p>
<p>We measure the error rate on the downstream addition and subtraction task in two separate cases -in the first case, both input tokens are decodable by the probe, in the second case, at least one value is not.The results can be seen in Table 2.</p>
<p>The results show that models tend to make more errors when the input embeddings are misaligned with the pattern used by the probe, as undecodable inputs lead to higher error rates in 8 out of 12 configurations.The effect is more prominent for models with substantial error rates, such as OLMo2 1B.</p>
<p>Direct intervention.We hypothesize that embeddings of tokens that our probes can not correctly decode diverge from the model's robust representa- tion scheme and thus contribute to errors in arithmetic tasks.With this motivation, we test whether a direct intervention on the embeddings of these tokens can improve models' performance on arithmetic.In practice, we start from the f sin probes described in Equation ( 3) and trained for Llama 3 1B and freeze all probe parameters.We then perform gradient descent to optimize the embeddings of all incorrectly decoded tokens (namely 0, 4, 977 and 999) with respect to the probe decoding loss, aiming to align those tokens with the overall pattern discovered by the probe.</p>
<p>We finally measure how this embedding intervention impacts model error rate on addition and multiplication tasks involving these four tokens as one of the inputs or expected outputs, using the model's best-performing template (a set of our experimental templates is listed in Table 3a).</p>
<p>We find that in additions involving these assumably divergent tokens, our intervention reduces 26% of errors (from 17.6% to 13.0%).In multiplications, our intervention brings error reduction by 9.4% (from 8.5% to 7.7%).This experiment, while of an anecdotal scale determined by a low error rate of our probes, shows that more accurate probes of models' representations can also guide direct refinements of models' possibly imprecise embeddings, aligning them with the model's general hidden structure and bringing improvements in accuracy of the model's predictions.</p>
<p>Conclusion</p>
<p>In this paper, we have inspected the embedding representations for number tokens across a range of widely used open-source LMs.Our observations consolidate a growing body of studies showcasing how LMs learn sinusoidal hidden structure in number representations.Building upon this observation, we design a probing method leveraging this structure that decodes LMs' embeddings with nearperfect accuracy across multiple models, demonstrating that the quality of numeric representations in pretrained LMs was strongly underestimated in previous work.Still, we find a model (OLMo 2 32B) that deviates from this pattern, calling into question the generalizability of the conclusions of works such as Zhou et al.'s (2024).Finally, we show that the preciseness of embeddings relative to the sinusoidal pattern can explain a proportion of practical errors on arithmetic tasks, especially when models fail to align closely with this sinusoidal pattern.Furthermore, we demonstrate improved accuracy on those tasks by aligning imprecise embeddings to the model's learned embedding pattern.To some extent, our findings curtail the validity of offloading approaches for numerical reasoning (Schick et al., 2023;Kadlčík et al., 2023): showing that their initial premise -of models not learn accurately representations of numbers -is incorrect.We hope that our findings will motivate future work to rigorously compare relative advantages of tool-using models in terms of computational efficiency, and challenge future work towards the data (Štefánik et al., 2024) and architecture refinements (Spiegel et al., 2025) accelerating more efficient learning of accurate representations of exact elements of language.This work is supported by the Research Council of Finland through project No. 353164 "Green NLPcontrolling the carbon footprint in sustainable language technology".
⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆
This project has received funding from the European Union's (EU) Horizon Europe research and innovation programme under Grant agreement No. 101070350 and from UK Research and Innovation (UKRI) under the UK government's Horizon Europe funding guarantee (grant number 10052546).The contents of this publication are the sole responsibility of its authors and do not necessarily reflect the opinion of the EU.</p>
<p>Limitations</p>
<p>Our work, while demonstrating the remarkable accuracy of number embeddings in pre-trained language models, comes with several limitations that warrant consideration for future research.</p>
<p>First, our probing method, though highly effective for many models, relies on an assumed hidden structure of models' learned representations, and therefore expects a broad a priori understanding of models' representation space.This necessarily limits the applicability of our approach to models where a known structure exists; Our results aim to show that some language models indeed do exhibit alternativel encoding schemes, exemplified by OLMo 2 32B that, ableit being highly accurate in arithmetics, can not be accurately probed by our sinusoidal probes.</p>
<p>Second, our intervention method was performed on a small-scale experiment, and its generalization across a large suite of models remains an object for future work.</p>
<p>Third, even when we do not perform any pretraining of models, reproducing our experiments requires access to computational resources.We estimate that replicating all our results requires around several hundred GPU hours.</p>
<p>Fourth, our analysis targets model embeddings.It is thus limited to single-token representations, and does not address the inner mechanisms of numeric information processing in large language model.This area also calls for further research.</p>
<p>While we recognize the ethical risks associated with AI research, given that our paper focuses on fundamentals of internal representations of numbers within pre-trained language models and their immediate impact on basic arithmetic tasks, broader societal ethical concerns like bias, discrimination, privacy, or job displacement are not directly relevant.Our research operates at a fundamental level of understanding how models encode numerical information, rather than exploring their application or misuse in real-world systems with downstream societal consequences.A.2 Explainability plots for arithmetic tasks.</p>
<p>Model behavior.To better explain the behavior of the LMs, we conduct a simple circuit analysis and a feature attribution experiment using integrated gradients (Sundararajan et al., 2017).For convenience, we focus on the two smaller models in our panel.OLMo 2 1B and Llama 3 1B.Both experiments suggest one major difference between operand pairs leading to failure and to success: the probability assigned by the LLM to the predicted output token tends to be statistically lower when the model produces an incorrect output, see Figure 4. We also observe the same subset of heads being activated for failure and success on the arithmetic task.Besides the usefulness of this difference in probability mass for diagnostic purposes, these experiments also suggest a difference in degree rather than kind between failures and successes.</p>
<p>In Figure 5, we present an overview of head-level attribution of the logits in Llama 2 1B.The same heads in Layers 13 through 15 appear activated in all cases, playing the same inhibitor and booster roles.Incorrectly performed addition leads to a noisier overall pattern.Remarkably, we observe that activity occurs in the latter stages of the model, whereas input embeddings (layer 0) already contain precise numeric information, as per our probing experiments.This delayed processing may explain some of the errors we observe, despite the high accuracy of our probes in Section 3.</p>
<p>A.3 Analysis of sin-base probes' learned representations</p>
<p>In Figure 6, we can see that our newly proposed sin-like probes indeed learn to project input em-beddings of models into an expected, generalized wave-like representation.</p>
<p>B Experimental details</p>
<p>1 "x 1 +x 2 equals to " 2 "The result of x 1 +x 2 is " 3 "The result of x 1 plus x 2 is " 4 "The result of x 1 plus x 2 = " 5 "The result of x 1 plus x 2 =" 6 "x 1 plus x 2 equals to " 7 "x 1 +x 2 =" 8 "x 1 plus x 2 equals " 9 "x 1 plus x 2 is equal to " 10 "x 1 +x 2 equals " 11 "x 1 +x 2 is equal to " 12 "x 1 plus x 2 equals " 13 "x 1 plus x 2 is equal to " (a) Prompts considered for addition task.x1 and x2 are placeholders for the augend and the addend.Prompts are delimited by double quotes; trailing white-space is significant.</p>
<p>1</p>
<p>"The result of x 1 minus x 2 is " 2 "The result of x 1 minus x 2 = " 3 "The result of x 1 minus x 2 =" 4 "x 1 minus x 2 equals to " 5 "x 1 -x 2 =" 6 "x 1 minus x 2 equals " 7 "x 1 minus x 2 is equal to " 8 "x 1 -x 2 equals " 9 "x 1 -x 2 is equal to " 10 "x 1 minus x 2 equals " 11 "x 1 minus x 2 is equal to " (b) Prompts considered for subtraction task.x1 and x2 are placeholders for the minuend and the subtrahend.Prompts are delimited by double quotes; trailing white-space is significant.We conduct a minimal prompt optimization in Section 4 to maximize the performances on arithmetic task.For all models below 20B parameters, we explore the prompts listed in Table 3a and Table 3b, and report results with the highest performance in a zero-shot setting in Section 4.</p>
<p>The most successful prompts for addition are prompt #4 for Llama 3 1B, 2B and 8B as well as OLMo2 1B, and prompt #3 for OLMO2 7B and 13B.As for subtraction, the most effective prompt was prompt #1 for Llama 3 1B, OLMo2 1B and 7B, and prompt #2 for Llama 3 3B and 8B as well as OLMo2 13B.</p>
<p>C Disclosure of usage of AI assistance</p>
<p>We disclose that we used AI assistance during implementation of this work and its writing.Specifically, we used AI-based code auto-completion (Github Copilot) for increasing productivity of programming, and conversational chatbots (OpenAI ChatGPT, Google Gemini) for improving grammar and fluency of the text.We guarantee that all content is original and factually accurate.</p>
<p>Figure 1 :
1
Figure 1: Overview of probes' accuracy (↑).</p>
<p>Figure 2 :
2
Figure 2: Visualization of PCA (DIM=16) reduced number embeddings, selected models.Although most model exhibit relatively regular wave-like patterns, OLMO 2 32B exhibit little regularity.</p>
<p>Figure 3 :Figure 6 :
36
Figure 3: Maximal contribution (magnitude) of each Fourier base frequency's to embedding features in PCA (d=128) reduced space.Sparsity in this plot indicates strong regularity in the hidden structure of model embeddings.OLMo 2 32B has noticeably stronger contribution of all low-contribution frequencies, indicating high irregularity.</p>
<p>1
fbinfsinflinflog lin1.000.960.981.000.990.961.000.940.00 0.25 0.50 0.75OLMo 2 1B 0.59 0.10 0.01 0.020.14OLMo 2 7B 0.03 0.030.12OLMo 2 13B 0.02 0.02OLMo 2 32B 0.07 0.07 0.00 0.000.33Llama 3 1B 0.03 0.050.41Llama 3 3B 0.04 0.050.18Llama 3 8B 0.01 0.030.33Llama 3 70B 0.02 0.020.40Phi 4 15B 0.01 0.02</p>
<p>Table 2 :
2
Downstream arithmetic error rate (%, ↓) given that (1) all tokens are decodable, and (2) at least one token is non-decodable.Results are measured on all possible input combinations.Phi is omitted because it does not make errors.
OLMo2 1BOLMo2 7BOLMo2 13BLlama 3 1BLlama 3 3BLlama 3 8BAdditiondecodable20.30 0.98 0.16 2.48 0.46 0.24undecodable 23.33 1.84 0.20 14.86 0.28 0.28Subtractiondecodable24.61 0.32 0.19 1.43 0.03 0.01undecodable 31.45 0.53 0.13 0.90 0.04 0.00</p>
<p>features to compute addition.In Advances in Neural Information Processing Systems, volume 37, pages 25120-25151.Curran Associates, Inc.Fangwei Zhu, Damai Dai, and Zhifang Sui.2025.Language models encode the value of numbers linearly.
In Proceedings of the 31st International Conferenceon Computational Linguistics, pages 693-709, AbuDhabi, UAE. Association for Computational Linguis-tics.A Supplementary visualizationA.1 Wave-like patterns in embeddingsFigure 2 displays the sinusoidal patterns in Llama3 70B and OLMo2 13B after PCA dimensionalityreduction. For clarity, we only include the first 16principal components.</p>
<p>Table 3 :
3
Prompts considered for engineering of arithmetic zero-shot setting.</p>
<p>https://github.com/prompteus/numllama
Acknowledgements
Jyoti Marah Abdin, Harkirat Aneja, Sébastien Behl, Ronen Bubeck, Suriya Eldan, Michael Gunasekar, Russell J Harrison, Mojan Hewett, Piero Javaheripi, James R Kauffmann, Yin Lee, Yuanzhi Tat Lee, Weishung Li, Liu, C T Caio, Anh Mendes, Eric Nguyen, Price, arXiv:2412.08905Olli Saarikivi, and 8 others. 2024. Phi-4 technical report. Gustavo de RosaPreprint</p>
<p>Large language models for mathematical reasoning: Progresses and challenges. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, arXiv:2402.001572024Preprint</p>
<p>Towards analysis and interpretation of large language models for arithmetic reasoning. Shapna Mst, Hossain Akter, Alfredo Shahriar, Cuzzocrea, 10.1109/SDS60720.2024.000492024 11th IEEE Swiss Conference on Data Science (SDS). 2024</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, 202012</p>
<p>Language models are few-shot learners. arXiv:2005.14165Preprint</p>
<p>Linear algebra with transformers. Francois Charton, Transactions on Machine Learning Research. 2022</p>
<p>How numerical precision affects mathematical reasoning capabilities of llms. Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, Liwei Wang, arXiv:2410.138572024Preprint</p>
<p>xval: A continuous number encoding for large language models. Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael Mccabe, Ruben Ohana, Liam Parker, Bruno Régaldo-Saint Blancard, Tiberiu Tesileanu, Kyunghyun Cho, Shirley Ho, NeurIPS 2023 AI for Science Workshop. 2023</p>
<p>Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, arXiv:2407.217832024PreprintThe llama 3 herd of models</p>
<p>Designing and interpreting probes with control tasks. John Hewitt, Percy Liang, 10.18653/v1/D19-1275Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Calc-x and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems. Marek Kadlčík, Michal Štefánik, Ondřej Sotolář, Vlastimil Martinek, Proceedings of the The 2023 Conference on Empirical Methods in Natural Language Processing: Main track, Singapore. the The 2023 Conference on Empirical Methods in Natural Language Processing: Main track, SingaporeSingaporeAssociation for Computational Linguistics2023</p>
<p>Language models use trigonometry to do addition. Subhash Kantamneni, Max Tegmark, arXiv:2502.008732025Preprint</p>
<p>Language models encode numbers using digit representations in base 10. Amit Arnold, Levy , Mor Geva, 10.18653/v1/2025.naacl-short.33Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language TechnologiesAlbuquerque, New MexicoAssociation for Computational Linguistics20252Short Papers)</p>
<p>Proving olympiad inequalities by synergizing llms and symbolic reasoning. Zenan Li, Zhaoyu Li, Wen Tang, Xian Zhang, Yuan Yao, Xujie Si, Fan Yang, Kaiyu Yang, Xiaoxing Ma, arXiv:2502.138342025Preprint</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Team Olmo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, arXiv:2501.00656Pradeep Dasigi, Nouha Dziri, and 21 others. 2025. 2 olmo 2 furious. Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher ClarkPreprint</p>
<p>Toolformer: language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessí, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Attend or perish: Benchmarking attention in algorithmic reasoning. Michal Spiegel, Michal Štefánik, Marek Kadlčík, Josef Kuchař, arXiv:2503.019092025Preprint</p>
<p>Concept-aware data construction improves in-context learning of language models. Michal Štefánik, Marek Kadlčík, Petr Sojka, 10.18653/v1/2024.findings-acl.733Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Axiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningPMLR201770of Proceedings of Machine Learning Research</p>
<p>Pre-trained large language models use fourier. Tianyi Zhou, Deqing Fu, Sharan Vatsal, Robin Jia, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
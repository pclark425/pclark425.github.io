<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8068 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8068</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8068</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-da3c41164c7502709cca508710127336e138621e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/da3c41164c7502709cca508710127336e138621e" target="_blank">TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> TreeEval is introduced, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage.</p>
                <p><strong>Paper Abstract:</strong> Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce TreeEval, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate 6 models of different parameter sizes, including 7B, 13B, and 34B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around 45 questions. We also conduct more analysis to show the robustness and reliability of TreeEval.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8068.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8068.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model as Judge evaluation paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation paradigm that uses a high-performance LLM to score or compare outputs from evaluated models, simulating human annotation but operating automatically and at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Evaluation of LLM capabilities (knowledge entailment and question-answering; general instruction-following evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Benchmark-free session; compared against AlpacaEval2.0, MT-Bench, MMLU, BBH</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (used by AlpacaEval / AlpacaEval2.0); GPT-4-0613 (used by authors as Examiner/Judge in TreeEval pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>AlpacaEval/AlpacaEval2.0 use ChatGPT as the judge for single-turn interactions (closed model); TreeEval uses GPT-4-0613 as the examiner (prompted LLM with temperature=1). Exact pretraining data and model internals unspecified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No direct LLM-vs-human agreement metric reported in this paper; paper instead cites literature about LLM-evaluator biases (positional, verbosity, style) and contrasts human evaluation qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>positional bias; verbosity bias (preference for more verbose answers); style bias (preference for answers matching LLM's own style, e.g., penalizing spelling errors); potential data leakage due to judge pretraining; limited domain expertise outside judge's strengths</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper states human evaluation is intuitive but slow and biased; LLM-as-judge simulates human evaluation and enables automation but introduces systematic biases (positional, verbosity, style) and potential data-leakage-related distortions. Authors note LLM judges can prefer verbose/style-matching responses and can be unreliable outside their expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Speed, automation, scalability, reproducibility, and ability to run standard evaluation pipelines without human labor; enables pairwise comparisons at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Discussion-level: paper compares TreeEval (benchmark-free) to LLM-as-judge baselines (AlpacaEval/AlpacaEval2.0 which use ChatGPT) and MT-Bench. Authors describe judge protocol used inside TreeEval: pairwise comparisons with exchange evaluation (judge invoked twice with swapped response order); scoring: 2 for agreed winner, 0 for loser, 1/1 if judge outputs disagree; memory/history passed to Examiner for follow-ups. Authors used GPT-4-0613 as Examiner with temperature=1; reference model for pairwise comparisons: Mistral-7B-Instruct-v0.2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8068.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8068.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TreeEval vs AlpacaEval2.0 correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Correlation between TreeEval rankings and AlpacaEval2.0 (LLM-as-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative agreement reported in this paper between the novel TreeEval method and an existing LLM-as-judge leaderboard (AlpacaEval2.0), using rank-correlation metrics to evaluate alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ranking and pairwise comparison of LLMs' overall performance on question-answering / instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Comparison against AlpacaEval2.0 leaderboard; also compared to MMLU and BBH benchmark re-implementations</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>AlpacaEval2.0 (ChatGPT used as judge for that leaderboard); TreeEval uses GPT-4-0613 as Examiner and GPT-4-based judging protocol inside experiments</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>AlpacaEval/AlpacaEval2.0: ChatGPT as judge (single-turn). TreeEval: GPT-4-0613 used to generate exam questions and to support judge interactions internally; temperature set to 1 for Examiner.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho; Kendall's tau (rank-correlation metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.83</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Not an LLM-vs-human comparison; reported caveats include that agreement is between two automated evaluation methods (TreeEval and AlpacaEval2.0) and thus inherits biases of LLM judges (data-leakage risk, judge bias).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>TreeEval achieves high rank-correlation with AlpacaEval2.0 while using far fewer questions (≈45 avg questions for TreeEval vs 804 for AlpacaEval2.0 in one comparison). Reported correlations: in main comparison TreeEval vs AlpacaEval2.0 Spearman rho = 0.83 and Kendall tau = 0.73; in an expanded experiment with more powerful models the paper reports rho = 0.94 and tau = 0.86 (authors present multiple experimental conditions). These high correlations indicate TreeEval reproduces LLM-as-judge rankings closely, but the paper does not report direct LLM-vs-human agreement numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Using an LLM-based judge (as in AlpacaEval2.0) provides a reference ranking that TreeEval can be compared to; LLM-judges enable leaderboard-style comparisons without human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>TreeEval experiments: 6 (main) to more models (extended) evaluated; TreeEval repeated 3 times to reduce randomness; examiner GPT-4-0613 temperature=1; termination depth T=3 and branching k=3; scoring via pairwise judge with exchange evaluation; correlations computed between model rankings from TreeEval and AlpacaEval2.0 using Spearman rho and Kendall tau. TreeEval uses ~45 questions on average per session; AlpacaEval2.0 uses ~804 questions (leaderboard values taken directly from Alpaca/MT-bench leaderboards).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models are not Fair Evaluators <em>(Rating: 2)</em></li>
                <li>Verbosity Bias in Preference Labeling by Large Language Models <em>(Rating: 2)</em></li>
                <li>Style Over Substance: Evaluation Biases for Large Language Models <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An Automatic Evaluator of Instruction-following Models <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8068",
    "paper_id": "paper-da3c41164c7502709cca508710127336e138621e",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "LLM-as-judge",
            "name_full": "Large Language Model as Judge evaluation paradigm",
            "brief_description": "An evaluation paradigm that uses a high-performance LLM to score or compare outputs from evaluated models, simulating human annotation but operating automatically and at scale.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
            "evaluation_task": "Evaluation of LLM capabilities (knowledge entailment and question-answering; general instruction-following evaluation)",
            "dataset_name": "Benchmark-free session; compared against AlpacaEval2.0, MT-Bench, MMLU, BBH",
            "judge_model_name": "ChatGPT (used by AlpacaEval / AlpacaEval2.0); GPT-4-0613 (used by authors as Examiner/Judge in TreeEval pipeline)",
            "judge_model_details": "AlpacaEval/AlpacaEval2.0 use ChatGPT as the judge for single-turn interactions (closed model); TreeEval uses GPT-4-0613 as the examiner (prompted LLM with temperature=1). Exact pretraining data and model internals unspecified in paper.",
            "human_evaluator_type": null,
            "agreement_metric": "No direct LLM-vs-human agreement metric reported in this paper; paper instead cites literature about LLM-evaluator biases (positional, verbosity, style) and contrasts human evaluation qualitatively.",
            "agreement_score": null,
            "reported_loss_aspects": "positional bias; verbosity bias (preference for more verbose answers); style bias (preference for answers matching LLM's own style, e.g., penalizing spelling errors); potential data leakage due to judge pretraining; limited domain expertise outside judge's strengths",
            "qualitative_findings": "Paper states human evaluation is intuitive but slow and biased; LLM-as-judge simulates human evaluation and enables automation but introduces systematic biases (positional, verbosity, style) and potential data-leakage-related distortions. Authors note LLM judges can prefer verbose/style-matching responses and can be unreliable outside their expertise.",
            "advantages_of_llm_judge": "Speed, automation, scalability, reproducibility, and ability to run standard evaluation pipelines without human labor; enables pairwise comparisons at scale.",
            "experimental_setting": "Discussion-level: paper compares TreeEval (benchmark-free) to LLM-as-judge baselines (AlpacaEval/AlpacaEval2.0 which use ChatGPT) and MT-Bench. Authors describe judge protocol used inside TreeEval: pairwise comparisons with exchange evaluation (judge invoked twice with swapped response order); scoring: 2 for agreed winner, 0 for loser, 1/1 if judge outputs disagree; memory/history passed to Examiner for follow-ups. Authors used GPT-4-0613 as Examiner with temperature=1; reference model for pairwise comparisons: Mistral-7B-Instruct-v0.2.",
            "uuid": "e8068.0",
            "source_info": {
                "paper_title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "TreeEval vs AlpacaEval2.0 correlation",
            "name_full": "Correlation between TreeEval rankings and AlpacaEval2.0 (LLM-as-judge)",
            "brief_description": "Quantitative agreement reported in this paper between the novel TreeEval method and an existing LLM-as-judge leaderboard (AlpacaEval2.0), using rank-correlation metrics to evaluate alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
            "evaluation_task": "Ranking and pairwise comparison of LLMs' overall performance on question-answering / instruction-following",
            "dataset_name": "Comparison against AlpacaEval2.0 leaderboard; also compared to MMLU and BBH benchmark re-implementations",
            "judge_model_name": "AlpacaEval2.0 (ChatGPT used as judge for that leaderboard); TreeEval uses GPT-4-0613 as Examiner and GPT-4-based judging protocol inside experiments",
            "judge_model_details": "AlpacaEval/AlpacaEval2.0: ChatGPT as judge (single-turn). TreeEval: GPT-4-0613 used to generate exam questions and to support judge interactions internally; temperature set to 1 for Examiner.",
            "human_evaluator_type": null,
            "agreement_metric": "Spearman's rho; Kendall's tau (rank-correlation metrics)",
            "agreement_score": 0.83,
            "reported_loss_aspects": "Not an LLM-vs-human comparison; reported caveats include that agreement is between two automated evaluation methods (TreeEval and AlpacaEval2.0) and thus inherits biases of LLM judges (data-leakage risk, judge bias).",
            "qualitative_findings": "TreeEval achieves high rank-correlation with AlpacaEval2.0 while using far fewer questions (≈45 avg questions for TreeEval vs 804 for AlpacaEval2.0 in one comparison). Reported correlations: in main comparison TreeEval vs AlpacaEval2.0 Spearman rho = 0.83 and Kendall tau = 0.73; in an expanded experiment with more powerful models the paper reports rho = 0.94 and tau = 0.86 (authors present multiple experimental conditions). These high correlations indicate TreeEval reproduces LLM-as-judge rankings closely, but the paper does not report direct LLM-vs-human agreement numbers.",
            "advantages_of_llm_judge": "Using an LLM-based judge (as in AlpacaEval2.0) provides a reference ranking that TreeEval can be compared to; LLM-judges enable leaderboard-style comparisons without human annotators.",
            "experimental_setting": "TreeEval experiments: 6 (main) to more models (extended) evaluated; TreeEval repeated 3 times to reduce randomness; examiner GPT-4-0613 temperature=1; termination depth T=3 and branching k=3; scoring via pairwise judge with exchange evaluation; correlations computed between model rankings from TreeEval and AlpacaEval2.0 using Spearman rho and Kendall tau. TreeEval uses ~45 questions on average per session; AlpacaEval2.0 uses ~804 questions (leaderboard values taken directly from Alpaca/MT-bench leaderboards).",
            "uuid": "e8068.1",
            "source_info": {
                "paper_title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models are not Fair Evaluators",
            "rating": 2
        },
        {
            "paper_title": "Verbosity Bias in Preference Labeling by Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Style Over Substance: Evaluation Biases for Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models",
            "rating": 2
        },
        {
            "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "rating": 2
        },
        {
            "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "rating": 1
        }
    ],
    "cost": 0.014886999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</h1>
<p>Xiang $\mathbf{L i}^{1}$, Yunshi Lan ${ }^{1 *}$, Chao Yang ${ }^{2}$<br>${ }^{1}$ East China Normal University<br>${ }^{2}$ Shanghai AI Laboratory<br>xiang.li@stu.ecnu.edu.cn, yslan@dase.ecnu.edu.cn, yangchao@pjlab.org.cn</p>
<h4>Abstract</h4>
<p>Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce TreeEval, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate 6 models of different parameter sizes, including 7B, 13B, and 33B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around 45 questions. We also conduct more analysis to show the robustness and reliability of TreeEval. Our code can be accessed via the provided URL ${ }^{1}$.</p>
<h2>Introduction</h2>
<p>The recent surge in Large Language Models (LLMs) has been significant, transitioning from closed-source (OpenAI 2023; Team 2023a) to open-source (Touvron et al. 2023; et al. 2023a; Jiang et al. 2023) models. Various Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) techniques have been proposed to further enhance the performance of LLMs (Taori et al. 2023; Chiang et al. 2023; Bai et al. 2022; Ouyang et al. 2022; Tunstall et al. 2023a). These LLMs demonstrate capabilities to address diverse tasks and are widely utilized in both academic and industrial fields. While human evaluation is intuitive for assessing the performance of LLMs, it is timeconsuming and susceptible to unexpected bias (Zheng et al. 2023b; Wang et al. 2024). Thus, investigating automatic evaluation approaches for LLMs becomes crucial.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>To date, numerous automatic evaluation methods have been proposed. One approach involves annotating benchmark datasets, such as MMLU and BBH (Hendrycks et al. 2021; Suzgun et al. 2022), to test various capabilities of an LLM. The performance is assessed by checking the overlap between annotated answers and generated answers, producing a holistic score to indicate the LLM's performance. We refer to this category of evaluation methods as the benchmark paradigm. However, the holistic score can be inflexible for measuring the quality of LLM outputs since token mismatches do not necessarily indicate incorrect answers.</p>
<p>With the advent of high-performance LLMs, another approach leverages them to simulate human evaluation. This involves providing the evaluated LLM with predefined benchmark questions and using another LLM, such as GPT-4, to judge its responses (Zheng et al. 2023a; Li et al. 2023b; Bai et al. 2023; Wang et al. 2023a; Zhang et al. 2023b; Wang et al. 2023c; Li et al. 2023a; Zhu, Wang, and Wang 2023). We refer to this category of evaluation methods as the LLM-as-judge paradigm. However, this evaluation approach can also introduce additional biases, including positional bias (Wang et al. 2023a), verbosity bias (Saito et al. 2023), and style bias (Wu and Aji 2023). Positional bias refers to the tendency to assign higher scores to answers based on their specific positions. Verbosity bias indicates that large language models often prefer more verbose answers, even if these longer responses are not necessarily of higher quality than shorter ones. Style bias manifests in the inclination of large language models to favor answers that match their own generated style, such as giving lower scores to correct responses with spelling errors, since LLMs rarely produce content with spelling mistakes.</p>
<p>Despite enabling automatic evaluation with standard pipelines, both the benchmark and LLM-as-judge paradigms face significant data leakage issues. The extensive training data used in LLM development, considered a valuable asset by many closed and even open-source models, can easily lead to benchmark data leakage, severely biasing evaluation results (Zhou et al. 2023b). To solve this issue, we propose a novel evaluation paradigm, which takes an LLM as an examiner to raise questions. The examiner should produce different evaluation session for each time which makes it hard to duplicate the evaluation questions and protect the evaluation benchmark from disclosure for fine-tuning and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: Comparison of TreeEval with existing evaluation paradigms.</p>
<p>pre-training an LLM deliberately. However, simply adopting an LLM as examiner would lead to arbitrary evaluation question generation without a goal. Designing such a benchmark-free evaluation method need take the following aspects into consideration: (1) Similar as the question in a benchmark <em>Taori et al. (2023); Zheng et al. (2023a)</em>, the generated questions should be derived from certain topics, which ensures the scope of the evaluation. (2) Drawing inspiration from the interview, within a topic, the examiner should generate a line of questions that are diverse to cover different knowledge rather than producing a single question. (3) The generation procedure should be flexible enough to generate mutually connected questions and control the difficulty level of these questions. When the current line of question cannot distinguish two LLMs, more difficult questions should be raised up. Otherwise, the evaluation could be terminated immediately.</p>
<p>To this end, we propose TreeEval, which is a benchmark-free evaluation of the knowledge implication and question-answering capabilities of LLM through tree planning. The line of questions within a topic for evaluation are organized in a tree, where each node contains a question. In the process of constructing a tree, we repeatedly revisit the status of the current tree and generate the next node until the tree is enough to differentiate two LLMs. The difference between our evaluation method and previous paradigms can be found in Figure 1. To verify the effect of our method, we evaluate multiple LLMs. The results demonstrate that our method shows similar ranking as AlpacaEval2.0 in LLM-as-judge paradigm with only $45$ questions in average for each round of evaluation. Further analysis shows our advantages in measuring fine-grained capabilities and conducting robust comparison for LLMs.</p>
<p>Our contributions are summarized as follows:</p>
<ul>
<li>We introduce a novel evaluation paradigm, TreeEval, which allows for efficient and comprehensive evaluation of LLMs, inherently preventing data leakage issues.</li>
<li>TreeEval has advantage in distinguishing two LLMs with similar performance by constructing a deeper tree, which extends the evaluation process to obtain more stable and accurate assessment results.</li>
<li>We compare with a set of automatic evaluation baselines, and find that our TreeEval achieves the highest correlation coefficient with AlpacaEval2.0.</li>
</ul>
<h2>Related Work</h2>
<h3>Methods of LLM Evaluation</h3>
<p>Due to the explosive growth and rapid update of LLMs, a significant challenge is to conduct accurate and comprehensive evaluation for them <em>Chang et al. (2023)</em>. Early studies leverage open-ended question answering datasets and math word problems as the evaluation benchmarks <em>Touvron et al. (2023); et al. (2023b); Chen et al. (2024)</em> to evaluate the commonsense knowledge and reasoning capabilities of LLMs. Subsequently, more benchmark datasets like MMLU <em>Hendrycks et al. (2021)</em>, AGIEval <em>Zhong et al. (2023)</em>, IFEval <em>Zhou et al. (2023a)</em> have been elaborately designed to gauge diverse abilities of LLMs. Some studies <em>Wang et al. (2023a, a); Saha et al. (2023)</em> go beyond standard evaluation metrics. They evaluate the quality and accuracy of predicted results through human annotation, which is able to provide a more comprehensive feedback. With the emergence of high-performance LLMs like GPT-4 <em>OpenAI (2023)</em>, Gemini Pro <em>Team (2023a)</em>, more recent studies start to utilize them to simulate the human evaluation process. In this realm, PandaLM <em>Wang et al. (2023c)</em> strives to provide reproducible and automated comparison between various LLMs by training a LLM as the judge. GPTScore <em>Fu et al. (2023)</em> and G-Eval <em>Liu et al. (2023)</em> utilize GPT-3 and GPT-4 as the judge to evaluate the LLMs with incorporation of in-context learning and chain-of-thought strategies. The above methods rely heavily on a well-organized benchmark dataset. However, there have been some recent works focusing on data leakage of LLM reviews. <em>Zhu et al. (2024)</em> proposed a method based on DAG to dynamically generate samples to evaluate LLM reasoning capabilities during the evaluation process. And our method is benchmark-free and has LLMs performing as the examiner to evaluate other models’ knowledge entailment and question answering capabilities.</p>
<h3>Data Leakage of LLM Evaluation</h3>
<p>As the number of benchmarks for language model evaluation increases, data leakage emerges as an inevitable concern. However, there appear to be a limited number of studies addressing this issue. Sainz et al.<em>(2023)</em> propose a method to detect data breaches in closed-source LLMs, based on the premise that LLMs can recall training data and</p>
<p>tend to reproduce similar content. Zhou et al.(2023b) conduct qualitative analysis of the impact of data leakage, which suggests that a data breach in one benchmark significantly enhances the LLM's performance on that specific benchmark while diminishing its capabilities on other uncompromised benchmarks. Yang et al.(2023) propose a more accurate approach which employs an LLM detector with top-k closest training data to determine if they match the test data. In contrast to these methods, which develop additional models for detecting data leakage during LLM evaluation with given benchmark datasets, our proposed method introduces a novel paradigm for LLM evaluation. It not only ensures the high quality of test questions but also inherently avoids data leakage.</p>
<h2>Methodology</h2>
<h2>Overall Architecture</h2>
<p>Figure 2 shows the overall structure of TreeEval. TreeEval organizes evaluations in a tree format, using components such as an Examiner, a Judge, and an Eval Controller. After the tree is built, an Aggregator compiles the scores. This framework allows for benchmark-free evaluation of LLMs through tree planning. Here's how it works:</p>
<ol>
<li>Session Setup: For each evaluation session, we choose two LLMs and start with an initial topic.</li>
<li>Question Generation: The Examiner generates questions within this topic.</li>
<li>Response Collection: These questions are sent to the LLMs, and their responses are collected.</li>
<li>Response Evaluation: The Judge compares the responses and decides the winner for each question.</li>
<li>Evaluation Control: If the responses are closely matched, the Eval Controller deepens the question. If a clear winner is found, the process moves to a new question. This follows a breadth-first search strategy, ensuring diverse and reliable questions.</li>
<li>Score Aggregation: Finally, the Aggregator compiles the scores from all nodes in the tree to produce a comprehensive evaluation score.
TreeEval uses a tree structure to evaluate LLMs, minimizing the number of questions needed. Questions are generated automatically, preventing benchmark leakage. The root node starts with the session's topic, and each node represents a question within that topic. Connections between nodes show how questions evolve. Deeper nodes indicate more similar abilities between the LLMs. Sibling nodes, derived from the same parent, cover different subtopics of the same main topic.</li>
</ol>
<h2>TreeEval Modules</h2>
<p>In this section, we provide more details of the components of the TreeEval and illustrate how to construct a tree for evaluation via these components.
Examiner. The examiner is a LLM-based module, which takes charge of generating exam questions that are able to cover diverse topics. Following (Bai et al. 2023), we predefine a set of topics as the scope of evaluation.</p>
<p>As the initialization of an evaluation session, we randomly sample a topic from the pre-defined topic set, which
is denoted as $\mathcal{F C}_{\text {pre-define }}$. Given a topic, the examiner is requested to craft a question that related to it via a prompt with the consideration of the coherence to the topic and the required format of the question. The detailed instruction is displayed in (Appendix Prompt for Examiner).</p>
<p>Once the session begins, we organize the follow-up questions in a tree structure. For simplify, we generally denote the follow-up topic at the $t$-th time step as $C_{t}$. And the above procedure can be presented as:</p>
<p>$$
Q_{t}=\operatorname{Examiner}\left(C_{t}\right)
$$</p>
<p>Subsequently, $Q_{t}$ is utilized as the question to test the LLMs under review.
Judge. Previous studies (Wang et al. 2023a) conduct pairwise comparison and identify the superior responses among two evaluated LLMs, which has advantage in providing more nuanced assessment. Following these studies, we consult a pair of LLMs with the same question. The detailed instruction is displayed in (Appendix Prompt for Judge). After the responses have been produced via the LLMs, another LLM performs as the judge to the responses.</p>
<p>To ensure the reliability of the judge, we further conduct exchange evaluation, that is to switch the order of the responses. This procedure can be denoted as:</p>
<p>$$
\begin{aligned}
&amp; S_{t}^{1}=\operatorname{Judge}\left(Q_{t}, A_{t}^{1}, A_{t}^{2}\right) \
&amp; S_{t}^{2}=\operatorname{Judge}\left(Q_{t}, A_{t}^{2}, A_{t}^{1}\right)
\end{aligned}
$$</p>
<p>where $A_{t}^{1}, A_{t}^{2}$ denote the responses from the pair of LLMs for $Q_{t}$. Each output judges the winner is $A_{t}^{1}$ or $A_{t}^{2}$ or a tie exits. If there is an agreement for $S_{t}^{1}$ and $S_{t}^{2}$, We assign 2 score to the winner and 0 score to the loser to form $S_{t}$. Otherwise, we assign 1 to each model as $S_{t}$.</p>
<p>As the evaluation proceeds, we maintain a memory to record the history of the session, including the initial topic, historical questions as well as responses from the two evaluated LLMs. After $Q_{t}$ has been responded, the history at the $t$-th time step in the evaluation session can be denoted as $\mathcal{M}<em 0="0">{t}=\left{C</em>\right}$. To involve the coherence of the flowing conversation and raise up rational follow-up questions, we prompt the examiner with the consideration of the history.
Eval Controller. The evaluation controller takes charge of the process of tree planning. Arbitrary generation of questions result in unorganized evaluation of LLMs with repeated questions and limited topics. To ensure the relevance and diversity of the generated questions, we have the following consideration: (1) To simulate the real-world interview of a certain subject, where the questions in an examination are mutually connected, we assume the generated followup question should be closely linked to its previous question via topics. For example, in Figure 2, inheriting from the root topic "technology and communication", we can raise a question on " $5 G$ " that is relevant to the root topic and goes deeper. (2) The generated questions should not be repeated in the existing questions and we should ensure the diverse knowledge covered by the tree. For example, in Figure 2, under the topic " $A I$ ", we can come up with distinct but related sub-topics as siblings such as "AI Ethics", "Accessibility Tools" and "Human Machine Interaction".}, Q_{0}, A_{0}^{1}, A_{0}^{1}, \ldots, C_{t}, Q_{t}, A_{t}^{1}, A_{t}^{2</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: TreeEval system with an illustrative tree for evaluation. The left section contains the components and their workflow in TreeEval. The right section displays a constructed tree within topic <em>Technology and Communication</em> for evaluation (the leaf nodes are shown in red boxes), where each node denotes a question annotated with its topic and evaluation score. We further display the generated questions of the tree in the (Appendix Eval Controller Example).</p>
<p>Algorithm 1: Procedure of TreeEval</p>
<p>1: Input $\mathcal{FC}<em t="t">{\text{pre-define}}$;
2: Initial $t \leftarrow 0$; $\mathcal{M}</em> \leftarrow \emptyset$
3: while Termination strategy is not satisfied do
4: for $C_{t} \in \mathcal{FC}<em 0="0">{\text{parent}}$ or $C</em>} \in \mathcal{FC<em t="t">{\text{pre-define}}$ do
5: $\tilde{\mathcal{Q}}</em>\right)$
6: $\quad Q_{t} \leftarrow \arg \max_{Q_{t}^{i} \in \tilde{\mathcal{Q}}} \leftarrow \operatorname{Examiner}\left(C_{t<em t="t">{t}}\left(\operatorname{Sim}\left(Q</em>}^{i}, C_{t}\right)-\max_{Q_{k} \in \mathcal{M<em t="t">{t}} \operatorname{Sim}\left(Q</em>\right)\right)$
7: $\quad A_{t}^{1}, A_{t}^{2} \leftarrow \operatorname{LLMs}\left(Q_{t}\right)$
8: $\quad S_{t}=\operatorname{Judge}\left(Q_{t}, A_{t}^{1}, A_{t}^{2}\right)$
9: $\quad \mathcal{M}}^{i}, Q_{k<em t="t">{t} \leftarrow \mathcal{M}</em>\right}$
10: $\quad \tilde{\mathcal{F}} \mathcal{C}} \cup\left{C_{t}, Q_{t}, A_{t}^{1}, A_{t}^{2<em t="t">{t} \leftarrow \operatorname{NER}\left(A</em>\right)$
11: $\quad \mathcal{F} \mathcal{C}}^{1}\right) \cup \operatorname{NER}\left(A_{t}^{2<em t="t">{t} \leftarrow \emptyset$
12: while $\left|\tilde{\mathcal{F}} \mathcal{C}</em>\right|&lt;k$ do
13: $\quad C_{t}^{i} \leftarrow \arg \max_{\tilde{C}<em t="t">{t}^{i} \in \tilde{\mathcal{F}} \mathcal{C}</em>}}\left(\operatorname{Sim}\left(\tilde{C<em t="t">{t}^{i}, C</em>\right)\right)$
14: $\quad \mathcal{F} \mathcal{C}<em t="t">{t} \leftarrow \mathcal{F} \mathcal{C}</em>\right}$
15: $\quad \tilde{\mathcal{F}} \mathcal{C}} \cup\left{C_{t}^{i<em t="t">{t} \leftarrow \tilde{\mathcal{F}} \mathcal{C}</em>$
16: for $\tilde{C}} \backslash C_{t}^{i<em t="t">{t}^{j} \in \tilde{\mathcal{F}} \mathcal{C}</em>$ do
17: $\quad \operatorname{Sim}\left(\tilde{C}<em t="t">{t}^{j}, C</em>}\right) \leftarrow \operatorname{Sim}\left(\tilde{C<em t="t">{t}^{j}, C</em>}\right)-\operatorname{Sim}\left(\tilde{C<em t="t">{t}^{j}, C</em>\right)$
18: $t \leftarrow t+1$}^{i</p>
<p>Inspired by the Tree-of-Thought (Long 2023), where a controller produces the next thought step, we let Eval Controller arrange the follow-up evaluation according to $\mathcal{M}<em t="t">{t}$. On the one hand, it prepares the follow-up topics $\tilde{\mathcal{F}} \mathcal{C}</em>}$ based on $\left{C_{t}, A_{t}^{1}, A_{t}^{2}\right} \in \mathcal{M<em t_1="t+1">{t}$ for any of its child nodes in advance. On the other hand, it determines $Q</em>}$ based on the $\mathcal{F} \mathcal{C<em 1="1">{t}$ and $\left{Q</em>$ if the $t$-th node is the parent node at $t+1$ time step. We next describe the above two steps in detail:}, Q_{2}, \ldots, Q_{t}\right} \in \mathcal{M}_{t</p>
<ul>
<li>Step One: Sample topics from the responses of the previous question: $\mathcal{F} \mathcal{C}<em t="t">{t} \sim \operatorname{NER}\left(A</em>$. This works better when the Named Entity Recognition (NER) tool is built upon a LLM as some relevant entities could be revised via the model instead of solely being extracted (Wang et al. 2023b).
We sample candidate topics from both $A_{t}^{1}$ and $A_{t}^{2}$ then}\right)^{2</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>merge them together, which results in a set of candidate topics $\tilde{\mathcal{F}} \mathcal{C}<em t="t">{t}$ as the follow-up topics of $t$-th node. However, this may produce some candidates that are repeated. To avoid this, we first measure the similarity between $C</em>}^{i} \in \tilde{\mathcal{F}} \mathcal{C<em t="t">{t}$ and $C</em>}$ by computing the Cosine Similarity of their encoded vector representation (Zhang et al. 2023a), which is denoted as $\operatorname{Sim}\left(C_{t}^{i}, C_{t}\right)$. Then, we iteratively push out $C_{t}^{i}$ with the largest score. Next, we update the similarity scores of the rest topic $C_{t}^{j}$ by subtracting the similarity score of $C_{t}^{j} \in \tilde{\mathcal{F}} \mathcal{C<em t="t">{t} \backslash C</em>$ for the follow-up question generation.}^{i}$ and $C_{t}^{i}$, which is to decrease the possibility of retrieving similar topics. This procedure continues until we have pushed out $k$ topics as $\mathcal{F} \mathcal{C}_{t</p>
<ul>
<li>Step Two: If the question at $(t+1)$-th time step is the child node of the node at $t$-th time step, we generate questions based on the sampled topic via $Q_{t+1}^{i} \sim$ $\operatorname{Examiner}\left(C_{t+1}\right)$, where $C_{t+1} \in \mathcal{F} \mathcal{C}<em t_1="t+1">{t}$. This could form a candidate question set $\tilde{\mathcal{Q}}</em>$. Still, to avoid repetition</li>
</ul>
<p>of the generated questions and ensure a broad spectrum of inquiry questions, we conduct ranking for the candidate questions. Specifically, we measure the similarity between $Q_{t+1}^{i} \in \tilde{\mathcal{Q}}<em t_1="t+1">{t+1}$ and $C</em>\right)$ and the least similarity score of $\arg \min }$ via Cosine Similarity. Then we push out $Q_{t+1}^{i}$ with the largest similarity score of $\operatorname{Sim}\left(Q_{t+1}^{i}, C_{t+1<em k="k">{Q</em>} \in \mathcal{M<em t_1="t+1">{t}} \operatorname{Sim}\left(Q</em>\right)$.
Termination Strategy. To determine whether we should stop generating subsequent questions along a topic, we identify several termination criteria:}^{i}, Q_{k</p>
<ul>
<li>For each node in the tree, if the question posed by the current topic successfully distinguish the capability of the two LLMs under review. Alternatively, if there is no tie for the current question, we terminate the child node search under the current node.</li>
<li>After we have generated the sibling nodes of a parent node, we revisit the scores of these siblings. If it shows a dominate score over all these siblings, this indicates that we have a winner for this branch. Hence we stop further search for any of these sibling nodes.</li>
<li>To prevent the evaluation session preceding indefinitely, a maximum depth $T$ for the tree search is pre-defined. Once the limit reaches, we terminate the child node search under the current topic.</li>
</ul>
<p>We terminate a tree search when every node in the tree satisfies the above criteria. The entire process is described in Algorithm 1.</p>
<h2>Score Aggregator</h2>
<p>After we have constructed the multiple trees across $\mathcal{F C}_{\text {pre-define }}$, where the nodes in each tree implies the win-rate between two LLMs under review towards a specific topic. To yield a final win-rate result, we aggregate the scores of these constructed trees. However, it is irrational to consider all the nodes in a tree equally due to their different features and result scores. Specifically, we take the following aspects of $t$-th node in a tree into account when we aggregate their scores:</p>
<ul>
<li>Distance to the root node. Based on the principle of an evaluation session, a longer distance to the root node indicates a more intensive competition between the evaluated LLMs and the more important the node is. This suggests that the winner only has a marginal advantage over the other one. Therefore, we define one aspect of an important node as $w_{t}^{\text {root }}=\frac{1}{d}$, where $d$ is the distance from the $t$-th node to the root node in a tree.</li>
<li>Origin of the topic. As the topic is derived from the responses in its parent node, a node inherited the topic generated from responses of the losing LLM is more important considering it is more likely to balance the situation. Hence, we define one aspect of an important node as:</li>
</ul>
<p>$$
w_{t}^{\text {topic }}=\left{\begin{array}{ll}
1 &amp; \text { Topic originated from the loser } \
0.5 &amp; \text { Otherwise }
\end{array}\right.
$$</p>
<ul>
<li>Variance of the sibling nodes. The disagreement of the evaluation of the sibling node may implicit a potential
randomness derived from the topic. So we define the sibling consensus as:</li>
</ul>
<p>$$
w_{t}^{\text {topic }}=\frac{1}{\sigma^{2}+1}
$$</p>
<p>where $\sigma$ is the variance of the score of its sibling nodes.
Considering the above aspects, we compute the final importance weights of $t$-th node as:</p>
<p>$$
w_{t}=w_{t}^{\text {root }}{ }^{\alpha} \cdot w_{t}^{\text {topic }}{ }^{\beta} \cdot w_{t}^{\text {sibling } \gamma}
$$</p>
<p>where $\alpha, \beta$, and $\gamma$ are hyper-parameters indicating the relative importance of these aspects. As a result, we sum up the $w_{t}$ multiplying with the win-rate of an LLM and devide the total evaluation questions to obtain its final scores:</p>
<p>$$
S=\frac{1}{N} \sum_{i \text {-th Tree from } \mathcal{F C}<em t="t">{\text {pre-define }} ; t \text {-th node in } i \text {-th Tree }} w</em>
$$} \cdot S_{t</p>
<p>where $N$ is the sum of node weights in the evaluation session and $S$ is normalized.</p>
<h2>Experiments</h2>
<h2>Experimental Setup</h2>
<p>Evaluated LLMs. We evaluated the following open-source LLMs, including two 7B models, two 13B models, and two 33B models. These models are either derived from LLaMA (Touvron et al. 2023; et al. 2023a) or trained from scratch using the LLaMA architecture, and some show similar performance according to the open-source LLM leaderboard ${ }^{3}$.</p>
<ul>
<li>Yi-34B-Chat (01.AI 2023) is a product from 01.AI, built on a large-scale multilingual dataset.</li>
<li>Xwin-LM-13B-V0.1 (Team 2023b) is based on LLaMA2-13B and tuned through SFT and RLHF.
Mistral-7B-Instruct-v0.2 (Jiang et al. 2023) is tuned on the Mistral-7B model, built with the LLaMA architecture.</li>
<li>Vicuna-33B-v1.3 (Zheng et al. 2023a) originates from LLaMA-33B and is fine-tuned using dialogues from ShareGPT.</li>
<li>WizardLM-13B-V1.2 (Xu et al. 2023) is based on LLaMA2-13B and fine-tuned with enhanced instruction data using Evol-Instruct.</li>
<li>Zephyr-7B-beta (Tunstall et al. 2023b) is derived from Mistral-7B and aligned using SFT and DPO methods.</li>
</ul>
<p>Comparable Evaluation Methods. We compare TreeEval with several existing methods, including:</p>
<ul>
<li>Benchmark Paradigm:</li>
<li>MMLU (Hendrycks et al. 2021)</li>
<li>Big-Bench Hard (BBH) (Suzgun et al. 2022)</li>
<li>LLMs as Judges:</li>
<li>AlpacaEval and AlpacaEval2.0 (Li et al. 2023b)</li>
<li>MT-Bench (Zheng et al. 2023a)</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>LLMs</th>
<th>MMLU^{∗}</th>
<th>BBH^{∗}</th>
<th>AlpacaEval^{†}</th>
<th>MT-bench^{†}</th>
<th>AlpacaEval2.0^{†}</th>
<th>TreeEval(Ours)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc</td>
<td>Acc</td>
<td>Win-Rate</td>
<td>score</td>
<td>Win-Rate</td>
<td>#Q</td>
<td>Score(var)</td>
</tr>
<tr>
<td>Mistral-7B-Instruct-v0.2</td>
<td>70.6</td>
<td>46.4</td>
<td>92.78</td>
<td>8.30</td>
<td>14.72</td>
<td>-</td>
<td>2.50(0.000)</td>
</tr>
<tr>
<td>Yi-34B-Chat</td>
<td>73.46</td>
<td>71.74</td>
<td>94.08</td>
<td>8.65</td>
<td>27.19</td>
<td>31.67</td>
<td>3.48(0.011)</td>
</tr>
<tr>
<td>xwinlm-13b-v0.1</td>
<td>56.6</td>
<td>37.58</td>
<td>91.76</td>
<td>7.34</td>
<td>17.43</td>
<td>62.33</td>
<td>2.67(0.000)</td>
</tr>
<tr>
<td>WizardLM-13B-V1.2</td>
<td>52.7</td>
<td>40.12</td>
<td>89.17</td>
<td>7.2</td>
<td>12.03</td>
<td>44.67</td>
<td>1.10(0.070)</td>
</tr>
<tr>
<td>zephyr-7b-beta</td>
<td>61.4</td>
<td>42.72</td>
<td>90.60</td>
<td>7.34</td>
<td>10.99</td>
<td>45.67</td>
<td>2.19(0.003)</td>
</tr>
<tr>
<td>Vicuna-33b-v1.3</td>
<td>59.2</td>
<td>52.0</td>
<td>88.99</td>
<td>7.12</td>
<td>12.71</td>
<td>41.33</td>
<td>1.61(0.044)</td>
</tr>
<tr>
<td>Average #Q ↓</td>
<td>14,079</td>
<td>6,511</td>
<td>804</td>
<td>80</td>
<td>804</td>
<td>45.1</td>
<td>-</td>
</tr>
<tr>
<td>ρ ↑</td>
<td>0.43</td>
<td>0.37</td>
<td>0.71</td>
<td>0.61</td>
<td>1.0</td>
<td>-</td>
<td>0.83</td>
</tr>
<tr>
<td>τ ↑</td>
<td>0.33</td>
<td>0.33</td>
<td>0.47</td>
<td>0.41</td>
<td>1.0</td>
<td>-</td>
<td>0.73</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of LLMs across various evaluation methods. “$\star$” denotes we re-implement MMLU and BBH benchmarks (Chia et al. 2023), calculating results in both 5-shot and 3-shot contexts. “$\dagger$” denotes we directly take results from the respective leader-boards from MT-bench, AlpacaEval, and AlpacaEval2.0. “#Q” denotes the number of questions used for evaluation. We report the correlation of rankings obtained through different methods with those from AlpacaEval2.0, using τ for the Kendall correlation coefficient (KENDALL 1938) and ρ for the Spearman correlation coefficient (Spearman 1904).</p>
<p>AlpacaEval and AlpacaEval2.0 use ChatGPT as the judge for single-turn interactions, while MT-Bench focuses on multi-turn dialogues.</p>
<p>Implementation Details. We use GPT-4-0613 as the examiner, deployed with FastChat (Zheng et al. 2023a). The temperature is set to 1 to generate varied questions. We set T and k to 3. Parameters α, β, and γ are set to 1, 1, and 0.4, respectively. To reduce randomness, we repeat the experiments three times, average the scores, and additionally report the variance across runs to demonstrate that our method enables more stable comparisons between models with similar performance. We use Mistral-7B-Instruct-v0.2 as the reference model for pairwise comparison due to its moderate performance on public leaderboards.</p>
<h3>Performance of TreeEval</h3>
<p>We display the performance of TreeEval in Table 1, from which we have the following observations: (1) Among all the comparable evaluation methods, our method is able to achieve the highest correlation coefficient with the rankings of AlpacaEval2.0 on the indicators of both ρ and τ. AlpacaEval2.0 is commonly viewed as the recognized LLM evaluation leader-board and the high consistency between our ranks indicates the reliability of our method. (2) Our method is able to complete the evaluation procedure with only 45 questions in average while the other evaluation methods require much more questions to generate an evaluation result. This indicates that our evaluation is efficient on evaluating LLMs with minimum questions. (3) Since we treat Mistral-7B-Instruct-v0.2 as the reference for the pairwise comparison, we notice the larger gap between the evaluated LLM and the reference is, the less test questions are proposed in the evaluation session, which shows that tree planning indeed meets our expected motivation. We further display the pairwise correlation in (Appendix More analysis) to show the correlation between TreeEval and AlpacaEval is also high.</p>
<h3>Further Analysis</h3>
<p>We further analyze to verify TreeEval's effect.</p>
<p>More powerful models. To demonstrate the performance of our approach in comparison with more powerful models, Table 2 presents the results of using Yi-34B-Chat as the baseline. We selected some of the most advanced open-source models currently available for testing against Yi-34B-Chat. Our TreeEval achieved performance closest to that of AlpacaEval2. More model results can be found in the (appendix More model results).</p>
<p>Pairwise Comparison for different model pairs. We iteratively change the references for the pairwise comparison and the results are shown in Table 3. Choosing the right baseline model is a critical step in our evaluation strategy. We selected the Mistral-7B-Instruct-v0.2 as our baseline, emphasizing the significance of selecting a baseline that accurately reflects the broad insights from pairwise model comparisons. Ideally, a baseline model should have a performance level that is neither too high nor too low, ensuring fair and balanced comparisons across all models. Interestingly, our observations indicate that even a baseline model chosen at random can lead to rankings that closely resemble those from a thorough pairwise evaluation. Thus, it's feasible to start with a randomly chosen baseline model to set up an initial order of performance. This preliminary order can be refined effectively using the bubble sort method. Given the initial order's similarity to the final ranking, this refinement</p>
<p>Due to the page limit, more analyses (i.e., Fine-grained Evaluation and robustness of TreeEval) are provided in (Appendix Fine-grained Evaluation) and (Appendix Robustness of TreeEval), respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLMs</th>
<th style="text-align: center;">MMLU*</th>
<th style="text-align: center;">BBH*</th>
<th style="text-align: center;">MT-bench ${ }^{\dagger}$</th>
<th style="text-align: center;">AlpacaEval2.0 ${ }^{\ddagger}$</th>
<th style="text-align: center;">TreeEval(Ours)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">score</td>
<td style="text-align: center;">Win-Rate</td>
<td style="text-align: center;">#Q</td>
<td style="text-align: center;">Score(Var)</td>
</tr>
<tr>
<td style="text-align: left;">Yi-34B-Chat</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">8.65</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$2.50(0.000)$</td>
</tr>
<tr>
<td style="text-align: left;">Qwen1.5-110B-Chat</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">8.88</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">42.67</td>
<td style="text-align: center;">$4.03(0.110)$</td>
</tr>
<tr>
<td style="text-align: left;">Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">8.92</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">36.33</td>
<td style="text-align: center;">$3.82(0.128)$</td>
</tr>
<tr>
<td style="text-align: left;">Qwen1.5-72B-Chat</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">8.61</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">31.33</td>
<td style="text-align: center;">$3.45(0.089)$</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8x7B-Instruct-v0.1</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">8.30</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">44.67</td>
<td style="text-align: center;">$2.02(0.027)$</td>
</tr>
<tr>
<td style="text-align: left;">vicuna-33b-v1.3</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">7.12</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">21.33</td>
<td style="text-align: center;">$0.35(0.033)$</td>
</tr>
<tr>
<td style="text-align: left;">$\rho \uparrow$</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{0 . 9 4}$</td>
</tr>
<tr>
<td style="text-align: left;">$\tau \uparrow$</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{0 . 8 6}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of More Powerful Models. Given the strong capabilities of the evaluated models, we use Yi-34B-Chat as the baseline and exclude the AlpacaEval benchmark, which is relatively simple for these models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Yi-34B <br> -Chat</th>
<th style="text-align: center;">Xwin-LM <br> -13B-V0.1</th>
<th style="text-align: center;">Mistral-7B <br> -Instruct-v0.2</th>
<th style="text-align: center;">vicuna <br> -33b-v1.3</th>
<th style="text-align: center;">WizardLM <br> -13B-V1.2</th>
<th style="text-align: center;">zephyr <br> -7b-beta</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Yi-34B-Chat</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$1.88(0.400)$</td>
<td style="text-align: center;">$1.52(0.010)$</td>
<td style="text-align: center;">$2.1(0.070)$</td>
<td style="text-align: center;">$1.21(0.076)$</td>
<td style="text-align: center;">$1.75(0.143)$</td>
</tr>
<tr>
<td style="text-align: left;">Xwin-LM-13B-V0.1</td>
<td style="text-align: center;">$3.12(0.400)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$2.33(0.000)$</td>
<td style="text-align: center;">$1.53(0.403)$</td>
<td style="text-align: center;">$1.57(0.109)$</td>
<td style="text-align: center;">$2.41(0.000)$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B-Instruct-v0.2</td>
<td style="text-align: center;">$3.48(0.010)$</td>
<td style="text-align: center;">$2.67(0.000)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$1.61(0.044)$</td>
<td style="text-align: center;">$1.10(0.070)$</td>
<td style="text-align: center;">$2.19(0.003)$</td>
</tr>
<tr>
<td style="text-align: left;">vicuna-33b-v1.3</td>
<td style="text-align: center;">$2.9(0.070)$</td>
<td style="text-align: center;">$3.47(0.403)$</td>
<td style="text-align: center;">$3.39(0.044)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$2.01(0.374)$</td>
<td style="text-align: center;">$3.7(0.071)$</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">$3.79(0.076)$</td>
<td style="text-align: center;">$3.43(0.109)$</td>
<td style="text-align: center;">$3.90(0.070)$</td>
<td style="text-align: center;">$2.99(0.374)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$3.94(0.069)$</td>
</tr>
<tr>
<td style="text-align: left;">zephyr-7b-beta</td>
<td style="text-align: center;">$3.25(0.143)$</td>
<td style="text-align: center;">$2.59(0.000)$</td>
<td style="text-align: center;">$2.81(0.003)$</td>
<td style="text-align: center;">$1.3(0.071)$</td>
<td style="text-align: center;">$1.06(0.069)$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Our result for each model pairs. The elements in this table represent the scores obtained by comparing models using treeEval, with the column model being compared against the row model.
process tends towards an $O(n)$ complexity, significantly enhancing the evaluation's precision and efficiency.
Ablation Studies. As we can see in Table 4, changing BFS search to DFS search dramatically increases the number of questions but decreases the performance. This is because DFS search generates the child node first rather than the sibling node such that the influence of sibling node will be neglected in both question generation and termination identification procedures. Removing step one, which indicates skip the topic generation step, decreases the performance. This indicates the significant role of identifying the topic for question generation. When we iteratively remove the scores in aggregator, we observe general performance drop on $\tau$. This indicates that all the scores in the aggregator are important in producing a comprehensive score.
Case studies are presented in (Appendix Case Studies).</p>
<h2>Conclusions</h2>
<p>In this paper, we introduce TreeEval, a benchmark-free evaluation approach for LLMs with tree planning, which automatically controls the evaluation process with tree planning. We experimentally verify that TreeEval can not only produce reliable evaluation results without data leakage but also enhance discrimination between similarly performing LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">#Q</th>
<th style="text-align: center;">$\rho$</th>
<th style="text-align: center;">$\tau$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TreeEval</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: left;">BFS $\rightarrow$ DFS</td>
<td style="text-align: center;">149.4</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">w/o Step One</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">w/o $w^{\text {root }}$</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;">w/o $w^{\text {topic }}$</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;">w/o $w^{\text {obling }}$</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.47</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation study on TreeEval.</p>
<h2>Limitations</h2>
<p>Using LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias.</p>
<p>While GPT-4 is a powerful examiner, it has limitations, particularly in areas outside its expertise. This can be addressed by providing more contextual guidance during evaluations. In the future, training specialized evaluators to extract questions from document repositories and assess comprehension could ensure more accurate, domain-specific evaluations.</p>
<h2>Ethical Considerations</h2>
<p>Although we prioritize the security of the LLMs we use during evaluations, striving to employ aligned LLMs with higher safety standards, and endeavor to ensure that LLM outputs adhere to ethical and legal requirements, limitations arising from model size and probabilistic generation paradigms may lead to various unexpected outputs. These could include questions or responses containing biases, discrimination, or other harmful content. Please refrain from disseminating such content.</p>
<h2>Acknowledgements</h2>
<p>The authors would like to thank the anonymous reviewers for their insightful comments. This work is supported by the National Key Research \&amp; Develop Plan (2023YFF0725100) and Young Scientists Project of National Natural Science Foundation (Project No. 62206097).</p>
<h2>References</h2>
<p>01.AI. 2023. Yi.</p>
<p>Bai, Y.; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; DasSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.; Joseph, N.; Kadavath, S.; Kernion, J.; Conerly, T.; ElShowk, S.; Elhage, N.; Hatfield-Dodds, Z.; Hernandez, D.; Hume, T.; Johnston, S.; Kravec, S.; Lovitt, L.; Nanda, N.; Olsson, C.; Amodei, D.; Brown, T.; Clark, J.; McCandlish, S.; Olah, C.; Mann, B.; and Kaplan, J. 2022. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. arXiv preprint arXiv:2204.05862.
Bai, Y.; Ying, J.; Cao, Y.; Lv, X.; He, Y.; Wang, X.; Yu, J.; Zeng, K.; Xiao, Y.; Lyu, H.; Zhang, J.; Li, J.; and Hou, L. 2023. Benchmarking Foundation Models with Language-Model-as-an-Examiner. arXiv preprint arXiv:2306.04181.
Chang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu, K.; Chen, H.; Yi, X.; Wang, C.; Wang, Y.; Ye, W.; Zhang, Y.; Chang, Y.; Yu, P. S.; Yang, Q.; and Xie, X. 2023. A Survey on Evaluation of Large Language Models. arXiv preprint arXiv:2307.03109.
Chen, H.; Jiao, F.; Li, X.; Qin, C.; Ravaut, M.; Zhao, R.; Xiong, C.; and Joty, S. 2024. ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up? arXiv preprint arXiv:2311.16989.
Chen, L.; Saifullah, K.; Li, M.; Zhou, T.; and Huang, H. 2023. Claude2-Alpaca: Instruction tuning datasets distilled from claude. https://github.com/Lichang-Chen/claude2alpaca.
Chia, Y. K.; Hong, P.; Bing, L.; and Poria, S. 2023. INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. arXiv preprint arXiv:2306.04757.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality.
et al., H. T. 2023a. Llama 2: Open Foundation and FineTuned Chat Models. arXiv preprint arXiv:2307.09288.
et al., R. A. 2023b. PaLM 2 Technical Report. arXiv preprint arXiv:2305.10403.
Fu, J.; Ng, S.-K.; Jiang, Z.; and Liu, P. 2023. GPTScore: Evaluate as You Desire. arXiv preprint arXiv:2302.04166.
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding. arXiv preprint arXiv:2009.03300.
Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; de las Casas, D.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.A.; Stock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and Sayed, W. E. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825.
KENDALL, M. G. 1938. A NEW MEASURE OF RANK CORRELATION. Biometrika, 30(1-2): 81-93.
Li, J.; Sun, S.; Yuan, W.; Fan, R.-Z.; Zhao, H.; and Liu, P. 2023a. Generative Judge for Evaluating Alignment. arXiv preprint arXiv:2310.05470.
Li, X.; Lan, Y.; and Yang, C. 2024. TreeEval: BenchmarkFree Evaluation of Large Language Models through Tree Planning. arXiv preprint arXiv:2402.13125.
Li, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-lab/alpaca_eval.
Liu, Y.; Iter, D.; Xu, Y.; Wang, S.; Xu, R.; and Zhu, C. 2023. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. arXiv preprint arXiv:2303.16634.
Long, J. 2023. Large Language Model Guided Tree-ofThought. arXiv preprint arXiv:2305.08291.
OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and Lowe, R. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.
Saha, S.; Levy, O.; Celikyilmaz, A.; Bansal, M.; Weston, J.; and Li, X. 2023. Branch-Solve-Merge Improves Large Language Model Evaluation and Generation. arXiv preprint arXiv:2310.15123.
Sainz, O.; Campos, J.; García-Ferrero, I.; Etxaniz, J.; de Lacalle, O. L.; and Agirre, E. 2023. NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. In Bouamor, H.; Pino, J.; and Bali, K., eds., Findings of the Association for Computational Linguistics: EMNLP 2023, 10776-10787. Singapore: Association for Computational Linguistics.
Saito, K.; Wachi, A.; Wataoka, K.; and Akimoto, Y. 2023. Verbosity Bias in Preference Labeling by Large Language Models. arXiv preprint arXiv:2310.10076.</p>
<p>Spearman, C. 1904. The Proof and Measurement of Association between Two Things. The American Journal of Psychology, 15(1): 72-101.
Suzgun, M.; Scales, N.; Schärli, N.; Gehrmann, S.; Tay, Y.; Chung, H. W.; Chowdhery, A.; Le, Q. V.; Chi, E. H.; Zhou, D.; and Wei, J. 2022. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. arXiv preprint arXiv:2210.09261.
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford Alpaca: An Instruction-following LLaMA model. GitHub repository.
Team, G. 2023a. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805.
Team, X.-L. 2023b. Xwin-LM.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971.
Tunstall, L.; Beeching, E.; Lambert, N.; Rajani, N.; Huang, S.; Rasul, K.; Rush, A. M.; and Wolf, T. 2023a. The Alignment Handbook. https://github.com/huggingface/ alignment-handbook.
Tunstall, L.; Beeching, E.; Lambert, N.; Rajani, N.; Rasul, K.; Belkada, Y.; Huang, S.; von Werra, L.; Fourrier, C.; Habib, N.; Sarrazin, N.; Sanseviero, O.; Rush, A. M.; and Wolf, T. 2023b. Zephyr: Direct Distillation of LM Alignment. arXiv preprint arXiv:2310.16944.
Wang, B.; Zheng, R.; Chen, L.; Liu, Y.; Dou, S.; Huang, C.; Shen, W.; Jin, S.; Zhou, E.; Shi, C.; Gao, S.; Xu, N.; Zhou, Y.; Fan, X.; Xi, Z.; Zhao, J.; Wang, X.; Ji, T.; Yan, H.; Shen, L.; Chen, Z.; Gui, T.; Zhang, Q.; Qiu, X.; Huang, X.; Wu, Z.; and Jiang, Y.-G. 2024. Secrets of RLHF in Large Language Models Part II: Reward Modeling. arXiv preprint arXiv:2401.06080.
Wang, P.; Li, L.; Chen, L.; Cai, Z.; Zhu, D.; Lin, B.; Cao, Y.; Liu, Q.; Liu, T.; and Sui, Z. 2023a. Large Language Models are not Fair Evaluators. arXiv preprint arXiv:2305.17926.
Wang, S.; Sun, X.; Li, X.; Ouyang, R.; Wu, F.; Zhang, T.; Li, J.; and Wang, G. 2023b. GPT-NER: Named Entity Recognition via Large Language Models. arXiv preprint arXiv:2304.10428.
Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.; Ye, W.; Zhang, S.; and Zhang, Y. 2023c. PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. arXiv preprint arXiv:2306.05087.
Wu, M.; and Aji, A. F. 2023. Style Over Substance: Evaluation Biases for Large Language Models. arXiv preprint arXiv:2307.03025.
Xu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng, J.; Tao, C.; and Jiang, D. 2023. WizardLM: Empowering Large Language Models to Follow Complex Instructions. arXiv preprint arXiv:2304.12244.</p>
<p>Yang, S.; Chiang, W.-L.; Zheng, L.; Gonzalez, J. E.; and Stoica, I. 2023. Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. arXiv preprint arXiv:2311.04850.
Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al. 2022. Glm130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.
Zhang, P.; Xiao, S.; Liu, Z.; Dou, Z.; and Nie, J.-Y. 2023a. Retrieve Anything To Augment Large Language Models. arXiv preprint arXiv:2310.07554.
Zhang, X.; Yu, B.; Yu, H.; Lv, Y.; Liu, T.; Huang, F.; Xu, H.; and Li, Y. 2023b. Wider and Deeper LLM Networks are Fairer LLM Evaluators. arXiv preprint arXiv:2308.01862.
Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023a. Judging LLM-as-aJudge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685.
Zheng, R.; Dou, S.; Gao, S.; Hua, Y.; Shen, W.; Wang, B.; Liu, Y.; Jin, S.; Liu, Q.; Zhou, Y.; Xiong, L.; Chen, L.; Xi, Z.; Xu, N.; Lai, W.; Zhu, M.; Chang, C.; Yin, Z.; Weng, R.; Cheng, W.; Huang, H.; Sun, T.; Yan, H.; Gui, T.; Zhang, Q.; Qiu, X.; and Huang, X. 2023b. Secrets of RLHF in Large Language Models Part I: PPO. arXiv preprint arXiv:2307.04964.
Zhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang, Y.; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. arXiv preprint arXiv:2304.06364.
Zhou, J.; Lu, T.; Mishra, S.; Brahma, S.; Basu, S.; Luan, Y.; Zhou, D.; and Hou, L. 2023a. Instruction-Following Evaluation for Large Language Models. arXiv preprint arXiv:2311.07911.
Zhou, K.; Zhu, Y.; Chen, Z.; Chen, W.; Zhao, W. X.; Chen, X.; Lin, Y.; Wen, J.-R.; and Han, J. 2023b. Don’t Make Your LLM an Evaluation Benchmark Cheater. arXiv preprint arXiv:2311.01964.
Zhu, K.; Chen, J.; Wang, J.; Gong, N. Z.; Yang, D.; and Xie, X. 2024. DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks. arXiv preprint arXiv:2309.17167.
Zhu, L.; Wang, X.; and Wang, X. 2023. JudgeLM: Finetuned Large Language Models are Scalable Judges. arXiv preprint arXiv:2310.17631.</p>
<h1>Appendix</h1>
<h2>More analysis</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Radar chart illustrating the scores of various LLMs under different pre-defined topics.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Re-run TreeEval 5 times for various LLMs.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Examples of evaluation process for two pairs of LLMs under topic "Business and Finance", which are shown in two colored trees. The detailed contents of a node is displayed in a dashed box and the recognized entities used for follow-up topics are shown in red fonts.</p>
<p>Fine-grained Evaluation. From the Figure 3, we can observe that different LLMs excel in various knowledge domains. The performance of the same model may vary across different fields. For example, Yi-34B-Chat is short in Travel and Shopping while it demonstrates relatively good performance on other topics. Through our TreeEval, we can diagnose an LLM with the fine-grained results in diverse domains.</p>
<p>Robustness of TreeEval. We draw Figure 4 to show the evaluation results of different LLMs in multiple runs. We can see that, for a given LLM under evaluation, conducting multiple repeated experiments yields relatively similar scores when the examiner's temperature is set to 1 . The low variance indicates that TreeEval is able to generate stable and robust results.</p>
<p>Case Studies. In Figure 5, it's clear that TreeEval effectively identifies performance gaps between LLMs. For instance, Mistral-7B-Instruct-v0.2 notably outperforms WizardLM-13B-V1.2, reflected in a smaller tree. Conversely, when models perform similarly, like Mistral-7B-Instruct-v0.2 and Xwin-LM-13B-V0.1, TreeEval constructs a larger tree to discern subtle performance differences.</p>
<p>More model results. To further demonstrate the performance of our method on different models to be tested, we also show the performance of the vicuna series models as well as chatglm2-6b (Zeng et al. 2022) and alpaca-13b (Chen et al. 2023) in Table 5. But the main analysis still focuses on the previous six models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLMs</th>
<th style="text-align: center;">MMLU*</th>
<th style="text-align: center;">BBH*</th>
<th style="text-align: center;">AlpacaEval ${ }^{\dagger}$</th>
<th style="text-align: center;">MT-bench ${ }^{\dagger}$</th>
<th style="text-align: center;">AlpacaEval2.0 ${ }^{\dagger}$</th>
<th style="text-align: center;">TreeEval(Ours)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Win-Rate</td>
<td style="text-align: center;">score</td>
<td style="text-align: center;">Win-Rate</td>
<td style="text-align: center;">#Q</td>
<td style="text-align: center;">Score</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B-Instruct-v0.2</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">92.78</td>
<td style="text-align: center;">8.30</td>
<td style="text-align: center;">14.72</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.50</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7b-v1.3</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">76.84</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">4.64</td>
<td style="text-align: center;">34.67</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7b-v1.5</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">74.50</td>
<td style="text-align: center;">6.17</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">36.33</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-13b-v1.3</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">82.11</td>
<td style="text-align: center;">6.39</td>
<td style="text-align: center;">7.13</td>
<td style="text-align: center;">38.33</td>
<td style="text-align: center;">1.28</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-13b-v1.5</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">81.07</td>
<td style="text-align: center;">6.57</td>
<td style="text-align: center;">6.72</td>
<td style="text-align: center;">34.33</td>
<td style="text-align: center;">1.06</td>
</tr>
<tr>
<td style="text-align: left;">chatglm2-6b</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">47.12</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">2.76</td>
<td style="text-align: center;">39.67</td>
<td style="text-align: center;">1.12</td>
</tr>
<tr>
<td style="text-align: left;">alpaca-13b</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">78.93</td>
<td style="text-align: center;">6.53</td>
<td style="text-align: center;">7.43</td>
<td style="text-align: center;">55.00</td>
<td style="text-align: center;">2.19</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-14B-chat</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">79.83</td>
<td style="text-align: center;">6.96</td>
<td style="text-align: center;">7.50</td>
<td style="text-align: center;">36.67</td>
<td style="text-align: center;">1.40</td>
</tr>
<tr>
<td style="text-align: left;">Starling-LM-7B-alpha</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">91.99</td>
<td style="text-align: center;">8.09</td>
<td style="text-align: center;">14.24</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">2.36</td>
</tr>
<tr>
<td style="text-align: left;">$\rho \uparrow$</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{0 . 7 0}$</td>
</tr>
<tr>
<td style="text-align: left;">$\tau \uparrow$</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{0 . 5 8}$</td>
</tr>
</tbody>
</table>
<p>Table 5: More model results
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: To facilitate analysis, Figure 2 is re-placed here.</p>
<h1>Eval Controller Example</h1>
<p>For the first aspect: the generated follow-up question should be closely linked to its previous question via topics. For example, in Figure 2, we generate $Q_{2}$ as "How does 5G technology improve Internet of Things (IoT) applications and smart city initiatives?" Based on the $C_{6}$ we obtained we generated a new question $Q_{6}$ "In what ways does 5G technology enable advancements in smart home devices and automation?", $Q_{6}$ is an expansion and extension of $Q_{2}$ on $C_{6}$ "applications".</p>
<p>For the second aspect: the generated questions should not be repeated in the existing questions. For example, in Figure 2, $Q_{5}$ was initially "What are the ethical considerations when designing AI systems for human-machine interaction?" However, this significantly overlapped with our previously established $Q_{3}$ "What are the key ethical considerations when developing AI technologies for communication platforms?" Consequently, we opted for $Q_{5}$ to be "What role does AI play in the development of voice-activated systems, and how does it change human-machine interaction?" to ensure variety and specificity in our discussion topics.</p>
<h2>Prompt for Examiner</h2>
<p>I want you to assume the role of the expert and ask a question that expands and reflects your understanding of ${$ topic $}$. Your task is to ask a question about ${$ topic $}$. Only through a profound understanding of ${$ topic $}$ can one correctly answer this question. Please adhere strictly to the following 4 task guidelines:</p>
<ol>
<li>Your question should begin with a question word, such as "what", "which", "when", "where", "how", "why", etc.</li>
<li>The objective of your question should be to manifest the respondent's understanding of ${$ topic $}$ and to differentiate respondents based on their comprehension level.</li>
<li>
<p>Questions should be self-explanatory, not requiring additional context or clarification.</p>
</li>
<li>
<p>Please format your question in the JSON structure provided below. Remember, only output the content in the following format, and nothing else: ${{$ "question": your question $}}$</p>
</li>
</ol>
<h1>Prompt for Judge</h1>
<p>You are assessing two submitted responses to a user's query based on specific criteria. Evaluate the quality, relevance, accuracy, clarity, and any other relevant factors to determine which response is superior, or if they are equally valuable or lacking. Here is the data for your assessment:</p>
<p>[Response 1]: {answer 1}
[Response 2]: {answer 2}
Assessment Criteria:</p>
<ol>
<li>Relevance to the query: Does the response directly address the user's question or concern?</li>
<li>Accuracy of information: Are the facts or solutions provided in the response correct and reliable?</li>
<li>Clarity and comprehensibility: Is the response easy to understand, well-structured, and free of jargon or ambiguity?</li>
<li>Completeness: Does the response cover all aspects of the query or offer a comprehensive solution?</li>
<li>Additional value: Does the response provide extra insights, tips, or information that enhances the user's understanding or solves the problem more effectively?
Instructions for Assessment:</li>
<li>Identify and focus on the criteria that significantly distinguish the two responses. Disregard criteria that do not offer a clear distinction.</li>
<li>Consider any specific aspects of the query and the responses that may require additional factors for a fair comparison. Mention these factors explicitly.</li>
<li>Conclude your assessment by deciding which response is better, or if they are tied. Your decision must be based on a coherent evaluation across the mentioned criteria and any additional factors you've identified.
Please return your final decision in the following JSON format: {"Eval_result": "Response 1"/"Response 2"/"Tie"}
Note: Remember, the output should only contain the decision in the specified JSON format and nothing else.</li>
</ol>
<h2>Prompt for NER</h2>
<p>You are asking questions and answers based on a topic you know and based on this topic. Please extract some subtopics from the answers. Here's an example:</p>
<p>Here is the data:
[Input data]</p>
<hr />
<p><a href="0">topic</a>: programming languages</p>
<hr />
<p><a href="1">question</a>: Which programming languages can you write code in?</p>
<hr />
<p><a href="2">answer</a>: I know python, C++, R language, etc.</p>
<hr />
<p>[Output Data]</p>
<hr />
<p>[subtopic] : ["python","C++","R language"]
now the official question
Here is the data:
[Input data]</p>
<hr />
<hr />
<hr />
<hr />
<p>[Output Data]</p>
<hr />
<p>Please return your final decision in list format. Remember, you only need to output the content in the following List format, with each element as a subtopic and nothing else. Remember, you only need to output the three most important subtopics in the following List format.
[subtopic] : [",subtopic1","subtopic2","subtopic3"]</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://tatsu-lab.github.io/alpaca_eval/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
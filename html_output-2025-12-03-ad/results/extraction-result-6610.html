<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6610 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6610</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6610</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-fa0beb3f4d7f6e7e49b153af7e8a7c30f2937b60</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fa0beb3f4d7f6e7e49b153af7e8a7c30f2937b60" target="_blank">Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states is studied.</p>
                <p><strong>Paper Abstract:</strong> Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps. This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state. We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly long-term dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6610.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6610.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Attentive Backtracking (SAB) - augmented LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM augmented with a growing episodic memory of past hidden states and a learned hard (sparse) self-attention that retrieves at most k_top past microstates; retrieved summaries are added to the current hidden state and gradients are backpropagated only through those sparse retrieval paths (plus optional local TBPTT 'mental updates').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAB-augmented LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A unidirectional LSTM whose hidden states (every k_att'th timestep) are stored in a memory list; at each step a small MLP scores concatenations of the provisional hidden state with every memory entry to produce raw attention scores, the top-k_top scores are kept (sparsifier: subtract (k_top+1)th score, ReLU, normalize), a weighted sum of the selected memory vectors is computed and added to the provisional hidden state to form the final state; during backward pass gradients flow only through the sequential path plus the sparse skip connections and local k_trunc TBPTT around selected memories ('mental updates').</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Task-dependent: experiments use 128 hidden units for synthetic and vision tasks; 1000 hidden units for language tasks (parameter count not explicitly reported).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Growing episodic buffer of past hidden states (implicit external memory / list of microstates)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Hidden-state vectors (microstates) of the underlying LSTM, stored every k_att timesteps</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned content-based scoring via an MLP over (provisional hidden state || memory vector) producing scalar raw scores; sparsification by selecting top k_top (subtract (k_top+1)th, ReLU, renormalize) to produce sparse attention weights; write: append current hidden state to memory when t mod k_att == 0; updates: local TBPTT of k_trunc timesteps around attended memories ('mental updates').</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Copying memory (T up to 5000 transfer), Adding (T=200,400), Character-level PTB, Text8, permuted pixel MNIST (pMNIST), CIFAR10 (pixel-by-pixel), permuted MNIST, other synthetic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-range temporal dependency benchmarks (copy/add), language modeling, sequential image classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Representative reported results (selected best configs): Copying: T=200 accuracy ≈ 95% (last-10 accuracy, Table 5), T=300 ≈ 83%, T=400 ≈ 75%; Adding: T=200 CE as low as 4.26e-5 (different configs; Table 2/3); PTB char-level BPC ~ 1.37–1.42 depending on k_trunc/k_top (best SAB configs near 1.37 BPC); Text8 BPC ~ 1.44 (SAB k_trunc=10,k_top=10 reported 1.44 in Table 6); pMNIST accuracy up to ≈94.2% for some hyperparameters; CIFAR10 (pixel-by-pixel) accuracy up to ≈64.5% in some SAB configs (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Comparative baselines reported in paper: vanilla LSTM (full BPTT) Copying: T=200 accuracy 56%, T=300 35.9% (Table 1); LSTM + full self-attention (full BPTT) often attains best performance on some tasks (e.g., copying reaches 100%), but SAB matches or outperforms vanilla LSTM (BPTT/TBPTT) and TBPTT LSTM: Text8 LSTM (full BPTT) BPC 1.42 vs SAB ≈1.44 (SAB slightly worse in some configs); TBPTT LSTM degrades substantially (e.g., Text8 TBPTT k_trunc=5 BPC 1.56). Transfer: models trained on T=100 copying -> at T=2000 vanilla LSTM ≈12% (random), LSTM+self-attn ≈12%, SAB ≈47% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (copying last-10), Cross-entropy (CE) / CE_last10 (copy/add), Bits-per-character (BPC) for language modeling, classification accuracy for pMNIST/CIFAR10</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Sparsity imposes a fixed budget k_top of skip-connections per timestep, forcing competition among past timesteps (useful for focusing credit but excludes other events); memory growth implies O(t n) space for inference (authors discuss bounded memory in practice), forward-time complexity can be O(t^2 n^2) with unbounded memory; backward pass runtime can be high in worst case (many microstates referenced); SAB approximates true gradient (not exact BPTT) which is a trade-off between scalability and exactness; requires tuning of k_trunc, k_top, k_att; dense-attention-with-truncation variant was harder to train; SAB needs gradient clipping on some datasets (Text8).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SAB sometimes underperforms full BPTT/self-attention trained with full BPTT (e.g., adding task at T=400 BPTT slightly outperforms SAB; some BPC numbers on PTB/Text8 show small gaps); ablation shows performance drops when 'mental updates' (local TBPTT around attended memories) are removed, especially with very small k_top (k_top=1); dense (non-sparse) attention with truncation is harder to train and achieves worse results in experiments; worst-case computational/gradient propagation complexity can be high if many microstates are relevant; requires hyperparameter choices (k_trunc, k_top, k_att) to balance performance and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C. Mozer, Chris Pal, Yoshua Bengio. Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding. (paper text provided)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6610.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6610.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM+Self-Attn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM augmented with full (dense) self-attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM augmented with a full (dense) self-attention mechanism over all past hidden states (no sparsification); attention weights normalized with softmax and gradients trained with full BPTT in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM + full self-attention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Adds a dense attention over the entire list of past hidden states (memory = list of past hidden states); attention computed with softmax over scores (Bahdanau-style scoring), combined into summary and used by LSTM; trained with full BPTT in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Same experimental sizes as other LSTM baselines (128 hidden units for synthetic/vision tasks; 1000 for language tasks where indicated).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Growing list of past hidden states (full self-attention over entire history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Past hidden-state vectors (microstates)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Dense softmax attention over all past hidden states (no sparsifier); read accesses are linear combinations of all past states; writes: implicit by storing every relevant hidden state (paper's baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Copying memory, Adding, PTB, Text8, pMNIST, CIFAR10 (same suites as SAB comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-range dependency benchmarks, language modeling, sequential classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as the strongest baseline on several tasks when trained with full BPTT: Copying (T up to 300) achieves 100% accuracy for many lengths (Table 1); Adding T=200 CE can be extremely low (e.g., 5.541e-8 for full self-attn in Table 2); generally obtains best CE/BPC on many synthetic tasks when trained with full BPTT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to vanilla LSTM: dense self-attention with full BPTT often outperforms vanilla LSTM (e.g., copying T=200 LSTM+self-attn 100% vs LSTM(full BPTT) 56%); when trained with truncation (TBPTT) the dense-attention variant performs worse and is harder to train (ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (copying), CE (adding/copy), BPC (language modeling), classification accuracy (pMNIST/CIFAR10)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Dense attention with truncation (i.e., attending to all past events while performing truncated updates) is empirically harder to train and attains worse performance than SAB; full self-attention with full BPTT requires full backward replay (biologically implausible) and can be computationally expensive over long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>When trained under TBPTT (truncated BPTT), LSTM+dense self-attention performed worse than SAB and was harder to train (ablation); also scale and memory cost grows with sequence length since attention is over all past states.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6610.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6610.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer (Vaswani et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A purely-attentional sequence model using multi-head self-attention layers (dense soft attention) and feed-forward blocks; in this paper used as an experimental baseline for sequential image tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is all you need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transformer (as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Self-attention-only architecture (multi-head softmax attention) applied to sequentialized images; attends densely to all positions (no explicit episodic memory beyond model's activations).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in the paper's experiments (Transformer reported as baseline, exact hyperparameters not given here).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Implicit dense self-attention over sequence positions (no explicit external memory store beyond internal layer activations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Layer activations / positional embeddings of sequence tokens</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Multi-head softmax attention over all positions (dense attention)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Permuted pixel-by-pixel MNIST (pMNIST), pixel-by-pixel CIFAR10</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Sequential image classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported baseline results: pMNIST accuracy 97.9% (Table 4), CIFAR10 accuracy 62.2% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable in this paper; Transformer is a dense-attention architecture (no ablation without attention reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Classification accuracy (percent)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Transformer attends to all past positions (no sparsity), which can be computationally expensive and biologically implausible according to authors; no sparsity budget constraint means no competition to focus credit on a few timesteps (contrasted conceptually with SAB).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In the paper's experiments, Transformer outperformed SAB on pMNIST but underperformed on CIFAR10 (SAB performed better on CIFAR10), suggesting dataset-dependent benefits; Transformer requires access to all past activations which may limit transfer to much longer sequences without architectural/hyperparameter changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia. Attention is all you need. (NIPS/NeurIPS 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6610.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6610.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DNC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in related work as an example of an architecture using an explicit external memory tensor with learned read/write controllers (Graves et al., 2016).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hybrid computing using a neural network with dynamic external memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>External fixed-size tensor memory with learned read and write operations and controllers (cited as an alternative memory structure in related work); not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External fixed-size differentiable memory tensor (addressable memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory matrix / slots holding vector contents (read/write content-based and location-based addressing)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned read/write controllers (content and location-based addressing) as in Graves et al. (2016); not implemented or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Mentioned only as an example of different memory structure; paper does not evaluate DNC or report task-specific trade-offs for it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this work; no failure cases reported here (citation only)</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago; Agapiou, John; et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471, 2016.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6610.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6610.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla LSTM (BPTT/TBPTT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla LSTM trained with full BPTT or truncated BPTT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard LSTM recurrent network baselines trained either with full backpropagation-through-time (BPTT) or truncated BPTT (TBPTT); serves as baseline to quantify the benefit of explicit sparse retrieval memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Vanilla LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Unaugmented LSTM recurrent model relying solely on sequential recurrence (no explicit episodic external memory), trained either with full BPTT (exact gradients) or TBPTT (truncated gradients).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Same experimental sizes as other LSTM baselines (128 hidden units for synthetic/vision tasks; 1000 for language tasks where indicated).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Implicit short-term memory via recurrent hidden state (no explicit external memory buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Hidden-state vector retained in recurrence</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Sequential recurrence (hidden-to-hidden transitions); no content-based retrieval from past separate memory store.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Copying memory, Adding, PTB, Text8, pMNIST, CIFAR10 (used as baseline tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-range dependency, language modeling, sequential classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Performance as reported for vanilla LSTM: Copying T=200 accuracy 56.0%, T=300 35.9% (Table 1) when trained with full BPTT; TBPTT versions (small k_trunc) perform substantially worse (e.g., TBPTT LSTM on Text8 BPC 1.56 with k_trunc=5 vs full BPTT 1.42).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (copy task last-10), CE, BPC, classification accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Full BPTT gives better training signals but is computationally and memory expensive for very long sequences; TBPTT is efficient but fails to capture long-term dependencies beyond truncation length.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>TBPTT cannot propagate error signals beyond truncation length so fails on very long-range dependencies (copying task with large T); full BPTT is computationally infeasible for extremely long sequences (biologically implausible per authors).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hybrid computing using a neural network with dynamic external memory <em>(Rating: 2)</em></li>
                <li>Neural machine translation by jointly learning to align and translate <em>(Rating: 2)</em></li>
                <li>Attention is all you need <em>(Rating: 2)</em></li>
                <li>Training recurrent networks online without backtracking <em>(Rating: 1)</em></li>
                <li>An efficient gradient-based algorithm for on-line training of recurrent network trajectories <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6610",
    "paper_id": "paper-fa0beb3f4d7f6e7e49b153af7e8a7c30f2937b60",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "SAB",
            "name_full": "Sparse Attentive Backtracking (SAB) - augmented LSTM",
            "brief_description": "An LSTM augmented with a growing episodic memory of past hidden states and a learned hard (sparse) self-attention that retrieves at most k_top past microstates; retrieved summaries are added to the current hidden state and gradients are backpropagated only through those sparse retrieval paths (plus optional local TBPTT 'mental updates').",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAB-augmented LSTM",
            "agent_description": "A unidirectional LSTM whose hidden states (every k_att'th timestep) are stored in a memory list; at each step a small MLP scores concatenations of the provisional hidden state with every memory entry to produce raw attention scores, the top-k_top scores are kept (sparsifier: subtract (k_top+1)th score, ReLU, normalize), a weighted sum of the selected memory vectors is computed and added to the provisional hidden state to form the final state; during backward pass gradients flow only through the sequential path plus the sparse skip connections and local k_trunc TBPTT around selected memories ('mental updates').",
            "model_size": "Task-dependent: experiments use 128 hidden units for synthetic and vision tasks; 1000 hidden units for language tasks (parameter count not explicitly reported).",
            "memory_used": true,
            "memory_type": "Growing episodic buffer of past hidden states (implicit external memory / list of microstates)",
            "memory_representation": "Hidden-state vectors (microstates) of the underlying LSTM, stored every k_att timesteps",
            "memory_access_mechanism": "Learned content-based scoring via an MLP over (provisional hidden state || memory vector) producing scalar raw scores; sparsification by selecting top k_top (subtract (k_top+1)th, ReLU, renormalize) to produce sparse attention weights; write: append current hidden state to memory when t mod k_att == 0; updates: local TBPTT of k_trunc timesteps around attended memories ('mental updates').",
            "task_name": "Copying memory (T up to 5000 transfer), Adding (T=200,400), Character-level PTB, Text8, permuted pixel MNIST (pMNIST), CIFAR10 (pixel-by-pixel), permuted MNIST, other synthetic tasks",
            "task_category": "Long-range temporal dependency benchmarks (copy/add), language modeling, sequential image classification",
            "performance_with_memory": "Representative reported results (selected best configs): Copying: T=200 accuracy ≈ 95% (last-10 accuracy, Table 5), T=300 ≈ 83%, T=400 ≈ 75%; Adding: T=200 CE as low as 4.26e-5 (different configs; Table 2/3); PTB char-level BPC ~ 1.37–1.42 depending on k_trunc/k_top (best SAB configs near 1.37 BPC); Text8 BPC ~ 1.44 (SAB k_trunc=10,k_top=10 reported 1.44 in Table 6); pMNIST accuracy up to ≈94.2% for some hyperparameters; CIFAR10 (pixel-by-pixel) accuracy up to ≈64.5% in some SAB configs (Table 4).",
            "performance_without_memory": "Comparative baselines reported in paper: vanilla LSTM (full BPTT) Copying: T=200 accuracy 56%, T=300 35.9% (Table 1); LSTM + full self-attention (full BPTT) often attains best performance on some tasks (e.g., copying reaches 100%), but SAB matches or outperforms vanilla LSTM (BPTT/TBPTT) and TBPTT LSTM: Text8 LSTM (full BPTT) BPC 1.42 vs SAB ≈1.44 (SAB slightly worse in some configs); TBPTT LSTM degrades substantially (e.g., Text8 TBPTT k_trunc=5 BPC 1.56). Transfer: models trained on T=100 copying -&gt; at T=2000 vanilla LSTM ≈12% (random), LSTM+self-attn ≈12%, SAB ≈47% (Table 5).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (copying last-10), Cross-entropy (CE) / CE_last10 (copy/add), Bits-per-character (BPC) for language modeling, classification accuracy for pMNIST/CIFAR10",
            "tradeoffs_reported": "Sparsity imposes a fixed budget k_top of skip-connections per timestep, forcing competition among past timesteps (useful for focusing credit but excludes other events); memory growth implies O(t n) space for inference (authors discuss bounded memory in practice), forward-time complexity can be O(t^2 n^2) with unbounded memory; backward pass runtime can be high in worst case (many microstates referenced); SAB approximates true gradient (not exact BPTT) which is a trade-off between scalability and exactness; requires tuning of k_trunc, k_top, k_att; dense-attention-with-truncation variant was harder to train; SAB needs gradient clipping on some datasets (Text8).",
            "limitations_or_failure_cases": "SAB sometimes underperforms full BPTT/self-attention trained with full BPTT (e.g., adding task at T=400 BPTT slightly outperforms SAB; some BPC numbers on PTB/Text8 show small gaps); ablation shows performance drops when 'mental updates' (local TBPTT around attended memories) are removed, especially with very small k_top (k_top=1); dense (non-sparse) attention with truncation is harder to train and achieves worse results in experiments; worst-case computational/gradient propagation complexity can be high if many microstates are relevant; requires hyperparameter choices (k_trunc, k_top, k_att) to balance performance and cost.",
            "citation": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C. Mozer, Chris Pal, Yoshua Bengio. Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding. (paper text provided)",
            "uuid": "e6610.0",
            "source_info": {
                "paper_title": "Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding",
                "publication_date_yy_mm": "2018-09"
            }
        },
        {
            "name_short": "LSTM+Self-Attn",
            "name_full": "LSTM augmented with full (dense) self-attention",
            "brief_description": "An LSTM augmented with a full (dense) self-attention mechanism over all past hidden states (no sparsification); attention weights normalized with softmax and gradients trained with full BPTT in the experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "LSTM + full self-attention",
            "agent_description": "Adds a dense attention over the entire list of past hidden states (memory = list of past hidden states); attention computed with softmax over scores (Bahdanau-style scoring), combined into summary and used by LSTM; trained with full BPTT in experiments.",
            "model_size": "Same experimental sizes as other LSTM baselines (128 hidden units for synthetic/vision tasks; 1000 for language tasks where indicated).",
            "memory_used": true,
            "memory_type": "Growing list of past hidden states (full self-attention over entire history)",
            "memory_representation": "Past hidden-state vectors (microstates)",
            "memory_access_mechanism": "Dense softmax attention over all past hidden states (no sparsifier); read accesses are linear combinations of all past states; writes: implicit by storing every relevant hidden state (paper's baseline).",
            "task_name": "Copying memory, Adding, PTB, Text8, pMNIST, CIFAR10 (same suites as SAB comparisons)",
            "task_category": "Long-range dependency benchmarks, language modeling, sequential classification",
            "performance_with_memory": "Reported as the strongest baseline on several tasks when trained with full BPTT: Copying (T up to 300) achieves 100% accuracy for many lengths (Table 1); Adding T=200 CE can be extremely low (e.g., 5.541e-8 for full self-attn in Table 2); generally obtains best CE/BPC on many synthetic tasks when trained with full BPTT.",
            "performance_without_memory": "Compared to vanilla LSTM: dense self-attention with full BPTT often outperforms vanilla LSTM (e.g., copying T=200 LSTM+self-attn 100% vs LSTM(full BPTT) 56%); when trained with truncation (TBPTT) the dense-attention variant performs worse and is harder to train (ablation).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (copying), CE (adding/copy), BPC (language modeling), classification accuracy (pMNIST/CIFAR10)",
            "tradeoffs_reported": "Dense attention with truncation (i.e., attending to all past events while performing truncated updates) is empirically harder to train and attains worse performance than SAB; full self-attention with full BPTT requires full backward replay (biologically implausible) and can be computationally expensive over long sequences.",
            "limitations_or_failure_cases": "When trained under TBPTT (truncated BPTT), LSTM+dense self-attention performed worse than SAB and was harder to train (ablation); also scale and memory cost grows with sequence length since attention is over all past states.",
            "citation": "",
            "uuid": "e6610.1",
            "source_info": {
                "paper_title": "Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding",
                "publication_date_yy_mm": "2018-09"
            }
        },
        {
            "name_short": "Transformer",
            "name_full": "Transformer (Vaswani et al., 2017)",
            "brief_description": "A purely-attentional sequence model using multi-head self-attention layers (dense soft attention) and feed-forward blocks; in this paper used as an experimental baseline for sequential image tasks.",
            "citation_title": "Attention is all you need",
            "mention_or_use": "use",
            "agent_name": "Transformer (as baseline)",
            "agent_description": "Self-attention-only architecture (multi-head softmax attention) applied to sequentialized images; attends densely to all positions (no explicit episodic memory beyond model's activations).",
            "model_size": "Not specified in the paper's experiments (Transformer reported as baseline, exact hyperparameters not given here).",
            "memory_used": true,
            "memory_type": "Implicit dense self-attention over sequence positions (no explicit external memory store beyond internal layer activations)",
            "memory_representation": "Layer activations / positional embeddings of sequence tokens",
            "memory_access_mechanism": "Multi-head softmax attention over all positions (dense attention)",
            "task_name": "Permuted pixel-by-pixel MNIST (pMNIST), pixel-by-pixel CIFAR10",
            "task_category": "Sequential image classification",
            "performance_with_memory": "Reported baseline results: pMNIST accuracy 97.9% (Table 4), CIFAR10 accuracy 62.2% (Table 4).",
            "performance_without_memory": "Not applicable in this paper; Transformer is a dense-attention architecture (no ablation without attention reported here).",
            "has_comparative_results": true,
            "performance_metric": "Classification accuracy (percent)",
            "tradeoffs_reported": "Transformer attends to all past positions (no sparsity), which can be computationally expensive and biologically implausible according to authors; no sparsity budget constraint means no competition to focus credit on a few timesteps (contrasted conceptually with SAB).",
            "limitations_or_failure_cases": "In the paper's experiments, Transformer outperformed SAB on pMNIST but underperformed on CIFAR10 (SAB performed better on CIFAR10), suggesting dataset-dependent benefits; Transformer requires access to all past activations which may limit transfer to much longer sequences without architectural/hyperparameter changes.",
            "citation": "Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia. Attention is all you need. (NIPS/NeurIPS 2017)",
            "uuid": "e6610.2",
            "source_info": {
                "paper_title": "Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding",
                "publication_date_yy_mm": "2018-09"
            }
        },
        {
            "name_short": "DNC",
            "name_full": "Differentiable Neural Computer (DNC)",
            "brief_description": "Mentioned in related work as an example of an architecture using an explicit external memory tensor with learned read/write controllers (Graves et al., 2016).",
            "citation_title": "Hybrid computing using a neural network with dynamic external memory",
            "mention_or_use": "mention",
            "agent_name": "Differentiable Neural Computer (DNC)",
            "agent_description": "External fixed-size tensor memory with learned read and write operations and controllers (cited as an alternative memory structure in related work); not used in experiments in this paper.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "External fixed-size differentiable memory tensor (addressable memory)",
            "memory_representation": "Memory matrix / slots holding vector contents (read/write content-based and location-based addressing)",
            "memory_access_mechanism": "Learned read/write controllers (content and location-based addressing) as in Graves et al. (2016); not implemented or evaluated in this paper.",
            "task_name": null,
            "task_category": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Mentioned only as an example of different memory structure; paper does not evaluate DNC or report task-specific trade-offs for it.",
            "limitations_or_failure_cases": "Not evaluated in this work; no failure cases reported here (citation only)",
            "citation": "Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago; Agapiou, John; et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471, 2016.",
            "uuid": "e6610.3",
            "source_info": {
                "paper_title": "Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding",
                "publication_date_yy_mm": "2018-09"
            }
        },
        {
            "name_short": "Vanilla LSTM (BPTT/TBPTT)",
            "name_full": "Vanilla LSTM trained with full BPTT or truncated BPTT",
            "brief_description": "Standard LSTM recurrent network baselines trained either with full backpropagation-through-time (BPTT) or truncated BPTT (TBPTT); serves as baseline to quantify the benefit of explicit sparse retrieval memory.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Vanilla LSTM",
            "agent_description": "Unaugmented LSTM recurrent model relying solely on sequential recurrence (no explicit episodic external memory), trained either with full BPTT (exact gradients) or TBPTT (truncated gradients).",
            "model_size": "Same experimental sizes as other LSTM baselines (128 hidden units for synthetic/vision tasks; 1000 for language tasks where indicated).",
            "memory_used": false,
            "memory_type": "Implicit short-term memory via recurrent hidden state (no explicit external memory buffer)",
            "memory_representation": "Hidden-state vector retained in recurrence",
            "memory_access_mechanism": "Sequential recurrence (hidden-to-hidden transitions); no content-based retrieval from past separate memory store.",
            "task_name": "Copying memory, Adding, PTB, Text8, pMNIST, CIFAR10 (used as baseline tasks)",
            "task_category": "Long-range dependency, language modeling, sequential classification",
            "performance_with_memory": null,
            "performance_without_memory": "Performance as reported for vanilla LSTM: Copying T=200 accuracy 56.0%, T=300 35.9% (Table 1) when trained with full BPTT; TBPTT versions (small k_trunc) perform substantially worse (e.g., TBPTT LSTM on Text8 BPC 1.56 with k_trunc=5 vs full BPTT 1.42).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (copy task last-10), CE, BPC, classification accuracy",
            "tradeoffs_reported": "Full BPTT gives better training signals but is computationally and memory expensive for very long sequences; TBPTT is efficient but fails to capture long-term dependencies beyond truncation length.",
            "limitations_or_failure_cases": "TBPTT cannot propagate error signals beyond truncation length so fails on very long-range dependencies (copying task with large T); full BPTT is computationally infeasible for extremely long sequences (biologically implausible per authors).",
            "citation": "",
            "uuid": "e6610.4",
            "source_info": {
                "paper_title": "Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding",
                "publication_date_yy_mm": "2018-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory",
            "rating": 2
        },
        {
            "paper_title": "Neural machine translation by jointly learning to align and translate",
            "rating": 2
        },
        {
            "paper_title": "Attention is all you need",
            "rating": 2
        },
        {
            "paper_title": "Training recurrent networks online without backtracking",
            "rating": 1
        },
        {
            "paper_title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories",
            "rating": 1
        }
    ],
    "cost": 0.01657,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding</h1>
<p>Nan Rosemary $\mathrm{Ke}^{1}$, Anirudh Goyal ${ }^{1}$, Olexa Bilaniuk ${ }^{1}$, Jonathan Binas ${ }^{1}$, Michael C. Mozer ${ }^{5}$, Chris Pal ${ }^{1,6}$, Yoshua Bengio ${ }^{1 \dagger}$<br>${ }^{1}$ Mila, Université de Montréal<br>${ }^{5}$ University of Colorado, Boulder<br>${ }^{6}$ MILA, Polytechnique Montréal<br>${ }^{\dagger}$ CIFAR Senior Fellow.</p>
<h4>Abstract</h4>
<p>Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps. This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state. We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly longterm dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.</p>
<h2>1 Introduction</h2>
<p>Humans have a remarkable ability to remember events from the distant past which are associated with the current mental state (Ciaramelli et al., 2008). Most experimental and theoretical analyses of memory have focused on understanding the deliberate route to memory formation and recall. But automatic reminding-when memories pop into one's head-can have a potent influence on cognition. Reminding is normally triggered by contextual features present at the moment of retrieval which match distinctive features of the memory being recalled (Berntsen et al., 2013; Wharton et al., 1996), and can occur more often following unexpected events (Read \&amp; Cesa, 1991). Thus, an individual's current state of understanding can trigger reminding of a past state. Reminding can provide distracting sources of irrelevant information (Forbus et al., 1995; Novick, 1988), but it can also serve a useful computational role in ongoing cognition by providing information essential to decision making (Benjamin \&amp; Ross, 2010).
In this paper, we identify another possible role of reminding: to perform credit assignment across long time spans. Consider the following scenario. As you drive down the highway, you hear an</p>
<p>unusual popping sound. You think nothing of it until you stop for gas and realize that one of your tires has deflated, at which point you are suddenly reminded of the pop. The reminding event helps determine the cause of your flat tire, and probably leads to synaptic changes by which a future pop sound while driving would be processed differently. Credit assignment is critical in machine learning. Back-propagation is fundamentally performing credit assignment. Although some progress has been made toward credit-assignment mechanisms that are functionally equivalent to back-propagation (Lee et al., 2014; Scellier \&amp; Bengio, 2016; Whittington \&amp; Bogacz, 2017), it remains very unclear how the equivalent of back-propagation through time, used to train recurrent neural networks (RNNs), could be implemented by brains. Here we explore the hypothesis that an associative reminding process could play an important role in propagating credit across long time spans, also known as the problem of learning long-term dependencies in RNNs, i.e., of learning to exploit statistical dependencies between events and variables which occur temporally far from each other.</p>
<h1>1.1 Credit Assignment in Recurrent Neural Networks</h1>
<p>RNNs are used to processes sequences of variable length. They have achieved state-of-the-art results for many machine learning sequence processing tasks. Examples where models based on RNNs shine include speech recognition (Miao et al., 2015; Chan et al., 2016), image captioning (Vinyals et al., 2015; Lu et al., 2017), machine translation (Luong et al., 2015).</p>
<p>It is common practice to train RNNs using gradients computed with backpropagation through time (BPTT), wherein the network states are unrolled in time over the whole trajectory of discrete time steps and gradients are back-propagated through the unrolled graph. The network unfolding procedure of BPTT does not seem biologically plausible because it requires storing and playing back these events much later (at the end of a trajectory of $T$ time steps) in reverse order to propagate gradients backwards. If a discrete time instant corresponds to a saccade (about 200-300ms,) then a trajectory of 100 days would require replaying back computations through over 42 million time steps. This is not only inconvenient, but more importantly a small error to any one of these events could either vanish or blow up and cause catastrophic outcomes. Also, if this unfolding and backpropagation is done only over shorter sequences, then learning typically will not capture longer-term dependencies linking events across larger temporal spans then the length of the back-propagated trajectory.
What are the alternatives to BPTT? One approach we explore here exploits associative reminding of past events which may be triggered by the current state and added to it, thus making it possible to propagate gradients with respect to the current state into approximate gradients in the state corresponding to the recalled event. The approximation comes from not backpropagating through the unfolded ordinary recurrence across long time spans, but only through this memory retrieval mechanism. Completely different approaches are possible but are not currently close to BPTT in terms of learning performance on large networks, such as methods based on the online estimation of gradients (Ollivier et al., 2015). Assuming that no exact gradient estimation method is possible (which seems likely) it could well be that brains combine multiple estimators.
In machine learning, the most common practical alternative to full BPTT is truncated BPTT (TBPTT) Williams \&amp; Peng (1990). In TBPTT, a long sequence is sliced into a number of (possibly overlapping) subsequences, gradients are backpropagated only for a fixed, limited number of time steps into the past, and the parameters are updated after each backpropagation through a subsequence. Unfortunately, this truncation makes capturing dependencies across distant timesteps nigh-impossible, because no error signal reaches further back into the past than TBPTT's truncation length.
Neurophysiological findings support the existence of remembering memories and their involvement in credit assignment and learning in biological systems. In particular, hippocampal recordings in rats indicate that brief sequences of prior experience are replayed both in the awake resting state and during sleep, both of which conditions are linked to memory consolidation and learning (Foster \&amp; Wilson, 2006; Davidson et al., 2009; Gupta et al., 2010; Ambrose et al., 2016). Thus, the mental look back into the past seems to occur exactly when credit assignment is to be performed. Thus, it is plausible that hippocampal replay could be a way of doing temporal credit assignment (and possibly</p>
<p>BPTT) on a short time scale, but here we argue for a solution which could handle credit assignment over much longer durations.</p>
<h1>1.2 Novel Credit Assignment Mechanism: Sparse Attentive Backtracking</h1>
<p>Inspired by the ability of brains to selectively reactivate memories of the past based on the current context, we propose here a novel solution called Sparse Attentive Backtracking (SAB) that incorporates a differentiable, sparse (hard) attention mechanism to select from past states. Inspired by the cognitive analogy of reminding, SAB is designed to retrieve one or very few past states. This may also be advantageous in focusing the credit assignment, although this hypothesis remains to be tested. SAB meshes well with TBPTT, yet allows gradient to propagate over distances far in excess of the TBPTT truncation length. We experimentally answer affirmatively the following questions:</p>
<p>Q1. Can Sparse Attentive Backtracking (SAB) capture long-term dependencies? SAB captures long-term dependencies. See results for 7 tasks supporting this in $\S 4$.
Q2. Generalization and transfer ability of SAB? See the strong transfer results in $\S 4$.
Q3. How does SAB perform compared to the Transformers (Vaswani et al., 2017)? SAB outperforms the Transformers (comparison in §4).
Q4. Is sparsity important for SAB and does it learn to retrieve meaningful memories? See the results on the Importance of Sparsity and Table 3 in $\S 4$.</p>
<h2>2 Related Machine Learning Work</h2>
<p>Skip-connections and gradient flow Neural architectures such as Residual Networks (He et al., 2016) and Dense Networks (Huang et al., 2016) allow information to skip over convolutional processing blocks of an underlying convolutional network architecture. This construction provably mitigates the vanishing gradient problem by allowing the gradient at any given layer to be bounded. Densely-connected convolutional networks alleviate the vanishing gradient problem by allowing a direct path from any layer in the network to the output layer. In contrast, in this work we propose and explore what one might regard as a form of dynamic skip connection, modulated by an attention mechanism corresponding to a reminding process, which matches the current state with an older state which is retrieved from memory.</p>
<p>The transformer network The Transformer network (Vaswani et al., 2017) takes sequence processing using attention to its logical extreme - using attention only, not relying on RNNs at all. The attention mechanism is a softmax not over the sequence itself but over the outputs of the previous self-attention layer. In order to attend to multiple parts of the layer outputs simultaneously, the Transformer uses 8 small attention "heads" per layer (instead of a single large head) and combines the attention heads' outputs by concatenation. No attempt is made to make the attention weights sparse, and the authors do not test their models on sequences of length greater than the intermediate representations of the Transformer model. With brains clearly involving a recurrent computation, this approach would seem to miss an important characteristic of biological credit assignment through time. Another implausible aspect of the Transformer architecture is the simultaneous access to (and linear combination of) all past memories (as opposed to a handful with SAB.)</p>
<h2>3 Sparse Attentive Backtracking</h2>
<p>Mindful that humans use a very sparse subset of past experiences in credit assignment, and are capable of direct random access to past experiences and their relevance to the present, we present here SAB: the principle of learned, dynamic, sparse access to, and replay of, relevant past states for credit assignment in neural network models, such as RNNs.</p>
<p>In the limit of maximum sparsity (no access to the past), SAB degenerates to the use of a regular static neural network. In the limit of minimum sparsity (full access to the past), SAB degenerates to the use of a full self-attention mechanism. For the purposes of this paper, we explore the gap between these with a specific variety of augmented LSTM models; but SAB does not refer to any particular architecture, and the augmented LSTM described herein is used purely as a vehicle to explore and validate our hypotheses in $\S 1$.
Broadly, an SAB neural network is required to do two things:</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: This figure illustrates the forward pass in SAB for the configuration $k_{\text {top }}=3, k_{\text {att }}=2, k_{\text {trunc }}=2$. This involves sparse retrieval (§ 3.1) and summarization of memories into the next RNN hidden state. Gray arrows depict how attention weights $\boldsymbol{a}^{(t)}$ are evaluated, first by broadcasting and concatenating the current provisional hidden state $\tilde{\boldsymbol{h}}^{(t)}$ against the set of all memories $\mathcal{M}$ and computing raw attention weights with an MLP. The sparsifier selects and normalizes only the $k_{\text {top }}$ greatest raw attention weights, while the others are zeroed out. Red arrows show memories corresponding to non-zero sparsified attention weights being weighted, summed, then added into $\tilde{\boldsymbol{h}}^{(t)}$ to compute the final hidden state $\boldsymbol{h}^{(t)}$.</p>
<ul>
<li>During the forward pass, manage a memory unit and select at most a sparse subset of past memories at every timestep. We will call this sparse retrieval.</li>
<li>During the backward pass, propagate gradient only to that sparse subset of memory and its local surroundings. We will call this sparse replay.</li>
</ul>
<h1>3.1 Sparse retrieval of memories</h1>
<p>Just as humans make a selective use of all past memories to inform their decisions in the present, so must an SAB model learn to remember and dynamically select only a few memories that could be potentially useful in the present. There are several alternative implementations of this concept. An important class of them are attention mechanisms, especially self-attention over a model's own past states. Closely linked to the question of dynamic access to memory is the structure of the memory itself; for instance, in the Differentiable Neural Computer (DNC) (Graves et al., 2016), the memory is a fixed-size tensor accessed with explicit read and write operations, while in Bahdanau et al. (2014), the memory is implicitly a list of past hidden states that continuously grows.
For the purposes of this paper, we choose a simple approach similar to Bahdanau et al. (2014). Many other options are possible, and the question of memory representation in humans (faithful to actual brains) and machines (with good computationsl properties) remains open. Here, to test the principle of SAB without having to answer that question, we use an approach already shown to work well in machine learning. We augment a unidirectional LSTM with the memory of every $k_{\text {att }}$ 'th hidden state from the past, with a modified hard self-attention mechanism limited to selecting at most $k_{\text {top }}$ memories at every timestep. Future work should investigate more realistic mechanisms for storing memories, e.g., based on saliency, novelty, etc. But this simple scheme allows us to test the hypothesis that neural network models can still perform well even when compelled at every timestep to access their past sparsely. If they cannot, then it would be meaningless to further encumber them with a bounded-size memory.</p>
<p>SAB-augmented LSTM We now describe the sparse retrieval mechanism that we have settled on. It determines which memories will be selected on the forward pass of the RNN, and therefore also which memories will receive gradient on the backward pass during training.</p>
<p>At time $t$, the underlying LSTM receives a vector of hidden states $\boldsymbol{h}^{(t-1)}$, a vector of cell states $\boldsymbol{c}^{(t-1)}$, and an input $\boldsymbol{x}^{(t)}$, and computes new cell states $\boldsymbol{c}^{(t)}$ and a provisional hidden state vector $\tilde{\boldsymbol{h}}^{(t)}$ that also serves as a provisional output. We next use an attention mechanism that is similar to Bahdanau et al. (2014), but modified to produce sparse attention decisions. First, the provisional hidden state vector $\tilde{\boldsymbol{h}}^{(t)}$ is concatenated to each memory vector $\boldsymbol{m}^{(i)}$ in the memory $\mathcal{M}$. Then, an MLP with one hidden layer maps each such concatenated vector to a scalar, non-sparse, raw attention weight $a_{i}^{(i)}$ representing the salience of the memory $i$ at the current time $t$. The MLP is parametrized with weight matrices $\boldsymbol{W}<em 2="2">{1}, \boldsymbol{W}</em>$.}$ and $\boldsymbol{W}_{3</p>
<p>With the raw attention weights, we compute the sparsified attention weights $\tilde{a}<em o="o" p="p" t="t">{i}^{(t)}$ by subtracting out the $\left(k</em>$ memories, weigh the selected memories by their prominence over the others, as opposed to their raw value. This is different from typical attention mechanisms that normalize attention weights using a softmax function (Bahdanau et al., 2014), whose output is never sparse.}+1\right)$ 'th raw weight from all the others, passing the intermediate result through ReLU, then normalizing to sum to 1 . This effectively implements a discrete, hard decision to drop all but $k_{\text {top }</p>
<p>A summary vector $\boldsymbol{s}^{(t)}$ is then computed using a simple sum of the selected memories, weighted by their respective sparsified attention weight. Given that this sum is very sparse, the summary operation is very fast. This summary is then added into the provisional hidden state $\tilde{\boldsymbol{h}}^{(t)}$ computed previously to obtain final state $\boldsymbol{h}^{(t)}$.
Lastly, to compute the SAB-augmented LSTM cell's output $\boldsymbol{y}^{(t)}$ at $t$, we concatenate $\boldsymbol{h}^{(t)}$ and summary vector $\boldsymbol{s}^{(t)}$, then apply an affine output transform parametrized with learned weights matrices $\boldsymbol{V}<em 2="2">{1}$ and $\boldsymbol{V}</em>$.
The forward pass into a hidden state $\boldsymbol{h}^{(t)}$ has two paths contributing to it. One path is the regular sequential forward path in an RNN; the other path is through the dynamic but sparse skip connections in the attention mechanism that connect the present states to potentially very distant past experiences.}$ and bias vector $\boldsymbol{b</p>
<h3>3.2 Sparse replay</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">Algorithm 1 SAB-augmented LSTM</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1: procedure $\operatorname{SABCELL}\left(\boldsymbol{h}^{(t-1)}, \boldsymbol{c}^{(t-1)}, \boldsymbol{x}^{(t)}\right)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Require: $k_{\text {top }}&gt;0, k_{\text {att }}&gt;0, k_{\text {trunc }}&gt;0$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Require: Memories $\boldsymbol{m}^{(i)} \in \mathcal{M}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Require: Previous hidden state $\boldsymbol{h}^{(t-1)}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Require: Previous cell state $\boldsymbol{c}^{(t-1)}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Require: Input $\boldsymbol{x}^{(t)}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2: $\quad \tilde{\boldsymbol{h}}^{(t)}, \boldsymbol{c}^{(t)} \leftarrow \operatorname{LSTMCell}\left(\boldsymbol{h}^{(t-1)}, \boldsymbol{c}^{(t-1)}, \boldsymbol{x}^{(t)}\right)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3: for all $i \in 1 \ldots</td>
<td style="text-align: center;">\mathcal{M}</td>
</tr>
<tr>
<td style="text-align: center;">4: $\quad \boldsymbol{d}<em i="i">{i}^{(t)} \leftarrow \boldsymbol{W}</em>$} \boldsymbol{m}^{(i)}+\boldsymbol{W}_{2} \tilde{\boldsymbol{h}}^{(t)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5: $\quad a_{i}^{(t)} \leftarrow \boldsymbol{W}<em i="i">{3} \tanh \left(\boldsymbol{d}</em>\right)$}^{(t)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">6: $\quad a_{\text {ktop }}^{(t)} \leftarrow \operatorname{sorted}\left(\boldsymbol{a}^{(t)}\right)\left[k_{\text {top }} \bullet 1\right]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">7: $\quad \tilde{\boldsymbol{a}}^{(t)} \leftarrow \operatorname{ReLU}\left(\boldsymbol{a}^{(t)}-a_{\text {ktop }}^{(t)}\right)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">8: $\quad \boldsymbol{s}^{(t)} \leftarrow \sum_{\boldsymbol{m}^{(i)} \in \mathcal{M}} \tilde{a}<em i="i">{i}^{(t)} \boldsymbol{m}^{(i)} / \sum</em>$} \tilde{a}_{i}^{(t)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">9: $\quad \boldsymbol{h}^{(t)} \leftarrow \tilde{\boldsymbol{h}}^{(t)}+\boldsymbol{s}^{(t)}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10: $\quad \boldsymbol{y}^{(t)} \leftarrow \boldsymbol{V}<em 2="2">{1} \boldsymbol{h}^{(t)}+\boldsymbol{V}</em>$} \boldsymbol{s}^{(t)}+\boldsymbol{b</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11: if $t \equiv 0\left(\bmod k_{\text {att }}\right)$ then</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">12: $\quad \mathcal{M} . \operatorname{append}\left(\boldsymbol{h}^{(t)}\right)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">13: $\quad$ return $\boldsymbol{h}^{(t)}, \boldsymbol{c}^{(t)}, \boldsymbol{y}^{(t)}$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Humans are trivially capable of assigning credit or blame to events even a long time after the fact, and do not need to replay all events from the present to the credited event sequentially and in reverse to do so. But that is effectively what RNNs trained with full BPTT require, and this does not seem biologically plausible when considering events which are far from each other in time. Even less plausible is TBPTT because it ignores time dependencies beyond the truncation length $k_{\text {trunc }}$.</p>
<p>SAB networks' twin paths during the forward pass (sequential connection and sparse skip connections) allow gradient to flow not just from $\boldsymbol{h}^{(t)}$ to $\boldsymbol{h}^{(t-1)}$, but also to the at-most $k_{\text {top }}$ memories $\boldsymbol{m}^{(i)}$ retrieved by the attention mechanism (and no others.) Learning to deliver gradient directly (and sparsely) where it is needed (and nowhere else) (1) avoids competition for the limited informationcarrying capacity of the sequential path, (2) is a simple form of credit assignment, (3) and imposes a trade-off that is absent in previous, dense self-attentive mechanisms: opening a connection to an interesting or useful timestep must be made at the price of excluding others. This competition for a limited budget of $k_{\text {top }}$ connections results in interesting timesteps being given frequent attention and strong gradient flow, while uninteresting timesteps are ignored and starve.</p>
<p>Mental updates If we not only allow gradient to flow directly to a past timestep, but on to a few local timesteps around it as well, we have mental updates: a type of local credit assignment around a memory. There are various ways of enabling this. In our SAB-augmented LSTM, we choose to perform TBPTT locally before the selected timesteps ( $k_{\text {trunc }}$ timesteps before a selected one.)</p>
<h2>4 Experimental Setup and Results</h2>
<p>Baselines For all tasks, We compare SAB to two baseline models for all tasks. The first is an LSTM trained both using full BPTT and TBPTT with various truncation length. The second is an LSTM augmented with full self-attention trained using full BPTT. For pixel-by-pixle Cifar10 classification task, we also compare to the Transformer (Vaswani et al., 2017) architecture.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: This figure illustrates the backward pass in SAB for the configuration $k_{\text {top }}=3, k_{\text {att }}=2, k_{\text {trunc }}=2$. The gradients are passed to the microstates selected in the forward pass and a local truncated backprop is performed around those microstates. Blue arrows show the gradient flow in the backward pass. Red crosses indicate TBPTT truncation points, where the gradient stops being backpropagated.</p>
<p>Copying and Adding problems (Q1) The copy and adding problems defined in Hochreiter \&amp; Schmidhuber (1997) are synthetic tasks specifically designed to evaluate a model's performance on long-term dependencies by testing its ability to remember a sub-sequence for a large number of timesteps. The performance of SAB almost matches the performance of LSTMs augmented with self-attention trained using full BPTT. Note that our copy and adding LSTM baselines are more competitive compared to ones reported in the existing literature (Arjovsky et al., 2016). These findings support our hypothesis that at any given time step, only a few past events need to be recalled for the correct prediction of output of the current timestep.</p>
<p>Table 2 reports the cross-entropy (CE) of the model predictions on unseen sequences in the adding task. LSTM with full self-attention trained using BPTT obtains the lowest CE loss, followed by LSTM trained using BPTT. LSTM trained with truncated BPTT performs significantly worse. When $T=200$, SAB's performance is comparable to the best baseline models. With longer sequences ( $T=400$ ), SAB outperforms TBPTT, but is outperformed by pure BPTT. For more details regarding the setup, refer to supplementary material.</p>
<p>Character level Penn TreeBank (PTB) (Q1) Details about our experimental setup can be found in the supplementary material. We evaluate the performance of our model using the bits-per-character (BPC) metric. As shown in Table 2, SAB's performance is significantly better than TBPTT and almost matches BPTT, which is roughly what one expects from an approximate-gradient method like SAB.</p>
<p>Text8 (Q1) Details about our experimental setup can be found in supplementary material. Note that we did not carry out any additional hyperparameter search for our model. Table 2 reports the BPC of the model's predictions on the test sets. SAB outperforms LSTM trained using TBPTT. SAB also outperforms LSTM and self-attention trained with TBPTT. For more details, refer to supplementary material.</p>
<p>Permuted pixel-by-pixel MNIST (Q1) This task is a sequential version of the MNIST classification dataset. The task involves predicting the label of the image after being given its pixels as a sequence permuted in a fixed, random order. Our experiment setup can be found in the supplementary material. Table 4 shows that SAB performs well compared to BPTT.</p>
<p>CIFAR10 classification (Q1,Q3) We test our model's performance on pixel-by-pixel CIFAR10 (no permutation). This task involves predicting the label of the image after being given it as a sequence of pixels. This task is relatively difficult compared to other tasks, as sequences are substantially longer (length 1024.) Our method outperforms Transformers and LSTMs trained with BPTT (Table 4).</p>
<p>Learning long-term dependencies (Q1) Table 1 reports both accuracy and cross-entropy (CE) of the models' predictions on unseen sequences for the copy memory task. The best-performing baseline model is the LSTM with full self-attention trained using BPTT, followed by vanilla LSTMs trained using BPTT. Far behind are LSTMs trained using truncated BPTT. Table 1 demonstrates that SAB is able to learn the task almost perfectly for all copy lengths $T$. Further, SAB outperforms all LSTM baselines and matches the performance of LSTMs with full self-attention trained using BPTT on the copy memory task. This becomes particularly noticeable as the sequence length increases.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Copying (T=100)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Copying (T=200)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Copying (T=300)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$k_{\text {trunc }}$</td>
<td style="text-align: center;">$k_{\text {top }}$</td>
<td style="text-align: center;">acc.</td>
<td style="text-align: center;">CE $_{10}$</td>
<td style="text-align: center;">CE</td>
<td style="text-align: center;">acc.</td>
<td style="text-align: center;">CE $_{10}$</td>
<td style="text-align: center;">CE</td>
<td style="text-align: center;">acc.</td>
<td style="text-align: center;">CE $_{10}$</td>
<td style="text-align: center;">CE</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 圆 } \ &amp; \text { 圆 } \end{aligned}$</td>
<td style="text-align: center;">full BPTT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">0.030</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">0.046</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.047</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">full self-attn.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0008</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">7.5e-5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">1.984</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">2.077</td>
<td style="text-align: center;">0.065</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">1.737</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">1.772</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">1.98</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">1.714</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">1.61</td>
<td style="text-align: center;">0.073</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">1.848</td>
<td style="text-align: center;">0.197</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">1.596</td>
<td style="text-align: center;">0.073</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">1.857</td>
<td style="text-align: center;">0.058</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 圆 } \ &amp; \text { 圆 } \end{aligned}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">1.041</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">1.516</td>
<td style="text-align: center;">0.069</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.045</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.012</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Test accuracy and cross-entropy (CE) loss performance on the copying task with sequence lengths of $\mathrm{T}=100,200$, and 300. Accuracies are given in percent for the last 10 characters. $\mathrm{CE}_{10}$ corresponds to the CE loss on the last 10 characters. These results are with mental updates; Compare with Table 3 for without.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Adding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathrm{T}=200$</th>
<th style="text-align: center;">$\mathrm{~T}=400$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$k_{\text {trunc }}$</td>
<td style="text-align: center;">$k_{\text {top }}$</td>
<td style="text-align: center;">CE</td>
<td style="text-align: center;">CE</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">full BPTT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4.59e-6</td>
<td style="text-align: center;">1.554e-7</td>
</tr>
<tr>
<td style="text-align: center;">圆</td>
<td style="text-align: center;">full self-attn.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.541e-8</td>
<td style="text-align: center;">4.972e-7</td>
</tr>
<tr>
<td style="text-align: center;">圆</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.1e-3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">圆</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.0e-4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.8e-4</td>
</tr>
<tr>
<td style="text-align: center;">圆</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4.26e-5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">圆</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.30e-4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2.0e-6</td>
<td style="text-align: center;">1.001e-5</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PTB</th>
<th style="text-align: center;">Text8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$k_{\text {trunc }}$</td>
<td style="text-align: center;">$k_{\text {top }}$</td>
<td style="text-align: center;">$k_{\text {att }}$</td>
<td style="text-align: center;">BPC</td>
<td style="text-align: center;">BPC</td>
</tr>
<tr>
<td style="text-align: center;">full BPTT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">1.42</td>
</tr>
<tr>
<td style="text-align: center;">圆</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.47</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">圆</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">1.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1.42</td>
<td style="text-align: center;">1.47</td>
</tr>
<tr>
<td style="text-align: center;">圆</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">1.45</td>
</tr>
<tr>
<td style="text-align: center;">圆</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">1.39</td>
<td style="text-align: center;">1.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">1.37</td>
<td style="text-align: center;">1.44</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance on the adding task (left) and language modeling tasks (PTB and Text8; right). The adding task performance is evaluated on unseen sequences of the $\mathrm{T}=200$ and $\mathrm{T}=400$ (note that all methods have configurations that allow them to perform near optimally.) For $\mathrm{T}=400$, BPTT slightly outperforms SAB, which outperforms TBPTT. For the language modeling tasks, the BPC score is evaluated on the test sets of the character-level PTB and Text8.</p>
<p>Transfer Learning (Q2) We examine the generalization ability of SAB compared to full BPTT trained LSTM and LSTM with full self-attention. The experiment is set up as follows: For the copy task of length $T=100$, we train SAB, LSTM trained with BPTT, LSTM and full self-attention to convergence. We then take the trained model and evaluate them on the copy task for an array of larger $T$ values. The results are shown in Table 5. Although all 3 models have similar performance on $T=100$, it is clear that performance for all 3 models drops as $T$ grows. However, SAB still manages to complete the task at $T=5000$, whereas by $T=2000$ both vanilla LSTM and LSTM with full self-attention do no better than random guessing $(1 / 8=12.5 \%)$.</p>
<p>Importance of Sparisity and Mental Updates (Q4) We study the necessity of sparsity and mental updates by running an ablation study on the copying problem. The ablation study focuses on two variants. The first model attends to all events in the past while performing a truncated update. This can be seen either as a dense version of SAB or an LSTM with full self-attention trained using TBPTT. Empirically, we find that such models are both more difficult to train and do not reach the same performance as SAB. The second ablation experiment tests the necessity of mental updates, without which the model would only attend to the past time steps without passing gradients through them to preceding time steps. We observe a degradation of model performance when blocking gradients to past events. This effect is most evident when attending to only one timestep in the past ( $k_{\text {top }}=1$ ).
We evaluate SAB on language modeling, with the Penn TreeBank (PTB) (Marcus et al., 1993) and Text8 Mahoney (2011) datasets. For models trained using truncated BPTT, the performance drops as $k_{\text {trunc }}$ shrinks. We found that on PTB, SAB with $k_{\text {trunc }}=20, k_{\text {top }}=10$ performs almost as well as full BPTT. For the larger Text8 dataset, SAB with $k_{\text {trunc }}=10$ and $k_{\text {top }}=5$ outperforms LSTM trained using BPTT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Ablation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Copying. T=100</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Adding. $\mathrm{T}=200 \mathrm{CE}$.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$k_{\text {trunc }}$</td>
<td style="text-align: center;">$k_{\text {top }}$</td>
<td style="text-align: center;">acc.</td>
<td style="text-align: center;">$\mathrm{CE}_{\text {last } 10}$</td>
<td style="text-align: center;">CE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">1.252</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.0036</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.0018</td>
<td style="text-align: center;">2.171e-6</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">1.529</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Left: ablation studies on the adding and copying tasks. The limiting cases of dense attention ( $k_{\text {top }}=$ all) and of no mental updates (MU) were tested. Right: focus of the attention for the T=200 copying task, where reproduction of the inital 10 input symbols is required (black corresponds to stronger attention weights). The was generated at different points in training (a-c) within the first epoch. Attention quickly shifts to the relevant parts of the sequence (the initial 10 states.)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Image class.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">pMNIST</th>
<th style="text-align: center;">CIFAR10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$k_{\text {trunc }}$</td>
<td style="text-align: center;">$k_{\text {top }}$</td>
<td style="text-align: center;">$k_{\text {att }}$</td>
<td style="text-align: center;">acc.</td>
<td style="text-align: center;">acc.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">full BPTT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: center;">300</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: center;">Transformer (Vasvani'17)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">62.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Test accuracy for the permutated MNIST and CIFAR10 classification tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Copy len. <br> (T)</th>
<th style="text-align: center;">LSTM</th>
<th style="text-align: center;">LSTM <br> +self-a.</th>
<th style="text-align: center;">SAB</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$99 \%$</td>
</tr>
<tr>
<td style="text-align: center;">200</td>
<td style="text-align: center;">$34 \%$</td>
<td style="text-align: center;">$52 \%$</td>
<td style="text-align: center;">$\mathbf{9 5 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">300</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">$28 \%$</td>
<td style="text-align: center;">$\mathbf{8 3 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">400</td>
<td style="text-align: center;">$21 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$\mathbf{7 5 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">$\mathbf{4 7 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">$\mathbf{4 1 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Transfer performance (Accuracy for last 10 digits) for models trained on $T=100$ copy memory task. Comparisons to LSTM and LSTM with full selfattention trained with BPTT.</p>
<p>Comparison to Transformer (Q3) We test how SAB compares to the Transformer model (Vaswani et al., 2017), based a self-attention mechanism. On pMNIST, the Transformer model outperforms our best model, as shown in Table 4. On CIFAR10, however, our proposed model performs much better.</p>
<h1>5 Conclusions</h1>
<p>By considering how brains could perform long-term temporal credit assignment, we developed an alternative to the traditional method of training recurrent neural networks by unfolding of the computational graph and BPTT. We explored the hypothesis that a reminding process which uses the current state to evoke a relevant state arbitrarily far back in the past could be used to effectively teleport credit backwards in time to the computations performed to obtain the past state. To test this idea, we developed a novel temporal architecture and credit assignment mechanism called SAB for Sparse Attentive Backtracking, which aims to combine the strengths of full backpropagation through time and truncated backpropagation through time. It does so by backpropagating gradients only through paths for which the current state and a past state are associated. This allows the RNN to learn long-term dependencies, as with full backpropagation through time, while still allowing it to only backtrack for a few steps, as with truncated backpropagation through time, thus making it possible to update weights as frequently as needed rather than having to wait for the end of very long sequences.
Cognitive processes in reminding serve not only as the inspiration for SAB, but suggest two interesting directions of future research. First, we assumed a simple content-independent rule for selecting microstates for inclusion in the macrostate, whereas humans show a systematic dependence on content: salient, extreme, unusual, and unexpected experiences are more likely to be stored and subsequently remembered. These landmarks of memory should be useful for connecting past to current context, just as an individual learns to map out a city via distinctive geographic landmarks. Second, SAB determines the relevance of past microstates to the current state through a generic, flexible mapping, whereas humans perform similarity-based retrieval. We conjecture that a version of SAB with a strong inductive bias in the mechanism to select past states may further improve its performance.</p>
<h1>6 Acknowledgement</h1>
<p>The authors would like to thank Hugo Larochelle, Walter Senn, Alex Lamb, Remi Le Priol, Matthieu Courbariaux, Gaetan Marceau Caron, Sandeep Subramanian for the useful discussions, as well as NSERC, CIFAR, Google, Samsung, SNSF, Nuance, IBM, Canada Research Chairs, National Science Foundation awards EHR-1631428 and SES-1461535 for funding. We would also like to thank Compute Canada and NVIDIA for computing resources. The authors would also like to thank Alex Lamb for code review. The authors would also like to express debt of gratitude towards those who contributed to Theano over the years (now that it is being sunset), for making it such a great tool.</p>
<h1>References</h1>
<p>Ambrose, R. Ellen, Pfeiffer, Brad E., and Foster, David J. Reverse replay of hippocampal place cells is uniquely modulated by changing reward. Neuron, 91(5):1124 - 1136, 2016.</p>
<p>Arjovsky, Martin, Shah, Amar, and Bengio, Yoshua. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pp. 1120-1128, 2016.</p>
<p>Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Benjamin, A. S. and Ross, B. H. The causes and consequences of reminding. In Benjamin, A. S. (ed.), Successful remembering and successful forgetting: A Festschrift in honor of Robert A. Bjork. Psychology Press, 2010.</p>
<p>Berntsen, Dorthe, Staugaard, Søren Risløv, and Sørensen, Louise Maria Torp. Why am i remembering this now? predicting the occurrence of involuntary (spontaneous) episodic memories. Journal of Experimental Psychology: General, 142(2):426, 2013.</p>
<p>Chan, William, Jaitly, Navdeep, Le, Quoc, and Vinyals, Oriol. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pp. 4960-4964. IEEE, 2016.</p>
<p>Ciaramelli, E, Grady, C L, and Moscovitch, M. Top-down and bottom-up attention to memory: A hypothesis on the role of the posterior parietal cortex in memory retrieval. Neuropsychologia, 46 (7):1828-1851, 2008.</p>
<p>Cooijmans, Tim, Ballas, Nicolas, Laurent, César, Gülçehre, Çağlar, and Courville, Aaron. Recurrent batch normalization. arXiv preprint arXiv:1603.09025, 2016.</p>
<p>Davidson, Thomas J, Kloosterman, Fabian, and Wilson, Matthew A. Hippocampal replay of extended experience. Neuron, 63(4):497-507, 2009.</p>
<p>Forbus, K D, Gentner, D, and Law, K. Mac/fac: A model of similarity-based retrieval. Cognitive Science, 19:141-205, 1995.</p>
<p>Foster, David J and Wilson, Matthew A. Reverse replay of behavioural sequences in hippocampal place cells during the awake state. Nature, 440(7084):680-683, 2006.</p>
<p>Graves, Alex, Wayne, Greg, Reynolds, Malcolm, Harley, Tim, Danihelka, Ivo, Grabska-Barwińska, Agnieszka, Colmenarejo, Sergio Gómez, Grefenstette, Edward, Ramalho, Tiago, Agapiou, John, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538 (7626):471, 2016.</p>
<p>Gupta, Anoopum S, van der Meer, Matthijs AA, Touretzky, David S, and Redish, A David. Hippocampal replay is not a simple function of experience. Neuron, 65(5):695-705, 2010.</p>
<p>He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.</p>
<p>Hochreiter, Sepp and Schmidhuber, Jürgen. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.</p>
<p>Huang, Gao, Liu, Zhuang, Weinberger, Kilian Q, and van der Maaten, Laurens. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.</p>
<p>Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Lee, Dong-Hyun, Zhang, Saizheng, Biard, Antoine, and Bengio, Yoshua. Target propagation. CoRR, abs/1412.7525, 2014. URL http://arxiv.org/abs/1412.7525.</p>
<p>Lu, Jiasen, Xiong, Caiming, Parikh, Devi, and Socher, Richard. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 6, 2017.</p>
<p>Luong, Minh-Thang, Pham, Hieu, and Manning, Christopher D. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.</p>
<p>Mahoney, Matt. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text. html, 2011.</p>
<p>Marcus, Mitchell P, Marcinkiewicz, Mary Ann, and Santorini, Beatrice. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.</p>
<p>Miao, Yajie, Gowayyed, Mohammad, and Metze, Florian. Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding. In Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on, pp. 167-174. IEEE, 2015.</p>
<p>Novick, R L. Analogical transfer, problem similarity, and expertise. Journal of Experimental Psychology: Learning, Memory, \&amp; Cognition, 14:510-520, 1988.</p>
<p>Ollivier, Yann, Tallec, Corentin, and Charpiat, Guillaume. Training recurrent networks online without backtracking. arXiv preprint arXiv:1507.07680, 2015.</p>
<p>Read, S J and Cesa, I L. Expectation failures in reminding and explanation. Journal of Experimental Social Psychology, 27:1-25, 1991.</p>
<p>Scellier, Benjamin and Bengio, Yoshua. Towards a biologically plausible backprop. CoRR, abs/1602.05179, 2016. URL http://arxiv.org/abs/1602.05179.</p>
<p>Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Łukasz, and Polosukhin, Illia. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000-6010, 2017.</p>
<p>Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3156-3164, 2015.</p>
<p>Wharton, C M, Holyoak, K J, and Lange, T E. Remote analogical reminding. Memory \&amp; Cognition, 24:629-643, 1996.</p>
<p>Whittington, James CR and Bogacz, Rafal. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5): $1229-1262,2017$.</p>
<p>Williams, Ronald J and Peng, Jing. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural computation, 2(4):490-501, 1990.</p>
<h1>7 Supplementary material</h1>
<h3>7.1 Synthetic Experiments</h3>
<p>The copying memory problem We follow the setup of the copying memory problem from Hochreiter \&amp; Schmidhuber (1997). Specifically, the network is given a sequence of $T+20$ inputs consisting of: a) 10 (randomly generated) digits (digits 1 to 8 ) followed by; b) $T$ blank inputs followed by; c) a special end-of-sequence character followed by; d) 10 additional blank inputs. After the end-of-sequence character the network must output a copy of the initial 10 digits.</p>
<p>The adding task The adding task requires the model to sum two specific entries in a sequence of $T$ (input) entries (Hochreiter \&amp; Schmidhuber, 1997). In the spirit of the copying task, larger values of $T$ will require the model to keep track of longer-term dependencies. The exact setup is as follows. Each example in the task consists of two input vectors of length $T$. The first is a vector of uniformly generated values between 0 and 1 . The second vector encodes a binary mask which indicates the two entries in the first input to be added (the mask vector consists of $T-2$ zeros and 2 ones). The mask is randomly generated with the constraint that masked-in entries must be from different halves of the first input vector.</p>
<p>Hyperparameters The hyperparameters for both baselines and SAB are kept the same. All models has 128 hidden units and uses the Adam (Kingma \&amp; Ba, 2014) optimizer with a learning rate of $1 e-3$. The first model in the ablation study (dense version of SAB) was more difficult to train, therefore we explored different learning rate ranging from $1 e-3$ to $1 e-5$, we report the best performing model.</p>
<h3>7.2 Char Level PennTree Bank</h3>
<p>We follow the setup in Cooijmans et al. (2016) and all of our models use 1000 hidden units for and a learning rate of 0.002 . We used non-overlapping sequences of 100 in the batches of 32 as in Cooijmans et al. (2016). All models trained for upto 100 epochs with early stopping on the validation set. We evaluate the performance of our model using the bits-per-character (BPC) metric.</p>
<h3>7.3 Char Level Text8</h3>
<p>We follow the setup of Mikolov et al. (2012); use the first 90M characters for training, the next 5M for validation and the final 5 M characters for testing. We train on non-overlapping sequences of length 180. Due to computational constraints, all baselines use 1000 hidden units. We trained all models using a batch size of 64 . We trained SAB for a maximum of 30 epochs.</p>
<h3>7.4 Permuted Pixel-by-pixel MNIST</h3>
<p>All models use an LSTM with 128 hidden units. The prediction is produced by passing the final hidden state of the network into a softmax. We used a learning rate of 0.001 . We trained our model for about 100 epochs, and did early stopping based on the validation set.</p>
<h3>7.5 Comparison to LSTM + Self Attention(with truncation)</h3>
<p>While SAB is trained with truncated BPTT (and the vanilla LSTM+self-attention is not), Here we argue, that training the vanilla LSTM and self attention with truncation works less well on a more challenging Text8 language modelling dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Test BPC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LSTM (full BPTT)</td>
<td style="text-align: center;">1.42</td>
</tr>
<tr>
<td style="text-align: left;">LSTM (TBPTT, $k_{\text {trunc }}=5$ )</td>
<td style="text-align: center;">1.56</td>
</tr>
<tr>
<td style="text-align: left;">LSTM (Self Attention with Truncation, $k_{\text {trunc }}=10$ ))</td>
<td style="text-align: center;">1.48</td>
</tr>
<tr>
<td style="text-align: left;">$\operatorname{SAB}\left(k_{\text {trunc }}=10, k_{\text {top }}=10, k_{\text {att }}=10\right)$</td>
<td style="text-align: center;">1.44</td>
</tr>
</tbody>
</table>
<p>Table 6: Bit-per-character (BPC) Results on the test set for Text8 (lower is better).</p>
<h1>8 Computational Complexity of $S A B$</h1>
<p>If the memory was allowed to grow unbounded in size, then the computational complexity would scale linearly with the length of history. However, humans have a bounded memory. In a computer science context with unbounded memory, the time complexity of the forward pass of both training and inference in SAB is $O\left(t^{2} n^{2}\right)$, with $t$ the number of timesteps and $n$ the size of the hidden state. The space complexity of the forward pass of training is unchanged at $O(t n)$, but the space complexity of inference in SAB is now $O(t n)$ rather than $O(n)$. However, the time cost of the backward pass of training cost is very difficult to formulate. Hidden states depend on a sparse subset of past microstates, but each of those past microstates may itself depend on several other, even earlier microstates. The web of active connections is, therefore, akin to a directed acyclic graph, and it is quite possible in the worst case for a backpropagation starting at the last hidden state to touch all past microstates several times. However, if the number of microstates truly relevant to a task is low, the attention mechanism will repeatedly focus on them to the exclusion of all others, and pathological runtimes will not be encountered.</p>
<h3>8.1 Gradient Flow</h3>
<p>Our method approximates the true gradient but in a sense it's no different than the kind of approximation made with truncated gradient, except that instead of truncating to the last $k_{\text {trunc }}$ time steps, we truncate to one skip-step in the past, which can be arbitrarily far in the past. This provides a way of combating exploding and vanishing gradient problems by learning long-term dependencies. To verify the fact, we ran our model on all the datasets (Text8, Pixel-By-Pixel MNIST, char level PTB) with and without gradient clipping. We empirically found, that we need to use gradient clipping only for text8 dataset, for all the other datasets we observed little or no difference with gradient clipping.</p>            </div>
        </div>

    </div>
</body>
</html>
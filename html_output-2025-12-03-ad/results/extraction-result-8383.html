<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8383 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8383</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8383</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-9b45af10429681249fafb07c3b6012ea4ce63ffe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe" target="_blank">A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution, and applies this framework on a test bed of math word problems.</p>
                <p><strong>Paper Abstract:</strong> We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8383.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8383.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distil-GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distilled GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled smaller variant of the GPT-2 autoregressive transformer family evaluated for math word problems; shows low sensitivity and low assigned probability mass to correct numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Distil-GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Distilled GPT-2 model (small distilled variant of GPT-2 family). Autoregressive transformer; model size smaller than GPT-2 small (distillation applied).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand addition/subtraction (MWPs), evaluated also on three-operand templates via generated instances</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No internal mechanism localized; behavior consistent with reliance on surface-level heuristics and preference for certain memorized numeric results (diagonal/high-frequency results in heatmaps), i.e., memorization/shortcut patterns rather than algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Do-interventions on operands N and textual template T (including S vs O distinction); measurement of Total Causal Effect (TCE) and Direct Causal Effect (DCE) using delta_cp (change-in-prediction) and delta_rcc (relative change in confidence); heatmap visualization of P(g) over (n1,n2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Low overall probability mass on correct result g; low sensitivity (small TCE) and low robustness; accuracy figures reported in Appendix (low accuracy@1 and accuracy@10 compared to larger models).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Tends to produce inconsistent correct answers across operand permutations; prefers certain round results (e.g., 10,20,..) leading to frequent wrong but repeated outputs; low sensitivity to result-altering interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Heatmap (Figure 4) shows low P(g) and diagonal patterns indicating consistent preference for certain results; measured low TCE and low gap between TCE and DCE.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No internal probing (neurons/attention) performed; results limited to integer token space 1..300 and statement-style prompts; possible training-set leakage not excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8383.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8383.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (small/medium/large/XL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Series of autoregressive transformer models of increasing size (small → medium → large → XL) evaluated to examine size effects on sensitivity and robustness for math MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (small/medium/large/XL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 family (Radford et al.); transformer-based autoregressive language models with sizes increasing across variants (model sizes not individually enumerated in main text but standard GPT-2 sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand and some three-operand math word problems (addition, subtraction, multiplication, division when template implies).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Behavior consistent with mixed reliance on operands and surface text; larger GPT-2s show increased sensitivity compared to smallest versions but still exhibit spurious correlations and brittle behavior rather than systematic algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Same causal interventions (do(N), do(T) / do(S) vs do(O)), measurement of TCE/DCE via delta_cp and delta_rcc averaged over many generated instances per template.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Small GPT-2 variants: low sensitivity and low robustness. Larger GPT-2 variants: increased influence from interventions (higher TCE and DCE) but no consistent robustness improvement; accuracy increases with size but still below best-performing models in study.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Responds similarly to result-preserving and result-altering interventions (i.e., brittle); often fails to change prediction correctly when ground truth changes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Aggregated causal-effect measurements across sizes show larger models more influenced by N and T; correlation between accuracy and TCE positive (0.24 with TCE(N) on R) and negative with DCE.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No internal representation probes; conclusions are behavioral—cannot conclusively attribute mechanisms to attention/neuron-level processes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8383.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8383.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Neo / GPT-J / GPT-NeoX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Neo (1.3B/2.7B), GPT-J-6B, GPT-NeoX-20B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source autoregressive models pretrained on the Pile; evaluated to study how increasing parameter counts affect sensitivity (TCE) and undesired direct effects (DCE) on numeric MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (1.3B, 2.7B), GPT-J-6B, GPT-NeoX-20B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer models from EleutherAI (GPT-Neo variants, GPT-J 6B, GPT-NeoX 20B) pretrained on the Pile; sizes vary from ~1.3B to 20B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand addition/subtraction/multiplication/division MWPs; some three-operand problems considered for the best-performing models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Evidence suggests larger models (GPT-J, NeoX) develop stronger sensitivity to ground-truth changes (larger TCE) while also exhibiting increased DCE, indicating both improved assignment to correct g and persistent reliance on spurious numeric correlations or surface cues.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Do-interventions on operands and templates; measurement of TCE and DCE with delta_cp and delta_rcc; heatmaps of P(g) over operand grids.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-J-6B and GPT-NeoX: TCE was reported to be ~30x and ~1000x larger than DCE respectively (text reports these ratios for sensitivity vs direct effect). Larger models assign higher probability mass to correct result on average but show inconsistent predictions across different operand instantiations.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>While sensitivity (TCE) increases with size, robustness (low DCE) does not necessarily improve proportionally; models can still change predictions on result-preserving perturbations; inconsistent behavior across operand permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Heatmaps show higher P(g) for larger models but less consistency across different operands; large TCE/DCE ratios quantified from do-interventions indicate increased responsiveness to ground-truth but persistent direct effects from N.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Extreme TCE/DCE ratios vary by model (NeoX extremely large ratio) and may not translate to actual robustness in prediction-change metric; no internal representation analysis to show algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8383.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8383.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Davinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 Davinci variants (davinci-instruct-beta, text-davinci-002, text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned, large (175B) autoregressive models from OpenAI showing markedly higher sensitivity (TCE) and improved robustness (lower undesired DCE in some metrics) on two-operand MWPs relative to other GPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 Davinci (Instruct / Davinci-002 / Davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 175B-parameter models with instruction-tuning and RLHF variants (Davinci-002, Davinci-003 including PPO-trained Davinci-003), accessible via OpenAI API; large-scale autoregressive transformers trained and finetuned with instruction and human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand MWPs (addition/subtraction/multiplication/division), three-operand MWPs evaluated for best-performing variants.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Behaviorally closer to human-like computation path (O -> G -> R) than smaller models: models show higher sensitivity to ground-truth changes and lower harmful direct influence from surface text or operands in relative-confidence metrics, suggesting they rely more on extracting operations and computing results rather than purely surface cues or memorized numbers. Still no internal neuron-level mechanism identified.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Do-interventions on operands and text; measurement of TCE/DCE via delta_cp and delta_rcc (for GPT-3 delta_rcc approximated due to API top-k limitations); heatmap visualization of P(g) (computed from raw vocab probabilities when possible).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Davinci-003 (PPO-trained) shows an ~84% difference between direct and total effect for operand interventions (high TCE vs DCE) and ~76% difference between direct and total effect for text interventions; Davinci variants show highest DCE_rcc but low DCE_cp (i.e., large relative-confidence shifts despite often unchanged argmax prediction). For three-operand problems Davinci-003's direct effect (delta_cp) rose from 0.17 (two-operand) to 0.87 (three-operand), indicating degraded robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>On two-operand problems: better sensitivity and apparent robustness in many metrics, but relative confidence can vary widely even when argmax prediction stable; on three-operand problems: brittleness increases dramatically (high rate of changed predictions even for result-preserving interventions); more susceptible to textual framing variations (higher DCE(S->R) in some Davinci variants).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral causal-effect measurements (TCE vs DCE), large gaps between TCE and DCE indicate sensitivity to ground-truth G; heatmaps and relative-confidence measures (delta_rcc) show probability reallocation consistent with extracting and using operations; instruction-tuning and RLHF correlated with improved metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>API limitations required approximations for delta_rcc (top-k truncation), affecting metric accuracy; inability to probe internal representations prevents definitive claims about algorithmic computation; improvement does not generalize to more complex (three-operand) problems where robustness collapses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8383.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8383.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Curie / Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 Curie and Instruct variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller instruction-tuned GPT-3 variants evaluated alongside Davinci to study the effect of instruction tuning and size on arithmetic robustness and sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 Curie / Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 models smaller than Davinci (e.g., text-curie-001 and davinci-instruct-beta), trained with instruction-following objectives; accessible via OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand MWPs (addition/subtraction/multiplication/division).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Tend to be more susceptible to textual framing variation than non-instruction-tuned smaller models; show improved sensitivity relative to small non-instruction models but less than Davinci-003.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Same do-interventions on N and T; delta_cp and approximated delta_rcc via top-k token probabilities from API.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Intermediate sensitivity and robustness between small GPT-family models and Davinci; specific numeric deltas reported in aggregate figures (Figure 5) showing higher TCE and higher DCE(S->R) relative to non-instruction models.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>More influenced by surface text S than some non-instruction-tuned models (i.e., DCE(S->R) comparable or larger than DCE(N->R)), indicating susceptibility to framing.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Causal effect comparisons (TCE vs DCE) across models; instruction-tuned variants show different profile with larger direct effect from surface text.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Top-k API limitations and coarse approximations for probability mass; no internal representation probing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8383.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8383.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7B, 13B, 30B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open foundation models of increasing sizes evaluated to disentangle the effects of parameter count vs instruction tuning on arithmetic robustness and sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (7B, 13B, 30B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA family of autoregressive transformer models (7B, 13B, 30B) with different parameter counts; tokenization treats each digit as independent token (affects multi-digit numeric probability comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand MWPs (addition etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Larger LLaMA models show increased gap between TCE and DCE, suggesting size-driven improvements in sensitivity; Alpaca instruction-tuned LLaMA variant shows increased robustness at expense of sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Do-interventions on N and T; measurement of TCE and DCE using delta_cp (delta_rcc not computed reliably due to tokenization treating digits individually).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As size increases (7B→30B) the difference between TCE and DCE increases indicating greater sensitivity; specific numeric comparisons: GPT-NeoX-20B showed smaller TCE_cp - DCE_cp gap than LLaMA 7B (5.2% vs 9.0%), Alpaca shows decreased TCE and DCE relative to base LLaMA 7B but slightly increased gap (9.5% vs 9.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Tokenization of multi-digit numbers complicates probability normalization and metric computation; instruction tuning variant (Alpaca) improved robustness but reduced sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral causal metrics across sizes reveal size correlates with larger TCE-DCE gaps; comparison between LLaMA and Alpaca shows instruction tuning can shift sensitivity/robustness tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Tokenization makes cross-model numeric-probability comparisons hard; Alpaca instruction data (focused on user tasks) may not improve math reasoning—type of instruction tuning matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8383.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8383.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca (LLaMA-7B ft)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stanford Alpaca (instruction-tuned LLaMA 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned version of LLaMA-7B using user-oriented instruction data; shows increased robustness but reduced sensitivity on MWPs compared to base LLaMA 7B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca (LLaMA-7B fine-tuned with instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLaMA-7B (Stanford Alpaca) using synthetic/user-oriented instructions; same architecture as LLaMA 7B but finetuned for instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand math word problems (addition/subtraction/multiplication/division).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Instruction tuning with general user-oriented prompts can increase robustness (lower DCE) but may reduce sensitivity (lower TCE), suggesting instruction data focused away from reasoning tasks yields different tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Do-interventions on N and T; TCE/DCE computed with delta_cp (delta_rcc not computed due to tokenization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Alpaca: decreased both TCE and DCE relative to base LLaMA 7B (robustness improved but sensitivity reduced); gap between TCE and DCE slightly increased (9.5% vs 9.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Less sensitive to result-altering interventions (may under-react), suggesting instruction tuning dataset composition matters for math performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Comparison of causal-effect metrics before/after instruction tuning indicates shift in sensitivity/robustness tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Instruction data for Alpaca not focused on reasoning so limited generality; tokenization issues for numeric tokens persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8383.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8383.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Overall causal-framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal intervention framework for MWPs (TCE/DCE, delta_cp, delta_rcc)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's central methodological contribution: a causal graph and do-intervention protocol to separate desired sensitivity to ground-truth solution G (TCE) from undesired direct effects of surface text S and operands N (DCE).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>causal-framework (paper methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework models MWPs as (T=(O,S), N) with ground-truth G=f_O(N); interventions: do(N), do(T) with/without changing O; metrics: TCE and DCE measured via distributional distance delta, concretely delta_cp (change-in-argmax) and delta_rcc (relative change in confidence).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applies to MWPs instantiated from templates (two- and three-operand problems) across arithmetic operations (+, -, ×, ÷).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Does not posit internal neuron mechanisms but frames possible model causal paths: T->S->R (surface effects), T->O->R (operation extraction), N->G->R (human-like path), and direct N->R; aims to quantify which paths the model uses.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral interventions (do-operator) on input factors; causal mediation analysis (Pearl 1995/2001) to compute TCE and DCE; heatmaps and token-probability inspections for numeric outputs; API-specific approximations for GPT-3 top-k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Provides population-level measures of sensitivity vs brittleness across models: e.g., large GPT-3 Davinci models show much larger TCE vs DCE for two-operand problems; correlations between accuracy@10 and TCE positive (0.24 for N, 0.49 for T) and negative with DCE (-0.26, -0.36).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Highlights brittle behavior when models rely on surface cues or numeric memorization (high DCE), and degradation on more complex instances (three-operand problems) even for models that do well on simpler problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical do-interventions across 437 two-operand templates and 307 three-operand templates, averaged over many instances; heatmaps and effect-size statistics demonstrate differing reliance on T, N, and inferred O across models.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Framework constrained by practicable interventions (cannot fully isolate O without affecting S), limited numeric range (1..300), tokenization differences across models, and API top-k limitations for GPT-3 causing approximation in delta_rcc computations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Are NLP models really able to solve simple math word problems? <em>(Rating: 2)</em></li>
                <li>Impact of pretraining term frequencies on few-shot reasoning <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Emergent abilities of large language models <em>(Rating: 1)</em></li>
                <li>Causal analysis of syntactic agreement mechanisms in neural language models <em>(Rating: 1)</em></li>
                <li>Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8383",
    "paper_id": "paper-9b45af10429681249fafb07c3b6012ea4ce63ffe",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Distil-GPT-2",
            "name_full": "Distilled GPT-2",
            "brief_description": "A distilled smaller variant of the GPT-2 autoregressive transformer family evaluated for math word problems; shows low sensitivity and low assigned probability mass to correct numeric results.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Distil-GPT-2",
            "model_description": "Distilled GPT-2 model (small distilled variant of GPT-2 family). Autoregressive transformer; model size smaller than GPT-2 small (distillation applied).",
            "arithmetic_task_type": "Two-operand addition/subtraction (MWPs), evaluated also on three-operand templates via generated instances",
            "mechanism_or_representation": "No internal mechanism localized; behavior consistent with reliance on surface-level heuristics and preference for certain memorized numeric results (diagonal/high-frequency results in heatmaps), i.e., memorization/shortcut patterns rather than algorithmic computation.",
            "probing_or_intervention_method": "Do-interventions on operands N and textual template T (including S vs O distinction); measurement of Total Causal Effect (TCE) and Direct Causal Effect (DCE) using delta_cp (change-in-prediction) and delta_rcc (relative change in confidence); heatmap visualization of P(g) over (n1,n2).",
            "performance_metrics": "Low overall probability mass on correct result g; low sensitivity (small TCE) and low robustness; accuracy figures reported in Appendix (low accuracy@1 and accuracy@10 compared to larger models).",
            "error_types_or_failure_modes": "Tends to produce inconsistent correct answers across operand permutations; prefers certain round results (e.g., 10,20,..) leading to frequent wrong but repeated outputs; low sensitivity to result-altering interventions.",
            "evidence_for_mechanism": "Heatmap (Figure 4) shows low P(g) and diagonal patterns indicating consistent preference for certain results; measured low TCE and low gap between TCE and DCE.",
            "counterexamples_or_challenges": "No internal probing (neurons/attention) performed; results limited to integer token space 1..300 and statement-style prompts; possible training-set leakage not excluded.",
            "uuid": "e8383.0",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-2 family",
            "name_full": "GPT-2 (small/medium/large/XL)",
            "brief_description": "Series of autoregressive transformer models of increasing size (small → medium → large → XL) evaluated to examine size effects on sensitivity and robustness for math MWPs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (small/medium/large/XL)",
            "model_description": "GPT-2 family (Radford et al.); transformer-based autoregressive language models with sizes increasing across variants (model sizes not individually enumerated in main text but standard GPT-2 sizes).",
            "arithmetic_task_type": "Two-operand and some three-operand math word problems (addition, subtraction, multiplication, division when template implies).",
            "mechanism_or_representation": "Behavior consistent with mixed reliance on operands and surface text; larger GPT-2s show increased sensitivity compared to smallest versions but still exhibit spurious correlations and brittle behavior rather than systematic algorithmic arithmetic.",
            "probing_or_intervention_method": "Same causal interventions (do(N), do(T) / do(S) vs do(O)), measurement of TCE/DCE via delta_cp and delta_rcc averaged over many generated instances per template.",
            "performance_metrics": "Small GPT-2 variants: low sensitivity and low robustness. Larger GPT-2 variants: increased influence from interventions (higher TCE and DCE) but no consistent robustness improvement; accuracy increases with size but still below best-performing models in study.",
            "error_types_or_failure_modes": "Responds similarly to result-preserving and result-altering interventions (i.e., brittle); often fails to change prediction correctly when ground truth changes.",
            "evidence_for_mechanism": "Aggregated causal-effect measurements across sizes show larger models more influenced by N and T; correlation between accuracy and TCE positive (0.24 with TCE(N) on R) and negative with DCE.",
            "counterexamples_or_challenges": "No internal representation probes; conclusions are behavioral—cannot conclusively attribute mechanisms to attention/neuron-level processes.",
            "uuid": "e8383.1",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-Neo / GPT-J / GPT-NeoX",
            "name_full": "GPT-Neo (1.3B/2.7B), GPT-J-6B, GPT-NeoX-20B",
            "brief_description": "Open-source autoregressive models pretrained on the Pile; evaluated to study how increasing parameter counts affect sensitivity (TCE) and undesired direct effects (DCE) on numeric MWPs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (1.3B, 2.7B), GPT-J-6B, GPT-NeoX-20B",
            "model_description": "Autoregressive transformer models from EleutherAI (GPT-Neo variants, GPT-J 6B, GPT-NeoX 20B) pretrained on the Pile; sizes vary from ~1.3B to 20B parameters.",
            "arithmetic_task_type": "Two-operand addition/subtraction/multiplication/division MWPs; some three-operand problems considered for the best-performing models.",
            "mechanism_or_representation": "Evidence suggests larger models (GPT-J, NeoX) develop stronger sensitivity to ground-truth changes (larger TCE) while also exhibiting increased DCE, indicating both improved assignment to correct g and persistent reliance on spurious numeric correlations or surface cues.",
            "probing_or_intervention_method": "Do-interventions on operands and templates; measurement of TCE and DCE with delta_cp and delta_rcc; heatmaps of P(g) over operand grids.",
            "performance_metrics": "GPT-J-6B and GPT-NeoX: TCE was reported to be ~30x and ~1000x larger than DCE respectively (text reports these ratios for sensitivity vs direct effect). Larger models assign higher probability mass to correct result on average but show inconsistent predictions across different operand instantiations.",
            "error_types_or_failure_modes": "While sensitivity (TCE) increases with size, robustness (low DCE) does not necessarily improve proportionally; models can still change predictions on result-preserving perturbations; inconsistent behavior across operand permutations.",
            "evidence_for_mechanism": "Heatmaps show higher P(g) for larger models but less consistency across different operands; large TCE/DCE ratios quantified from do-interventions indicate increased responsiveness to ground-truth but persistent direct effects from N.",
            "counterexamples_or_challenges": "Extreme TCE/DCE ratios vary by model (NeoX extremely large ratio) and may not translate to actual robustness in prediction-change metric; no internal representation analysis to show algorithmic computation.",
            "uuid": "e8383.2",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-3 Davinci",
            "name_full": "GPT-3 Davinci variants (davinci-instruct-beta, text-davinci-002, text-davinci-003)",
            "brief_description": "Instruction-tuned, large (175B) autoregressive models from OpenAI showing markedly higher sensitivity (TCE) and improved robustness (lower undesired DCE in some metrics) on two-operand MWPs relative to other GPT variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 Davinci (Instruct / Davinci-002 / Davinci-003)",
            "model_description": "GPT-3 175B-parameter models with instruction-tuning and RLHF variants (Davinci-002, Davinci-003 including PPO-trained Davinci-003), accessible via OpenAI API; large-scale autoregressive transformers trained and finetuned with instruction and human feedback.",
            "arithmetic_task_type": "Two-operand MWPs (addition/subtraction/multiplication/division), three-operand MWPs evaluated for best-performing variants.",
            "mechanism_or_representation": "Behaviorally closer to human-like computation path (O -&gt; G -&gt; R) than smaller models: models show higher sensitivity to ground-truth changes and lower harmful direct influence from surface text or operands in relative-confidence metrics, suggesting they rely more on extracting operations and computing results rather than purely surface cues or memorized numbers. Still no internal neuron-level mechanism identified.",
            "probing_or_intervention_method": "Do-interventions on operands and text; measurement of TCE/DCE via delta_cp and delta_rcc (for GPT-3 delta_rcc approximated due to API top-k limitations); heatmap visualization of P(g) (computed from raw vocab probabilities when possible).",
            "performance_metrics": "Davinci-003 (PPO-trained) shows an ~84% difference between direct and total effect for operand interventions (high TCE vs DCE) and ~76% difference between direct and total effect for text interventions; Davinci variants show highest DCE_rcc but low DCE_cp (i.e., large relative-confidence shifts despite often unchanged argmax prediction). For three-operand problems Davinci-003's direct effect (delta_cp) rose from 0.17 (two-operand) to 0.87 (three-operand), indicating degraded robustness.",
            "error_types_or_failure_modes": "On two-operand problems: better sensitivity and apparent robustness in many metrics, but relative confidence can vary widely even when argmax prediction stable; on three-operand problems: brittleness increases dramatically (high rate of changed predictions even for result-preserving interventions); more susceptible to textual framing variations (higher DCE(S-&gt;R) in some Davinci variants).",
            "evidence_for_mechanism": "Behavioral causal-effect measurements (TCE vs DCE), large gaps between TCE and DCE indicate sensitivity to ground-truth G; heatmaps and relative-confidence measures (delta_rcc) show probability reallocation consistent with extracting and using operations; instruction-tuning and RLHF correlated with improved metrics.",
            "counterexamples_or_challenges": "API limitations required approximations for delta_rcc (top-k truncation), affecting metric accuracy; inability to probe internal representations prevents definitive claims about algorithmic computation; improvement does not generalize to more complex (three-operand) problems where robustness collapses.",
            "uuid": "e8383.3",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-3 Curie / Instruct",
            "name_full": "GPT-3 Curie and Instruct variants",
            "brief_description": "Smaller instruction-tuned GPT-3 variants evaluated alongside Davinci to study the effect of instruction tuning and size on arithmetic robustness and sensitivity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 Curie / Instruct",
            "model_description": "Instruction-tuned GPT-3 models smaller than Davinci (e.g., text-curie-001 and davinci-instruct-beta), trained with instruction-following objectives; accessible via OpenAI API.",
            "arithmetic_task_type": "Two-operand MWPs (addition/subtraction/multiplication/division).",
            "mechanism_or_representation": "Tend to be more susceptible to textual framing variation than non-instruction-tuned smaller models; show improved sensitivity relative to small non-instruction models but less than Davinci-003.",
            "probing_or_intervention_method": "Same do-interventions on N and T; delta_cp and approximated delta_rcc via top-k token probabilities from API.",
            "performance_metrics": "Intermediate sensitivity and robustness between small GPT-family models and Davinci; specific numeric deltas reported in aggregate figures (Figure 5) showing higher TCE and higher DCE(S-&gt;R) relative to non-instruction models.",
            "error_types_or_failure_modes": "More influenced by surface text S than some non-instruction-tuned models (i.e., DCE(S-&gt;R) comparable or larger than DCE(N-&gt;R)), indicating susceptibility to framing.",
            "evidence_for_mechanism": "Causal effect comparisons (TCE vs DCE) across models; instruction-tuned variants show different profile with larger direct effect from surface text.",
            "counterexamples_or_challenges": "Top-k API limitations and coarse approximations for probability mass; no internal representation probing.",
            "uuid": "e8383.4",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LLaMA family",
            "name_full": "LLaMA (7B, 13B, 30B)",
            "brief_description": "Open foundation models of increasing sizes evaluated to disentangle the effects of parameter count vs instruction tuning on arithmetic robustness and sensitivity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA (7B, 13B, 30B)",
            "model_description": "LLaMA family of autoregressive transformer models (7B, 13B, 30B) with different parameter counts; tokenization treats each digit as independent token (affects multi-digit numeric probability comparisons).",
            "arithmetic_task_type": "Two-operand MWPs (addition etc.).",
            "mechanism_or_representation": "Larger LLaMA models show increased gap between TCE and DCE, suggesting size-driven improvements in sensitivity; Alpaca instruction-tuned LLaMA variant shows increased robustness at expense of sensitivity.",
            "probing_or_intervention_method": "Do-interventions on N and T; measurement of TCE and DCE using delta_cp (delta_rcc not computed reliably due to tokenization treating digits individually).",
            "performance_metrics": "As size increases (7B→30B) the difference between TCE and DCE increases indicating greater sensitivity; specific numeric comparisons: GPT-NeoX-20B showed smaller TCE_cp - DCE_cp gap than LLaMA 7B (5.2% vs 9.0%), Alpaca shows decreased TCE and DCE relative to base LLaMA 7B but slightly increased gap (9.5% vs 9.0%).",
            "error_types_or_failure_modes": "Tokenization of multi-digit numbers complicates probability normalization and metric computation; instruction tuning variant (Alpaca) improved robustness but reduced sensitivity.",
            "evidence_for_mechanism": "Behavioral causal metrics across sizes reveal size correlates with larger TCE-DCE gaps; comparison between LLaMA and Alpaca shows instruction tuning can shift sensitivity/robustness tradeoff.",
            "counterexamples_or_challenges": "Tokenization makes cross-model numeric-probability comparisons hard; Alpaca instruction data (focused on user tasks) may not improve math reasoning—type of instruction tuning matters.",
            "uuid": "e8383.5",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Alpaca (LLaMA-7B ft)",
            "name_full": "Stanford Alpaca (instruction-tuned LLaMA 7B)",
            "brief_description": "Instruction-tuned version of LLaMA-7B using user-oriented instruction data; shows increased robustness but reduced sensitivity on MWPs compared to base LLaMA 7B.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Alpaca (LLaMA-7B fine-tuned with instructions)",
            "model_description": "Instruction-tuned LLaMA-7B (Stanford Alpaca) using synthetic/user-oriented instructions; same architecture as LLaMA 7B but finetuned for instruction following.",
            "arithmetic_task_type": "Two-operand math word problems (addition/subtraction/multiplication/division).",
            "mechanism_or_representation": "Instruction tuning with general user-oriented prompts can increase robustness (lower DCE) but may reduce sensitivity (lower TCE), suggesting instruction data focused away from reasoning tasks yields different tradeoffs.",
            "probing_or_intervention_method": "Do-interventions on N and T; TCE/DCE computed with delta_cp (delta_rcc not computed due to tokenization).",
            "performance_metrics": "Alpaca: decreased both TCE and DCE relative to base LLaMA 7B (robustness improved but sensitivity reduced); gap between TCE and DCE slightly increased (9.5% vs 9.0%).",
            "error_types_or_failure_modes": "Less sensitive to result-altering interventions (may under-react), suggesting instruction tuning dataset composition matters for math performance.",
            "evidence_for_mechanism": "Comparison of causal-effect metrics before/after instruction tuning indicates shift in sensitivity/robustness tradeoff.",
            "counterexamples_or_challenges": "Instruction data for Alpaca not focused on reasoning so limited generality; tokenization issues for numeric tokens persist.",
            "uuid": "e8383.6",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Overall causal-framework",
            "name_full": "Causal intervention framework for MWPs (TCE/DCE, delta_cp, delta_rcc)",
            "brief_description": "The paper's central methodological contribution: a causal graph and do-intervention protocol to separate desired sensitivity to ground-truth solution G (TCE) from undesired direct effects of surface text S and operands N (DCE).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "causal-framework (paper methodology)",
            "model_description": "Framework models MWPs as (T=(O,S), N) with ground-truth G=f_O(N); interventions: do(N), do(T) with/without changing O; metrics: TCE and DCE measured via distributional distance delta, concretely delta_cp (change-in-argmax) and delta_rcc (relative change in confidence).",
            "arithmetic_task_type": "Applies to MWPs instantiated from templates (two- and three-operand problems) across arithmetic operations (+, -, ×, ÷).",
            "mechanism_or_representation": "Does not posit internal neuron mechanisms but frames possible model causal paths: T-&gt;S-&gt;R (surface effects), T-&gt;O-&gt;R (operation extraction), N-&gt;G-&gt;R (human-like path), and direct N-&gt;R; aims to quantify which paths the model uses.",
            "probing_or_intervention_method": "Behavioral interventions (do-operator) on input factors; causal mediation analysis (Pearl 1995/2001) to compute TCE and DCE; heatmaps and token-probability inspections for numeric outputs; API-specific approximations for GPT-3 top-k.",
            "performance_metrics": "Provides population-level measures of sensitivity vs brittleness across models: e.g., large GPT-3 Davinci models show much larger TCE vs DCE for two-operand problems; correlations between accuracy@10 and TCE positive (0.24 for N, 0.49 for T) and negative with DCE (-0.26, -0.36).",
            "error_types_or_failure_modes": "Highlights brittle behavior when models rely on surface cues or numeric memorization (high DCE), and degradation on more complex instances (three-operand problems) even for models that do well on simpler problems.",
            "evidence_for_mechanism": "Empirical do-interventions across 437 two-operand templates and 307 three-operand templates, averaged over many instances; heatmaps and effect-size statistics demonstrate differing reliance on T, N, and inferred O across models.",
            "counterexamples_or_challenges": "Framework constrained by practicable interventions (cannot fully isolate O without affecting S), limited numeric range (1..300), tokenization differences across models, and API top-k limitations for GPT-3 causing approximation in delta_rcc computations.",
            "uuid": "e8383.7",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Are NLP models really able to solve simple math word problems?",
            "rating": 2
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "rating": 1
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Emergent abilities of large language models",
            "rating": 1
        },
        {
            "paper_title": "Causal analysis of syntactic agreement mechanisms in neural language models",
            "rating": 1
        },
        {
            "paper_title": "Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics",
            "rating": 2
        }
    ],
    "cost": 0.016578,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models</h1>
<p>Alessandro Stolfo*<br>ETH Zürich<br>stolfoa@ethz.ch</p>
<p>Kumar Shridhar<br>ETH Zürich<br>shkumar@ethz.ch</p>
<p>Bernhard Schölkopf
MPI \&amp; ETH Zürich
bs@tue.mpg.de</p>
<h2>Zhijing Jin*</h2>
<p>MPI \&amp; ETH Zürich
jinzhi@ethz.ch</p>
<h2>Mrinmaya Sachan</h2>
<p>ETH Zürich
msachan@ethz.ch</p>
<h2>Abstract</h2>
<p>We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Many natural language understanding situations, such as understanding the financial news, require reasoning with text that includes numbers. However, such mathematical reasoning is challenging for NLP models (Cobbe et al., 2021; Mishra et al., 2022b). Mathematical reasoning for text has been an active area of research for a while (Seo et al., 2015; Sachan and Xing, 2017; Sachan et al., 2017, 2018, inter alia), and has also emerged as a key task to track the capabilities of large language models (LLMs) in recent years (Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022a, inter alia).</p>
<p>However, despite the impressive performance of LLMs on various math reasoning benchmarks (e.g.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Through our framework, we conduct dointerventions on the input and evaluate the change in the distribution $\mathbb{P}(R)$ of the prediction $R$ by LLMs, in this figure, GPT-J. This allows us to measure the causal effect of each factor in the input on the model's response.</p>
<p>Ouyang et al., 2022; Chowdhery et al., 2022), it remains unclear whether these models have learned mere artifacts in the data or have truly mastered the mathematical concepts needed to consistently solve all variations of the same problem (Patel et al., 2021; Razeghi et al., 2022; Welleck et al., 2022). In sharp contrast with a large number of papers on improving the performance of LLMs on various types of math-based problems, there has been little effort on behavioral analysis of LLMs for these tasks. Existing methods for understanding the robustness of these models (Patel et al., 2021) rely on manually constructing variations of math problems, and we do not yet have a principled, comprehensive framework for quantifying such robustness.</p>
<p>Thus, in this work, we propose a formal framework based on causal inference, to quantify the robustness of NLP models' math reasoning abilities. Specifically, we describe a causal graph formulation of math reasoning, where the graph allows us to measure the difference in the structural causal</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Causal graph of model predictions on math questions. We highlight the difference between a cognitively-inspired correct reasoning path (G<sup>L</sup>) and the undesired effects that some factors might have on the model's prediction (red arrows). By performing controlled interventions of the numerical values (N) and on the textual framing of the problem (T, S), we are able to quantify the causal effects of each factor.</p>
<p>models of human reasoning and model judgment. We consider various causal factors such as the textual framing of the question, numerical operands, and operation types. Then, we identify a set of interventions in the context of math word problems (an example of which is illustrated in Figure 1), and provide a causal inference framework to obtain causal effects of each factor via direct do-interventions (Pearl, 1995) and causal mediation analysis (Pearl, 2001). While our approach is reminiscent of recent studies using causal analysis for LLMs (Finlayson et al., 2021; Vig et al., 2020; Meng et al., 2022), in this work, we provide a new theoretical analysis framework specifically suitable for math reasoning. Using our framework, we disentangle factors affecting the model's predictions and measure their influences. This way, we are able to provide insights into the model's reasoning in terms of <em>robustness</em> and <em>sensitivity</em> with respect to changes in these factors.</p>
<p>We apply our framework to study a set of thirteen GPT models with various sizes and training procedures (i.e., instruction-tuned and non-instruction-tuned). We observe that, among non-instruction-tuned language models, the larger ones tend to be more sensitive to changes in the ground-truth result of a math word problem, but not necessarily more robust. However, we observe a different behavior in the instruction-tuned GPT-3 models (Ouyang et al., 2022), which show a remarkable improvement in both sensitivity and robustness, although the robustness reduces when problems get more complicated. We additionally investigate the role of size and instruction tuning on the model's performance with three models of the LLaMA family (Touvron et al., 2023) and Stanford Alpaca (Taori et al., 2023).</p>
<h2>2 Problem Setup</h2>
<p>We consider a dataset D of math word problems (MWPs), where each MWP is denoted as a question Q. Q is a list (T, N) consisting of a question template T and an ordered list of operands N = (N<sub>1</sub>, N<sub>2</sub>, . . . , N<sub>m</sub>). Each question template T := (O, S) further contains two types of information: a set of arithmetic operations O implicitly expressed in the question, and the text surface form S irrelevant to the arithmetic operations. O incorporates the information relative to the operations as a collection of tuples {(O<sub>1</sub>, i<sub>1</sub>, j<sub>1</sub>), (O<sub>2</sub>, i<sub>2</sub>, j<sub>2</sub>), . . . }, where O<sub>k</sub> ∈ {+, −, ×, 亠} (k ∈ ℕ) and i<sub>k</sub>, j<sub>k</sub> ∈ ℕ represent the indices of the operands to which operator O<sub>k</sub> should be applied to.<sup>2</sup> The ground-truth result G = f<sub>O</sub>(N) is calculated by computing the function f<sub>O</sub>, which represents the application of all the operators in O to the respective operands. We illustrate the factors in Q and their inter-dependency in the causal graph in Figure 2. A two-operand instance q of Q in this form from Patel et al. (2021) is:</p>
<ul>
<li>Template t: Mark has n<sub>1</sub> trees in his backyard. If he plants n<sub>2</sub> more, how many trees will he have?</li>
<li>Operands n: (n<sub>1</sub> = 12, n<sub>2</sub> = 13)</li>
<li>Operations o: {(“+”, 1, 2)}</li>
<li>Result: g = f<sub>o</sub>(n) = n<sub>1</sub> + n<sub>2</sub> = 25</li>
</ul>
<p><sup>2</sup>The intermediate result of operation O<sub>l</sub> is indicated by i<sub>k</sub> = m + l.</p>
<p>Our goal is to quantify the robustness of a model $\mathcal{M}$ on the set of problems $\boldsymbol{q} \in \mathcal{D}$. Ideally, $\mathcal{D}$ should be a dataset not seen by the model during training. We assume that a model takes $\boldsymbol{q}$ as input and predicts a probability distribution of the result $R: \mathbb{P}(R \mid \boldsymbol{t}, \boldsymbol{n})$. Our formulation below will be easier to understand using this finite discrete set and can be generalized to any kind of data pairing a natural language template with a function that maps a set of operands to a result (e.g., a Python program; Mishra et al. 2022a).</p>
<h2>3 A Causal Framework</h2>
<p>In this section, we describe our framework in three steps. First, we define the idea of model robustness on MWPs. Then, we identify possible dointerventions (Pearl, 1995) that we can perform. Finally, we describe the causal effects that we measure to quantify the robustness of various models.</p>
<h3>3.1 Step 1. Question Reformulation</h3>
<p>We address the research question "Is a model reasoning robustly on MWPs?" by comparing the causal mechanisms of the model's decisions to a hypothesized human reasoning mechanism. Note that we do not claim to know how humans reason about these problems. We simply propose a reasonable and intuitive way to judge model robustness given a reasonable and intuitive human reasoning mechanism inspired by findings regarding the independence of language and mathematical reasoning in humans (Brannon, 2005; Monti et al., 2012).</p>
<p>Human Reasoning Mechanisms. The causal mechanisms of how humans might solve $\boldsymbol{q}$ include</p>
<p>$$
\begin{aligned}
\boldsymbol{o} &amp; =f_{\text {abstract }}(\boldsymbol{q}) \
g &amp; =f_{\boldsymbol{o}}(\boldsymbol{n})
\end{aligned}
$$</p>
<p>where they first abstract the arithmetic operations $\boldsymbol{o}$ from the problem $\boldsymbol{q}$ by some cognitive process $f_{\text {abstract }}$, and then apply the operation to the operands to obtain the result $g$. We show these mechanisms in the green subgraph $\mathcal{G}_{h}$ of Figure 2.</p>
<p>Model Reasoning Mechanisms. In contrast, the causal mechanisms of how a model might solve $\boldsymbol{q}$ are as follows:</p>
<p>$$
r=f_{\text {blackBox }}(\boldsymbol{t}, \boldsymbol{n})
$$</p>
<p>where we are unsure about (1) what part(s) of $\boldsymbol{t}$ the model takes into account, and (2) how it operates over the relevant variables.</p>
<p>Thus, we draw all possible causal mechanisms that might take place in the black-box model $f_{\text {blackBox }}$ in the complete causal graph in Figure 2. Some possible fine-grained causal mechanisms are</p>
<ol>
<li>The model might attend over the question template $\boldsymbol{t}$ in two ways: paying attention to the text surface form $s$ via the causal path $\boldsymbol{T} \rightarrow S \rightarrow R$, or text relevant to the math operations $\boldsymbol{o}$ via the causal path $\boldsymbol{T} \rightarrow \boldsymbol{O} \rightarrow R$.</li>
<li>The model might also attend to the operands $\boldsymbol{n}:=\left(n_{1}, n_{2}, \ldots\right)$ via a causal path $\boldsymbol{N} \rightarrow R$.</li>
<li>If the model learns the correct causal mechanisms as in the human cognitive process, it should capture how the operator and the operands matter to the ground-truth result $g$ (via $\boldsymbol{O} \rightarrow G$ and $\boldsymbol{N} \rightarrow G$ ) and then the model prediction should be sensitive to any changes in the ground truth, namely $G \rightarrow R$. No spurious correlations can directly affect $R$ without going through the mediator $G$.
Hence, to answer the question "How robust is the mathematical reasoning of a model on MWPs?" we can answer the following subquestions:</li>
<li>How does $R$ change in response to $G$ ? By quantifying this, we assess the sensitivity (correct responsiveness) of the model to changes in the problem. In other words, does the model correctly adjust its prediction in response to a change in the correct solution of the problem?</li>
<li>What is the (unwanted) direct causal effect size of $S \rightarrow R$, and $\boldsymbol{N} \rightarrow R$ ? We see the quantities as a measure of the brittleness (i.e., wrong responsiveness) of the model to resultpreserving changes in the input. The lower the direct causal effect of $S$ and $\boldsymbol{N}$, the more robust the model is.</li>
</ol>
<h3>3.2 Step 2. Causal Intervention List</h3>
<p>After formulating the cognitively-inspired subgraph $\mathcal{G}_{h}$ and defining the undesired causal paths in Figure 2, we list all feasible limited actions that allow us to perform our causal analysis. In the context of MWPs, we use the following interventions:</p>
<ol>
<li>Direct intervention on all possible $n_{1}, n_{2}, \ldots$;</li>
<li>Partially controllable interventions on $\boldsymbol{T}$. We can replace the template $\boldsymbol{T}$ in two ways:</li>
</ol>
<p>(a) both $S$ and $\boldsymbol{O}$ are affected, or
(b) $S$ is affected but $\boldsymbol{O}$ is not affected.</p>
<h3>3.3 Step 3. Turning Limited Actions into Causal Effect Sizes</h3>
<p>Next, we explain how we can obtain the causal effect sizes we want (listed in Step 1) from the limited set of interventions we can do (listed in Step 2). Specifically, we first start from all the feasible interventions, and for variables that we cannot directly intervene on, we apply deductions from do-calculus (Pearl, 1995) to obtain or approximate the direct causal effect sizes. In the following, we describe a list of causal effect sizes that we need.</p>
<p>General Formulation. Let us consider an intervention $\operatorname{do}\left(X: x \rightarrow x^{\prime}\right)$, where $X \in{\boldsymbol{T}, S, \boldsymbol{N}}$ and a problem $\boldsymbol{Q}={\boldsymbol{T}, \boldsymbol{N}}$. The support of the numerical values $N_{i}$ 's and $R$ is $\mathcal{I} \subseteq \mathbb{N}$, and we consider $\boldsymbol{N}$ to be distributed uniformly over the set $\left{\boldsymbol{n} \in \mathcal{I}^{2} \mid f_{\boldsymbol{O}}(\boldsymbol{n}) \in \mathcal{I}\right}$. We denote the distribution before the intervention $\mathbb{P}(R \mid \boldsymbol{T}, \boldsymbol{N})$ as $P$ and the distribution after the intervention as $P^{\prime}$.</p>
<p>Following the distributional definition of causal effect by Pearl (1995), we quantify the effect of factor $X$ in our causal graph using a distance metric $\delta$ between the distributions $P$ and $P^{\prime}$. That is,</p>
<p>$$
\mathrm{CE}=\delta\left(P, P^{\prime}\right)
$$</p>
<p>where CE can refer to the total causal effect (TCE, i.e., the joint effect through all the directed causal paths from a variable to another), or the direct causal effect (DCE, i.e., the effect from the directed causal path from a variable to another that does not go through any intermediate variables) (Pearl, 2001). We describe our choices for $\delta$ in Section 3.4.</p>
<p>Causal Effects of the Operands. When intervening on the operands $\boldsymbol{N}:=\left(N_{1}, N_{2}, \ldots\right)$, we can obtain the size of the total causal effect of $\boldsymbol{N}$ on $R$, namely</p>
<p>$$
\begin{aligned}
&amp; \operatorname{TCE}(\boldsymbol{N} \text { on } R):=\mathbb{E}_{\boldsymbol{n}^{\prime} \sim \mathbb{P}(\boldsymbol{N})}\left[\delta\left(P, P^{\prime}\right)\right] \
&amp; \text { where } P^{\prime}=\mathbb{P}\left(R \mid \boldsymbol{T}, \operatorname{do}\left(\boldsymbol{N}=\boldsymbol{n}^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>Note that this TCE is not the exact desired quantity, because we want to separate two different paths of how $\boldsymbol{N}$ affects $R$ : (1) the path $\boldsymbol{N} \rightarrow G \rightarrow R$, which is the correct decision path that we want the model to pick up (where the model reacts to
the change in the ground-truth answer), and (2) the path $\boldsymbol{N} \rightarrow R$, which is the spurious correlation that the model might have learned (where the model relies on some spurious correlations with certain numerical values, which could be traced to perhaps their frequencies in the training corpus).</p>
<p>We can quantify the direct causal effect (DCE, i.e., the effect from the directed causal path from a variable to another that does not go through any intermediate variables) (Pearl, 2001) of $\boldsymbol{N}$ on $R$, namely the strength of the direct causal path $\boldsymbol{N} \rightarrow$ $R$, by controlling for $G$ to be fixed every time we intervene on $\boldsymbol{N}$ :</p>
<p>$$
\begin{aligned}
&amp; \operatorname{DCE}(\boldsymbol{N} \rightarrow R):=\mathbb{E}_{\boldsymbol{n}^{\prime} \sim \mathbb{P}(\boldsymbol{N} \mid G)}\left[\delta\left(P, P^{\prime}\right)\right] \
&amp; \text { where } P^{\prime}=\mathbb{P}\left(R \mid \boldsymbol{T}, \operatorname{do}\left(\boldsymbol{N}=\boldsymbol{n}^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>For example, if we observe a model doing $100+$ $100=200$ correctly, we want to separate the math ability here into (1) the model's sensitivity towards the ground-truth answer, and (2) the model's decisions based on its familiarity with just the operand 100. Here, the overall effect is the calculable $\operatorname{TCE}(\boldsymbol{N}$ on $R)$ by Eq. 5, and one of the subeffects is the calculable $\operatorname{DCE}(\boldsymbol{N} \rightarrow R)$ by Eq. 7.</p>
<p>Causal Effects of the Text Surface Form. As for the operands, we can compute both the direct and indirect effects of the surface form representing the math problem. In particular, intervening on $\boldsymbol{T}$ without controlling for $\boldsymbol{O}$ (intervention 2a in Sec. 3.2), we can compute the total effect, i.e.,</p>
<p>$$
\begin{aligned}
&amp; \operatorname{TCE}(\boldsymbol{T} \text { on } R):=\mathbb{E}_{\boldsymbol{t}^{\prime} \sim \mathbb{P}(\boldsymbol{T})}\left[\delta\left(P, P^{\prime}\right)\right] \
&amp; \text { where } P^{\prime}=\mathbb{P}\left(R \mid \boldsymbol{N}, \operatorname{do}\left(\boldsymbol{T}=\boldsymbol{t}^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>Controlling for the operations $\boldsymbol{O}$ (intervention 2 b in Sec. 3.2) will instead allow us to obtain the direct causal effect of the surface text:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{DCE}(S \rightarrow R):=\mathbb{E}_{\boldsymbol{t}^{\prime} \sim \mathbb{P}(\boldsymbol{T} \mid O)}\left[\delta\left(P, P^{\prime}\right)\right] \
&amp; \text { where } P^{\prime}=\mathbb{P}\left(R \mid \boldsymbol{N}, \operatorname{do}\left(\boldsymbol{T}=\boldsymbol{t}^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>Note that since there is no mediator between $S$ and $R$, the $\operatorname{DCE}(S \rightarrow R)$ is also TCE of $S$ on $R$. The only adaptation that we need to make with regard to the MWPs is that it is not feasible to enumerate all possible perturbations of $S$. Therefore, the practical results that researchers can achieve are over a certain subset of $S$. In practice, we obtain this by intervening on $\boldsymbol{T}$ without affecting $\boldsymbol{O}$.</p>
<p>Causal Effects of the Operators. The ideal way to obtain the TCE of $\boldsymbol{O}$ on $R$ is through some careful human annotation that minimally changes the templates as Kaushik et al. (2020) do for sentiment classification. The challenge for MWPs in our case is that with all our possible interventions, we cannot only intervene on $\boldsymbol{O}$ without introducing changes to the irrelevant surface form. However, we might get some information about $\operatorname{TCE}(\boldsymbol{O}$ on $R)$ because, on the causal graph, the total causal influence of $\boldsymbol{T}$ on $R$ actually flows into two directed paths, one through $S$ to $R$ (which is the $\operatorname{DCE}(S \rightarrow R)$ ), and the other from $\boldsymbol{O}$ to $R$, which is our interested quantity $\operatorname{TCE}(\boldsymbol{O}$ on $R)$. Therefore, we compare the two quantities we know, $\operatorname{TCE}(\boldsymbol{T} \rightarrow R)$ and $\operatorname{DCE}(S \rightarrow R)$, to get a sense of the causal influence of $\boldsymbol{O}$ on $R$ that we cannot obtain in any other way.</p>
<h3>3.4 Step 4. Quantifying the Causal Influence</h3>
<p>Consider a realization of problem $\boldsymbol{Q}$ with operands $\boldsymbol{n}$ and ground-truth result $g=f_{\boldsymbol{o}}(\boldsymbol{n})$, and denote by $g^{\prime}$ the result after the intervention do $(X: x \rightarrow$ $\left.x^{\prime}\right)$. We quantify the causal effect of factor $X$ on the model's prediction $R$ in two ways: by assessing the change in the predicted result, and by measuring the change in the probability assigned by the model to the correct result $g$ (or $g^{\prime}$ ).
Change in the Prediction. To account for the inability of LMs to capture the continuous property of numbers (Jin et al., 2021a), we measure the change in the model's prediction using an indicator of the "change result" event:</p>
<p>$$
\delta_{\mathrm{cp}}\left(P, P^{\prime}\right):=\mathbb{1}\left(r \neq r^{\prime}\right)
$$</p>
<p>where $r=\arg \max <em _in="\in" _mathcal_I="\mathcal{I" x="x">{x \in \mathcal{I}} P(x)$, and $r^{\prime}=$ $\arg \max </em>(x)$.
Relative Change in Confidence. Inspired by Finlayson et al. (2021), we also highlight the change in terms of the relative difference in the probability assigned to $g$ and $g^{\prime}$. We formulate two types of relative change, one quantifying the relative change in the confidence of $g$, and the other quantifying the relative change in the confidence of $g^{\prime}$ :}} P^{\prime</p>
<p>$$
\begin{aligned}
&amp; \Delta_{\text {rel }}=\frac{P(g)-P^{\prime}(g)}{P^{\prime}(g)} \
&amp; \Delta_{\text {rel }}^{\prime}=\frac{P^{\prime}\left(g^{\prime}\right)-P\left(g^{\prime}\right)}{P\left(g^{\prime}\right)}
\end{aligned}
$$</p>
<p>We quantify the overall relative change in confidence (RCC) as the average of the two relative
changes above:</p>
<p>$$
\delta_{\mathrm{rcc}}\left(P, P^{\prime}\right)=\frac{1}{2}\left(\Delta_{\mathrm{rel}}+\Delta_{\mathrm{rel}}^{\prime}\right)
$$</p>
<p>A Unified Form. We are interested in the average causal effect of the intervention across all problems in $\mathcal{D}$. Thus, we measure the average of the effects over all instances $\boldsymbol{q} \in \mathcal{D}$. We denote by the subscripts $\mathrm{TCE}<em _mathrm_cp="\mathrm{cp">{\mathrm{cp}} / \mathrm{DCE}</em>}}$ and $\mathrm{TCE<em _mathrm_rec="\mathrm{rec">{\mathrm{rec}} / \mathrm{DCE}</em>$ in Section 4.2.}}$ the causal effects computed using the change in prediction metric and the relative change in confidence, respectively. We describe how we construct the dataset $\mathcal{D</p>
<h2>4 Experimental Setup</h2>
<p>In this section, we describe the data used to perform the interventions and to measure the causal effects.</p>
<h3>4.1 Datasets</h3>
<p>For our analyses, we use instances of math word problems from three popular datasets: ASDiv-A (Miao et al., 2020), MAWPS (Koncel-Kedziorski et al., 2016), and SVAMP (Patel et al., 2021). The examples contained in these collections are pairs $(\boldsymbol{t}, \boldsymbol{o})$ consisting of a question template $\boldsymbol{t}$ with its annotated operations $\boldsymbol{o}$. Each of these pairs can be instantiated multiple times into problems $\boldsymbol{q}=(\boldsymbol{t}, \boldsymbol{n})$ by filling the template with numerical values $\left(n_{1}, n_{2}, \ldots\right)$ and computing the groundtruth result $g=f_{\boldsymbol{o}}(\boldsymbol{n})$ (most problems involve two to three operands, i.e., $|\boldsymbol{n}| \in{2,3})$. We select a set of 437 two-operand and 307 three-operand template-expression pairs that we use to generate pairs of prompts representing an intervention. More details about the prompt generation procedure are in Appendix A. We use $(\boldsymbol{t}, \boldsymbol{n})$ to refer to an instantiated template that we use as a prompt.</p>
<h3>4.2 Intervention Data</h3>
<p>Given an MWP $\boldsymbol{q}=(\boldsymbol{t}, \boldsymbol{n})$ and its solution $g$, we generate a second problem-solution instance $\left(\boldsymbol{q}^{\prime}, g^{\prime}\right)$ depending on the type of causal effect CE we want to measure and on the considered variable. When intervening on the operands of the problem, the text of the problem is kept unaltered and a set of new operands $\boldsymbol{n}$ is sampled in such a way that the result $g$ is affected or not depending on the effect that is being measured. When changing the textual description of the problem, we change $\boldsymbol{t}$ such that either $\boldsymbol{o}^{\prime}=\boldsymbol{o}$, or $\boldsymbol{o}^{\prime} \neq \boldsymbol{o}$. In the former case, we sample a different template $\boldsymbol{t}^{\prime}=\left(s^{\prime}, \boldsymbol{o}\right)$ from the</p>
<p>set of templates describing the same operations $\boldsymbol{o}$, in the latter case we sample a new $\boldsymbol{t}^{\prime}$ describing a different operation. In Appendix B. 1 we report some examples of $\left(\boldsymbol{q}, \boldsymbol{q}^{\prime}\right)$ pairs representing the different types of interventions.</p>
<p>Given a model, we use the question pair $\left(\boldsymbol{q}, \boldsymbol{q}^{\prime}\right)$ to obtain a pair of answer distributions $\mathbb{P}(R \mid \boldsymbol{t}, \boldsymbol{n})$ and $\mathbb{P}\left(R \mid \boldsymbol{t}^{\prime}, \boldsymbol{n}^{\prime}\right)$, which we use to measure the causal effect of the intervention. We consider the space for the numerical values to be $\mathcal{I}=$ ${1,2, \ldots, C}$ consisting of integer values, following the setup of several existing MWP datasets (Miao et al., 2020; Koncel-Kedziorski et al., 2016; Patel et al., 2021). To control our experimental costs and make sure the models keep the number as one token, we set $C=300$. From all the tokens in a model's vocabulary, we focus on the probability assigned to the numbers in our numerical space $\mathcal{I}$, and thus we use $\mathbb{P}(R=r)$ to denote the normalized probability $\mathbb{P}<em r="1">{\text {raw }}(R=r) / Z$, where $Z=\sum</em>}^{C} \mathbb{P<em _raw="{raw" _text="\text">{\text {raw }}(R=r)$, and $\mathbb{P}</em>\right)$ pairs. Unless otherwise specified, for our experiments we generate 500 intervention pairs for each template, and results are averaged over three seeds.}}(x)$ is the raw probability score assigned to the vocabulary token $x$. For each intervention type, we generate a dataset $\mathcal{D}$ consisting of $\left(\boldsymbol{q}, \boldsymbol{q}^{\prime</p>
<h3>4.3 Models to Evaluate</h3>
<p>We use our framework to assess the robustness of reasoning in thirteen pre-trained language models. We consider five sizes of the GPT-2 model (Radford et al., 2019): distilled (Sanh et al., 2019), small, medium, large, and XL. We evaluate four models from EleutherAI that were pre-trained on the Pile (Gao et al., 2020): GPT-Neo 1.3B and 2.7B (Black et al., 2021), GPT-J-6B (Wang and Komatsuzaki, 2021), and GPT-NeoX-20B (Black et al., 2022). We use HuggingFace Transformers (Wolf et al., 2019) to access the models. Additionally, we experiment with a set of instruction-tuned versions of GPT-3 (Brown et al., 2020): Instruct (Ouyang et al., 2022), Curie, Davinci-002, and Davinci-003. ${ }^{3}$ Experiments with GPT-3 are carried out under the constraints set by the OpenAI APIS ${ }^{4}$, which prevent us from computing the causal effect using the same procedure as for the other models. We report the details about how the metrics were computed</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison of $\operatorname{DCE}(\boldsymbol{N} \rightarrow R)$ and $\operatorname{TCE}(\boldsymbol{N}$ on $R) .{ }^{<em>}$ approx values, see Appendix C.
for GPT-3 in Appendix C. In the reported results, we indicate with an asterisk (</em>) the metrics that were influenced by this limitation.</p>
<h2>5 Results</h2>
<p>Our analyses focus primarily on two-operand problems (Sections 5.1 and 5.2) and later extend to more complex problems that involve three operands (Section 5.5) for the models that perform best on the two-operand test bed. We compare the direct causal effect DCE and the total causal effect TCE of $\boldsymbol{N}$ and $\boldsymbol{T}$ on $R$. DCE represents the undesired effect for a model to being mistakenly responsive to a change in $\boldsymbol{N}$ or $\boldsymbol{T}$ not leading to a change in the result $g$ (low robustness), whereas higher values of TCE indicate a higher ability of the model to correctly adjust the probability weight assigned to the new solution $g^{\prime}$ after the intervention (high sensitivity).</p>
<h3>5.1 Effect of $N$ on $R$</h3>
<p>From the results in Figure 3, we notice that larger models exhibit a larger $\mathrm{TCE}<em _rec="{rec" _text="\text">{\text {rec }} / \mathrm{DCE}</em>\right)$, for which the models show to be affected by resultpreserving changes almost as equally as by resultaltering interventions. This behavior changes sig-}}$ ratio. In particular, in GPT-J-6B and NeoX, the TCE is, respectively, 30x and 1000x larger than the DCE. However, this improvement in sensitivity is not manifested in terms of change of prediction $\left(\delta_{\mathrm{cp}</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Heatmaps displaying $P(g)$ for Distil-GPT-2 (left), GPT-J-6B (center), and GPT-3 Davinci-002 (right). $g$ is the ground-truth result $g=n_{1}+n_{2}$ ( $n_{1}$ and $n_{2}$ are represented by the x and y axes, respectively. The probability values for each combination of $\left(\left(n_{1}, n_{2}\right), g\right)$ are averaged over 20 different templates. Probability values over 0.2 are displayed with the darkest color.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of $\operatorname{DCE}(S \rightarrow R)$ and $\operatorname{TCE}(\boldsymbol{T}$ on $R)$. We use * to denote approximated values, explained in Appendix C.
nificantly in instruction-tuned models. In particular, for the 175B-parameter GPT-3, performance varies depending on the type of supervision, with the PPOtrained Davinci-003 exhibiting an $84 \%$ difference between direct and total effect.</p>
<p>In Figure 4, we present a different visualization of the direct causal effect of $\boldsymbol{N}$ on the model's prediction. We report the heatmaps showing the probability assigned by the model to the result $g$ of a problem $\left(\boldsymbol{t},\left(n_{1}, n_{2}\right), g\right) \mid g=n_{1}+n_{2}, \forall g \in$ ${0,1, \ldots, 50}, \forall\left(n_{1}, n_{2}\right) \in{0,1, \ldots, 50}^{2}$. For Distil-GPT-2 we observe low overall probability assigned to $g$ and diagonal patterns indicating consistency in assigning higher probability to specific results (e.g., 10, 20, 30, 40, 50). For the two larger models we notice a higher probability mass as-
signed to the problem's result, but less consistency on the prediction of the same result with different sets of operands (this is true for GPT-J in particular). This result is consistent with the observed higher DCE and TCE in larger models: $P(g)$ might vary more considerably when intervening on $\boldsymbol{N}$ without affecting $g$, but overall the model assigns higher probability weight to the correct result, which correlates with higher sensitivity.</p>
<h3>5.2 Effect of $T$ on $R$</h3>
<p>In Figure 5, we report the total causal effect of the textual framing $\boldsymbol{T}$ and the direct causal effect of the irrelevant text elements $S$ on the model's prediction. For the instruction-tuned models, the improvement in terms of prediction change ( $\delta_{\mathrm{cp}}$ ) follows a similar trend as for $\boldsymbol{N}$, with GPT-3 Davinci-003 showing a $76 \%$ difference between direct and total effect. An interesting observation is that the irrelevant textual information $S$ appears to have a lower direct effect than $\boldsymbol{N}$ for all non-instruction-tuned models. However, in the GPT-3 Davinci-00x models, we observe the opposite (i.e., $\operatorname{DCE}(\boldsymbol{N} \rightarrow R) \leq \operatorname{DCE}(S \rightarrow R)$ ). This suggests that large instruction-based models tend to be more susceptible to variation in the textual framing of a problem, while smaller models are more responsive to changes in the numerical values (though not necessarily correctly).</p>
<h3>5.3 Overall Insights</h3>
<p>In comparison to other models, GPT-3 Davinci shows the highest $\mathrm{DCE}<em _mathrm_cp="\mathrm{cp">{\text {rcc }}$, but low $\mathrm{DCE}</em>(g)$ might vary significantly.}}$. This discrepancy is related to the quantities that the two metrics consider. $\delta_{\text {rcc }}$ takes into account the probability assigned to $g$, while $\delta_{\mathrm{cp}}$ does not consider the ground truth solution. One interpretation of this result is that GPT-3 Davinci consistently predicts the same answer $r=r^{\prime}$ when $g=g^{\prime}$, but the probabilities $P(g)$ and $P^{\prime</p>
<p>The results observed for the two kinds of intervention $\operatorname{do}\left(\boldsymbol{T}: \boldsymbol{t} \rightarrow \boldsymbol{t}^{\prime}\right)$ and $\operatorname{do}\left(\boldsymbol{N}:\left(n_{1}, n_{2}\right) \rightarrow\right.$ $\left.\left(n_{1}^{\prime}, n_{2}^{\prime}\right)\right)$ show similar trends. Small models (Distilled and Small GPT-2) exhibit low sensitivity to interventions. Larger models (from GPT-2 Medium to GPT-Neo) appear to be more influenced by changes in both $\boldsymbol{N}$ and $\boldsymbol{T}$. However, they display similar sensitivity to both result-altering and resultpreserving interventions. An improvement in sensitivity is noticeable in GPT-J and NeoX, though not accompanied by an improvement in robustness. Remarkably different behavior is instead shown by the GPT-3 Davinci models, which demonstrate substantially higher sensitivity to result-altering interventions (high TCE), and higher robustness (in terms of prediction change). In Appendix B.2, we report the accuracy of the models on the generated instances of MWPs, which exhibits a similar trend as the robustness/sensitivity changes we observed.</p>
<p>Possible explanations for the improved robustness and sensitivity demonstrated by the large GPT3 models might be the dramatic size increase and extension/enhancement of the training procedure involving instructions. The former idea is aligned with the emergent abilities hypothesis (Wei et al., 2022a), which postulates the existence of skills that are displayed by large-scale models but are not present in smaller-scale models. However, our observations show different performances in versions of GPT-3 Davinci that differ in the training procedure. ${ }^{5}$ This raises the question of whether the capability of LLMs to reason about math problems benefits from instruction-based tuning. We address this question in the following section.</p>
<h3>5.4 Extending to LLaMA-Based Models</h3>
<p>To further investigate the roles played by size and training method in the model's performance, we carry out our experimental procedure on three versions with different sizes (7B, 13B, and 30B) of the LLaMA model (Touvron et al., 2023), and on Stanford Alpaca (which applies instruction tuning on LLaMA 7B) (Taori et al., 2023). We present these results separately, as the LLaMA tokenization makes the prediction setup different from the one used from the other models, and prevents us from computing the relative change in confidence</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison of direct and total effects of $N$ on $R$ for LLaMA and Alpaca.
$\left(\delta_{\text {rcc }}\right) .{ }^{6}$
From the results (Figure 6), two notable observations emerge. Firstly, the increased difference between TCE and DCE observed with the increasing size of the LLaMA models suggests that a larger number of parameters can be a significant driver behind robustness/sensitivity improvement. However, this is not necessarily the case across different models: GPT-NeoX-20B shows a smaller $\mathrm{TCE}<em _mathrm_cp="\mathrm{cp">{\mathrm{cp}}-\mathrm{DCE}</em>$ gap compared to LLaMA 7B ( $5.2 \%$ vs $9.0 \%$ ). Secondly, the instruction tuning procedure of Alpaca does not seem to help significantly with mathematical computation: the decrease in both TCE and DCE shows that robustness improves at the expense of sensitivity. Nonetheless, overall, when comparing Alpaca compared to its base model, LLaMA 7B, we observe an increase in the gap between TCE and DCE, although this difference is minimal ( $9.5 \%$ vs $9.0 \%$ ).}</p>
<p>The limited improvement of Alpaca might be attributed to its instruction tuning procedure consisting of "a list of user-oriented instructions including email writing, social media, and productivity tools" (Taori et al., 2023), which differs from reasoningintensive tasks. We suggest future work to examine different types of instruction tuning (e.g., focused on reasoning procedures or reinforcement learning from human feedback), which might help the model answer more complex types of questions in a step-by-step manner and more accurately. We hypothesize that the different performances in versions of GPT-3 Davinci might be produced by the specific type of instructions used for training, by the reinforcement learning component (Ouyang et al., 2022), or simply by an extension of the language modeling pre-training. It is challenging to</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparison of direct and total effects of $\boldsymbol{N}$ on $R$ for three-operand problems.
pinpoint the exact factor in the training procedure that contributes to this improvement, as specific methodological details are not available.</p>
<h3>5.5 Moving to Three-Operand Problems</h3>
<p>We extend our evaluation to consider the threeoperand problems in the dataset. In these experiments, we consider only the GPT-3 175Bparameter models, as they are the only models performing well on the simpler bivariate problems. The results regarding the effects of $\boldsymbol{N}$ are reported in Figure 7. We notice that the large difference between the desired (TCE) and undesired (DCE) effects observed on simpler problems shrinks significantly for both metrics. In particular, for Davinci003, the direct effect of $\boldsymbol{N}$ (measured as $\delta_{\mathrm{cp}}$ ) grows from 0.17 to 0.87 . That is, GPT-3 Davinci-003 predicts a different result $87 \%$ of the time after an intervention that does not affect the ground-truth solution. The increase in direct effect indicates a performance degradation in terms of brittleness: even the models that show good performance on two-operand problems, now display an unstable behavior after result-preserving interventions.</p>
<h2>6 Related Work</h2>
<p>Causal NLP. Causal inference aims to study the cause and effect from observational and interventional data (Pearl, 2009; Peters et al., 2017). Traditionally, researchers usually apply causal techniques to phenomena in nature and human society. With the rise of powerful models in NLP, recent research has started to explore the intersection of causal inference and NLP, forming the study of Causal NLP (Jin et al., 2022; Feder et al., 2021a).</p>
<p>There are several formulations for Causal NLP: the causality for NLP thread involves using the causal framework for data collection and task for-
mulation (Jin et al., 2021c), inspecting the (pathspecific) causal effect of certain neurons on predictions (Vig et al., 2020; Meng et al., 2022), understanding the causal effect of data and learning paradigm for model performance ( Ni et al., 2022), and as a way to frame prompts (Lyu et al., 2023); and NLP for causality involves testing the pure causal inference skills of LLMs (Jin et al., 2023a,b), and use text as a variable for causal effect estimation (Roberts et al., 2020; Veitch et al., 2020; Jin et al., 2021b, 2023c).</p>
<p>The most similar line of research to our work is the application of causal effect estimation on interpreting models' behavior, such as how models understand syntactic agreement (Finlayson et al., 2021), and how interventions in the representations and weights affect the model prediction (Feder et al., 2021b). To the best of our knowledge, our work is the first to formulate a causal framework for robustness behavioral tests, and also we are the first to introduce the idea to quantify the differences in the causal mechanisms of human reasoning and model decisions.</p>
<p>Math Reasoning in NLP. A growing body of work tries to improve the math reasoning capability in NLP models (Zhang et al., 2020; Geva et al., 2020; Spokoyny et al., 2021), and prompting techniques for LLMs (Cobbe et al., 2021; Shen et al., 2021; Kojima et al., 2022; Wei et al., 2022b; Chowdhery et al., 2022). For analysis, significant attention has been given to models' ability to understand numerical quantities (Wallace et al., 2019; Thawani et al., 2021) and numerical operations (Pal and Baral, 2021; Berg-Kirkpatrick and Spokoyny, 2020; Piękos et al., 2021; Razeghi et al., 2022).</p>
<h2>7 Conclusion</h2>
<p>We developed a framework to disentangle and separately measure the effect of different factors influencing the predictions of LLMs for math reasoning. Our results indicate that a drastic increase in both robustness and sensitivity emerges in the GPT-3 Davinci models. Additionally, we study the contribution of size and instruction tuning in the models of the LLaMA family, observing that the Alpaca instruction tuning, while increasing the model's robustness, does not significantly improve the overall performance. Our framework provides a formalized theory of behavioral testing for math reasoning models and opens new future directions to design behavioral tests of models in a principled way.</p>
<h2>Ethical Considerations</h2>
<p>As for the ethical practice in this work, the data involved are from existing MWP datasets with no private user information, and available under the MIT license. As for the ethical impact of the use of this work, the study is about providing a metric and analyzing existing models' robustness, so there is less concern over harmful usage. Rather, it is more about putting checks on existing AI models and helping humans understand them better before use. Potential stakeholders that could benefit from this research include NLP researchers working on math models, practitioners working on various applications involving mathematical reasoning with text, and e-learning design.</p>
<h2>Limitations</h2>
<p>A key limitation in our work is that LLMs might have seen these math problems. Our work theoretically assumes this is not the case. Another limitation is that for the sake of simplicity, our work makes some assumptions. For example, we assume all numbers in the range of integers 0 to $C=300$. This would not cover every MWP out there. And future work is needed to generalize our framework to other forms of MWPs. In this work, we are also constrained by the limitations of the OpenAI policy on the GPT-3 API. This limits the number of perturbations we consider in this work as well as the accuracy with which we can estimate our causal distributions. Finally, our work is restricted to English, and extending it to other languages will require us to create an MWP dataset in that language.</p>
<h2>Acknowledgments</h2>
<p>This material is based in part upon works supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039B; by the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645; by the John Templeton Foundation (grant #61156); by a Responsible AI grant by the Haslerstiftung; and an ETH Grant (ETH-19 21-1). Alessandro Stolfo is supported by armasuisse Science and Technology through a CYD Doctoral Fellowship. Zhijing Jin is supported by PhD fellowships from the Future of Life Institute and Open Philanthropy, as well as the travel support from ELISE (GA no 951847) for the ELLIS pro-
gram. We also thank OpenAI Researcher Access Program for granting our team credits to their API.</p>
<h2>References</h2>
<p>Taylor Berg-Kirkpatrick and Daniel Spokoyny. 2020. An empirical investigation of contextualized number prediction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4754-4764, Online. Association for Computational Linguistics. 9</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. GPT-NeoX-20B: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745. 6</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata. 6</p>
<p>Elizabeth M. Brannon. 2005. The independence of language and mathematical reasoning. Proceedings of the National Academy of Sciences, 102(9):31773178. 3</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc. 1,6</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. 1, 9</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. 1, 9</p>
<p>Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brando n M. Stewart, Victor Veitch, and Diyi Yang. 2021a. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. CoRR, abs/2109.00725. 9</p>
<p>Amir Feder, Nadav Oved, Uri Shalit, and Roi Reichart. 2021b. CausaLM: Causal model explanation through counterfactual language models. Computational Linguistics, 47(2):333-386. 9</p>
<p>Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan Belinkov. 2021. Causal analysis of syntactic agreement mechanisms in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1828-1843, Online. Association for Computational Linguistics. 2, 5, 9</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. 6</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946-958, Online. Association for Computational Linguistics. 9</p>
<p>Zhihua Jin, Xin Jiang, Xingbo Wang, Qun Liu, Yong Wang, Xiaozhe Ren, and Huamin Qu. 2021a. Numgpt: Improving numeracy ability of generative pre-trained models. arXiv preprint arXiv:2109.03137. 5</p>
<p>Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, and Bernhard Schoelkopf. 2023a. Cladder: Assessing causal reasoning in language models. 9</p>
<p>Zhijing Jin, Amir Feder, and Kun Zhang. 2022. CausaNLP tutorial: An introduction to causality for natural language processing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 1722, Abu Dubai, UAE. Association for Computational Linguistics. 9</p>
<p>Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab, and Bernhard Schoelkopf. 2023b. Can large language models infer causation from correlation? 9</p>
<p>Zhijing Jin, Zhiheng Lyu, Yiwen Ding, Mrinmaya Sachan, Kun Zhang, Rada Mihalcea, and Bernhard Schoelkopf. 2023c. AI Scholars: A dataset for NLPinvolved causal inference. 9</p>
<p>Zhijing Jin, Zeyu Peng, Tejas Vaidhya, Bernhard Schoelkopf, and Rada Mihalcea. 2021b. Mining the cause of political decision-making from social media: A case study of COVID-19 policies across the US
states. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 288-301, Punta Cana, Dominican Republic. Association for Computational Linguistics. 9</p>
<p>Zhijing Jin, Julius von Kügelgen, Jingwei Ni, Tejas Vaidhya, Ayush Kaushal, Mrinmaya Sachan, and Bernhard Schoelkopf. 2021c. Causal direction of data collection matters: Implications of causal and anticausal learning for NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9499-9513, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. 9</p>
<p>Divyansh Kaushik, Eduard H. Hovy, and Zachary Chase Lipton. 2020. Learning the difference that makes A difference with counterfactually-augmented data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. 5</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916. 9</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics. 5, 6</p>
<p>Zhiheng Lyu, Zhijing Jin, Justus Mattern, Rada Mihalcea, Mrinmaya Sachan, and Bernhard Schölkopf. 2023. Psychologically-inspired causal prompts. CoRR, abs/2305.01764. 9</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35:17359-17372. 2, 9</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, Online. Association for Computational Linguistics. 5, 6</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. 2022a. Lila: A unified benchmark for mathematical reasoning. arXiv preprint arXiv:2210.17517. 3</p>
<p>Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. 2022b. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In Proceedings of the 60th Annual Meeting of</p>
<p>the Association for Computational Linguistics (Volume 1: Long Papers), pages 3505-3523, Dublin, Ireland. Association for Computational Linguistics. 1</p>
<p>Martin M Monti, Lawrence M Parsons, and Daniel N Osherson. 2012. Thought beyond language: Neural dissociation of algebra and natural language. Psychological science, 23(8):914-922. 3</p>
<p>Jingwei Ni, Zhijing Jin, Markus Freitag, Mrinmaya Sachan, and Bernhard Schölkopf. 2022. Original or translated? A causal analysis of the impact of translationese on machine translation performance. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5303-5320, Seattle, United States. Association for Computational Linguistics. 9</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155. 1, 2, 6, 8</p>
<p>Kuntal Kumar Pal and Chitta Baral. 2021. Investigating numeracy learning ability of a text-to-text transfer model. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3095-3101, Punta Cana, Dominican Republic. Association for Computational Linguistics. 9</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics. 1, 2, 5, 6, 14</p>
<p>Judea Pearl. 1995. Causal diagrams for empirical research. Biometrika, 82(4):669-688. 2, 3, 4</p>
<p>Judea Pearl. 2001. Direct and indirect effects. In UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University of Washington, Seattle, Washington, USA, August 2-5, 2001, pages 411-420. Morgan Kaufmann. 2, 4</p>
<p>Judea Pearl. 2009. Causality. Cambridge University Press. 9</p>
<p>Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2017. Elements of causal inference: Foundations and learning algorithms. The MIT Press. 9</p>
<p>Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational</p>
<p>Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 383-394, Online. Association for Computational Linguistics. 9</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. 6</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206. 1, 9</p>
<p>Margaret E Roberts, Brandon M Stewart, and Richard A Nielsen. 2020. Adjusting for confounding with text matching. American Journal of Political Science, 64(4):887-903. 9</p>
<p>Mrinmaya Sachan, Kumar Dubey, and Eric Xing. 2017. From textbooks to knowledge: A case study in harvesting axiomatic knowledge from textbooks to solve geometry problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 773-784. 1</p>
<p>Mrinmaya Sachan, Kumar Avinava Dubey, Tom M Mitchell, Dan Roth, and Eric P Xing. 2018. Learning pipelines with limited data and domain knowledge: A study in parsing physics problems. Advances in Neural Information Processing Systems, 31. 1</p>
<p>Mrinmaya Sachan and Eric Xing. 2017. Learning to solve geometry problems from natural language demonstrations in textbooks. In Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 251-261. 1</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. In NeurIPS EMC ${ }^{2}$ Workshop. 6</p>
<p>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. 2015. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1466-1476, Lisbon, Portugal. Association for Computational Linguistics. 1</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021. Generate \&amp; rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034. 9</p>
<p>Daniel Spokoyny, Ivan Lee, Zhao Jin, and Taylor BergKirkpatrick. 2021. Masked measurement prediction: Learning to jointly predict quantities and units from textual context. arXiv preprint arXiv:2112.08616. 9</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca. 2, 8</p>
<p>Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely. 2021. Representing numbers in NLP: a survey and a vision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644-656, Online. Association for Computational Linguistics. 9</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. 2, 8</p>
<p>Victor Veitch, Dhanya Sridhar, and David M. Blei. 2020. Adapting text embeddings for causal inference. In Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI 2020, virtual online, August 3-6, 2020, volume 124 of Proceedings of Machine Learning Research, pages 919-928. AUAI Press. 9</p>
<p>Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. Advances in Neural Information Processing Systems, 33:1238812401. 2, 9</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5307-5315, Hong Kong, China. Association for Computational Linguistics. 9</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 billion parameter autoregressive language model. 6</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682. 1, 8</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903. 9</p>
<p>Sean Welleck, Peter West, Jize Cao, and Yejin Choi. 2022. Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8629-8637. 1</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771. 6</p>
<p>Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4889-4896, Online. Association for Computational Linguistics. 9</p>
<h2>A Creation of the Prompts</h2>
<p>We consider MWP examples from the union of the three datasets SVAMP, ASDiv-A, and MAWPS. The textual template $\boldsymbol{t}$ of a problem consists of a context (describing a real-world state and/or actions) and a question. In order to obtain suitable prompts for the models, we convert the problems' questions into statements where the result of the problem is expected to be the first token after the prompt. E.g., in the example in section 2, how many trees will he have? is converted into the number of trees that he will have is ... From the MWP templates of the SVAMP/ASDiv-A/MAWPS collection (we consider all splits), we filter out the templates whose questions do not start with How many..., and we use spaCy $^{7}$ to identify the subject, the object and the verbs in the sentence. This allows us to convert the last sentence of the template from The number of... is. This way, we obtain 437 statement-based MWP templates for two-operand problems and 307 for three-operand problems. We manually checked a subset of the templates to identify possible mistakes in the conversion procedure.</p>
<h2>B Frequently Asked Questions</h2>
<h2>B. 1 How do the intervention data look like?</h2>
<p>In Table 1 we report examples of MWP pairs representing different types of intervention.</p>
<h2>B. 2 What is the accuracy of the evaluated models on the generated problems?</h2>
<p>We report the accuracy of the models considered for evaluation in terms of accuracy at 1 and accuracy at 10. Results are displayed in Figure 8.</p>
<h2>B. 3 What is the relation between accuracy and the RCC metric?</h2>
<p>We examine the relationship between performance and robustness, computing the Pearson correlation coefficient between accuracy (accuracy@10) and the relative confidence change (RCC) metric. On a per-template basis ( 500 instances for each template), we found accuracy to be positively</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\operatorname{TCE}(\boldsymbol{N} \rightarrow R)$</th>
<th style="text-align: center;">Ruby has 87 candies. If she shares the candies among 29 friends, the number of candies that each friend gets is</th>
<th style="text-align: center;">$g=87 / 29=3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ruby has 35 candies. If she shares the candies among 5 friends, the number of candies that each friend gets is</td>
<td style="text-align: center;">$g=35 / 5=7$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{DCE}(\boldsymbol{N} \rightarrow R)$</td>
<td style="text-align: center;">The school is composed of 13 buildings each having 10 classrooms. The number of classrooms that the school has is</td>
<td style="text-align: center;">$g=10 \times 13=130$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The school is composed of 65 buildings each having 2 classrooms. The number of classrooms that the school has is</td>
<td style="text-align: center;">$g=65 \times 2=130$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{DCE}(S \rightarrow R)$</td>
<td style="text-align: center;">The razorback t-shirt shop ordered 6 cases of t-shirts. If each case contains 17 t-shirts the number of t-shirts that they ordered is</td>
<td style="text-align: center;">$g=17 \times 6=102$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The roller coaster at the state fair costs 6 tickets per ride. If 17 friends were going to ride the roller coaster the number of tickets that they would need is</td>
<td style="text-align: center;">$g=17 \times 6=102$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{TCE}(\boldsymbol{T} \rightarrow R)$</td>
<td style="text-align: center;">Sean has 23 whistles. He has 6 more whistles than Charles. The number of whistles that Charles has is</td>
<td style="text-align: center;">$g=23-6=17$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jovana filled her bucket with 23 pounds of shells. If she adds 6 more pounds of shell to fill her bucket, the number of pounds that she has is</td>
<td style="text-align: center;">$g=23+6=29$</td>
</tr>
</tbody>
</table>
<p>Table 1: For each of the causal effects measured (left column), we report a pair of MWPs illustrating the intervention performed (center), along with their respective ground-truth result (left column).
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Average accuracy of the models on the generated instances of MWPs. Results are averaged over two sets consisting of 500 problem instances generated for each template. The lower figure shows a zoomed-in visualization of the accuracy at 1 .
correlated with $\operatorname{TCE}(\boldsymbol{N}$ on $R)$ and $\operatorname{TCE}(T$ on $R)$ ( 0.24 and 0.49 , respectively) and negatively correlated with $\operatorname{DCE}(\boldsymbol{N} \rightarrow R)$ and $\operatorname{DCE}(S \rightarrow R)$ $(-0.26$ and -0.36 , respectively). We see these results as a quantitative validation of the intuition behind our framework: the better the model's performance, the more the model tends to correctly adjust its prediction after a result-altering intervention (higher sensitivity) and to correctly not change its prediction after a result-preserving intervention (higher robustness).</p>
<p>Moreover, we conduct an additional sanity check as in Patel et al. (2021): removing the question from the MWP templates, we observe a sensitivityrobustness degradation to random guessing (i.e., $\mathrm{TCE} \simeq \mathrm{DCE}$ ). This indicates that the measurement of the causal effects within our framework is not affected by patterns in the templates that might have been picked up or memorized by large models.</p>
<h2>C Computation of Causal Effects for GPT-3</h2>
<p>We access GPT-3 through the OpenAI APIs, which allow a user to prompt the model and obtain the probabilities assigned by the model to the $k$-th most likely vocabulary entries, for each token generated. To overcome this limitation, we approximate the</p>
<p>relative probability change $\delta_{\text {rcc }}$ as follows, depending on the kind of effect measured.</p>
<p>The limit for $k$ is set by OpenAI to 5. However, for our main set of experiments (i.e., computing the causal effects of $\boldsymbol{N}, S$, and $\boldsymbol{T}$ ) we were granted an increased limit of $k$ to 100 . This allowed us to obtain reasonable estimates for the causal effects, as the number of cases in which $P(g)$ is not defined is less than $10 \%$ of the number of examples that we consider.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Computation of \(\delta_{\text {rcc }}\) for GPT-3
\(\overline{\boldsymbol{Q}}=(\boldsymbol{t}, \boldsymbol{n}, \bar{g})\)
\(Q^{\prime}=\left(t^{\prime}, \boldsymbol{n}^{\prime}, g^{\prime}\right)\)
if \(P(g)\) is defined then
    if \(P^{\prime}(g)\) is defined then
        \(\Delta=\frac{P(g)-P^{\prime}(g)}{P^{\prime}(g)}\)
    else
        \(\hat{P}^{\prime} \leftarrow P^{\prime}(k\)-th most likely token)
        \(\Delta=\frac{P(g)-\hat{P}^{\prime}}{\hat{P}^{\prime}}\)
    end
else
    \(\Delta=0\)
end
if \(P^{\prime}\left(g^{\prime}\right)\) is defined then
    if \(P\left(g^{\prime}\right)\) is defined then
        \(\Delta^{\prime}=\frac{P^{\prime}\left(g^{\prime}\right)-P\left(g^{\prime}\right)}{P\left(g^{\prime}\right)}\)
    else
        \(\hat{P} \leftarrow P(k\)-th most likely token)
        \(\Delta^{\prime}=\frac{P^{\prime}\left(g^{\prime}\right)-\hat{P}}{\hat{P}}\)
    end
else
    \(\Delta^{\prime}=0\)
end
\(\delta_{\mathrm{rcc}}=\frac{1}{2}\left(\Delta+\Delta^{\prime}\right)\)
</code></pre></div>

<h2>C. $1 \operatorname{TCE}(\boldsymbol{N}$ on $R)$ and $\operatorname{TCE}(\boldsymbol{T}$ on $R)$</h2>
<p>In cases when $P(g)$ is defined (i.e. when $g$ appears in the top $k$ token predictions) and $P^{\prime}(g)$ is not defined, we compute a lower bound on the relative change using the upper bound on $P^{\prime}(g)$ given by the probability of the $k$-th most likely token. This gives us a conservative estimate of $\Delta$. For cases in which $P(g)$ is not defined, we cannot say anything about the relative change, and we set $\Delta=0$. The same applies when swapping $P$ and $P^{\prime}$. This procedure is illustrated by Algorithm 1.
C. $2 \operatorname{DCE}(\boldsymbol{N} \rightarrow R)$ and $\operatorname{DCE}(S \rightarrow R)$</p>
<p>In this case, we simply discard the examples for which $P(g)$ is not defined or $P^{\prime}(g)$ are not defined. In that is not the case, then we compute $\delta_{\text {rcc }}$ as in Section 3.4.</p>
<h2>C. 3 Heatmap Illustration</h2>
<p>The heatmap for GPT-3 displayed in Figure 4 was computed by taking the raw probability score produced by the model over the whole vocabulary, as the limit on the available top predicted tokens makes it impossible to normalize it over the set ${0, \ldots, 300}$, as done for the other models. The probability was set to 0 when $g$ did not appear in the model's top 5 predictions for the next token after the prompt.</p>
<h2>D Computing Infrastructure \&amp; Inference Details</h2>
<p>To run our experiments, we used a single NVIDIA TITANRTX with 24GB of memory for all the versions of GPT-2 and GPT-Neo. We used a single NVIDIA A100 with 40GB of memory for GPT-J6B and a single NVIDIA A100 with 80GB of memory for GPT-NeoX and the LLaMA models (two for the 30B version). We accessed GPT-3 using the OpenAI APIs. The longest run (GPT-J) on the four kinds of experiments corresponding to the four kinds of effects measured took $\sim 12$ hours, using 500 MWP instances for each of the 437 templates. Due to budget and resource constraints, the experiments on GPT-3, GPT-NeoX, and LLaMA were carried out using 20 examples generated for each template and took $\sim 7$ hours. Experiment tracking was carried out using Weights \&amp; Biases ${ }^{8}$.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>A For every submission:</h1>
<p>A1. Did you describe the limitations of your work?
Section "Limitations".
$\checkmark$ A2. Did you discuss any potential risks of your work?
Section "Ethical Considerations".
$\checkmark$ A3. Do the abstract and introduction summarize the paper's main claims?
Section 1: Introduction.
$\mathscr{H}$ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B Did you use or create scientific artifacts?</h2>
<h2>Section 4</h2>
<p>$\checkmark$ B1. Did you cite the creators of artifacts you used?
Section 4.1
$\checkmark$ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section "Limitations"
$\checkmark$ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Section "Limitations"
$\checkmark$ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
Section "Limitations"
$\checkmark$ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Section "Limitations"
$\checkmark$ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Section 4.1 and Appendix A</p>
<h2>C Did you run computational experiments?</h2>
<p>Sections 4 and 5
$\checkmark$ C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>Sections 4.3 and Appendix D
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
Sections 4.3
C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
Sections 4.1, 4.2, and 5
C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
Section 4.3 and Appendix A
D Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank.</p>
<p>D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
No response.
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
No response.
D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
No response.
D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
No response.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ http://wandb.ai/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ The LLaMA tokenizer considers each digit as an independent token in the vocabulary. This makes it problematic to compare the probability value assigned by the model to multi-digit numbers.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8295 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8295</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8295</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-276885458</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.05226v1.pdf" target="_blank">Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments</a></p>
                <p><strong>Paper Abstract:</strong> Monte Carlo Tree Search (MCTS) has emerged as a powerful tool for decision-making in robotics, enabling efficient exploration of large search spaces. However, traditional MCTS methods struggle in environments characterized by high uncertainty and noisy data due to their reliance on final-step reward evaluation. The lack of intermediate feedback during search often results in suboptimal decision-making and computational inefficiencies. This paper introduces Reward-Centered ReST-MCTS, a novel framework that enhances MCTS by incorporating intermediate reward shaping. The core of our approach is the Rewarding Center, which refines search trajectories by dynamically assigning partial rewards using rule-based validation, heuristic guidance, and neural estimation. By integrating these mechanisms, our method enables real-time optimization of search paths, mitigating the effects of error propagation. We evaluate Reward-Centered ReST-MCTS in robotic manipulation tasks under high uncertainty, demonstrating consistent improvements in decision accuracy. Compared to baseline methods, including Chain-of-Thought (CoT) prompting and Vanilla ReST-MCTS, our framework achieves a 2-4% accuracy improvement while maintaining computational feasibility. Ablation studies confirm the effectiveness of intermediate feedback in search refinement, particularly in pruning incorrect decision paths early. Furthermore, robustness tests show that our method retains high performance across varying levels of uncertainty.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8295.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8295.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique for large language models that elicits multi-step, explicit intermediate reasoning by decomposing complex problems into chains of logical steps; described in the paper as a single-pass reasoning method where intermediate errors persist.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM baseline (CoT applied as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language model(s) used with Chain-of-Thought prompting; the paper references LLaMA-3-8B and Mistral-7B as base models elsewhere but does not explicitly attribute the standalone CoT baseline to a single model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT) single-pass decomposition']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompts the LLM to produce an explicit sequence of intermediate reasoning steps (a 'chain') before producing the final answer; implemented as single-pass generation in which the chain is produced once and the final answer follows, so mistakes in intermediate steps propagate to the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>CoT is included as part of the CoT + Self-Consistency baseline; the paper characterizes CoT as a single-pass method and compares it (often in combination with self-consistency) against search-augmented and ReST-MCTS approaches, but does not run an isolated ablation comparing CoT-only vs multi-path methods in a controlled study.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MATH (500 advanced math problems), GPQA Diamond (300 factoid tasks), CEval-Hard (1,000 Chinese MCQs), Sci QA (200 expert-level scientific questions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CoT alone not reported separately; CoT combined with self-consistency baseline (see Self-Consistency entry) reported per-benchmark results in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper notes CoT provides structured reasoning but as a single-pass method its intermediate errors persist and there is no mechanism to dynamically refine chains during execution; thus CoT lacks real-time search adjustments required for noisy/high-uncertainty decision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Chain-of-Thought improves multi-step reasoning structure but, when used alone, remains single-pass and therefore susceptible to propagated intermediate errors; CoT lacks dynamic, real-time adjustment to refine or prune reasoning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8295.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8295.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency for Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that samples multiple reasoning chains from an LLM and selects the most consistent final answer across these diverse chains to improve reliability at the cost of increased computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM baseline (CoT + Self-Consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language model(s) used with stochastic decoding to generate multiple CoT chains; paper includes this as a baseline approach though it does not fix a single underlying model for the baseline explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['self-consistency (multi-chain sampling and majority/consensus selection)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate multiple independent CoT reasoning traces by sampling, then aggregate final answers (e.g., majority voting or consistency scoring) to select the most consistent solution; implemented as a baseline (CoT + Self-Consistency) in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as a baseline called 'CoT + Self-Consistency' and compared against LLaMA+Beam, Vanilla MCTS, Vanilla ReST-MCTS, and Reward-Centered ReST-MCTS in accuracy and runtime; no dedicated ablation isolates sampling diversity vs single-run CoT, but the baseline provides an implicit diverse-method comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MATH, GPQA Diamond, CEval-Hard, Sci QA (same benchmarks as main evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported accuracy (mean ± std) over 3 runs per benchmark: MATH 30.0 ± 0.8; GPQA 31.3 ± 0.6; CEval-Hard 25.7 ± 0.9; Sci QA 29.0 ± 1.1.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper states self-consistency increases reliability by considering multiple reasoning paths but substantially increases computational cost; still lacks real-time trajectory optimization (post-hoc aggregation rather than dynamic pruning during search).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency (sampling diverse reasoning traces) improves robustness over single-pass CoT but is computationally expensive and does not provide the real-time search adjustments that Reward-Centered ReST-MCTS offers; thus it is less suitable for online robotic decision-making under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8295.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8295.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Memory-based Self-Correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent approach that iteratively refines decisions by recording past failures and applying memory-based self-correction (verbal reinforcement learning), positioned as a post-hoc correction method rather than a real-time search refiner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reflexion (method applied to LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An agent-level technique for LLMs that stores and reuses corrective memory traces to iteratively improve future outputs; discussed in related work but not used as an experimental baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['iterative memory-based self-correction (post-hoc refinement)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>After producing an initial output, the agent records failures and applies corrective reasoning in subsequent attempts (memory-based corrections), effectively a post-hoc iterative refinement loop rather than dynamic tree-search adjustments during a single decision process.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (iterative multistage corrections but not explicitly multi-path sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned as a hybrid/post-hoc correction approach in related work; not included in experimental comparisons or ablation studies in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Not experimentally evaluated in this paper (mentioned as related work applicable to decision-making tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No performance numbers provided in this paper for Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper contrasts Reflexion's post-hoc correction with the proposed Reward-Centered ReST-MCTS, noting Reflexion refines outputs after an initial attempt whereas ReST-MCTS provides structured intermediate rewards for real-time pruning and trajectory optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Reflexion exemplifies post-hoc self-correction strategies; the paper argues such approaches do not provide the real-time, structured intermediate feedback necessary for dynamic search refinement in high-uncertainty robotic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8295.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8295.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA+Beam</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3-8B with Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that uses the LLaMA-3-8B generative model with beam search decoding to generate multiple candidate outputs (tokens/sequences) and select the highest-scoring candidate; used in experiments to compare token-level beam search against structured tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLaMA: Open and Efficient Foundation Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B + Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-3-8B is a large language model used here with heuristic beam search decoding (beam search width not specified) to produce multiple candidate answers; included as an experimental baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['beam search decoding (multiple candidate generation)', 'autoregressive token sampling']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Beam search keeps multiple top-scoring token sequences during autoregressive generation to produce several candidate outputs, then selects the best-scoring candidate according to model scores; this provides some diversity at the token/sequential level but lacks structured tree-search for long-horizon decision optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (multiple token-sequence candidates but not structured multi-path reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Included as a baseline for runtime-accuracy trade-off comparisons against MCTS-based methods (Vanilla MCTS, Vanilla ReST-MCTS, Reward-Centered ReST-MCTS); reported per-benchmark accuracy and runtime characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MATH, GPQA Diamond, CEval-Hard, Sci QA (same evaluation suite)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported accuracy (mean ± std) over 3 runs per benchmark: MATH 32.2 ± 0.5; GPQA 26.0 ± 0.7; CEval-Hard 28.0 ± 0.8; Sci QA 29.1 ± 0.9. Noted to incur significant computational overhead due to autoregressive sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Beam search offers strong per-query accuracy in some benchmarks but is less effective for long-horizon/adaptive decision-making because it lacks structured tree search and real-time intermediate feedback; paper notes LLaMA+Beam has competitive accuracy but higher runtime overhead compared to MCTS variants.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Beam search improves token-level candidate quality but cannot replace structured tree-based search with intermediate reward shaping for dynamic trajectory optimization; Reward-Centered ReST-MCTS outperforms LLaMA+Beam on the reported benchmarks while maintaining a reasonable runtime increase (~35% over Vanilla MCTS).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8295.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8295.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReST-MCTS (Reward-Centered)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward-Centered ReST-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed framework that augments Monte Carlo Tree Search with structured intermediate reward shaping via a Rewarding Center (rule-based validation, heuristic feedback, and neural estimation) to provide real-time pruning and trajectory refinement, and which leverages LLMs (LLaMA-3-8B, Mistral-7B) for neural value estimation components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reward-Centered ReST-MCTS (uses LLaMA-3-8B and Mistral-7B as base models for neural estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hybrid decision-making framework: MCTS core (modified UCB selection) combined with a Rewarding Center that computes intermediate rewards R_c(s) = αR_rule + βR_heuristic + γR_neural; neural component uses learned value estimation (implemented with LLaMA-3-8B and Mistral-7B as base models per experimental setup).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['search-augmented reasoning via MCTS', 'intermediate reward shaping (rule-based, heuristic, neural value estimation)', 'integration of LLM-based neural value estimation with tree search']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The framework performs structured tree search (MCTS) with a modified selection probability that includes intermediate rewards; the Rewarding Center assigns partial rewards using rule-based checks, domain heuristics, and neural estimators (LLM-derived value functions), enabling real-time pruning and prioritization of high-value branches during search.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared against multiple baselines that exemplify single-pass (CoT), multi-path sampling (self-consistency), beam search, and vanilla MCTS; includes an ablation study removing Rewarding Center components (no rule-based checks, no heuristics, no neural estimation) showing relative contributions but does not perform a dedicated ablation isolating 'diversity of reasoning methods' beyond baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MATH, GPQA Diamond, CEval-Hard, Sci QA (benchmarks) plus real-world robotic datasets DexHand-500 and Indust-200</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported accuracy (mean ± std) over 3 runs per benchmark: MATH 33.5 ± 0.9; GPQA 27.2 ± 0.8; CEval-Hard 29.4 ± 0.9; Sci QA 30.1 ± 1.0. Paper additionally claims a 2–4% accuracy improvement over Vanilla MCTS and Vanilla ReST-MCTS and states the method retains >85% accuracy under high uncertainty. Runtime: ~35% increase over Vanilla MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Integrating multiple intermediate-feedback mechanisms (rule, heuristic, neural) allows dynamic pruning of infeasible/low-value branches, reduces error propagation, and yields more robust decisions under noise; ablation shows neural estimation contributes most, then rule-based validation, then heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining multiple reasoning/feedback sources (diverse intermediate methods) within a search-augmented framework yields better accuracy and robustness than single-method LLM reasoning (CoT) or multi-sample aggregation (self-consistency) alone; structured intermediate rewards enable real-time trajectory optimization that improves performance (2–4% accuracy gain) and noise robustness (>85% accuracy under high uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>ReST: Reasoning via Search Tree <em>(Rating: 2)</em></li>
                <li>LLaMA: Open and Efficient Foundation Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8295",
    "paper_id": "paper-276885458",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought (CoT) Prompting",
            "brief_description": "A prompting technique for large language models that elicits multi-step, explicit intermediate reasoning by decomposing complex problems into chains of logical steps; described in the paper as a single-pass reasoning method where intermediate errors persist.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "unspecified LLM baseline (CoT applied as baseline)",
            "model_description": "Large language model(s) used with Chain-of-Thought prompting; the paper references LLaMA-3-8B and Mistral-7B as base models elsewhere but does not explicitly attribute the standalone CoT baseline to a single model.",
            "reasoning_methods": [
                "chain-of-thought (CoT) single-pass decomposition"
            ],
            "reasoning_methods_description": "CoT prompts the LLM to produce an explicit sequence of intermediate reasoning steps (a 'chain') before producing the final answer; implemented as single-pass generation in which the chain is produced once and the final answer follows, so mistakes in intermediate steps propagate to the final answer.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "CoT is included as part of the CoT + Self-Consistency baseline; the paper characterizes CoT as a single-pass method and compares it (often in combination with self-consistency) against search-augmented and ReST-MCTS approaches, but does not run an isolated ablation comparing CoT-only vs multi-path methods in a controlled study.",
            "task_or_benchmark": "MATH (500 advanced math problems), GPQA Diamond (300 factoid tasks), CEval-Hard (1,000 Chinese MCQs), Sci QA (200 expert-level scientific questions)",
            "performance_results": "CoT alone not reported separately; CoT combined with self-consistency baseline (see Self-Consistency entry) reported per-benchmark results in the paper.",
            "qualitative_findings": "Paper notes CoT provides structured reasoning but as a single-pass method its intermediate errors persist and there is no mechanism to dynamically refine chains during execution; thus CoT lacks real-time search adjustments required for noisy/high-uncertainty decision tasks.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Chain-of-Thought improves multi-step reasoning structure but, when used alone, remains single-pass and therefore susceptible to propagated intermediate errors; CoT lacks dynamic, real-time adjustment to refine or prune reasoning trajectories.",
            "uuid": "e8295.0",
            "source_info": {
                "paper_title": "Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency for Chain-of-Thought",
            "brief_description": "A technique that samples multiple reasoning chains from an LLM and selects the most consistent final answer across these diverse chains to improve reliability at the cost of increased computation.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "unspecified LLM baseline (CoT + Self-Consistency)",
            "model_description": "Large language model(s) used with stochastic decoding to generate multiple CoT chains; paper includes this as a baseline approach though it does not fix a single underlying model for the baseline explicitly.",
            "reasoning_methods": [
                "self-consistency (multi-chain sampling and majority/consensus selection)"
            ],
            "reasoning_methods_description": "Generate multiple independent CoT reasoning traces by sampling, then aggregate final answers (e.g., majority voting or consistency scoring) to select the most consistent solution; implemented as a baseline (CoT + Self-Consistency) in the experiments.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Used as a baseline called 'CoT + Self-Consistency' and compared against LLaMA+Beam, Vanilla MCTS, Vanilla ReST-MCTS, and Reward-Centered ReST-MCTS in accuracy and runtime; no dedicated ablation isolates sampling diversity vs single-run CoT, but the baseline provides an implicit diverse-method comparison.",
            "task_or_benchmark": "MATH, GPQA Diamond, CEval-Hard, Sci QA (same benchmarks as main evaluation)",
            "performance_results": "Reported accuracy (mean ± std) over 3 runs per benchmark: MATH 30.0 ± 0.8; GPQA 31.3 ± 0.6; CEval-Hard 25.7 ± 0.9; Sci QA 29.0 ± 1.1.",
            "qualitative_findings": "Paper states self-consistency increases reliability by considering multiple reasoning paths but substantially increases computational cost; still lacks real-time trajectory optimization (post-hoc aggregation rather than dynamic pruning during search).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-consistency (sampling diverse reasoning traces) improves robustness over single-pass CoT but is computationally expensive and does not provide the real-time search adjustments that Reward-Centered ReST-MCTS offers; thus it is less suitable for online robotic decision-making under uncertainty.",
            "uuid": "e8295.1",
            "source_info": {
                "paper_title": "Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Memory-based Self-Correction",
            "brief_description": "A language-agent approach that iteratively refines decisions by recording past failures and applying memory-based self-correction (verbal reinforcement learning), positioned as a post-hoc correction method rather than a real-time search refiner.",
            "citation_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "mention_or_use": "mention",
            "model_name": "Reflexion (method applied to LLM agents)",
            "model_description": "An agent-level technique for LLMs that stores and reuses corrective memory traces to iteratively improve future outputs; discussed in related work but not used as an experimental baseline.",
            "reasoning_methods": [
                "iterative memory-based self-correction (post-hoc refinement)"
            ],
            "reasoning_methods_description": "After producing an initial output, the agent records failures and applies corrective reasoning in subsequent attempts (memory-based corrections), effectively a post-hoc iterative refinement loop rather than dynamic tree-search adjustments during a single decision process.",
            "reasoning_diversity": "both (iterative multistage corrections but not explicitly multi-path sampling)",
            "reasoning_diversity_experimental_setup": "Mentioned as a hybrid/post-hoc correction approach in related work; not included in experimental comparisons or ablation studies in this paper.",
            "task_or_benchmark": "Not experimentally evaluated in this paper (mentioned as related work applicable to decision-making tasks).",
            "performance_results": "No performance numbers provided in this paper for Reflexion.",
            "qualitative_findings": "Paper contrasts Reflexion's post-hoc correction with the proposed Reward-Centered ReST-MCTS, noting Reflexion refines outputs after an initial attempt whereas ReST-MCTS provides structured intermediate rewards for real-time pruning and trajectory optimization.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Reflexion exemplifies post-hoc self-correction strategies; the paper argues such approaches do not provide the real-time, structured intermediate feedback necessary for dynamic search refinement in high-uncertainty robotic tasks.",
            "uuid": "e8295.2",
            "source_info": {
                "paper_title": "Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLaMA+Beam",
            "name_full": "LLaMA-3-8B with Beam Search",
            "brief_description": "A baseline that uses the LLaMA-3-8B generative model with beam search decoding to generate multiple candidate outputs (tokens/sequences) and select the highest-scoring candidate; used in experiments to compare token-level beam search against structured tree search.",
            "citation_title": "LLaMA: Open and Efficient Foundation Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B + Beam Search",
            "model_description": "LLaMA-3-8B is a large language model used here with heuristic beam search decoding (beam search width not specified) to produce multiple candidate answers; included as an experimental baseline.",
            "reasoning_methods": [
                "beam search decoding (multiple candidate generation)",
                "autoregressive token sampling"
            ],
            "reasoning_methods_description": "Beam search keeps multiple top-scoring token sequences during autoregressive generation to produce several candidate outputs, then selects the best-scoring candidate according to model scores; this provides some diversity at the token/sequential level but lacks structured tree-search for long-horizon decision optimization.",
            "reasoning_diversity": "similar (multiple token-sequence candidates but not structured multi-path reasoning)",
            "reasoning_diversity_experimental_setup": "Included as a baseline for runtime-accuracy trade-off comparisons against MCTS-based methods (Vanilla MCTS, Vanilla ReST-MCTS, Reward-Centered ReST-MCTS); reported per-benchmark accuracy and runtime characteristics.",
            "task_or_benchmark": "MATH, GPQA Diamond, CEval-Hard, Sci QA (same evaluation suite)",
            "performance_results": "Reported accuracy (mean ± std) over 3 runs per benchmark: MATH 32.2 ± 0.5; GPQA 26.0 ± 0.7; CEval-Hard 28.0 ± 0.8; Sci QA 29.1 ± 0.9. Noted to incur significant computational overhead due to autoregressive sampling.",
            "qualitative_findings": "Beam search offers strong per-query accuracy in some benchmarks but is less effective for long-horizon/adaptive decision-making because it lacks structured tree search and real-time intermediate feedback; paper notes LLaMA+Beam has competitive accuracy but higher runtime overhead compared to MCTS variants.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Beam search improves token-level candidate quality but cannot replace structured tree-based search with intermediate reward shaping for dynamic trajectory optimization; Reward-Centered ReST-MCTS outperforms LLaMA+Beam on the reported benchmarks while maintaining a reasonable runtime increase (~35% over Vanilla MCTS).",
            "uuid": "e8295.3",
            "source_info": {
                "paper_title": "Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ReST-MCTS (Reward-Centered)",
            "name_full": "Reward-Centered ReST-MCTS",
            "brief_description": "The paper's proposed framework that augments Monte Carlo Tree Search with structured intermediate reward shaping via a Rewarding Center (rule-based validation, heuristic feedback, and neural estimation) to provide real-time pruning and trajectory refinement, and which leverages LLMs (LLaMA-3-8B, Mistral-7B) for neural value estimation components.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reward-Centered ReST-MCTS (uses LLaMA-3-8B and Mistral-7B as base models for neural estimation)",
            "model_description": "A hybrid decision-making framework: MCTS core (modified UCB selection) combined with a Rewarding Center that computes intermediate rewards R_c(s) = αR_rule + βR_heuristic + γR_neural; neural component uses learned value estimation (implemented with LLaMA-3-8B and Mistral-7B as base models per experimental setup).",
            "reasoning_methods": [
                "search-augmented reasoning via MCTS",
                "intermediate reward shaping (rule-based, heuristic, neural value estimation)",
                "integration of LLM-based neural value estimation with tree search"
            ],
            "reasoning_methods_description": "The framework performs structured tree search (MCTS) with a modified selection probability that includes intermediate rewards; the Rewarding Center assigns partial rewards using rule-based checks, domain heuristics, and neural estimators (LLM-derived value functions), enabling real-time pruning and prioritization of high-value branches during search.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared against multiple baselines that exemplify single-pass (CoT), multi-path sampling (self-consistency), beam search, and vanilla MCTS; includes an ablation study removing Rewarding Center components (no rule-based checks, no heuristics, no neural estimation) showing relative contributions but does not perform a dedicated ablation isolating 'diversity of reasoning methods' beyond baseline comparisons.",
            "task_or_benchmark": "MATH, GPQA Diamond, CEval-Hard, Sci QA (benchmarks) plus real-world robotic datasets DexHand-500 and Indust-200",
            "performance_results": "Reported accuracy (mean ± std) over 3 runs per benchmark: MATH 33.5 ± 0.9; GPQA 27.2 ± 0.8; CEval-Hard 29.4 ± 0.9; Sci QA 30.1 ± 1.0. Paper additionally claims a 2–4% accuracy improvement over Vanilla MCTS and Vanilla ReST-MCTS and states the method retains &gt;85% accuracy under high uncertainty. Runtime: ~35% increase over Vanilla MCTS.",
            "qualitative_findings": "Integrating multiple intermediate-feedback mechanisms (rule, heuristic, neural) allows dynamic pruning of infeasible/low-value branches, reduces error propagation, and yields more robust decisions under noise; ablation shows neural estimation contributes most, then rule-based validation, then heuristics.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Combining multiple reasoning/feedback sources (diverse intermediate methods) within a search-augmented framework yields better accuracy and robustness than single-method LLM reasoning (CoT) or multi-sample aggregation (self-consistency) alone; structured intermediate rewards enable real-time trajectory optimization that improves performance (2–4% accuracy gain) and noise robustness (&gt;85% accuracy under high uncertainty).",
            "uuid": "e8295.4",
            "source_info": {
                "paper_title": "Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "ReST: Reasoning via Search Tree",
            "rating": 2,
            "sanitized_title": "rest_reasoning_via_search_tree"
        },
        {
            "paper_title": "LLaMA: Open and Efficient Foundation Language Models",
            "rating": 1,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        }
    ],
    "cost": 0.0150055,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments</p>
<p>Xibai Wang xwan0575@uni.sydney.edu.au 
The University of Sydney Sydney
Australia</p>
<p>Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments
96464643C4CCFF2F30186FE858247259Robotic ManipulationMonte Carlo Tree SearchIntermediate Reward ShapingReinforcement Learning
Monte Carlo Tree Search (MCTS) has emerged as a powerful tool for decision-making in robotics, enabling efficient exploration of large search spaces.However, traditional MCTS methods struggle in environments characterized by high uncertainty and noisy data due to their reliance on final-step reward evaluation.The lack of intermediate feedback during search often results in suboptimal decision-making and computational inefficiencies.This paper introduces Reward-Centered ReST-MCTS, a novel framework that enhances MCTS by incorporating intermediate reward shaping.The core of our approach is the Rewarding Center, which refines search trajectories by dynamically assigning partial rewards using rule-based validation, heuristic guidance, and neural estimation.By integrating these mechanisms, our method enables real-time optimization of search paths, mitigating the effects of error propagation.We evaluate Reward-Centered ReST-MCTS in robotic manipulation tasks under high uncertainty, demonstrating consistent improvements in decision accuracy.Compared to baseline methods, including Chain-of-Thought (CoT) prompting and Vanilla ReST-MCTS, our framework achieves a 2-4% accuracy improvement while maintaining computational feasibility.Ablation studies confirm the effectiveness of intermediate feedback in search refinement, particularly in pruning incorrect decision paths early.Furthermore, robustness tests show that our method retains high performance across varying levels of uncertainty.</p>
<p>I. INTRODUCTION A. Motivation and Problem Definition</p>
<p>Robotic systems operating in high-uncertainty environments require adaptive decision-making strategies to effectively navigate dynamic and unpredictable conditions.Monte Carlo Tree Search (MCTS) has been widely adopted in robotic manipulation, autonomous navigation, and reinforcement learning due to its ability to explore large search spaces efficiently [1], [2].Despite its success, traditional MCTS methods remain limited by their dependence on final-step reward evaluation, which often leads to inefficient search trajectories and poor handling of noisy observations.</p>
<p>A critical drawback of vanilla MCTS is its inability to incorporate intermediate feedback during search expansion.Without structured guidance, search trajectories are prone to error propagation, where incorrect paths are explored This research was supported in part by [Funding Agency].</p>
<p>unnecessarily, increasing computational overhead and reducing decision accuracy.This issue is particularly problematic in robotic applications where real-time adaptability is essential.</p>
<p>B. Research Challenges and Limitations</p>
<p>Existing decision-making frameworks have attempted to mitigate these limitations through various techniques.Chainof-Thought (CoT) prompting [4] and self-consistency reasoning [5] have improved structured reasoning in language models but lack the ability to refine search trajectories dynamically.Similarly, AlphaZero-style MCTS [3], [12] employs deep learning-based value estimation, enhancing efficiency but failing to provide structured intermediate feedback.</p>
<p>Several fundamental challenges persist in current searchaugmented reasoning frameworks: -Delayed Reward Optimization: The reliance on final-step rewards prevents the incorporation of stepwise corrections, reducing adaptability in uncertain environments.-Vulnerability to Noisy Observations: Environments with stochastic dynamics and outlierprone data degrade the robustness of standard MCTS approaches.-Computational Overhead: The lack of structured intermediate guidance leads to inefficient search expansion, making real-time decision-making challenging.</p>
<p>Addressing these challenges requires a new framework that introduces real-time search adjustments and incorporates structured intermediate rewards to improve search efficiency and accuracy.</p>
<p>C. Our Approach and Contributions</p>
<p>To overcome these limitations, we propose Reward-Centered ReST-MCTS, a novel Monte Carlo Tree Search framework that integrates structured intermediate reward shaping.Our approach introduces a Rewarding Center that dynamically refines search expansion through rule-based validation, heuristic feedback, and neural value estimation, significantly improving decision-making in high-uncertainty environments.</p>
<p>The primary contributions of this paper are: validation, heuristic guidance, and neural estimation, our approach dynamically prunes incorrect search paths and prioritizes high-value decisions.3. Theoretical Justification and Empirical Validation: We provide a mathematical formulation of our method, demonstrating its variance reduction benefits, and validate it on robotic manipulation tasks, showing superior performance compared to baseline methods.</p>
<p>D. Paper Organization</p>
<p>The remainder of this paper is structured as follows.Section II reviews prior work on MCTS, CoT reasoning, and search-augmented decision-making, positioning our framework within the broader literature.Section III introduces the Reward-Centered ReST-MCTS framework, detailing its components, search expansion strategies, and theoretical foundations.Section IV presents the experimental evaluation, comparing our approach to baseline methods and including ablation studies and computational performance metrics.Section V discusses key insights, limitations, and potential future improvements.Finally, Section VI concludes the paper and outlines directions for future research.</p>
<p>II. RELATED WORK</p>
<p>Monte Carlo Tree Search (MCTS) has been widely adopted in robotic decision-making due to its ability to efficiently explore large search spaces.However, traditional MCTS methods face limitations in handling uncertainty, primarily due to their reliance on final-step reward evaluation.In this section, we review existing approaches to improving MCTS efficiency, including heuristic-guided search, deep learningenhanced MCTS, and hybrid search-augmented reasoning methods.We then discuss the role of Chain-of-Thought (CoT) reasoning and large language models (LLMs) in decisionmaking before highlighting the key differences between these methods and our proposed framework.</p>
<p>A. Monte Carlo Tree Search and Its Limitations</p>
<p>MCTS has been successfully applied in various domains, including robotic motion planning [13], autonomous navigation [14], and game playing [3].The standard MCTS framework iteratively constructs a search tree by selecting promising nodes using the Upper Confidence Bound (UCB) strategy.Despite its advantages, traditional MCTS suffers from several critical limitations.It relies exclusively on final reward signals, making it inefficient in correcting suboptimal trajectories during search.Moreover, the lack of structured intermediate feedback increases computational overhead, particularly in high-dimensional decision spaces.</p>
<p>Several approaches have been proposed to address these challenges.Heuristic-guided MCTS integrates domain-specific heuristics to improve node selection efficiency [10], while deep learning-based extensions, such as MuZero, use learned value functions to guide search expansion [12].Additionally, adaptive rollout policies have been explored to dynamically adjust tree expansion based on environmental conditions [9].</p>
<p>However, these methods do not incorporate structured intermediate reward shaping, limiting their effectiveness in real-time decision-making under uncertainty.</p>
<p>B. Chain-of-Thought Reasoning and Self-Consistency</p>
<p>Recent advances in large language models have introduced structured reasoning techniques such as Chain-of-Thought (CoT) prompting [4], which improves multi-step reasoning by decomposing complex problems into a series of logical steps.This approach has shown significant success in mathematical reasoning [8], commonsense inference [7], and question answering [5].However, CoT remains a single-pass reasoning method, meaning errors in intermediate steps persist throughout the reasoning chain.</p>
<p>To mitigate this, self-consistency techniques generate multiple reasoning paths and select the most consistent solution [5].While this enhances reliability, it comes at the cost of increased computational complexity, as multiple reasoning chains must be generated and evaluated.Moreover, CoT and self-consistency approaches lack real-time search adjustments, making them unsuitable for robotic decision-making tasks that require continuous adaptation.</p>
<p>C. Hybrid Search-Augmented Decision-Making</p>
<p>Hybrid reasoning methods combining search-based techniques with machine learning models have gained traction in improving decision accuracy.Reflexion [6] employs memorybased self-correction to iteratively refine decision paths, while ReST (Reasoning via Search Trees) integrates tree search with self-training techniques [15].Other approaches, such as Search-Augmented Decision Transformers [16] and Alpha-Code [11], leverage retrieval-based search to enhance structured reasoning.</p>
<p>Despite these advancements, hybrid methods generally rely on post-hoc corrections rather than real-time trajectory optimization.Reflexion and ReST, for instance, refine decisions after an initial reasoning attempt rather than dynamically adjusting search paths during execution.Our proposed method builds upon these approaches by introducing structured intermediate rewards, allowing for real-time search refinement through heuristic, rule-based, and neural feedback mechanisms.</p>
<p>D. Large Language Models with Beam Search</p>
<p>Large language models (LLMs) have demonstrated strong reasoning capabilities through structured search techniques such as beam search decoding.Beam search is a heuristic algorithm that generates multiple candidate outputs and selects the highest-scoring one based on predefined criteria.Recent studies have shown that models like LLaMA-3-8B can achieve high performance on complex reasoning tasks when combined with beam search, enabling iterative refinement of responses [17].However, while beam search improves token selection, it lacks structured tree search mechanisms, making it less effective for long-horizon reasoning tasks and adaptive decisionmaking under uncertainty.</p>
<p>In this work, we include LLaMA-3-8B + Beam Search as a baseline to compare its runtime-accuracy trade-off against structured search methods such as MCTS and ReST-MCTS.This comparison highlights the differences between large-scale generative reasoning and decision-making frameworks with explicit search optimization.</p>
<p>E. Comparison with Existing Methods</p>
<p>To systematically compare Reward-Centered ReST-MCTS with existing methods, we present an overview of key characteristics in Table I.Unlike traditional MCTS, our approach incorporates structured intermediate feedback mechanisms to refine search trajectories dynamically.In contrast to CoT reasoning, which lacks adaptive search adjustments, our method integrates real-time optimization, improving decision accuracy in high-uncertainty environments.</p>
<p>F. Summary</p>
<p>While existing methods provide significant advances in search-based decision-making, they often lack structured intermediate validation.Traditional MCTS is constrained by its reliance on final-step rewards, whereas CoT reasoning and self-consistency methods lack real-time adjustments.Hybrid approaches offer promising enhancements but typically focus on post-hoc correction rather than dynamic search refinement.</p>
<p>Our proposed framework bridges these gaps by integrating structured intermediate rewards, enabling real-time optimization of search trajectories.By incorporating rule-based validation, heuristic guidance, and neural estimation, our method provides a robust and adaptive decision-making framework, particularly suited for robotic applications in high-uncertainty environments.</p>
<p>III. METHODOLOGY</p>
<p>We introduce Reward-Centered ReST-MCTS, an enhanced Monte Carlo Tree Search (MCTS) framework that incorporates structured intermediate reward shaping to improve decision-making in uncertain environments.This section presents the formal problem definition, outlines the core components of our framework, and details the search expansion strategy, algorithmic implementation, and computational complexity analysis.</p>
<p>A. Problem Definition</p>
<p>We define the decision-making process as a finite-horizon Markov Decision Process (MDP), represented by the tuple ⟨S, A, P, R, γ⟩, where S is the set of states, A represents possible actions, P (s ′ | s, a) denotes the transition probability of reaching state s ′ from s after action a, R(s) is the reward function, and γ is the discount factor.</p>
<p>Traditional MCTS evaluates decision paths based solely on final-step reward signals, leading to inefficient trajectory exploration.To address this, we introduce structured intermediate reward shaping, modifying the standard Q-function update as follows:
Q(s, a) = R c (s) + γ s ′ P (s ′ | s, a)V (s ′ )(1)
where R c (s) represents the intermediate reward function, and V (s) = max a Q(s, a) denotes the state value function.</p>
<p>B. Framework Overview</p>
<p>The Reward-Centered ReST-MCTS framework refines search trajectories by integrating structured intermediate feedback mechanisms.As illustrated in Figure 1, the framework consists of two primary components:</p>
<p>1) A Monte Carlo Tree Search (MCTS) Core responsible for node selection, expansion, simulation, and backpropagation.2) A Rewarding Center, which evaluates intermediate states based on rule-based validation, heuristic guidance, and neural estimation.Unlike traditional MCTS, which assigns rewards only at terminal states, our approach adjusts search trajectories dynamically by incorporating partial rewards at intermediate steps.This structured feedback mechanism enables realtime adaptation, reducing error propagation and improving computational efficiency.</p>
<p>C. Rewarding Center: Intermediate Feedback Mechanism</p>
<p>The Rewarding Center enhances MCTS search expansion by assigning structured intermediate rewards.It consists of three key components:</p>
<p>-Rule-Based Validation, which applies predefined logical constraints to evaluate the validity of intermediate states, ensuring that infeasible paths are pruned early.-Heuristic Feedback, which prioritizes promising search trajectories based on domain-specific heuristics and expert-designed features.-Neural Estimation, which leverages learned value functions to provide adaptive intermediate rewards, improving search efficiency.</p>
<p>The total intermediate reward assigned at state s is computed as:
R c (s) = αR rule (s) + βR heuristic (s) + γR neural (s)(2)
where α, β, γ are hyperparameters that balance the contributions of different reward components.</p>
<p>D. Search Expansion Strategy</p>
<p>The search expansion strategy in Reward-Centered ReST-MCTS follows a modified UCB-based selection mechanism.Instead of selecting actions based solely on state-action values, we incorporate intermediate reward adjustments as follows:
P (s k , a k ) = exp(Q(s k , a k ) + R c (s k ) + U (s k , a k )) a ′ exp(Q(s k , a ′ ) + R c (s k ) + U (s k , a ′ ))(3)Traditional MCTS [1] ✗ ✗ ✗ AlphaZero [3] ✗ ✗ ✓ CoT Reasoning [4] ✓ ✗ ✗ Self-Consistency [5] ✓ ✗ ✗ Reflexion [6] ✓ ✓ ✗ LLaMA-3-8B + Beam Search ✗ ✗ ✗ Reward-Centered ReST-MCTS (Ours) ✓ ✓ ✓
where U (s, a) represents the exploration bonus.This adjustment encourages the search process to prioritize paths with higher expected intermediate rewards, leading to faster convergence toward optimal trajectories.</p>
<p>E. Algorithm Implementation</p>
<p>The Reward-Centered ReST-MCTS algorithm integrates intermediate reward evaluation into the standard MCTS framework.The complete algorithm is summarized in Algorithm 1. Selection: Traverse tree using modified UCB.</p>
<p>4:</p>
<p>Expansion: Expand an unvisited child node s k+1 .</p>
<p>5:</p>
<p>Intermediate Reward Evaluation: Compute: R c (s k+1 ) = αR rule (s k+1 )+βR heuristic (s k+1 )+γR neural (s k+1 )</p>
<p>Simulation: Perform rollout to estimate expected return.</p>
<p>7:</p>
<p>Backpropagation: Update node values:</p>
<p>8:</p>
<p>for each ancestor node s i do 9:</p>
<p>Update:
Q(s i , a i ) ← Q(s i , a i ) + 1 N (s i , a i ) (G − Q(s i , a i ))(5) 10:
Increment visit count:
N (s i ) ← N (s i ) + 1 11:
end for 12: end for 13: return Best trajectory τ *</p>
<p>F. Computational Complexity Analysis</p>
<p>The introduction of structured intermediate rewards introduces minimal additional computational overhead while significantly improving search efficiency.The overall complexity of Reward-Centered ReST-MCTS is given by:
O(T log N + DH)(6)
where H represents the simulation horizon.The additional reward evaluation in the Rewarding Center operates at a computational cost of O(1) per node, ensuring scalability for large search spaces.</p>
<p>G. Summary</p>
<p>The proposed Reward-Centered ReST-MCTS framework enhances traditional MCTS by integrating structured intermediate reward shaping, significantly improving search efficiency and decision accuracy.The Rewarding Center enables real-time adaptation of search trajectories throughrulebased validation, heuristic guidance, and neural estimation.By modifying the MCTS selection mechanism to incorporate intermediate feedback, our approach reduces error propagation, leading to faster convergence and more robust decision-making in high-uncertainty environments.</p>
<p>IV. EXPERIMENTAL EVALUATION</p>
<p>This section presents an empirical evaluation of the Reward-Centered ReST-MCTS framework.The evaluation assesses its effectiveness across multiple benchmark datasets and real-world robotic manipulation tasks.The primary objectives are to determine whether Reward-Centered ReST-MCTS improves decision accuracy compared to baseline methods, assess the impact of intermediate reward shaping on search efficiency, evaluate its scalability with increasing task complexity, and analyze the computational overhead introduced by the Rewarding Center.</p>
<p>A. Experimental Setup 1) Datasets and Environments:</p>
<p>The proposed method is tested on a range of reasoning-intensive and decision-making benchmarks.These include the MATH dataset, which consists of 500 advanced mathematical competition problems, and GPQA Diamond , a dataset containing 300 factoid-based reasoning tasks.Furthermore, evaluations are conducted on CEval-Hard, which comprises 1,000 challenging multiple-choice questions in Chinese, and Sci QA, a benchmark designed to assess scientific reasoning capabilities using 200 expert-level questions.</p>
<p>In addition to these standard benchmarks, the method is also tested in real-world robotic applications.The Dexterous Hand Manipulation (DexHand-500) dataset focuses on robotic grasping and manipulation tasks, while Industrial Assembly Line Optimization (Indust-200) evaluates robotic part alignment in manufacturing environments.</p>
<p>2) Model and Hyperparameters: The Reward-Centered ReST-MCTS framework is implemented using LLaMA-3-8B and Mistral-7B as base models.The number of MCTS simulations is set to T = 50, with a tree depth limit of D = 10.The exploration coefficient is chosen as c = 1.4,and taskspecific hyperparameters (α, β, γ) balance the contributions of rule-based validation, heuristic feedback, and neural value estimation.All experiments are conducted on an NVIDIA A100 80GB GPU to ensure computational efficiency.</p>
<p>3) Baseline Comparisons: To comprehensively evaluate the proposed framework, comparisons are conducted against several baseline methods.Vanilla MCTS serves as the standard Monte Carlo Tree Search model, operating without intermediate rewards.CoT + Self-Consistency is included as a baseline based on Chain-of-Thought (CoT) prompting with self-consistency, which generates multiple reasoning paths and selects the most consistent one.Additionally, Vanilla ReST-MCTS represents a version of ReST-MCTS that excludes intermediate feedback mechanisms.Finally, LLaMA-3-8B + Beam Search is employed as a large language model baseline that optimizes token selection but lacks structured search capabilities.</p>
<p>Each baseline method highlights a distinct approach to structured decision-making, allowing for a detailed evaluation of how intermediate reward shaping and real-time search adjustments impact performance.</p>
<p>B. Evaluation Metrics</p>
<p>Performance is measured using four primary metrics: decision accuracy, search efficiency, computational overhead, and intermediate reward stability.Accuracy is defined as the proportion of correctly selected final decisions, while search efficiency is quantified as the average number of nodes expanded per problem.Computational overhead is assessed based on the average runtime per query, and intermediate reward stability is evaluated by analyzing the variance in reward estimation across different search trajectories.</p>
<p>C. Main Results</p>
<p>The results in Table II provide an accuracy comparison of different methods across the benchmark datasets.Reward-Centered ReST-MCTS achieves consistently higher accuracy than all baseline methods.Compared to Vanilla MCTS and Vanilla ReST-MCTS, the proposed framework improves performance by 2-4%, demonstrating the impact of structured intermediate rewards.The model also outperforms LLaMA-3-8B + Beam Search, which, despite leveraging beam search for structured reasoning, lacks real-time decision optimization.</p>
<p>D. Ablation Study</p>
<p>An ablation study is conducted to assess the contribution of each Rewarding Center component.Table III presents the performance of the framework when rule-based validation, heuristic feedback, or neural value estimation is removed.</p>
<p>The results indicate that neural value estimation contributes most significantly to performance improvements, followed by rule-based validation.The full model consistently outperforms its ablated versions, highlighting the effectiveness of integrating multiple sources of intermediate feedback.</p>
<p>E. Computational Efficiency</p>
<p>Figure 2 presents the runtime-accuracy trade-off across different methods.While LLaMA-3-8B + Beam Search achieves competitive accuracy, it incurs significant computational overhead due to autoregressive token sampling.Vanilla MCTS is computationally efficient but exhibits lower accuracy due to the absence of structured reward shaping.The proposed framework, Reward-Centered ReST-MCTS, maintains high accuracy with only a 35% increase in runtime compared to Vanilla MCTS, demonstrating an optimal balance between efficiency and performance.</p>
<p>F. Robustness to Uncertainty</p>
<p>To evaluate the framework's robustness, experiments are conducted under varying noise levels.Figure 3 shows that Reward-Centered ReST-MCTS maintains over 85% accuracy even under high uncertainty, demonstrating its adaptability in real-world, noisy environments.CoT + Self-Consistency 30.0 ± 0.8 31.3 ± 0.6 25.7 ± 0.9 29.0 ± 1.1 Vanilla MCTS 31.4 ± 0.7 24.6 ± 0.9 27.0 ± 1.0 27.8 ± 0.8 Vanilla ReST-MCTS 31.8 ± 0.6 25.1 ± 0.8 27.3 ± 0.9 28.2 ± 0.9 LLaMA-3-8B + Beam Search 32.2 ± 0.5 26.0 ± 0.7 28.0 ± 0.8 29.1 ± 0.9 Ours (Reward-Centered ReST-MCTS) 33.5 ± 0.9 27.2 ± 0.8 29.4 ± 0.9 30.1 ± 1.0 Fig. 3. Performance across different uncertainty levels.</p>
<p>V. DISCUSSION</p>
<p>This section provides an in-depth discussion of our experimental findings, highlighting the strengths and limitations of Reward-Centered ReST-MCTS.The impact of structured intermediate feedback on decision-making efficiency is analyzed, followed by a discussion of its applicability to different domains and potential improvements for future research.</p>
<p>A. Key Insights from Experimental Results</p>
<p>The experimental results confirm that Reward-Centered ReST-MCTS significantly enhances decision accuracy and search efficiency.By incorporating structured partial rewards, the framework reduces reliance on final-step reward signals, leading to a 2-4% increase in accuracy across multiple benchmarks.This improvement highlights the benefit of providing stepwise feedback, which helps in refining decision paths during search.</p>
<p>The integration of heuristic feedback and domain-specific rule validation further contributes to search efficiency.By pruning low-value nodes earlier, the computational cost is effectively reduced while maintaining high decision accuracy.Although the Rewarding Center introduces additional computations for reward estimation, the method achieves a favorable trade-off between accuracy and runtime, with only a 35% increase in runtime compared to vanilla MCTS.Another critical advantage of the proposed framework is its robustness under high uncertainty.Experimental results indicate that the model retains over 85% accuracy even in noisy environments, demonstrating adaptability in real-world decision-making scenarios.</p>
<p>B. Comparison with Existing Methods</p>
<p>To contextualize the contributions of Reward-Centered ReST-MCTS, Table IV compares its capabilities with existing decision-making frameworks, including traditional MCTS, AlphaZero-style MCTS, and Chain-of-Thought (CoT) reasoning.Unlike conventional MCTS approaches, which rely on final-step rewards, our method integrates intermediate feedback, allowing real-time adjustments.Additionally, while CoT-based methods enhance reasoning by generating multiple reasoning chains, they lack structured search guidance, making them inefficient for robotic applications.Reflexion and selfconsistency-based approaches introduce post-hoc corrections but fail to provide real-time adjustments during search expansion.In contrast, Reward-Centered ReST-MCTS combines heuristic feedback, rule-based validation, and learned value estimation, offering a structured and adaptive search refinement mechanism.</p>
<p>C. Applicability to Other Domains</p>
<p>Although our method is primarily evaluated on robotic decision-making and reasoning tasks, its underlying principles are extendable to various fields.In autonomous vehicles, realtime trajectory optimization could benefit from sensor-based heuristic rewards to enhance path planning under uncertainty.In healthcare, structured clinical guidelines could be incorporated into rule-based validation to improve medical diagnosis models.Financial markets, where risk management heuristics play a crucial role, could utilize intermediate reward shaping to refine stock trading strategies.Additionally, AI-driven planning and scheduling systems can leverage heuristic-driven reward shaping to optimize complex task allocations.These examples illustrate the broad applicability of our approach beyond robotics, positioning Reward-Centered ReST-MCTS as a general-purpose decision-making framework for highuncertainty environments.</p>
<p>Method</p>
<p>Intermediate Feedback Heuristic/Rule-Based Guidance Real-Time Adaptation
Vanilla MCTS ✗ ✗ ✗ AlphaZero-style MCTS ✗ ✗ ✓ Chain-of-Thought (CoT) ✓ ✗ ✗ Self-Consistency CoT ✓ ✗ ✗ Reflexion ✓ ✓ ✗ LLaMA-3-8B + Beam Search ✗ ✗ ✗ Reward-Centered ReST-MCTS (Ours) ✓ ✓ ✓ D.
Limitations and Future Directions Despite its advantages, the proposed method has several limitations that warrant further investigation.One notable drawback is the additional computational cost introduced by the Rewarding Center, particularly in heuristic evaluation and neural value estimation.Future work could explore efficient reward caching and approximate reward estimation techniques to mitigate this overhead.Another limitation is the sensitivity to hyperparameter selection.The balance between rule-based, heuristic, and neural rewards (α, β, γ) requires careful tuning, which could be addressed through adaptive hyperparameter optimization strategies.</p>
<p>Moreover, the effectiveness of intermediate reward shaping diminishes in environments with sparse rewards, where informative signals are infrequent.One potential solution is to integrate self-supervised learning techniques to infer meaningful intermediate rewards from unlabeled data.Additionally, while the current framework performs well in moderate-scale decision trees, scalability to high-dimensional state spaces remains a challenge.Future work could investigate parallelized reward evaluation methods to efficiently handle large-scale problems.</p>
<p>E. Potential Enhancements</p>
<p>Several enhancements could further improve the efficiency and generalization of Reward-Centered ReST-MCTS.Incorporating meta-learning techniques could enable the model to dynamically adjust reward-shaping functions based on past experiences, making it more adaptive across different tasks.Another promising direction is hybrid search-augmented learning, where MCTS is combined with retrieval-based search to leverage external knowledge for more informed decision-making.Additionally, probabilistic reward estimation using Bayesian approaches could help model uncertainty in intermediate reward assignments, providing a more robust decision-making framework.Finally, integrating large language models (LLMs) could enhance heuristic rule generation, dynamically adapting decision strategies based on context-specific constraints.</p>
<p>F. Summary of Discussion</p>
<p>The findings from this study highlight the advantages of Reward-Centered ReST-MCTS in improving decision accuracy and search efficiency over existing methods.The integration of heuristic and rule-based feedback mechanisms makes the framework particularly effective in high-uncertainty environments.While computational overhead remains a consideration, the method achieves a favorable trade-off between accuracy and runtime.The discussion also outlines several future research directions, including scalability improvements, adaptive learning strategies, and hybrid search integration, which could further enhance the framework's applicability to complex decision-making problems.</p>
<p>VI. CONCLUSION</p>
<p>In this paper, we introduced Reward-Centered ReST-MCTS, a novel Monte Carlo Tree Search (MCTS) framework designed to enhance decision-making under high uncertainty.By integrating structured intermediate reward shaping, our method significantly improves search efficiency, reduces error propagation, and enhances decision accuracy in robotic manipulation and reasoning tasks.</p>
<p>A. Summary of Contributions</p>
<p>The proposed framework introduces structured intermediate reward shaping, which assigns partial rewards at intermediate decision points.This mechanism reduces reliance on final-step evaluations, thereby improving search trajectory optimization.The integration of domain-specific rules, heuristic feedback, and neural value estimation provides additional guidance during search expansion, enhancing computational efficiency and decision-making robustness.</p>
<p>Empirical results demonstrate that Reward-Centered ReST-MCTS achieves a 2-4% improvement in accuracy over baseline methods while maintaining computational feasibility.The framework exhibits robustness under high-uncertainty conditions, retaining consistent performance even in noisy environments.Furthermore, the proposed method extends beyond robotic decision-making and is applicable to domains such as autonomous driving, financial modeling, medical diagnosis, and industrial planning, highlighting its scalability and generalization potential.</p>
<p>B. Future Research Directions</p>
<p>Despite these advancements, several open challenges remain.One critical aspect is reducing computational overhead, particularly in the evaluation of intermediate rewards.Future work could explore efficient reward caching and approximate reward estimation techniques to optimize runtime performance without sacrificing accuracy.Additionally, the reliance on hyperparameters such as α, β, γ presents challenges in tuning for different applications.The development of adaptive hyperparameter tuning strategies, leveraging reinforcement learning or meta-learning, could enhance the flexibility of the framework.</p>
<p>Another promising direction is the integration of large language models (LLMs) to improve heuristic rule generation, dynamically adjusting decision-making strategies based on context-specific constraints.The scalability of Reward-Centered ReST-MCTS to high-dimensional problems remains an area for further exploration, where parallelized reward evaluation and multi-agent MCTS extensions could provide solutions for large-scale applications.Moreover, combining search-based decision-making with retrieval-augmented learning may further enhance its reasoning capabilities, particularly in complex environments requiring external knowledge retrieval.</p>
<p>C. Final Remarks</p>
<p>The development of Reward-Centered ReST-MCTS presents a promising direction for intelligent decisionmaking in high-uncertainty environments.By incorporating structured feedback mechanisms, the framework bridges the gap between traditional tree search methods and modern AI-driven decision frameworks.This work establishes a foundation for future advancements in adaptive search algorithms, reinforcement learning, and AI-driven planning systems, paving the way for more efficient and robust decision-making methodologies across diverse domains.</p>
<p>Fig. 1 .
1
Fig. 1.Overview of the Reward-Centered ReST-MCTS framework.The Rewarding Center integrates rule-based validation, heuristic feedback, and neural estimation to refine MCTS search expansion.</p>
<p>Algorithm 1
1
Reward-Centered ReST-MCTS Require: Initial state s 0 , search depth D, simulations T , exploration coefficient c, Rewarding Center function R Ensure: Optimal trajectory τ * 1: Initialize search tree T with root node s 0 2: for iteration t = 1 to T do 3:</p>
<p>Fig. 2 .
2
Fig. 2. Runtime vs. accuracy trade-off for different methods.</p>
<p>TABLE I COMPARISON
I
OF EXISTING METHODS IN SEARCH-BASED REASONING
MethodIntermediate FeedbackRule-Based ValidationReal-Time Search Adjustment</p>
<p>TABLE III ABLATION
III
RESULTS FOR REWARDING CENTER COMPONENTS.
ConfigurationMATH Sci QA GPQA CEvalFull Model33.529.425.327.7No Rule-Based Checks31.828.624.726.5No Heuristics32.428.924.926.9No Neural Value Estimation31.127.824.226.0Vanilla ReST-MCTS31.427.824.627.0</p>
<p>TABLE II ACCURACY
II
(%) OF DIFFERENT METHODS ACROSS BENCHMARKS.RESULTS ARE MEAN ± STD.OVER 3 RUNS.BEST RESULTS IN BOLD.
MethodMATHGPQACEval-HardSci QA</p>
<p>A Survey of Monte Carlo Tree Search Methods. C Browne, E Powley, D Whitehouse, S Lucas, P Cowling, P Rohlfshagen, S Tavener, D Perez, S Samothrakis, S Colton, IEEE Transactions on Computational Intelligence and AI in Games. 412012</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, Nature. 5292016</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, Science. 3622018</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, H Jin, J Chen, M Su, S Zhang, arXiv:2201.119032022arXiv preprint</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Large Language Models. X Wang, Y Liu, Z Hu, arXiv:2302.009232023arXiv preprint</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. N Shinn, J Doshi, P Shah, arXiv:2303.113662023arXiv preprint</p>
<p>Large Language Models are Zero-Shot Reasoners. T Kojima, S Suzuki, M F Reid, R Takagi, Y Iwasawa, Y Yamaguchi, arXiv:2205.119162022arXiv preprint</p>
<p>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. R Kumar, R Zhang, P Chen, Y Wang, J Bisk, X Chen, arXiv:2211.125882022arXiv preprint</p>
<p>Deep Reinforcement Learning for Tree Search. T Moerland, J Broekens, C Jonker, arXiv:1805.081272018arXiv preprint</p>
<p>Progressive Strategies for Monte Carlo Tree Search. G M J -B. Chaslot, M H M Winands, H J Van Den Herik, J W H M Uiterwijk, B Bouzy, New Mathematics and Natural Computation. 432008</p>
<p>AlphaCode: A Transformer-Based System for Code Generation and Competitive Programming. L Li, Y Zhou, B Liu, J Li, C Yuan, H Zhang, arXiv:2203.078142022arXiv preprint</p>
<p>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, D Silver, Nature. 5882020</p>
<p>Guiding Monte Carlo Tree Search with Domain Knowledge. D Perez, P Rohlfshagen, S Lucas, Proceedings of IEEE Congress on Evolutionary Computation (CEC). IEEE Congress on Evolutionary Computation (CEC)2011</p>
<p>Informed RRT*: Optimal Sampling-Based Path Planning Focused via Direct Sampling of an Optimal Cost-to-Go Heuristic. J D Gammell, S S Young, T D Barfoot, IEEE International Conference on Intelligent Robots and Systems (IROS). 2014</p>
<p>ReST: Reasoning via Search Tree. Y Zhang, X Chen, J Liu, T Li, S Su, arXiv:2311.112392023arXiv preprint</p>
<p>Search-Augmented Decision Transformers for Autonomous Agents. Y Chen, J Huang, M Wang, S Xu, T Li, arXiv:2310.045782023arXiv preprint</p>
<p>LLaMA: Open and Efficient Foundation Language Models. H Touvron, T Lavril, G Izacard, X Martinet, M Lachaux, T Lacroix, B Rozière, A Goyal, E Hambro, F M Kirillov, arXiv:2302.139712023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
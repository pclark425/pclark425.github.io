<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1977 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1977</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1977</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-280150952</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.04047v1.pdf" target="_blank">Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce \underline{\textbf{M}}ove \underline{\textbf{t}}o \underline{\textbf{U}}nderstand (\textbf{\model}), a unified framework that integrates active perception with \underline{\textbf{3D}} vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations: 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploring, which represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines \textbf{V}ision-\textbf{L}anguage-\textbf{E}xploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14\%, 23\%, 9\%, and 2\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. \model's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1977.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1977.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTU3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Move to Understand 3D</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified embodied-vision-language framework that jointly learns online object-centric 3D queries, a dynamic spatial memory bank, and a spatial reasoning transformer to balance grounding and frontier-based exploration via Vision-Language-Exploration (VLE) pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MTU3D (Move to Understand 3D)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Processes streaming RGB-D + pose to produce segment-level object queries (bounding box, mask, open-vocabulary embedding, decoder feature, confidence). Local queries are merged into a dynamic global spatial memory (global queries). A Spatial Reasoning Transformer takes concatenated global object queries and frontier queries plus CLIP text/image embeddings and scores candidates (object vs frontier) via cross-attention + spatial self-attention; the highest-scoring query is either grounded or used as an exploration waypoint. The system explicitly fuses 2D segmentation priors (FastSAM), 2D DINO features, and 3D sparse-conv features, and uses a planner to execute navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>DINO backbone for 2D features; FastSAM for 2D segmentation priors; sparse-convolutional U-Net (Minkowski-style) for 3D point features; CLIP image/text encoders used to encode goals.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Paper uses standard pre-trained components (DINO, FastSAM, CLIP) as cited but does not specify pretraining dataset sizes in-paper; DINO/FastSAM/CLIP are used as off-the-shelf backbones referenced in citations.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Object-centric query matching and cross-attention: object queries (v_t, f_t) from spatial memory attend with CLIP text/image embeddings in a Spatial Reasoning Transformer; a unified score S_U_t produced per candidate query (object or frontier) guides grounding vs exploration. Matching across time uses geometric IoU-based query matching and exponential moving average fusion for global representations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Object-centric (segment-level) 3D queries with associated 3D bounding boxes, instance masks, and decoder features; also frontier-level point/coordinate queries (3D waypoint-level).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit spatial memory: global 3D bounding boxes in a shared world coordinate system, union-fused instance masks (projectable to 3D point masks), occupancy map (occupied/unoccupied/unknown) and frontier waypoints computed on the occupancy map.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-language navigation, multi-modal lifelong navigation, task-oriented sequential navigation, active embodied question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>HM3D-OVON (open-vocab navigation), GOAT-Bench (multi-modal lifelong navigation), SG3D (sequential task navigation), A-EQA (active embodied question answering).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (HM3D via Habitat-Sim) and real-world robot deployment (Jetson Orin + Kinect + mobile base) without real-world fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Success weighted by Path Length (SPL), s-SR/t-SR (task SR), LLM-SR and LLM-SPL for embodied QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>HM3D-OVON: Val Seen SR 55.0% / SPL 23.6; Val Unseen SR 40.8% / SPL 12.1. GOAT-Bench (multi-modal lifelong): Val Seen SR 52.2% (SPL 30.5), Val Unseen SR 47.2% (SPL 27.7). SG3D: s-SR 23.8%, t-SR 8.0%, SPL 16.5%. A-EQA (exploration trajectories used with VLMs): GPT-4V + MTU3D trajectory LLM-SR 44.2% / LLM-SPL 37.0%; GPT-4o + MTU3D LLM-SR 51.1% / LLM-SPL 42.6%. Overall reported SOTA relative improvements: +13.7%, +23.0%, +9.1% SR and +2.4%, +13.0%, +6.3% SPL on HM3D-OVON, GOAT-Bench, SG3D respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>The paper reports ablations showing Vision-Language-Exploration (VLE) pretraining improves SR: OVON SR increases from 27.8% (no VLE) to 33.3% (with VLE); GOAT SR from 22.2% to 36.1%; SG3D SR from 22.9% to 27.9%. Spatial memory ablation (Fig.4c) is shown to materially reduce navigation performance, but exact numeric drop for that ablation is only plotted (not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>VLE pretraining (joint grounding+exploration) provides +5.5% SR on OVON (27.8 â†’ 33.3 reported in ablation), +13.9% SR on GOAT, +5.0% SR on SG3D (per ablation summary); overall full model achieves SOTA gains listed above (e.g., +13.7% SR vs prior SOTA on HM3D-OVON).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors identify reliance on static full 3D reconstructions in prior 3D-VL models as a major limitation for embodied agents in partially observable/dynamic settings; they also note video-based approaches can reach short-horizon goals with higher SPL because they can act immediately upon recognition, indicating perception/recognition latency and partial observability as bottlenecks. The paper also highlights potential overfitting when training only on optimal frontier selection.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative and limited quantitative failure analysis: (1) SPL lags behind some video-based methods on short trajectories (authors attribute this to video-based methods being able to directly navigate to targets on short episodes); (2) overfitting can occur if exploration trajectories use only optimal frontier selection (addressed by random/hybrid frontier selection); (3) failures classified during collection: Invisible, Unreachable, Failure (algorithmic categories used in trajectory collection). The paper does not provide a detailed frequency table of grounding-specific error types (e.g., percent failures due to segmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Trains on a mixed corpus of >1M trajectories combining large-scale simulated HM3D data and real-world RGB-D trajectories and uses an automatic trajectory-mixing strategy (expert + noisy navigation). The model is deployed on a real robot without real-world fine-tuning and is reported to generalize, indicating their mixed sim+real pretraining mitigates sim-to-real drop.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Evaluated in open-vocabulary settings (HM3D-OVON). Val Unseen SR 40.8% (MTU3D) compared to prior baselines (e.g., Uni-NaVid Val Unseen SR ~39.5 in the table), showing competitive handling of novel / unseen vocabulary objects; explicit per-object novel/seen breakdown beyond Val Unseen aggregate is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>The paper freezes the stage-1 query proposal (local query extractor) during later training stages, but it does not present a controlled frozen-vs-finetuned comparison of the underlying visual encoders (DINO/CLIP) in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Authors demonstrate that large-scale VLE pretraining (over one million mixed sim+real trajectories) improves performance compared to no VLE pretraining, but they do not present a controlled sweep varying pretraining dataset scale to quantify scaling laws.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Concatenation of global object and frontier queries into the Spatial Reasoning Transformer followed by cross-attention to current input features and additional cross-attention to CLIP language embeddings; spatial self-attention captures query-query spatial relations. Type embeddings distinguish object vs frontier queries.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No exact sample-efficiency numbers comparing grounding vs non-grounding; paper argues VLE pretraining reduces the sample inefficiency typically faced by RL agents by leveraging large-scale trajectory datasets and behavior mixing but does not present a direct demonstration like 'X demos vs Y demos'.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Jointly optimizing object-centric visual grounding and frontier-based exploration with an explicit dynamic spatial memory (object queries + frontier queries) and a Spatial Reasoning Transformer yields better SR and generalization across open-vocabulary, lifelong, sequential, and QA embodied tasks; VLE pretraining on large-scale mixed sim+real trajectories and the spatial memory are both crucial (ablation shows measurable SR gains); grounding is implemented as cross-attention between CLIP embeddings and object-centric 3D queries rather than relying on offline complete 3D reconstructions, enabling online embodied grounding and sim-to-real transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1977.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1977.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Online Query Rep. Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Online Query Representation Learning (local proposal + refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online module that converts RGB-D frames and FastSAM segments into local object queries with 3D box, mask, open-vocabulary embedding, decoder features and confidence, refined through PQ3D-inspired decoder layers with masked cross-attention and spatial self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Online Query Representation Learning (MTU3D stage-1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extracts pixel-level DINO features pooled by FastSAM segmentation to segment-level 2D representations; projects depth to point cloud and computes segment-level 3D features via sparse conv U-Net; concatenated features passed through an MLP to propose queries, refined by multiple decoder layers with masked cross-attention to local features and a spatial self-attention module; outputs q_L_t = [b_t, m_t, f_t, v_t, s_t].</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>DINO 2D backbone; FastSAM segmentation; sparse convolutional U-Net for 3D.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified in-paper beyond citations to DINO and FastSAM.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Produces object-centric query embeddings (v_t) that are later matched to language via the Spatial Reasoning Transformer; grounding is enabled by producing open-vocabulary embeddings aligned for similarity to language encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Segment-level / object-centric (masks pooled to segments, outputs per-segment queries).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>3D bounding boxes in world coordinates and per-segment 3D point masks.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Supports downstream vision-language navigation and exploration decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used within MTU3D for HM3D-OVON, GOAT-Bench, SG3D, A-EQA.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic sim and real RGB-D streams.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Implicit: improves grounding recall/precision used by decision model; paper reports end-task SR/SPL improvements when this module is trained and used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Stage-1 training described (50 epochs) but per-module numeric metrics not isolated in main text; overall system performance improvements attributed to these learned queries.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation implicitly shows that removing VLE/spatial memory degrades end-task SR (see main MTU3D ablations); a direct ablation isolating only this module is not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Module is core to end-to-end grounding; contribution is implicit in full-model gains reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors comment that geometric similarity is often sufficient for query matching and that some perception challenges come from partial observability; local queries must predict global geometry from partial input (box loss).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>No per-error rates for query-level failures; matching thresholding uses IoU and can set low values to -inf to avoid spurious matches, indicating sensitivity to bounding-box quality.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Stage-1 trained on ScanNet and HM3D RGB-D trajectories; query representations are then used in mixed sim+real VLE pretraining to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Enables open-vocabulary embeddings (v_t) but per-object novel accuracy is not isolated.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Query proposal is trained in stage-1 then frozen for later stages; no frozen-vs-finetuned comparison for the underlying encoders is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Stage-1 trained on ScanNet + HM3D datasets; scale effects not explicitly studied.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Feature pooling by segment (2D+3D), MLP for query proposal, decoder-level cross-attention with masked attention and spatial self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed to work online from streaming RGB-D; no numeric sample-efficiency comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Object-centric online queries combining 2D segmentation priors and 3D sparse features provide rich semantic-spatial tokens that are effective units for grounding when stored in a dynamic spatial memory and queried by language-conditioned reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1977.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1977.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial Memory Bank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Spatial Memory Bank (global queries)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lifelong memory that merges local queries across timesteps using IoU-based matching and exponential moving averages to maintain global object-centric representations used for grounding and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dynamic Spatial Memory Bank (MTU3D)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Maintains global queries Q_G_t by matching incoming local queries to stored global queries via 3D bounding-box IoU; fuses box params, embeddings and scores via EMA and unions masks; preserves history counts n for weighted averaging and enables retrieval of object queries for grounding and frontier selection.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (memory module operating on query outputs from encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Provides persistent object-centric embeddings (v_t and f_t) which are retrieved and attended to by the Spatial Reasoning Transformer for grounding decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Scene-level persistent object queries (3D boxes + masks + embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>3D world-coordinate bounding boxes, fused instance masks, occupancy map-derived frontiers.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Lifelong/multi-episode navigation and grounding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>GOAT-Bench (multi-modal lifelong navigation) and general episodic navigation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic sim and real-world RGB-D streams.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, SPL on lifelong navigation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports large gains on lifelong navigation (e.g., GOAT-Bench Val Seen SR 52.2%) and attributes substantial contribution to spatial memory (Fig.4c ablation shows memory is important), but per-module numeric breakdown is not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Fig.4c shows reduction in navigation performance when spatial memory contribution is removed; explicit numeric values for the ablation are plotted but not fully tabulated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Spatial memory is cited as key for multi-modal lifelong navigation with 'much larger performance gain over baselines' in GOAT-Bench (authors report strong SR/SPL improvements attributable to lifelong memory).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper emphasizes that without a lifelong spatial memory, multi-modal and long-horizon tasks suffer due to forgetting and inability to utilize past observations; occupancy/frontier estimation can also be limited by partial observations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Discussion notes over-exploration avoidance heuristics (visited frontier lists) and termination when no better frontiers exist; failure categories during collection include Invisible, Unreachable, Failure.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Memory stores mixed sim+real observations from pretraining, helping stability across domain shifts; deployed without fine-tuning on a real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Memory enables recalling previously seen novel objects across episodes, which supports lifelong performance gains; explicit per-object novel metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Memory benefits observed when trained with large mixed trajectory dataset (>1M trajectories); no ablation on memory size vs performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>IoU-based matching + exponential moving average for box/feature fusion; mask union for segmentation masks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Enables reuse of past observations across episodes which the authors argue improves effective sample efficiency for lifelong tasks; no numeric factor reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A dynamic spatial memory that stores object-centric 3D queries is critical for integrating grounding and exploration: it enables retrieval of candidate object representations for language matching, supports frontier-driven exploration, and materially improves multi-episode/lifelong navigation performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1977.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1977.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial Reasoning Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial Reasoning Transformer (for unified grounding & exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based decision module that scores concatenated global object queries and frontier queries with cross-attention to current sensory features and cross-attention to language embeddings, producing unified scores S_U_t for grounding vs exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Spatial Reasoning Transformer (MTU3D)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Operates on query tokens formed by concatenating global object queries and frontier queries; each decoder layer performs cross-attention to current input features, an additional cross-attention to CLIP language embedding, followed by spatial self-attention to model inter-query spatial relations; outputs unified score S_U_t for each candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Consumes features from DINO and sparse-conv 3D encoders; uses CLIP for language/image embeddings as cross-attention inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Uses off-the-shelf CLIP/DINO as referenced; paper does not provide their pretraining details.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-attention between query tokens and language embeddings (CLIP), plus cross-attention to current visual features, enabling alignment between stored object-centric representations and natural language goals.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Operates on object- and frontier-level queries (token-level reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Includes explicit coordinate-aware spatial self-attention; uses type embeddings to distinguish object vs frontier tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Decision-making for grounding vs exploration in navigation and QA.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used across HM3D-OVON, GOAT-Bench, SG3D, A-EQA within MTU3D.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulated and real RGB-D streams.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>End-task SR/SPL; decision accuracy implicit via ablations showing VLE and memory importance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Contributes to the reported system-level performance improvements (see MTU3D main metrics); per-module accuracy not separately tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablations (VLE pretraining and memory removal) degrade system SR as reported in Fig.4; transformer-specific removal ablation numbers not separately given in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Enables unified scoring of object vs frontier queries which authors show is essential for balancing grounding and exploration and leads to reported SR gains.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>No separate failure breakdown for transformer decision errors is tabulated; paper discusses general exploration decision issues (optimal vs random frontier selection leading to overfitting).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Transformer trained with mixed sim+real trajectories as part of VLE pretraining to improve robustness to domain variations.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Transformer aligns CLIP embeddings with stored object queries enabling open-vocabulary grounding in unseen conditions as measured by Val Unseen results.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Transformer performance benefits from large VLE pretraining (>1M trajectories) according to ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention to visual features and to CLIP language encodings; concatenation of object+frontier queries with type embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No exact numbers; intended to improve decision learning efficiency via supervised mixture-of-expert/hybrid trajectory collection rather than pure RL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Integrating language cross-attention with spatial self-attention over persistent object and frontier queries yields an effective decision module for online grounding vs exploration that improves success rates and generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1977.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1977.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbodiedSAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmbodiedSAM: Online segment any 3d thing in real time</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced related work that processes streaming RGB-D for online 3D instance segmentation using SAM-based segmentation priors, but lacks active exploration and high-level reasoning according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embodiedsam: Online segment any 3d thing in real time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EmbodiedSAM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Online instance segmentation system that uses image segmentation priors (SAM / FastSAM) to produce 3D instance masks from streaming RGB-D; cited by MTU3D for 2D segmentation priors but noted to lack active exploration/decision layers.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>SAM / FastSAM for segmentation (cited); underlying encoders not detailed in MTU3D.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified in MTU3D text beyond citation to FastSAM work.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Mentioned as producing instance segmentation and 3D masks but not integrated with active grounding/exploration in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Instance-level segmentation (3D masks).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>3D point masks from RGB-D streams.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Perception (online segmentation), not a full grounding+exploration agent in cited usage.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Real-time RGB-D streams / embodied inputs (as per citation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors cite EmbodiedSAM as effective for online segmentation but insufficient for active exploration and high-level reasoning â€” indicating segmentation-only perception is a bottleneck for full embodied grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Segmentation priors are used to pool features into segment-level tokens for query proposal.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Segmentation priors (SAM/FastSAM) are useful for producing object-centric tokens but must be combined with active exploration and language reasoning to perform embodied grounding effectively.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1977.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1977.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uni-NaVid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uni-NaVid: A video-based vision-language-action model for unifying embodied navigation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced baseline (video-based) that attains high SPL on short HM3D-OVON episodes because it can act immediately upon recognizing a target in video frames, illustrating a tradeoff between video-based policies and memory-based grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Uni-navid: A video-based visionlanguage-action model for unifying embodied navigation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Uni-NaVid</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Video-based vision-language-action model that uses temporal visual cues to directly act on recognized targets; cited as a baseline that excels on short-horizon episodes in HM3D-OVON.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Video-based visual encoder (details in original Uni-NaVid paper, cited).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified in MTU3D; cited from Uni-NaVid.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Video recognition-driven direct navigation rather than explicit 3D spatial memory-based grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Video/global frame level (temporal), not necessarily object-centric 3D queries.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit via video frames; does not rely on an explicit persistent 3D spatial memory as MTU3D does.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-language navigation (unified embodied navigation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Evaluated on HM3D-OVON in MTU3D comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Video sequences in simulation (HM3D) / potentially real videos in original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SPL, SR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table reports Uni-NaVid Val Unseen SR ~39.5% and SPL ~19.8 (approx from Tab.3); MTU3D reports higher SR (40.8% Val Unseen) but lower SPL in some short-trajectory settings.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>MTU3D authors use Uni-NaVid's stronger SPL on short trajectories to highlight that video-based instant recognition can outperform memory-based methods for short episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Video-temporal fusion in Uni-NaVid (details in source paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Video-based recognition provides immediate actions and can achieve high efficiency (SPL) on short episodes, but lacks lifelong memory and generalization advantages of memory-grounded agents.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1977.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1977.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLFM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLFM: Vision-Language Frontier Maps for zero-shot semantic navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced baseline that constructs frontier/semantic maps for zero-shot navigation; compared in MTU3D experiments as a modular approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vision-language frontier maps for zero-shot semantic navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VLFM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modular approach that produces vision-language conditioned frontier maps to guide semantic navigation in a zero-shot manner (as cited). MTU3D is compared against VLFM in OVON benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Not detailed in MTU3D (see original VLFM paper).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Frontier maps conditioned on visual-language signals (modular mapping + planner approach) rather than end-to-end object-query retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Map-level / frontier-level semantic maps.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Frontier-based occupancy/semantic map.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Zero-shot semantic navigation / object goal navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Compared on HM3D-OVON in MTU3D experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation (HM3D) / mapping domain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, SPL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>VLFM reported in Tab.3 Val Seen SR ~35.2% (SPL 18.6) per MTU3D comparisons; MTU3D outperforms with 55.0% SR Val Seen.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Modular fusion of language and semantic frontier maps (details in VLFM paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Modular frontier-based mapping is a viable zero-shot approach, but MTU3D's integrated object-query memory + transformer decision yields higher SR and better lifelong performance in comparisons.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Embodiedsam: Online segment any 3d thing in real time <em>(Rating: 2)</em></li>
                <li>Uni-navid: A video-based visionlanguage-action model for unifying embodied navigation tasks <em>(Rating: 2)</em></li>
                <li>Vision-language frontier maps for zero-shot semantic navigation <em>(Rating: 2)</em></li>
                <li>Sceneverse: Scaling 3d vision-language learning for grounded scene understanding <em>(Rating: 2)</em></li>
                <li>Conceptfusion: Open-set multimodal 3d mapping <em>(Rating: 2)</em></li>
                <li>Voxposer: Composable 3d value maps for robotic manipulation with language models <em>(Rating: 1)</em></li>
                <li>HM3D-OVON: A dataset and benchmark for open-vocabulary object goal navigation <em>(Rating: 2)</em></li>
                <li>Goat-bench: A benchmark for multi-modal lifelong navigation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1977",
    "paper_id": "paper-280150952",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "MTU3D",
            "name_full": "Move to Understand 3D",
            "brief_description": "A unified embodied-vision-language framework that jointly learns online object-centric 3D queries, a dynamic spatial memory bank, and a spatial reasoning transformer to balance grounding and frontier-based exploration via Vision-Language-Exploration (VLE) pre-training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MTU3D (Move to Understand 3D)",
            "model_description": "Processes streaming RGB-D + pose to produce segment-level object queries (bounding box, mask, open-vocabulary embedding, decoder feature, confidence). Local queries are merged into a dynamic global spatial memory (global queries). A Spatial Reasoning Transformer takes concatenated global object queries and frontier queries plus CLIP text/image embeddings and scores candidates (object vs frontier) via cross-attention + spatial self-attention; the highest-scoring query is either grounded or used as an exploration waypoint. The system explicitly fuses 2D segmentation priors (FastSAM), 2D DINO features, and 3D sparse-conv features, and uses a planner to execute navigation.",
            "visual_encoder_type": "DINO backbone for 2D features; FastSAM for 2D segmentation priors; sparse-convolutional U-Net (Minkowski-style) for 3D point features; CLIP image/text encoders used to encode goals.",
            "visual_encoder_pretraining": "Paper uses standard pre-trained components (DINO, FastSAM, CLIP) as cited but does not specify pretraining dataset sizes in-paper; DINO/FastSAM/CLIP are used as off-the-shelf backbones referenced in citations.",
            "grounding_mechanism": "Object-centric query matching and cross-attention: object queries (v_t, f_t) from spatial memory attend with CLIP text/image embeddings in a Spatial Reasoning Transformer; a unified score S_U_t produced per candidate query (object or frontier) guides grounding vs exploration. Matching across time uses geometric IoU-based query matching and exponential moving average fusion for global representations.",
            "representation_level": "Object-centric (segment-level) 3D queries with associated 3D bounding boxes, instance masks, and decoder features; also frontier-level point/coordinate queries (3D waypoint-level).",
            "spatial_representation": "Explicit spatial memory: global 3D bounding boxes in a shared world coordinate system, union-fused instance masks (projectable to 3D point masks), occupancy map (occupied/unoccupied/unknown) and frontier waypoints computed on the occupancy map.",
            "embodied_task_type": "Vision-language navigation, multi-modal lifelong navigation, task-oriented sequential navigation, active embodied question answering.",
            "embodied_task_name": "HM3D-OVON (open-vocab navigation), GOAT-Bench (multi-modal lifelong navigation), SG3D (sequential task navigation), A-EQA (active embodied question answering).",
            "visual_domain": "Photorealistic simulation (HM3D via Habitat-Sim) and real-world robot deployment (Jetson Orin + Kinect + mobile base) without real-world fine-tuning.",
            "performance_metric": "Success Rate (SR), Success weighted by Path Length (SPL), s-SR/t-SR (task SR), LLM-SR and LLM-SPL for embodied QA.",
            "performance_value": "HM3D-OVON: Val Seen SR 55.0% / SPL 23.6; Val Unseen SR 40.8% / SPL 12.1. GOAT-Bench (multi-modal lifelong): Val Seen SR 52.2% (SPL 30.5), Val Unseen SR 47.2% (SPL 27.7). SG3D: s-SR 23.8%, t-SR 8.0%, SPL 16.5%. A-EQA (exploration trajectories used with VLMs): GPT-4V + MTU3D trajectory LLM-SR 44.2% / LLM-SPL 37.0%; GPT-4o + MTU3D LLM-SR 51.1% / LLM-SPL 42.6%. Overall reported SOTA relative improvements: +13.7%, +23.0%, +9.1% SR and +2.4%, +13.0%, +6.3% SPL on HM3D-OVON, GOAT-Bench, SG3D respectively.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "The paper reports ablations showing Vision-Language-Exploration (VLE) pretraining improves SR: OVON SR increases from 27.8% (no VLE) to 33.3% (with VLE); GOAT SR from 22.2% to 36.1%; SG3D SR from 22.9% to 27.9%. Spatial memory ablation (Fig.4c) is shown to materially reduce navigation performance, but exact numeric drop for that ablation is only plotted (not tabulated in text).",
            "grounding_improvement": "VLE pretraining (joint grounding+exploration) provides +5.5% SR on OVON (27.8 â†’ 33.3 reported in ablation), +13.9% SR on GOAT, +5.0% SR on SG3D (per ablation summary); overall full model achieves SOTA gains listed above (e.g., +13.7% SR vs prior SOTA on HM3D-OVON).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors identify reliance on static full 3D reconstructions in prior 3D-VL models as a major limitation for embodied agents in partially observable/dynamic settings; they also note video-based approaches can reach short-horizon goals with higher SPL because they can act immediately upon recognition, indicating perception/recognition latency and partial observability as bottlenecks. The paper also highlights potential overfitting when training only on optimal frontier selection.",
            "failure_mode_analysis": "Qualitative and limited quantitative failure analysis: (1) SPL lags behind some video-based methods on short trajectories (authors attribute this to video-based methods being able to directly navigate to targets on short episodes); (2) overfitting can occur if exploration trajectories use only optimal frontier selection (addressed by random/hybrid frontier selection); (3) failures classified during collection: Invisible, Unreachable, Failure (algorithmic categories used in trajectory collection). The paper does not provide a detailed frequency table of grounding-specific error types (e.g., percent failures due to segmentation).",
            "domain_shift_handling": "Trains on a mixed corpus of &gt;1M trajectories combining large-scale simulated HM3D data and real-world RGB-D trajectories and uses an automatic trajectory-mixing strategy (expert + noisy navigation). The model is deployed on a real robot without real-world fine-tuning and is reported to generalize, indicating their mixed sim+real pretraining mitigates sim-to-real drop.",
            "novel_object_performance": "Evaluated in open-vocabulary settings (HM3D-OVON). Val Unseen SR 40.8% (MTU3D) compared to prior baselines (e.g., Uni-NaVid Val Unseen SR ~39.5 in the table), showing competitive handling of novel / unseen vocabulary objects; explicit per-object novel/seen breakdown beyond Val Unseen aggregate is not provided.",
            "frozen_vs_finetuned": "The paper freezes the stage-1 query proposal (local query extractor) during later training stages, but it does not present a controlled frozen-vs-finetuned comparison of the underlying visual encoders (DINO/CLIP) in the paper.",
            "pretraining_scale_effect": "Authors demonstrate that large-scale VLE pretraining (over one million mixed sim+real trajectories) improves performance compared to no VLE pretraining, but they do not present a controlled sweep varying pretraining dataset scale to quantify scaling laws.",
            "fusion_mechanism": "Concatenation of global object and frontier queries into the Spatial Reasoning Transformer followed by cross-attention to current input features and additional cross-attention to CLIP language embeddings; spatial self-attention captures query-query spatial relations. Type embeddings distinguish object vs frontier queries.",
            "sample_efficiency": "No exact sample-efficiency numbers comparing grounding vs non-grounding; paper argues VLE pretraining reduces the sample inefficiency typically faced by RL agents by leveraging large-scale trajectory datasets and behavior mixing but does not present a direct demonstration like 'X demos vs Y demos'.",
            "key_findings_grounding": "Jointly optimizing object-centric visual grounding and frontier-based exploration with an explicit dynamic spatial memory (object queries + frontier queries) and a Spatial Reasoning Transformer yields better SR and generalization across open-vocabulary, lifelong, sequential, and QA embodied tasks; VLE pretraining on large-scale mixed sim+real trajectories and the spatial memory are both crucial (ablation shows measurable SR gains); grounding is implemented as cross-attention between CLIP embeddings and object-centric 3D queries rather than relying on offline complete 3D reconstructions, enabling online embodied grounding and sim-to-real transfer.",
            "uuid": "e1977.0"
        },
        {
            "name_short": "Online Query Rep. Learning",
            "name_full": "Online Query Representation Learning (local proposal + refinement)",
            "brief_description": "An online module that converts RGB-D frames and FastSAM segments into local object queries with 3D box, mask, open-vocabulary embedding, decoder features and confidence, refined through PQ3D-inspired decoder layers with masked cross-attention and spatial self-attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Online Query Representation Learning (MTU3D stage-1)",
            "model_description": "Extracts pixel-level DINO features pooled by FastSAM segmentation to segment-level 2D representations; projects depth to point cloud and computes segment-level 3D features via sparse conv U-Net; concatenated features passed through an MLP to propose queries, refined by multiple decoder layers with masked cross-attention to local features and a spatial self-attention module; outputs q_L_t = [b_t, m_t, f_t, v_t, s_t].",
            "visual_encoder_type": "DINO 2D backbone; FastSAM segmentation; sparse convolutional U-Net for 3D.",
            "visual_encoder_pretraining": "Not specified in-paper beyond citations to DINO and FastSAM.",
            "grounding_mechanism": "Produces object-centric query embeddings (v_t) that are later matched to language via the Spatial Reasoning Transformer; grounding is enabled by producing open-vocabulary embeddings aligned for similarity to language encodings.",
            "representation_level": "Segment-level / object-centric (masks pooled to segments, outputs per-segment queries).",
            "spatial_representation": "3D bounding boxes in world coordinates and per-segment 3D point masks.",
            "embodied_task_type": "Supports downstream vision-language navigation and exploration decisions.",
            "embodied_task_name": "Used within MTU3D for HM3D-OVON, GOAT-Bench, SG3D, A-EQA.",
            "visual_domain": "Photorealistic sim and real RGB-D streams.",
            "performance_metric": "Implicit: improves grounding recall/precision used by decision model; paper reports end-task SR/SPL improvements when this module is trained and used.",
            "performance_value": "Stage-1 training described (50 epochs) but per-module numeric metrics not isolated in main text; overall system performance improvements attributed to these learned queries.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation implicitly shows that removing VLE/spatial memory degrades end-task SR (see main MTU3D ablations); a direct ablation isolating only this module is not tabulated.",
            "grounding_improvement": "Module is core to end-to-end grounding; contribution is implicit in full-model gains reported.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors comment that geometric similarity is often sufficient for query matching and that some perception challenges come from partial observability; local queries must predict global geometry from partial input (box loss).",
            "failure_mode_analysis": "No per-error rates for query-level failures; matching thresholding uses IoU and can set low values to -inf to avoid spurious matches, indicating sensitivity to bounding-box quality.",
            "domain_shift_handling": "Stage-1 trained on ScanNet and HM3D RGB-D trajectories; query representations are then used in mixed sim+real VLE pretraining to improve robustness.",
            "novel_object_performance": "Enables open-vocabulary embeddings (v_t) but per-object novel accuracy is not isolated.",
            "frozen_vs_finetuned": "Query proposal is trained in stage-1 then frozen for later stages; no frozen-vs-finetuned comparison for the underlying encoders is reported.",
            "pretraining_scale_effect": "Stage-1 trained on ScanNet + HM3D datasets; scale effects not explicitly studied.",
            "fusion_mechanism": "Feature pooling by segment (2D+3D), MLP for query proposal, decoder-level cross-attention with masked attention and spatial self-attention.",
            "sample_efficiency": "Designed to work online from streaming RGB-D; no numeric sample-efficiency comparison provided.",
            "key_findings_grounding": "Object-centric online queries combining 2D segmentation priors and 3D sparse features provide rich semantic-spatial tokens that are effective units for grounding when stored in a dynamic spatial memory and queried by language-conditioned reasoning.",
            "uuid": "e1977.1"
        },
        {
            "name_short": "Spatial Memory Bank",
            "name_full": "Dynamic Spatial Memory Bank (global queries)",
            "brief_description": "A lifelong memory that merges local queries across timesteps using IoU-based matching and exponential moving averages to maintain global object-centric representations used for grounding and exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Dynamic Spatial Memory Bank (MTU3D)",
            "model_description": "Maintains global queries Q_G_t by matching incoming local queries to stored global queries via 3D bounding-box IoU; fuses box params, embeddings and scores via EMA and unions masks; preserves history counts n for weighted averaging and enables retrieval of object queries for grounding and frontier selection.",
            "visual_encoder_type": "N/A (memory module operating on query outputs from encoders)",
            "visual_encoder_pretraining": "",
            "grounding_mechanism": "Provides persistent object-centric embeddings (v_t and f_t) which are retrieved and attended to by the Spatial Reasoning Transformer for grounding decisions.",
            "representation_level": "Scene-level persistent object queries (3D boxes + masks + embeddings).",
            "spatial_representation": "3D world-coordinate bounding boxes, fused instance masks, occupancy map-derived frontiers.",
            "embodied_task_type": "Lifelong/multi-episode navigation and grounding tasks.",
            "embodied_task_name": "GOAT-Bench (multi-modal lifelong navigation) and general episodic navigation evaluation.",
            "visual_domain": "Photorealistic sim and real-world RGB-D streams.",
            "performance_metric": "SR, SPL on lifelong navigation benchmarks.",
            "performance_value": "Paper reports large gains on lifelong navigation (e.g., GOAT-Bench Val Seen SR 52.2%) and attributes substantial contribution to spatial memory (Fig.4c ablation shows memory is important), but per-module numeric breakdown is not tabulated.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Fig.4c shows reduction in navigation performance when spatial memory contribution is removed; explicit numeric values for the ablation are plotted but not fully tabulated in the main text.",
            "grounding_improvement": "Spatial memory is cited as key for multi-modal lifelong navigation with 'much larger performance gain over baselines' in GOAT-Bench (authors report strong SR/SPL improvements attributable to lifelong memory).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper emphasizes that without a lifelong spatial memory, multi-modal and long-horizon tasks suffer due to forgetting and inability to utilize past observations; occupancy/frontier estimation can also be limited by partial observations.",
            "failure_mode_analysis": "Discussion notes over-exploration avoidance heuristics (visited frontier lists) and termination when no better frontiers exist; failure categories during collection include Invisible, Unreachable, Failure.",
            "domain_shift_handling": "Memory stores mixed sim+real observations from pretraining, helping stability across domain shifts; deployed without fine-tuning on a real robot.",
            "novel_object_performance": "Memory enables recalling previously seen novel objects across episodes, which supports lifelong performance gains; explicit per-object novel metrics not provided.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Memory benefits observed when trained with large mixed trajectory dataset (&gt;1M trajectories); no ablation on memory size vs performance.",
            "fusion_mechanism": "IoU-based matching + exponential moving average for box/feature fusion; mask union for segmentation masks.",
            "sample_efficiency": "Enables reuse of past observations across episodes which the authors argue improves effective sample efficiency for lifelong tasks; no numeric factor reported.",
            "key_findings_grounding": "A dynamic spatial memory that stores object-centric 3D queries is critical for integrating grounding and exploration: it enables retrieval of candidate object representations for language matching, supports frontier-driven exploration, and materially improves multi-episode/lifelong navigation performance.",
            "uuid": "e1977.2"
        },
        {
            "name_short": "Spatial Reasoning Transformer",
            "name_full": "Spatial Reasoning Transformer (for unified grounding & exploration)",
            "brief_description": "A transformer-based decision module that scores concatenated global object queries and frontier queries with cross-attention to current sensory features and cross-attention to language embeddings, producing unified scores S_U_t for grounding vs exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Spatial Reasoning Transformer (MTU3D)",
            "model_description": "Operates on query tokens formed by concatenating global object queries and frontier queries; each decoder layer performs cross-attention to current input features, an additional cross-attention to CLIP language embedding, followed by spatial self-attention to model inter-query spatial relations; outputs unified score S_U_t for each candidate.",
            "visual_encoder_type": "Consumes features from DINO and sparse-conv 3D encoders; uses CLIP for language/image embeddings as cross-attention inputs.",
            "visual_encoder_pretraining": "Uses off-the-shelf CLIP/DINO as referenced; paper does not provide their pretraining details.",
            "grounding_mechanism": "Cross-attention between query tokens and language embeddings (CLIP), plus cross-attention to current visual features, enabling alignment between stored object-centric representations and natural language goals.",
            "representation_level": "Operates on object- and frontier-level queries (token-level reasoning).",
            "spatial_representation": "Includes explicit coordinate-aware spatial self-attention; uses type embeddings to distinguish object vs frontier tokens.",
            "embodied_task_type": "Decision-making for grounding vs exploration in navigation and QA.",
            "embodied_task_name": "Used across HM3D-OVON, GOAT-Bench, SG3D, A-EQA within MTU3D.",
            "visual_domain": "Simulated and real RGB-D streams.",
            "performance_metric": "End-task SR/SPL; decision accuracy implicit via ablations showing VLE and memory importance.",
            "performance_value": "Contributes to the reported system-level performance improvements (see MTU3D main metrics); per-module accuracy not separately tabulated.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablations (VLE pretraining and memory removal) degrade system SR as reported in Fig.4; transformer-specific removal ablation numbers not separately given in main text.",
            "grounding_improvement": "Enables unified scoring of object vs frontier queries which authors show is essential for balancing grounding and exploration and leads to reported SR gains.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "No separate failure breakdown for transformer decision errors is tabulated; paper discusses general exploration decision issues (optimal vs random frontier selection leading to overfitting).",
            "domain_shift_handling": "Transformer trained with mixed sim+real trajectories as part of VLE pretraining to improve robustness to domain variations.",
            "novel_object_performance": "Transformer aligns CLIP embeddings with stored object queries enabling open-vocabulary grounding in unseen conditions as measured by Val Unseen results.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Transformer performance benefits from large VLE pretraining (&gt;1M trajectories) according to ablations.",
            "fusion_mechanism": "Cross-attention to visual features and to CLIP language encodings; concatenation of object+frontier queries with type embeddings.",
            "sample_efficiency": "No exact numbers; intended to improve decision learning efficiency via supervised mixture-of-expert/hybrid trajectory collection rather than pure RL.",
            "key_findings_grounding": "Integrating language cross-attention with spatial self-attention over persistent object and frontier queries yields an effective decision module for online grounding vs exploration that improves success rates and generalization.",
            "uuid": "e1977.3"
        },
        {
            "name_short": "EmbodiedSAM",
            "name_full": "EmbodiedSAM: Online segment any 3d thing in real time",
            "brief_description": "Referenced related work that processes streaming RGB-D for online 3D instance segmentation using SAM-based segmentation priors, but lacks active exploration and high-level reasoning according to the authors.",
            "citation_title": "Embodiedsam: Online segment any 3d thing in real time",
            "mention_or_use": "mention",
            "model_name": "EmbodiedSAM",
            "model_description": "Online instance segmentation system that uses image segmentation priors (SAM / FastSAM) to produce 3D instance masks from streaming RGB-D; cited by MTU3D for 2D segmentation priors but noted to lack active exploration/decision layers.",
            "visual_encoder_type": "SAM / FastSAM for segmentation (cited); underlying encoders not detailed in MTU3D.",
            "visual_encoder_pretraining": "Not specified in MTU3D text beyond citation to FastSAM work.",
            "grounding_mechanism": "Mentioned as producing instance segmentation and 3D masks but not integrated with active grounding/exploration in the cited work.",
            "representation_level": "Instance-level segmentation (3D masks).",
            "spatial_representation": "3D point masks from RGB-D streams.",
            "embodied_task_type": "Perception (online segmentation), not a full grounding+exploration agent in cited usage.",
            "embodied_task_name": null,
            "visual_domain": "Real-time RGB-D streams / embodied inputs (as per citation).",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors cite EmbodiedSAM as effective for online segmentation but insufficient for active exploration and high-level reasoning â€” indicating segmentation-only perception is a bottleneck for full embodied grounding.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Segmentation priors are used to pool features into segment-level tokens for query proposal.",
            "sample_efficiency": null,
            "key_findings_grounding": "Segmentation priors (SAM/FastSAM) are useful for producing object-centric tokens but must be combined with active exploration and language reasoning to perform embodied grounding effectively.",
            "uuid": "e1977.4"
        },
        {
            "name_short": "Uni-NaVid",
            "name_full": "Uni-NaVid: A video-based vision-language-action model for unifying embodied navigation tasks",
            "brief_description": "Referenced baseline (video-based) that attains high SPL on short HM3D-OVON episodes because it can act immediately upon recognizing a target in video frames, illustrating a tradeoff between video-based policies and memory-based grounding.",
            "citation_title": "Uni-navid: A video-based visionlanguage-action model for unifying embodied navigation tasks",
            "mention_or_use": "mention",
            "model_name": "Uni-NaVid",
            "model_description": "Video-based vision-language-action model that uses temporal visual cues to directly act on recognized targets; cited as a baseline that excels on short-horizon episodes in HM3D-OVON.",
            "visual_encoder_type": "Video-based visual encoder (details in original Uni-NaVid paper, cited).",
            "visual_encoder_pretraining": "Not specified in MTU3D; cited from Uni-NaVid.",
            "grounding_mechanism": "Video recognition-driven direct navigation rather than explicit 3D spatial memory-based grounding.",
            "representation_level": "Video/global frame level (temporal), not necessarily object-centric 3D queries.",
            "spatial_representation": "Implicit via video frames; does not rely on an explicit persistent 3D spatial memory as MTU3D does.",
            "embodied_task_type": "Vision-language navigation (unified embodied navigation tasks).",
            "embodied_task_name": "Evaluated on HM3D-OVON in MTU3D comparisons.",
            "visual_domain": "Video sequences in simulation (HM3D) / potentially real videos in original paper.",
            "performance_metric": "SPL, SR",
            "performance_value": "Table reports Uni-NaVid Val Unseen SR ~39.5% and SPL ~19.8 (approx from Tab.3); MTU3D reports higher SR (40.8% Val Unseen) but lower SPL in some short-trajectory settings.",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "MTU3D authors use Uni-NaVid's stronger SPL on short trajectories to highlight that video-based instant recognition can outperform memory-based methods for short episodes.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Video-temporal fusion in Uni-NaVid (details in source paper).",
            "sample_efficiency": null,
            "key_findings_grounding": "Video-based recognition provides immediate actions and can achieve high efficiency (SPL) on short episodes, but lacks lifelong memory and generalization advantages of memory-grounded agents.",
            "uuid": "e1977.5"
        },
        {
            "name_short": "VLFM",
            "name_full": "VLFM: Vision-Language Frontier Maps for zero-shot semantic navigation",
            "brief_description": "A referenced baseline that constructs frontier/semantic maps for zero-shot navigation; compared in MTU3D experiments as a modular approach.",
            "citation_title": "Vision-language frontier maps for zero-shot semantic navigation",
            "mention_or_use": "mention",
            "model_name": "VLFM",
            "model_description": "Modular approach that produces vision-language conditioned frontier maps to guide semantic navigation in a zero-shot manner (as cited). MTU3D is compared against VLFM in OVON benchmarks.",
            "visual_encoder_type": "Not detailed in MTU3D (see original VLFM paper).",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Frontier maps conditioned on visual-language signals (modular mapping + planner approach) rather than end-to-end object-query retrieval.",
            "representation_level": "Map-level / frontier-level semantic maps.",
            "spatial_representation": "Frontier-based occupancy/semantic map.",
            "embodied_task_type": "Zero-shot semantic navigation / object goal navigation.",
            "embodied_task_name": "Compared on HM3D-OVON in MTU3D experiments.",
            "visual_domain": "Simulation (HM3D) / mapping domain.",
            "performance_metric": "SR, SPL",
            "performance_value": "VLFM reported in Tab.3 Val Seen SR ~35.2% (SPL 18.6) per MTU3D comparisons; MTU3D outperforms with 55.0% SR Val Seen.",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Modular fusion of language and semantic frontier maps (details in VLFM paper).",
            "sample_efficiency": null,
            "key_findings_grounding": "Modular frontier-based mapping is a viable zero-shot approach, but MTU3D's integrated object-query memory + transformer decision yields higher SR and better lifelong performance in comparisons.",
            "uuid": "e1977.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Embodiedsam: Online segment any 3d thing in real time",
            "rating": 2
        },
        {
            "paper_title": "Uni-navid: A video-based visionlanguage-action model for unifying embodied navigation tasks",
            "rating": 2
        },
        {
            "paper_title": "Vision-language frontier maps for zero-shot semantic navigation",
            "rating": 2
        },
        {
            "paper_title": "Sceneverse: Scaling 3d vision-language learning for grounded scene understanding",
            "rating": 2
        },
        {
            "paper_title": "Conceptfusion: Open-set multimodal 3d mapping",
            "rating": 2
        },
        {
            "paper_title": "Voxposer: Composable 3d value maps for robotic manipulation with language models",
            "rating": 1
        },
        {
            "paper_title": "HM3D-OVON: A dataset and benchmark for open-vocabulary object goal navigation",
            "rating": 2
        },
        {
            "paper_title": "Goat-bench: A benchmark for multi-modal lifelong navigation",
            "rating": 2
        }
    ],
    "cost": 0.02451275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation
30 Jul 2025</p>
<p>Ziyu Zhu 
Tsinghua University</p>
<p>State Key Laboratory of General Artificial Intelligence
BIGAI
China</p>
<p>Xilin Wang 
Beihang University</p>
<p>State Key Laboratory of General Artificial Intelligence
BIGAI
China</p>
<p>Yixuan Li 
Beijing Institute of Technology</p>
<p>Zhuofan Zhang 
Tsinghua University</p>
<p>State Key Laboratory of General Artificial Intelligence
BIGAI
China</p>
<p>Xiaojian Ma 
State Key Laboratory of General Artificial Intelligence
BIGAI
China</p>
<p>Yixin Chen 
State Key Laboratory of General Artificial Intelligence
BIGAI
China</p>
<p>Baoxiong Jia 
State Key Laboratory of General Artificial Intelligence
BIGAI
China</p>
<p>Wei Liang 
Beijing Institute of Technology</p>
<p>State Key Laboratory of General Artificial Intelligence
BIGAI
China</p>
<p>Qian Yu 
Beihang University</p>
<p>Zhidong Deng 
Tsinghua University</p>
<p>Siyuan Huang 
State Key Laboratory of General Artificial Intelligence
BIGAI
China</p>
<p>Qing Li 
State Key Laboratory of General Artificial Intelligence
BIGAI
China</p>
<p>Department of Computer Science
THUAI
Tsinghua University
100084BeijingBNRistChina</p>
<p>Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation
30 Jul 20256E1D1277D1F5A2671D4DA544F969B3F0arXiv:2507.04047v2[cs.CV]Goal Type Image Description Task plan Question Exploring step Start position Grounding step Target position
How many pillows on the bed?Step2: Turn on the Stove.Step3: Take some snacks from Box.Step4: Sit on sofa and relax.</p>
<p>Introduction</p>
<p>Embodied scene understanding requires not only recognizing observed objects but also actively exploring and reasoning in the 3D physical world [8,18,47,93].Imagine stepping into an unfamiliar room with the goal of "find something to eat".As a human, your instinct would be to explore: perhaps heading to the kitchen first, then scanning the countertops, checking the fridge, or even looking around for a dining area.Your search is driven by a seamless combination of commonsense knowledge, spatial reasoning, and visual grounding [8,42,64,76].Similarly, an embodied agent navigating a new environment must operate in a continuous closed-loop cycle of exploration, perception, reasoning, and action [31,64,73].A crucial part of this process is understanding both 3D vision and language [1-3, 9, 48, 85, 88] (3D-VL), enabling the agent to think spatially and make informed decisions about where to explore [32,54,94].</p>
<p>In recent years, we have seen significant progress in the field of 3D-VL [10,29,31,55,93,94].These models leverage 3D reconstructions to perform visual grounding [9,26,69,87], question answering [3,48,92], dense captioning [12], and situated reasoning [31,48].Recent approaches, such as 3DVLP [84], PQ3D [94] and LEO [31], aim to handle multiple tasks within a single architecture by pre-training [84,93] or unified training [5,29,31].</p>
<p>However, existing 3D-VL models rely on static 3D representations [9,63,67], assuming that a complete reconstruction of the environment is available beforehand [10,93].While effective for offline vision language grounding, this assumption is impratical for real-world embodied agents, operating in partially observable and dynamic environments [42,51,64].Moreover, these models typically lack active perception and exploration capabilities [37,79,94].In contrast, reinforcement learning (RL)-based embodied agents can explore environments but often struggle with sample inefficiency [71], poor generalization due to limited training data [20,57,62] and the lack of explicit spatial representation.Bridging passive 3D-VL grounding and active exploration remains a key challenge in developing intelligent systems capable of efficiently exploring and understanding the 3D world.</p>
<p>To develop a 3D-VL model with active perception capabilities, three key challenges must be addressed: First, how to effectively learn online representations from raw RGB-D inputs without costly 3D reconstruction, ensuring rich semantics, spatial awareness, and lifelong memory?Second, the joint optimization of object grounding and spatial exploration remains underexplored.Third, training embodied agents requires large-scale trajectory data for robust exploration strategies, yet collecting diverse real-world trajecto-Figure 2. Our approach bridges online exploration with dynamically spatial memory updates for lifelong grounding.</p>
<p>ries presents significant challenges, and methods for effectively leveraging such data remain an open problem.</p>
<p>To address these challenges, we propose Move to Understand (MTU3D), a unified framework that bridges visual grounding and exploration for versatile embodied navigation as shown in Fig. 2. Our approach introduces three key innovations: 1) Online Query Representation Learning.Our model processes raw RGB-D frames as input.It generates single-frame local queries and writes them to a global spatial memory bank.By leveraging feature extraction and segment priors from 2D foundation models like DINO [6,53] and SAM [39], our query representations capture rich semantics and precise 3D spatial information [73].2) Unified Exploration-Grounding Objective: We introduce a joint optimization framework where unexplored regions are represented as frontier queries.This allows for simultaneous learning of object grounding and exploration.By feeding object queries retrieved from spatial memory bank along with frontier queries [75] detected from the occupancy map [30], we enable a cohesive training process that integrates both tasks.3) End-to-End Vision-Language-Exploration (VLE) Pre-training: We train MTU3D using large-scale trajectory data, combining over a million real-world RGB-D trajectories and simulated data from HM3D in total.To enhance training diversity, we develop an automatic trajectory mixing strategy that blends expert and noisy navigation data [57].After VLE pre-training, MTU3D can be seamlessly transferred to both simulated environments and real-world scenarios for inference.</p>
<p>Extensive experiments on embodied navigation and question-answering benchmarks demonstrate that MTU3D outperforms existing offline modular-based and RL-based approaches, achieving higher exploration efficiency, better generalization to unseen environments, and improved real- time decision-making.Specifically, MTU3D improves the state-of-the-art results by 13.7%, 23.0%, and 9.1% in SR, and 2.4%, 13.0%, and 6.3% in SPL on HM3D-OVON [79], GOAT-Bench [37], and SG3D [87], respectively.When combined with a large vision-language model, serving as its trajectory generator, our approach improves the embodied question answering for LM-SR by 2.4% and LLM-SPL by 29.5%.Furthermore, we deploy the model on a real robot, demonstrating its effectiveness in handling realistic 3D environments.These results highlight the significance of bridging visual grounding and exploration as a crucial step toward efficient, versatile, and generalizable embodied intelligence.</p>
<p>Our main contributions can be summarized as follows: â€¢ We present MTU3D, bridging visual grounding and exploration for efficient and versatile embodied navigation.â€¢ We propose a unified objective that jointly optimizes grounding and exploration, leveraging their complementary nature to enhance overall performance.â€¢ We propose a novel vision-language-exploration training scheme, leveraging large-scale trajectories from simulation and real-world data.â€¢ Extensive experiments validate the effectiveness of our approach, demonstrating significant improvements in exploration efficiency and grounding accuracy across openvocabulary navigation, multi-modal lifelong navigation, task-oriented sequential navigation, and active embodied question-answering benchmarks.</p>
<p>Related work</p>
<p>3D Vision-Language Understanding.In recent years, 3D vision-language (3D-VL) learning [18,31,32,54,93] has attracted significant attention, with focus on grounding language in 3D scenes by understanding spatial relationships [2,68], object semantics [9,24,26,67], and scene structures [35,36,44,69].A wide range of tasks has emerged in this domain, including 3D visual grounding [1,2,9,85], question answering [3,48,88], and dense captioning [12].More recently, the field has expanded to cover intention understanding and task-oriented sequential grounding [87], further pushing the limits of 3D-VL models in complex reasoning [4,58] and interaction [33,45,65].Existing 3D-VL models can be broadly categorized into task-specific models [10,26,27,34,81] with specialized architectures for individual tasks, pretrained models [84,93] that leverage large-scale multi-modal data to improve generalization, and unified models [11,13,29,31,90,94] that seek to handle multiple tasks within a single framework.Despite progress, a key limitation of existing 3D-VL models is their reliance on static 3D representations (e.g., precomputed meshes [63,67] or point clouds [9]), making them unsuitable for real-world embodied AI (EAI), where agents must explore and perceive environments in real-time.</p>
<p>EmbodiedSAM [73] partly addresses this issue by taking streaming RGB-D video as input for online 3D instance segmentation [25,72]; however, it lacks active exploration and high-level reasoning capability.In contrast, our MTU3D framework is proposed as a unified model aiming to simultaneously learn scene representations, exploration strategies, and grounding directly from dynamic spatial memory bank during online RGB-D exploration.</p>
<p>Embodied Navigation and Reasoning.The recent advances in embodied AI, particularly embodied navigation [28,38,61,66] and reasoning [17,42,50,59,64,64], primarily rely on three crucial capabilities: perception, reasoning, and exploration.Several benchmarks are proposed to assess navigation capabilities [37,79] across different goal specifications (images, objects, or language instructions), examine the sequential awareness [87], or questionanswering in an embodied setting [51].Efforts to tackle these challenges generally follow two main approaches [37,46,83,95]: end-to-end reinforcement learning and modular architectures.End-to-end approaches, like PIRLNav [57] and VER [71], utilize RNNs or transformers [19,82] trained directly for navigation tasks, integrating perception and reasoning to take actions.However, its end-to-end nature without explicit 3D representations limits its performance under complex instructions in intricate environments [28,80].In contrast, modular approaches [7,43,59,77,78,80] decompose navigation into specialized components, maintaining distinct models for mapping and navigation policies.More recently, CLIP on Wheels [23,40,56] leverages pre-trained vision-language models to interpret navigation goals without fine-tuning [49].Unlike these methods as in Tab. 1, our model employs a vision-language-exploration training paradigm, actively exploring to construct a scene representation and reasoning within an end-to-end system.</p>
<p>MTU3D</p>
<p>In this section, we present the architecture of our model in Fig. 3 and the training pipeline.We begin by detailing our approach to query representation learning, which extracts object and frontier queries from partial RGB-D sequences and dynamically stores them in a spatial memory bank.Next, we introduce our unified grounding and exploration objective, where queries are processed through a spatial reasoning layer for selection.Finally, we describe our trajectory collection strategy and training procedure.</p>
<p>Online Query Representation Learning</p>
<p>Our model takes as input a partial RGBD sequence spanning an arbitrary time range, denoted as O = [o t1 , o t2 , . . ., o t ].At each timestep t, we extract local queries Q L t , which are subsequently aggregated across the interval t 1 to t 2 to generate global queries Q G t2 that are stored in a memory bank.This process operates in an online manner, enabling flexible processing at any timestep.Our online representation learning framework comprises three essential components, as illustrated below.2D and 3D Encoding.For each input observation o t = [I t , D t , P t ], we process three components: RGB image I t âˆˆ R HÃ—W Ã—3 , depth image D t âˆˆ R HÃ—W , and camera pose P t âˆˆ SE(3).Following ESAM [73], we apply FastSAM [39,89] to segment the image into distinct regions, generating a segment index map S t âˆˆ N HÃ—W where each value represents a region ID.For 2D feature extraction, we process I t through a DINO backbone [6,53] to obtain pixel-level features F 2D t âˆˆ R HÃ—W Ã—C .These features are pooled according to S t , producing segment-level representations [94]
F 2D t âˆˆ R M Ã—C ,
with M denoting the number of segments identified by FastSAM.Each segment corresponds to a coarse image region associated with a group of pixels and their respective 3D points.</p>
<p>For 3D feature extraction, we project the depth image D t into a point cloud and uniformly downsample it to N points P t âˆˆ R N Ã—3 , which is processed through a sparse convolutional U-Net [14,15,63] to generate 3D features F 3D t âˆˆ R N Ã—C , with C being the feature dimension.We then perform segment-wise pooling [22]
using S t , resulting in segment-level 3D representations F 3D t âˆˆ R M Ã—C .
Local Query Proposal.After obtaining 2D and 3D features, we generate object queries through a multi-layer perceptron:
Q t = MLP( F 2D t , F 3D t ) âˆˆ R M Ã—C .
These initial queries are refined via PQ3D-inspired [94,96] decoder layers, producing local output queries Q L t .Each refined local query q L t âˆˆ Q L t comprises multiple components:
q L t = [b t , m t , f t , v t , s t ]
, where b t âˆˆ R 6 encodes a 3D bounding box in global coordinates, m t âˆˆ R M represents segment-level mask predictions for instance segmentation, v t âˆˆ R C captures open-vocabulary feature embeddings for semantic alignment, f t âˆˆ R C contains output features from the query decoder, and s t âˆˆ R denotes a confidence score indicating detection reliability.Each mask in shape M can be converted to a 2D mask (H Ã— W ) or a single frame 3D point cloud mask (N points).As the input point cloud is transformed into world coordinates using pose information, the predicted b t exists in a shared coordinate system, enabling subsequent query merging operations.</p>
<p>Dynamic Spatial Memory Bank.We merge local queries with historical queries from our spatial memory bank [52,73,91] by calculating bounding box IoU between current local queries Q L t and previous global queries Q G tâˆ’1 , generating updated global queries Q G t , which contains f t and v t for further grounding and exploration.For matched query pairs, we employ a fusion strategy where bounding box parameters b t , feature embeddings f t , semantic vectors v t , and confidence scores s t are combined using exponential mov-ing averages, while instance segmentation masks m t are fused through a union operation following [73].To identify unexplored regions, we maintain an occupancy map M âˆˆ R XÃ—Y Ã—3 that categorizes spatial segments as occupied, unoccupied, or unknown.Central to our exploration approach is the concept of frontiers, denoted as Q F t , which represent boundaries between explored (known) and unexplored (unknown) areas.These frontiers are identified by traversing the boundaries of the explored space and detecting adjacent unknown areas.Each element of Q F t corresponds to a coordinate in 3D space R 3 , serving as a potential exploration target [75].These frontier waypoints are periodically recalculated upon reaching each target position, directing autonomous agents to explore unmapped regions.</p>
<p>Unified Grounding and Exploration</p>
<p>Our unified approach combines object grounding and exploration in a single decision framework.Given a natural language goal L (e.g., "find a red chair"), our model decides between grounding an object from current global queries Q G t or selecting a frontier from Q F t for exploration.A spatial reasoning transformer, described in detail in the appendix, integrates language instructions, object representations, and frontier information to produce a unified score
S U t = f (Q G t , Q F t , L)
for each candidate decision.Frontier points are encoded through a simple two-layer MLP before being fed into the transformer.The input for Q G t consists of f t and v t .We incorporate type embeddings to distinguish between object and frontier queries.For language goals, we use a CLIP text encoder [56], while imagebased goals employ the CLIP image encoder [41,56].These embeddings are projected into the transformer's feature space for cross-attention with query representations.The final decision selects the query with the highest score:
q * = arg max qiâˆˆQ G t âˆªQ F t S U t (q i ). If q * âˆˆ Q G t
, the system grounds the corresponding object; if q * âˆˆ Q F t , it navigates to that frontier location.We employ the Habitat-Sim shortest path planner to generate local trajectories.This mechanism dynamically balances exploration and grounding based on current scene knowledge and goal [8].</p>
<p>Trajectory Data Collection</p>
<p>Our unified model training requires diverse trajectory data, which is difficult to collect manually [57,70].We implement a systematic collection process spanning simulated and real environments, combining visual grounding data and exploration data as shown in Tab. 2. Visual Grounding Trajectory.Our visual grounding data follows the structure (Object Q G t , Language L) âˆ’â†’ (Decision S U t ).Leveraging offline RGB-D videos in Scan-Net [16,60] as trajectories, we can directly use these paired examples for pre-training.Each sample associates object queries with language descriptions to generate decisions.Exploration Trajectory.Exploration data follows the format (Object
Q G t , Frontier Q F t , Goal G) âˆ’â†’ (Decision S U t )
, with frontiers changing during exploration [75].Training with only optimal frontiers (closest to target) leads to overfitting.We address this by implementing random frontier selection and a hybrid strategy combining random and optimal approaches [57].We collect trajectories from simulation scans (HM3D [74] via Habitat-Sim [61]) to apply these varied exploration strategies.Exploration succeeds when the target becomes visible and reachable.To prevent unnecessary exploration, we maintain a visited frontier list and only explore when better potential frontiers exist (much closer to target); otherwise, we raise an exception and terminate the current collection.The complete collection process appears in the appendix.Lvocab is cosine similarity loss.This approach ensures that output local queries Q L t effectively capture spatial, semantic, and confidence information.Stage 2: Vision-Language-Exploration Pre-training.In VLE pre-training, we utilize stage 1 output queries to jointly train exploration and grounding.Using the dataset in Tab. 2, we train our decision model on over one million trajectories.The unified decision scores S U t are optimized with binary cross-entropy loss, teaching the model to assign higher scores to appropriate query locations based on the current state and goal.Stage 3: Task-Specific Navigation Fine-tuning.During this stage, we employ the same objective as stage 2 and fine-tune our MTU3D to specific navigation trajectories, optimizing performance for targeted deployment scenarios.</p>
<p>Vision-Language-Exploration Training</p>
<p>Experiment</p>
<p>Experimental setting</p>
<p>Dataset and Benchmarks.We evaluate on diverse benchmarks: GOAT-Bench [37] (multi-modal lifelong navigation), HM3D-OVON [79] (open-vocabulary navigation), SG3D [87] (sequential task navigation), and A-EQA [51] (embodied question answering).Common metrics include Success Rate (SR = Nsuccess Ntotal ) and Success weighted by Path Length (SPL = 1 Ntotal Ntotal i=1 S i â€¢ li max(pi,li) ), where S i indicates success, l i is shortest path length, and p i is agent path length.SG3D uses task SR (t-SR) to measure step coherence.A-EQA employs LLM match score (LLM-SR) and score averaged by exploration length (LLM-SPL).We compare against a diverse range of baseline methods, including modular approaches such as GOAT [37] and VLFM [78], end-to-end reinforcement learning approaches [57,79], and video-based approaches [83].Implementation Details.In Stage 1, we train for 50 epochs using AdamW (learning rate 1e-4, Î² 1 = 0.9, Î² 2 = 0.98) with loss weights Î» b = 1.0, Î» m = 1.0, Î» v = 1.0, Î» s = 0.5.Stages 2 and 3 use identical optimizer settings for 10 epochs each.Both Stages 1 and 2 use 4 transformer layers.Query proposal is trained in stage 1 then frozen, and spatial reasoning is trained in later stages.All training runs on four NVIDIA A100 GPUs around 164 GPU hours.For simulation evaluation, we follow [37,79,87] using Stretch embodiment (1.41m tall, 17cm base radius), processing 360Ã—640 RGB images I t , depth maps D t , and pose P t with actions: MOVE FORWARD(0.25m),TURN LEFT, TURN RIGHT, LOOK UP, and LOOK DOWN.Spatial reasoning is activated upon arrival at each target position.We subsample 18 frames along the trajectory between consecutive target positions.For A-EQA [51], our model is used solely to generate the exploration trajectory and collect the corresponding video for each question.</p>
<p>Quantitative Results</p>
<p>Open Vocabulary Navigation.The results in Tab. 3 demonstrate that our proposed MTU3D significantly outperforms all baselines in terms of SR across both Val Seen and Val Unseen settings.Notably, MTU3D achieves the highest SR in Val Unseen (40.8%), showcasing its strong generalization ability to unseen episodes.This highlights the effectiveness of our approach in handling unseen scenarios compared to prior methods such as RL and behavior cloning (BC), which are not pre-trained on large-scale language data.However, we observe that MTU3D's SPL is lower than Uni-Navid, especially in the Val Synonyms and Val Unseen settings.Given that the HM3D-OVON dataset consists of relatively short trajectories, video-based models inherently gain an advantage because they can directly navigate to the goal upon recognizing a target.</p>
<p>Qualitative results</p>
<p>These qualitative results in Fig. 5 showcase the agent's ability to navigate and complete diverse goal types, including language, image, description, and task planning goals.The trajectories illustrate how the agent efficiently locates objects based on visual and semantic cues, demonstrating its capability to understand both image-based and textual instructions.Notably, for task planning goals, the agent follows a structured sequence of actions.</p>
<p>Goal: TV</p>
<p>Goal: Couch that is located next to the table and the flower vase.</p>
<p>Goal: Change all bathroom towels.1. Remove the towels from rack 2. Retrieve fresh towels from the shelf.3. Hang the fresh towels.</p>
<p>Goal:</p>
<p>Real World Testing</p>
<p>We evaluate MTU3D in realistic 3D environments by deploying it on an NVIDIA Jetson Orin with a Kinect [86] for real-time RGB-D data and a mobile robot equipped with Lidar for exploration.Without any real-world fine-tuning, we test the model in three diverse scenes: home, corridor, and meeting room.As shown in Fig. 6, MTU3D effectively navigates to the target position.Since MTU3D is trained on both simulated and real data, it overcomes the Sim-to-Real transfer challenges commonly faced in RL-based methods.This ability not only enhances its real-world applicability but also makes it highly scalable and impactful for advancing embodied intelligence in the future.</p>
<p>Conclusions</p>
<p>In this paper, we introduce Move to Understand (MTU3D), a unified framework that bridges visual grounding and exploration to advance embodied scene understanding.By jointly optimizing grounding and exploration, MTU3D enables efficient navigation across diverse input modalities, fostering a deeper understanding of spatial environments.Our Vision-Language-Exploration (VLE) training leverages large-scale trajectories, achieving state-of-the-art performance on multiple Embodied AI benchmarks.Experimental results highlight the crucial role of spatial memory in enabling lifelong multi-modal navigation and spatial intelligence, allowing agents to reason about and adapt to complex environments.Furthermore, real-world deployment demonstrates MTU3D 's generalization ability, validating the effectiveness of our mixed training with both simulation and real-world data.By bridging visual grounding and exploration, MTU3D paves the way for more capable, scalable, and generalizable embodied agents, bringing us closer to the goal of truly intelligent embodied AI.</p>
<p>[95] Filippo Ziliotto, Tommaso Campari, Luciano Serafini, and Lamberto Ballan.</p>
<p>A. More Implementation Details</p>
<p>Local Query Refinement.After obtaining the initial queries, we refine them through a series of decoder layers inspired by PQ3D.Specifically, within each decoder layer l, our goal is to better retrieve object-relevant information by enhancing the interaction between the object queries Q l t and the input features { F 2D t , F 3D t }.To achieve this, we first employ a cross-attention mechanism, allowing the object queries Q l t to attend to the input features { F 2D t , F 3D t }.This step is crucial for aggregating relevant information from both 2D and 3D features.</p>
<p>To further improve the efficiency and performance of the cross-attention, we adopt a masked attention mechanism.This restricts the attention scope to localized features centered around each query, ensuring that the queries focus only on relevant regions rather than the entire feature map.This approach not only enhances the model's ability to capture fine-grained details but also significantly reduces computational overhead by limiting the attention span.</p>
<p>Following the cross-attention, we introduce a spatial self-attention module.This module leverages the coordinates of the queries to explore their spatial relationships, enabling the model to better understand the positional context of each query.By incorporating spatial information, the model can more effectively distinguish between different objects and their locations within the scene.Each attention layer is followed by a forward feed network (FFN) and a normalization layer to stabilize the training process and enhance the representation learning capabilities of the model.The entire process within a single decoder layer can be formulated as follows:
Q l â€² t = FFN Norm Q l t + F âˆˆFin CrossAttn(Q l t , F )(1)Q l+1 t = FFN Norm SpatialSelfAttn(Q l â€² t )(2)
After L decoder layers, the refined local queries Q L t are expected to effectively capture the current observations.These refined queries encapsulate both the spatial and semantic information from the input features, making them highly representative of the objects within the scene.Query Matching and Fusion.In our approach, we introduce a Dynamic Spatial Memory Bank to efficiently manage and update the queries across steps.Through extensive experiments, we observe that geometric similarity alone is often sufficient for matching queries.This observation motivates our design to leverage the geometric information of object queries to establish correspondences between the current queries and previous queries.Our model is designed to predict global geometry based on partial input, thanks to a box loss function that aligns the predicted bounding boxes with ground-truth global values.This capability is crucial for establishing correspondences between the current and previous queries, even when only partial information is available.</p>
<p>To measure the similarity between the current local queries and previous global queries, we compute the Intersection over Union (IoU) matrix C between the bounding boxes B L t and B G tâˆ’1 :
C = IoU(B L t , B G tâˆ’1 )(3)
Here, IoU(â€¢, â€¢) denotes the element-wise IoU score between two sets of axis-aligned bounding boxes.To ensure robust matching, we set elements in C that are smaller than a predefined threshold Ïµ to âˆ’âˆž.This step effectively filters out low-confidence matches and focuses on high-similarity pairs.</p>
<p>We then perform matching between B L t and B G tâˆ’1 based on the cost matrix âˆ’C.For matched query pairs, we perform fusion to update the global representation.Specifically, instance segmentation masks m t are fused through a union operation to maintain the most comprehensive segmentation information.For other representations, such as bounding box coordinates, we adopt a weighted average fusion strategy: Here, we assume that the j-th current local query is matched with the i-th previous global query.The variable n denotes the number of queries that have been merged into Q G tâˆ’1 [i] so far.This weighted average approach ensures that the global representation is updated smoothly, incorporating new information while retaining historical context.Spatial Reasoning.The Spatial Reasoning Transformer is designed to integrate spatial context and language instructions for effective reasoning.It shares a similar transformer architecture with the local query refinement module but includes additional mechanisms to incorporate language goals.Specifically, the transformer operates on concatenated global and frontier queries, denoted as Q G t and Q F t respectively, to form the initial queries Q 0 t :
B G t [i] = n n + 1 B G tâˆ’1 [i] + 1 n + 1 B L t <a href="4">j</a>Q 0 t = Concat(Q G t , Q F t )(5)
For each decoder layer l, the queries first attend to the input features F in to aggregate spatial information from the current observation.This is achieved through a crossattention mechanism followed by normalization and a feedforward network (FFN):
Q l â€² t = FFN Norm Q l t + F âˆˆFin CrossAttn(Q l t , F )(6)
Next, an additional cross-attention layer is introduced to incorporate the language goal L. This step allows the model to align its spatial reasoning with the provided language instructions:
Q l â€²â€² t = FFN Norm Q l â€² t + CrossAttn(Q l â€² t , L)(7)
Finally, a spatial self-attention layer is applied to capture the spatial relationships among the queries, further refining their representations:</p>
<p>Kinect Camera Jetson Orin
Mobile RobotQ l+1 t = FFN Norm SpatialSelfAttn(Q l â€²â€² t )(8)
Through the process, the Spatial Reasoning Transformer effectively fuses spatial and linguistic information for further exploration decision.</p>
<p>B. Benchmarks and baseline</p>
<p>HM3D-OVON.HM3D-OVON is an open-vocabulary navigation benchmark.Due to its large-scale test set and resource limitations, we random sample 360 episodes for evaluation.For baselines, BC trains the agent using supervised learning on expert trajectories.DAgger involves an expert providing corrective actions online during training.RL, BCRL, and DAgRL represent reinforcement learning from scratch, reinforcement learning initialized with behavior cloning, and reinforcement learning with DAgger, respectively.Uni-Navid is a video-based navigation method, while TANGO is a training-free navigation approach that leverages large language models.Goat-Bench.Goat-Bench evaluate multi-modal life long navigation, goals include image, class, and description, due to the large-scale test set and resource limitations, we random sample 90 tasks for evaluation.Modular Goat and Modular clip one wheels represent module approaches using pre-trained detector and feature for zeroshot navigation.SenseAct-NN Skill Chain and SenseAct-NN Monotholic are RL approaches, with single head or multi head for each goat type.SG3D.Sequential Navigation requires an agent to navigate to a target object in a specified order within a 3D simulation environment.The Embodied Video Agent is a modular approach that incorporates persistent memory and utilizes a large language model as the planner.SenseAct-NN Monolithic is identical to the variant used in Goat-Bench, employing a unified reinforcement learning policy for all goal types.A-EQA.A-EQA evaluates a model's ability to explore an environment in response to a given question.For A-EQA evaluation, our model is solely responsible for generating the exploration trajectory and collecting the corresponding video for each question.The question answering itself is handled by GPT-4o/V.To ensure a fair comparison, we use the same prompts and the same number of video frames as the baseline methods when measuring exploration performance.</p>
<p>C. Trajectory collection</p>
<p>Algorithm 1 outlines our trajectory collection strategy, in which we randomly select each action to simulate the behavior of agents in HM3D.Relying solely on random or ground-truth actions can lead to model overfitting.</p>
<p>Algorithm 1 Explore an Episode Global: explored map, visited frontiers, visible ids END type: Success, Unreachable, Invisible, Failure</p>
<p>Figure 3 .
3
Figure 3.Our proposed model processes RGB-D sequences to generate object queries, which are stored in a memory bank.The spatial reasoning layer then selects either object queries or frontier points for exploration.These selected query locations are passed to a trajectory planner that guides navigation and facilitates the acquisition of new RGB-D sequences, creating a continuous perception-action loop.</p>
<p>Stage 1 :
1
Low-level Perception Training.We utilize RGB-D trajectories from ScanNet and HM3D to train query representation with instance segmentation loss.Our loss function combines multiple components: L = Î» b Lbox + Î» m Lmask+Î» v Lvocab+Î» s Lscore.Lbox is the 3D box IoU loss; Lmask and Lscore are binary cross-entropy losses;</p>
<p>Usefulness of spatial memory.</p>
<p>Figure 4 .
4
Figure 4. Ablation studies showing (a) the impact of vision-language-exploration pretraining, (b) exploration efficiency on seen environments, and (c) the contribution of spatial memory to navigation performance.</p>
<p>Figure 5 .
5
Figure 5. Visualization of results in Habitat-Sim.</p>
<p>Figure 6 .
6
Figure 6.Real World Testing.</p>
<p>Figure 7 .
7
Figure 7. Real world trajectory.</p>
<p>Figure 8 .
8
Figure 8. Real Device.</p>
<p>Table 1 .
1
Comparison of different paradigms.MTU3D uniquely integrates advantages from both sides, supporting online exploration and lifelong visual grounding.
MethodOnline Exploration Grounding LifelongRLâœ“âœ“âœ—âœ—3D-VLâœ—âœ—âœ“âœ“MTU3D (Ours)âœ“âœ“âœ“âœ“</p>
<p>Table 2 .
2
Data source statistic for Vision-Language-Exploration Pre-training.Sim denotes simulation, VG denotes visual grounding, Exp denotes exploration, Traj denotes trajectory, Dec denotes decision and Goal denotes number of goals.
SourceScanSim Real VG Exp TrajDecGoalScanRefer [9]ScanNetâœ—âœ“âœ“âœ—120237k37kScanQA [3]ScanNetâœ—âœ“âœ“âœ—120226k30kMulti3DRefer [85]ScanNetâœ—âœ“âœ“âœ—120244k44kSG3D-VG-HM3D [87]HM3Dâœ“âœ—âœ“âœ—14530k30kSG3D-VG-ScanNet [87] ScanNetâœ—âœ“âœ“âœ—120213k13kNr3D [2]ScanNetâœ—âœ“âœ“âœ—120230k30kSceneVerse-HM3D [36]HM3Dâœ“âœ—âœ“âœ—14548k48kHM3D-OVON [79]HM3Dâœ“âœ—âœ—âœ“290k 1940k290kGOAT-Bench [37]HM3Dâœ“âœ—âœ—âœ“680k 14119k 5098kSG3D-Nav [87]HM3Dâœ“âœ—âœ—âœ“2k34k11k</p>
<p>Table 3 .
3
[79]-vocab navigation results on HM3D-OVON[79].
Val SeenVal Seen SynonymsVal UnseenMethodSR (â†‘) SPL (â†‘) SR (â†‘) SPL (â†‘) SR (â†‘) SPL (â†‘)BC11.14.59.93.85.41.9DAgger18.19.415.07.410.24.7RL39.218.727.811.718.67.5BCRL20.28.215.25.38.02.8DAgRL41.321.229.414.418.37.9VLFM35.218.632.417.335.219.6Uni-NaVid41.321.143.921.839.519.8TANGOâˆ’âˆ’âˆ’âˆ’35.519.5DAgRL+OD38.521.139.021.437.119.9MTU3D (Ours)55.023.645.014.740.812.1Task-oriented Sequential Navigation. Tab. 4 presents re-sults on task-oriented sequential navigation, a challengingtask requiring an embodied agent to understand relation-ships between task steps. The results show that MTU3Dachieves the highest s-SR (23.8%), t-SR (8.0%), and SPL(16.5%) on the SG3D benchmark, demonstrating its effec-tiveness in sequential task execution and task understand-ing. Unlike other benchmarks, SG3D emphasizes task con-sistency across multiple steps, making it more complex.While MTU3D significantly outperforms Embodied VideoAgent [21] and SenseAct-NN Monolithic [37, 87], over-all success rates remain lower than in GOAT-Bench andHM3D-OVON, highlighting SG3D's inherent difficulty inrequiring both navigation and sustained task accuracy.s-SR(â†‘) t-SR(â†‘) SPL(â†‘)Embodied Video Agent14.73.810.2SenseAct-NN Monolithic12.17.710.1MTU3D (Ours)23.88.016.5</p>
<p>Table 4 .
4
[87]ential task navigation results on SG3D-Nav[87].
Multi-modal Lifelong Navigation. The results in Tab. 5highlight the significant performance improvement of ourMTU3D over baseline methods in lifelong setting. No-tably, MTU3D achieves the highest SR across all settings,with 52.2% in Val Seen, 48.4% in Val Seen Synonyms, and47.2% in Val Unseen, demonstrating its superior navigationcapability. Compared to open-vocabulary navigation, multi-modal lifelong navigation is a more challenging task due tothe need for continuous spatial memory and long-term rea-soning. Our model's lifelong spatial memory allows it toretain and utilize past experiences more effectively, leadingto a much larger performance gain over baselines. Addition-ally, MTU3D achieves the highest SPL across all settings,
with 30.5% in Val Seen and 27.7% in Val Unseen, indicating that it not only reaches the goal more accurately but also follows more efficient trajectories.This suggests that our approach effectively balances success rate and efficiency,</p>
<p>Table 5 .
5
[37]i-modal lifelong navigation results on GOAT-Bench[37].
a crucial factor in lifelong navigation. Overall, these re-sults emphasize that lifelong spatial memory is key to multi-modal navigation, and MTU3D's substantial improvementsvalidate its effectiveness in handling this complex task.Active Embodied Question Answering. Tab. 6 demon-strate that our MTU3D-enhanced GPT-4V significantly out-performs the baseline GPT-4V model, achieving 44.2%LLM-SR vs. 41.8% and a much higher LLM-SPL of 37.0%vs. 7.5%. This indicates that our approach enables a moreefficient trajectory, avoiding the exhaustive search across alllocations that baseline models rely on. Furthermore, GPT-4o with MTU3D achieves even better performance, reach-ing 51.1% LLM-SR and 42.6% LLM-SPL.MethodLLM-SR(â†‘) LLM-SPL(â†‘)Blind LLMsGPT-435.5N/ALLaMA-229.0N/ALLM with captionsGPT-4 w/ LLaVA-1.538.17.0LLaMA-2 w/ LLaVA-1.530.95.9VLMsGPT-4V41.87.5VLMs with MTU3D TrajectoryGPT-4V (Ours)44.237.0GPT-4o (Ours)51.142.6</p>
<p>Table 6 .
6
[51]died question answering results on A-EQA[51].
4.3. DiscussionsDoes Vision-Langauge-Exploration Pe-training benefitnavigation? The results in the Fig. 4a show that Vision-Language Exploration (VLE) Pre-training significantly im-proves navigation performance, as indicated by the SRacross all datasets. Specifically, SR increases from 27.8% to33.3% in OVON, 22.2% to 36.1% in GOAT, and 22.9% to27.9% in SG3D, demonstrating a consistent benefit of VLEacross different task settings and distribution.</p>
<p>Table 7 .
7
Model speed and parameter metrics, results are average from 5 runs across multiple frames and episodes on 3090 Ti.</p>
<p>Tango: Training-free embodied ai agents for open-world tasks.arXiv preprint arXiv:2412.10402,2024.3 [96] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,
Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang,Lu Yuan, et al. Generalized decoding for pixel, image, andlanguage. In The IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 15116-15127, 2023.4</p>
<p>Given the current local queries Q L t and previous global queries Q G tâˆ’1 , we can obtain their respective bounding boxes: the current local bounding boxes B L t âˆˆ R M Ã—6 and the historical global bounding boxes B G tâˆ’1 âˆˆ R N Ã—6 .Here, M and N represent the number of local and global queries, respectively.</p>
<p>1 :
1
function EXPLORE AN EPISODE(strategy, goals)
2:decision list â† []3:while not END do4:Spin and Update5:goal â† select closest goal in goals6:visible â† goal in visible ids7:reachable â† goal in explored map8:better chance â† exist frontier closer to goal9:if visible and reachable then10:Record decision and Goto goal, Success11:else if better chance then12:Record decision and Goto next frontier13:else if visible but not reachable then14:Unreachable15:else if reachable but not visible then16:</p>
<p>Invisible 17 :
17
else if not visible and not reachable then
18:Failure19:end if20:end while21:status = END type22:return (decision list, status)23: end function
Acknowledgements.This work was supported in part by the National Science Foundation of China (NSFC) under Grant No. 62176134 and by a grant from the Assisted Medical Consultation Project Based on DeepSeek.
Scanents3d: Exploiting phrase-to-3d-object correspondences for improved visiolinguistic models in 3d scenes. Ahmed Abdelreheem, Kyle Olszewski, Hsin-Ying Lee, Proceedings of Winter Conference on Applications of Computer Vision (WACV). Winter Conference on Applications of Computer Vision (WACV)202423Peter Wonka, and Panos Achlioptas</p>
<p>Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas Guibas, European Conference on Computer Vision (ECCV). 20205</p>
<p>Scanqa: 3d question answering for spatial scene understanding. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 202235</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning (CoRL). 2022</p>
<p>3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, Dong Xu, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022</p>
<p>Emerging properties in self-supervised vision transformers. Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, Armand Joulin, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision202124</p>
<p>Object goal navigation using goal-oriented semantic exploration. Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, Russ R Salakhutdinov, Advances in Neural Information Processing Systems (NeurIPS). 2020</p>
<p>Object goal navigation using goal-oriented semantic exploration. Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, Russ R Salakhutdinov, Advances in Neural Information Processing Systems (NeurIPS). 2020335</p>
<p>Scanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu, Chen , Angel X Chang, Matthias NieÃŸner, European Conference on Computer Vision (ECCV). 202035</p>
<p>Language conditioned spatial relation reasoning for 3d object grounding. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev, Advances in Neural Information Processing Systems (NeurIPS). 202223</p>
<p>Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, Tao Chen, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024</p>
<p>Scan2cap: Context-aware dense captioning in rgbd scans. Zhenyu Chen, Ali Gholami, Matthias NieÃŸner, Angel X Chang, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 202123</p>
<p>Unit3d: A unified transformer for 3d dense captioning and visual grounding. Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias NieÃŸner, Angel X Chang, International Conference on Computer Vision (ICCV). 2023</p>
<p>Masked-attention mask transformer for universal image segmentation. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, Rohit Girdhar, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022</p>
<p>4d spatio-temporal convnets: Minkowski convolutional neural networks. Christopher Choy, Junyoung Gwak, Silvio Savarese, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias NieÃŸner, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2017</p>
<p>Embodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2018</p>
<p>A survey of embodied ai: From simulators to research tasks. Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, Cheston Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 6232022</p>
<p>The one ring: a robotic indoor navigation generalist. Ainaz Eftekhar, Luca Weihs, Rose Hendrix, Ege Caglar, Jordi Salvador, Alvaro Herrasti, Winson Han, Eli Vander-Bil, Aniruddha Kembhavi, Ali Farhadi, arXiv:2412.144012024arXiv preprint</p>
<p>Spoc: Imitating shortest paths in simulation enables effective navigation and manipulation in the real world. Kiana Ehsani, Tanmay Gupta, Rose Hendrix, Jordi Salvador, Luca Weihs, Kuo-Hao Zeng, Kunal Pratap Singh, Yejin Kim, Winson Han, Alvaro Herrasti, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024</p>
<p>Embodied videoagent: Persistent memory from egocentric videos and embodied sensors enables dynamic scene understanding. Xiaojian Yue Fan, Rongpeng Ma, Jun Su, Rujie Guo, Xi Wu, Qing Chen, Li, arXiv:2501.003582024arXiv preprint</p>
<p>Efficient graph-based image segmentation. F Pedro, Daniel P Felzenszwalb, Huttenlocher, International Journal of Computer Vision (IJCV). 5942004</p>
<p>Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation. Yitzhak Samir, Mitchell Gadre, Gabriel Wortsman, Ludwig Ilharco, Shuran Schmidt, Song, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>Scaling open-vocabulary image segmentation with imagelevel labels. Golnaz Ghiasi, Xiuye Gu, Yin Cui, Tsung-Yi Lin, European Conference on Computer Vision (ECCV). Springer2022</p>
<p>Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Viewrefer: Grasp the multi-view knowledge for 3d visual grounding. Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li, International Conference on Computer Vision (ICCV). 202323</p>
<p>Transrefer3d: Entity-andrelation aware transformer for fine-grained 3d visual grounding. Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, Si Liu, Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on Multimedia2021</p>
<p>Vln bert: A recurrent visionand-language bert for navigation. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021</p>
<p>3d-llm: Injecting the 3d world into large language models. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan, Advances in Neural Information Processing Systems (NeurIPS). 2023363</p>
<p>A real-time occupancy map from multiple video streams. Adam Hoover, Bent David Olsen, Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No. 99CH36288C). 1999 IEEE International Conference on Robotics and Automation (Cat. No. 99CH36288C)1999</p>
<p>An embodied generalist agent in 3d world. Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang, International Conference on Machine Learning (ICML). 202423</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning (ICML). 202223</p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, Conference on Robot Learning (CoRL). 2023</p>
<p>Bottom up top down detection transformers for language grounding in images and point clouds. Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, Katerina Fragkiadaki, European Conference on Computer Vision (ECCV). 2022</p>
<p>Conceptfusion: Open-set multimodal 3d mapping. Krishna Murthy, Jatavallabhula , Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Alaa Maalouf, Shuang Li, Ganesh Subramanian Iyer, Soroush Saryazdi, Nikhil Varma Keetha, ICRA2023 Workshop on Pretraining for Robotics (PT4R). 2023</p>
<p>Sceneverse: Scaling 3d vision-language learning for grounded scene understanding. Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, Siyuan Huang, European Conference on Computer Vision (ECCV). 20245</p>
<p>Devendra Singh Chaplot, Dhruv Batra, and Roozbeh Mottaghi. Goat-bench: A benchmark for multi-modal lifelong navigation. Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024. 2, 3, 5, 6, 7</p>
<p>Realfred: An embodied instruction following benchmark in photo-realistic environments. Taewoong Kim, Cheolhong Min, Byeonghwi Kim, Jinyeon Kim, Wonje Jeung, Jonghyun Choi, European Conference on Computer Vision. Springer2024</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, International Conference on Computer Vision (ICCV). 202324</p>
<p>Uniclip: Unified framework for contrastive language-image pretraining. Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bumsoo Kim, Seung Hwan Kim, Honglak Lee, Junmo Kim, arXiv:2209.134302022arXiv preprint</p>
<p>Less is more: Clipbert for video-and-language learning via sparse sampling. Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, Jingjing Liu, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021</p>
<p>Behavior-1k: A human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto MartÃ­n-MartÃ­n, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, arXiv:2403.09227202423arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023arXiv preprint</p>
<p>Panoptic segformer: Delving deeper into panoptic segmentation with transformers. Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo, Tong Lu, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, International Conference on Robotics and Automation (ICRA). 2023</p>
<p>. Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, </p>
<p>Navcot: Boosting llm-based vision-andlanguage navigation via learning disentangled reasoning. Xiaodan Liang, arXiv:2403.073762024arXiv preprint</p>
<p>Aligning cyber space with physical world: A comprehensive survey on embodied ai. Yang Liu, Weixing Chen, Yongjie Bai, Xiaodan Liang, Guanbin Li, Wen Gao, Liang Lin, arXiv:2407.068862024arXiv preprint</p>
<p>Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, Siyuan Huang, Sqa3d: Situated question answering in 3d scenes. International Conference on Learning Representations (ICLR). 202323</p>
<p>Zson: Zero-shot object-goal navigation using multimodal goal embeddings. Arjun Majumdar, Gunjan Aggarwal, Bhavika Devnani, Judy Hoffman, Dhruv Batra, Advances in Neural Information Processing Systems. 202235</p>
<p>Embodied question answering in the era of foundation models. Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024</p>
<p>Embodied question answering in the era of foundation models. Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024. 2, 3, 6, 7</p>
<p>Spatial memory. David S Olton, Scientific American. 23661977</p>
<p>Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, arXiv:2304.07193Learning robust visual features without supervision. 202324arXiv preprint</p>
<p>Teach: Task-driven embodied agents that chat. Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, Dilek Hakkani-Tur, AAAI Conference on Artificial Intelligence (AAAI). 202223</p>
<p>Openscene: 3d scene understanding with open vocabularies. Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International Conference on Machine Learning (ICML). PMLR, 2021. 3, 5</p>
<p>Pirlnav: Pretraining with imitation and rl finetuning for objectnav. Ram Ramrakhya, Dhruv Batra, Erik Wijmans, Abhishek Das, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023. 2, 3, 5, 6</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf, 7th Annual Conference on Robot Learning. 2023</p>
<p>Explore until confident: Efficient exploration for embodied question answering. Jaden Allen Z Ren, Anushri Clark, Masha Dixit, Anirudha Itkina, Dorsa Majumdar, Sadigh, arXiv:2403.159412024arXiv preprint</p>
<p>Languagegrounded indoor 3d semantic segmentation in the wild. David Rozenberszki, Or Litany, Angela Dai, European Conference on Computer Vision (ECCV). Springer2022</p>
<p>Habitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 201935</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Mask3d: Mask transformer for 3d semantic instance segmentation. Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, Bastian Leibe, International Conference on Robotics and Automation (ICRA). 202334</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202023</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, International Conference on Computer Vision (ICCV). 2023</p>
<p>Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>OpenMask3D: Open-Vocabulary 3D Instance Segmentation. Elisabetta Ayc Â¸a Takmaz, Robert W Fedele, Marc Sumner, Federico Pollefeys, Francis Tombari, Engelmann, Advances in Neural Information Processing Systems (NeurIPS). 202323</p>
<p>Rio: 3d object instance relocalization in changing indoor environments. Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, Matthias NieÃŸner, International Conference on Computer Vision (ICCV). 2019</p>
<p>Embodiedscan: A holistic multimodal 3d perception suite towards embodied ai. Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202423</p>
<p>Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra, arXiv:1911.00357Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. 2019arXiv preprint</p>
<p>Ver: Scaling onpolicy rl leads to the emergence of navigation in embodied rearrangement. Erik Wijmans, Irfan Essa, Dhruv Batra, Advances in Neural Information Processing Systems (NeurIPS). 2022353</p>
<p>Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences. Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, Federico Tombari, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021</p>
<p>Embodiedsam: Online segment any 3d thing in real time. Xiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu, arXiv:2408.118112024. 2, 3, 4, 5arXiv preprint</p>
<p>Habitat-matterport 3d semantics dataset. Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah Maestre, Xuan Angel, Dhruv Chang, Manolis Batra, Savva, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Frontier-based exploration using multiple robots. Brian Yamauchi, Proceedings of the second international conference on Autonomous agents. the second international conference on Autonomous agents199825</p>
<p>Thinking in space: How multimodal large language models see, remember, and recall spaces. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, Saining Xie, arXiv:2412.141712024arXiv preprint</p>
<p>3d-mem: 3d scene memory for embodied exploration and reasoning. Yuncong Yang, Han Yang, Jiachen Zhou, Peihao Chen, Hongxin Zhang, Yilun Du, Chuang Gan, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Vlfm: Vision-language frontier maps for zero-shot semantic navigation. Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, Bernadette Bucher, International Conference on Robotics and Automation (ICRA). </p>
<p>Hm3d-ovon: A dataset and benchmark for open-vocabulary object goal navigation. Naoki Yokoyama, Ram Ramrakhya, Abhishek Das, Dhruv Batra, Sehoon Ha, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2024. 2, 3, 5, 6</p>
<p>Frontier semantic exploration for visual target navigation. Bangguo Yu, Hamidreza Kasaei, Ming Cao, International Conference on Robotics and Automation (ICRA). 2023</p>
<p>Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, Shuguang Cui, International Conference on Computer Vision (ICCV). 2021</p>
<p>Poliformer: Scaling on-policy rl with transformers results in masterful navigators. Kuo-Hao Zeng, Zichen Zhang, Kiana Ehsani, Rose Hendrix, Jordi Salvador, Alvaro Herrasti, Ross Girshick, Aniruddha Kembhavi, Luca Weihs, arXiv:2406.200832024arXiv preprint</p>
<p>Uni-navid: A video-based visionlanguage-action model for unifying embodied navigation tasks. Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, He Wang, arXiv:2412.06224202436arXiv preprint</p>
<p>Vision-language pre-training with object contrastive learning for 3d scene understanding. Taolin Zhang, Sunan He, Dai Tao, Bin Chen, Zhi Wang, Shu-Tao Xia, arXiv:2305.10714202323arXiv preprint</p>
<p>Multi3drefer: Grounding text description to multiple 3d objects. Yiming Zhang, Zeming Gong, Angel X Chang, International Conference on Computer Vision (ICCV). 202325</p>
<p>Microsoft kinect sensor and its effect. Zhengyou Zhang, IEEE multimedia. 1922012</p>
<p>Task-oriented sequential grounding and navigation in 3d scenes. Zhuofan Zhang, Ziyu Zhu, Junhao Li, Pengxiang Li, Tianxu Wang, Tengyu Liu, Xiaojian Ma, Yixin Chen, Baoxiong Jia, Siyuan Huang, Qing Li, arXiv:2408.040342024. 2, 3, 5, 6arXiv preprint</p>
<p>Towards explainable 3d grounded visual question answering: A new benchmark and strong baseline. Lichen Zhao, Daigang Cai, Jing Zhang, Lu Sheng, Dong Xu, Rui Zheng, Yinjie Zhao, Lipeng Wang, Xibo Fan, Technology. 232022</p>
<p>. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, Jinqiao Wang, arXiv:2306.121562023Fast segment anything. arXiv preprint</p>
<p>Xiaowen Haoyu Zhen, Peihao Qiu, Jincheng Chen, Xin Yang, Yilun Yan, Yining Du, Chuang Hong, Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024arXiv preprint</p>
<p>Dual memory units with uncertainty regulation for weakly supervised video anomaly detection. Hang Zhou, Junqing Yu, Wei Yang, AAAI Conference on Artificial Intelligence (AAAI). 2023</p>
<p>Scanreason: Empowering 3d visual grounding with reasoning capabilities. Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, Xihui Liu, European Conference on Computer Vision (ECCV). Springer2024</p>
<p>3d-vista: Pre-trained transformer for 3d vision and text alignment. Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li, International Conference on Computer Vision (ICCV). 202323</p>
<p>Unifying 3d vision-language understanding via promptable queries. Ziyu Zhu, Zhuofan Zhang, Xiaojian Ma, Xuesong Niu, Yixin Chen, Baoxiong Jia, Zhidong Deng, Siyuan Huang, Qing Li, European Conference on Computer Vision. Springer202434</p>            </div>
        </div>

    </div>
</body>
</html>
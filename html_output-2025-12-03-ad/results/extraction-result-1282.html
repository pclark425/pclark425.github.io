<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1282 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1282</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1282</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-e97ede666148c7893ceb265349a4633adddac459</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e97ede666148c7893ceb265349a4633adddac459" target="_blank">Multi-Information Source Optimization</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work presents a novel algorithm that provides a rigorous mathematical treatment of the uncertainties arising from model discrepancies and noisy observations, and conducts an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques.</p>
                <p><strong>Paper Abstract:</strong> We consider Bayesian optimization of an expensive-to-evaluate black-box objective function, where we also have access to cheaper approximations of the objective. In general, such approximations arise in applications such as reinforcement learning, engineering, and the natural sciences, and are subject to an inherent, unknown bias. This model discrepancy is caused by an inadequate internal model that deviates from reality and can vary over the domain, making the utilization of these approximations a non-trivial task. 
We present a novel algorithm that provides a rigorous mathematical treatment of the uncertainties arising from model discrepancies and noisy observations. Its optimization decisions rely on a value of information analysis that extends the Knowledge Gradient factor to the setting of multiple information sources that vary in cost: each sampling decision maximizes the predicted benefit per unit cost. 
We conduct an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques: it finds designs of considerably higher objective value and additionally inflicts less cost in the exploration process.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1282.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1282.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>misoKG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Information Source Optimization using Cost-sensitive Knowledge Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian optimization algorithm that places a single Gaussian process prior over a true objective and multiple biased/noisy information sources, and adaptively selects which source and design to query by maximizing expected reduction in simple regret per unit cost (cost-sensitive Knowledge Gradient).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>misoKG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model-based Bayesian optimization agent: a single hierarchical Gaussian process models the true objective g(x) and model-discrepancy processes for each information source; the acquisition function is a cost-sensitive extension of the Knowledge Gradient (CKG) that selects (information source ℓ, design x) to maximize expected one-step reduction in the posterior maximum of g divided by query cost c_ℓ(x). Implementation includes discretization of domain for inner maximization, gradient-based outer optimization (multi-start), hyperparameter MAP estimation, and optional parallelization.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization via cost-sensitive Knowledge Gradient (value-of-information maximization)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each round, misoKG computes the posterior GP over (ℓ,x), estimates the expected increase in the posterior maximum of g (simple-regret reduction) after a hypothetical observation from candidate (ℓ,x), divides that expected benefit by the query cost c_ℓ(x), and selects the (ℓ,x) maximizing this ratio (CKG). It uses observed data (past queries and their noisy/bias-corrupted outputs), the GP posterior mean and covariance (including modeled biases), and known/estimated noise and cost functions to decide the next query. Inner maximization is discretized (Latin Hypercube set A) and h(a,b)=E[max_i a_i + b_i Z] - max_i a_i is used to compute expected gain; gradients can be used to optimize x.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Benchmarks: Rosenbrock MISO, MNIST hyperparameter tuning (MNIST/USPS), Assemble-to-Order simulator (ATO)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Black-box objective(s) observed via multiple information sources with unknown, possibly spatially-varying biases (model discrepancy) and observational noise; continuous design spaces (continuous parameters / hyperparameters / inventory targets); stochastic simulators (sampling noise), partially observable true objective (only accessible via queries), cost-varying queries across sources; sources may be correlated or independent in their biases.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by benchmark: Rosenbrock (continuous 2-D design space); Image classification hyperparameter tuning (4 continuous/discrete hyperparameters, experiments on full MNIST (IS0) and smaller USPS dataset (IS1)); Assemble-to-Order (8-D continuous target inventory vector b in [0,20]^8, stochastic simulator with multiple replication counts defining fidelity/cost). Query costs and noise levels vary per source (examples: Rosenbrock config: c0=1000,c1=1; ATO: IS0 cost 17.1, IS1 cost 0.5, IS2 cost 3.9).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Consistently superior gain-per-cost across benchmarks: Rosenbrock (config 1): almost-optimal solution within ~5–10 cheap-source samples (gain vs cumulative cost much higher than baselines); Rosenbrock (config 2): outperforms competitors with larger relative margin under higher noise; Image classification (MNIST/USPS): typically reaches ~7.1% test error on MNIST after ≈80 total information-source queries; Assemble-to-Order: average gain ≈26.1 with average cumulative query cost ≈54.6 over first 150 steps, while obtaining comparable objective requires substantially higher cost for baseline misoEI (misoKG's cost is ≈6.3% of misoEI's cost for comparable score). (Metrics reported as gain over best initial solution, test error %, cumulative query cost units = cost units defined per benchmark.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines (non-CKG strategies) perform worse: misoEI often queries high-fidelity expensive sources and accumulates far greater cost to reach comparable objectives (in ATO misoEI's cost to reach comparable gain is ~16x higher); MTBO+ (cost-sensitive entropy search) is competitive early but attains worse final performance at equivalent cost in some benchmarks (e.g., image classification after ~80 queries and ATO over first 150 steps). Exact baseline numeric curves are given in paper figures (averaged over ≥100 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High: in Rosenbrock misoKG finds near-optimal designs in ~5–10 cheap-source queries for config1; in MNIST hyperparameter tuning ≈80 queries to reach ≈7.1% test error; in ATO it attains average gain=26.1 using ≈54.6 cumulative cost in first 150 steps. Sample-efficiency is reported as gain per cumulative cost and shown to be substantially better than baselines across tested problems.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly balanced by the CKG acquisition: one-step Bayes-optimal tradeoff of expected reduction in simple regret vs query cost (benefit-per-unit-cost). The GP posterior (including modeled biases) quantifies uncertainty; CKG values options that yield larger expected reduction of the posterior max, naturally encouraging exploratory queries when uncertainty about high-value regions is high and switching to higher-fidelity/expensive sources when required to reduce remaining uncertainty near promising designs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>MTBO+ (Multi-Task Bayesian Optimization with cost-sensitive Entropy Search — Swersky et al. 2013, improved acquisition), misoEI (Lam et al. 2015: separate GP per information source fused via Winkler's method and EI with a heuristic cost tradeoff).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) misoKG (single GP + CKG) outperforms state-of-the-art multifidelity methods on three benchmarks in gain-per-cost and final solution quality, consistently using cheap biased sources to zoom and switching to more accurate/noisier sources only when needed; 2) CKG is computable analytically for the model and is one-step Bayes optimal for benefit-per-cost; 3) the single-GP model can represent correlated/independent model discrepancies and thereby exploit cross-source correlations to reduce uncertainty faster than methods that keep separate surrogates; 4) misoKG attains high sample-efficiency and lower cumulative query cost by prioritizing low-cost informative queries; 5) practical considerations such as discretization of inner maximization, gradient-based outer optimization, MAP hyperparameter estimation, and parallelization strategies are provided to make the method scalable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported limitations include: need to discretize the inner maximization (choice of set A can affect performance) and potential local maxima in outer continuous optimization (mitigated via multi-start gradients); computational complexity of enumerative outer-loop (O(M|A|^2 log|A|)) though parallelization reduces impact; reliance on GP modeling assumptions (kernel choice, MAP hyperparameter estimates) and on known/estimable cost and noise functions; not explicitly evaluated in adversarial, highly non-stationary or extremely high-dimensional settings; performance depends on quality and amount of initial data per information source (they used 2.5 points per dimension). The paper does not report catastrophic failure cases but notes practical tradeoffs and modeling assumptions that could limit applicability in some real-world scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Information Source Optimization', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1282.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1282.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTBO+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Task Bayesian Optimization (improved, MTBO+)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task Gaussian process Bayesian optimization approach that models correlations between tasks (information sources) with a task×input product kernel and uses a cost-sensitive variant of Entropy Search (information gain about optimum location normalized by cost) as acquisition function; MTBO+ denotes an improved implementation used as a benchmark in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-task Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MTBO+ (improved Multi-Task Bayesian Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-task GP model (task covariance K_t tensor input covariance K_x) capturing fixed relationships across information sources; acquisition is cost-sensitive Entropy Search (predictive-entropy-reduction about the argmax) with implementation improvements per Hernández-Lobato et al. (as used in Spearmint variants).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization via cost-sensitive Entropy Search (information gain maximization about argmax)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects next (task, x) to maximize expected reduction in entropy over the location of the optimum of the objective, normalized by query cost; uses the multi-task GP posterior to predict cross-source effects. Adaptation uses posterior uncertainty about optimum location to prefer informative queries (often cheap biased sources early).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmarks as in the paper: Rosenbrock MISO, MNIST hyperparameter tuning, Assemble-to-Order simulator</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same partially observable black-box objectives with multiple biased/noisy sources; continuous parameter spaces; stochastic simulators or deterministic functions with model discrepancy between sources.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Benchmarks vary: 2-D Rosenbrock, 4-D hyperparameter tuning, 8-D ATO; costs and noise vary per source as defined in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Competitive with misoKG early in runs on some benchmarks (e.g., image classification initial behavior), but generally attains worse final performance at equal cumulative cost in the paper's experiments; typically obtains slightly worse gain-per-cost than misoKG and is outperformed on ATO and later stages of MNIST tuning (quantitative curves in paper figures averaged over ≥100 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to misoKG, MTBO+ tends to use cheap biased sources but trades off gain vs cost differently and sometimes fails to refine to as good final solutions at the same cost; exact numerical baselines are shown in figures (no single scalar baseline given).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Moderate: performs well early (comparable to misoKG in some first-steps) but requires more cumulative cost to match misoKG's later performance in some experiments (e.g., MNIST after ~80 queries).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration driven by maximizing expected information gain (entropy reduction about argmax) per unit cost; exploitation emerges as posterior concentrates near promising designs reducing entropy gains from other regions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against misoKG and misoEI in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>MTBO+ (cost-sensitive Entropy Search with multi-task GP) is a strong baseline that can exploit correlations across tasks and cheap auxiliary sources; it matches misoKG early on in some settings but is outperformed by misoKG in gain-per-cost and final objective quality on the tested benchmarks, indicating advantages of the CKG one-step Bayes-optimal benefit-per-cost criterion and the single-GP discrepancy modeling in misoKG.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Observed to achieve worse final performance at the same cumulative cost in some benchmarks; relies on product-kernel assumption that inter-task relationships are fixed across the domain (less flexible than misoKG's covariance structure), and uses Monte Carlo approximations for its acquisition (computational/approximation tradeoffs noted).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Information Source Optimization', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1282.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1282.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>misoEI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Information Source Expected Improvement (misoEI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multifidelity surrogate-based approach that models each information source with an independent Gaussian process, fuses posteriors per-design using Winkler's method into an intermediate surrogate, and uses Expected Improvement (EI) to pick designs with a heuristic to choose the information source per query (EI per unit cost heuristic).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multifidelity optimization using statistical surrogate modeling for non-hierarchical information sources</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>misoEI</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains separate GPs for each information source to quantify their per-source uncertainty; fuses these distributions at each candidate design using Winkler's method into a single Gaussian surrogate representing the posterior over g(x); selects x by maximizing EI computed on the fused surrogate, and then selects an information source via a heuristic balancing EI and query cost (two-step heuristic).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization using Expected Improvement on fused surrogates with a heuristic source-selection (EI per unit cost)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Chooses the next design x that maximizes EI on the fused surrogate; then uses a heuristic to select which information source to query for that x, aiming to maximize EI per unit cost. Uses per-source GP posteriors (and their fusion) derived from past observations to guide decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmarks: Rosenbrock MISO, MNIST hyperparameter tuning, Assemble-to-Order simulator</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Multiple biased/noisy information sources approximating a true objective; continuous parameter spaces; simulator stochasticity and significant model discrepancy present (not hierarchical in fidelities).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Benchmarks: Rosenbrock (2-D), Image classification tuning (4-D), ATO (8-D simulator) with varying per-source costs and noise variances (examples given in Table 1 of paper).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Generally less cost-efficient in experiments: misoEI often queries the expensive true simulator more frequently, accumulating substantially higher cumulative query cost to reach comparable gains; in ATO misoKG achieved similar gain with only ≈6.3% of the query cost misoEI required. In Rosenbrock misoEI queried the expensive truth and incurred higher cost than misoKG and MTBO+ (figures in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>When compared to misoKG and MTBO+, misoEI's heuristic source-selection typically leads to heavier use of expensive high-fidelity sources and thus poorer gain-per-cost; precise numeric baselines are shown in paper plots (averaged over many runs).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower sample-efficiency in tested benchmarks due to frequent expensive source queries; requires substantially more cumulative cost to reach comparable objective values than misoKG (e.g., in ATO misoEI's cost to reach comparable gain is ~16x that of misoKG).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>EI on fused surrogate encourages sampling where expected improvement is largest (exploitation of promising designs); source selection heuristic trades EI vs cost but is not one-step Bayes-optimal in benefit-per-cost and may favor expensive accurate evaluations more often, reducing cost-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against misoKG and MTBO+ in paper experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>misoEI (separate-GP + Winkler fusion + EI + heuristic source selection) is a reasonable approach for MISO but in the experiments it is less cost-efficient: it tends to rely on high-fidelity expensive evaluations and thus accumulates higher query cost to reach comparable objective values. The two-step heuristic for source selection is less effective than the CKG's integrated benefit-per-cost optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Heuristic-based source selection can cause over-use of expensive, high-fidelity sources leading to high cumulative cost; fusion-and-two-step approach less able to exploit cross-source correlations globally compared to single-GP approaches, and lacks a provably one-step Bayes-optimal cost-aware acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Information Source Optimization', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multi-task Bayesian Optimization <em>(Rating: 2)</em></li>
                <li>Multifidelity optimization using statistical surrogate modeling for non-hierarchical information sources <em>(Rating: 2)</em></li>
                <li>The Knowledge Gradient Policy for Correlated Normal Beliefs <em>(Rating: 2)</em></li>
                <li>Predicting the output from a complex computer code when fast approximations are available <em>(Rating: 1)</em></li>
                <li>Multi-fidelity gaussian process bandit optimisation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1282",
    "paper_id": "paper-e97ede666148c7893ceb265349a4633adddac459",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "misoKG",
            "name_full": "Multi-Information Source Optimization using Cost-sensitive Knowledge Gradient",
            "brief_description": "A Bayesian optimization algorithm that places a single Gaussian process prior over a true objective and multiple biased/noisy information sources, and adaptively selects which source and design to query by maximizing expected reduction in simple regret per unit cost (cost-sensitive Knowledge Gradient).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "misoKG",
            "agent_description": "A model-based Bayesian optimization agent: a single hierarchical Gaussian process models the true objective g(x) and model-discrepancy processes for each information source; the acquisition function is a cost-sensitive extension of the Knowledge Gradient (CKG) that selects (information source ℓ, design x) to maximize expected one-step reduction in the posterior maximum of g divided by query cost c_ℓ(x). Implementation includes discretization of domain for inner maximization, gradient-based outer optimization (multi-start), hyperparameter MAP estimation, and optional parallelization.",
            "adaptive_design_method": "Bayesian optimization via cost-sensitive Knowledge Gradient (value-of-information maximization)",
            "adaptation_strategy_description": "At each round, misoKG computes the posterior GP over (ℓ,x), estimates the expected increase in the posterior maximum of g (simple-regret reduction) after a hypothetical observation from candidate (ℓ,x), divides that expected benefit by the query cost c_ℓ(x), and selects the (ℓ,x) maximizing this ratio (CKG). It uses observed data (past queries and their noisy/bias-corrupted outputs), the GP posterior mean and covariance (including modeled biases), and known/estimated noise and cost functions to decide the next query. Inner maximization is discretized (Latin Hypercube set A) and h(a,b)=E[max_i a_i + b_i Z] - max_i a_i is used to compute expected gain; gradients can be used to optimize x.",
            "environment_name": "Benchmarks: Rosenbrock MISO, MNIST hyperparameter tuning (MNIST/USPS), Assemble-to-Order simulator (ATO)",
            "environment_characteristics": "Black-box objective(s) observed via multiple information sources with unknown, possibly spatially-varying biases (model discrepancy) and observational noise; continuous design spaces (continuous parameters / hyperparameters / inventory targets); stochastic simulators (sampling noise), partially observable true objective (only accessible via queries), cost-varying queries across sources; sources may be correlated or independent in their biases.",
            "environment_complexity": "Varies by benchmark: Rosenbrock (continuous 2-D design space); Image classification hyperparameter tuning (4 continuous/discrete hyperparameters, experiments on full MNIST (IS0) and smaller USPS dataset (IS1)); Assemble-to-Order (8-D continuous target inventory vector b in [0,20]^8, stochastic simulator with multiple replication counts defining fidelity/cost). Query costs and noise levels vary per source (examples: Rosenbrock config: c0=1000,c1=1; ATO: IS0 cost 17.1, IS1 cost 0.5, IS2 cost 3.9).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Consistently superior gain-per-cost across benchmarks: Rosenbrock (config 1): almost-optimal solution within ~5–10 cheap-source samples (gain vs cumulative cost much higher than baselines); Rosenbrock (config 2): outperforms competitors with larger relative margin under higher noise; Image classification (MNIST/USPS): typically reaches ~7.1% test error on MNIST after ≈80 total information-source queries; Assemble-to-Order: average gain ≈26.1 with average cumulative query cost ≈54.6 over first 150 steps, while obtaining comparable objective requires substantially higher cost for baseline misoEI (misoKG's cost is ≈6.3% of misoEI's cost for comparable score). (Metrics reported as gain over best initial solution, test error %, cumulative query cost units = cost units defined per benchmark.)",
            "performance_without_adaptation": "Baselines (non-CKG strategies) perform worse: misoEI often queries high-fidelity expensive sources and accumulates far greater cost to reach comparable objectives (in ATO misoEI's cost to reach comparable gain is ~16x higher); MTBO+ (cost-sensitive entropy search) is competitive early but attains worse final performance at equivalent cost in some benchmarks (e.g., image classification after ~80 queries and ATO over first 150 steps). Exact baseline numeric curves are given in paper figures (averaged over ≥100 runs).",
            "sample_efficiency": "High: in Rosenbrock misoKG finds near-optimal designs in ~5–10 cheap-source queries for config1; in MNIST hyperparameter tuning ≈80 queries to reach ≈7.1% test error; in ATO it attains average gain=26.1 using ≈54.6 cumulative cost in first 150 steps. Sample-efficiency is reported as gain per cumulative cost and shown to be substantially better than baselines across tested problems.",
            "exploration_exploitation_tradeoff": "Explicitly balanced by the CKG acquisition: one-step Bayes-optimal tradeoff of expected reduction in simple regret vs query cost (benefit-per-unit-cost). The GP posterior (including modeled biases) quantifies uncertainty; CKG values options that yield larger expected reduction of the posterior max, naturally encouraging exploratory queries when uncertainty about high-value regions is high and switching to higher-fidelity/expensive sources when required to reduce remaining uncertainty near promising designs.",
            "comparison_methods": "MTBO+ (Multi-Task Bayesian Optimization with cost-sensitive Entropy Search — Swersky et al. 2013, improved acquisition), misoEI (Lam et al. 2015: separate GP per information source fused via Winkler's method and EI with a heuristic cost tradeoff).",
            "key_results": "1) misoKG (single GP + CKG) outperforms state-of-the-art multifidelity methods on three benchmarks in gain-per-cost and final solution quality, consistently using cheap biased sources to zoom and switching to more accurate/noisier sources only when needed; 2) CKG is computable analytically for the model and is one-step Bayes optimal for benefit-per-cost; 3) the single-GP model can represent correlated/independent model discrepancies and thereby exploit cross-source correlations to reduce uncertainty faster than methods that keep separate surrogates; 4) misoKG attains high sample-efficiency and lower cumulative query cost by prioritizing low-cost informative queries; 5) practical considerations such as discretization of inner maximization, gradient-based outer optimization, MAP hyperparameter estimation, and parallelization strategies are provided to make the method scalable.",
            "limitations_or_failures": "Reported limitations include: need to discretize the inner maximization (choice of set A can affect performance) and potential local maxima in outer continuous optimization (mitigated via multi-start gradients); computational complexity of enumerative outer-loop (O(M|A|^2 log|A|)) though parallelization reduces impact; reliance on GP modeling assumptions (kernel choice, MAP hyperparameter estimates) and on known/estimable cost and noise functions; not explicitly evaluated in adversarial, highly non-stationary or extremely high-dimensional settings; performance depends on quality and amount of initial data per information source (they used 2.5 points per dimension). The paper does not report catastrophic failure cases but notes practical tradeoffs and modeling assumptions that could limit applicability in some real-world scenarios.",
            "uuid": "e1282.0",
            "source_info": {
                "paper_title": "Multi-Information Source Optimization",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "MTBO+",
            "name_full": "Multi-Task Bayesian Optimization (improved, MTBO+)",
            "brief_description": "A multi-task Gaussian process Bayesian optimization approach that models correlations between tasks (information sources) with a task×input product kernel and uses a cost-sensitive variant of Entropy Search (information gain about optimum location normalized by cost) as acquisition function; MTBO+ denotes an improved implementation used as a benchmark in the paper.",
            "citation_title": "Multi-task Bayesian Optimization",
            "mention_or_use": "use",
            "agent_name": "MTBO+ (improved Multi-Task Bayesian Optimization)",
            "agent_description": "Multi-task GP model (task covariance K_t tensor input covariance K_x) capturing fixed relationships across information sources; acquisition is cost-sensitive Entropy Search (predictive-entropy-reduction about the argmax) with implementation improvements per Hernández-Lobato et al. (as used in Spearmint variants).",
            "adaptive_design_method": "Bayesian optimization via cost-sensitive Entropy Search (information gain maximization about argmax)",
            "adaptation_strategy_description": "Selects next (task, x) to maximize expected reduction in entropy over the location of the optimum of the objective, normalized by query cost; uses the multi-task GP posterior to predict cross-source effects. Adaptation uses posterior uncertainty about optimum location to prefer informative queries (often cheap biased sources early).",
            "environment_name": "Same benchmarks as in the paper: Rosenbrock MISO, MNIST hyperparameter tuning, Assemble-to-Order simulator",
            "environment_characteristics": "Same partially observable black-box objectives with multiple biased/noisy sources; continuous parameter spaces; stochastic simulators or deterministic functions with model discrepancy between sources.",
            "environment_complexity": "Benchmarks vary: 2-D Rosenbrock, 4-D hyperparameter tuning, 8-D ATO; costs and noise vary per source as defined in experiments.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Competitive with misoKG early in runs on some benchmarks (e.g., image classification initial behavior), but generally attains worse final performance at equal cumulative cost in the paper's experiments; typically obtains slightly worse gain-per-cost than misoKG and is outperformed on ATO and later stages of MNIST tuning (quantitative curves in paper figures averaged over ≥100 runs).",
            "performance_without_adaptation": "Compared to misoKG, MTBO+ tends to use cheap biased sources but trades off gain vs cost differently and sometimes fails to refine to as good final solutions at the same cost; exact numerical baselines are shown in figures (no single scalar baseline given).",
            "sample_efficiency": "Moderate: performs well early (comparable to misoKG in some first-steps) but requires more cumulative cost to match misoKG's later performance in some experiments (e.g., MNIST after ~80 queries).",
            "exploration_exploitation_tradeoff": "Exploration driven by maximizing expected information gain (entropy reduction about argmax) per unit cost; exploitation emerges as posterior concentrates near promising designs reducing entropy gains from other regions.",
            "comparison_methods": "Compared against misoKG and misoEI in experiments.",
            "key_results": "MTBO+ (cost-sensitive Entropy Search with multi-task GP) is a strong baseline that can exploit correlations across tasks and cheap auxiliary sources; it matches misoKG early on in some settings but is outperformed by misoKG in gain-per-cost and final objective quality on the tested benchmarks, indicating advantages of the CKG one-step Bayes-optimal benefit-per-cost criterion and the single-GP discrepancy modeling in misoKG.",
            "limitations_or_failures": "Observed to achieve worse final performance at the same cumulative cost in some benchmarks; relies on product-kernel assumption that inter-task relationships are fixed across the domain (less flexible than misoKG's covariance structure), and uses Monte Carlo approximations for its acquisition (computational/approximation tradeoffs noted).",
            "uuid": "e1282.1",
            "source_info": {
                "paper_title": "Multi-Information Source Optimization",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "misoEI",
            "name_full": "Multi-Information Source Expected Improvement (misoEI)",
            "brief_description": "A multifidelity surrogate-based approach that models each information source with an independent Gaussian process, fuses posteriors per-design using Winkler's method into an intermediate surrogate, and uses Expected Improvement (EI) to pick designs with a heuristic to choose the information source per query (EI per unit cost heuristic).",
            "citation_title": "Multifidelity optimization using statistical surrogate modeling for non-hierarchical information sources",
            "mention_or_use": "use",
            "agent_name": "misoEI",
            "agent_description": "Maintains separate GPs for each information source to quantify their per-source uncertainty; fuses these distributions at each candidate design using Winkler's method into a single Gaussian surrogate representing the posterior over g(x); selects x by maximizing EI computed on the fused surrogate, and then selects an information source via a heuristic balancing EI and query cost (two-step heuristic).",
            "adaptive_design_method": "Bayesian optimization using Expected Improvement on fused surrogates with a heuristic source-selection (EI per unit cost)",
            "adaptation_strategy_description": "Chooses the next design x that maximizes EI on the fused surrogate; then uses a heuristic to select which information source to query for that x, aiming to maximize EI per unit cost. Uses per-source GP posteriors (and their fusion) derived from past observations to guide decisions.",
            "environment_name": "Same benchmarks: Rosenbrock MISO, MNIST hyperparameter tuning, Assemble-to-Order simulator",
            "environment_characteristics": "Multiple biased/noisy information sources approximating a true objective; continuous parameter spaces; simulator stochasticity and significant model discrepancy present (not hierarchical in fidelities).",
            "environment_complexity": "Benchmarks: Rosenbrock (2-D), Image classification tuning (4-D), ATO (8-D simulator) with varying per-source costs and noise variances (examples given in Table 1 of paper).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Generally less cost-efficient in experiments: misoEI often queries the expensive true simulator more frequently, accumulating substantially higher cumulative query cost to reach comparable gains; in ATO misoKG achieved similar gain with only ≈6.3% of the query cost misoEI required. In Rosenbrock misoEI queried the expensive truth and incurred higher cost than misoKG and MTBO+ (figures in paper).",
            "performance_without_adaptation": "When compared to misoKG and MTBO+, misoEI's heuristic source-selection typically leads to heavier use of expensive high-fidelity sources and thus poorer gain-per-cost; precise numeric baselines are shown in paper plots (averaged over many runs).",
            "sample_efficiency": "Lower sample-efficiency in tested benchmarks due to frequent expensive source queries; requires substantially more cumulative cost to reach comparable objective values than misoKG (e.g., in ATO misoEI's cost to reach comparable gain is ~16x that of misoKG).",
            "exploration_exploitation_tradeoff": "EI on fused surrogate encourages sampling where expected improvement is largest (exploitation of promising designs); source selection heuristic trades EI vs cost but is not one-step Bayes-optimal in benefit-per-cost and may favor expensive accurate evaluations more often, reducing cost-efficiency.",
            "comparison_methods": "Compared against misoKG and MTBO+ in paper experiments.",
            "key_results": "misoEI (separate-GP + Winkler fusion + EI + heuristic source selection) is a reasonable approach for MISO but in the experiments it is less cost-efficient: it tends to rely on high-fidelity expensive evaluations and thus accumulates higher query cost to reach comparable objective values. The two-step heuristic for source selection is less effective than the CKG's integrated benefit-per-cost optimization.",
            "limitations_or_failures": "Heuristic-based source selection can cause over-use of expensive, high-fidelity sources leading to high cumulative cost; fusion-and-two-step approach less able to exploit cross-source correlations globally compared to single-GP approaches, and lacks a provably one-step Bayes-optimal cost-aware acquisition.",
            "uuid": "e1282.2",
            "source_info": {
                "paper_title": "Multi-Information Source Optimization",
                "publication_date_yy_mm": "2016-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multi-task Bayesian Optimization",
            "rating": 2
        },
        {
            "paper_title": "Multifidelity optimization using statistical surrogate modeling for non-hierarchical information sources",
            "rating": 2
        },
        {
            "paper_title": "The Knowledge Gradient Policy for Correlated Normal Beliefs",
            "rating": 2
        },
        {
            "paper_title": "Predicting the output from a complex computer code when fast approximations are available",
            "rating": 1
        },
        {
            "paper_title": "Multi-fidelity gaussian process bandit optimisation",
            "rating": 2
        }
    ],
    "cost": 0.014653,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multi-Information Source Optimization</h1>
<p>Matthias Poloczek, Jialei Wang, and Peter I. Frazier<br>School of Operations Research and Information Engineering Cornell University<br>{poloczek,jw865,pf98}@cornell.edu</p>
<p>We consider Bayesian optimization of an expensive-to-evaluate black-box objective function, where we also have access to cheaper approximations of the objective. In general, such approximations arise in applications such as reinforcement learning, engineering, and the natural sciences, and are subject to an inherent, unknown bias. This model discrepancy is caused by an inadequate internal model that deviates from reality and can vary over the domain, making the utilization of these approximations a non-trivial task.</p>
<p>We present a novel algorithm that provides a rigorous mathematical treatment of the uncertainties arising from model discrepancies and noisy observations. Its optimization decisions rely on a value of information analysis that extends the Knowledge Gradient factor to the setting of multiple information sources that vary in cost: each sampling decision maximizes the predicted benefit per unit cost.</p>
<p>We conduct an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques: it finds designs of considerably higher objective value and additionally inflicts less cost in the exploration process.</p>
<h2>1. Introduction</h2>
<p>We consider the problem of optimizing an expensive-to-evaluate black-box function, where we additionally have access to cheaper approximations of the objective. For instance, this scenario arises when tuning hyper-parameters of machine learning algorithms, e.g., parameters of a neural network architecture or a re-
gression method: one may assess the performance of a particular setting on a smaller related dataset or even a subset of the whole database (e.g., see Swersky et al. [2013], Kandasamy et al. [2016], Klein et al. [2016]). Or consider a ride-sharing company, e.g., Uber or Lyft, that matches clients looking for a ride to drivers. These real-time decisions then could be made by a system whose performance crucially depends on a variety of parameters. These parameters can be evaluated either in a simulation or by deploying them in a real-world experiment. Similarly, the tasks of configuring the system that navigates a mobile robot in a factory, or steers a driverless vehicle on the road, can be approached by invoking a simulator, or by evaluating its performance on a closed test course or even a public road. The approach of exploiting cheap approximations is fundamental to make such tasks tractable.</p>
<p>We formalize these problems as multi-information source optimization problems (MISO), where the goal is to optimize a complex design under an expensive-to-evaluate black-box function. To reduce the overall cost we may utilize cheap approximate estimates of the objective that are provided by so-called "information sources". However, in the general settings we consider their output is not necessarily an unbiased estimator only subject to observational noise, but in addition may be inherently biased: the behavior of an autonomous car on a test course or the performance of a dispatch algorithm in a ride-sharing simulation will differ significantly from the real world. Thus, inevitably we face a model discrepancy which denotes an inherent inability to describe the reality</p>
<p>accurately. We stress that this notion goes far beyond typical "noise" such as measurement errors or numerical inaccuracies. The latter grasps uncertainty that arises when sampling from an information source, and is the type of inaccuracy that most of the previous work on multifidelity optimization deals with, e.g., see Balabanov and Venter [2004], Eldred et al. [2004], Rajnarayan et al. [2008], March and Willcox [2012], Lam et al. [2015] for problem formulations in the engineering sciences. In particular, such an understanding of noise assumes implicitly that the internal value (or state) of the information source itself is an accurate description of the truth. Our more general notion of model discrepancy captures the uncertainty about the truth that originates from the inherent deviation from reality.</p>
<p>Related Work. The task of optimizing a single, expensive-to-evaluate black-box function has received a lot of attention. A successful approach to this end is Bayesian optimization, a prominent representative being the efficient global optimization (EGO) method by Jones et al. [1998]. EGO assumes that there is only a single information source that returns the true value; the goal is to find a global maximum of the objective while minimizing the query cost. It consists of the following two steps that have served as a blueprint for many subsequent Bayesian optimization techniques. First a stochastic, in their case Gaussian, process is formulated and fitted to a set of initial data points. Then they search for a global optimum by iteratively sampling a point of highest predicted score according to some "acquisition criterion": Jones et al. employ expected improvement (EI) that samples a point next whose improvement over the best current design is maximum in expectation. Subsequently, the expected improvement method was extended to deal with observational noise, e.g., see Huang et al. [2006b], Scott et al. [2011], Picheny et al. [2013]. The extension of Picheny et al. also dynamically controls the precision of a measurement, making it well suited for many multifidelity optimization problems.</p>
<p>The strategy of reducing the overall cost of the optimization process by utilizing cheaper approximations of the objective function was successfully employed in a seminal paper by Swersky, Snoek, and Adams [2013], who showed that the task of tuning hyper-parameters for two classification problems can be sped up significantly by evaluating settings on a small sample instead of the whole database. To this end, they proposed a Gaussian process model to quantify the correlation between such an "auxiliary task" and the primary task, building on previous work on Gaussian process regression for multiple tasks in Bonilla et al. [2007], Goovaerts [1997], Teh et al. [2005]: their kernel is given by the tensor product $K_{t} \otimes K_{x}$, where $K_{t}$ (resp., $K_{x}$ ) denotes the covariance matrix of the tasks (resp., of the points). Their acquisition criterion is a cost-sensitive formulation of Entropy Search [Hennig and Schuler, 2012, Villemonteix et al., 2009]: here one samples in each iteration a point that yields a maximum reduction in the uncertainty over the location of the optimum.</p>
<p>Besides, interesting variants of the MISO problem have been studied recently: Kandasamy et al. [2016] examined an alternative objective for multifidelity settings that asks to minimize the cumulative regret over a series of function evaluations: besides classification problems they also presented an application where the likelihood of three cosmological constants is to be maximized based on Supernovae data. Klein et al. [2016] considered hyper-parameter optimization of machine learning algorithms over a large dataset $D$. Supposing access to subsets of $D$ of arbitrary sizes, they show how to exploit regularity of performance across dataset sizes to significantly speed up the optimization process for support vector machines and neural networks.</p>
<p>In engineering sciences the approach of building cheap-to-evaluate, approximate models for the real function, that offer different fidelity-cost trade-offs, is also known as "surrogate modeling" and has gained a lot of popularity (e.g., see the survey Queipo et al. [2005]). Kennedy and O'Hagan [2000] introduced Gaussian process regression to multifidelity optimization to optimize a design given several computer codes that vary in accuracy and computational complexity. Contrasting the related work discussed above, these articles consider model discrepancy, but impose several restrictions on its nature: a common constraint in multifidelity optimization (e.g., see Kennedy and O'Hagan [2000], Balabanov and Venter [2004], Eldred et al. [2004], Rajnarayan et al. [2008], March and Willcox [2012], Gratiet and Cannamela [2015]) is that information sources are required to form a hierarchy, thereby limiting possible correlations among their outputs: in particular, once one has queried a high fidelity source for some point $x$, then no further knowledge on $g(x)$ can be gained by querying any other information source of lower fidelity (at any</p>
<p>point). A second frequent assumption is that information sources are unbiased, admitting only (typically normally distributed) noise that further must be independent across different sources. Lam, Allaire, and Willcox [2015] addressed several of these shortcomings by a novel surrogate-based approach that requires the information sources to be neither hierarchical nor unbiased, and allows a more general notion of model discrepancy building on theoretical foundations by Allaire and Willcox [2014]. Their model has a separate Gaussian process for each information source that in particular quantifies its uncertainty over the domain. Predictions are obtained by fusing that information via the method of Winkler [1981]. Then they apply the EI acquisition function on these surrogates to first decide what design $x^{<em>}$ should be evaluated next and afterwards select the respective information source to query $x^{</em>}$; the latter decision is based on a heuristic that aims to balance information gain and query cost (see also Sect. 4).</p>
<p>Our Contributions. We present an approach to multi-information source optimization that allows exploiting cheap approximative estimates of the objective, while handling model discrepancies in a general and stringent way. Thus, we anticipate a wide applicability, including scenarios such as described above. We build on the common technique to capture the model discrepancy of each information source by a Gaussian process. However, we break through the separation and build a single statistical model that allows a uniform Bayesian treatment of the black-box objective function and the information sources. This model improves on previous works in multifidelity optimization and in particular on Lam et al. [2015], as it allows rigorously exploiting correlations across different information sources and hence enables us to reduce our uncertainty about all information sources when receiving one new data point, even if it originates from a source with lower fidelity. Thus, we obtain a more accurate estimate of the true objective function from each sample. Note that this feature is also accomplished by the product kernel of Swersky et al. [2013]. Their model differs in the assumption that the relationship of any pair of information sources is fixed throughout the domain, whereas our covariance function is more flexible. Additionally, our model incorporates and quantifies the bias between the objective and any other source explicitly.</p>
<p>Another important contribution is an acquisition
function that quantifies the uncertainty about the true objective and information sources, in particular due to model discrepancies and noise. To this end, we show how the Knowledge Gradient (KG) factor proposed by Frazier, Powell, and Dayanik [2009] can be computed efficiently in the presence of multiple information sources that vary in cost. This cost-sensitive Knowledge Gradient selects a pair $(\ell, x)$ such that the simple regret (i.e. the loss of the current best solution with respect to the unknown global optimum) relative to the specific query $\operatorname{cost} c_{\ell}(x)$ is minimized in expectation. Specifically, we show that the query cost can be incorporated as part of the objective in a natural way: our policy picks a pair that offers an optimal trade-off between the predicted simple regret and the corresponding cost. Specifically, its choice is even provably one-step Bayes optimal in terms of this benefit per unit cost. We regard it a conceptual advantage that the cost-sensitive KG factor can be computed analytically, whereas Swersky et al. [2013] rely on Monte Carlo approximations (see also Hennig and Schuler [2012] for a discussion). Lam et al. [2015] utilize a two-step heuristic as acquisition function.</p>
<p>We also demonstrate that our model is capable of handling information sources whose model discrepancies are interrelated in a more sophisticated way: in particular, we address the scenario of groups of information sources whose models deviate from the truth in a correlated fashion. For instance, in silico simulations of physical or chemical processes might employ similar approximations whose deviations from the physical laws are thus correlated, e.g., finite element analyses with different mesh fineness by Huang et al. [2006a] or calculations based on shared data sets. Additionally, experiments conducted in the same location are exposed to the same environmental conditions or singular events, thus the outputs of these experiments might deviate from the truth by more than independent measurement errors. Another important factor is humans involved in the lab work, as typically workers have received the comparable training and may have made similar experiences during previous joint projects, which influences their actions and decisions.</p>
<h2>2. The Model</h2>
<p>Each design (or point) $x$ is specified by $d$ parameters. Given some compact set $\mathcal{D} \subset \mathbb{R}^{d}$ of feasible designs, our goal is to find a best design under some contin-</p>
<p>uous objective function $g: \mathcal{D} \rightarrow \mathbb{R}$, i.e. we want to find a design in $\operatorname{argmax}<em 1="1">{x \in \mathcal{D}} g(x)$. Restrictions on $\mathcal{D}$ such as box constraints can be easily incorporated in our model. We have access to $M$ possibly biased and/or noisy information sources $\mathcal{I} \mathcal{S}</em>}, \mathcal{I} \mathcal{S<em M="M">{2}, \ldots, \mathcal{I} \mathcal{S}</em>}$ that provide information about $g$. Note that the $\mathcal{I} \mathcal{S<em 0="0">{\ell}$ (with $\ell \in[M]</em>}$ ) are also called "surrogates"; in the context of hyper-parameter tuning they are sometimes referred to as "auxiliary tasks" and $g$ is the primary task. We suppose that repeatedly observing $\mathcal{I} \mathcal{S<em _ell="\ell">{\ell}(x)$ for some $\ell$ and $x$ provides independent and normally distributed observations with mean value $f(\ell, x)$ and variance $\lambda</em>}(x)$. These sources are thought of as approximating $g$, with variable model discrepancy or bias $\delta_{\ell}(x)=g(x)-f(\ell, x)$. We suppose that $g$ can be observed directly without bias (but possibly with noise) and set $\mathcal{I} \mathcal{S<em _ell="\ell">{0}=g$. Each $\mathcal{I} \mathcal{S}</em>(x)$ are both known and continuously differentiable. In practice, these functions may either be provided by domain experts or may be estimated along with other model parameters from data (see Sect. 4, the supplement, and Rasmussen and Williams [2006]). Our motivation in having the cost and noise vary over the space of designs is that physical experiments may become difficult to conduct and/or expensive when environmental parameters are extreme. Moreover, simulations may be limited to certain specified parameter settings and their accuracy diminish quickly.}$ is also associated with a query cost function $c_{\ell}(x): \mathcal{D} \rightarrow \mathbb{R}^{+}$. We assume that the cost function $c_{\ell}(x)$ and the variance function $\lambda_{\ell</p>
<p>We now place a single Gaussian process prior on $f$ (i.e., on $g$ and the mean response from the $M$ information sources). Let $\mu:[M] \times \mathcal{D} \mapsto \mathbb{R}$ be the mean function of this Gaussian process, and $\Sigma:([M] \times \mathcal{D})^{2} \mapsto \mathbb{R}$ be the covariance kernel. (Here, for any $a \in \mathbb{Z}^{+}$we use $[a]$ as a shorthand for the set ${1,2, \ldots, a}$, and further define $[a]_{0}={0,1,2, \ldots, a}$.) While our method can be used with an arbitrary mean function and positive semidefinite covariance kernel, we provide two concrete parameterized classes of mean functions and covariance kernels that are useful for multi-information source optimization. Due to space constraints the second class is deferred to the supplement, where we also detail how to estimate the hyper-parameters of the mean function and the covariance kernel.</p>
<p>Independent Model Discrepancy. We first propose a parameterized class of mean functions $\mu$ and covariance functions $\Sigma$ derived by supposing that model discrepancies are chosen independently across informa-
tion sources. This first approach is appropriate when information sources are different in kind from each other and share no relationship except the fact that they are modeling a common objective. We also propose a more general parameterized class that models correlation between model discrepancies, as is typical when information sources can be partitioned into groups, such that information sources within a group tend to agree more amongst themselves than they do with information sources in other groups. Due to space constraints, this class was deferred to the the online supplement.</p>
<p>We suppose here that $\delta_{\ell}$ for each $\ell&gt;0$ was drawn from a separate independent Gaussian process, $\delta_{\ell} \sim$ $G P\left(\mu_{\ell}, \Sigma_{\ell}\right)$. We also suppose that $\delta_{0}$ is identically 0 , and that $f(0, \cdot) \sim G P\left(\mu_{0}, \Sigma_{0}\right)$, for some given $\mu_{0}$ and $\Sigma_{0}$. We then define $f(\ell, x)=f(0, x)+\delta_{\ell}(x)$ for each $\ell$. Typically, one would not have a strong prior belief as to the direction of the bias inherent in an information source, and so we set $\mu_{\ell}(x)=0$. (If one does have a strong prior opinion that an information source is biased in one direction, then one may instead set $\mu_{\ell}$ to a constant estimated using maximum a posteriori estimation.) With this modeling choice, we see that the mean of $f \sim G P(\mu, \Sigma)$ with mean function $\mu$ and covariance kernel $\Sigma$ is given by</p>
<p>$$
\mu(\ell, x)=\mathbb{E}[f(\ell, x)]=\mathbb{E}[f(0, x)]+\mathbb{E}\left[\delta_{\ell}(x)\right]=\mu_{0}(x)
$$</p>
<p>for each $\ell \in[M]<em _ell="\ell">{0}$, since $\mathbb{E}\left[\delta</em>$,}(x)\right]=0$ holds. Additionally, for $\ell, m \in[M]_{0}$ and $x, x^{\prime} \in \mathcal{D</p>
<p>$$
\begin{aligned}
&amp; \Sigma\left((\ell, x),\left(m, x^{\prime}\right)\right) \
= &amp; \operatorname{Cov}\left(f(0, x)+\delta_{\ell}(x), f\left(0, x^{\prime}\right)+\delta_{m}\left(x^{\prime}\right)\right) \
= &amp; \Sigma_{0}\left(x, x^{\prime}\right)+\mathbb{1}<em _ell="\ell">{\ell, m} \cdot \Sigma</em>\right)
\end{aligned}
$$}\left(x, x^{\prime</p>
<p>where $\mathbb{1}<em _ell="\ell">{\ell, m}$ denotes Kronecker's delta, and where we have used independence of $\delta</em>$, and $f(0, \cdot)$.}, \delta_{m</p>
<h2>3. The Value of Information Analysis</h2>
<p>Our optimization algorithm proceeds in rounds, where in each round it selects a design $x \in \mathcal{D}$ and an information source $\mathcal{I} \mathcal{S}<em 0="0">{\ell}$ with $\ell \in[M]</em>$. Let us assume for the moment that query costs are uniform over the whole domain and all information sources; we will show how to remove this assumption later. Further, assume that we have already sampled $n$}$. The goal is to find an $x$ that maximizes $g(x)$ over $\mathcal{D</p>
<p>points $X$ and made the observations $Y$. Finally, denote by $\mathbb{E}<em n="n">{n}[f(\ell, x)]$ the expected value according to the posterior distribution given $X$ and $Y$ and shorthand $\mu^{(n)}(\ell, x):=\mathbb{E}</em>[f(\ell, x)]$. Since that distribution is normal, the best expected objective value of any design, as estimated by our statistical model, is $\max <em n="n">{x^{\prime} \in \mathcal{D}} \mu^{(n)}\left(0, x^{\prime}\right)$. If we were to pick an $x \in \mathcal{D}$ now irrevocably, then we would select an $x$ of maximum expectation. This motivates choosing the next design $x^{(n+1)}$ and information source $\ell^{(n+1)}$ that we will sample such that we maximize $\mathbb{E}</em>\left[\max <em n="n">{x^{\prime} \in \mathcal{D}} \mu^{(n+1)}\left(0, x^{\prime}\right) \mid \ell^{(n+1)}=\ell, x^{(n+1)}=x\right]$, or equivalently maximize the expected gain over the current optimum by $\mathbb{E}</em>\left[\max <em x_prime="x^{\prime">{x^{\prime} \in \mathcal{D}} \mu^{(n+1)}\left(0, x^{\prime}\right) \mid \ell^{(n+1)}=\ell, x^{(n+1)}=x\right]-$ $\max </em>$ given $X$ and $Y$.} \in \mathcal{D}} \mu^{(n)}\left(0, x^{\prime}\right)$. Note that the equivalence of the maximizers for both expressions follows immediately from the observation that $\mu^{(n)}\left(0, x^{\prime}\right)$ is a constant for all $x^{\prime} \in \mathcal{D</p>
<p>Next we show how the assumption made at the beginning of this section, that query costs are uniform across the domain and for all information sources, can be removed. To this end, we associate a query cost function $c_{\ell}(x): \mathcal{D} \rightarrow \mathbb{R}^{+}$with each information source $\mathcal{I S}<em 0="0">{\ell}$ for $\ell \in[M]</em>$. Then the cost-sensitive Knowledge Gradient policy picks a sample $(\ell, x)$ that maximizes the expectation}$. Then our goal becomes to find a sample $\left(\ell^{(n+1)}, x^{(n+1)}\right)$ whose value of information divided by its respective query cost is maximum. The gist is that conditioned on any $\left(\ell^{(n+1)}, x^{(n+1)}\right)$, the expected gain of all $x^{\prime} \in \mathcal{D}$ is scaled by $c_{\ell^{(n+1)}}\left(x^{(n+1)}\right)^{-1</p>
<p>$$
\begin{array}{r}
\mathbb{E}<em x_prime="x^{\prime">{n}\left[\frac{\max </em>\right)-\max } \in \mathcal{D}} \mu^{(n+1)}\left(0, x^{\prime<em _ell="\ell">{x^{\prime} \in \mathcal{D}} \mu^{(n)}\left(0, x^{\prime}\right)}{c</em>\right] \
\left.\ell^{(n+1)}=\ell, x^{(n+1)}=x\right]
\end{array}
$$}(x)</p>
<p>denoted by $\operatorname{CKG}(\ell, x)$. Our task is to compute $\left(\ell^{(n+1)}, x^{(n+1)}\right) \in \operatorname{argmax}<em 0="0">{\ell \in[M]</em>(\ell, x)$, which is a nested optimization problem.}, x \in \mathcal{D}} \operatorname{CKG</p>
<p>To make this task feasible in practice, we discretize the domain of the inner maximization problem stated in Eq. (1): for simplicity, we choose the discrete set $\mathcal{A} \subset \mathcal{D}$ via a Latin Hypercube design. Alternatively, one could scatter the points to emphasize promising areas of $\mathcal{D}$ and resample regularly. Now we have reduced the inner optimization problem for each information source to the setting of Frazier et al. [2009] who showed how to compute the value of information over a discrete set if there is only one information
source and query costs are uniform.
We provide an outline and refer to their article for details. For their setting let $\bar{\mu}^{n}$ be the vector of posterior means for $\mathcal{A}$ and define for each $x \in \mathcal{A}$ $\bar{\sigma}^{n}(x)=\Sigma^{n} e_{x} /\left(\lambda(x)+\Sigma_{x x}^{n}\right)$, where $\Sigma^{n}$ is the posterior covariance matrix and $e_{x} \in{0,1}^{|\mathcal{A}|}$ with a one for $x$ and zeros elsewhere. Given these vectors, observe that
$\mathbb{E}<em x_prime="x^{\prime">{n}\left[\max </em>\right)-\max } \in \mathcal{A}} \mu^{(n+1)}\left(0, x^{\prime<em n="n">{x^{\prime} \in \mathcal{A}} \mu^{(n)}\left(0, x^{\prime}\right) \mid x^{(n+1)}=x\right]$
$=h\left(\bar{\mu}^{n}, \bar{\sigma}^{n}(x)\right)$,
where $h(a, b)=\mathbb{E}</em>\left[\max <em i="i">{i} a</em> Z\right]-\max }+b_{i<em i="i">{i} a</em>$ for vectors $a, b$, and $Z$ is a one-dimensional standard normal random variable. Frazier et al. show how to compute $h$.</p>
<p>Thus, following our initial considerations, we approximate the cost-sensitive Knowledge Gradient by maximizing $\frac{h\left(\bar{\mu}^{n}, \bar{\sigma}^{n}(\ell, x)\right)}{c_{\ell}(x)}$ over $[M]<em _ell="\ell">{0} \times \mathcal{D}$, i.e. the outer optimization problem is still formulated over $\mathcal{D}$. Note that we can compute the gradient of $\frac{h\left(\bar{\mu}^{n}, \bar{\sigma}^{n}(\ell, x)\right)}{c</em>\right)$.}(x)}$ with respect to $x$ assuming that $c_{\ell}$ is differentiable (e.g., when given by a suitable Gaussian process). Thus, we may apply a multi-start gradient-based optimizer to compute $\left(\ell^{(n+1)}, x^{(n+1)</p>
<p>We summarize our algorithm misoKG:</p>
<ol>
<li>Using samples from all information sources, estimate hyper-parameters of the Gaussian process prior as described in the online supplement.
Then calculate the posterior $f$ based on the prior and samples.</li>
<li>Until the budget for samples is exhausted do:</li>
</ol>
<p>Determine the information source $\ell \in[M]<em _ell="\ell">{0}$ and the design $x \in \mathcal{D}$ that maximize the cost-sensitive Knowledge Gradient proposed in Eq. (1) and observe $\mathcal{I} \mathcal{S}</em>(x)$.
Update the posterior distribution with the new observation.
3. Return the point with the largest estimated value according to the current posterior $f(0, \cdot)$.</p>
<h2>4. Numerical Experiments</h2>
<p>We demonstrate the performance of the proposed multi-information source optimization algorithm, misoKG, by comparing it with the state-of-the-art Bayesian optimization algorithms for MISO problems.</p>
<p>The statistical model and the value of information analysis were implemented in Python 2.7 and C++ using the functionality provided by the Metrics Optimization Engine MOE.</p>
<p>Benchmark Algorithms. The first benchmark method, MTBO+, is an improved version of Multi-Task Bayesian Optimization proposed by Swersky et al. [2013]. It uses a cost-sensitive version of Entropy Search as acquisition function that picks samples to maximize the information gain over the location of the optimum of the objective function, normalized by the respective query cost (see their paper for details). MTBO combines acquisition function with a "multi-task" Gaussian process model that captures the relationships between information sources (the "tasks") and the objective function. Following a recommendation of Snoek 2016, our implementation MTBO+ uses an improved formulation of the acquisition function given by Hernández-Lobato et al. [2014], Snoek and et al., but otherwise is identical to MTBO; in particular, it uses the statistical model of Swersky et al. [2013].</p>
<p>The other algorithm, misoEI of Lam et al. [2015], was developed to solve MISO problems that involve model discrepancy and therefore is a good competing method to compare with. It maintains a separate Gaussian process for each information source: to combine this knowledge, the corresponding posterior distributions are fused for each design via Winkler's method (1981) into a single intermediate surrogate, which is a normally distributed random variable. Then Lam et al. adapt the Expected Improvement (EI) acquisition function to select the design which is to be sampled next: for the sake of simplicity, assume that observations are noiseless and that $y^{<em>}$ is the objective value of a best sampled design. If $Y_{x}$ denotes a Gaussian random variable with the posterior distribution of the objective value for design $x$, then $\mathbb{E}\left[\max \left{Y_{x}-y^{</em>}, 0\right}\right]$ is the expected improvement for $x$, and the EI acquisition function selects an $x$ that maximizes this expectation. Based on this decision, the information source to invoke is chosen by a heuristic that aims at maximizing the EI per unit cost.</p>
<p>The Experimental Setups. We conducted numerical experiments on the following test problems: the first is the 2-dimensional Rosenbrock function which is a standard benchmark in the literature, tweaked into the MISO setting by Lam et al. [2015]. The second is a MISO benchmark proposed by Swersky et al. [2013]:
the goal is to optimize the four hyper-parameters of a machine learning algorithm, using a small, related set of smaller images as cheap information source. The third is an assemble-to-order problem introduced by Hong and Nelson [2006]: here the objective is to optimize an 8-dimensional target stock vector in order to maximize the expected daily profit of a company, for which an estimate is provided as an output by their simulator.</p>
<p>In MISO settings the amount of initial data that one can use to inform the methods about each information source is typically dictated by the application, in particular by resource constraints and the availability of the respective source. In our experiments all methods were given identical initial datasets for each information source in every replication; these sets were drawn randomly via Latin Hypercube designs. For the sake of simplicity, we provided the same number of points for each $\mathcal{I S}$, deliberately set in advance to 2.5 points per dimension of the design space $\mathcal{D}$. Regarding the kernel and mean function, MTBO+ uses the settings provided in [Snoek and et al.]. The other algorithms used the squared exponential kernel and a constant mean function set to the average of a random sample.</p>
<p>We report the "gain" over the best initial solution, that is the true objective value of the respective design that a method would return at each iteration minus the best value in the initial data set. If the true objective value is not known for a given design, we report the value obtained from the information source of highest fidelity. This gain is plotted as a function of the total cost, that is the cumulative cost for invoking the information sources plus the fixed cost for the initial data; this metric naturally generalizes the number of function evaluations prevalent in Bayesian optimization. Note that the computational overhead of choosing the next information source and sample is omitted, as it is negligible compared to invoking an information source in real-world applications. Error bars are shown at the mean plus and minus two standard errors averaged over at least 100 runs of each algorithm. Note that even for deterministic sources a tiny observational noise of $10^{-6}$ is supposed to avoid numerical issues during matrix inversion.</p>
<h3>4.1. The Rosenbrock Benchmarks</h3>
<p>We consider the design space $\mathcal{D}=[-2,2]^{2}$, and $M=2$ information sources. $\mathcal{I S}<em 1="1">{0}$ is the Rosenbrock function $f(\cdot)$ plus optional Gaussian noise, and $\mathcal{I S}</em>$</p>
<p>equals $f(\cdot)$ with an additional oscillatory component:</p>
<p>$$
\begin{aligned}
&amp; f(\boldsymbol{x})=\left(1-x_{1}\right)^{2}+100 \cdot\left(x_{2}-x_{1}^{2}\right)^{2} \
&amp; \mathcal{I S}<em 1="1">{0}(\boldsymbol{x})=f(\boldsymbol{x})+u \cdot \varepsilon \
&amp; \mathcal{I S}</em>\right)
\end{aligned}
$$}(\boldsymbol{x})=f(\boldsymbol{x})+v \cdot \sin \left(10 \cdot x_{1}+5 \cdot x_{2</p>
<p>where $\boldsymbol{x}=\left(x_{1}, x_{2}\right)^{T} \in \mathcal{D}$, and $\varepsilon$ is i.i.d. noise drawn from the standard normal distribution. $u$ and $v$ are configuration constants. We suppose that $\mathcal{I} \mathcal{S}<em 0="0">{1}$ is not subject to observational noise, hence the uncertainty only originates from the model discrepancy. We experimented on two different configurations to gain a better insight into characteristics of the algorithms. Since Lam et al. reported a good performance of their method on (2), we replicated their experiment using the same parameters to compare the performance of the four methods: that is, we set $u=0, v=0.1$. Replicating the setting in [Lam et al., 2015, p. 15], we also suppose a tiny uncertainty for $\mathcal{I} \mathcal{S}</em>}$, although it actually outputs the truth, and set $\lambda_{0}(x)=10^{-3}$ and $\lambda_{1}(x)=10^{-6}$ for all $x$. Furthermore, we assume a cost of 1000 for each query to $\mathcal{I} \mathcal{S<em 1="1">{0}$ and of 1 for $\mathcal{I} \mathcal{S}</em>$.</p>
<p>Since all methods converged to good solutions within few queries, we investigate the ratio of gain to cost: Fig. 1 (t) displays the gain of each method over the best initial solution as a function of the total cost, that is the cost of the initial data and the query cost accumulated by the acquisition functions. We see that the new method misoKG offers a better gain per unit cost, typically finding an almost optimal solution within $5-10$ samples. We note that misoKG relies only on cheap samples, therefore managing the uncertainties successfully. This is also true for MTBO+ that obtains a slightly worse solution. misoEI on the other hand queries the expensive truth to find the global optimum, thereby accumulating considerably higher cost.</p>
<p>For the second setup, we make the following changes: we set $u=1$ and $v=2$, and suppose for $\mathcal{I} \mathcal{S}<em 0="0">{0}$ uniform observational noise of $\lambda</em>(x)=50$. Note that now the difference of the costs of both sources is much smaller, while their uncertainties are considerably bigger. The results are displayed in Fig. 1 (b): as for the first configuration, misoKG outperforms the other methods, making efficient use of the cheap biased information source. In fact, the relative difference in performance is even larger, which might suggest that misoKG handles the observational noise slightly better than its competitors for this benchmark.
}(x)=1$ and uniform query cost $c_{0<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (t) The Rosenbrock benchmark with the parameter setting of Lam et al. [2015]. (b) The Rosenbrock benchmark with the alternative setup. misoKG offers an excellent gain-tocost ratio and outperforms its competitors substantially.</p>
<p>4.2. The Image Classification Benchmark</p>
<p>This classification problem was introduced by Swersky et al. [2013] to demonstrate the performance of their MTBO algorithm. The goal is to optimize hyper-parameters of the Logistic Regression algorithm Theano in order to minimize the classification error on the MNIST dataset of LeCun et al.. The weights are trained using a stochastic gradient method with mini-batches. We have four hyper-parameters: the learning rate, the L2-regularization parameter, the batch size, and the number of epochs. The MNIST dataset contains 70,000 grayscale images of handwritten digits, where each image consists of 784 pixels. In the experimental setup information source $\mathcal{IS}_{0}$ corresponds to invoking the machine learning algorithm on this dataset.</p>
<p>Following Swersky et al., the USPS dataset serves as cheap information source $\mathcal{IS}<em 1="1">{1}$ : this set comprises only about 9000 images of handwritten digits that are also smaller, only 256 pixels each USPS. Again we suppose a tiny observational noise of $10^{-6}$ and set the invocation costs of the sources to 4.5 for $\mathcal{IS}</em>}$ and 43.69 for $\mathcal{IS<em 1="1">{0}$. A closer examination shows that $\mathcal{IS}</em>$, making it a challenge for MISO algorithms.}$ is subject to considerable bias with respect to $\mathcal{IS}_{0</p>
<p>Initially, misoKG and MTBO+ are on par and both outperform misoEI (cp. Fig.2 (t)). In order to study the convergence behavior, we evaluated misoKG and MTBO+ for 150 steps, with a lower number of replications but using the same initial data for each replication. We observe that misoKG usually achieves an optimal testerror of about $7.1\%$ on the MNIST testset after about 80 queries to information sources (see Fig.2 (b)). In its late iterations MTBO+ achieves a worse performance than misoKG has at the same costs. Note that the experimental results of Swersky et al. [2013] show that MTBO+ will also converge to the optimum eventually.</p>
<h3>4.3. The Assemble-To-Order Benchmark</h3>
<p>In the assemble-to-order (ATO) benchmark, a reinforcement learning problem from a business application, we are managing the inventory of a company that manufactures $m$ products. Each of these products is made from a selection from $n$ items, where we distinguish for each product between key items and non-key items: if the company runs out of key items, then it cannot sell the respective products until it has restocked its inventory; non-key items are optional</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The performance on the image classification benchmark with significant model discrepancy. (t) The first 50 steps of each algorithm: misoKG and MTBO+ perform better than misoEI. (b) The first 150 steps of misoKG and MTBO+. While the initial performance of misoKG and MTBO+ is comparable, misoKG achieves better testerrors after about 80 steps and converges to the global optimum.</p>
<p>and used if available. When the company sends a replenishment order, the required item is delivered after a random period whose distribution is known. Since items in the inventory inflict holding cost, our goal is to find an optimal target inventory level vector $b$ that determines the amount of each item we want to stock, such that we maximize the expected profit per day (cp. Hong and Nelson [2006] for details).</p>
<p>Hong and Nelson proposed a specific scenario with $m=5$ different products that depend on a subset of $n=8$ items, thus our task is to optimize the 8 dimensional target vector $b \in[0,20]^{8}$. For each such vector their simulator provides an estimate of the expected daily profit by running the simulation for a variable number of replications (see "runs" in Table 1). Increasing this number yields a more accurate estimate but also has higher computational cost. The observational noise and query cost, i.e. the computation time of the simulation, are estimated from samples for each information source, assuming that both functions are constant over the domain for the sake of simplicity.</p>
<p>There are two simulators for this assemble-to-order setting that differ subtly in the model of the inventory system. However, the effect in estimated objective value is significant: on average the outputs of both simulators at the same target vector differ by about $5 \%$ of the score of the global optimum, which is about 120 , whereas the largest observed bias out of 1000 random samples was 31.8. Moreover, the sample variance of the difference between the outputs of both simulators is about 200. Thus, we are witnessing a significant model discrepancy. We set up three information sources (cp. Table 1): $\mathcal{I S}<em 2="2">{0}$ and $\mathcal{I S}</em>}$ use the simulator of Xie et al. [2012], whereas the cheapest source $\mathcal{I} \mathcal{S<em 0="0">{1}$ invokes the implementation of Hong and Nelson. We assume that $\mathcal{I} \mathcal{S}</em>$ models the truth.</p>
<p>Table 1: The Parameters for the ATO problem</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"># Runs</th>
<th style="text-align: center;">Noise Variance</th>
<th style="text-align: center;">Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathcal{I} \mathcal{S}_{0}$</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">0.056</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{I} \mathcal{S}_{1}$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2.944</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{I} \mathcal{S}_{2}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">3.9</td>
</tr>
</tbody>
</table>
<p>Fig. 3 displays the performance over the first 150 steps for misoKG and MTBO+ and the first 50 steps of misoEI, all averaged over 100 runs. misoKG has the best start and dominates in particular MTBO+ clearly. misoKG averages at a gain of 26.1 , but inflicts only an average query cost of 54.6 to the information sources,
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The performance on the assemble-to-order benchmark with significant model discrepancy. misoKG has the best gain per cost ratio among the algorithms.
excluding the fixed cost for the initial datasets that are identical for all algorithms for the moment. This is only $6.3 \%$ of the query cost that misoEI requires to achieve a comparable score. Interestingly, misoKG and MTBO+ utilize in their optimization of the target inventory level vector mostly the cheap, biased source, and therefore are able to obtain significantly better gain per cost ratios than misoEI.</p>
<p>Looking closer, we see that typically misoKG's first call to $\mathcal{I} \mathcal{S}<em 2="2">{2}$ occurs after about $60-80$ steps. In total, misoKG queries $\mathcal{I} \mathcal{S}</em>}$ about ten times within the first 150 steps; in some replications misoKG makes one late call to $\mathcal{I} \mathcal{S<em 1="1">{0}$ when it has already converged. Our interpretation is that misoKG exploits the cheap, biased $\mathcal{I} \mathcal{S}</em>}$ to zoom in on the global optimum and switches to the unbiased but noisy $\mathcal{I} \mathcal{S<em 1="1">{2}$ to identify the optimal solution exactly. This is the expected (and desired) behavior for misoKG when the uncertainty of $f\left(0, x^{<em>}\right)$ for some $x^{</em>}$ is not expected to be reduced sufficiently by queries to $\mathcal{I} \mathcal{S}</em>$.</p>
<p>MTBO+ trades off the gain versus cost differently: it queries $\mathcal{I} \mathcal{S}<em 1="1">{0}$ once or twice after 100 steps and directs all other queries to $\mathcal{I} \mathcal{S}</em>$, which might explain the observed lower performance. misoEI that employs a two-step heuristic for trading off predicted gain and query cost chose almost always the most expensive simulation to evaluate the selected design.</p>
<h2>Acknowledgments</h2>
<p>The authors were partially supported by NSF CAREER CMMI-1254298, NSF CMMI-1536895, NSF IIS1247696, AFOSR FA9550-12-1-0200, AFOSR FA9550-15-1-0038, and AFOSR FA9550-16-1-0046.</p>
<h2>References</h2>
<p>D. Allaire and K. Willcox. A mathematical and computational framework for multifidelity design and analysis with computer models. International Journal for Uncertainty Quantification, 4(1), 2014.
V. Balabanov and G. Venter. Multi-fidelity optimization with high-fidelity analysis and low-fidelity gradients. In 10th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference, 2004.
E. V. Bonilla, K. M. Chai, and C. Williams. Multitask gaussian process prediction. In Advances in Neural Information Processing Systems, pages 153160, 2007.
M. S. Eldred, A. A. Giunta, and S. S. Collis. Secondorder corrections for surrogate-based optimization with model hierarchies. In Proceedings of the 10th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference, pages 2013-2014, 2004.
P. I. Frazier, W. B. Powell, and S. Dayanik. The Knowledge Gradient Policy for Correlated Normal Beliefs. INFORMS Journal on Computing, 21(4): $599-613,2009$.
P. Goovaerts. Geostatistics for Natural Resources Evaluation. Oxford University, 1997.
L. L. Gratiet and C. Cannamela. Cokriging-based sequential design strategies using fast cross-validation techniques for multi-fidelity computer codes. Technometrics, 57(3):418-427, 2015.
P. Hennig and C. J. Schuler. Entropy search for information-efficient global optimization. The Journal of Machine Learning Research, 13(1):1809-1837, 2012.
J. M. Hernández-Lobato, M. W. Hoffman, and Z. Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918-926, 2014.
L. J. Hong and B. L. Nelson. Discrete optimization via simulation using compass. Operations Research, $54(1): 115-129,2006$.
D. Huang, T. Allen, W. Notz, and R. Miller. Sequential kriging optimization using multiple-fidelity evaluations. Structural and Multidisciplinary Optimization, 32(5):369-382, 2006a.
D. Huang, T. T. Allen, W. I. Notz, and N. Zeng. Global Optimization of Stochastic Black-Box Systems via Sequential Kriging Meta-Models. Journal of Global Optimization, 34(3):441-466, 2006b.
D. R. Jones, M. Schonlau, and W. J. Welch. Efficient Global Optimization of Expensive Black-Box Functions. Journal of Global Optimization, 13(4): $455-492,1998$.
K. Kandasamy, G. Dasarathy, J. B. Oliva, J. Schneider, and B. Poczos. Multi-fidelity gaussian process bandit optimisation. In Advances in Neural Information Processing Systems, 2016.
M. C. Kennedy and A. O'Hagan. Predicting the output from a complex computer code when fast approximations are available. Biometrika, 87(1): $1-13,2000$.
A. Klein, S. Falkner, S. Bartels, P. Hennig, and F. Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. CoRR, abs/1605.07079, 2016.
R. Lam, D. Allaire, and K. Willcox. Multifidelity optimization using statistical surrogate modeling for non-hierarchical information sources. In 56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, 2015.
Y. LeCun, C. Cortes, and C. J. Burges. The MNIST database of handwritten digits. http: //yann.lecun.com/exdb/mnist/. Last Accessed on 10/09/2016.
A. March and K. Willcox. Provably convergent multifidelity optimization algorithm not requiring highfidelity derivatives. AIAA Journal, 50(5):1079-1089, 2012.</p>
<p>MOE. Metrics optimization engine. http://yelp. github.io/MOE/. Last Accessed on 10/04/2016.</p>
<p>V. Picheny, D. Ginsbourger, Y. Richet, and G. Caplin. Quantile-based optimization of noisy computer experiments with tunable precision. Technometrics, $55(1): 2-13,2013$.
N. V. Queipo, R. T. Haftka, W. Shyy, T. Goel, R. Vaidyanathan, and P. K. Tucker. Surrogatebased analysis and optimization. Progress in aerospace sciences, 41(1):1-28, 2005.
D. Rajnarayan, A. Haas, and I. Kroo. A multifidelity gradient-free optimization method and application to aerodynamic design. In Proceedings of the 12th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference, number 6020, 2008.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. ISBN ISBN 0-262-18253-X.
W. R. Scott, P. I. Frazier, and W. B. Powell. The correlated knowledge gradient for simulation optimization of continuous parameters using gaussian process regression. SIAM Journal on Optimization, 21(3):996-1026, 2011.
J. Snoek. Personal communication, 2016.
J. Snoek and et al. Spearmint. http://github.com/ HIPS/Spearmint. Last Accessed on 10/04/2016.
K. Swersky, J. Snoek, and R. P. Adams. Multi-task bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004-2012, 2013.
Y.-W. Teh, M. Seeger, and M. Jordan. Semiparametric latent factor models. In Artificial Intelligence and Statistics 10, 2005.</p>
<p>Theano. Theano: Logistic regression. http://deeplearning.net/tutorial/code/ logistic_sgd.py. Last Accessed on 10/08/16.</p>
<p>USPS. USPS dataset. http://mldata.org/ repository/data/viewslug/usps/. Last Accessed on 10/09/2016.
J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global optimization of expensive-to-evaluate functions. Journal of Global Optimization, 44(4):509-534, 2009.
R. L. Winkler. Combining probability distributions from dependent information sources. Management Science, 27(4):479-488, 1981.
J. Xie, P. I. Frazier, and S. Chick. Assemble to order simulator. http://simopt.org/wiki/index. php?title=Assemble_to_Order\&amp;oldid=447, 2012. Last Accessed on 10/02/2016.</p>
<h2>Multi-Information Source Optimization <br> Supplementary Material</h2>
<h2>A. The Model Revisited</h2>
<h2>A.1. Correlated Model Discrepancies</h2>
<p>Next we demonstrate that our approach is flexible and can easily be extended to scenarios where some of the information sources have correlated model discrepancies. This arises for hyper-parameter tuning if the auxiliary tasks are formed from data that was collected in batches and thus is correlated over time (see Sect. 1 for a discussion). In engineering sciences we witness this if some sources share a common modeling approach, as for example, if one set of sources for an airfoil modeling problem correspond to different discretizations of a PDE that models wing flutter, while another set provides various discretizations of another PDE that modeling airflow. Two information sources that solve the same PDE will be more correlated than two that solve different PDEs.</p>
<p>For example, let $P=\left{P_{1}, \ldots, P_{Q}\right}$ denote a partition of $[M]<em 0="0">{0}$ and define the function $k:[M]</em>\right)$ for each partition. (Note that in principle we could take this approach further to arbitrary sets of $[M]} \rightarrow[Q]$ that gives for each IS its corresponding partition in $P$. Then we suppose an independent Gaussian process $\varepsilon(k(\ell), x) \sim G P\left(\mu_{k(\ell)}, \Sigma_{k(\ell)<em 0="0">{0}$. However, this comes at the expense of a larger number of hyper-parameters that need to be estimated.) Again our approach is to incorporate all Gaussian processes into a single one with prior distribution $f \sim G P(\mu, \Sigma):^{1}$ therefore, for all $\ell \in[M]</em>(x)$, where $f(0, x)=g(x)$ is the objective function that we want to optimize. Due to linearity of expectation, we have}$ and $x \in \mathcal{D}$ we define $f(\ell, x)=$ $f(0, x)+\varepsilon(k(\ell), x)+\delta_{\ell</p>
<p>$$
\begin{aligned}
\mu(\ell, x) &amp; =\mathbb{E}\left[f(0, x)+\varepsilon(k(\ell), x)+\delta_{\ell}(x)\right] \
&amp; =\mathbb{E}[f(0, x)]+\mathbb{E}[\varepsilon(k(\ell), x)]+\mathbb{E}\left[\delta_{\ell}(x)\right] \
&amp; =\mu_{0}(x)
\end{aligned}
$$</p>
<p>since $\mathbb{E}[\varepsilon(k(\ell), x)]=\mathbb{E}\left[\delta_{\ell}(x)\right]=0$. Recall that the indicator variable $\mathbb{1}<em 0="0">{\ell, m}$ denotes Kronecker's delta. Let $\ell, m \in[M]</em>$, then we define the following composite covariance function $\Sigma$ :}$ and $x, x^{\prime} \in \mathcal{D</p>
<p>$$
\Sigma\left((\ell, x),\left(m, x^{\prime}\right)\right)
$$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>[</p>
<p>2]
An $\operatorname{Cov}\left(f(0, x)+\varepsilon(k(\ell), x)+\delta_{\ell}(x), f\left(0, x^{\prime}\right)\right.$</p>
<p>$$
\left.+\varepsilon(k(m), x^{\prime})+\delta_{m}\left(x^{\prime}\right)\right)
$$</p>
<p>$$
\begin{aligned}
= &amp; \operatorname{Cov}\left(f(0, x), f\left(0, x^{\prime}\right)\right)+\operatorname{Cov}\left(\varepsilon(k(\ell), x), \varepsilon\left(k(m), x^{\prime}\right)\right) \
&amp; +\operatorname{Cov}\left(\delta_{\ell}(x), \delta_{m}\left(x^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>$$
=\Sigma_{0}\left(x, x^{\prime}\right)+\mathbb{1}<em k_ell_="k(\ell)">{k(\ell), k(m)} \cdot \Sigma</em>}\left(x, x^{\prime}\right)+\mathbb{1<em _ell="\ell">{\ell, m} \cdot \Sigma</em>\right)
$$}\left(x, x^{\prime</p>
<h2>A.2. Estimation of Hyper-Parameters</h2>
<p>In this section we detail how to set the hyperparameters via maximum a posteriori (MAP) estimation and propose a specific prior that has proven its value in our application and thus is of interest in its own right.</p>
<p>In typical MISO scenarios little data is available, that is why we suggest MAP estimates that in our experience are more robust than maximum likelihood estimates (MLE) under these circumstances. However, we wish to point out that we observed essentially the same performances of the algorithms when conducting the Rosenbrock and Assemble-to-Order benchmarks with maximum likelihood estimates for the hyperparameters.</p>
<p>In what follows we use the notation introduced in Sect. 2. One would suppose that the functions $\mu_{0}(\cdot)$ and $\Sigma_{\ell}(\cdot, \cdot)$ with $\ell \in[M]<em 0="0">{0}$ belong to some parameterized class: for example, one might set $\mu</em>$ each belong to the class of Matérn covariance kernels (cp. Sect. 4 for the choices used in the experimental evaluation). The hyper-parameters are fit from data using maximum a posteriori (MAP) estimation; note that this approach ensures that covariances between information sources and the objective function are inferred from data.}(\cdot)$ and each $\lambda_{\ell}(\cdot)$ to constants, and suppose that $\Sigma_{\ell</p>
<p>For a Matérn kernel we have to estimate $d+1$ hyper-parameters for each information source (see next subsection): $d$ length scales and the signal variance. We suppose a normal prior $\mathcal{N}\left(\mu_{i}, \sigma_{i}^{2}\right)$ for hyper-parameter $\theta_{i}$. Let $D \in \mathcal{D}$ be a set of points, for example chosen via a Latin Hypercube design, and evaluate every information source at all points in $D$. We estimate the hyper-parameters for $f(0, \cdot)$ and the $\delta_{i}$ for $i \in[M]$, using the "observations" $\Delta_{i}=\left{\mathcal{I S}<em 0="0">{i}(x)-\mathcal{I S}</em>}(x) \mid x \in D\right}$ for the $\delta_{i}$. The prior mean of the signal variance parameter of $\mathcal{I} \mathcal{S<em 0="0">{0}$ is set to the variance of the observations at $\mathcal{I} \mathcal{S}</em>}$ minus their average observational noise. The mean for the signal variance of $\mathcal{I} \mathcal{S<em i="i">{i}$ with $i \in[M]$ is obtained analogously using the "observations" in $\Delta</em>$; here we subtract the mean noise variance of the obser-</p>
<p>vations at $\mathcal{I S}<em 0="0">{i}$ and the mean noise at $\mathcal{I S}</em>$ is the mean of the prior.}$, exploiting the assumption that observational noise is independent. Regarding the means of the priors for length scales, we found it useful to set each prior mean to the length of the interval that the corresponding parameter is optimized over. For all hyper-parameters $\theta_{i}$ we set the variance of the prior to $\sigma_{i}^{2}=\left(\frac{\mu_{i}}{\sigma}\right)^{2}$, where $\mu_{i</p>
<h2>A.3. How to Express Beliefs on Fidelities of Information Sources</h2>
<p>In many applications one has beliefs about the relative accuracies of information sources. One approach to explicitly encode these is to introduce a new coefficient $\alpha_{\ell}$ for each $\Sigma_{\ell}$ that typically would be fitted from data along with the other hyper-parameters. But we may also set it at the discretion of a domain expert, which is particularly useful if none of the information sources is an unbiased estimator and we rely on regression to estimate the true objective. In case of the squared exponential kernel this coefficient is sometimes part of the formulation and referred to as "signal variance" (e.g., see [Rasmussen and Williams, 2006, p. 19]). For the sake of completeness, we detail the effect for our model of uncorrelated information sources stated in Sect. 2. Recall that we suppose $f \sim G P(\mu, \Sigma)$ with a mean function $\mu$ and covariance kernel $\Sigma$, and observe that the introduction of the new coefficient $\alpha_{\ell}$ does not affect $\mu(\ell, x)$. But it changes $\Sigma\left((\ell, x),\left(m, x^{\prime}\right)\right)$ to</p>
<p>$$
\Sigma\left((\ell, x),\left(m, x^{\prime}\right)\right)=\Sigma_{0}\left(x, x^{\prime}\right)+\mathbb{1}<em _ell="\ell">{\ell, m} \cdot \alpha</em>\right)
$$} \cdot \Sigma_{\ell}\left(x, x^{\prime</p>
<p>We observe that setting $\alpha_{\ell}$ to a larger value results in a bigger uncertainty. The gist is that then samples from such an information source have less influence in the Gaussian process regression (e.g., see Eq. (A.6) on pp. 200 in Rasmussen and Williams [2006]). It is instructive to consider the case that we observe a design $x$ at a noiseless and deterministic information source: then its observed output coincides with $f(\ell, x)$ (with zero variance). Our estimate $f(0, x)$ for $g(x)$, however, is a Gaussian random variable whose variance depends (in particular) on the uncertainty of the above information source as encoded in $\alpha_{\ell}$, since $\lambda_{\ell}(x)=0$ holds.</p>
<h2>B. Parallel Computation of the Cost-Sensitive Knowledge Gradient</h2>
<p>In Sect. 3 we detailed how the cost-sensitive Knowledge Gradient can be computed. In particular, we discretized the inner optimization problem in Eq. (1) to obtain</p>
<p>$$
\begin{gathered}
\mathbb{E}<em x_prime="x^{\prime">{n}\left[\frac{\max </em>\right)-\max } \in \mathcal{A}} \mu^{(n+1)}\left(0, x^{\prime<em _ell="\ell">{x^{\prime} \in \mathcal{A}} \mu^{(n)}\left(0, x^{\prime}\right)}{c</em>\right] \
\ell^{(n+1)}=\ell, x^{(n+1)}=x]
\end{gathered}
$$}(x)</p>
<p>Then we suggested exploiting the gradient of the CKG factor to obtain the next sample point and information source that maximize the expected gain per unit cost.</p>
<p>While we found this approach to work well in our experimental evaluation, there are scenarios where it is beneficial to also discretize the outer maximization problem $\operatorname{argmax}<em 0="0">{\ell \in[M]</em>|)$. In this section we propose a parallel algorithmic solution of essentially linear speed up, that makes efficient use of modern multi-core architectures.}, x \in \mathcal{A}} \operatorname{CKG}(\ell, x)$ and find the best $x \in \mathcal{A}$ by enumerating all CKG factors, for example when the CKG over $\mathcal{D}$ has many local maxima and therefore gradient-based optimization techniques may fail to find the best sample location. However, the running time of this approach is $O\left(M \cdot|\mathcal{A}|^{2} \cdot \log (|\mathcal{A}|)\right)$, and therefore may become a bottleneck in probing the domain with high accuracy. We note in passing that the logarithmic factor can be shaved off by a suitable sorting algorithm that exploits the property that we are sorting numbers and hence runs in time $O(|\mathcal{A</p>
<p>We present two ideas to improve the scalability: the first stems from the observation that the computations for different choices of the next sample decision $\left(\ell^{(n+1)}, x^{(n+1)}\right)$ are independent and thus can be done in parallel; the only required communication is to scatter the data and afterwards determine the best sample, hence the speedup is essentially linear. The second optimization is more intricate: we also parallelize the computation of the value of information for each $\left(\ell^{(n+1)}, x^{(n+1)}\right)$. Thus, we offer two levels of parallelization that can be used separately or combined to efficiently utilize several multi-core CPUs of a cluster.</p>
<p>First recall that the query cost only depends on the choice of $\left(\ell^{(n+1)}, x^{(n+1)}\right)$ and thus can be trivially incorporated; we omit it in the sequel for a</p>
<p>more compact presentation. Note that the same set of discrete points $\mathcal{A} \subset \mathcal{D}$ is used in the inner and the outer maximization problem only for the sake of simplicity; we could also use different sets and additionally exploit potential benefits of choosing them adaptively. Moreover, let $\rho$ : $[M]<em _rho_left_ell_prime="\rho\left(\ell^{\prime">{0} \times \mathcal{A} \rightarrow[(M+1) \cdot|\mathcal{A}|]$ be a bijection and define the $((M+1) \cdot|\mathcal{A}|)$-dimensional vectors $\bar{\mu}^{n}$ and $\bar{\sigma}(\ell, x)$ as $\bar{\mu}</em>(\ell, x)}, x^{\prime}\right)}^{n}=\mu^{(n)}\left(0, x^{\prime}\right)$ and $\bar{\sigma}^{n<em x_prime="x^{\prime">{\rho\left(\ell^{\prime}, x^{\prime}\right)}=\bar{\sigma}</em>(0,1)$}}^{n}(\ell, x)$ respectively, where $\bar{\sigma}^{n}(\ell, x)$ is the analog of $\bar{\sigma}^{n}(x)$ in [Frazier et al., 2009] (see also Sect. 3). Then we can define and using $Z \sim \mathcal{N</p>
<p>$$
h(\vec{a}, \vec{b})=\mathbb{E}\left[\max <em i="i">{i} a</em> \cdot Z\right]-\max }+b_{i<em i="i">{i} a</em>
$$</p>
<p>and thus observe that</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}<em x_prime="x^{\prime">{n}\left[\max </em>\right)\right] \
&amp; \quad \ell^{(n+1)}=\ell, x^{(n+1)}=x] \
= &amp; h\left(\bar{\mu}^{n}, \bar{\sigma}^{n}(\ell, x)\right)
\end{aligned}
$$} \in \mathcal{A}} \mu^{(n+1)}\left(0, x^{\prime}\right)-\max _{x^{\prime} \in \mathcal{A}} \mu^{(n)}\left(0, x^{\prime</p>
<p>Comparing these ideas to [Frazier et al., 2009], the first modification corresponds to a parallel execution of the outer loop of Algorithm 2 of Frazier et al. [2009] that computes the $h$-function, whereas the second aims at parallelizing each iteration of the loop itself. Due to space constraints we only provide a sketch of the parallel algorithm here and assume familiarity with the algorithm of Frazier et al. [2009].</p>
<p>We begin its computation by sorting the entries of $\bar{\mu}^{n}$ and $\bar{\sigma}^{n}(\ell, x)$ in parallel by ascending $\bar{\sigma}^{n}(\ell, x)$ value; if multiple entries have the same $\bar{\sigma}^{n}(\ell, x)$, we only keep the entry with largest value in $\bar{\mu}^{n}$ and discard all others, since they are dominated [Frazier et al., 2009]. W.l.o.g. we assume in the sequel that both vectors do not contain dominated entries and that $\bar{\sigma}^{n}(\ell, x)<em j="j">{i}&lt;\bar{\sigma}^{n}(\ell, x)</em>}$ whenever $i&lt;j$ for $i, j \in$ $[(M+1) \cdot|\mathcal{A}|]$. However, there is another type of domination that is even more important: for each $z \in$ $\mathbb{R}$ it is sufficient to find the $g(z):=\max \operatorname{argmax<em i="i">{i} \bar{\mu}</em>(\ell, x)}^{n}+$ $\bar{\sigma}^{n<em i="i">{i} \cdot z$, which is equivalent to removing those $i$ that never maximize $\bar{\mu}</em>(\ell, x)}^{n}+\bar{\sigma}^{n<em i="i">{i} \cdot z$ for any $z$. Let $n^{\prime}$ be the number of sample points that remain after this step and consider the sequence $(c)$ with $c</em>}=$ $\frac{\bar{\mu<em i_1="i+1">{i}^{n}-\bar{\mu}</em>(\ell, x)}^{n}}{\bar{\sigma}^{n<em i="i">{i+1}-\bar{\sigma}^{n}(\ell, x)</em>=$ $\infty$. We observe that the intervals between these points uniquely determine the respective $g(z)$ for all $z$ in that interval.}}$ for $i \in\left[n^{\prime}-1\right], c_{0}=-\infty$ and $c_{n^{\prime}</p>
<p>In order to parallelize Algorithm 1 of Frazier et al. [2009] that determines un-dominated candidates for
the next sample point (also called alternatives), we divide the previously sorted sequence $(c)$ into $p$ subsequences, where $p$ is the number of cores we wish to use. After running the linear scan algorithm of Frazier et al. [2009] on each subsequence separately and in parallel, we "merge" adjacent subsequences pairwise in parallel: when merging two sequences, say $L$ and $R$, where $L$ contains the smaller elements, it suffices to search for the rightmost element in $L$ not dominated by the leftmost one in $R$ (or vice versa). The reason is that we have ensured previously that no element is dominated by another within each subsequence.</p>
<p>After at most $\left\lceil\log _{2} p\right\rceil$ merging rounds, each core has determined which of the elements in its respective subsequence of $(c)$ are not dominated as required by Algorithm 2. Note that the "merging" procedure does not require actually transfer the elements among cores. Hence the final step, the summation in Eq. (14) on p. 605 in [Frazier et al., 2009] that calculates $h\left(\bar{\mu}^{n}, \bar{\sigma}^{n}(\ell, x)\right)$, is trivial to parallelize.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For simplicity we reuse the notation from the first model to denote their pendants in this model.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8770 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8770</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8770</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-274928787</p>
                <p><strong>Paper Title:</strong> Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation</p>
                <p><strong>Paper Abstract:</strong> Given the remarkable text generation capabilities of pre-trained language models, impressive results have been realized in graph-to-text generation. However, while learning from knowledge graphs, these language models are unable to fully grasp the structural information of the graph, leading to logical errors and missing key information. Therefore, an important research direction is to minimize the loss of graph structural information during the model training process. We propose a framework named Edge-Optimized Multi-Level Information refinement (EMLR), which aims to maximize the retention of the graph’s structural information from an edge perspective. Based on this framework, we further propose a new graph generation model, named TriELMR, highlighting the comprehensive interactive learning relationship between the model and the graph structure, as well as the importance of edges in the graph structure. TriELMR adopts three main strategies to reduce information loss during learning: (1) Knowledge Sequence Optimization; (2) EMLR Framework; and (3) Graph Activation Function. Experimental results reveal that TriELMR exhibits exceptional performance across various benchmark tests, especially on the webnlgv2.0 and Event Narrative datasets, achieving BLEU-4 scores of 66.5%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$66.5\%$$\end{document} and 37.27%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$37.27\%$$\end{document}, respectively, surpassing the state-of-the-art models. These demonstrate the advantages of TriELMR in maintaining the accuracy of graph structural information.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8770.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8770.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KSO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Sequence Optimization (KSO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing linearization strategy that reconstructs the input knowledge graph and produces optimized serialized input sequences for Transformer models via graph traversal to reduce information-entropy mismatch between graph structure and linear model input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Knowledge Sequence Optimization (graph linearization via traversal)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Reconstruct the KG, choose a start node (using degree-centrality heuristics), and produce a serialized ordering of triples by applying graph traversal algorithms (DFS, BFS, or topological sort with Kosaraju for cycles). The optimized sequence reduces the information-entropy gap between pretraining text distributions and the serialized KG input to the Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Directed knowledge graphs (general KGs, possibly with cycles and multiple connected components)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph traversal-based linearization: Depth-First Search (DFS), Breadth-First Search (BFS), and Topological Sorting (with Kosaraju algorithm to handle cycles/strongly connected components). Start node determined by in-/out-degree centrality.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Knowledge graph-to-text generation (text generation from KGs for WebNLGv2.0 and EventNarrative datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using BART-base baseline (BLEU-4 64.55%): KSO+DFS → BLEU-4 66.04% (Δ +1.49 vs baseline), METEOR 46.72%, ROUGE-L 75.96%, CIDEr 4.6. KSO+Kosaraju → BLEU-4 65.16%. Ablation: removal of KSO causes BLEU-4 drop of 1.3 percentage points (from TriELMR 66.22% to 64.92%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared different traversal methods: DFS performed best (BLEU-4 66.04%) > Kosaraju (65.16%) > BFS (64.85%) > default order (baseline 64.55%). Authors argue traversal choice affects convergence speed and final text quality.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces semantic/entropy mismatch between KG structure and linear Transformer input; empirically improves BLEU-4 and other metrics; DFS ordering aligns better with human sentence structure and helps model fitting/convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires heuristic selection of start node (degree-centrality) which can bias ordering; relies on traversal choice — suboptimal traversal degrades performance; introduces preprocessing complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Topological sorting is not directly applicable to graphs with cycles; requires Kosaraju remediation. Poor start-node choice or using BFS/default order can produce worse results and sentence incoherence; when removed, substantial performance drop indicates sensitivity to this step.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8770.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8770.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MultiRel-M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-relationship Heterogeneous Graph M</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph representation that extends the adjacency-matrix idea by adding explicit edge nodes (verbs/relations) to balance node and edge roles, representing subjects, objects, and relation nodes in a unified matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Multi-relationship Heterogeneous Graph (node+edge node adjacency)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constructs a (S+O)×(S+O) matrix analogous to adjacency, but augments it with explicit edge nodes V (relations) so the graph includes subject nodes S, object nodes O, and relation/edge nodes V; represents subject→verb and verb→object links and allows attention matrices to incorporate both node- and edge-centric indices.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Heterogeneous knowledge graphs (subject, relation, object triples); directed KGs with varied relation types</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Create index sequences X_graph_index and X_edge_index that map tokens to unique node or edge IDs; use mixed-dispersed mean pooling to aggregate text-level representations into node/edge representations according to these indices; integrate with attention via an added matrix M in attention computation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (KG-to-text), used inside ELMR mixed perceptual learning to supply structural information to the Transformer encoder</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used within ELMR and TriELMR; contributes to ELMR-enhanced BLEU-4 (ELMR+Linear BLEU-4 65.46% vs baseline 64.55%). Exact isolated ablation of Multi-relationship M not provided, but ELMR (which uses M) removal yields BLEU-4 drop of 0.87 from TriELMR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Contrasted with traditional adjacency matrices (node-only): M adds explicit edge nodes and in/out-degree considerations for edges, enhancing expressiveness; authors claim better capture of edge diversity than standard adjacency.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly models edges as first-class nodes capturing edge diversity and directionality; balances node/edge roles, potentially captures higher-order relationships and improves expressiveness and model fitting.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>More complex representation (larger index space), can introduce redundant information requiring further compression (handled by ELMR); increased memory/compute in attention matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Paper notes mixed perceptual learning can introduce redundant info; without careful pooling/fusion this representation could increase irrelevant information (necessitating the information-bottleneck style ELMR stages).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8770.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8770.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-Optimized Multi-Level Information Refinement (EMLR) Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An edge-centric multi-stage attention framework inspired by Information Bottleneck theory that separately learns mixed node-edge global representations and focused edge representations, then fuses them to reduce information loss and emphasize decisive edge features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Edge-Optimized Multi-Level Information refinement (mixed + edge perceptual learning)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Three-stage pipeline: (1) global multi-head attention for text-level features, (2) mixed perceptual learning aggregates node+edge structure via mixed-dispersed mean pooling into X_graph and applies attention (Attn_M,T) with MultiRel-M, maximizing I(T;Y)-β1 I(T;X), (3) edge perceptual learning aggregates edge-only indices into X_edge and applies edge-focused attention maximizing I(Z;Y)-β2 I(Z;X). Final fusion (direct or learnable linear α) combines T and Z.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Directed knowledge graphs (KGs) with explicit relations; heterogeneous graphs where edge information is crucial</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Uses the optimized serialized sequence (from KSO) as input; constructs X_global via multi-head attention, maps tokens to X_graph_index and X_edge_index, pools to produce X_graph and X_edge, then applies separate attention projections (Q,K,V) and computes Attn_Mixed and Attn_Edge, finally fuses representations (T + αZ).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Text generation from knowledge graphs (KG-to-text); integrated into Transformer encoder to improve generated text quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ELMR (added to baseline BART): BLEU-4 improved from 64.55% to 65.46% (ELMR+Linear). In full TriELMR (with KSO and GA) final BLEU-4 66.22%. Ablation: removing ELMR from TriELMR → BLEU-4 drops by 0.87 percentage points (from 66.22% to 65.35%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively and quantitatively to GAP (graph-aware attention) and Xia-GCN: TriELMR (which includes ELMR) achieves higher BLEU-4 than GAP and marginally outperforms Xia-GCN while being simpler than Xia-GCN's pipeline. The authors note ELMR outperforms simple attention modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicit edge-focused learning reduces structural information loss; applies information-bottleneck-inspired compression stages to retain task-relevant features and discard redundancy; improves BLEU and other metrics; fusion allows balancing global vs edge-specific signals.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds modeling complexity (two separate attention paths and fusion parameter α); can introduce redundant information in mixed representation without careful fusion; requires setting of β1/β2 hyperparameters and selection of fusion scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Mixed perceptual learning can include redundant information and needs fusion to avoid noise; when ELMR removed performance drops—indicating ELMR is necessary but also sensitive to its configuration. No explicit pathological graph types reported, but complexity may hinder scaling to much larger models/datasets without further validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8770.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8770.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Activation Function (GA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learnable rational activation function (ratio of polynomials) used in place of fixed activations (e.g., GELU) to adapt inductive biases per layer for better fitting of complex graph-structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Learnable Rational Graph Activation Function</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Parametric activation F(x) = P(x)/Q(x) where P(x) and Q(x) are polynomials with learnable coefficients (degrees m and n). Initialized by least-squares fitting to GELU (m=5, n=4, sample range [-3,3]) so it starts close to GELU then adapts during training. Applied in mixed and edge perceptual modules and across encoder/decoder layers.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applies to representations derived from knowledge graphs fed into Transformer encoders/decoders (general KG-to-text pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Not a graph-to-text conversion per se, but a learnable nonlinearity applied to transformed (pooled/attended) tensors (batch, SeqLen, Dffn) to better model nonlinear mappings arising from graph-structured inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation as part of TriELMR; used during fine-tuning of BART on WebNLGv2.0 and EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: removing GA from TriELMR reduces BLEU-4 by 0.88 points (from 66.22% to 65.34%), METEOR −0.43, ROUGE-L −0.46. Alone GA effect is modest; combined with ELMR shows interaction (removing both yields smaller additional drop in some metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared conceptually to fixed activations (GELU/ReLU); GA initialized to approximate GELU and then learns per-layer activation. Authors cite works on learnable activations for transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides layer-wise adaptable nonlinearity to better fit diverse graph-induced signals; allows the model to explore a richer function space and potentially reduce information loss during nonlinear transformation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds additional learnable parameters and slight computational overhead; single removal produces relatively small metric changes (i.e., limited effect alone), meaning benefit is incremental and interacts with ELMR.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When used alone (without ELMR or KSO) GA gives modest gains; possible risk of overfitting or increased optimization complexity on larger architectures not yet validated (paper limits experiments to BART-base).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8770.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8770.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TraversalMethods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Traversal Methods (DFS, BFS, Topological/Kosaraju)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard graph traversal algorithms used within KSO to produce different serializations of a knowledge graph for Transformer input; Kosaraju is used to handle cycles when topological ordering is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Traversal-based serialization (DFS/BFS/Topological w/ Kosaraju)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply DFS or BFS from a chosen start node to visit graph nodes and emit triple order; topological sort used when DAG-like structure is available, with Kosaraju algorithm applied to contract strongly connected components in cyclic graphs before ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Directed knowledge graphs (possibly cyclic; multiple connected components)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Standard DFS/BFS traversal emitting triples in visitation order; for topological sorting, collapse strongly connected components using Kosaraju, then produce an ordering of acyclic components.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation (serves as conversion step inside KSO)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical comparison on WebNLGv2.0: DFS BLEU-4 66.04%, BFS 64.85%, Kosaraju/topological 65.16%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>DFS outperformed BFS and Kosaraju-based ordering on the evaluated datasets; author analysis links DFS to human-style clause ordering and better model fitting.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, interpretable conversions; can dramatically affect model convergence and final quality; DFS shown to yield best BLEU among tested methods.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Choice of traversal and start node is heuristic and dataset-dependent; BFS and naive orders can interleave unrelated events causing unnatural sentence structure.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Topological sort cannot be applied to graphs with cycles without SCC processing; BFS produced worse BLEU and more incoherent ordering in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8770.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8770.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdjMatrix</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traditional Adjacency Matrix Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard graph representation using a 2-D node-indexed matrix marking node-to-node connections; noted as limited in expressiveness for edge diversity in KG-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adjacency matrix (node-to-node)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two-dimensional matrix with nodes as indices where entries indicate edges/connection strengths between node pairs; used historically to support GNN-based propagation and attention encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs and knowledge graphs (node-centric view)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Represent graph structure as adjacency matrix; possibly integrated into attention/GCN modules; does not directly linearize for sequence models but provides structural embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used in graph neural network encoders and structural encodings for KG-to-text or other graph tasks (as cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not directly evaluated in this paper; authors argue adjacency-only matrices have limited expressiveness for edge diversity compared to MultiRel-M.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper contrasts adjacency matrices (node-only) with Multi-relationship Heterogeneous Graph M (node+edge nodes) and claims M offers richer expressiveness for edges.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple and widely used; supports neighborhood aggregation and propagation in GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Edge diversity and attributes are not first-class; typical adjacency emphasizes node-to-node links and provides limited representation for relation types or edge nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May fail to capture heterogeneous relation semantics or edge-level features that are important for faithful text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8770.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8770.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN/GAT/GGNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Network Encoders (GCN, GAT, GGNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned prior approaches that operate directly on graph structures to capture node and relation information before text generation; used as baselines or inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN-based graph encoding (GCN, GAT, GGNN)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply convolutional/attention-based message passing across graph nodes (and sometimes edges) to produce node/edge embeddings that capture local/global graph structure for downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs, heterogeneous graphs, structured data graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Do not linearize; instead perform graph convolutions or attention (e.g., GCN, Graph Attention) to compute embeddings, which are then fed to decoder or fused with transformer inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation and structured-data-to-text tasks (cited works used GCN/GAT as encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced models: Xia-GCN reported BLEU-4 66.16% on WebNLG; Marcheggiani & Perez-Beltrachini GCN used in earlier work (no direct metric reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>TriELMR achieves similar or slightly better BLEU-4 (66.22%) than Xia-GCN (66.16%) while using a simpler single-model pipeline; authors note GCN-based approaches can require extra models/stages.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly model graph structure without losing topology through linearization; proven effective in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Often increases complexity (separate graph encoder + generator), may require heavier computation and integration effort with pre-trained transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not discussed in depth in this paper; authors point out more complex pipelines (e.g., Xia-GCN's two-model approach) increase training complexity compared to TriELMR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8770.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8770.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SequenceLinearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-based Linearization (default linear concatenation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The common baseline approach that serializes KG triples by simple concatenation/usual ordering and feeds them as a sequence to Transformer models; criticized for neglecting graph structure and causing information loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Default linearization (naive concatenation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Concatenate triplets into a single token sequence in dataset order or heuristic order without explicit traversal optimization or edge-aware indices, then feed into a sequence model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (triplet form)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Linear concatenation of triples in dataset-provided/default order; no structural reordering or explicit edge indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation (baseline approach for Transformer models)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline BART using default order: BLEU-4 64.55% on WebNLGv2.0; TriELMR improvements underscore shortcomings of default linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>KSO improves over default linearization by up to ~1.49 BLEU-4 points (DFS ordering), showing default linearization loses structural info.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple and fast to implement; no graph preprocessing required.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Ignores graph structural information, leads to higher information-entropy gap and worse model fitting and text coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Produces poorer BLEU/METEOR/ROUGE results compared to optimized linearizations; can interleave unrelated facts producing incoherent natural language ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Text generation from knowledge graphs with graph transformers <em>(Rating: 2)</em></li>
                <li>JointGT: graph-text joint representation learning for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Gap: a graph-aware language model framework for knowledge graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Efficient incorporation of knowledge graph information for enhanced graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Self-supervised graph masking pre-training for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>EventNarrative: a large-scale event-centric dataset for knowledge graph-to-text generation <em>(Rating: 2)</em></li>
                <li>WebNLG v2.0 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8770",
    "paper_id": "paper-274928787",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "KSO",
            "name_full": "Knowledge Sequence Optimization (KSO)",
            "brief_description": "A preprocessing linearization strategy that reconstructs the input knowledge graph and produces optimized serialized input sequences for Transformer models via graph traversal to reduce information-entropy mismatch between graph structure and linear model input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Knowledge Sequence Optimization (graph linearization via traversal)",
            "representation_description": "Reconstruct the KG, choose a start node (using degree-centrality heuristics), and produce a serialized ordering of triples by applying graph traversal algorithms (DFS, BFS, or topological sort with Kosaraju for cycles). The optimized sequence reduces the information-entropy gap between pretraining text distributions and the serialized KG input to the Transformer.",
            "graph_type": "Directed knowledge graphs (general KGs, possibly with cycles and multiple connected components)",
            "conversion_method": "Graph traversal-based linearization: Depth-First Search (DFS), Breadth-First Search (BFS), and Topological Sorting (with Kosaraju algorithm to handle cycles/strongly connected components). Start node determined by in-/out-degree centrality.",
            "downstream_task": "Knowledge graph-to-text generation (text generation from KGs for WebNLGv2.0 and EventNarrative datasets)",
            "performance_metrics": "Using BART-base baseline (BLEU-4 64.55%): KSO+DFS → BLEU-4 66.04% (Δ +1.49 vs baseline), METEOR 46.72%, ROUGE-L 75.96%, CIDEr 4.6. KSO+Kosaraju → BLEU-4 65.16%. Ablation: removal of KSO causes BLEU-4 drop of 1.3 percentage points (from TriELMR 66.22% to 64.92%).",
            "comparison_to_others": "Compared different traversal methods: DFS performed best (BLEU-4 66.04%) &gt; Kosaraju (65.16%) &gt; BFS (64.85%) &gt; default order (baseline 64.55%). Authors argue traversal choice affects convergence speed and final text quality.",
            "advantages": "Reduces semantic/entropy mismatch between KG structure and linear Transformer input; empirically improves BLEU-4 and other metrics; DFS ordering aligns better with human sentence structure and helps model fitting/convergence.",
            "disadvantages": "Requires heuristic selection of start node (degree-centrality) which can bias ordering; relies on traversal choice — suboptimal traversal degrades performance; introduces preprocessing complexity.",
            "failure_cases": "Topological sorting is not directly applicable to graphs with cycles; requires Kosaraju remediation. Poor start-node choice or using BFS/default order can produce worse results and sentence incoherence; when removed, substantial performance drop indicates sensitivity to this step.",
            "uuid": "e8770.0",
            "source_info": {
                "paper_title": "Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MultiRel-M",
            "name_full": "Multi-relationship Heterogeneous Graph M",
            "brief_description": "A graph representation that extends the adjacency-matrix idea by adding explicit edge nodes (verbs/relations) to balance node and edge roles, representing subjects, objects, and relation nodes in a unified matrix.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Multi-relationship Heterogeneous Graph (node+edge node adjacency)",
            "representation_description": "Constructs a (S+O)×(S+O) matrix analogous to adjacency, but augments it with explicit edge nodes V (relations) so the graph includes subject nodes S, object nodes O, and relation/edge nodes V; represents subject→verb and verb→object links and allows attention matrices to incorporate both node- and edge-centric indices.",
            "graph_type": "Heterogeneous knowledge graphs (subject, relation, object triples); directed KGs with varied relation types",
            "conversion_method": "Create index sequences X_graph_index and X_edge_index that map tokens to unique node or edge IDs; use mixed-dispersed mean pooling to aggregate text-level representations into node/edge representations according to these indices; integrate with attention via an added matrix M in attention computation.",
            "downstream_task": "Graph-to-text generation (KG-to-text), used inside ELMR mixed perceptual learning to supply structural information to the Transformer encoder",
            "performance_metrics": "Used within ELMR and TriELMR; contributes to ELMR-enhanced BLEU-4 (ELMR+Linear BLEU-4 65.46% vs baseline 64.55%). Exact isolated ablation of Multi-relationship M not provided, but ELMR (which uses M) removal yields BLEU-4 drop of 0.87 from TriELMR.",
            "comparison_to_others": "Contrasted with traditional adjacency matrices (node-only): M adds explicit edge nodes and in/out-degree considerations for edges, enhancing expressiveness; authors claim better capture of edge diversity than standard adjacency.",
            "advantages": "Explicitly models edges as first-class nodes capturing edge diversity and directionality; balances node/edge roles, potentially captures higher-order relationships and improves expressiveness and model fitting.",
            "disadvantages": "More complex representation (larger index space), can introduce redundant information requiring further compression (handled by ELMR); increased memory/compute in attention matrices.",
            "failure_cases": "Paper notes mixed perceptual learning can introduce redundant info; without careful pooling/fusion this representation could increase irrelevant information (necessitating the information-bottleneck style ELMR stages).",
            "uuid": "e8770.1",
            "source_info": {
                "paper_title": "Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "EMLR",
            "name_full": "Edge-Optimized Multi-Level Information Refinement (EMLR) Framework",
            "brief_description": "An edge-centric multi-stage attention framework inspired by Information Bottleneck theory that separately learns mixed node-edge global representations and focused edge representations, then fuses them to reduce information loss and emphasize decisive edge features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Edge-Optimized Multi-Level Information refinement (mixed + edge perceptual learning)",
            "representation_description": "Three-stage pipeline: (1) global multi-head attention for text-level features, (2) mixed perceptual learning aggregates node+edge structure via mixed-dispersed mean pooling into X_graph and applies attention (Attn_M,T) with MultiRel-M, maximizing I(T;Y)-β1 I(T;X), (3) edge perceptual learning aggregates edge-only indices into X_edge and applies edge-focused attention maximizing I(Z;Y)-β2 I(Z;X). Final fusion (direct or learnable linear α) combines T and Z.",
            "graph_type": "Directed knowledge graphs (KGs) with explicit relations; heterogeneous graphs where edge information is crucial",
            "conversion_method": "Uses the optimized serialized sequence (from KSO) as input; constructs X_global via multi-head attention, maps tokens to X_graph_index and X_edge_index, pools to produce X_graph and X_edge, then applies separate attention projections (Q,K,V) and computes Attn_Mixed and Attn_Edge, finally fuses representations (T + αZ).",
            "downstream_task": "Text generation from knowledge graphs (KG-to-text); integrated into Transformer encoder to improve generated text quality",
            "performance_metrics": "ELMR (added to baseline BART): BLEU-4 improved from 64.55% to 65.46% (ELMR+Linear). In full TriELMR (with KSO and GA) final BLEU-4 66.22%. Ablation: removing ELMR from TriELMR → BLEU-4 drops by 0.87 percentage points (from 66.22% to 65.35%).",
            "comparison_to_others": "Compared qualitatively and quantitatively to GAP (graph-aware attention) and Xia-GCN: TriELMR (which includes ELMR) achieves higher BLEU-4 than GAP and marginally outperforms Xia-GCN while being simpler than Xia-GCN's pipeline. The authors note ELMR outperforms simple attention modifications.",
            "advantages": "Explicit edge-focused learning reduces structural information loss; applies information-bottleneck-inspired compression stages to retain task-relevant features and discard redundancy; improves BLEU and other metrics; fusion allows balancing global vs edge-specific signals.",
            "disadvantages": "Adds modeling complexity (two separate attention paths and fusion parameter α); can introduce redundant information in mixed representation without careful fusion; requires setting of β1/β2 hyperparameters and selection of fusion scheme.",
            "failure_cases": "Mixed perceptual learning can include redundant information and needs fusion to avoid noise; when ELMR removed performance drops—indicating ELMR is necessary but also sensitive to its configuration. No explicit pathological graph types reported, but complexity may hinder scaling to much larger models/datasets without further validation.",
            "uuid": "e8770.2",
            "source_info": {
                "paper_title": "Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GA",
            "name_full": "Graph Activation Function (GA)",
            "brief_description": "A learnable rational activation function (ratio of polynomials) used in place of fixed activations (e.g., GELU) to adapt inductive biases per layer for better fitting of complex graph-structured data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Learnable Rational Graph Activation Function",
            "representation_description": "Parametric activation F(x) = P(x)/Q(x) where P(x) and Q(x) are polynomials with learnable coefficients (degrees m and n). Initialized by least-squares fitting to GELU (m=5, n=4, sample range [-3,3]) so it starts close to GELU then adapts during training. Applied in mixed and edge perceptual modules and across encoder/decoder layers.",
            "graph_type": "Applies to representations derived from knowledge graphs fed into Transformer encoders/decoders (general KG-to-text pipeline)",
            "conversion_method": "Not a graph-to-text conversion per se, but a learnable nonlinearity applied to transformed (pooled/attended) tensors (batch, SeqLen, Dffn) to better model nonlinear mappings arising from graph-structured inputs.",
            "downstream_task": "KG-to-text generation as part of TriELMR; used during fine-tuning of BART on WebNLGv2.0 and EventNarrative",
            "performance_metrics": "Ablation: removing GA from TriELMR reduces BLEU-4 by 0.88 points (from 66.22% to 65.34%), METEOR −0.43, ROUGE-L −0.46. Alone GA effect is modest; combined with ELMR shows interaction (removing both yields smaller additional drop in some metrics).",
            "comparison_to_others": "Compared conceptually to fixed activations (GELU/ReLU); GA initialized to approximate GELU and then learns per-layer activation. Authors cite works on learnable activations for transformers.",
            "advantages": "Provides layer-wise adaptable nonlinearity to better fit diverse graph-induced signals; allows the model to explore a richer function space and potentially reduce information loss during nonlinear transformation.",
            "disadvantages": "Adds additional learnable parameters and slight computational overhead; single removal produces relatively small metric changes (i.e., limited effect alone), meaning benefit is incremental and interacts with ELMR.",
            "failure_cases": "When used alone (without ELMR or KSO) GA gives modest gains; possible risk of overfitting or increased optimization complexity on larger architectures not yet validated (paper limits experiments to BART-base).",
            "uuid": "e8770.3",
            "source_info": {
                "paper_title": "Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "TraversalMethods",
            "name_full": "Graph Traversal Methods (DFS, BFS, Topological/Kosaraju)",
            "brief_description": "Standard graph traversal algorithms used within KSO to produce different serializations of a knowledge graph for Transformer input; Kosaraju is used to handle cycles when topological ordering is desired.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Traversal-based serialization (DFS/BFS/Topological w/ Kosaraju)",
            "representation_description": "Apply DFS or BFS from a chosen start node to visit graph nodes and emit triple order; topological sort used when DAG-like structure is available, with Kosaraju algorithm applied to contract strongly connected components in cyclic graphs before ordering.",
            "graph_type": "Directed knowledge graphs (possibly cyclic; multiple connected components)",
            "conversion_method": "Standard DFS/BFS traversal emitting triples in visitation order; for topological sorting, collapse strongly connected components using Kosaraju, then produce an ordering of acyclic components.",
            "downstream_task": "KG-to-text generation (serves as conversion step inside KSO)",
            "performance_metrics": "Empirical comparison on WebNLGv2.0: DFS BLEU-4 66.04%, BFS 64.85%, Kosaraju/topological 65.16%.",
            "comparison_to_others": "DFS outperformed BFS and Kosaraju-based ordering on the evaluated datasets; author analysis links DFS to human-style clause ordering and better model fitting.",
            "advantages": "Simple, interpretable conversions; can dramatically affect model convergence and final quality; DFS shown to yield best BLEU among tested methods.",
            "disadvantages": "Choice of traversal and start node is heuristic and dataset-dependent; BFS and naive orders can interleave unrelated events causing unnatural sentence structure.",
            "failure_cases": "Topological sort cannot be applied to graphs with cycles without SCC processing; BFS produced worse BLEU and more incoherent ordering in experiments.",
            "uuid": "e8770.4",
            "source_info": {
                "paper_title": "Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AdjMatrix",
            "name_full": "Traditional Adjacency Matrix Representation",
            "brief_description": "Standard graph representation using a 2-D node-indexed matrix marking node-to-node connections; noted as limited in expressiveness for edge diversity in KG-to-text tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Adjacency matrix (node-to-node)",
            "representation_description": "Two-dimensional matrix with nodes as indices where entries indicate edges/connection strengths between node pairs; used historically to support GNN-based propagation and attention encodings.",
            "graph_type": "General graphs and knowledge graphs (node-centric view)",
            "conversion_method": "Represent graph structure as adjacency matrix; possibly integrated into attention/GCN modules; does not directly linearize for sequence models but provides structural embedding.",
            "downstream_task": "Used in graph neural network encoders and structural encodings for KG-to-text or other graph tasks (as cited prior work)",
            "performance_metrics": "Not directly evaluated in this paper; authors argue adjacency-only matrices have limited expressiveness for edge diversity compared to MultiRel-M.",
            "comparison_to_others": "Paper contrasts adjacency matrices (node-only) with Multi-relationship Heterogeneous Graph M (node+edge nodes) and claims M offers richer expressiveness for edges.",
            "advantages": "Simple and widely used; supports neighborhood aggregation and propagation in GNNs.",
            "disadvantages": "Edge diversity and attributes are not first-class; typical adjacency emphasizes node-to-node links and provides limited representation for relation types or edge nodes.",
            "failure_cases": "May fail to capture heterogeneous relation semantics or edge-level features that are important for faithful text generation.",
            "uuid": "e8770.5",
            "source_info": {
                "paper_title": "Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GCN/GAT/GGNN",
            "name_full": "Graph Neural Network Encoders (GCN, GAT, GGNN)",
            "brief_description": "Mentioned prior approaches that operate directly on graph structures to capture node and relation information before text generation; used as baselines or inspiration.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GNN-based graph encoding (GCN, GAT, GGNN)",
            "representation_description": "Apply convolutional/attention-based message passing across graph nodes (and sometimes edges) to produce node/edge embeddings that capture local/global graph structure for downstream generation.",
            "graph_type": "Knowledge graphs, heterogeneous graphs, structured data graphs",
            "conversion_method": "Do not linearize; instead perform graph convolutions or attention (e.g., GCN, Graph Attention) to compute embeddings, which are then fed to decoder or fused with transformer inputs.",
            "downstream_task": "KG-to-text generation and structured-data-to-text tasks (cited works used GCN/GAT as encoders)",
            "performance_metrics": "Referenced models: Xia-GCN reported BLEU-4 66.16% on WebNLG; Marcheggiani & Perez-Beltrachini GCN used in earlier work (no direct metric reported here).",
            "comparison_to_others": "TriELMR achieves similar or slightly better BLEU-4 (66.22%) than Xia-GCN (66.16%) while using a simpler single-model pipeline; authors note GCN-based approaches can require extra models/stages.",
            "advantages": "Directly model graph structure without losing topology through linearization; proven effective in prior literature.",
            "disadvantages": "Often increases complexity (separate graph encoder + generator), may require heavier computation and integration effort with pre-trained transformers.",
            "failure_cases": "Not discussed in depth in this paper; authors point out more complex pipelines (e.g., Xia-GCN's two-model approach) increase training complexity compared to TriELMR.",
            "uuid": "e8770.6",
            "source_info": {
                "paper_title": "Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SequenceLinearization",
            "name_full": "Sequence-based Linearization (default linear concatenation)",
            "brief_description": "The common baseline approach that serializes KG triples by simple concatenation/usual ordering and feeds them as a sequence to Transformer models; criticized for neglecting graph structure and causing information loss.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Default linearization (naive concatenation)",
            "representation_description": "Concatenate triplets into a single token sequence in dataset order or heuristic order without explicit traversal optimization or edge-aware indices, then feed into a sequence model.",
            "graph_type": "Knowledge graphs (triplet form)",
            "conversion_method": "Linear concatenation of triples in dataset-provided/default order; no structural reordering or explicit edge indexing.",
            "downstream_task": "KG-to-text generation (baseline approach for Transformer models)",
            "performance_metrics": "Baseline BART using default order: BLEU-4 64.55% on WebNLGv2.0; TriELMR improvements underscore shortcomings of default linearization.",
            "comparison_to_others": "KSO improves over default linearization by up to ~1.49 BLEU-4 points (DFS ordering), showing default linearization loses structural info.",
            "advantages": "Simple and fast to implement; no graph preprocessing required.",
            "disadvantages": "Ignores graph structural information, leads to higher information-entropy gap and worse model fitting and text coherence.",
            "failure_cases": "Produces poorer BLEU/METEOR/ROUGE results compared to optimized linearizations; can interleave unrelated facts producing incoherent natural language ordering.",
            "uuid": "e8770.7",
            "source_info": {
                "paper_title": "Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Text generation from knowledge graphs with graph transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "JointGT: graph-text joint representation learning for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Gap: a graph-aware language model framework for knowledge graph-to-text generation",
            "rating": 2,
            "sanitized_title": "gap_a_graphaware_language_model_framework_for_knowledge_graphtotext_generation"
        },
        {
            "paper_title": "Efficient incorporation of knowledge graph information for enhanced graph-to-text generation",
            "rating": 2,
            "sanitized_title": "efficient_incorporation_of_knowledge_graph_information_for_enhanced_graphtotext_generation"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "Self-supervised graph masking pre-training for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "selfsupervised_graph_masking_pretraining_for_graphtotext_generation"
        },
        {
            "paper_title": "EventNarrative: a large-scale event-centric dataset for knowledge graph-to-text generation",
            "rating": 2,
            "sanitized_title": "eventnarrative_a_largescale_eventcentric_dataset_for_knowledge_graphtotext_generation"
        },
        {
            "paper_title": "WebNLG v2.0",
            "rating": 2,
            "sanitized_title": "webnlg_v20"
        }
    ],
    "cost": 0.016491,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation</p>
<p>Yao Zheng zhengyao99@gs.zzu.edu.cn 
Henan Institute of Advanced Technology
Zhengzhou University
450001Zhengzhou, HenanChina</p>
<p>Jingyuan Li li.jingyuan.jerry@btbu.edu.cn 0009-0008-9384-9455
School of Computer and Artificial Intelligence
Beijing Technology and Business University
100048Beijing, BeijingChina</p>
<p>Jianhe Cen 
Henan Institute of Advanced Technology
Zhengzhou University
450001Zhengzhou, HenanChina</p>
<p>Shiqi Sun 
Henan Institute of Advanced Technology
Zhengzhou University
450001Zhengzhou, HenanChina</p>
<p>Dahu Yin 
Henan Institute of Advanced Technology
Zhengzhou University
450001Zhengzhou, HenanChina</p>
<p>Yuanzhuo Wang wangyuanzhuo@ict.ac.cn 
Institute of Computing Technology
Chinese Academy of Sciences
100190Beijing, BeijingChina</p>
<p>Edge-centric optimization: a novel strategy for minimizing information loss in graph-to-text generation
DE5A34F6F572EA3A0F1BDF594D87BD2410.1007/s40747-024-01690-yReceived: 15 June 2024 / Accepted: 6 November 2024
Given the remarkable text generation capabilities of pre-trained language models, impressive results have been realized in graph-to-text generation.However, while learning from knowledge graphs, these language models are unable to fully grasp the structural information of the graph, leading to logical errors and missing key information.Therefore, an important research direction is to minimize the loss of graph structural information during the model training process.We propose a framework named Edge-Optimized Multi-Level Information refinement (EMLR), which aims to maximize the retention of the graph's structural information from an edge perspective.Based on this framework, we further propose a new graph generation model, named TriELMR, highlighting the comprehensive interactive learning relationship between the model and the graph structure, as well as the importance of edges in the graph structure.TriELMR adopts three main strategies to reduce information loss during learning: (1) Knowledge Sequence Optimization; (2) EMLR Framework; and (3) Graph Activation Function.Experimental results reveal that TriELMR exhibits exceptional performance across various benchmark tests, especially on the webnlgv2.0and Event Narrative datasets, achieving BLEU-4 scores of 66.5% and 37.27%, respectively, surpassing the state-of-the-art models.These demonstrate the advantages of TriELMR in maintaining the accuracy of graph structural information.KeywordsGraph-generated text • Knowledge representation • Knowledge graphs • Graph theory • Edge-aware attention B Jingyuan Li</p>
<p>Introduction</p>
<p>Knowledge graphs (KG) [1], with their unique "graph" format, illustrate complex relationships between entities with high efficiency, and can integrate multi-source heterogeneous information.They have achieved significant results in areas such as collaborative social networks [2], search engine optimization [3], the Internet of Things [4], financial services [5], and healthcare [6].However, Gerasimou [7] pointed out that despite the advantages of knowledge graphs in organizing and handling complex information, understanding natural language remains a more intuitive and straightforward way for most ordinary users.Therefore, transforming complex graph information into natural language that people can easily understand is of great importance in many cases, which helps better explore and utilize the potential of knowledge graphs.For example, the transformation of medical knowledge graphs into medical reports can help both patients and doctors to have a better understanding of the illness [8].It can also enable chatbots to extract information from knowledge graphs and respond in natural language for more human-like dialogues [9].Moreover, further research findings [10] show that through this transformation, we can uncover more hidden relationships and patterns in the process of converting graphs into text [11].This powerful tool not only helps people understand and apply knowledge graphs more intuitively, but also greatly improves the user experience in human-computer interaction.</p>
<p>Early researchers typically used rule-based or templatebased methods, which involve arranging certain elements from the knowledge graph in a fixed order, and filling them into predefined templates.For instance, Holmes-Higgin [12] used a rule-based system to automatically generate text describing a series of events, pioneering rule-based text generation research.With the development of deep learning, more advanced models like Transformer [13] and GPT [14] have been applied for text generation.For example, Koncel-Kedziorski et al. [15] introduced a Transformerbased approach, where the generation of text from knowledge graphs strikes a superior balance between semantic coherence and factual consistency, with the benefit of its self-attention mechanism and an evenly-distributed context window.Later with the continuous emergence of various outstanding models, the potential of pre-training tasks has gradually been recognized and deeply exploited by researchers.Ke et al. [16] designed new pre-training tasks specifically for knowledge graph structures, and proposed the outstanding JointGT model.Li et al. [17] proposed a unified pre-training framework that integrates graph structure information with text generation tasks, significantly enhancing graph-to-text generation performance through multi-task and contrastive learning strategies.Subsequently, understanding information from knowledge graphs has gradually become a key element for breakthroughs in the field of text generation.Considering this, Colas et al. [18] designed a graph-aware attention mechanism that enables the model to learn global information from knowledge graphs.Xia et al. [19] proposed a model that captures information through GCN and ultimately feeds it back into the Transformer model, improving the efficiency of graph-to-text generation.</p>
<p>Current research on generating text from knowledge graphs begins with different forms of graphs, using language models to simulate and learn graph information.These methods have proven crucial in generating text with good semantic coherence and factual consistency, as well as generalizing better to unseen knowledge graph patterns.However, they still face the following challenges.</p>
<p>Challenge 1: Loss of Input Graph Structure Information.</p>
<p>Although sequence-based studies have performed excellently in a large number of tasks, they still fall short when it comes to handling knowledge graph-to-text generation.Knowledge graphs contain rich structural information, such as relationships between nodes and their order.However, common approaches often treat the knowledge graph as an unordered sequence and directly inject it into the model, without fully utilizing its structural information.This type of transformation may dilute the complexity of the original graph, hence reducing the quality of the information available to the model during the learning process, which can affect the coherence and accuracy of the generated text.</p>
<p>Challenge 2: The Neglect of Edge Importance in Graphs.In previous studies, the importance of edges in a knowledge graph was often underestimated or even ignored, which to a certain extent limited the expressive power and precision of the model.This limitation is even more serious when dealing with complex graphs.For instance, in heterogeneous graphs, there are multiple types of nodes and various complex relationships, all of which are embodied in the form of edges.Current model is unable to fully understand and parse these types of graphs, nor harness the information from these complex edges.As a result, the output generated presents a discrepancy from the original input information, failing to achieve the desired effect.To parse these complex graphs to the greatest extent possible, existing studies often handle this by simplifying, such as inputting all the information of the entire graph into the model, treating edge weights equal to nodes.This somewhat limits the model's ability to comprehend the complexity of heterogeneous graphs, which in turn negatively impacts the graph's processing and subsequent text generation.</p>
<p>To address the challenges mentioned above, and to retain the knowledge information of the graph to the maximum extent, we propose the TriELMR model.(1) To cope with the loss of structural information from graph inputs, the TriELMR model designs a knowledge sequence recombination scheme, which takes into account the mismatch between the serialized input information of the transformer model and the knowledge graph, and the fact that default linear concatenation cannot represent graph structures.Hence, it restores the dataset to the graph, generates optimized linear sequences through a traversal algorithm, and achieves the expected information recombination.(2) To address the issue of overlooking the importance of edges in graphs, we propose the ELMR framework, which consists of two main parts: perceptual attention calculation and perception fusion.The perceptual attention calculation includes a mixed perceptual learning unit and an edge perceptual learning unit, which focuses on preserving edge information.These allow the model to maximally learn the most useful graph comprehensive information and edge information for the final text generation.The results of the two learning units are fused into a final output by the perception fusion component of the ELMR framework, which possesses less redundant information and retains the information contributing the most to the final result.(3) We introduce a learnable graph activation function.We hope that the TriELMR model can explore a broader mapping space while learning the complex graph information, stimulate the model's potential learning ability, and enhance the model's diversity and generalization capabilities.The combination of these three components constitutes the TriELMR model, which introduces a new model for the field of graph-to-text generation and achieved noteworthy results.</p>
<p>In general,our major contributions are as follows:</p>
<p>• We introduce a new model for graph-to-text generation, TriELMR (Multi-dimensional Maintains the original structure of Knowledge Graph), which can pay constant attention to the structural information of the graph during the information transformation process, and ultimately generate high-quality text.• Inspired by information theory, we propose the EMLR Framework.The framework optimizes the information bottleneck of graph-to-text generation through mixed perceptual learning and edge perceptual learning, resulting in better-performing generated text.</p>
<p>• Through experiments, we analyze information from the input end, attention module, and activation function, explore the loss of graph structural information on these three dimensions, and analyze their impact on the work of text generation.• Compared with the models in the graph-to-text task, the TriELMR model scores highly on multiple indices in the general datasets WebNLGv2.0[20] and EventNarrative [21], scoring 66.22% and 37.27% on BLEU-4 respectively, all higher than the most advanced models, bringing the possibility of exploration on edges in the field of graph-to-text generation.</p>
<p>The rest of this paper is structured as follows.Section 2 presents related work in the field of text generation from graphs.Section 3 introduces the background knowledge needed to understand the TriELMR model.Section 4 contains the theories for the three tasks in the TriELMR model.Section 5 introduces the information related to the experiments.Section 6 discusses the constraints of our approach and potential future directions, and the last section is a summary of our work.</p>
<p>Related work</p>
<p>In this section, we discuss the related work addressing the two challenges in the domain of knowledge graph-to-text generation mentioned earlier.These previous research contributions have provided valuable guidance and insights for our work.By deeply understanding and thoroughly analyzing these contributions, we have designed the TriELMR model.This model is capable of preserving the structural information of the graph as much as possible during text generation tasks, thereby enhancing the accuracy of the generated text.</p>
<p>Different forms of graph information input</p>
<p>In the task of text generation from knowledge graphs, we can use various forms of input sequences [22].We can directly handle the graphic structure with Graph Neural Networks (GNN) [23], or use models like Transformer to process linear sequences.GNN is a powerful model that directly operates and handles graph structures, effectively capturing complex relationships between nodes.For example, Zhao et al. [24] proposed a method that integrates a graph structure-aware module with pre-trained language models, utilizing relative distance encoding, planning selection, and similarity distinction to generate coherent and accurate text.Later studies introduced convolution operations, giving rise to GCNs.Marcheggiani and Perez-Beltrachini [25] applied the GCN model for text generation from graphs, where these models can understand the structure and information of graphics from a global and local perspective.</p>
<p>Meanwhile, many researchers used sequence models to process graph structured data [26].Unlike graph-based methods that directly operate on graph structures, sequence-based methods first need to convert the graph structure into a linear sequence.The most typical ones are models with Transformer architecture, such as BERT [27], BART [28], T5 [29], and GPT [14], which are large pre-trained language models.Transformers can capture long-distance dependencies in sequence, even if they may not handle graphical structure idealistically.However, due to their powerful parallelism and ability to handle long sequences, Transformers have achieved significant success in many generation tasks.To extend this success to text generation from graph tasks, scholars have investigated how to better translate graph structure into a linear sequence.Han [30] believed that the traditional linearization ignores the structural information of the graph, and PLM is usually pre-trained on free text.This approach does not match the G2T task.Therefore, their team proposed a graph masking pre-training strategy that requires no supervisory signal and does not adjust the underlying pre-training encoder-decoder structure.They adapted the pre-training strategy to suit G2T.Liu [31] also recognized the problems with linearization, as KG order is usually achieved through heuristics, which leads to semantic consistency and order issues.He therefor proposed a method to predict the optimization knowledge description order and further enhance the consistency between the generated sentences and KG through syntactic and semantic norms.Authors attempted to improve the traditional linearization KG input method using the supervised order extracted from the title and semantic context scoring function.This approach helps generate more accurate and coherent text that is consistent with KG's structure.Additionally, Shi [32] proposed a cross-structure attention distillation mechanism to enhance text generation from knowledge graphs by capturing rich contextual semantic representations.</p>
<p>Preservation of graph structure information</p>
<p>In the research direction of maintaining graph structure, Beck et al. [33] designed a GGNN-based architectural neural network structure that integrates the Transformer architecture for better graph structure handling.The GGNN employs a gating mechanism to control information flow, allowing the neural network to more accurately learn and comprehend the relationship between nodes and edges in the graph.Veli¡cković et al. [34] introduced a self-attention mechanism targeted at graph data.The GAT model uses the attention mechanism to compute the relevance between nodes in the graph, further using it to build a soft adjacency matrix for feature transmission.This adaptation allows the GAT model to balance information interactions between nodes in an adaptive way, forming a more enriched nodes representation.Wei et al. [35] proposed a novel GAT-based method called GATH, which includes an entity-specific attention network module and an entity-relation joint attention network to enhance the completion of heterogeneous knowledge graphs.Ke et al. [16] noticed the advantages of graph attention mechanisms, and added three pre-training tasks based on the large Bart and T5 models.They also inserted a graph attention mechanism into the encoder.The Colas utilized thoughts from Colas et al. [18], recognizing the power of the graph attention mechanism, and abandoning the cumbersome, lowcost-effective pre-training tasks.Based on the original graph attention mechanism, they added connection encoding and graph encoding matrix, enriching graph information learned by the model from a node perspective.This makes the graph not only focus on the adjacency matrix but also explore the relationship between relationships and entities.Xia et al. [19] proposed a graph convolutional-based network to incorporate structural information of knowledge graphs into pre-trained generative models, achieving superior performance while reducing computational overhead.</p>
<p>Inspired by these two works, we explored the issue of structural information loss from multiple perspectives, innovating the text generation model from graphs based on the pre-trained Bart model.Through practical experiments, we proved that TriELMR can better follow graph structural diversity, edge directionality, and high degree of freedom.Effectively reducing information loss, it generates more accurate and ideal text.This conclusion shows the important value of our research and provides references for related work in text generation from graphs.</p>
<p>Preliminaries</p>
<p>Task definition</p>
<p>Our goal is to generate a text that accurately describes a given knowledge graph.Given a graph G = (V , E), where V = {e 1 , e 2 , . . ., e V } is a set of entity vectors, and E = (r i j ) ⊂ V × R × V represents all the relationships between the entities.An entity vector e i represents a specific head or tail node in the knowledge graph, with each node containing its specific attributes or features.A relationship r i j represents a specific relationship between entity e i and entity e j .To input the non-linear knowledge graph into the model, the graph G needs to be linearized into a sequence G linear = (w 1 , w 2 , . . ., w m ), where m is the maximum input sequence length, and w i represents the tokenized sequence of entities or relationships, ensuring the preservation of the original graph G's structure and meaning.After processing through a large model, a text sequence X = (x 1 , x 2 , . . ., x n ) is generated, where x i are the characters in the generated sequence.The generated sequence X should accurately convey the meaning of the input graph while being grammatically correct and contextually appropriate.</p>
<p>The Information Bottleneck method</p>
<p>The Information Bottleneck Theory [36] is a method of information theory, initially proposed by Naftali Tishby et.al.This theory aims to solve how to most efficiently extract information related to the output data from the input data.In the information bottleneck method, the goal is to compress the input data X to get some compact representation T of the data, while maximizing the amount of information about the relevant output Y .This problem can be formalized as:
max p(t|x) I (T ; Y ) − β I (T ; X )(1)
where I (T ; Y ) is the mutual information between T and Y , and I (T ; X ) is the mutual information between T and X .β is a trade-off parameter that controls our balance between preserving information about X and preserving relevant information about Y .</p>
<p>Model approach</p>
<p>Based on the aforementioned task description, this paper proposes a new model approach named TriELMR, which originates from multiple dimensions and aims to reduce the loss of information during the model's learning process of knowledge graph structural information.The overall architecture of the TriELMR model is shown in Fig. 1.The • Knowledge Sequence Optimization.The information input of the Transformer structure is in a serialized form.By rearranging the order of the knowledge triplets and adjusting the input of the triplets to align with the structure of the graph, we can input the knowledge as a serialized form into the text.• EMLR Framework.Edges in the graph-to-text task are explorable.Influenced by information bottleneck theory [37], we set a multi-level attention framework in our work.The framework uses mixed perceptual learning and edge perceptual learning to implement a top-down knowledge learning approach.Evaluating the model from a broad scope first, then focusing on the details, and finally, both are fused for perceptual learning to fully learn the structural information of the graph.• Graph Activation Function.In traditional model training, each learning layer uses a fixed activation function for computation.However, when training models with complex and diverse graph structures, fixed activation functions limit the potential for transforming linear to nonlinear representations in knowledge graphs.Therefore, to accommodate the complex structure of graphs, we use learnable graph activation functions in model fitting to handle the diversity of graph variations.</p>
<p>The following sections describe the above three steps in detail.</p>
<p>Knowledge sequence optimization</p>
<p>The Transformer architecture [13] was initially designed for text tasks, therefore its input sequence has a linear structure, while a graph is a nonlinear structure [38,39].As a result, in the graph-to-text generation task, this transformation process could lead to a significant change in information entropy, which may result in semantic understanding deviation.If we directly concatenate the triplets in the dataset into illogically serialized information, the information entropy between the pre-training semantics and the final obtained semantics will be very large.The model training process would require multiple iterations of calculations, making the final fitting difficult.In this paper, the method of knowledge sequence optimization [40] is utilized, where rearranging the input triplets can effectively reduce this difference.As shown in Fig. 2, the traditional sequence a and the rearranged sequence b are compared with the final result.The information entropy gap between the rearranged sequence b and the answer is smaller, making it easier to fit.</p>
<p>Start node</p>
<p>The determination of the starting node for traversal is a crucial task in the process of optimizing knowledge sequences.Given that the number of nodes is fixed, an incorrect positioning of any node would necessitate nearly double the amount of ordered learning for the model to achieve a final result with zero information entropy.Studies by Dong et al. [41] and Khurana et al. [42] indicate that, in natural language systems, a common principle is that the subject is often prioritized and appears as the first word in a sentence.In related research on while the structured method requires structure restoration of the information, then forms a structured sequence through a graph traversal strategy, and finally feeds it into the model graph structures, Esfandiari and Fakhrahmad [43] typically refer to the subject as the topic or the most influential node, exploring its impact within the entire graph.Degree centrality theory, as discussed by Yang et al. [44], is a widely used and compared method in the study of influential nodes, noted for its efficiency and simplicity.The core of degree centrality theory posits that nodes with high in-degree and out-degree connections generally represent richer information and more complex relationships, and thus are often the most influential nodes.This paper leverages the principles of degree centrality theory to reconstruct the graph from the input sequence, then calculates the in-degree and out-degree of each node to determine the starting node for traversal.</p>
<p>Graph sequence determination</p>
<p>The graph sequence transforms the non-sequential graph structure information into serialized information through a graph traversal algorithm.Different traversal algorithms lead to distinct knowledge sequences, and they impact the convergence speeds of the corresponding models during the convergence process, ultimately affecting the quality of the final model-generated text.The paper compares three types of graph traversal methods [45,46]:</p>
<p>(1) Depth-First Search (DFS): DFS starts traversing from a point in the graph (starting point), first visits an adjacent node, and then performs a depth traversal with this adjacent node as the starting point until no unvisited adjacent nodes can be found.Then backtrack to the previous node and start the same depth traversal from other neighbor nodes until all nodes in the graph are visited.</p>
<p>(2) Breadth-First Search (BFS): BFS starts from a node and first visits all directly adjacent nodes.While visiting direct adjacent nodes, it records these adjacent nodes' neighboring nodes and then visits these recorded nodes in order.Repeat this process until all nodes have been visited.</p>
<p>(3) Topological Sorting: Topological sorting first chooses a node without predecessors, then outputs it.This node and all outgoing edges are then removed from the graph.Repeat this process until all nodes have been output or it is impossible to continue.</p>
<p>Knowledge graphs are directed graphs, and there may be cycles and multiple connected components.For DFS and BFS, both can use visit markings to solve the problems of cycles and multiple connected components.Topological sorting starts by default from a node without a predecessor, so when using topological sorting, the first step to choose the starting node is skipped by default.Topological sorting is applied to directed acyclic graphs, but actual knowledge graphs may have cycles and multiple connected components, so to solve this problem, the Kosaraju algorithm is introduced.The specific implementation of Kosaraju is available in Appendix A. Through three different traversal methods, three graph sequences are determined, and experiments prove that the three sequences have certain impacts on the final graph-to-text model.Specific experimental results can be seen in Sect.5.2 analysis.</p>
<p>EMLR framework</p>
<p>Overview of the EMLR framework</p>
<p>Traditional language models typically use a fixed-length sliding window, and they have limitations in capturing long-term dependencies, which means that if the concatenation size of the graph exceeds the model's window size, the model cannot remember the structure of the entire graph.The attention mechanism is a powerful technology that can make the model consider global information when understanding sequences.This mechanism allows the model to distribute its attention over all parts of the input information, not just the most recent parts, making it suitable for processing graph structure information.</p>
<p>In the task of graph-to-text generation, researchers would love to stuff all the graph information into the model and have it remember everything directly.This is indeed a good choice, but not all information is very important.The Information Bottleneck Theory points out that the goal of effective communication is to convey the most important information while reducing the amount of transmitted data as much as possible.Similarly, in the task of graph-to-text generation, not all structure information is of equal importance.In graph structures, edges give the graph various properties and features distinct from other structures, allowing the graph to accommodate more information.Therefore, inspired by the Information Bottleneck Theory, we designed the Edge-Optimized Multi-Level Information refinement(ELMR) Framework, which includes three attention mechanisms, namely the global multi-head attention mechanism at the text level, mixed perceptual learning, and edge perceptual learning, as shown in Fig. 3.The global multi-head attention mechanism is a preprocessing operation performed on the optimized sequence, learning the characteristics of information at the text level.Mixed perceptual learning helps the model learn the overall relationship between nodes and between nodes and edges.This step provides global structural features of the graph, allowing the model to understand the macro structure of the graph.Edge perceptual attention refines the edge information from the initial global attention results, helping the model to focus more on the most decisive features.</p>
<p>The calculation of hybrid node and edge information perception will introduce a large amount of redundant information, while there will also be some information lost due to the absence of separate attention calculation on the edges.The information fusion of the two can not only eliminate redundant information but also achieve information complementation to a certain extent, generating more comprehensive information representation, which helps to enhance the expressiveness of data.</p>
<p>Through these two stages of operations, the model can learn on a richer information level and distinguish and uti-lize features at different abstraction levels.This is equivalent to enabling the model to perform feature abstraction more effectively and understand complex graph structures more efficiently.</p>
<p>Design of the EMLR framework</p>
<p>Currently, many model training and deep learning algorithms [47] are inspired by the Information Bottleneck Theory, which aims to maximize task-relevant information while minimizing unnecessary information, as indicated by formula 1.In the ELMR framework, during mixed perceptual learning and edge perceptual learning, we utilize different beta parameters in both stages to balance I (T ; Y ) and I (T ; X ), ultimately fusing the results.</p>
<p>Mixed Perception Learning:</p>
<p>In mixed perception learning, the attention mechanism is used to learn the integrated structural information of nodes and edges.At this stage, we define a probability coding mapping p(t|x) that maps the input data X to an intermediate representation T .T is a compact representation that attempts to capture information about the text generation task Y .This can be described by the following optimization objective:
max p(t|x) I (T ; Y ) − β 1 I (T ; X )(2)
where I (T ; Y )) represents the mutual information between T and task Y , I (T ; X ) indicates the amount of information about X that T carries,and β 1 is the weighting parameter of the first stage.</p>
<p>As shown in Fig. 3, after being learned by the global multi-head attention mechanism, the sequence has a textlevel perception sequence X global , which is the input data X of the mixed perceptual learning stage.In order to aggregate such text-level information to the structural information, this paper sets up a graph structure sequence, as shown in X graph index and X edge index in Fig. 3.The non -1 numbers in X graph index are unique numbers for nodes and edges in the subgraph.The text-level information is aggregated with the overall structure information of the graph through a mixed-dispersed mean pooling, and its calculation formula is as follows:
x graph i j = 1 count i j count i j k=1 x global ik (3)
where count i j means taking each number in X graph index as a position index, and T ,where n is the maximum number.Each value in the first dimension of X graph obtained by mixed-dispersed mean pooling represents the characterization of each node or edge after fusing text information.This step further optimizes the representation of input X , making X graph contain corresponding  4 Multi-relationship Heterogeneous Graph M. In the graph, S i represents the subject entities, O represents the object entities, and verbs are the verbs connecting the subjects and objects structural information, minimizing the mutual information I (T ; X ) between X graph and the original input X under the final objective, while capturing the most informative part about the output Y .
x global ik ∈ R b×n×d is (X global )
In the field of knowledge graph research, many models have proven that using adjacency matrices to represent graph information can effectively capture complex graph structures during the model training process [48,49].Adjacency matrices not only represent the relationships between nodes and edges but also facilitate information propagation and aggregation, thereby capturing higher-order relationships and complex interactions between nodes [50].Through these mechanisms, adjacency matrices significantly enhance model performance and generalization.However, traditional adjacency matrices use nodes as indices and mark connections between nodes in a two-dimensional matrix.While this method can represent node relationships, it has limited expressiveness for edge diversity, usually only serving as connection identifiers.This form emphasizes node-to-node connections but does not fully utilize the rich information of edges.Therefore, some existing works have introduced edge relationships to enhance information expressiveness [51].This paper proposes a new graph representation method-Multi-relationship Heterogeneous Graph M.</p>
<p>As illustrated in Fig. 4, the Multi-relationship Heterogeneous Graph M represents node information from three perspectives: subject nodes, object nodes, and relationships, aiming to comprehensively represent the information in the graph.In M, (S 1 , . . ., S n ) represent all possible subject nodes, and (O 1 , . . ., O n ) represent all possible object nodes, with the direction always being from subject nodes to object nodes.Thus, the two-dimensional matrix of size (S + O)×(S + O), where S represents the number of subject nodes and O represents the number of object nodes, is analogous to the traditional adjacency matrix.The novelty of M lies in the addition of (V 1 , . . ., V n )) nodes, which represent all edge nodes, extending the traditional adjacency matrix.The addition of edge nodes in M provides a perspective from the edge nodes, where both the in-degree and out-degree associated with them are considered.This is distinguished by the subject-verbs and verbs-object relationships in the graph to differentiate in-degrees and out-degrees.This design not only enhances the expressiveness of information but also balances the roles of edges and nodes in the graph, thereby capturing complex higher-order relationships and improving model generalization during training.The symmetry of relationship nodes (i.e., the symmetry of A SV and A V O ) allows for dynamic allocation of the importance of different edges, thereby improving the expressiveness of the graph structure and accelerating model fitting.The integration of the attention matrix with the Multirelationship Heterogeneous Graph enhances the attention matrix of mixed perceptual learning, which includes both text-level and structure-level information of the entire knowledge graph.The calculation formula for the l-th layer of attention is as follows:
X mi xed l = Attn M,T (Q mi xed , K mi xed , V mi xed ) = so f tmax Q mi xed K T mi xed √ d k + M V mi xed (4)
where
Q mi xed ∈ R n×b×d ,K mi xed ∈ R n×b×d , V mi xed ∈ R n×b×d .Q mi xed
, K mi xed and V mi xed are projections of the structural sequence in three different spaces, and the calculation formula is as follows:
Q l = X l−1 W Q l (5) K l = X l−1 W K l (6) V l = X l−1 W V l (7)
The calculation result is the information bottleneck criterion representation learning result of this stage, that is, the maximized I (T ; Y ).The X mi xed obtained contains the global representation and structural representation information of the graph, and this representation is to maximize the associated information with task Y .This is the result of representation learning under the information bottleneck criterion of this stage, i.e., it maximizes I (T ; Y ).After the mixed perceptual learning, the model will retain some information most relevant to the text generation task Y , but this representation contains some unnecessary information.</p>
<p>Edge Perceptual Learning:</p>
<p>In edge perceptual learning, attention learning is carried on by the edges that are independent of the nodes.Also at this stage, we similarly need to define a probability coding mapping p(z|e), where e represents all the edges in the graph, and different from T , Z is a compact representation aimed at edges, also we need to optimize the following objective:
max p(z|e) I (Z ; Y ) − β 2 I (Z ; X ) (8)
where I (Z ; Y ) represents the mutual information of task Z and task Y , I (Z ; X ) is the amount of information about Z regarding X , and β 2 is a balancing parameter of the edge perceptual learning stage.Different from the X graph index sequence in the mixed-dispersed mean pooling, the X edge index only performs index processing for the edges.As can be seen in Fig. 3, the non-edge positions are marked with the number −1, and all the edges in the graph have their numbers.The edge-dispersed mean pooling also aggregates the text-level information and the edge-coding information, and its calculation formula is as follows:
x edge i j = 1 count i j count i j k=1
x global ik (9) where count i j represents taking each number in X edge index as a position index.The mixed representation, X edge , focusing solely on edge information obtained by edge average pooling is a transformation process from global text representation to specific edge representation, in order to extract useful edge information for the predictive task Y .Furthermore, X edge is taken as input in the edge perceptual learning, to capture correlations between edges.In the attention mechanism of the edge perceptual learning, the calculation formula of every l layer is as follows: (10) where
X edge l = Attn Q edge , K edge , V edge = so f tmax Q edge K T edge √ d k V edgeX edge l ∈ R n×b×d , Q edge ∈ R n×b×d , K edge ∈ R n×b×d ,V edge ∈ R n×b×d .
The final calculation result is the result of representation learning under the information bottleneck criterion of this stage, i.e., the maximized I (Z ; Y ).Q edge , K edge and V edge are projections of the structural sequences in three different spaces, calculated according to formulas 5, 6, and 7 respectively.Although the calculation formulas for Q edge , K edge , and V edge are the same as those for Q mi xed , K mi xed , and V mi xed , they project onto different spaces.In the iterative update process of X edge , we can obtain a representation that more reflects the information of the edge, and update and optimize p(z|e), helping the model to assign more importance to the information of the edge in the prediction task.</p>
<p>The target of the result of edge perceptual learning can be considered as further compression of the representation T in mixed perceptual learning, highlighting the role of edges.</p>
<p>Perception fusion</p>
<p>Through the two stages of perception learning, we obtain the solutions of two information bottleneck problems, namely the two feature representations T and Z .Readers can con-sider them as two different perspectives to capture relevant information about the task Y : one is the global representation of nodes and edges, and the other is a more specific representation targeted at edges.Finally, to obtain the final representation T containing useful information, we need to aggregate the results of the two.We propose two fusion schemes: (1) Direct fusion, and (2) Linear weighted fusion.</p>
<p>Direct Fusion:</p>
<p>The two results are directly fused.This method is straightforward and doesn't incur additional computational overhead.It equates the information weights from two stages directly and serves to verify the effectiveness of our method.The computational approach is as follows:
T = T + Z (11)
Linear Weighted Fusion: Direct fusion treats the weights of representation T and representation Z equally, not allowing us to further explore the weights of two different stages.Therefore, we introduce an adjustable parameter α to control the weight.The computational approach is as follows:
T = T + α * Z (12)
Our ultimate goal is to find p(t|x), p(z|e), and α, so that T maximizes I (T ; Y ) while minimizing I (T ; X ).This way, we can capture the most relevant information about task Y and discard irrelevant information.By applying different β parameters in the two stages, we can carefully balance the amount of information retained in different representations.</p>
<p>GA: graph activation function</p>
<p>Activation functions [52] can disrupt the linear superposition of neural networks, enabling the model to learn complex representations.Graphs are highly complex, diverse, and structurally free data structures.Thus, appropriate activation functions can ensure that the structured information of graphs is preserved when transforming linearized sequences into final nonlinear results.</p>
<p>Traditional activation functions include Sigmoid [53], ReLU [54], and GELU [55].In the configuration files of large language models, activation functions are usually preconfigured and remain unchanged during training.This leads to a problem: during training or hyper-parameter tuning, the activation functions cannot be adjusted to modify the inductive bias of the model.If we aim to find the most effective activation function to improve model performance, we might need to use an enumeration method.For k types of activation functions and n layers of encoders, we will have k n choices, making the search space very large and increasing the difficulty of experiments.</p>
<p>Moreover, these activation functions were initially designed for seq-to-seq models in text processing and were not directly designed for the complex structure of graphs.To address this issue, we introduce the concept of graph activation functions.Graph activation functions actually apply rational activation functions in the task of generating text from knowledge graphs.They can learn the most appropriate activation function for each learning layer in the current space.The calculation formula is as follows:
F(x) = P(x) Q(x) = m j=0 a j x j 1 + | n k=0 b k x k |(13)
where P(x) is an m-th degree polynomial with learnable coefficients a j , and Q(x) is an n-th degree polynomial with learnable coefficients b k .m and n together determine the complexity and fitting ability of the graph activation function F(x).The higher the degrees m and n, the higher the precision of F(x) after convergence.</p>
<p>In the TriELMR model, both the mixed perceptual learning module and the edge perceptual learning module apply graph activation functions.Taking the encoder layer as an example, after residual fusion and linear transformation, the mixed perceptual learning module and the edge perceptual learning module obtain a tensor x with dimensions (batch, SeqLen, Dffn), where batch represents the number of samples input to the model at one time, SeqLen represents the length of the input sequence, and Dffn represents the feature dimension after linear transformation.</p>
<p>When initializing the graph activation function, m and n need to be determined.Based on different baseline models (for example, this paper uses the BART model as the baseline model, with its default activation function being GELU), We use the least squares fitting method to make F(x) as close as possible to the default GELU activation function, thereby determining the values of a and b.This approach ensures that during the initial training stage, the graph activation function can exhibit characteristics similar to GELU, providing a reliable baseline performance.During training, the model can further optimize and adjust the learnable parameters of F(x), generating graph activation functions that fit the specific task requirements and data structures, thereby improving the model's expressive power and performance.</p>
<p>Experiments</p>
<p>Experimental setting</p>
<p>Dataset introduction</p>
<p>The main dataset used in this experiment is the WebNLGv2.0dataset.Three optimization strategies are validated on this</p>
<p>Evaluation criteria</p>
<p>Text Generation Performance Validation</p>
<p>We adopt two evaluation metrics as the main criteria for the model's performance, namely BLEU and METEOR.Since there are multiple versions of BLEU, i.e., BLEU-1, BLEU-2, BLEU-3, and BLEU-4, where the number corresponds to the n-gram division, higher-order BLEU can measure the fluency of sentences.Thus, we use BLEU-4 as a measure in our evaluation analysis.</p>
<p>BLEU is a metric commonly used to evaluate machine translation or natural language generation tasks, measuring the similarity between generated text and the reference text.It uses n-gram matches to assess the quality of the generated text.The calculation process is simple and intuitive, with relatively low complexity, making it suitable for largescale datasets.In the knowledge graph-to-text generation task, using BLEU for evaluation can rapidly gauge the fluency level of the generated text and quickly verify whether the information from the graph has been absorbed by the model and subsequently transformed into fluent text.</p>
<p>METEOR is a metric used to evaluate machine translation performance.This metric is based on the harmonic mean of precision and recall, with more weight given to recall.It was designed to rectify some of the deficiencies in BLEU and produces good correlation at the sentence or phrase level with human judgement.Hence, its primary philosophy is the correctness of the translation model.</p>
<p>Text Consistency Validation.After the basic model metric comparison, to further move towards practical applications of the model, we conducted a human-machine consistency validation.For AI usage, people tend to choose products that align more with human expression, making this an essential experiment.</p>
<p>ROUGE L : This is a set of metrics used to compare the similarity between auto-generated text and human-generated reference text.The main difference between ROUGE L and BLEU is that ROUGE L only considers recall; it does not concern about the fluency of the translation results, just whether the translation is accurate.</p>
<p>CIDEr: CIDEr evaluates the consistency between human descriptions and computer-generated descriptions.It is a method based on consistency evaluation.For an ideal candidate answer, if each evaluator's score is completely consistent with all other evaluators' scores, the highest score is 10.However, under normal circumstances, the actual score will be far below this theoretical maximum, with models typically scoring between 0 and 10.</p>
<p>Implementation details</p>
<p>We adopted Facebook's bart-base model (with 139 M parameters) and performed three optimization tasks on it, evolving it into the TriELMR model (with 196 M parameters).We obtain two datasets, WebNLGv2.0and EventNarrative, from Hugging Face, load them into the knowledge sequence reorganization module, and configure different traversal algorithms to get the optimized sequence.The Bart model consists of an encoder and a decoder.In the encoder part of the encoder layer, we replace the original multi-head attention mechanism with our designed ELMR framework.</p>
<p>In the encoder and decoder layers of the model, we replace the default GELU activation function with the graph activation function and determine the learnable parameters a and b through an initialization task.In the initialization task, we set m = 5 and n = 4, and sample the GELU activation function within the range [−3, 3] with 13 sampling points.The range [−3, 3] is chosen because most activation functions (including GELU) exhibit significant nonlinear characteristics within this range, covering the main variation region of the activation function.This ensures that the fitting process can capture the key features of the activation function.The choice of 13 sampling points is because 13 is a relatively small yet sufficient number, which can balance computational complexity and fitting accuracy.For the parameter update during the training process, we choose AdamW [56] as the optimizer for its strong model generalization ability, stable training, and faster convergence speed.The initial learning rate is set to 2e-5, with a batch size of 16.We also define a scheduler for learning rate adjustment optimization.The scheduler sets a linear warm-up phase of 1600 time steps, allowing the learning rate to heat up from a very small value to the initial learning rate, then enter a linear decay phase, ensuring the balance of the learning rate in the model, thus improving training stability and performance.The initial value of the linear weight fusion parameter α in the two learning units of the ELMR framework is set to 1, making it easy to compare with direct fusion.</p>
<p>We trained our model on a server equipped with 2 NVIDIA GeForce RTX 4090 GPUs.The server's additional configurations included an Intel(R) Xeon(R) Silver 4310 CPU @ 2.10GHz and 128GB of memory.The TriELMR model did not require pre-training tasks to adapt to the graph structure.The model achieved convergence on the WebNLGv2.0and EventNarrative datasets in approximately 2.3 h and 28 h, respectively.</p>
<p>Experiment results</p>
<p>Programmatic choices</p>
<p>Previously, we introduced various implementation schemes in the knowledge sequence reorganization and ELMR framework.In this section, we will analyze which scheme is the most suitable as a component of the TriELMR model.The experimental results of different schemes are as shown in Table 2.It displays the results under four evaluation indicators using different techniques in the same task.</p>
<p>Better Organization of Inputs</p>
<p>The optimization of the knowledge sequence aim is to modify the order of knowledge traversal.This approach ensures the structure of the graph at the input end of the model as much as possible.From the comparison of the knowledge sequence reorganization experiment group in Table 2, the effect of adding depth-first traversal (DFS) and Kosaraju algorithm on the basis of the baseline model is significantly better than the default order and breadth-first traversal (BFS).Among these, depth-first traversal is slightly better than the Kosaraju algorithm.In Table 2, DFS score is higher than the default order by 1.49%, 0.21%, 0.89%, 0.06 on BLEU-4, METEOR, ROUGE L (%), and CIDEr, respectively.It surpasses BFS by 1.19%, 0.48%, 0.51%, 1.06, and it is 0.88% higher than Kosaraju on the key indicator BLEU-4.Other indicators have minimal difference.The better score of DFS is as expected, following the law of grammar.As shown in 2b, in the restored subgraph, the in and out degree of "Elliot See" is 3, which is the maximum among all nodes.Therefore, it will serve as our starting node.From the fit of the results, biases exist in the determination of the starting node in the default traversal order.This situation is quite common.For breadth-first traversal, although it uses the same starting node, it will introduce his "alma mater", "death date", and "death place", and then introduce the information related to the "University of Texas at Austin".In line with human language habits, an introduction to one thing is typically completed before another is introduced.This perception is consistent with the results set, which first introduces "Elliot's alma mater", and then uses the attributive clause to introduce "Texas".The different edges departing from the same node represent different events, so the death event and death place are different from the "alma mater".They shouldn't be inserted in the middle, but separated by different sentences using periods.The experimental results show that the best TriELMR model chooses depth-first traversal as the optimal solution.</p>
<p>Better Fusion Method</p>
<p>To enable the model to better learn information useful for generating the final content, our team designed the ELMR framework by incorporating the Information Bottleneck Theory.The ELMR framework consists of the Mixed Perceptual Learning Module and the Edge Perceptual Learning Module, both of which comprehensively learn graph information from different perspectives.Additionally, the introduction of a multi-relationship heterogeneous graph in the ELMR framework aligns the model's learning perspective with our view that edges hold a more important position in graph structures.As shown in the ELMR experimental group in Table 2, the baseline model enhanced with the ELMR framework improved its BLEU-4 score from 64.55% to 65.46%, demonstrating an improved capability in graph-to-text generation.According to Table4, the knowledge learned by the model through the ELMR framework exhibits high potential and can inspire other works.Specifically, the BLEU-4 score reached 66.22%, 1.67 points higher than the baseline model, thereby significantly improving the model's performance.In the ELMR framework, we employed two different fusion schemes-linear weight fusion and direct fusion.During model training, when comparing linear weight fusion with direct fusion under specific conditions, the learnable parameter α in linear weight fusion successfully converged to 0.659.This convergence allowed the BART model using linear weight fusion to outperform the direct fusion method</p>
<p>Result on datasets</p>
<p>We compared the TriELMR model with recent advanced and classic models in the field of graph-based text generation across the WebNLG and Event Narrative datasets.Our evaluation metrics primarily include BLEU-4 and METEOR, which jointly measure the overall quality of the generated text.When the model performs well and is close to other models, we further use ROUGE L and CIDEr for consistency verification to ensure that the results in terms of text fluency and grammar are close to standard grammatical structures, aligning with everyday human expressions.</p>
<p>To demonstrate the reliability of the TriELMR model's results, we conducted significance tests with the latest GAP model and GCN model.The GAP model does not involve pre-training tasks, and we approach the graph structure information from different perspectives to design the graph-aware module.Therefore, the significance tests with the GAP model are elaborated upon in the paper.In the WebNLGv2.0and Event Narrative datasets, we conducted five experiments each for the TriELMR and GAP models, recording their performance on the test sets and visually presenting the results using box plots [57], as shown in Fig. 5.In the significance tests with the GAP model, we used pvalue comparisons.The significance level was set at 0.05; if the p-value is less than 0.05, the null hypothesis is rejected, indicating that the TriELMR model's results are significant.As shown in Fig. 5a and Table 3 for the WebNLGv2.0dataset, the maximum values for TriELMR and ELMR are 66.54% and 66.13%, respectively, the minimum values are 65.8% and 64.84%, respectively, and the standard deviations are 0.3 and 0.522, respectively.The calculated p-value is 4.38e−02, which is less than the significance level of 0.05, thus rejecting the null hypothesis.In the results for the EventNarrative dataset shown in Fig. 5b and Table 3, the maximum values for TriELMR and ELMR are 37.8% and 35.08%, respectively, the minimum values are 36.57%and 33.61%, respectively, and the standard deviations are 0.499 and 0.595, respectively.The calculated p-value is 3.91e−05, which is significantly less than the significance level of 0.05, thus rejecting the null hypothesis.Similarly, when comparing with the latest model Xia-GCN, proposed by Xia et al. [19], we set the significance level at 0.1 for WebNLGv2.0and 0.05 for Event Narrative, and the results still showed statistical significance.The significant results across both datasets indicate that the TriELMR model's generated results are statistically valid, and as the datasets become more complex, TriELMR demonstrates superior performance compared to other models, highlighting the model's advantages.</p>
<p>In subsequent experimental result comparisons, the experimental results of the GAP, Xia-GCN, and TriELMR models all use the mean values from the significance tests, while other methods reference the results from their respective papers.</p>
<p>Result on WebNLGv2.0</p>
<p>The TriELMR model introduces a novel approach to maintaining graph structure integrity in text generation from graphs.In Table 4, the TriELMR model is compared with other models performing the same task on the WebNLGv2.The Xia et al. [19]'s Xia-GCN architecture employs two large models to learn graph information.Initially, structural information is extracted through GCN, and the results are then fed into a pre-trained generative model.In the primary metric BLEU-4, TriELMR outperforms Xia-GCN by 0.06%.In consistency verification, TriELMR surpasses Xia-GCN by 0.13% in ROUGE L and by 0.5 points in CIDEr.Although experimental results indicate that the performance difference in content generation between TriELMR and Xia-GCN is minimal, the Xia team requires processing data through two large models, significantly increasing the complexity and workload of model training.In contrast, TriELMR achieves superior performance to Xia-GCN with only the KSO(Knowledge Sequence Optimization) task.</p>
<p>Clearly, TriELMR is superior to the current graph-totext generation models in terms of main indicators and also surpasses other models in terms of aggregate consistency verification scores.This proves that the TriELMR model not only generates excellent text but also gains a larger audience in user groups.</p>
<p>Result on EventNarrative</p>
<p>We further evaluated the performance of the TriELMR model on the larger and more complex EventNarrative dataset and compared it with models that performed well on the WebNLGv2.0dataset.The EventNarrative dataset is approximately five times the size of the WebNLGv2.0dataset and focuses on event descriptions, which imposes higher requirements on the model's generalization and robustness.As shown in</p>
<p>Ablation Study</p>
<p>In this subsection, we conducted a set of ablation studies to examine various aspects of the proposed model and analyze the impact of every optimization on the optimal model.All ablation experiments were conducted on the test set of the WebNLGv2.0dataset.</p>
<p>Effect of Knowledge Sequence Optimization.</p>
<p>As observed in the ablation experiments, Knowledge Sequence Optimization (KSO) noticeably impacts the model's performance.The ablation stats (Table 6) show that removing this component results in a significant decline in all performance indicators.Specifically, the absence of KSO triggers a substantial decay with a decrease of 1.3% in BLEU-4, 0.22% in METEOR, 0.63% in ROUGE L , and 0.05% in CIDEr.These results highlight KSO as a key factor to effectively improve the model's performance.Moreover, when KSO and other techniques (like ELMR, GA) are removed simultaneously, the drop in performance is more prominent.This indicates a synergic effect these techniques have towards performance improvement.However, even under these conditions, excluding KSO leads to a notable decline in performance indicators, especially those vital ones such as BLEU-4 and ROUGE L .This further emphasizes that KSO is an indispensable part for maximizing model performance.These results shed light on a crucial research trend, especially the importance of pre-processing the graph before inputting it into the model, with a particular focus on the reorganization of the knowledge sequence.By appropriate data preprocessing, we could potentially guide the model to better understand the hierarchy of information, and generate more accurate textual content.These insights point out a direction for our future work, offering new perspectives for enhancing the performance of knowledge graph-to-text generation models.</p>
<p>Effect of EMLR Framework.The EMLR framework helps the model learn the structural information of the graph better by focusing on the edges.This helps reduce information loss during the graph learning process.The ablation experiment results in Table 6 show that the standard model TriELMR exhibits excellent performance in all measurements.However, when we remove the EMLR part, the performance of the model decreases significantly.More specifically, after removing EMLR, BLEU-4, METEOR, ROUGE L , and CIDEr drop by 0.87%, 0.19%, 0.68%, and 0.02% respectively.This indicates that models with EMLR have a clear advantage in graph-to-text generation tasks over those without EMLR.Furthermore, when observing the results of the "-w/o KSO+ELMR" ablation combina-</p>
<p>Effect of Graph Activation Function.</p>
<p>To mimic the complex structure of graphs, we introduced a learnable rational activation function into the graph-to-text generation field.It allows results to map to a complex graph space with less information loss compared to the original graph structure.The ablation experiment of the graph activation function can be seen in Table 6.When our model lacks GA, all performance indicators show a slight decline.Specifically, the model's BLEU-4 drops by 0.88%, METEOR by 0.43%, and ROUGE L and CIDEr decrease by 0.46% and 0.02% respectively.Though these changes are subtle, they implicate the role of the GA graph activation function in the graph-to-text generation tasks.Based on the ablation experiment results of "-w/o ELMR + GA," it can be seen that the joint operation of ELMR and GA can still improve the model's performance.Although this impact is minor, with a decrease of 0.18% in BLEU-4, negligible changes in METEOR, a drop of 0.41% in ROUGE L , and a 0.02% decrease in CIDEr.This suggests that there might be significant interactions between ELMR and GA in the system.In sum, the Graph Activation function (GA) plays a certain role in graph-to-text generation tasks.Though the single removal of GA seemingly has little impact on the model performance, when combined with ELMR, the combination of GA and ELMR more significantly enhances the performance.These results show the importance of a learnable graph acti-vation function in providing the model with a more complex space to explore and in handling higher levels of complexity.</p>
<p>Limitations and future work</p>
<p>On one hand, to quickly validate the methods applied in the TriELMR model and produce relatively controllable outputs within a short time frame, and on the other hand, to facilitate the practical application and deployment of the results, we adopted a 160 M-sized BART model as the baseline.The results show that the TriELMR model performs satisfactorily on small datasets, demonstrating the significance of the methods.However, whether the optimization strategies in the TriELMR model can effectively impact the results generated by large models on large datasets has not yet been verified.Therefore, our team's next step is to apply our methods to fine-tuning large models to explore this possibility.If large models can also achieve significant effects through multiperspective and different weight inputs, this would represent a major breakthrough in the field of graph-to-text generation.</p>
<p>By addressing these limitations and conducting future research, we can continue to enhance the effectiveness and applicability of the TriELMR model in the field of graph-totext generation.</p>
<p>Conclusion</p>
<p>In the field of graph-to-text generation, our team proposed a model named TriELMR that maintains the original structure of the knowledge graph.This work explores how to preserve graph structure information to enhance text generation from three aspects.First, we reorganized the order of knowledge graph triples at the model's input to retain the graph structure information in the input data.Second, we designed the ELMR framework based on the information bottleneck theory, enabling the model to fully learn graph structural information from different perspectives.Lastly, we introduced graph activation functions, allowing the model's linear transformation process to explore activation functions that adapt to the diversity and freedom of graph structures.Through these three optimization strategies, TriELMR achieved improvements in BLEU-4, METEOR, ROUGE L , and CIDEr indices on the WebNLGv2.0and EventNarrive datasets, reaching 66.22%, 26.71%, 76.37%, and 4.6, as well as 37.27%, 29.15%, 67.48%, and 4.26, respectively.These results significantly surpass the current state-of-the-art models and have been validated for statistical significance.Evidently, the TriELMR model successfully explores the possibility of learning graph structures from multiple perspectives and different weights, and attempts to maximize useful content while minimizing redundant information through the guidance of information bottleneck theory.This provides a new research perspective for the field of graph-to-text generation.</p>
<p>Appendix A. The Kosaraju Algorithm</p>
<p>Appendix B. Text Generation Content</p>
<p>See Tables 7 and 8.</p>
<p>Fig. 1
1
Fig. 1 Three Design Plans for the Optimization of the TriELMR Model.The numbers in the diagram represent the TriELMR model's data processing steps, and the final result is achieved after three optimization tasks</p>
<p>Fig. 2
2
Fig. 2 Knowledge Sequence Optimization.Describes the triplet knowledge of Elliot See's life course information, which is formed into a knowledge sequence according to the traditional method (a) or through the method (b) of this work.The traditional method directly linearizes</p>
<p>Fig. 3
3
Fig. 3 Implementation Logic of the Edge-Optimized Multi-Level Information refinement Framework(ELMR).The ELMR framework consists of three modules: multi-head attention(bottom), mixed perceptual learning(left), and edge perceptual learning(right).Multi-head attention preprocesses global contextual text features.The mixed perceptual learning module learns the overall structural information of the</p>
<p>0 dataset.Compared to the baseline model BART, TriELMR achieves a 1.67% improvement in BLEU-4 and a 0.2% improvement in METEOR.The JointGT model employs pre-training tasks to better learn the structural information of graphs, thereby enhancing performance.Specifically, TriELMR surpasses JointGT (BART) by 1.62% in BLEU-4, and shows competitive performance in other metrics.In terms of consistency verification, TriELMR exceeds JointGT (BART) by 0.27% in ROUGE L and by 1 point in CIDEr.The G AP − M e,r + γ model eliminates the cumbersome pre-training tasks and addresses the issue by modifying the attention mechanism to incorporate global structure learning of graphs.Compared to the G AP − M e,r + γ model, TriELMR improves BLEU-4 by 0.64% and surpasses GAP by 1 point in CIDEr in consistency validation experiments.</p>
<p>Fig. 5
5
Fig. 5 Boxplot of significance testing results for WebNLGv2.0and EventNarrive datasets.GAP uses G AP − M e,r + γ and the indicators are replicated results according to the code from aco-las1/GAP_COLING2022 (github.com),Here, the asterisk indicates that TriELMR is significantly different from GAP, p is the probability of sig-</p>
<p>Table 1 The
1statistical information of the supervised knowledge graph to text datasetsDatasetWebNLGv2.0 Train/Val/TestEventNarrative Train/Val/Testused for experimentation, where Max., Min., and Avg. represent the maximum, minimum, and#KG-text pairs Max.34,352/4,316/4,224 939/804/927179,543/22,441/22,441 6,952/4,756/4,234average input sequence lengthsMin.63/63/6362/58/65on the training set, validation set, and test set, respectivelyAvg.257/258/257295/295/298dataset. After obtaining the best-performing model on theWebNLGv2.0 dataset, further experiments are conducted onthe EventNarrative dataset. Table 1 records the amount ofdata in the two datasets.WebNLG v2.0: WebNLG v2.0 [20] is a public dataset forNatural Language Generation (NLG) tasks, which is usedto transform structured data (like triplet data in SemanticWeb) into natural language descriptions. The data originatesfrom triples extracted from the Semantic Web. The dataset'sgoal is to convert these structured triples into natural lan-guage descriptions. It encompasses a variety of topics andattributes, covering information in different fields, and canhandle text generation tasks in different domains and top-ics. The reference texts of the data are manually annotated,ensuring very high-quality data.EventNarrative: EventNarrative [21] is a publicly avail-able open-world knowledge graph-to-text dataset, whichfocuses more on event-centric data. EventNarrative data isdiverse, with all content manually annotated, ensuring high-quality data.</p>
<p>Indicate the highest performance values of the TriELMR model on the WebNLGv2.0dataset using different schemes with α = 1 in terms of the BLEU-4 score, with minimal differences in other metrics.To enhance the model's generalization capability, the optimal TriELMR model in subsequent experiments chose linear weight fusion as the best solution.
Complex &amp; Intelligent Systems(2025) 11:74Page 13 of 1974Table 2 Performance of Different Approaches on theBLEU-4%METEOR(%)ROUGE L (%)CIDErWebNLGv2.0 DatasetBaseline [28]64.5546.5175.074.54Knowledge Sequence Optimization+DFS66.0446.7275.964.6+BFS64.8546.2475.453.54+Kosaraju65.1647.0775.684.61EMLR Framework+Sum65.3146.4175.734.59+Linear65.4646.3975.684.56123</p>
<p>Table 3
3
Results of Significance Testing for GAP and TriELMR Models on WebNLGv2.0and Event Narrative Datasets In this, SD stands for Standard Deviation.GAP uses G AP − M e,r + γ and the indicators are replicated results according to the code from acolas1/GAP_COLING2022 (github.com)
WebNLGv2.0EventNarrativeModelMaxMinMeanSDMaxMinMeanSDGAP [18]66.1364.8465.580.52235.0833.6134.450.595TriELMR66.5465.866.220.337.836.5737.270.499</p>
<p>Table 4
4
PerformanceIndicate the highest values among different models for the same metric in the WebNLGv2.0dataset, highlighted in bold.There are four metrics, hence four bold values In this, GAP uses the mean results from significance testing.JointGT is fine-tuned by BART and trained using JointGT pretrain.Xia-GCN refers to the GCN model proposed by Xia et al.
of Different models on theModelBLEU-4%METEOR(%)ROUGE L (%)CIDErWebNLGv2.0 DatasetKGPT [58]64.1146.3--BART [28]64.5546.5175.074.54JointGT [16]64.647.1576.13.6GAP [18]65.5846.7776.363.6Xia-GCN [19]66.1647.2076.244.1TriELMR66.2246.7176.374.6</p>
<p>Table 5
5
Indicate the highest values among different models for the same metric in the EventNarrative dataset, highlighted in bold.Similarly, there are four metrics, resulting in four bold values In this, GAP uses the mean results from significance testing.JointGT is fine-tuned by BART and trained using JointGTpretrain.Xia-GCN refers to the GCN model proposed byXia et al.
Performance of Different models on theModelsBLEU-4%METEOR(%)ROUGE L (%)CIDErEventNarrative DatasetBART [28]31.3826.6862.653.31JointGT [16]31.1926.5864.914.01GAP [18]34.4527.564.283.98Xia-GCN [19]35.8328.8768.224.03TriELMR37.2729.1567.484.26</p>
<p>Table 5
5, the TriELMR model outperformsnearly all existing graph-to-text generation models in termsof BLEU-4, METEOR, and ROUGE L metrics. Compared tothe BART baseline model, TriELMR achieves improvementsof 5.89%, 2.47%, 4.83%, and 0.95% on BLEU-4, METEOR,ROUGE L , and CIDEr metrics, respectively. Additionally,compared to models focused on generating text based ongraph structure information, such as GAP and JointGT,TriELMR shows improvements of 2.19%, 1.65%, 2.65%,and 0.28% over GAP in BLEU-4, METEOR, ROUGE L , andCIDEr, and improvements of 6.08%, 2.57%, 3.%, and 0.25%over JointGT in the same metrics. In comparison with thelatest research by Xia et al., TriELMR demonstrates betterperformance, with improvements of 1.44% in BLEU-4 and0.28% in METEOR. In consistency verification, althoughTriELMR is slightly lower than the Xia-GCN model by 0.74points in ROUGE L , it achieves a higher score by 0.23 pointsin CIDEr.In summary, the overall experimental results of theTriELMR model on the EventNarrative dataset almost com-prehensively outperform the current research. This not onlydemonstrates TriELMR's strong capability in handling largeand complex datasets and generating knowledge content butalso highlights its high adaptability, performing excellentlyacross different datasets.</p>
<p>Table 6 The
6
This table presents the results of the distillation experiments.The first row shows the metric values when all three schemes are present, while rows two to seven display the values when one scheme is removed.The bold values indicate the highest experimental results obtained from different distillation methods tion, we can see a greater performance drop compared to models with only EMLR ablated.When the model lacks both KSO and EMLR, BLEU-4 drops by 1.81%, METEOR slightly increases (+0.06%), but ROUGE L and CIDEr both decline significantly, at −1.09% and −0.07%respectively.This implies that the absence of EMLR has a greater impact on model performance without KSO.Overall, in graph-totext tasks, the EMLR framework plays an important role in optimizing model performance.Although the improvement it brings might be less noticeable than KSO, whether removed alone or in conjunction with other components, the presence of EMLR proves its positive impact on model performance.This emphasizes the importance of integrating edge information into the model to reduce information loss.
results of the ablation experiment on theModel variantBLEU-4(%)METEOR(%)ROUGE L (%)CIDEr(%)WebNLGv2.0 dataset, where KSO stands for Knowledge Sequence OptimizationTriELMR -w/o KSO66.22 64.92(−1.3)46.71 46.93(−0.22)76.37 75.74(−0.63)4.62 4.57(−0.05)-w/o ELMR65.35(−0.87)46.52(−0.19)75.69(−0.68)4.6(−0.02)-w/o GA65.34(−0.88)46.28(−0.43)75.91(−0.46)4.6(−0.02)-w/o ELMR+GA66.04(−0.18)46.72(+0.01)75.96(−0.41)4.6(−0.02)-w/o KSO+GA65.46(−0.76)46.39(−0.32)75.68(−0.69)4.56(−0.06)-w/o KSO+ELMR64.41(−1.81)46.77(+0.06)75.28(−1.09)4.55(−0.07)</p>
<p>Table 7
7
TriELMR Model Multi-Dimensional Analysis Experimental Results in WebNLGv2.0Pre.The Pakistan Civil Aviation Authority is the operating organisation of the Allama Iqbal International Airport in Pakistan.The airport serves the city of Lahore and has a runway length of 2900.0.The city of Lahore, Pakistan, is served by Allama Iqbal International airport.It is operated by the Pakistan Civil Aviation Authority.It has a runway length of 2900 Pre.Bakewell pudding is a dessert from the Derbyshire Dales region.The main ingredients are ground almonds, jam, butter and eggs Re f .Bakewell pudding 's main ingredients are almond, jam, butter and eggs.It is a dessert from the Derbyshire Dales region Pre.Ashgabat International Airport is 211 ms above sea level and its 1st runway has a length of 12,467 feet Re f .The 1st runway at Ashgabat International airport is 12467 feet in length and the Airport is elevated 211 ms above sea level Pre.Rock and roll originated from blues music Re f .Rock and roll music originated from blues music
Re f .</p>
<p>Table 8
8
TriELMR Model Multi-Dimensional Analysis Experimental Results in EventNarrative Pre.The 1988 Benson and Hedges Open was a men's 1988 Grand Prix circuit tennis tournament held in Auckland, New Zealand and played on outdoor hard courts Re f .The 1988 Benson and Hedges Open was a men's 1988 Grand Prix circuit tennis tournament held in Auckland, New Zealand Pre.The 2016 Tianjin Health Industry Park was a professional tennis tournament played on outdoor hard courts.It took place in Tianjin, China, on 2016-2016 Re f .The 2016 Tianjin Health Industry Park was a professional tennis tournament played on outdoor hard courts.It took place in Tianjin, China, on 2016-2016
Pre.A Cook Islands parliamentary term referendum, 1999was held in the Cook Islands on 1999Re f .A Cook Islands parliamentary term referendum, 1999was held in the Cook Islands on 1999Pre.The 2016 Torneo de Promoción y Reserva is a footballtournament in PeruRe f .The 2016 Torneo de Promoción y Reserva is a footballtournament in Peru
Complex &amp; Intelligent Systems (2025) 11:74
Complex &amp; Intelligent Systems (2025) 11:74 Page 7 of 19
Page 11 of 19<br />
Acknowledgements This paper was funded by the National Natural Science Foundation of China (No.62172393), Henan Province Key Research and Development Project (No.241111211900), Zhongyuanyingcai program-funded to central plains science and technology innovation leading Talent program (No.204200510002), and Ministry of Education Industry Education Collaborative Education Project funded by Tencent (No.230700006203144).Data Availability All datasets used in this study have been archived.The DOI for the WebNLGv2.0dataset is 10.18653/v1/W18-6543, and the article link is: https://aclanthology.org/W18-6543/.The original script can be accessed at: https://gitlab.com/shimorina/webnlg-dataset,using the release_v2_constrained dataset.The DOI for the EventNarrative dataset is https://doi.org/10.48550/arXiv.2111.00276,and the original script can be accessed at: https://www.kaggle.com/datasets/acolas1/eventnarration.DeclarationsConflict of interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper..The authors declare the following financial interests/personal relationships which may be considered as potential competing interests.Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material.You do not have permission under this licence to share adapted material derived from this article or parts of it.The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.
A comprehensive survey on automatic knowledge graph construction. L Zhong, J Wu, Q Li, H Peng, X Wu, ACM Comput Surv. 562023</p>
<p>Knowledge-enhanced graph convolutional network for recommendation. X Tang, J Yang, D Xiong, Y Luo, H Wang, D Peng, Multimedia Tools Appl. 812022</p>
<p>Optimization of recommendation algorithm based on knowledge graph. Q He, S Liu, 2022 4th International Conference on Applied Machine Learning (ICAML). IEEE2022</p>
<p>A recommender algorithm based on knowledge graph convolutional network and knowledge reasoning optimization. T Liu, S Cheng, 2023 26th International Conference on Computer Supported Cooperative Work in Design. IEEE2023</p>
<p>Kgcna: knowledge graph collaborative neighbor awareness network for recommendation. G He, Z Zhang, H Wu, S Luo, Y Liu, IEEE Trans Emerg Top Comput Intell. 82024</p>
<p>Musera: multiple sample enriched region assessment. V Jalili, M Matteucci, M J Morelli, M Masseroli, Brief Bioinform. 182017</p>
<p>Nlp4kgc: natural language processing for knowledge graph construction. E Vakaj, S Tiwari, N Mihindukulasooriya, F Ortiz-Rodríguez, R Mcgranaghan, Companion Proceedings of the ACM Web Conference 2023. 2023</p>
<p>Patient-centered radiology reports with generative artificial intelligence: adding value to radiology reporting. J Park, K Oh, K Han, Y H Lee, Sci Rep. 14132182024</p>
<p>The role of knowledge graphs in chatbots. E Rajabi, A N George, K Kumar, 2024The Electronic Library</p>
<p>Unifying large language models and knowledge graphs: a roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, IEEE Trans Knowl Data Eng. 2024</p>
<p>Cfgl-lcr: a counterfactual graph learning framework for legal case retrieval. K Zhang, C Chen, Y Wang, Q Tian, L Bai, Proceedings of the 29th ACM SIGKDD Conference on knowledge discovery and data mining. the 29th ACM SIGKDD Conference on knowledge discovery and data mining2023</p>
<p>Text generation-using discourse strategies and focus constraints to generate natural language text by kathleen r. mckeown. P Holmes-Higgin, The Knowledge Engineering Review. 91994. 1992cambridge university press</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 302017</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Ope-nAI blog. 192019</p>
<p>Text generation from knowledge graphs with graph transformers. R Koncel-Kedziorski, D Bekal, Y Luan, M Lapata, H Hajishirzi, arXiv:1904.023422019arXiv preprint</p>
<p>Jointgt: graph-text joint representation learning for text generation from knowledge graphs. P Ke, H Ji, Y Ran, X Cui, L Wang, L Song, X Zhu, M Huang, arXiv:2106.105022021arXiv preprint</p>
<p>Unifying structured data as graph for datato-text pre-training. S Li, L Li, R Geng, M Yang, B Li, G Yuan, W He, S Yuan, C Ma, F Huang, Transactions of the Association for. Comput Linguist. 122024</p>
<p>Gap: a graph-aware language model framework for knowledge graph-to-text generation. A Colas, M Alvandipour, D Z Wang, arXiv:2204.066742022arXiv preprint</p>
<p>Efficient incorporation of knowledge graph information for enhanced graph-to-text generation. Z Xia, S Tao, Y Qin, Y Zhou, J Liu, X Shi, 2024 IEEE 6th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC). 20246</p>
<p>Handling rare items in data-to-text generation. A Shimorina, C Gardent, 2018Association for Computational Linguistics</p>
<p>Eventnarrative: a large-scale event-centric dataset for knowledge graph-to-text generation. A Colas, A Sadeghian, Y Wang, D Z Wang, arXiv:2111.002762021arXiv preprint</p>
<p>Meta-cqg: a meta-learning framework for complex question generation over knowledge bases. K Zhang, Y Qiu, Y Wang, L Bai, W Li, X Jiang, H Shen, X Cheng, Proceedings of the 29th International Conference on computational linguistics. the 29th International Conference on computational linguistics2022</p>
<p>R Liu, P Xing, Z Deng, A Li, C Guan, H Yu, arXiv:2202.07256Federated graph neural networks: Overview, techniques and challenges. 2022arXiv preprint</p>
<p>Structure-aware knowledge graph-totext generation with planning selection and similarity distinction. F Zhao, H Zou, C Yan, Proceedings of the 2023 Conference on empirical methods in natural language processing. the 2023 Conference on empirical methods in natural language processing2023</p>
<p>Deep graph convolutional encoders for structured data to text generation. D Marcheggiani, L Perez-Beltrachini, arXiv:1810.099952018arXiv preprint</p>
<p>Tree-of-reasoning question decomposition for complex question answering with large language models. K Zhang, J Zeng, F Meng, Y Wang, S Sun, L Bai, H Shen, J Zhou, Proceedings of the AAAI Conference on artificial intelligence. the AAAI Conference on artificial intelligence202438</p>
<p>Bert: pretraining of deep bidirectional transformers for language understanding. J Devlin, M-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, Mohamed A Levy, O Stoyanov, V Zettlemoyer, L , arXiv:1910.134612019BartarXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The. J Mach Learn Res. 212020</p>
<p>Self-supervised graph masking pre-training for graph-to-text generation. J Han, E Shareghi, arXiv:2210.105992022arXiv preprint</p>
<p>Syntax controlled knowledge graph-to-text generation with order and semantic consistency. J Liu, C Fan, F Zhou, H Xu, arXiv:2207.007192022arXiv preprint</p>
<p>Enhancing text generation from knowledge graphs with cross-structure attention distillation. X Shi, Z Xia, P Cheng, Y Li, Eng Appl Artif Intell. 1361089712024</p>
<p>Graph-to-sequence learning using gated graph neural networks. D Beck, G Haffari, T Cohn, arXiv:1806.098352018arXiv preprint</p>
<p>. P Veličković, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, arXiv:1710.109032017Graph attention networks. arXiv preprint</p>
<p>Enhancing heterogeneous knowledge graph completion with a novel gat-based approach. W Wei, Y Song, B Yao, ACM Trans Knowl Discov Data. 182024</p>
<p>N Tishby, F C Pereira, W Bialek, arXiv preprint physics/0004057The information bottleneck method. 2000</p>
<p>Dynamic graph information bottleneck. H Yuan, Q Sun, X Fu, C Ji, J Li, Proceedings of the ACM on Web Conference. 20242024</p>
<p>Refsql: A retrieval-augmentation framework for text-to-sql generation. K Zhang, X Lin, Y Wang, X Zhang, F Sun, C Jianhe, H Tan, X Jiang, H Shen, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>A novel feature integration method for named entity recognition model in product titles. S Sun, J Li, K Zhang, X Sun, J Cen, Y Wang, Comput Intell. 40e126542024</p>
<p>Improving text-to-text pre-trained models for the graph-to-text task. Z Yang, A Einolghozati, H Inan, K Diedrick, A Fan, P Donmez, S Gupta, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), ACL. the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), ACL2020</p>
<p>A survey of natural language generation. C Dong, Y Li, H Gong, M Chen, J Li, Y Shen, M Yang, ACM Comput Surv. 552022</p>
<p>Natural language processing: state of the art, current trends and challenges. D Khurana, A Koli, K Khatter, S Singh, Multimedia Tool Appl. 822023</p>
<p>Predicting node influence in complex networks by the k-shell entropy and degree centrality. S Esfandiari, M Fakhrahmad, Companion Proceedings of the ACM on Web Conference 2024. 2024</p>
<p>Predicting node degree centrality with the node prominence profile. Y Yang, Y Dong, N V Chawla, Sci Rep. 472362014</p>
<p>Degree based search: a novel graph traversal algorithm using degree based priority queues. P V Shyma, Sanil Shanker, K P , Int J Adv Comput Sci Appl. 152024</p>
<p>Abi-bfs: a high-performance parallel breadthfirst search on shared-memory systems. J Shi, Y Xu, 2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS). IEEE2023</p>
<p>How does information bottleneck help deep learning?. K Kawaguchi, Z Deng, X Ji, J Huang, International Conference on Machine Learning, PMLR. 2023</p>
<p>St-dagcn: a spatiotemporal dual adaptive graph convolutional network model for traffic prediction. Y Liu, T Feng, S Rasouli, M Wong, Neurocomputing. 6011281752024</p>
<p>An adaptive adjacency matrix-based graph convolutional recurrent network for air quality prediction. Q Chen, R Ding, X Mo, H Li, L Xie, J Yang, Sci Rep. 1444082024</p>
<p>Prgnn: modeling high-order proximity with relational graph neural network for knowledge graph completion. D Zhu, Neurocomputing. 5941278572024</p>
<p>Leveraging relational graph neural network for transductive model ensemble. Z Hu, J Zhang, H Wang, S Liu, S Liang, Proceedings of the 29th ACM SIGKDD Conference on knowledge discovery and data mining. the 29th ACM SIGKDD Conference on knowledge discovery and data mining2023</p>
<p>H Fang, J-U Lee, N S Moosavi, I Gurevych, arXiv:2208.14111Transformers with learnable activation functions. 2022arXiv preprint</p>
<p>Sigmoid loss for language image pre-training. X Zhai, B Mustafa, A Kolesnikov, L Beyer, Proceedings of the IEEE/CVF International Conference on computer vision. the IEEE/CVF International Conference on computer vision2023</p>
<p>K Shen, J Guo, X Tan, S Tang, R Wang, J Bian, arXiv:2302.06461A study on relu and softmax in transformer. 2023arXiv preprint</p>
<p>Reusing softmax hardware unit for gelu computation in transformers. C Peltekis, K Alexandridis, G Dimitrakopoulos, 2024 IEEE 6th International Conference on AI Circuits and Systems (AICAS). IEEE2024</p>
<p>Towards understanding convergence and generalization of adamw. P Zhou, X Xie, Z Lin, S Yan, IEEE Trans Pattern Anal Mach Intell. 462024</p>
<p>Asset pricing with neural networks: significance tests. H Fallahgoul, V Franstianto, X Lin, J Economet. 2381055742024</p>
<p>Kgpt: knowledgegrounded pre-training for data-to-text generation. W Chen, Y Su, X Yan, W Y Wang, arXiv:2010.023072020arXiv preprint</p>
<p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. </p>            </div>
        </div>

    </div>
</body>
</html>
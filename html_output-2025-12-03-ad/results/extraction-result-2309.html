<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2309 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2309</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2309</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-259924636</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.07454v1.pdf" target="_blank">Generative adversarial networks for data-scarce spectral applications</a></p>
                <p><strong>Paper Abstract:</strong> Generative adversarial networks (GANs) are one of the most robust and versatile techniques in the field of generative artificial intelligence. In this work, we report on an application of GANs in the domain of synthetic spectral data generation, offering a solution to the scarcity of data found in various scientific contexts. We demonstrate the proposed approach by applying it to an illustrative problem within the realm of near-field radiative heat transfer involving a multilayered hyperbolic metamaterial. We find that a successful generation of spectral data requires two modifications to conventional GANs: (i) the introduction of Wasserstein GANs (WGANs) to avoid mode collapse, and, (ii) the conditioning of WGANs to obtain accurate labels for the generated data. We show that a simple feed-forward neural network (FFNN), when augmented with data generated by a CWGAN, enhances significantly its performance under conditions of limited data availability, demonstrating the intrinsic value of CWGAN data augmentation beyond simply providing larger datasets. In addition, we show that CWGANs can act as a surrogate model with improved performance in the low-data regime with respect to simple FFNNs. Overall, this work highlights the potential of generative machine learning algorithms in scientific applications beyond image generation and optimization.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2309.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2309.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CWGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Wasserstein Generative Adversarial Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional Wasserstein GAN augmented with a regression (MAE) conditioning term to generate labeled synthetic spectral data and to act as a surrogate model for spectra conditioned on geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Near-field radiative heat transfer (spectral heat transfer coefficient prediction) in multilayer hyperbolic metamaterials</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate synthetic spectral heat transfer coefficient (h_ω) data conditioned on 8 geometric layer thicknesses, augment training data for regressors, and serve as a surrogate forward model mapping geometry → spectrum.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Original (simulated) dataset of 6,561 labeled spectra (each spectrum sampled at 200 frequency points). Labeled (geometry parameters paired with spectra). The authors treat both moderate-data (80% train) and extreme low-data regimes (training reduced down to 1% of examples) by re-splitting the dataset; they also generated 10,000 synthetic spectra for augmentation. Data quality is high (physics-based simulation) and accessible within the study.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional numerical 1D spectra (200-point frequency vectors) paired with structured numerical condition vectors (8-dimensional layer-thickness vector); effectively structured dense arrays (multivariate continuous signals).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High nonlinearity and high-dimensional outputs (resonant spectral features sensitive to 8 continuous geometric parameters); spectrum features include narrow/broad resonances and multi-modal structures; underlying forward physics requires scattering-matrix calculations (computational cost non-trivial).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature physics domain with established theoretical models (fluctuational electrodynamics, scattering-matrix methods) but emergent use of generative ML for data augmentation and surrogate modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — scientific interpretability and correct conditioning are important so generated spectra reliably correspond to physical geometry; surrogate must preserve resonant features for scientific validation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Conditional Wasserstein Generative Adversarial Network (CWGAN) with MAE conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Generator: 4 fully connected layers (50, 100, 150, 200 neurons), selu activations, 20% dropout after each layer (used to provide variability instead of feeding explicit random z). Critic: two parallel FC branches for spectrum and condition (150 and 50 neurons), concatenation, then FC layers (100, 50), selu activations and a linear output score. Training: Wasserstein loss with gradient penalty (λ = 10) to enforce 1-Lipschitz; critic updated n_train = 5 times per generator update. Generator loss adds a Mean Absolute Error (MAE) term between generated and ground-truth spectrum conditioned on the same geometry, producing a hybrid adversarial+regression objective. Preprocessing: log of spectra, mean subtraction and standard deviation scaling of inputs/conditions. After training, generator was used to produce 10,000 synthetic labeled spectra for augmentation and also decoupled as a surrogate mapping geometry→spectrum.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Generative modeling (conditional GAN) with supervised conditioning / hybrid adversarial + regression</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate after two key modifications: (1) using Wasserstein loss with gradient penalty to avoid mode collapse and stabilize training; (2) adding a conditioning/regression (MAE) term to ensure generated spectra correspond to provided geometric labels. Well-suited for structured numerical spectral data and for data-augmentation or surrogate use in low-data regimes; less competitive than simple regression networks when abundant labeled data are available.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Generator MAE converged to values < 0.2 (training-monitoring MAE). Baseline FFNN (ample data) achieved validation L_point = 3.61% and L_integ = 1.45%. In extreme low-data (validation fraction = 99%), simple FFNN: L_integ = 13.2%, L_point = 18.2%; augmented FFNN (trained with 10k CWGAN spectra added): L_integ = 6.8%, L_point = 16.8%. CWGAN used as surrogate produced performance comparable to the augmented FFNN in the low-data regime (approaching L_integ ≈ 6.8% in the extreme case), but worse than FFNN when training data are abundant (validation fraction < ~70%). PCA projection of CWGAN outputs reproduced training-set structure (PCA retained >90% variance).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>CWGAN avoided mode collapse that afflicted a plain CGAN and produced realistic, label-consistent spectra; data augmentation with CWGAN substantially improved downstream FFNN performance in low-data regimes (half the integral error in the extreme case). As a standalone surrogate, CWGAN was more resilient than a simple FFNN when training data are scarce, but inferior to FFNN when data are plentiful. Training stability required WGAN techniques (gradient penalty and critic-heavy updates) and conditioning via MAE.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for scientific settings with costly/limited spectral data: reduces the need for large labeled simulation/experimental datasets, enables inexpensive generation of labeled spectra for training/prediction, and provides a conditional surrogate fast-to-evaluate model. Likely generalizable to other spectral scientific problems (optics, chemistry, astronomy, spectroscopy) where labeled spectra are expensive to obtain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Direct comparison in paper: CGAN (plain conditional GAN) exhibited mode collapse and failed to reproduce training diversity (via PCA). Compared to a simple FFNN regressor: with abundant data FFNN outperformed CWGAN surrogate; with scarce data CWGAN-augmented FFNN and CWGAN surrogate outperformed simple FFNN. Numerical comparisons: see Effectiveness Quantitative field above.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of Wasserstein loss and gradient penalty to stabilize adversarial training; conditioning via MAE term to tie generated spectra to geometry labels; sufficient critic updates per generator update (n_train = 5); dropout-based variability in generator obviating explicit latent z; careful input preprocessing (log transform, normalization); architecture choices tuned to spectral dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Conditioned Wasserstein GANs that combine adversarial training with a regression conditioning term produce labeled synthetic spectra that substantially improve downstream regression performance and surrogate accuracy in low-data regimes, while avoiding mode collapse of plain conditional GANs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2309.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2309.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wasserstein Generative Adversarial Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAN variant that uses the Wasserstein (Earth Mover's) distance as loss, employs a critic producing scalar scores (not probabilities), and (here) enforces 1-Lipschitz via a gradient penalty to stabilize training and avoid mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Generative modeling of scientific spectral data (same domain as CWGAN entry)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Address training instabilities and mode collapse present in standard GANs when generating complex spectral datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Used on the same simulated dataset of 6,561 labeled spectra (200 frequency points each).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional spectral vectors with associated geometry labels.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Training a generative model for multimodal, highly structured spectral outputs prone to mode collapse under standard GAN losses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>WGAN is an established technique in generative modeling literature; applied here to a mature physics simulation domain.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — stable training and meaningful loss metric help interpret distance between generated and real distributions but do not by themselves provide causal/physical interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Wasserstein GAN (WGAN) with gradient penalty</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Critic outputs a score (not a probability); loss is expectation over critic scores for real and generated samples plus gradient-penalty term: L_C = −E[C(x)] + E[C(G(z))] + λ E[(||∇C( x_hat )||_2 − 1)^2], with λ = 10. Critic trained n_train = 5 times per generator update. Enforces 1-Lipschitz via gradient penalty rather than weight clipping.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Generative modeling / adversarial learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Necessary modification versus vanilla GAN for stable spectral generation in this problem; proved essential to avoid mode collapse and produce diverse spectra.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Use of WGAN (with gradient penalty) in the conditional setting enabled the CWGAN to reproduce the training-set complexity (PCA shows broad coverage whereas plain CGAN collapsed). No standalone numeric metric for WGAN alone beyond its contribution to CWGAN results.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>WGAN formulation produced meaningful and stable critic loss behavior (allowed training to converge to stationary loss after ~10^4 steps) and prevented mode collapse observed with standard GAN loss.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Provides stable adversarial training for scientific generative tasks where mode diversity and stability are required; critical enabler for conditional spectral generators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to standard GAN/CGAN: standard conditional GAN showed mode collapse and poor coverage across PCA projection; WGAN-based CWGAN achieved good coverage and label-consistent generation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Gradient penalty enforcement of Lipschitz constraint (λ = 10), multiple critic updates per generator step (n_train = 5), and monitoring of loss components to reach stable training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Using Wasserstein loss with gradient penalty is essential to stabilize adversarial training and avoid mode collapse when generating complex scientific spectra.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2309.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2309.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Generative Adversarial Network (standard loss)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional GAN trained with the original GAN loss (Jensen-Shannon-based), used here as a baseline and shown to suffer from mode collapse on complex numerical spectral data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Generative modeling of spectral data for near-field radiative heat transfer</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Attempt to generate labeled synthetic spectra conditioned on geometry using the conventional conditional GAN loss.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Training on the same simulated dataset (6,561 labeled spectra).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional 200-point spectral vectors paired with 8-dim condition vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Multimodal spectral distributions with narrow resonances; standard GAN training struggles to cover all modes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>CGAN is a well-known method but here shown inadequate without WGAN modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — conditioning needed to preserve label-sample correspondence, which CGAN failed to maintain here due to mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Conditional GAN (original GAN loss)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Generator and discriminator trained with original GAN loss: discriminator minimizes negative log-likelihood on real vs generated samples; the conditional generator loss combines conditional reconstruction and adversarial loss terms. In practice, this model was trained with the same architecture design as CWGAN for comparison but with the original GAN losses.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Generative modeling / adversarial learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Not appropriate for this problem without modification — exhibited strong mode collapse and failed to reproduce the diversity of the training spectra.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>PCA visualization showed the CGAN reproduced only principal cluster structures and failed to cover most training examples (contrast: CWGAN reproduced most complexities); no numeric error metrics given because model failed qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Suffered from mode collapse; unable to reproduce the full variety of spectral features present in the training data and thus unsuitable for labeled spectral augmentation or surrogate tasks in this application.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Low for this specific spectral generation problem unless modified; demonstrates that naive conditional GANs may be unreliable for complex scientific numerical data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Directly compared to CWGAN: CGAN showed mode collapse while CWGAN (Wasserstein + conditioning) reproduced training set structure well (PCA).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Failure attributed to using original GAN loss; lack of Wasserstein loss/gradient penalty and lack of stronger conditioning/regression term led to mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Plain conditional GAN (original loss) is prone to mode collapse on complex, multimodal scientific spectral data and thus requires modifications (Wasserstein loss and conditioning) to be useful.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2309.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2309.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FFNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feed-Forward Neural Network (baseline regressor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard feed-forward neural network used both as a baseline supervised regressor for mapping geometry → spectrum and as the model retrained after CWGAN data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Supervised regression predicting spectral heat transfer coefficient from geometry in near-field radiative heat transfer</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict the 200-point h_ω spectrum given 8 geometric layer thicknesses using a dense neural network.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Trained on the original simulated dataset (6,561 spectra); also retrained on augmented datasets where 10,000 CWGAN-generated spectra were added. Used varying train/validation splits from 80/20 down to 1/99 to probe low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular-to-vector regression: 8-dimensional input vectors mapped to 200-dimensional continuous outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional output regression with nonlinear mapping sensitive to resonances; network used had five hidden layers of 200 neurons each with selu activation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Standard supervised learning approach; previously successful in this physics system when ample data are available (cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — model provides predictive accuracy but is a black-box regressor; physical validation still required for scientific trust.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Feed-forward dense neural network regressor (5 hidden layers × 200 neurons)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Architecture: 5 hidden fully connected layers, each with 200 neurons and selu activations, final linear output layer for 200-point spectrum. Training: Adam optimizer, learning rate 3×10^-4, trained for 50,000 epochs in baseline experiments. Used as baseline and retrained with CWGAN-augmented data.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (regression)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable when sufficient labeled data are available; served as a strong baseline achieving low errors with ample training examples. Performance degrades substantially in extreme low-data regimes but benefits from CWGAN augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>With abundant training data (80% train split): validation L_point = 3.61% and L_integ = 1.45%. In extreme low-data (validation fraction = 99%): simple FFNN L_integ = 13.2% and L_point = 18.2%; after CWGAN augmentation these drop to L_integ = 6.8% and L_point = 16.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>FFNN is effective and accurate with enough physics-simulated training data; however, in low-data regimes its performance falls off sharply, and synthetic augmentation via CWGAN materially improves integral-error-sensitive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Immediate practical utility as a fast forward predictor when trained on sufficient data; combining with CWGAN augmentation extends applicability into low-data settings and reduces reliance on expensive simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to CWGAN surrogate: FFNN outperforms CWGAN when abundant data are available; with scarce training data, CWGAN-augmented FFNN and CWGAN surrogate outperform simple FFNN. Compared to CGAN: FFNN unaffected by mode-collapse issues (CGAN is a generator).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Sizable architecture and long training (50k epochs) with Adam optimizer and good preprocessing allowed accurate mapping when data were sufficient; pairing with CWGAN augmentation rescued performance in low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Standard FFNNs provide accurate spectral regression when trained on ample labeled simulations, but their performance in low-data regimes can be substantially improved by augmenting training data with conditioned CWGAN-generated spectra.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2309.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2309.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCA (analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Principal Component Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dimensionality-reduction technique (SVD of covariance) used to visualize and compare the coverage of generated vs real spectra in 2D projection; used to diagnose mode collapse and reproduction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Analysis / evaluation of generative model coverage for spectral datasets</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Projecting 200-dimensional spectra into 2 principal components (retaining >90% reproduction rate) to visualize whether generative models reproduce the diversity of the training set.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Applied to the same set of simulated spectra (6,561 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional continuous spectral vectors projected to 2D via SVD on covariance matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Used for diagnostic visualization; complexity arises from capturing >90% variance in 2 components for this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Standard statistical tool used as diagnostic in ML and scientific data analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — employed as an analysis and diagnostic tool rather than a mechanistic model.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Principal Component Analysis (SVD of covariance)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Computed SVD of covariance Σ = (1/m) X^T X; retained first 2 columns of U to project data into 2D space. Reproduction Rate metric RR defined as ratio of first two singular values over total singular value sum; used to assert >90% variance retained.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised dimensionality reduction / exploratory analysis</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Useful diagnostic to compare generative model output vs training data and to reveal mode collapse in CGAN versus coverage in CWGAN.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>PCA retained >90% of variance (reproduction rate > 90%), enabling meaningful 2D visualization differences between CGAN (mode collapse) and CWGAN (good coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective as a qualitative diagnostic: clearly showed CGAN's mode collapse (only principal clusters reproduced) while CWGAN covered most training-set complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Provides a simple and interpretable diagnostic to evaluate generative model coverage on scientific datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Not compared to other diagnostics in the paper; used as the chosen projection method for visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High explained variance in first two components for these spectra permitted reliable 2D comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>PCA projection is a practical diagnostic to detect generative model failures (e.g., mode collapse) on high-dimensional scientific spectral datasets when principal components capture most variance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative Adversarial Networks <em>(Rating: 2)</em></li>
                <li>Wasserstein GAN <em>(Rating: 2)</em></li>
                <li>Improved Training of Wasserstein GANs <em>(Rating: 2)</em></li>
                <li>Conditional Generative Adversarial Nets <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2309",
    "paper_id": "paper-259924636",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "CWGAN",
            "name_full": "Conditional Wasserstein Generative Adversarial Network",
            "brief_description": "A conditional Wasserstein GAN augmented with a regression (MAE) conditioning term to generate labeled synthetic spectral data and to act as a surrogate model for spectra conditioned on geometry.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Near-field radiative heat transfer (spectral heat transfer coefficient prediction) in multilayer hyperbolic metamaterials",
            "problem_description": "Generate synthetic spectral heat transfer coefficient (h_ω) data conditioned on 8 geometric layer thicknesses, augment training data for regressors, and serve as a surrogate forward model mapping geometry → spectrum.",
            "data_availability": "Original (simulated) dataset of 6,561 labeled spectra (each spectrum sampled at 200 frequency points). Labeled (geometry parameters paired with spectra). The authors treat both moderate-data (80% train) and extreme low-data regimes (training reduced down to 1% of examples) by re-splitting the dataset; they also generated 10,000 synthetic spectra for augmentation. Data quality is high (physics-based simulation) and accessible within the study.",
            "data_structure": "High-dimensional numerical 1D spectra (200-point frequency vectors) paired with structured numerical condition vectors (8-dimensional layer-thickness vector); effectively structured dense arrays (multivariate continuous signals).",
            "problem_complexity": "High nonlinearity and high-dimensional outputs (resonant spectral features sensitive to 8 continuous geometric parameters); spectrum features include narrow/broad resonances and multi-modal structures; underlying forward physics requires scattering-matrix calculations (computational cost non-trivial).",
            "domain_maturity": "Mature physics domain with established theoretical models (fluctuational electrodynamics, scattering-matrix methods) but emergent use of generative ML for data augmentation and surrogate modeling.",
            "mechanistic_understanding_requirements": "Medium — scientific interpretability and correct conditioning are important so generated spectra reliably correspond to physical geometry; surrogate must preserve resonant features for scientific validation.",
            "ai_methodology_name": "Conditional Wasserstein Generative Adversarial Network (CWGAN) with MAE conditioning",
            "ai_methodology_description": "Generator: 4 fully connected layers (50, 100, 150, 200 neurons), selu activations, 20% dropout after each layer (used to provide variability instead of feeding explicit random z). Critic: two parallel FC branches for spectrum and condition (150 and 50 neurons), concatenation, then FC layers (100, 50), selu activations and a linear output score. Training: Wasserstein loss with gradient penalty (λ = 10) to enforce 1-Lipschitz; critic updated n_train = 5 times per generator update. Generator loss adds a Mean Absolute Error (MAE) term between generated and ground-truth spectrum conditioned on the same geometry, producing a hybrid adversarial+regression objective. Preprocessing: log of spectra, mean subtraction and standard deviation scaling of inputs/conditions. After training, generator was used to produce 10,000 synthetic labeled spectra for augmentation and also decoupled as a surrogate mapping geometry→spectrum.",
            "ai_methodology_category": "Generative modeling (conditional GAN) with supervised conditioning / hybrid adversarial + regression",
            "applicability": "Applicable and appropriate after two key modifications: (1) using Wasserstein loss with gradient penalty to avoid mode collapse and stabilize training; (2) adding a conditioning/regression (MAE) term to ensure generated spectra correspond to provided geometric labels. Well-suited for structured numerical spectral data and for data-augmentation or surrogate use in low-data regimes; less competitive than simple regression networks when abundant labeled data are available.",
            "effectiveness_quantitative": "Generator MAE converged to values &lt; 0.2 (training-monitoring MAE). Baseline FFNN (ample data) achieved validation L_point = 3.61% and L_integ = 1.45%. In extreme low-data (validation fraction = 99%), simple FFNN: L_integ = 13.2%, L_point = 18.2%; augmented FFNN (trained with 10k CWGAN spectra added): L_integ = 6.8%, L_point = 16.8%. CWGAN used as surrogate produced performance comparable to the augmented FFNN in the low-data regime (approaching L_integ ≈ 6.8% in the extreme case), but worse than FFNN when training data are abundant (validation fraction &lt; ~70%). PCA projection of CWGAN outputs reproduced training-set structure (PCA retained &gt;90% variance).",
            "effectiveness_qualitative": "CWGAN avoided mode collapse that afflicted a plain CGAN and produced realistic, label-consistent spectra; data augmentation with CWGAN substantially improved downstream FFNN performance in low-data regimes (half the integral error in the extreme case). As a standalone surrogate, CWGAN was more resilient than a simple FFNN when training data are scarce, but inferior to FFNN when data are plentiful. Training stability required WGAN techniques (gradient penalty and critic-heavy updates) and conditioning via MAE.",
            "impact_potential": "High for scientific settings with costly/limited spectral data: reduces the need for large labeled simulation/experimental datasets, enables inexpensive generation of labeled spectra for training/prediction, and provides a conditional surrogate fast-to-evaluate model. Likely generalizable to other spectral scientific problems (optics, chemistry, astronomy, spectroscopy) where labeled spectra are expensive to obtain.",
            "comparison_to_alternatives": "Direct comparison in paper: CGAN (plain conditional GAN) exhibited mode collapse and failed to reproduce training diversity (via PCA). Compared to a simple FFNN regressor: with abundant data FFNN outperformed CWGAN surrogate; with scarce data CWGAN-augmented FFNN and CWGAN surrogate outperformed simple FFNN. Numerical comparisons: see Effectiveness Quantitative field above.",
            "success_factors": "Use of Wasserstein loss and gradient penalty to stabilize adversarial training; conditioning via MAE term to tie generated spectra to geometry labels; sufficient critic updates per generator update (n_train = 5); dropout-based variability in generator obviating explicit latent z; careful input preprocessing (log transform, normalization); architecture choices tuned to spectral dimensionality.",
            "key_insight": "Conditioned Wasserstein GANs that combine adversarial training with a regression conditioning term produce labeled synthetic spectra that substantially improve downstream regression performance and surrogate accuracy in low-data regimes, while avoiding mode collapse of plain conditional GANs.",
            "uuid": "e2309.0"
        },
        {
            "name_short": "WGAN",
            "name_full": "Wasserstein Generative Adversarial Network",
            "brief_description": "A GAN variant that uses the Wasserstein (Earth Mover's) distance as loss, employs a critic producing scalar scores (not probabilities), and (here) enforces 1-Lipschitz via a gradient penalty to stabilize training and avoid mode collapse.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Generative modeling of scientific spectral data (same domain as CWGAN entry)",
            "problem_description": "Address training instabilities and mode collapse present in standard GANs when generating complex spectral datasets.",
            "data_availability": "Used on the same simulated dataset of 6,561 labeled spectra (200 frequency points each).",
            "data_structure": "High-dimensional spectral vectors with associated geometry labels.",
            "problem_complexity": "Training a generative model for multimodal, highly structured spectral outputs prone to mode collapse under standard GAN losses.",
            "domain_maturity": "WGAN is an established technique in generative modeling literature; applied here to a mature physics simulation domain.",
            "mechanistic_understanding_requirements": "Medium — stable training and meaningful loss metric help interpret distance between generated and real distributions but do not by themselves provide causal/physical interpretability.",
            "ai_methodology_name": "Wasserstein GAN (WGAN) with gradient penalty",
            "ai_methodology_description": "Critic outputs a score (not a probability); loss is expectation over critic scores for real and generated samples plus gradient-penalty term: L_C = −E[C(x)] + E[C(G(z))] + λ E[(||∇C( x_hat )||_2 − 1)^2], with λ = 10. Critic trained n_train = 5 times per generator update. Enforces 1-Lipschitz via gradient penalty rather than weight clipping.",
            "ai_methodology_category": "Generative modeling / adversarial learning",
            "applicability": "Necessary modification versus vanilla GAN for stable spectral generation in this problem; proved essential to avoid mode collapse and produce diverse spectra.",
            "effectiveness_quantitative": "Use of WGAN (with gradient penalty) in the conditional setting enabled the CWGAN to reproduce the training-set complexity (PCA shows broad coverage whereas plain CGAN collapsed). No standalone numeric metric for WGAN alone beyond its contribution to CWGAN results.",
            "effectiveness_qualitative": "WGAN formulation produced meaningful and stable critic loss behavior (allowed training to converge to stationary loss after ~10^4 steps) and prevented mode collapse observed with standard GAN loss.",
            "impact_potential": "Provides stable adversarial training for scientific generative tasks where mode diversity and stability are required; critical enabler for conditional spectral generators.",
            "comparison_to_alternatives": "Compared implicitly to standard GAN/CGAN: standard conditional GAN showed mode collapse and poor coverage across PCA projection; WGAN-based CWGAN achieved good coverage and label-consistent generation.",
            "success_factors": "Gradient penalty enforcement of Lipschitz constraint (λ = 10), multiple critic updates per generator step (n_train = 5), and monitoring of loss components to reach stable training.",
            "key_insight": "Using Wasserstein loss with gradient penalty is essential to stabilize adversarial training and avoid mode collapse when generating complex scientific spectra.",
            "uuid": "e2309.1"
        },
        {
            "name_short": "CGAN",
            "name_full": "Conditional Generative Adversarial Network (standard loss)",
            "brief_description": "A conditional GAN trained with the original GAN loss (Jensen-Shannon-based), used here as a baseline and shown to suffer from mode collapse on complex numerical spectral data.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Generative modeling of spectral data for near-field radiative heat transfer",
            "problem_description": "Attempt to generate labeled synthetic spectra conditioned on geometry using the conventional conditional GAN loss.",
            "data_availability": "Training on the same simulated dataset (6,561 labeled spectra).",
            "data_structure": "High-dimensional 200-point spectral vectors paired with 8-dim condition vectors.",
            "problem_complexity": "Multimodal spectral distributions with narrow resonances; standard GAN training struggles to cover all modes.",
            "domain_maturity": "CGAN is a well-known method but here shown inadequate without WGAN modifications.",
            "mechanistic_understanding_requirements": "Medium — conditioning needed to preserve label-sample correspondence, which CGAN failed to maintain here due to mode collapse.",
            "ai_methodology_name": "Conditional GAN (original GAN loss)",
            "ai_methodology_description": "Generator and discriminator trained with original GAN loss: discriminator minimizes negative log-likelihood on real vs generated samples; the conditional generator loss combines conditional reconstruction and adversarial loss terms. In practice, this model was trained with the same architecture design as CWGAN for comparison but with the original GAN losses.",
            "ai_methodology_category": "Generative modeling / adversarial learning",
            "applicability": "Not appropriate for this problem without modification — exhibited strong mode collapse and failed to reproduce the diversity of the training spectra.",
            "effectiveness_quantitative": "PCA visualization showed the CGAN reproduced only principal cluster structures and failed to cover most training examples (contrast: CWGAN reproduced most complexities); no numeric error metrics given because model failed qualitatively.",
            "effectiveness_qualitative": "Suffered from mode collapse; unable to reproduce the full variety of spectral features present in the training data and thus unsuitable for labeled spectral augmentation or surrogate tasks in this application.",
            "impact_potential": "Low for this specific spectral generation problem unless modified; demonstrates that naive conditional GANs may be unreliable for complex scientific numerical data.",
            "comparison_to_alternatives": "Directly compared to CWGAN: CGAN showed mode collapse while CWGAN (Wasserstein + conditioning) reproduced training set structure well (PCA).",
            "success_factors": "Failure attributed to using original GAN loss; lack of Wasserstein loss/gradient penalty and lack of stronger conditioning/regression term led to mode collapse.",
            "key_insight": "Plain conditional GAN (original loss) is prone to mode collapse on complex, multimodal scientific spectral data and thus requires modifications (Wasserstein loss and conditioning) to be useful.",
            "uuid": "e2309.2"
        },
        {
            "name_short": "FFNN",
            "name_full": "Feed-Forward Neural Network (baseline regressor)",
            "brief_description": "A standard feed-forward neural network used both as a baseline supervised regressor for mapping geometry → spectrum and as the model retrained after CWGAN data augmentation.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Supervised regression predicting spectral heat transfer coefficient from geometry in near-field radiative heat transfer",
            "problem_description": "Predict the 200-point h_ω spectrum given 8 geometric layer thicknesses using a dense neural network.",
            "data_availability": "Trained on the original simulated dataset (6,561 spectra); also retrained on augmented datasets where 10,000 CWGAN-generated spectra were added. Used varying train/validation splits from 80/20 down to 1/99 to probe low-data regimes.",
            "data_structure": "Structured tabular-to-vector regression: 8-dimensional input vectors mapped to 200-dimensional continuous outputs.",
            "problem_complexity": "High-dimensional output regression with nonlinear mapping sensitive to resonances; network used had five hidden layers of 200 neurons each with selu activation.",
            "domain_maturity": "Standard supervised learning approach; previously successful in this physics system when ample data are available (cited prior work).",
            "mechanistic_understanding_requirements": "Medium — model provides predictive accuracy but is a black-box regressor; physical validation still required for scientific trust.",
            "ai_methodology_name": "Feed-forward dense neural network regressor (5 hidden layers × 200 neurons)",
            "ai_methodology_description": "Architecture: 5 hidden fully connected layers, each with 200 neurons and selu activations, final linear output layer for 200-point spectrum. Training: Adam optimizer, learning rate 3×10^-4, trained for 50,000 epochs in baseline experiments. Used as baseline and retrained with CWGAN-augmented data.",
            "ai_methodology_category": "Supervised deep learning (regression)",
            "applicability": "Highly applicable when sufficient labeled data are available; served as a strong baseline achieving low errors with ample training examples. Performance degrades substantially in extreme low-data regimes but benefits from CWGAN augmentation.",
            "effectiveness_quantitative": "With abundant training data (80% train split): validation L_point = 3.61% and L_integ = 1.45%. In extreme low-data (validation fraction = 99%): simple FFNN L_integ = 13.2% and L_point = 18.2%; after CWGAN augmentation these drop to L_integ = 6.8% and L_point = 16.8%.",
            "effectiveness_qualitative": "FFNN is effective and accurate with enough physics-simulated training data; however, in low-data regimes its performance falls off sharply, and synthetic augmentation via CWGAN materially improves integral-error-sensitive performance.",
            "impact_potential": "Immediate practical utility as a fast forward predictor when trained on sufficient data; combining with CWGAN augmentation extends applicability into low-data settings and reduces reliance on expensive simulations.",
            "comparison_to_alternatives": "Compared to CWGAN surrogate: FFNN outperforms CWGAN when abundant data are available; with scarce training data, CWGAN-augmented FFNN and CWGAN surrogate outperform simple FFNN. Compared to CGAN: FFNN unaffected by mode-collapse issues (CGAN is a generator).",
            "success_factors": "Sizable architecture and long training (50k epochs) with Adam optimizer and good preprocessing allowed accurate mapping when data were sufficient; pairing with CWGAN augmentation rescued performance in low-data regimes.",
            "key_insight": "Standard FFNNs provide accurate spectral regression when trained on ample labeled simulations, but their performance in low-data regimes can be substantially improved by augmenting training data with conditioned CWGAN-generated spectra.",
            "uuid": "e2309.3"
        },
        {
            "name_short": "PCA (analysis)",
            "name_full": "Principal Component Analysis",
            "brief_description": "Dimensionality-reduction technique (SVD of covariance) used to visualize and compare the coverage of generated vs real spectra in 2D projection; used to diagnose mode collapse and reproduction quality.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Analysis / evaluation of generative model coverage for spectral datasets",
            "problem_description": "Projecting 200-dimensional spectra into 2 principal components (retaining &gt;90% reproduction rate) to visualize whether generative models reproduce the diversity of the training set.",
            "data_availability": "Applied to the same set of simulated spectra (6,561 examples).",
            "data_structure": "High-dimensional continuous spectral vectors projected to 2D via SVD on covariance matrix.",
            "problem_complexity": "Used for diagnostic visualization; complexity arises from capturing &gt;90% variance in 2 components for this dataset.",
            "domain_maturity": "Standard statistical tool used as diagnostic in ML and scientific data analysis.",
            "mechanistic_understanding_requirements": "Low — employed as an analysis and diagnostic tool rather than a mechanistic model.",
            "ai_methodology_name": "Principal Component Analysis (SVD of covariance)",
            "ai_methodology_description": "Computed SVD of covariance Σ = (1/m) X^T X; retained first 2 columns of U to project data into 2D space. Reproduction Rate metric RR defined as ratio of first two singular values over total singular value sum; used to assert &gt;90% variance retained.",
            "ai_methodology_category": "Unsupervised dimensionality reduction / exploratory analysis",
            "applicability": "Useful diagnostic to compare generative model output vs training data and to reveal mode collapse in CGAN versus coverage in CWGAN.",
            "effectiveness_quantitative": "PCA retained &gt;90% of variance (reproduction rate &gt; 90%), enabling meaningful 2D visualization differences between CGAN (mode collapse) and CWGAN (good coverage).",
            "effectiveness_qualitative": "Effective as a qualitative diagnostic: clearly showed CGAN's mode collapse (only principal clusters reproduced) while CWGAN covered most training-set complexities.",
            "impact_potential": "Provides a simple and interpretable diagnostic to evaluate generative model coverage on scientific datasets.",
            "comparison_to_alternatives": "Not compared to other diagnostics in the paper; used as the chosen projection method for visualization.",
            "success_factors": "High explained variance in first two components for these spectra permitted reliable 2D comparisons.",
            "key_insight": "PCA projection is a practical diagnostic to detect generative model failures (e.g., mode collapse) on high-dimensional scientific spectral datasets when principal components capture most variance.",
            "uuid": "e2309.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative Adversarial Networks",
            "rating": 2,
            "sanitized_title": "generative_adversarial_networks"
        },
        {
            "paper_title": "Wasserstein GAN",
            "rating": 2,
            "sanitized_title": "wasserstein_gan"
        },
        {
            "paper_title": "Improved Training of Wasserstein GANs",
            "rating": 2,
            "sanitized_title": "improved_training_of_wasserstein_gans"
        },
        {
            "paper_title": "Conditional Generative Adversarial Nets",
            "rating": 2,
            "sanitized_title": "conditional_generative_adversarial_nets"
        }
    ],
    "cost": 0.018512999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generative adversarial networks for data-scarce spectral applications Generative adversarial networks for data-scarce spectral applications 2
14 Jul 2023</p>
<p>J J García-Esteban 
Departamento de Física Teórica de la Materia Condensada and Condensed Matter Physics Center (IFIMAC)
Departamento de Física Teórica de la Materia Condensada and Condensed Matter Physics Center (IFIMAC)
Departamento de Física Teórica de la Materia Condensada and Condensed Matter Physics Center (IFIMAC)
Universidad Autónoma de Madrid
Universidad Autónoma de Madrid, E-, Universidad Autónoma de Madrid, EE-28049, 28049, 28049Madrid, Madrid, MadridSpain, Spain, Spain</p>
<p>J C Cuevas 
Departamento de Física Teórica de la Materia Condensada and Condensed Matter Physics Center (IFIMAC)
Departamento de Física Teórica de la Materia Condensada and Condensed Matter Physics Center (IFIMAC)
Departamento de Física Teórica de la Materia Condensada and Condensed Matter Physics Center (IFIMAC)
Universidad Autónoma de Madrid
Universidad Autónoma de Madrid, E-, Universidad Autónoma de Madrid, EE-28049, 28049, 28049Madrid, Madrid, MadridSpain, Spain, Spain</p>
<p>J Bravo-Abad 
Departamento de Física Teórica de la Materia Condensada and Condensed Matter Physics Center (IFIMAC)
Departamento de Física Teórica de la Materia Condensada and Condensed Matter Physics Center (IFIMAC)
Departamento de Física Teórica de la Materia Condensada and Condensed Matter Physics Center (IFIMAC)
Universidad Autónoma de Madrid
Universidad Autónoma de Madrid, E-, Universidad Autónoma de Madrid, EE-28049, 28049, 28049Madrid, Madrid, MadridSpain, Spain, Spain</p>
<p>Generative adversarial networks for data-scarce spectral applications Generative adversarial networks for data-scarce spectral applications 2
14 Jul 2023
Generative adversarial networks (GANs) are one of the most robust and versatile techniques in the field of generative artificial intelligence. In this work, we report on an application of GANs in the domain of synthetic spectral data generation, offering a solution to the scarcity of data found in various scientific contexts. We demonstrate the proposed approach by applying it to an illustrative problem within the realm of near-field radiative heat transfer involving a multilayered hyperbolic metamaterial. We find that a successful generation of spectral data requires two modifications to conventional GANs: (i) the introduction of Wasserstein GANs (WGANs) to avoid mode collapse, and, (ii) the conditioning of WGANs to obtain accurate labels for the generated data. We show that a simple feedforward neural network (FFNN), when augmented with data generated by a CWGAN, enhances significantly its performance under conditions of limited data availability, demonstrating the intrinsic value of CWGAN data augmentation beyond simply providing larger datasets. In addition, we show that CWGANs can act as a surrogate model with improved performance in the low-data regime with respect to simple FFNNs. Overall, this work highlights the potential of generative machine learning algorithms in scientific applications beyond image generation and optimization. arXiv:2307.07454v1 [physics.optics]</p>
<p>Introduction</p>
<p>Machine learning, a rapidly expanding area within computer science, focuses on advancing the foundations and technology that enables machines to learn from data [1,2,3,4]. Deep learning, on the other hand, is a subset of machine learning techniques that uses artificial neural networks (ANNs) to model and solve complex data-driven problems, such as image and speech recognition [5,6], natural language processing [7], and autonomous driving [8], among many others. With the current explosion of data and the rapid development of improved hardware and algorithms, machine learning and deep learning are becoming crucial tools in many industries, including healthcare, finance, and manufacturing.</p>
<p>Motivated by this success, machine learning and deep learning techniques are attracting increasing attention from a variety of scientific disciplines beyond computer science, revolutionizing traditional approaches to the modeling and analysis of datadriven scientific problems. In physics, these techniques are employed to tackle complex problems [9], including the representation of quantum many-body wave functions [10], the discovery and identification of phase transitions in condensed-matter systems [11,12,13], the solution of statistical problems [14], the development of novel quantum information technologies [15], the modeling of gravitational waves [16], and the design of nanophotonic devices with novel or improved functionalities [17,18,19,20]. Machine learning algorithms have been effectively used for the accelerated discovery and design of new materials and molecules in the fields of materials science and chemistry [21,22], being instrumental in molecular dynamics simulations [23], in predicting chemical reactions [24], and in modeling the quantum mechanical energies of molecules [25]. In the realm of biology, the applications of machine learning and deep learning are also vast, including breakthroughs in gene expression prediction tasks, prediction of micro-RNAs targets, and novel single-cell methods [26]. Overall, despite significant challenges, such as the interpretability and transferability, machine learning and deep learning are playing an increasingly pivotal role in the advance of a broad variety of scientific research methodologies.</p>
<p>In this context, generative adversarial networks (GANs), a powerful subset of generative machine learning, have emerged as a versatile tool for creating new data instances that closely resemble a given training set [27]. This innovative paradigm involves a two-player adversarial setup where a generative ANN strives to produce data instances that are indistinguishable from the training set, while a discriminative network attempts to distinguish between the instances generated by the generative network and the real data [28]. The competing nature of these two networks drives the generative network to generate increasingly realistic data, pushing the boundaries of what is achievable with generative models. This technology has been instrumental across a broad range of scientific disciplines, including physics, chemistry, and biology. In physics, GANs have been used for simulating complex systems and predicting outcomes of experiments, with examples in high energy physics [29], condensed matter physics [30,31,32], nanophotonics [33,34], and cosmology [35]. In the field of chemistry, GANs have been harnessed to generate novel chemical structures and predict their properties [36], thereby accelerating the process of drug discovery and materials design [37]. In biology, GANs have been employed in a variety of tasks, including protein engineering [38] and generate biological imaging data [39]. This myriad of applications demonstrate the significant potential of GANs in transforming scientific research by providing a powerful tool for hypothesis generation, experimental design, and data augmentation where empirical data is scarce or expensive to obtain. Despite their significant success in image generation, the use of GANs has been mostly limited to this area. It would be highly desirable to see their application more widely spread in the generation of scientific numerical data.</p>
<p>In this work, we introduce a novel application of GANs for synthetic spectral data generation. This offers a solution to the data scarcity found in scientific contexts where collecting a significant amount of spectral signals is critical for a subsequent application of data-driven approaches. Such a scenario is common across a wide range of fields, including physics, chemistry, astronomy, biology, medicine, and geology. Here, we particularly focus on an illustrative problem in the research area of nearfield radiative heat transfer, involving a multilayered hyperbolic metamaterial. We explore the use of a Conditional Wasserstein GAN (CWGAN) for data augmentation and investigate its impact on the predictive capabilities of a feed-forward neural network (FFNN). We find that the successful production of spectral data requires two main changes to standard GANs. Firstly, the implementation of Wasserstein GANs (WGANs) is necessary to counteract mode collapse, and secondly, these WGANs need to be conditioned to yield accurate labels for the generated data. We demonstrate that a simple FFNN, when augmented with data produced by a CWGAN, notably improves its performance under conditions of data scarcity. This underscores the intrinsic value of CWGAN data augmentation, not just as a means to expand datasets. Furthermore, we illustrate that CWGANs have the ability to serve as efficient surrogate model in low-data regimes. Overall, this research work contributes to highlighting generative AI algorithms' potential in applications extending beyond the conventional realm of image generation. We also anticipate that our findings will contribute to advancing the understanding and application of generative AI algorithms in data-limited scientific contexts.</p>
<p>This work is organized as follows. In Section 2, we review the fundamentals of the primary generative adversarial frameworks underlying this work. Section 3 discusses the basic principles of the particular physical problem we utilize to exemplify our approach. In Section 4, we present and discuss the results obtained from implementing the generative adversarial method detailed in Section 2 to the specific example problem outlined in Section 3. Finally, in Section 5 we sum up the conclusions of this work. Figure 1. Schematic representation of a Conditional Generative Adversarial Network (CGAN) employed to produce synthetic spectral data. The process begins with the combination of geometrical data (W), and a random vector (z), both of which are fed into the generator. The generator's role is to create a synthetic, or 'fake', spectrum from these inputs. The generated spectrum is then introduced to the discriminator, along with the real spectra and the geometrical data. The discriminator's task is to discern between the fake and real spectra. The conclusions drawn by the discriminator are subsequently used to guide the training of both the generator and discriminator networks, enhancing their ability to generate realistic spectra and to distinguish between real and synthetic data, respectively.</p>
<p>Generative adversarial route to synthetic spectral data generation</p>
<p>In this Section, we review the basics of the three main generative adversarial frameworks that form the basis of this work, namely, GANs, Wasserstein GANs (WGANs) and Conditional WGANs (CWGANs). Here, we assume that the origin of the real spectra used in these approaches is completely general, coming from any from physical, chemical or biological process (in the following Sections we discuss the application to a particular problem within the realm of near-field radiative heat transfer). Figure 1 shows a schematic representation of the underlying architecture of a general GAN algorithm. It comprises two main parts: a generator network and a discriminator network (represented as orange and blue rectangles, respectively, in Fig. 1). The generator and the discriminator are two interconnected networks in the GAN system. The generator generally begins with random noise (z in Fig. 1) and uses it to create new data samples (new spectra in our application). On the other hand, the discriminator takes these spectra and calculates the likelihood that each one originates from the actual training data set. The two networks have competing objectives. The generator's goal is to create spectra that perfectly mirrors the distribution of the training data. In doing so, it aims at generating spectral data so convincingly authentic that the discriminator cannot tell it apart from the real training data. Meanwhile, the discriminator aims at distinguishing between the actual training data and the data fabricated by the generator. Both networks are trained together in a competition until a Nash-type equilibrium is reached and the training process ends [27]. At that stage, the generator is producing spectra that the discriminator can no longer reliably classify as 'real' or 'fake', signaling the end of the training process.</p>
<p>Mathematically, the GAN configuration can be formulated as a minimization problem of the generator and discriminator loss functions, suitably written in terms of differentiable functions representing the discriminator and the generator network models, D(x; θ d ) and G(z; θ g ), respectively (θ d and θ g are the parameters of the corresponding ANNs -in what follows, for the sake of clarity, we do not include this dependence in the equations). We start by focusing on the loss function of the discriminator, L D (G, D), which can be written as [27] 
L D (G, D) = −E x∼p data (x) [log(D(x))] − E z∼pz(z) [log(1 − D(G(z)))],(1)
where E stands for expectation over either the training samples, x, or the input noise variables, z (characterized respectively by a probability distribution p data (x) and a prior distribution p z (z)). From Eq. (1), and considering that D outputs a single scalar representing a probability, we can see that the training of the discriminator is trying to minimize the likelihood of mistaking a real sample for a fake one or a fake sample for a real one (first and second terms, respectively, of the right-hand side of Eq. (1)). However, there are significant performance issues associated to this original choice of the loss function L D (G, D), including the difficulty to reach the above-mentioned Nash equilibrium state (each network is updated independently, and given the competitive nature of the generator and the discriminator, in general, there is no clear point to stop the training) or the so-called mode collapse (arising when a network fails to generalize accurately to all regions of the training data distribution). To address these issues, a different strategy to train GANs was introduced, the so-called Wasserstein Generative Adversarial Networks (WGANs) [40]. One of the main modifications of WGANs with respect to the original GAN architecture is the presence of a critic model, C(x), instead of the discriminator model. Importantly, C(x) outputs a score instead of a probability, which, in turn, allows us to define a new loss function for the critic, L C (G, C), given by
L C (G, C) = −E x∼p data (x) [C(x)] + E z∼pz(z) [C(G(z))].(2)
This loss function summarizes well some of the main advantages of the WGANs. WGANs help address the training challenges of GANs by using the Wasserstein distance (or Earth Mover's distance) as the loss function instead of the Jensen-Shannon divergence used in original GANs [40]. In addition, it provides a meaningful loss metric, i.e., the value of the critic in WGANs provides a meaningful measure of the distance between the real and generated data distributions. This is unlike the original GANs where the discriminator's output does not correlate well with the quality of the generated samples.</p>
<p>A pivotal aspect of WGANs is the need for the Critic to operate within the set of the so-called 1-Lipschitz functions, a critical component of the model [40]. Lipschitz functions are mathematical functions possessing a property where there exists a realvalued constant such that, for every pair of points, the absolute difference in function values can be bounded by this constant times the absolute difference of input values. When this constant is 1, the functions are known as 1-Lipschitz. The 1-Lipschitz constraint is crucial as it bounds how much a function's output can change with small variations in input, ensuring the function does not change too abruptly. In WGANs, this is vital to ensure that the Critic provides meaningful and stable gradients for the generator to learn from, facilitating a more reliable learning process. Originally, weight clipping was proposed to enforce this 1-Lipschitz condition. However, this sometimes resulted in convergence failure [40]. To counter these issues, a more robust technique known as the gradient penalty was introduced. This method involves adding a loss term to maintain the L2 norm (a measure of the vector length of parameters or weights) of the Critic close to a value of 1 [41]. This approach assists in keeping the Critic's function within the 1-Lipschitz constraint, enhancing the stability and performance of WGANs. Incorporating these improvements, the full loss for the critic in a WGAN now reads
L C (G, C) = − E x∼p data (x) [C(x)] + E z∼pz(z) [C(G(z))] + λ Ex ∼px (||∇xC(x)|| 2 − 1) 2 ,(3)
where || · || 2 is the L2 norm and λ is a weight parameter for the gradient penalty (throughout this work we assume λ = 10, as pointed out in Ref. [41]), andx = x + α(G(z) − x) is an interpolated point between a real sample and generated sample on which to calculate the gradient, with α ∈ [0, 1]. As a final remark, proper training of the WGAN requires the critic to be trained ahead of the generator, so that for each training step of the generator the critic is updated n train times (following [40], we chose n train = 5 in all our models). As for the generator loss function, L G (G, C), an important aspect to realize is that for synthetic spectral data generation our focus is on a regression problem. This implies that we need a greater control over the output than that obtained just by acquiring a random but realistic sample. It is therefore crucial to ensure that the generated data accurately corresponds to the correct system parameters that yield that response. To achieve this, we need to condition the WGAN, leading to the creation of a Conditional WGAN (CWGAN) [42]. In this work, we will implement that by adding an extra loss term quantifying the Mean Absolute Error (MAE) between the conditioned generated example and the training example corresponding to the ground truth of the condition. Accounting for this conditioning, L G (G, C) can be expressed as
L G (G, C) = E z∼pz(z), x∼p data (x) [|x − G(z|W)|] − E z∼pz(z) [C(G(z))],(4)
where x is the training example corresponding to the system parameters W, and G(z|W) is a generated example conditioned on the same parameters W. The first term in the r.h.s. of Eq. (4) corresponds to the above discussed conditioning procedure, while the second term is associated to the coupling of the generator and the critic.</p>
<ol>
<li>Illustrative problem: Near-field radiative heat transfer spectra in multilayer hyperbolic metamaterials</li>
</ol>
<p>In this Section we provide an overview of the fundamentals of the specific problem we use to illustrate the proposed approach. We have chosen a physical problem in the context of near-field radiative heat transfer involving multilayer hyperbolic metamaterials [43].</p>
<p>Despite the specific character of this class of systems, the chosen problem can be considered both representative of the types of problems that our approach can address effectively and complex enough to showcase the versatility of our method.</p>
<p>One of the major advances in recent years in the field of thermal radiation has been the experimental confirmation that the limit set by Stefan-Boltzmann's law for the radiative heat transfer between two bodies can be largely overcome by bringing them sufficiently close [44]. This phenomenon is possible because in the near-field regime, i.e., when the separation between two bodies is smaller than the thermal wavelength λ Th (∼10 µm at room temperature), radiative heat can also be transferred via evanescent waves (or photon tunneling). This new contribution is not taken into account in Stefan-Boltzmann's law and dominates the near-field radiative heat transfer (NFRHT) for sufficiently small gaps or separations [45,46,47]. Among the different strategies that have been recently proposed to further enhance NFRHT, one of the most popular ideas is based on the use of multiple surface modes that can naturally appear in multilayer structures. In this regard, a lot of attention has been devoted to multilayer systems where dielectric and metallic layers are alternated to give rise to the so-called hyperbolic metamaterials [48,49,50,51,52,53,54,55,56,57]. The hybridization of surface modes appearing in different metal-dielectric interfaces have indeed been shown to lead to a great enhancement of the NFRHT, as compared to the case of two infinite parallel plates [55].</p>
<p>Following Ref. [55], we consider here the radiative heat transfer between two identical multilayer structures separated by a gap d 0 , as shown in Fig. 2(a). Each body contains N total layers alternating between a metallic layer with a permittivity m and a lossless dielectric layer of permittivity d . The thickness of the layer i is denoted by d i and it can take any value within a given range (to be specified below). While the dielectric layers will be set to vacuum ( d = 1), the metallic layers will be described by a permittivity given by a Drude model:
m (ω) = ∞ − ω 2 p /[ω(ω + iγ)],
where ∞ is the permittivity at infinite frequency, ω p is the plasma frequency, and γ de damping rate. From now on, we set ∞ = 1, ω p = 2.5 × 10 14 rad/s, and γ = 1 × 10 12 rad/s (these parameters provide a surface plasmon frequency similar to the surface phonon-polariton frequency of the interface between SiC and vacuum).</p>
<p>We describe the radiative heat transfer within the framework of fluctuational electrodynamics [58,59], particularly focusing on the near-field regime. In this regime, the radiative heat transfer is dominated by TM-or p-polarized evanescent waves and the heat transfer coefficient (HTC) between the two bodies, i.e., the linear radiative  Table 1.</p>
<p>thermal conductance per unit of area, is given by [60] 
h = ∂ ∂T ∞ 0 dω 2π Θ(ω, T ) ∞ ω/c dk 2π k τ p (ω, k),(5)
where T is temperature, Θ(ω, T ) = ω/(e ω/k B T − 1) is the mean thermal energy of a mode of frequency ω, k is the magnitude of the wave vector parallel to the surface planes, and τ p (ω, k) is the transmission (between 0 and 1) of the p-polarized evanescent modes given by  Fig. 2(c).
τ p (ω, k) = 4 [Im {r p (ω, k)}] 2 e −2q 0 d 0 |1 − r p (ω, k) 2 e −2q 0 d 0 | 2 .(6)</p>
<p>Layer thicknesses (nm)</p>
<p>Label Here, r p (ω, k) is the Fresnel reflection coefficient of the p-polarized evanescent waves from the vacuum to one of the bodies and q 0 = k 2 − ω 2 /c 2 (ω/c &lt; k) is the wave vector component normal to the layers in vacuum. The Fresnel coefficient needs to be computed numerically and we have done it by using the scattering matrix method described in Ref. [61]. In our numerical calculations of the HTC we also took into account the contribution of s-polarized modes, but it turns out to be negligible for the gap sizes explored in this work. Let us briefly recall that, as explained in Ref. [55], the interest in the NFRHT in these multilayer structures resides in the fact that the heat exchange in this regime is dominated by surfaces modes that can be shaped by playing with the layer thicknesses. In the case of two parallel plates made of a Drude metal, the NFRHT is dominated by the two cavity surface modes resulting from the hybridization of the surface plasmon polaritons (SPPs) of the two metal-vacuum interfaces [55]. These two cavity modes give rise to two near-unity lines in the transmission function τ p (ω, k). Upon introducing more internal layers with appropriate thicknesses, one can have NFRHT contributions from surface states at multiple surfaces, as we illustrate in Fig. 2(b) for the case of N = 8 layers (4 metallic and 4 dielectric layers), separated by a vacuum gap of d 0 = 10 nm at T = 300 K. As shown in Ref. [55], the contribution of these additional surface states originating from internal layers can lead to a great enhancement of the NFRHT as compared to the bulk system (two parallel plates) in a wide range of gap values.
d 1 d 2 d 3 d 4 d 5 d 6 d 7 d 8
Of special interest for this work is the spectral HTC, h ω , defined as the HTC per unit of frequency: h = ∞ 0 h ω dω. To create the initial dataset of real spectra (which later on will be augmented by our generative adversarial approach), we apply the abovedescribed theoretical framework to compute a total of 6,561 h ω spectra. The thicknesses d i of each layer were varied between 5 and 20 nm, and every spectrum contains 200 frequency points in the range ω ∈ [0.3, 3] × 10 14 rad/s. As discussed in the next Section, that dataset of spectra will be split in different proportions to become training and test sets. Figure 2(c) shows several representative samples of h ω spectra, corresponding to the following thicknesses combinations listed in Table 1.</p>
<p>The spectra displayed in Fig. 2(c) show the broad variety of spectral features that can be obtained from the studied system (from double broad peaks with very narrow resonances in between, to single narrow peaks, or to two resonant peaks separated by a gap). This set of spectra essentially serves as a comprehensive blueprint for the whole adversarial approach, guiding it on the characteristics and features that should be exhibited in the synthetic data. Hence, this allows the proposed approach to capture a wider range of underlying patterns and relationships, which in turn allows it to generate a more realistic and diverse array of synthetic spectral data.</p>
<p>Results and discussion</p>
<p>We proceed in this Section to report on the results obtained when applying the generative adversarial approach summarized in Section 2 to the specific illustrative problem described in Section 3. We begin by providing a strong quantitative justification of the necessity of using a trained CWGAN for this problem, instead of a plain CGAN (Conditional GAN -the details of the specific architecture used in each case are provided below). Figures 3(a) and 3(b) show, respectively, the ability to reproduce the training set of a trained CGAN and a trained CWGAN projecting the data on two dimensions via a Principal Component Analysis (PCA) [62], which retains most of the training data structure due to its &gt; 90% reproduction rate (RR).</p>
<p>The PCA calculations shown in Fig. 3 were done as follows. First, we performed a singular value decomposition (SVD) of the covariance matrix Σ: [U, S, V] = S V D(Σ) with Σ = 1 m X T X [63]. Here, U and V are unitary matrices, S is the singular value matrix, m is the total number of data examples and X is the data matrix, containing in each column one data example x. To obtain the reduced 2-dimensional (2D) representation of the data, we calculated a reduced matrix U reduced retaining the first 2 columns of the U matrix obtained from the SVD, and use it as the projection matrix,x = U T reduced X, wherex is the 2-dimensional representation of the data. The reproduction rate (RR) of the whole 2D-PCA analysis is then defined as the ratio of the first 2 singular values over all the N singular values obtained:
RR = 2 i=1 S ii N i=1 S ii(7)
The Conditional GAN and the Conditional WGAN share the same architectural design, the specifics of which are detailed further on. Both networks underwent identical training conditions, with the complete spectral dataset partitioned into 80% for training and 20% for validation. The key distinguishing factor lies in their respective loss functions (L D (G, D) for CGAN and L C (G, C) for CWGAN). The CGAN employs Eq. (1) along with a generator loss which is the sum of the first term in the r.h.s. of Eq. (4) (the conditional term) and the second term in the r.h.s. of Eq. (1) with opposite sign. Meanwhile, the CWGAN utilizes the loss defined in Eq. (4) for the generator and Eq. (3) for the critic. Notably, as depicted in Fig. 3(a), the CGAN manifests evident signs of mode collapse, rendering it unable to reproduce most examples beyond the principal cluster structures in the PCA. In contrast, the CWGAN shows the capability of replicating most of the complexities within the training set. These results can be considered as a novel illustration, in the context of synthetic generation of spectral data, of the key role played by Wasserstein's loss function to create more robust generative adversarial approaches. Figure 4 summarizes the specific CWGAN architecture we have found as most efficient for the studied problem. The generator (left side of Fig. 4) is composed of 4 hidden fully connected layers of an increasing number of neurons in each layer (we consider 50, 100, 150, and 200 neurons for the four layers -represented as green blocks, labeled as FC1-FC4, in Fig. 4). The generator model takes the condition as input (the 8 values for the thicknesses of the layers) and returns a generated h ω spectrum (sampled by 200 frequencies). Consistently with the results in Refs. [64,65], we found that we did not need to feed a random data distribution z into the generator for operation: 20% dropout layers between all layers of the generator (i.e., connections between two consecutive neurons are dropped with a 20% chance) provide enough variability for this task (the dropout operations are represented as orange blocks in Fig. 4). On the other hand, the critic (right side of Fig. 4) takes both a sample spectrum and the condition (either generated or from the training distribution) in parallel processing lines of a single hidden layer (with 150 and 50 neurons, respectively -FC5 and FC6 in Fig. 4). Then, it concatenates and processes the information through two additional hidden layers (FC7 and FC8 in Fig. 4, with 100 and 50 neurons, respectively) to output a single number, the score. All hidden layers feature a scaled exponential linear unit (selu) activation function. In all models discussed in this work, as a pre-processing step, we will also As shown, the generator accepts geometrical parameters as input and produces a heat transfer coefficient (HTC) spectrum as output. It is constructed of four fully connected layers (FCs) (green blocks), dropout layers placed after each FC layer (orange blocks) and a final linear activation layer (red block). The critic section simultaneously accepts HTC spectra produced by the generator, as well as true spectra, and the geometrical parameters as inputs, providing a scalar score as output. The critic model consists of a unique FC layer for each parallel input branch followed by a concatenation layer that combines information from both input branches into a single vector. Afterwards, two more FC layers and a final linear activation layer produce the score. In both the generator and critic sections, normalization of the input data is applied as a preprocessing step (grey blocks).</p>
<p>calculate the logarithm of the spectra, subtract the mean of both the input parameters and the spectra, and divide both the system parameters and the spectra by the standard deviation (this normalization operation is represented by grey blocks in Fig. 4). Finally in both the generator and the critic a final linear activation layer is added to ensure the output has the correct size (red blocks in Fig. 4).</p>
<p>We focus now on analyzing the evolution with the training steps of the loss functions of the discriminator and the generator (L C (G, C) and L G (G, C), respectively), as obtained by training the architecture displayed in Fig. 4 with the 6,561 h ω spectra described in Section 3. Monitoring the loss functions underlying our model during the training process is critical to understanding, diagnosing, and improving the whole generative adversarial framework studied in this work. The obtained numerical results are summarized in Fig. 5 (panel (a) corresponds to L C (G, C), while panel (b) displays the results for L G (G, C)). As seen in Fig. 5(a), L C (G, C) initially displays a transient behavior based on a fast drop in value followed by a sudden increase and an oscillation (around 10 3 training steps), until it reaches at stationary value at approximately 10 4 training steps. To get additional insight into the numerical origin of this evolution, the inset of Fig. 5(a) shows the dependence with the training steps of the three different terms forming L C (G, C), namely, E x∼p data (x) [C(x)], E z∼pz(z) [C(G(z))], and the gradient penalty, λ Ex ∼px [(||∇xC(x)|| 2 − 1) 2 ] (see Eq. 3). As observed, L C (G, C) is dominated by the expected values of the score value produced by the critic model for both the true training samples, (x), and the samples fabricated by the generator, G(z) (black and red lines, respectively, in inset of of Fig. 5(a) -the green line corresponds to the gradient penalty term). A remarkable aspect of the overall evolution of both expected value contributions to L C (G, C) is the fact that, despite their complicated transient behavior, they lock their difference in values after around 10 4 training steps. This in turn allows L C (G, C) to reach a stationary value (notice the difference in sign between both terms in Eq. 3), which marks the completion of the learning process by the critic. Figure 5(b) shows the results for the evolution of the generator loss function, L G (G, C), during the training process. The inset of Fig. 5(b) displays the corresponding evolution of the MAE contribution to L G (G, C) (i.e., the evolution of the first term in the r.h.s. of Eq. 4). As deduced by comparing the values reached by the MAE with those of L G (G, C), the generator loss function is dominated by the expected value term, E z∼pz(z) [C(G(z))] (second term in the r.h.s. of Eq. 4). As described above, for large values of the training step ( 10 4 ), this latter term necessarily follows its counterpart term in the critic loss function (so, as also pointed out above, their difference is maintained and a stationary value of L C (G, C) is reached). Therefore, since we do not expect L G (G, C) to reach a stationary value, MAE becomes the key magnitude to monitor the quality of the learning process of the generator (note that that this is also in accordance with the overall goal of the proposed application: creating artificial spectral samples indistinguishable from the real ones). Indeed, as shown in inset of Fig. 5(b), the MAE computed for the studied problem converges to values &lt; 0.2 for the larger training step values considered in our calculations.</p>
<p>Next, we proceed to quantify our model's proficiency in reproducing the spectral heat transfer coefficient (HTC) of the studied physical system, based on the model architecture illustrated in Fig. 4. For this analysis, our focus is on two distinctive metrics, concentrating on two different aspects of the spectra. The first, the per-point relative mean error (L point ), gauges the model's capability to accurately represent each point in the spectrum through a relative error analysis. The second, the integral relative mean error (L integ ), is predominantly influenced by the model's accuracy in reproducing the primary spectral characteristics, notably the resonances. Their respective definitions are as follows:
L point = 1 N m N i=1 m j=1 y i j −ŷ i j y i j ,(8)L integ = 1 N N i=1 (y i −ŷ i ) dω y i dω ,(9)
where N is the number of spectra, m the number of points in each spectrum, y i is a data example andŷ i is the corresponding generated example by the model. Note that we undo all pre-processing operations to perform these calculations, and that all integrals are performed over frequency points. We start our analysis by establishing a baseline outcome for comparison using a simple FFNN, comprised of five hidden layers, each hosting 200 neurons, characterized by a selu activation function, and a final linear activation layer. An analogous network has been previously shown to effectively model this identical physical system given ample data [43] -consequently, we anticipate that this particular network will serve as a suitable benchmark. Another important aspect is the apparent similarities between the generator and the FFNN. We aim at demonstrating that this specific simple design, given a sufficiently large dataset, can accurately replicate the spectral characteristics inherent to the type of systems under study, without resorting to any data augmentation technique. To accomplish this, we split the original dataset, which consists of 6,561 h ω spectra, allocating 80% for training set and the remaining 20% for the validation set. Our findings reveal that after 50,000 training iterations (epochs), and using the Adam optimization algorithm with a steady learning rate of 3×10 −4 , this simple neural network is proficient in replicating the system's spectra, reaching values of L point and L integ for the validation set of 3.61% and 1.45%, respectively (note that these results are comparable to those found in Ref. [43]).</p>
<p>Then we proceed to assess the CWGAN capability as numerical engine for spectral data augmentation. To do that, we follow a two-step approach. First, once the CWGAN has been trained, we use it to generate a number of additional spectra to include on the training set. Second, we retrain the above-described FFNN with this new data set to create an augmented FFNN. In this work, we generate a total of 10,000 new spectra for this data augmentation process, which are added to the training set. This particular amount of additional spectra was chosen after observing that our results are converged when adding to the original dataset a number of extra spectra in the range 5,000 -10,000. Note that, once the generator has been trained, this augmentation of the original dataset is computationally inexpensive.</p>
<p>Following this approach, first, we found that when using the training/validation split considered until now (80%-20%), the FFNN and the augmented FFNN have similar performance, both in terms of per-point relative mean accuracy and integral relative mean error (respective error values of approximately 3.6% and 1.5% are obtained for both models). This suggests that, as anticipated, using a sufficiently large dataset minimizes the impact of augmenting the original dataset, making any data augmentation barely noticeable. However, that conclusion changes when a different data scenario is considered. To modify the amount of data available to the models, we increasingly reduced the size of the training set by transferring part of it to the validation set, and retrained both the simple and augmented FFNNs from scratch (different values of the split training/validation were considered in the range 80%-20% -1%-99% -note that the data augmentation is done separately for each split ratio). Figure 6 summarizes the main results of this analysis using the two error metrics considered in this work (panels (a) and (b) correspond to L integ and L point , respectively -blue dots (lines) display the results for the simple FFNN, whereas black dots (lines) correspond to the CWGAN-augmented FFNN). As seen, both in terms of L integ and L point , the simple FFNN and the augmented FFNN have similar performance when the validation set fraction is smaller than approximately 70% (we can therefore ascribe that interval to a sufficiently large training set scenario). However, in the case of L integ (Fig.  6(a)), further increasing of the validation set fraction (i.e., further reduction of the training set size) leads to the augmented FFNN to perform increasingly better with training data reduction than the simple FFNN . As observed, values of L integ = 13.2% are achieved using a simple FFNN in the limit case of a validation set fraction of 99%, whereas an augmented FFNN reduces that error close to a than a half (L integ = 6.8%). The performance improvement is not so dramatic in the case of the per-point relative mean error (Fig. 6(b)), but we still observe slight improvements in the low-data scenario (for the extreme case of a validation set fraction of 99%, L point = 18.2% is obtained for the simple FFNN, while the augmented FFNN leads to L point = 16.8%).</p>
<p>Overall, the numerical results discussed above suggest that CWGAN data augmentation has intrinsic value beyond simply providing larger datasets. In particular, the reported results show that CWGAN data augmentation holds value in creating synthetic data more adaptable and resilient to scenarios with limited data. To the best of our knowledge, this finding is reported here for the first time. We believe it could contribute to the development of more efficient models that require smaller datasets, through the synthetic generation capabilities offered by generative adversarial frameworks.</p>
<p>Finally, to conclude the present analysis, we compare the performance as surrogate model to generate h ω spectra of the trained CWGAN with that of the simple and augmented FFNNs. This application arises from the conditioning modification we introduced in this work to conventional GANs (see Eq. 4 and the corresponding discussion in Section 2). This conditioning allows us to accurately keep track of the geometrical parameters associated to each spectra created by the generator of our CWGAN model, so once the generator is trained, it can be decoupled from the whole architecture and used as a standalone surrogate model. The obtained numerical results are also displayed in Fig. 6 (red points and red lines corresponds to the results of a CWGAN used as surrogate model). As observed, for the integral relative mean error ( Fig. 6(a)) the CWGAN displays worse performance than both the simple and the augmented FFNNs when the validation set fraction is below 70% approximately (i.e., the data regime that we previously labeled as of sufficiently large training set scenario for both the simple and the augmented FFNNs). However, in the low-data regime (values of the validation set fraction larger than 70%) we observe that the CWGAN surrogate gradually improves the results of a simple FFNN, until fully reproducing the improvement reached by an augmented FFNN in the extreme case of 99% validation set fraction. Regarding the per-point relative mean error ( Fig. 6(b)), the CWGAN surrogate displays a significantly worse performance in comparison to both the simple and the augmented FFNNs for the validation set fractions below 70%, but from that point on, it starts converging to the results of an augmented FFNN in the low-data regime. Ultimately, this comparison between the surrogate role of all considered models reinforces our previous conclusion that the CWGAN shows higher resilience to the reduction of training data than the simple FFNN, becoming also in this context an efficient architecture in low-data scenarios.</p>
<p>Conclusions</p>
<p>In this work we have studied the application of Generative Adversarial Networks (GANs) for synthetic spectral data generation, providing a solution to the data scarcity challenge pervasive in scientific domains where acquiring substantial spectral signals is of paramount importance. Our main focus has been an illustrative problem in the domain of near-field radiative heat transfer involving a multilayered hyperbolic metamaterial. We have analyzed the use of a Conditional Wasserstein GAN (CWGAN) for data augmentation and studied its influence on the predictive capacities of a feedforward neural network (FFNN). Our results reveal that generating spectral data effectively entails two main modifications to traditional GANs: firstly, incorporating Wasserstein GANs (WGANs) to prevent mode collapse, and secondly, conditioning these WGANs to secure accurate labels for the generated data. It is demonstrated that a basic FFNN, augmented with data yielded by a CWGAN, substantially improves its efficiency in scenarios where data is scarce, showing the inherent importance of CWGAN data augmentation beyond the simple expansion of datasets. Moreover, we present that CWGANs can function as a superior surrogate model when data is limited.</p>
<p>Overall, this work aims at contributing to the research area of generative AI algorithms' applicability beyond the conventional field of image generation. We believe that our findings contribute to the understanding and applicability of generative AI algorithms in data-constrained contexts and could stimulate further research work in the application of generative AI in a variety of scientific scenarios.</p>
<p>Figure 2 .
2(a) Schematic representation of the physical system under study. It features a multilayered hyperbolic metamaterial comprised of two identical structures, each alternating between metal (depicted in grey) and dielectric (blue) layers that extend infinitely. These two systems are separated by a distance d 0 = 10 nm. Every layer has a thickness d i , and both systems are backed by a metallic substrate. (b) Transmission of evanescent waves as a function of the frequency (ω) and the parallel wavevector (k), considering the case where there are 8 layers per system and both d i = d 0 = 10 nm. The transmission pattern exhibits a series of lines nearing unity, resulting from the hybridization of surface plasmon polaritons (SPPs) within the metallic layers. (c) Sample spectra of heat transfer coefficients, h ω , for several representative combinations of layer thicknesses, which are specified in</p>
<p>Figure 3 .
3Reproduction of the training set using (a) a CGAN, showing in grey the training set and in green the CGAN reproduction of the set; and (b) the CWGAN, using the same color scheme. The results were obtained by applying PCA to the corresponding spectra, and plotting the first two components, z 1 and z 2 (with a &gt; 90% reproduction rate).</p>
<p>Figure 4 .
4Schematics of the architecture used for the Conditional Wasserstein Generative Adversarial Network (CWGAN) employed in this work. Both the generator and critic sections are shown in the left and right parts, respectively, of the figure.</p>
<p>Figure 5 .
5Evolution of the critic and generator loss functions during the training process (L C (G, C) and L G (G, C), panels (a) and (b), respectively). Inset of panel (a) shows dependence on the training steps of three contributions of L C (G, C) (see Eq. 3 in the main text). Inset of panel (b) corresponds to evolution with the training step of the mean absolute error (MAE) associated to the synthetic spectra produced by the generator. Notice that in our framework the critic evaluates the loss 5 times per generator loss evaluation.</p>
<p>Figure 6 .
6(a) Evolution of the integral relative mean error for the validation set as a function of the validation set fraction of the total data. (b) Same as in (a), but now using the per-point relative mean error metric. In both (a) and (b), blue and black dots (lines) show the results for a simple FFNN and a CWGAN-augmented FFNN, respectively, whereas red dots (lines) correspond to the results obtained by using the CWGAN as surrogate model.</p>
<p>Table 1 .
1Thicknesses combinations of the representative samples of the spectral heat transfer coefficient, h ω , shown in</p>
<p>T Mitchell, Machine Learning. McGraw-HillMitchell T 1997 Machine Learning (McGraw-Hill)</p>
<p>. Y Lecun, Y Bengio, G Hinton, Nature. 521LeCun Y, Bengio Y and Hinton G 2015 Nature 521 436-444</p>
<p>I Goodfellow, Y Bengio, A Courville, Deep Learning. MIT PressGoodfellow I, Bengio Y and Courville A 2016 Deep Learning (MIT Press)</p>
<p>. C Aggarwal, Neural Networks and Deep Learning. SpringerAggarwal C 2018 Neural Networks and Deep Learning (Cham: Springer)</p>
<p>. A Krizhevsky, I Sutskever, G Hinton, Advances in Neural Information Processing Systems. 25Krizhevsky A, Sutskever I and Hinton G 2012 Advances in Neural Information Processing Systems 25 1097-1105</p>
<p>. G Hinton, L Deng, D Yu, G Dahl, A R Mohamed, N Jaitly, A Senior, V Vanhoucke, P Nguyen, T Sainath, B Kingsbury, IEEE Signal Process. Mag. 29Hinton G, Deng L, Yu D, Dahl G, Mohamed A R, Jaitly N, Senior A, Vanhoucke V, Nguyen P, Sainath T and Kingsbury B 2012 IEEE Signal Process. Mag. 29 82-97</p>
<p>. K Cho, B Van Merrienboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, 1406.1078Cho K, van Merrienboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H and Bengio Y 2014 arXiv (Preprint 1406.1078)</p>
<p>. S Shalev-Shwartz, S Shammah, A Shashua, 1610.03295Shalev-Shwartz S, Shammah S and Shashua A 2016 arXiv (Preprint 1610.03295)</p>
<p>. G Carleo, I Cirac, K Cranmer, L Daudet, M Schuld, N Tishby, L Vogt-Maranto, L Zdeborová, Rev. Mod. Phys. 91445002Carleo G, Cirac I, Cranmer K, Daudet L, Schuld M, Tishby N, Vogt-Maranto L and Zdeborová L 2019 Rev. Mod. Phys. 91(4) 045002</p>
<p>. G Carleo, M Troyer, Science. 355Carleo G and Troyer M 2017 Science 355 602-606</p>
<p>. L Wang, Phys. Rev. B. 94195105Wang L 2016 Phys. Rev. B 94(19) 195105</p>
<p>. J Carrasquilla, R G Melko, Nature Physics. 13Carrasquilla J and Melko R G 2017 Nature Physics 13 431-434</p>
<p>. E P L Van Nieuwenburg, Y Liu, S D Huber, Nature Physics. 13van Nieuwenburg E P L, Liu Y H and Huber S D 2017 Nature Physics 13 435-439</p>
<p>. D Wu, Wang L Zhang, P , Phys. Rev. Lett. 122880602Wu D, Wang L and Zhang P 2019 Phys. Rev. Lett. 122(8) 080602</p>
<p>. V Dunjko, H J Briegel, Physics. 8174001Dunjko V and Briegel H J 2018 Reports on Progress in Physics 81 074001</p>
<p>. S R Green, C Simpson, J Gair, Phys. Rev. D. 10210104057Green S R, Simpson C and Gair J 2020 Phys. Rev. D 102(10) 104057</p>
<p>. J Peurifoy, Y Shen, L Jing, Y Yang, F Cano-Renteria, B G Delacy, J D Joannopoulos, Tegmark M Soljačić, Science Advances. 44206Peurifoy J, Shen Y, Jing L, Yang Y, Cano-Renteria F, DeLacy B G, Joannopoulos J D, Tegmark M and Soljačić M 2018 Science Advances 4 eaar4206</p>
<p>. S So, T Badloe, J Noh, J Bravo-Abad, J Rho, Nanophotonics. 9So S, Badloe T, Noh J, Bravo-Abad J and Rho J 2020 Nanophotonics 9 1041-1057</p>
<p>. J Jiang, Chen M Fan, J A , Nature Reviews Materials. 6Jiang J, Chen M and Fan J A 2021 Nature Reviews Materials 6 679-700</p>
<p>. M Frising, J Bravo-Abad, F Prins, Machine Learning: Science and Technology. 4Frising M, Bravo-Abad J and Prins F 2023 Machine Learning: Science and Technology 4 02LT02</p>
<p>. K T Butler, D W Davies, H Cartwright, O Isayev, A Walsh, Nature. 559Butler K T, Davies D W, Cartwright H, Isayev O and Walsh A 2018 Nature 559 547-555</p>
<p>. R Gómez-Bombarelli, J N Wei, D Duvenaud, J Hernández-Lobato, B Sánchez-Lengeling, D Sheberla, J Aguilera-Iparraguirre, T D Hirzel, R P Adams, Aspuru-Guzik , ACS Central Science. 4Gómez-Bombarelli R, Wei J N, Duvenaud D, Hernández-Lobato J, Sánchez-Lengeling B, Sheberla D, Aguilera-Iparraguirre J, Hirzel T D, Adams R P and Aspuru-Guzik A 2018 ACS Central Science 4 268-276</p>
<p>. L Zhang, J Han, H Wang, R Car, E , Phys. Rev. Lett. 12014143001Zhang L, Han J, Wang H, Car R and E W 2018 Phys. Rev. Lett. 120(14) 143001</p>
<p>. C W Coley, R Barzilay, T S Jaakkola, W Green, K F Jensen, ACS Central Science. 3Coley C W, Barzilay R, Jaakkola T S, Green W H and Jensen K F 2017 ACS Central Science 3 434-443</p>
<p>. M Rupp, A Tkatchenko, K Müller, Von Lilienfeld, O , Phys. Rev. Lett. 108558301Rupp M, Tkatchenko A, Müller K R and von Lilienfeld O A 2012 Phys. Rev. Lett. 108(5) 058301</p>
<p>. T Ching, D S Himmelstein, B K Beaulieu-Jones, A A Kalinin, B T Do, G P Way, E Ferrero, P M Agapow, M Zietz, M M Hoffman, W Xie, G L Rosen, B J Lengerich, J Israeli, J Lanchantin, S Woloszynek, A E Carpenter, A Shrikumar, J Xu, E M Cofer, C A Lavender, S C Turaga, A M Alexandari, Z Lu, D J Harris, D Decaprio, Y Qi, A Kundaje, Y Peng, L K Wiley, M H S Segler, S M Boca, S J Swamidass, A Huang, A Gitter, C S Greene, Journal of The Royal Society Interface. 1520170387Ching T, Himmelstein D S, Beaulieu-Jones B K, Kalinin A A, Do B T, Way G P, Ferrero E, Agapow P M, Zietz M, Hoffman M M, Xie W, Rosen G L, Lengerich B J, Israeli J, Lanchantin J, Woloszynek S, Carpenter A E, Shrikumar A, Xu J, Cofer E M, Lavender C A, Turaga S C, Alexandari A M, Lu Z, Harris D J, DeCaprio D, Qi Y, Kundaje A, Peng Y, Wiley L K, Segler M H S, Boca S M, Swamidass S J, Huang A, Gitter A and Greene C S 2018 Journal of The Royal Society Interface 15 20170387</p>
<p>. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, 2672-2680Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A and Bengio Y 2014 NIPS 2672-2680</p>
<p>T Salimans, I Goodfellow, W Zaremba, V Cheung, A Radford, X Chen, Proceedings of the 30th International Conference on Neural Information Processing Systems. the 30th International Conference on Neural Information Processing Systems16Salimans T, Goodfellow I, Zaremba W, Cheung V, Radford A and Chen X 2016 NIPS'16: Proceedings of the 30th International Conference on Neural Information Processing Systems 29 2234-2242</p>
<p>. M Paganini, L De Oliveira, B Nachman, Phys. Rev. D. 97114021Paganini M, de Oliveira L and Nachman B 2018 Phys. Rev. D 97(1) 014021</p>
<p>. J Carrasquilla, G Torlai, R Melko, L Aolita, Nature Machine Intelligence. 1Carrasquilla J, Torlai G, Melko R G and Aolita L 2019 Nature Machine Intelligence 1 155-161</p>
<p>. R Koch, J Lado, Phys. Rev. Res. 4333223Koch R and Lado J 2022 Phys. Rev. Res. 4(3) 033223</p>
<p>. J Carracedo-Cosme, R Pérez, arXiv (Preprint 205.00447Carracedo-Cosme J and Pérez R 2022 arXiv (Preprint 205.00447)</p>
<p>. S So, J Rho, Nanophotonics. 8So S and Rho J 2019 Nanophotonics 8 1255-1261</p>
<p>. T Christensen, C Loh, S Picek, D Jakobović, L Jing, S Fisher, V Ceperic, J Joannopoulos, M Soljačić, Nanophotonics. 9Christensen T, Loh C, Picek S, Jakobović D, Jing L, Fisher S, Ceperic V, Joannopoulos J D and Soljačić M 2020 Nanophotonics 9 4183-4192</p>
<p>. A C Rodríguez, T Kacprzak, A Lucchi, Amara A Sgier, R Fluri, J Hofmann, T Réfrégier, A , Computational Astrophysics and Cosmology. 54Rodríguez A C, Kacprzak T, Lucchi A, Amara A, Sgier R, Fluri J, Hofmann T and Réfrégier A 2018 Computational Astrophysics and Cosmology 5 4</p>
<p>. D M Anstine, O Isayev, Journal of the American Chemical Society. 145Anstine D M and Isayev O 2023 Journal of the American Chemical Society 145 8736-8750</p>
<p>. B Sanchez-Lengeling, Aspuru-Guzik , Science. 361Sanchez-Lengeling B and Aspuru-Guzik A 2018 Science 361 360-365</p>
<p>. D Repecka, V Jauniskis, L Karpus, E Rembeza, I Rokaitis, J Zrimec, S Poviloniene, A Laurynenas, S Viknander, W Abuajwa, O Savolainen, R Meskys, M K M Engqvist, A Zelezniak, Nature Machine Intelligence. 3Repecka D, Jauniskis V, Karpus L, Rembeza E, Rokaitis I, Zrimec J, Poviloniene S, Laurynenas A, Viknander S, Abuajwa W, Savolainen O, Meskys R, Engqvist M K M and Zelezniak A 2021 Nature Machine Intelligence 3 324-333</p>
<p>. P Goldsborough, N Pawlowski, J C Caicedo, S Singh, A E Carpenter, Preprint 227645Goldsborough P, Pawlowski N, Caicedo J C, Singh S and Carpenter A E 2017 bioRxiv (Preprint 227645)</p>
<p>. M Arjovsky, S Chintala, L Bottou, 1701.07875Arjovsky M, Chintala S and Bottou L 2017 arXiv (Preprint 1701.07875)</p>
<p>. I Gulrajani, F Ahmed, M Arjovsky, V Dumoulin, A Courville, 1704. 00028Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V and Courville A 2017 arXiv (Preprint 1704. 00028)</p>
<p>. M Mirza, S Osindero, arXiv (Preprint 1411.1784Mirza M and Osindero S 2014 arXiv (Preprint 1411.1784)</p>
<p>. J J García-Esteban, J Bravo-Abad, J C Cuevas, Phys. Rev. Appl. 1664006García-Esteban J J, Bravo-Abad J and Cuevas J C 2021 Phys. Rev. Appl. 16 064006</p>
<p>. D Polder, M Van Hove, Phys. Rev. B. 4Polder D and Van Hove M 1971 Phys. Rev. B 4 3303-3314</p>
<p>. B Song, A Fiorino, E Meyhofer, P Reddy, AIP Adv. 553503Song B, Fiorino A, Meyhofer E and Reddy P 2015 AIP Adv. 5 053503</p>
<p>. J C Cuevas, García-Vidal F J , ACS Photonics. 5Cuevas J C and García-Vidal F J 2018 ACS Photonics 5 3896-3915</p>
<p>. S A Biehs, R Messina, P S Venkataram, A W Rodriguez, J Cuevas, Ben-Abdallah , P , Rev. Mod. Phys. 9325009Biehs S A, Messina R, Venkataram P S, Rodriguez A W, Cuevas J C and Ben-Abdallah P 2021 Rev. Mod. Phys. 93 025009</p>
<p>. Y Guo, C L Cortes, Molesky S Jacob, Z , Appl. Phys. Lett. 101131106Guo Y, Cortes C L, Molesky S and Jacob Z 2012 Appl. Phys. Lett. 101 131106</p>
<p>. S A Biehs, M Tschikin, P Ben-Abdallah, Phys. Rev. Lett. 109104301Biehs S A, Tschikin M and Ben-Abdallah P 2012 Phys. Rev. Lett. 109 104301</p>
<p>. Y Guo, Z Jacob, Opt. Express. 21Guo Y and Jacob Z 2013 Opt. Express 21 15014-15019</p>
<p>. S A Biehs, M Tschikin, R Messina, P Ben-Abdallah, Appl. Phys. Lett. 102131106Biehs S A, Tschikin M, Messina R and Ben-Abdallah P 2013 Appl. Phys. Lett. 102 131106</p>
<p>. T J Bright, X Liu, Z M Zhang, Opt. Express. 22Bright T J, Liu X L and Zhang Z M 2014 Opt. Express 22 A1112-A1127</p>
<p>. O D Miller, S G Johnson, A W Rodriguez, Phys. Rev. Lett. 112157402Miller O D, Johnson S G and Rodriguez A W 2014 Phys. Rev. Lett. 112 157402</p>
<p>. S Biehs, P Ben-Abdallah, Zeitschrift für Naturforschung A. 72Biehs S A and Ben-Abdallah P 2017 Zeitschrift für Naturforschung A 72 115-127</p>
<p>. H Iizuka, S Fan, Phys. Rev. Lett. 12063901Iizuka H and Fan S 2018 Phys. Rev. Lett. 120 063901</p>
<p>. J Song, Q Cheng, L Lu, B Li, K Zhou, B Zhang, Z Luo, X Zhou, Phys. Rev. Appl. 1324054Song J, Cheng Q, Lu L, Li B, Zhou K, Zhang B, Luo Z and Zhou X 2020 Phys. Rev. Appl. 13 024054</p>
<p>. E Moncada-Villa, J C Cuevas, Phys. Rev. B. 10375432Moncada-Villa E and Cuevas J C 2021 Phys. Rev. B 103 075432</p>
<p>S Rytov, Theory of Electric Fluctuations and Thermal Radiation. Bedford: Air Force Cambrige Research CenterRytov S 1953 Theory of Electric Fluctuations and Thermal Radiation (Bedford: Air Force Cambrige Research Center)</p>
<p>. S Rytov, Y Kravtstov, V Tatarskii, Principles of Statistical Radiophysics. 3Springer-VerlagRytov S, Kravtstov Y and Tatarskii V 1989 Principles of Statistical Radiophysics vol 3 (Berlin: Springer-Verlag)</p>
<p>. S Basu, Z Zhang, C J Fu, Int. J. Energy Res. 33Basu S, Zhang Z M and Fu C J 2009 Int. J. Energy Res. 33 1203-1232</p>
<p>. B Caballero, A García-Martín, J C Cuevas, Phys. Rev. B. 85245103Caballero B, García-Martín A and Cuevas J C 2012 Phys. Rev. B 85 245103</p>
<p>. I Jolliffe, Principal Component Analysis. SpringerJolliffe I 2002 Principal Component Analysis (Springer)</p>
<p>. G James, D Witten, T Hastie, R Tibshirani, An Introduction to Statistical Learning. SpringerJames G, Witten D, Hastie T and Tibshirani R 2013 An Introduction to Statistical Learning (Springer)</p>
<p>. P Isola, J Y Zhu, T Zhou, A A Efros, 1611.07004Isola P, Zhu J Y, Zhou T and Efros A A 2016 arXiv (Preprint 1611.07004)</p>
<p>. S Ahmed, Sánchez Muñoz, C , Nori F Kockum, A F , Phys. Rev. Lett. 127140502Ahmed S, Sánchez Muñoz C, Nori F and Kockum A F 2021 Phys. Rev. Lett. 127 140502</p>            </div>
        </div>

    </div>
</body>
</html>
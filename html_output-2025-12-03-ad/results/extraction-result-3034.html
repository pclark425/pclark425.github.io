<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3034 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3034</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3034</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-261705614</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.06979v3.pdf" target="_blank">Auto-Regressive Next-Token Predictors are Universal Learners</a></p>
                <p><strong>Paper Abstract:</strong> Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our results demonstrate that the power of today's LLMs can be attributed, to a great extent, to the auto-regressive next-token training scheme, and not necessarily to a particular choice of architecture.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3034.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3034.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear AR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear Auto-Regressive Next-Token Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of next-token predictors whose next-token scores are linear in token embeddings; the paper shows theoretically that with Chain-of-Thought (CoT) supervision such linear AR predictors can implement linear-threshold circuits and simulate Turing machines, and experimentally a linear instantiation yields non-trivial language generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Linear AR predictor (paper's experimental linear model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Theoretical class: linear next-token predictors h_W mapping concatenated token embeddings to logits via a linear map (H_Lin). Experimental instantiation: embedding dimension d=256, a masked linear layer across context length T=64, output embedding; ~162M active parameters; trained with cross-entropy/teacher forcing on TinyStories.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Theoretical capability: parity functions and arbitrary Turing-computable functions (hence general arithmetic) when given CoT sequences; Experimental: no arithmetic experiment for this linear model (language generation only).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Chain-of-Thought supervision supplies intermediate tokens that encode internal computation; argmax / zero-temperature sampling at inference acts as an explicit non-linearity on top of linear scoring; intermediate tokens correspond to gate outputs in a linear-threshold circuit representation, enabling composition of non-linear computations via purely linear per-step predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Formal proofs (Theorems 3.6, 3.8, 3.9, 3.11) showing: (a) linear AR functions can implement linear-threshold circuits (Theorem 3.8); (b) consequently they can simulate any Turing-computable function with polynomial blow-up (Corollary 3.9); (c) parity can be computed with length complexity O(log n) (Theorem 3.11). The paper also argues (and cites) that argmax sampling provides the necessary non-linearity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No mechanistic probes/feature-attribution analyses are provided to demonstrate the internal representations in trained experimental linear models; practical limitations are noted (need for long CoT sequences and specialized datasets), and the experimental linear model is weaker than full transformers on broad language tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Chain-of-Thought / scratchpad supervision and teacher-forcing during training; use of zero-temperature/argmax sampling at inference (discussed as inducing non-linearity).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Theoretically, CoT supervision allows linear AR predictors to compute non-linear functions and implement algorithms; empirically, the linear model produced non-trivial coherent text on TinyStories when trained autoregressively with masked linear layers and next-token supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical: existence proofs that linear AR can compute any Turing-computable f with datasets of token-sequences poly(T(n)); parity class computed with length complexity O(log n). Experimental (language): linear model (~162M active params) produced coherent TinyStories outputs; automatic grammar-no-error rate ~64% (LanguageTool) and human GPT-4 ratings (grammar ~6.3±2.0, creativity ~6.2±1.8 etc.). No arithmetic accuracies reported for the linear model.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Requires explicit supervised intermediate computations (CoT) and potentially long token sequences (length complexity), which may be impractical to obtain; the experimental linear model still underperforms large transformers on general language quality; parameter-sharing differences from the idealized model create implementation gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Theoretical equivalence to Turing machines / symbolic computation is claimed (via simulation by linear-threshold circuits encoded as token sequences); no empirical human-vs-model comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Regressive Next-Token Predictors are Universal Learners', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3034.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3034.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLP-775M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shallow Multi-Layer Perceptron (775M parameters) for 4-digit multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 4-layer MLP without attention trained autoregressively with extensive Chain-of-Thought supervision and a custom tokenization; achieves near-transformer-level performance on 4-digit multiplication by learning an algorithmic, stepwise multiplication procedure encoded as intermediate tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MLP-775M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Four-layer MLP: token embedding to d=128; masked linear+ReLU mapping from d×T to d×T with context length T=307; per-token linear+ReLU; output embedding to token space; total ~775M active parameters. Trained from scratch ~17 hours on a single A100 over ~100M sequences (~307M tokens) sampled uniformly from training pairs of 4-digit numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit arithmetic: 4-digit × 4-digit multiplication; evaluated as exact-match on full product and per-digit accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Algorithmic/stepwise computation learned via supervised intermediate CoT tokens (the training sequences 'unfold' the multiplication algorithm); a custom tokenization includes single digits, signs (×,+,=) and pair tokens (e.g., '3×5') enabling the model to directly learn single-digit multiplication mappings that compose into the multi-digit algorithm; argmax/zero-temperature sampling is noted as the inference non-linearity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High empirical accuracy (Exact-match: 96.9%; Per-digit: 99.5% on 1000 validation examples). Training sequences explicitly include unfolded algorithmic intermediate steps; qualitative model outputs show intermediate steps in the multiplication process. Comparison: matches Goat-7B (reported 96.9% exact / 99.2% per-digit) and outperforms GPT-3.5/GPT-4 on the same evaluation set.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The paper does not present interpretability probes that conclusively demonstrate the model learned a general algorithm vs. memorizing many examples; the authors note that careful tokenization and CoT are critical, implying dependence on dataset design. No ablation isolating what part (tokenization vs CoT length vs model size) is the dominant cause of success.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Chain-of-Thought supervision with more intermediate steps than prior work (unfolded multiplication), custom tokenization (single digits, signs, digit-pair tokens), zero-padding for uniform length, teacher-forcing during training.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantially improved arithmetic performance: the MLP trained with extensive CoT and custom tokenization reached 96.9% exact-match and 99.5% per-digit accuracy on 4-digit multiplication, comparable to a 7B transformer fine-tuned with CoT, demonstrating that a simple architecture can learn algorithmic arithmetic when given intermediate supervision and suitable tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match accuracy: 96.9%; Per-digit accuracy: 99.5% (evaluated on 1000 validation examples). Training: 100M sequences (~307M tokens), 17 hours on one A100 GPU. Comparative reported baselines: Goat-7B 96.9% / 99.2% (Liu & Low 2023); GPT-3.5 1.2% / 61.9%; GPT-4 5.3% / 61.8% (per numbers used in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Paper does not list systematic failure modes of the trained MLP beyond general caveats: reliance on specialized CoT data and tokenization, potential lack of out-of-distribution generalization, and absence of fine-grained mechanistic analyses to rule out memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Compared to transformer baselines (Goat-7B, GPT-3.5, GPT-4): matches the fine-tuned transformer Goat-7B and strongly outperforms baseline GPTs on this task; functionally analogous to symbolic (schoolbook) multiplication due to the unfolded algorithmic supervision, but no human experimental comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Regressive Next-Token Predictors are Universal Learners', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3034.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3034.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTs & Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5, GPT-4 and Goat-7B transformer baselines (comparative references)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced transformer models used as comparison baselines for arithmetic performance: GPT-3.5 and GPT-4 (general-purpose LLMs) and Goat-7B (a 7B-parameter transformer fine-tuned with Chain-of-Thought on arithmetic tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4, Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5/GPT-4: large autoregressive transformers from OpenAI (exact sizes not specified in this paper). Goat-7B: a 7B-parameter transformer (LLaMA-based) fine-tuned with Chain-of-Thought techniques as reported by Liu & Low (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>4-digit multiplication (multi-digit arithmetic) used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Goat-7B: fine-tuned with CoT to learn an algorithmic pipeline; GPTs: typically rely on general pretraining and prompting; when not fine-tuned with explicit CoT their arithmetic performance can be poor, suggesting reliance on pattern matching/memorization rather than learned explicit algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Liu & Low report Goat-7B achieves high arithmetic accuracy after CoT fine-tuning (reported 96.9% exact / 99.2% per-digit). In this paper GPT-3.5 and GPT-4 evaluated on the same 4-digit examples performed poorly (reported 1.2%/61.9% and 5.3%/61.8% respectively), indicating low out-of-the-box arithmetic exactness without specialized CoT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The low performance of GPT-4/GPT-3.5 on exact-match arithmetic suggests that scale and general pretraining alone do not guarantee reliable arithmetic exactness; the paper does not provide internal analyses of GPT models to dispute or confirm algorithmic vs memorization behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>For Goat-7B: fine-tuning with Chain-of-Thought demonstrations; for GPTs: standard prompting or non-specialized prompting in these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Fine-tuning with CoT (Goat) dramatically improves arithmetic exactness compared to general GPTs; evidence here supports that CoT fine-tuning is the key intervention enabling high-accuracy arithmetic in transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported comparative numbers used in paper: Goat-7B 96.9% exact / 99.2% per-digit (Liu & Low 2023); GPT-3.5 1.2% exact / 61.9% per-digit (evaluated by authors on same set); GPT-4 5.3% exact / 61.8% per-digit (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>GPTs without CoT fine-tuning produce incorrect full products and fail exact-match on nearly all 4-digit multiplication cases; formatting and digit-level errors are common.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Goat's CoT-trained behavior resembles learning a symbolic multiplication pipeline; no human baseline comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Regressive Next-Token Predictors are Universal Learners', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3034.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3034.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought / Scratchpad Intermediate Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training/prompting paradigm that includes explicit intermediate reasoning steps (scratchpad) as tokens so that AR next-token predictors can learn and use internal multi-step computations to produce final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought / Scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Technique: include intermediate tokens (sub-computations, gate outputs, partial results) in training sequences (or elicit them via prompting) so each intermediate token is both predicted and later used as input; used with teacher forcing for training and autoregressive inference at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to enable multi-step arithmetic tasks such as multi-digit multiplication and parity computation; generally applicable to algorithmic arithmetic and mathematical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>By exposing intermediate computation as tokens, the AR learner can learn to compute and compose those steps; this converts a hard input→output mapping into a sequence prediction problem where each step is a learnable local function, reducing the effective complexity of learning certain functions (introducing the notion of length complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper provides formal results (Theorems 3.3, 3.6, 3.8, 3.11, 3.12) showing (a) AR learnability with CoT can recover functions computed by underlying AR predictors, (b) linear AR models can implement linear-threshold circuits via CoT, (c) parities can be learned with O(log n) intermediate tokens, and (d) empirical success of MLP on 4-digit multiplication when trained with unfolded algorithmic CoT and custom tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT requires producing/collecting intermediate-step supervision per example which can be costly; emergent CoT behavior in models trained on generic corpora without explicit CoT is outside scope of this paper; CoT length may be large and impractical for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised CoT demonstrations in training data; prompt-based CoT elicitation; scratchpad-style intermediate token recording; teacher forcing during training.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables simple linear and MLP architectures to learn complex arithmetic and parity functions that are otherwise hard or impossible under classical supervised training; empirically bridges gap between small models and much larger fine-tuned transformers on specific arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Enables parity learning with length complexity O(log n) (theoretical). Empirically enabled MLP-775M to reach 96.9% exact / 99.5% per-digit on 4-digit multiplication when trained with unfolded CoT and tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Requires curated CoT sequences, which can be long (length complexity cost) and dataset-generation intensive; generalization beyond distribution of CoT examples is not fully characterized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>CoT/scratchpad is explicitly analogous to human written scratchwork and to exposing intermediate states of symbolic algorithms; the paper frames CoT as a mechanism that enables AR predictors to emulate symbolic/Turing computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Regressive Next-Token Predictors are Universal Learners', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Fine-tuned llama outperforms gpt-4 on arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>Sub-task decomposition enables learning in sequence to sequence tasks <em>(Rating: 2)</em></li>
                <li>Tinystories: How small can language models be and still speak coherent english? <em>(Rating: 1)</em></li>
                <li>Towards revealing the mystery behind chain of thought: a theoretical perspective <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3034",
    "paper_id": "paper-261705614",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Linear AR",
            "name_full": "Linear Auto-Regressive Next-Token Predictor",
            "brief_description": "A class of next-token predictors whose next-token scores are linear in token embeddings; the paper shows theoretically that with Chain-of-Thought (CoT) supervision such linear AR predictors can implement linear-threshold circuits and simulate Turing machines, and experimentally a linear instantiation yields non-trivial language generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Linear AR predictor (paper's experimental linear model)",
            "model_description": "Theoretical class: linear next-token predictors h_W mapping concatenated token embeddings to logits via a linear map (H_Lin). Experimental instantiation: embedding dimension d=256, a masked linear layer across context length T=64, output embedding; ~162M active parameters; trained with cross-entropy/teacher forcing on TinyStories.",
            "arithmetic_task_type": "Theoretical capability: parity functions and arbitrary Turing-computable functions (hence general arithmetic) when given CoT sequences; Experimental: no arithmetic experiment for this linear model (language generation only).",
            "reported_mechanism": "Chain-of-Thought supervision supplies intermediate tokens that encode internal computation; argmax / zero-temperature sampling at inference acts as an explicit non-linearity on top of linear scoring; intermediate tokens correspond to gate outputs in a linear-threshold circuit representation, enabling composition of non-linear computations via purely linear per-step predictors.",
            "evidence_for_mechanism": "Formal proofs (Theorems 3.6, 3.8, 3.9, 3.11) showing: (a) linear AR functions can implement linear-threshold circuits (Theorem 3.8); (b) consequently they can simulate any Turing-computable function with polynomial blow-up (Corollary 3.9); (c) parity can be computed with length complexity O(log n) (Theorem 3.11). The paper also argues (and cites) that argmax sampling provides the necessary non-linearity.",
            "evidence_against_mechanism": "No mechanistic probes/feature-attribution analyses are provided to demonstrate the internal representations in trained experimental linear models; practical limitations are noted (need for long CoT sequences and specialized datasets), and the experimental linear model is weaker than full transformers on broad language tasks.",
            "intervention_type": "Chain-of-Thought / scratchpad supervision and teacher-forcing during training; use of zero-temperature/argmax sampling at inference (discussed as inducing non-linearity).",
            "effect_of_intervention": "Theoretically, CoT supervision allows linear AR predictors to compute non-linear functions and implement algorithms; empirically, the linear model produced non-trivial coherent text on TinyStories when trained autoregressively with masked linear layers and next-token supervision.",
            "performance_metrics": "Theoretical: existence proofs that linear AR can compute any Turing-computable f with datasets of token-sequences poly(T(n)); parity class computed with length complexity O(log n). Experimental (language): linear model (~162M active params) produced coherent TinyStories outputs; automatic grammar-no-error rate ~64% (LanguageTool) and human GPT-4 ratings (grammar ~6.3±2.0, creativity ~6.2±1.8 etc.). No arithmetic accuracies reported for the linear model.",
            "notable_failure_modes": "Requires explicit supervised intermediate computations (CoT) and potentially long token sequences (length complexity), which may be impractical to obtain; the experimental linear model still underperforms large transformers on general language quality; parameter-sharing differences from the idealized model create implementation gaps.",
            "comparison_to_humans_or_symbolic": "Theoretical equivalence to Turing machines / symbolic computation is claimed (via simulation by linear-threshold circuits encoded as token sequences); no empirical human-vs-model comparisons provided.",
            "uuid": "e3034.0",
            "source_info": {
                "paper_title": "Auto-Regressive Next-Token Predictors are Universal Learners",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "MLP-775M",
            "name_full": "Shallow Multi-Layer Perceptron (775M parameters) for 4-digit multiplication",
            "brief_description": "A 4-layer MLP without attention trained autoregressively with extensive Chain-of-Thought supervision and a custom tokenization; achieves near-transformer-level performance on 4-digit multiplication by learning an algorithmic, stepwise multiplication procedure encoded as intermediate tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MLP-775M",
            "model_description": "Four-layer MLP: token embedding to d=128; masked linear+ReLU mapping from d×T to d×T with context length T=307; per-token linear+ReLU; output embedding to token space; total ~775M active parameters. Trained from scratch ~17 hours on a single A100 over ~100M sequences (~307M tokens) sampled uniformly from training pairs of 4-digit numbers.",
            "arithmetic_task_type": "Multi-digit arithmetic: 4-digit × 4-digit multiplication; evaluated as exact-match on full product and per-digit accuracy.",
            "reported_mechanism": "Algorithmic/stepwise computation learned via supervised intermediate CoT tokens (the training sequences 'unfold' the multiplication algorithm); a custom tokenization includes single digits, signs (×,+,=) and pair tokens (e.g., '3×5') enabling the model to directly learn single-digit multiplication mappings that compose into the multi-digit algorithm; argmax/zero-temperature sampling is noted as the inference non-linearity.",
            "evidence_for_mechanism": "High empirical accuracy (Exact-match: 96.9%; Per-digit: 99.5% on 1000 validation examples). Training sequences explicitly include unfolded algorithmic intermediate steps; qualitative model outputs show intermediate steps in the multiplication process. Comparison: matches Goat-7B (reported 96.9% exact / 99.2% per-digit) and outperforms GPT-3.5/GPT-4 on the same evaluation set.",
            "evidence_against_mechanism": "The paper does not present interpretability probes that conclusively demonstrate the model learned a general algorithm vs. memorizing many examples; the authors note that careful tokenization and CoT are critical, implying dependence on dataset design. No ablation isolating what part (tokenization vs CoT length vs model size) is the dominant cause of success.",
            "intervention_type": "Chain-of-Thought supervision with more intermediate steps than prior work (unfolded multiplication), custom tokenization (single digits, signs, digit-pair tokens), zero-padding for uniform length, teacher-forcing during training.",
            "effect_of_intervention": "Substantially improved arithmetic performance: the MLP trained with extensive CoT and custom tokenization reached 96.9% exact-match and 99.5% per-digit accuracy on 4-digit multiplication, comparable to a 7B transformer fine-tuned with CoT, demonstrating that a simple architecture can learn algorithmic arithmetic when given intermediate supervision and suitable tokenization.",
            "performance_metrics": "Exact-match accuracy: 96.9%; Per-digit accuracy: 99.5% (evaluated on 1000 validation examples). Training: 100M sequences (~307M tokens), 17 hours on one A100 GPU. Comparative reported baselines: Goat-7B 96.9% / 99.2% (Liu & Low 2023); GPT-3.5 1.2% / 61.9%; GPT-4 5.3% / 61.8% (per numbers used in paper).",
            "notable_failure_modes": "Paper does not list systematic failure modes of the trained MLP beyond general caveats: reliance on specialized CoT data and tokenization, potential lack of out-of-distribution generalization, and absence of fine-grained mechanistic analyses to rule out memorization.",
            "comparison_to_humans_or_symbolic": "Compared to transformer baselines (Goat-7B, GPT-3.5, GPT-4): matches the fine-tuned transformer Goat-7B and strongly outperforms baseline GPTs on this task; functionally analogous to symbolic (schoolbook) multiplication due to the unfolded algorithmic supervision, but no human experimental comparison provided.",
            "uuid": "e3034.1",
            "source_info": {
                "paper_title": "Auto-Regressive Next-Token Predictors are Universal Learners",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPTs & Goat-7B",
            "name_full": "GPT-3.5, GPT-4 and Goat-7B transformer baselines (comparative references)",
            "brief_description": "Referenced transformer models used as comparison baselines for arithmetic performance: GPT-3.5 and GPT-4 (general-purpose LLMs) and Goat-7B (a 7B-parameter transformer fine-tuned with Chain-of-Thought on arithmetic tasks).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, GPT-4, Goat-7B",
            "model_description": "GPT-3.5/GPT-4: large autoregressive transformers from OpenAI (exact sizes not specified in this paper). Goat-7B: a 7B-parameter transformer (LLaMA-based) fine-tuned with Chain-of-Thought techniques as reported by Liu & Low (2023).",
            "arithmetic_task_type": "4-digit multiplication (multi-digit arithmetic) used for comparison.",
            "reported_mechanism": "Goat-7B: fine-tuned with CoT to learn an algorithmic pipeline; GPTs: typically rely on general pretraining and prompting; when not fine-tuned with explicit CoT their arithmetic performance can be poor, suggesting reliance on pattern matching/memorization rather than learned explicit algorithms.",
            "evidence_for_mechanism": "Liu & Low report Goat-7B achieves high arithmetic accuracy after CoT fine-tuning (reported 96.9% exact / 99.2% per-digit). In this paper GPT-3.5 and GPT-4 evaluated on the same 4-digit examples performed poorly (reported 1.2%/61.9% and 5.3%/61.8% respectively), indicating low out-of-the-box arithmetic exactness without specialized CoT fine-tuning.",
            "evidence_against_mechanism": "The low performance of GPT-4/GPT-3.5 on exact-match arithmetic suggests that scale and general pretraining alone do not guarantee reliable arithmetic exactness; the paper does not provide internal analyses of GPT models to dispute or confirm algorithmic vs memorization behavior.",
            "intervention_type": "For Goat-7B: fine-tuning with Chain-of-Thought demonstrations; for GPTs: standard prompting or non-specialized prompting in these comparisons.",
            "effect_of_intervention": "Fine-tuning with CoT (Goat) dramatically improves arithmetic exactness compared to general GPTs; evidence here supports that CoT fine-tuning is the key intervention enabling high-accuracy arithmetic in transformers.",
            "performance_metrics": "Reported comparative numbers used in paper: Goat-7B 96.9% exact / 99.2% per-digit (Liu & Low 2023); GPT-3.5 1.2% exact / 61.9% per-digit (evaluated by authors on same set); GPT-4 5.3% exact / 61.8% per-digit (reported).",
            "notable_failure_modes": "GPTs without CoT fine-tuning produce incorrect full products and fail exact-match on nearly all 4-digit multiplication cases; formatting and digit-level errors are common.",
            "comparison_to_humans_or_symbolic": "Goat's CoT-trained behavior resembles learning a symbolic multiplication pipeline; no human baseline comparisons provided.",
            "uuid": "e3034.2",
            "source_info": {
                "paper_title": "Auto-Regressive Next-Token Predictors are Universal Learners",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought / Scratchpad Intermediate Supervision",
            "brief_description": "A training/prompting paradigm that includes explicit intermediate reasoning steps (scratchpad) as tokens so that AR next-token predictors can learn and use internal multi-step computations to produce final answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought / Scratchpad",
            "model_description": "Technique: include intermediate tokens (sub-computations, gate outputs, partial results) in training sequences (or elicit them via prompting) so each intermediate token is both predicted and later used as input; used with teacher forcing for training and autoregressive inference at test time.",
            "arithmetic_task_type": "Used to enable multi-step arithmetic tasks such as multi-digit multiplication and parity computation; generally applicable to algorithmic arithmetic and mathematical reasoning.",
            "reported_mechanism": "By exposing intermediate computation as tokens, the AR learner can learn to compute and compose those steps; this converts a hard input→output mapping into a sequence prediction problem where each step is a learnable local function, reducing the effective complexity of learning certain functions (introducing the notion of length complexity).",
            "evidence_for_mechanism": "Paper provides formal results (Theorems 3.3, 3.6, 3.8, 3.11, 3.12) showing (a) AR learnability with CoT can recover functions computed by underlying AR predictors, (b) linear AR models can implement linear-threshold circuits via CoT, (c) parities can be learned with O(log n) intermediate tokens, and (d) empirical success of MLP on 4-digit multiplication when trained with unfolded algorithmic CoT and custom tokenization.",
            "evidence_against_mechanism": "CoT requires producing/collecting intermediate-step supervision per example which can be costly; emergent CoT behavior in models trained on generic corpora without explicit CoT is outside scope of this paper; CoT length may be large and impractical for some tasks.",
            "intervention_type": "Supervised CoT demonstrations in training data; prompt-based CoT elicitation; scratchpad-style intermediate token recording; teacher forcing during training.",
            "effect_of_intervention": "Enables simple linear and MLP architectures to learn complex arithmetic and parity functions that are otherwise hard or impossible under classical supervised training; empirically bridges gap between small models and much larger fine-tuned transformers on specific arithmetic tasks.",
            "performance_metrics": "Enables parity learning with length complexity O(log n) (theoretical). Empirically enabled MLP-775M to reach 96.9% exact / 99.5% per-digit on 4-digit multiplication when trained with unfolded CoT and tokenization.",
            "notable_failure_modes": "Requires curated CoT sequences, which can be long (length complexity cost) and dataset-generation intensive; generalization beyond distribution of CoT examples is not fully characterized.",
            "comparison_to_humans_or_symbolic": "CoT/scratchpad is explicitly analogous to human written scratchwork and to exposing intermediate states of symbolic algorithms; the paper frames CoT as a mechanism that enables AR predictors to emulate symbolic/Turing computation.",
            "uuid": "e3034.3",
            "source_info": {
                "paper_title": "Auto-Regressive Next-Token Predictors are Universal Learners",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
            "rating": 2,
            "sanitized_title": "finetuned_llama_outperforms_gpt4_on_arithmetic_tasks"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2,
            "sanitized_title": "teaching_arithmetic_to_small_transformers"
        },
        {
            "paper_title": "Sub-task decomposition enables learning in sequence to sequence tasks",
            "rating": 2,
            "sanitized_title": "subtask_decomposition_enables_learning_in_sequence_to_sequence_tasks"
        },
        {
            "paper_title": "Tinystories: How small can language models be and still speak coherent english?",
            "rating": 1,
            "sanitized_title": "tinystories_how_small_can_language_models_be_and_still_speak_coherent_english"
        },
        {
            "paper_title": "Towards revealing the mystery behind chain of thought: a theoretical perspective",
            "rating": 1,
            "sanitized_title": "towards_revealing_the_mystery_behind_chain_of_thought_a_theoretical_perspective"
        }
    ],
    "cost": 0.02026275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Auto-Regressive Next-Token Predictors are Universal Learners
29 Jul 2024</p>
<p>Eran Malach 
Auto-Regressive Next-Token Predictors are Universal Learners
29 Jul 20249C07B39085D549E4C19A42DC57C33FDEarXiv:2309.06979v3[cs.LG]
Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks.Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction.In this work, we present a theoretical framework for studying auto-regressive next-token predictors.We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine.We introduce a new complexity measure-length complexity-which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity.Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks.Our results demonstrate that the power of today's LLMs can be attributed, to a great extent, to the autoregressive next-token training scheme, and not necessarily to a particular choice of architecture. 1</p>
<p>Introduction</p>
<p>Large language models have achieved tremendous progress in various NLP tasks, such as machine translation, logical reasoning, coding and natural language understanding.These models, like GPT-3, GPT-4 and LaMDA (Brown et al., 2020;OpenAI, 2023;Thoppilan et al., 2022), are trained on massive amounts of text data and learn to generate coherent and contextually relevant responses to input prompts.Amazingly, such language models are mostly trained with a 1 Harvard University, Kempner Institute for the Study of Natural and Artificial Intelligence.Correspondence to: Eran Malach <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#109;&#97;&#108;&#97;&#99;&#104;&#64;&#102;&#97;&#115;&#46;&#104;&#97;&#114;&#118;&#97;&#114;&#100;&#46;&#101;&#100;&#117;">&#101;&#109;&#97;&#108;&#97;&#99;&#104;&#64;&#102;&#97;&#115;&#46;&#104;&#97;&#114;&#118;&#97;&#114;&#100;&#46;&#101;&#100;&#117;</a>.</p>
<p>Preprint.</p>
<p>1 Code for the experiments is available at: https:// github.com/emalach/LinearLMsingle objective: predicting the next token.While this objective seems extremely simplistic, auto-regressive next-token predictors trained on rich enough data are able to solve strikingly complex tasks (Bubeck et al., 2023).This raises the question of whether such next-token predictors are merely "glorified" autocomplete models, which happened to memorize the entire internet, or are they truly performing novel logical reasoning.To this end, it has been shown that the ability of language models to compute complex functions can be greatly enhanced by using Chain-of-Thought (CoT) and scratchpad techniques (Wei et al., 2022b;Kojima et al., 2022;Lightman et al., 2023;Nye et al., 2021), allowing the models to perform unrestricted intermediate computations before arriving at a final answer.</p>
<p>In this work, we introduce a theoretical framework for studying auto-regressive next-token predictors.We demonstrate that much of the power of today's language models in logical reasoning can be attributed to the nature of the autoregressive learning, and not to a particular choice of architecture.We show theoretically that very simple models trained to only predict the next token in an auto-regressive fashion can be used to solve extremely complex tasks when utilizing CoT techniques.In particular, we show that even linear predictors-models where the next-token probability is a linear function of the input sequence-are already powerful enough to compute any Turing computable function.</p>
<p>The main theoretical result in the paper is captured in the following informal statement: Theorem 1.1 (informal).For any function f that can be efficiently computed using a Turing machine, there exists a dataset D such that training a (linear) next-token predictor on D results in a predictor that approximates f .That is, any computer program or intelligent agent that can be simulated by a computer, can be learned, given the right dataset, by a simple next-token predictor.</p>
<p>To understand the power of auto-regressive learning, observe that a result equivalent to Theorem 1.1 is not possible in the classical supervised learning setting, where the learner is given access only to the input sequence and the target label.It is well-known that no learning algorithm can efficiently learn the class of all (efficient) Turing computable functions (Valiant, 1984), given only the input and the output of the function (without access to intermediate supervision).In fact, in classical supervised learning, there are only a few function classes that are known to be efficiently learnable-function classes for which there exists a learning algorithm that can efficiently recover the target function given a labeled dataset.Learnable function classes are known to have fundamental limitations to their computational capacity.For example, the class of linear predictors is efficiently learnable in many settings, e.g. using the Perceptron algorithm (Rosenblatt, 1958).However, a famous result in (Minsky &amp; Papert, 2017) shows that linear predictors cannot compute simple functions such as the XOR function.Auto-regressive learning, however, presents a striking difference.While linear next-token predictors are still efficiently learnable using simple algorithms such as SGD, their computational capacity greatly surpasses the capacity of their classical counterparts.Since auto-regressive inference introduces a sampling function2 after each step, it allows linear next-token predictors to compute non-linear functions.As implied by Theorem 1.1, linear next-token predictors can implement practically any target function of interest.</p>
<p>While next-token predictors have the capacity to generate highly proficient learners, this does not come without a cost.One significant expense is the requirement to provide the learning model with potentially long sequences of tokens that detail the internal computations of the target.This requirement can be resource-intensive and often impractical.As such, it prompts the introduction of a new measure of learning complexity, analogous to sample complexity or run-time complexity: the length complexity.This type of complexity measures the quantity of intermediate tokens in a CoT necessary for the model to learn a particular concept class.We explore this complexity in the context of the parity learning problem, an extension of the XOR problem that is known to be computationally hard to learn in some settings.We demonstrate how traditional forms of complexity, such as sample or run-time complexity, can be traded off with length complexity when learning parities.Specifically, we show that an increase in the complexity of the hypothesis class-and therefore in sample or computational complexity-leads to a decrease in length complexity.This opens up a new path for the theoretical investigation of auto-regressive learning, by studying the interplay between these different complexity measures.</p>
<p>To substantiate our theoretical results, we experimentally illustrate the power of auto-regressive learning in enhancing the performance of simple models.We train a linear next-token prediction network on the TinyStories dataset (Eldan &amp; Li, 2023), a collection of short stories composed of simple words.We observe that linear models, once trained on this dataset, frequently generate plausible and grammat-ically sound stories.Next, we demonstrate that a shallow Multi-Layer Perceptron (MLP) with 775M parameters (no attention layers), can learn to correctly multiply two 4-digit numbers, given CoT data.Our MLP achieves comparable results to Goat, a 7B-parameter transformer trained to solve arithmetic tasks (Liu &amp; Low, 2023).Remark 1.2.We use the term chain-of-thought to refer to situations where a language model, when given some input question, outputs a sequence of intermediate steps before arriving at the final answer.In general, this behavior may arise due to training on data with chain-of-thought demonstrations, or by prompting the model to "think step by step".Our theoretical results imply that linear auto-regressive models can compute complex functions using a chain of intermediate calculations, and that they learn to do so when trained on data with such chain-of-thought sequences.The study of chain-of-thought "prompting" of language models trained on a large corpus of data is beyond the scope of this paper.</p>
<p>Related Work</p>
<p>CoT Reasoning The proposition of supervising intermediate logical steps as an effective approach for problemsolving is well established, predating the advent of Transformer models.The technique was found to be particularly beneficial in solving arithmetic problems (Roy &amp; Roth, 2016).This idea became very popular with the introduction of the Chain-of-Thought (CoT) approach, where models are prompted to elucidate their thought process prior to yielding a final outcome (Wei et al., 2022b;Kojima et al., 2022;Lightman et al., 2023).Recent developments have further demonstrated the efficacy of the CoT method in the training of smaller student models (Li et al., 2023;2022;Magister et al., 2022).Another method that bears similarity to CoT is the "scratchpad" technique, which allows models to record intermediate computations that subsequently aid in deriving the final answer (Nye et al., 2021).Such techniques have been shown to enhance performance across a variety of logical reasoning and arithmetic tasks.The research presented in this paper aims to contribute to the theoretical understanding of CoT reasoning in auto-regressive models.Our work illustrates how the employment of CoT can significantly amplify the capabilities of simple models.Furthermore, we introduce a novel complexity measure, the length complexity, that allows us to study the influence of the length of the intermediate sequence of tokens within CoT on the difficulty of the learning problem.</p>
<p>Language Models for Arithmetic Tasks Leveraging large language models to tackle mathematical reasoning and arithmetic tasks has gained significant interest, a trend that is discussed at length in a recent survey (Lu et al., 2022).While these models have demonstrated a promising capacity for solving an array of mathematical problems, they often en-counter difficulties in executing straightforward arithmetic operations, such as the multiplication and addition of large numbers (Nogueira et al., 2021;Qian et al., 2022).Previous studies have suggested that the efficiency of language models in arithmetic tasks can be dramatically enhanced by structuring them to perform calculations using an algorithmic pipeline, facilitating step-by-step execution (Muffo et al., 2023).A notable contribution in this realm is the recent work by (Liu &amp; Low, 2023), where they fine-tuned a moderately sized (7B-parameter) transformer employing the CoT method to perform complex arithmetic operations, including the multiplication of large numbers-a challenge even for advanced models like GPT-4.A very recent work studies the ability of small transformers trained from scratch to solve arithmetic tasks (Lee et al., 2023).In our study, we further substantiate this claim by demonstrating that a small MLP, devoid of any attention mechanism, can match the performance of the transformer in (Liu &amp; Low, 2023) in 4-digit multiplication, provided that it receives appropriate intermediate supervision.This highlights that the capability of language models for arithmetic and mathematical reasoning is largely attributable to the CoT and next-token prediction techniques, rather than the specific architectural choice.</p>
<p>Beyond Transformers Although the transformer architecture (Vaswani et al., 2017) currently stands as the leading approach in language modeling, it is noteworthy that a diverse range of other architectures have served this purpose over time.A notable instance is the application of Recurrent Neural Networks (RNNs) (Hochreiter &amp; Schmidhuber, 1997), a model highly popular for language modeling only a few years back, due to its efficient and inherent sequence processing capabilities (Mikolov et al., 2010).Furthermore, convolutions have also been explored for language modeling tasks (Dauphin et al., 2017).A work more related to our own leveraged linear dynamical systems to model text (Belanger &amp; Kakade, 2015).Recent years have witnessed an emerging interest in substituting the attention layer of transformers, primarily due to its high computational cost, with simpler and more efficient alternatives.In this vein, the work of (Katharopoulos et al., 2020) introduced the linear transformer, where the attention layer was replaced with a more computationally-friendly linear layer.Concurrently, (Zhai et al., 2021) advanced an Attention-Free Transformer.More recent advancements include the RWKV architecture (Peng et al., 2023), a modern variant of the RNN architecture inspired by transformers, which exhibits competitive performance when trained on large datasets.Some studies have proposed the use of simpler MLP-based architectures as feasible alternatives to transformers (Tolstikhin et al., 2021;Liu et al., 2021).Our work contributes to this ongoing discourse by conducting both theoretical and empirical investigations into the potential of very simple models, such as linear models and small MLPs, training them to solve complex tasks by leveraging the power of next-token autoregressive learning.</p>
<p>Related Theoretical Work Despite the rapid pace of practical advancements in the realm of language models and transformers, the theoretical underpinning remains comparatively unexplored.Early investigations have established the universality of transformers (i.e., their ability to emulate any Turing machine) given the incorporation of a recurrent module (Yun et al., 2019;Wei et al., 2022a).More recently, it has been demonstrated that transformers can simulate universal computers when incorporated into an execution loop (Giannou et al., 2023).The work of (Liu et al., 2022) shows that Transformers can simulate Automata, which are equivalent to bounded-memory programs, using surprisingly few layers.Turing universality extends to other language modeling architectures, such as RNNs (Siegelmann &amp; Sontag, 1992).A study by (Edelman et al., 2022) underscores the inductive biases of self-attention, demonstrating that bounded-norm Transformer networks can represent sparse functions with logarithmically scaling sample complexity.The work of (Feng et al., 2023) theoretically demonstrates the importance of CoT for solving mathematical problems with transformers.Of particular relevance to our study is the work of (Wies et al., 2022), which delves into how subtask decomposition and the CoT technique can facilitate the learning of computationally challenging problems.Similarly to our study, (Wies et al., 2022) also explores parity learning with intermediate supervision and demonstrates that arbitrary Turing machines can be efficiently learned by language models trained with CoT.Our work extends these findings, introducing a theoretical framework that enables broader examination of auto-regressive learning.We show that even linear predictors can efficiently learn Turing computable functions.In addition, our results offer improved length complexity bounds for learning parities, indicating that parities can be learned using O(log n) intermediate tokens, a marked reduction from the O(n) intermediate tokens in (Wies et al., 2022).</p>
<p>Theory</p>
<p>The key principle in our theoretical results is the differentiation between "classical" supervised learning and Auto-Regressive (AR) learning.In supervised learning, there is a clear separation between the input and the label (or target).The learner gets a dataset of inputs with their labels, and needs to find a model that correctly predicts the label of a new input example.While supervised learning tasks can sometimes be easy (e.g., when the label is given by a linear function of the input features), this task becomes very hard, or even impossible, when the function used for generating the labels requires a complex computational process (Valiant, 1984).This hardness stems from the fact that the internal computation is not available to the learner, who only observes the input and the corresponding final output.</p>
<p>In AR learning, on the other hand, the situation is different.AR learners get a sequence of tokens, and treat every token both as an input (for predicting future tokens) and as a label (for sequences of previous tokens).Coupling AR learning with the CoT technique results in a learning paradigm where the internal computations required for reaching the final answer become available to the learner both as inputs and as labels.This naturally allows supervision on intermediate steps in the computation/reasoning process, which greatly simplifies the learning task.</p>
<p>In the following sections we detail our theoretical results.In Section 3.1 we formally define the framework of AR Learning and Learnability, in an analogous way to classical PAC Learning.We then show how PAC Learnable hypothesis classes can be used for constructing AR Learnable classes, and discuss the special case of linear classes (which are known to be efficiently PAC Learnable).In Section 3.2 we discuss approximation results, namely understanding what types of function a given AR model can compute.To this end, we consider the function computed by the model to be the function mapping the input tokens to the final token(s), allowing the model to arbitrarily use internal computations in a chain-of-thought manner.Following this, we show that even linear AR models can compute very complex functions, for example emulating arbitrary Turing machines.Finally, in Section 3.3 we introduce length complexity, which measures how many intermediate tokens are required in order to learn to compute a given function.We show that using more intermediate tokens, i.e. increasing the length complexity, can reduce time/sample complexity, and vice-versa.</p>
<p>Remark 3.1.It is important to note that in AR learning there is a crucial difference between the "training mode" and "inference mode" of the model.During training we use "teacher forcing": training the model to predict the next token given an input from the ground truth data.During inference, however, we feed the model with some input and let it generate auto-regressively, predicting the next token given the sequence of tokens that were generated by the trained model itself.In our theoretical analysis we prove results in both the training mode setting (e.g., Theorem 3.3) and the inference mode (Theorem 3.8).These results, while seeming independent, can be viewed as two parts of the same proof, giving guarantees on the inference-time behavior of some predictor trained with teacher forcing.</p>
<p>Learnability Results</p>
<p>Let D be a finite set of tokens, let X = D n be the space of contexts of n tokens, and let Z = D * be a space of strings of tokens.For some t, we denote Z t = D t .An Auto-Regressive (AR) function h is a mapping X × Z → D (we assume a deterministic function).An AR hypothesis class H is a set of AR functions.Fix some T ∈ N.3 For some distribution D over X × Z T , we say that D is realizable by the AR class H if there exists a function h ∈ H such, with probability 1 over (x, z) ∼ D, we have h(x, z &lt;t ) = z t for all t ≤ T (where z &lt;t denotes the first t − 1 coordinates of z).In other words, the pair (x, z) is realizable by h if h accurately predicts the next token for all prefixes z &lt;t of z.We now define Learnability in the AR framework: Definition 3.2.We say that H is AR Learnable if there exists a function m : (0, 1) 2 → N and an algorithm such that for every ϵ, δ ∈ (0, 1) and distribution D realizable by H, given a sample of size m(ϵ, δ) from D, returns with probability (w.p.) ≥ 1 − δ a function ĥ ∈ H s.t.Pr ∃t ≤ T s.t.ĥ(x, z &lt;t ) ̸ = z t ≤ ϵ.Furthermore, we say that H is efficiently AR Learnable if it is AR Learnable with an algorithm running in polynomial time.</p>
<p>That is, a class H is (efficiently) AR Learnable if there exists an (efficient) algorithm that finds, w.h.p., a next-token predictor with low error.</p>
<p>We now show that hypothesis classes that are learnable in the classical sense (i.e., by supervised learning), naturally induce hypothesis classes that are AR Learnable.Let H be some AR hypothesis class.We assume that H can be decomposed into "standard" hypothesis classes in the following sense.Let {H t } ∞ t=1 be a sequence of classes, where H t is a class of functions X × Z t−1 → D. We assume that H = H 1 × H 2 × . . . .Namely, we associate every h ∈ H with a sequence (h 1 , h 2 , . . .), where h i ∈ H i , s.t. for every x ∈ X and z ∈ Z t−1 we have h(x, z &lt;t ) = h t (x, z &lt;t ).While we define H on arbitrarily long sequences, when we study learnability we limit ourselves to discussing sequences of length at most T .In particular, we can assume
H = H 1 × • • • × H T .
The following result shows that PAC Learnability of the underlying hypothesis classes (as defined e.g. in (Shalev-Shwartz &amp; Ben-David, 2014)) implies AR Learnability of the class H:
Theorem 3.3. If H 1 , . . . , H T are (efficiently) PAC Learn- able with sample complexity m(ϵ, δ), then H = H 1 × • • • × H T is (efficiently) AR Learnable with sample complexity m(ϵ/T, δ/T ).
The proof (in Appendix A) is a simple reduction using the standard notion of PAC Learnability.</p>
<p>Linear Decoder</p>
<p>From Theorem 3.3, efficiently learnable classes induce classes that are efficiently learnable in the Auto-Regressive setting.For example, by letting H t be a class of linear func-tions, we can use known results on learning linear classifiers to show that the induced AR hypothesis class is efficiently learnable.We define the linear AR hypothesis class as follows.Definition 3.4.Let ψ : D → R d be some embedding of the dictionary.With some abuse of notations, for z ∈ D t we define ψ(z) = [ψ(z 1 ), . . ., ψ(z t )] ∈ R d×t .Fix some t, let W ∈ R D×d×(n+t) , and for all x ∈ X and z ∈ Z t define h W (x, z) = arg max D∈D ⟨W D , ψ([x, z])⟩.Denote the function class of all linear predictors
H Lin t = {h W : W ∈ R D×d×(n+t) }.
Observe that the class H Lin t is PAC-learnable in polynomial time.Under some margin conditions and using a convex surrogate loss function, this class is in fact learnable using SGD (Shalev-Shwartz &amp; Ben-David, 2014).Therefore, for the linear AR hypothesis class H Lin = H Lin 1 × • • • × H Lin T , we get that H Lin is efficiently learnable in the Auto-Regressive setting.</p>
<p>Approximation Results</p>
<p>We showed that when the AR hypothesis class H is induced from a sequence of (efficiently) learnable hypothesis classes, then H is also (efficiently) AR learnable.In particular, H Lin is efficiently AR learnable, as a product of linear classes.We now show that while learnability transfers from the classical setting to the AR setting, in AR learning we can get much stronger approximation guarantees.In fact, while linear classes are relatively limited in the standard setting, we show that the linear AR class H Lin is extremely powerful.Namely, we show that linear AR functions can efficiently approximate any Turing computable function.</p>
<p>We first need a proper definition of what are the functions that AR hypotheses "compute".For some AR hypothesis h, define the output of the auto-regression process at time t to be h (t) (x), defined recursively by: h (1) (x) = h(x, ∅), and h (t) (x) = h x, h (1) (x), . . ., h (t−1) (x) .For now, we focus on AR hypotheses that are evaluated for T steps, for some fixed T ∈ N. In Section 3.3 we discuss how the choice of T (length complexity) interacts with different measures of complexity.We define the function computed (approximated) by h as follows: Definition 3.5.Fix some target f : D n → D and some AR hypothesis h.Then, we say that h computes f , if for every input x ∈ D n we have h (T ) (x) = f (x).Additionally, for some distribution D over D n , we say that h ϵ-approximates
f w.r.t. D, if Pr D h (T ) (x) ̸ = f (x) ≤ ϵ.
In other words, we say that h computes f if after running auto-regression for T steps, it outputs a value that agrees with f .Note that we ignore all the intermediate outputs of h and observe only the final output.This is in alignment with common practice, where we let language models use arbitrarily long chain-of-thought/scratchpad before arriving at the final answer4 .</p>
<p>Next, we show that if some AR class H is learnable, then auto-regressive learning of distributions realizable by h ∈ H returns an approximator for the function computed by h: Theorem 3.6.Assume that H is (efficiently) AR Learnable with sample complexity m(ϵ, δ).Then, there is an (efficient) algorithm s.t. for any ϵ, δ and distribution D realizable by some h ∈ H, given a sample of size m(ϵ, δ), returns w.p. ≥ 1 − δ a function ĥ s.t.ĥ(T ) ϵ-approximate h (T ) w.r.t.D.</p>
<p>The proof follows by induction from the definitions (see Appendix A).Theorem 3.6 shows that using AR learning, we can learn to approximate the function computed by the underlying AR function h.</p>
<p>Approximation Capacity of Linear Hypotheses</p>
<p>We now limit ourselves to a dictionary with only two tokens D = {0, 1}, to be compatible with standard analysis of computations with Boolean inputs/outputs.We will show that linear AR functions can approximate a very large class of functions-namely, the class of linear threshold circuits.Definition 3.7.A linear threshold function is a func. of the form x → σ(⟨w, x⟩ + b) for σ(x) = 1 x≥0 .A linear threshold circuit is a Boolean circuit where every gate computes a linear threshold function.</p>
<p>The following result shows that linear AR functions can approximate any linear threshold circuit: Theorem 3.8.Assume that f : {0, 1} n → {0, 1} can be computed by a linear threshold circuit with at most T gates.Then, f can be computed by a linear AR function h ∈ H Lin .</p>
<p>The proof of the above result uses the fact that a linear threshold function can be implemented using arg max over a linear function, in the case where D = {0, 1} (full proof in Appendix A).</p>
<p>We note that any Turing computable function can be computed by a linear threshold circuit of some size T that scales polynomially with the runtime of the Turing machine (see e.g.(Arora &amp; Barak, 2009)).Therefore, we get that linear AR functions can compute any Turing computable function, with only polynomial blow-up in run-time.This leads to the following result: Corollary 3.9.For any function f that is Turing computable in time T (n), and for any distribution D over inputs of size n, there exists a dataset of strings of tokens, each of size poly(T (n)), s.t.training a linear AR model over this dataset efficiently recovers a function that approximates f w.r.t.D.</p>
<p>To prove the above, we consider a dataset generated by a linear model simulating the target Turing machine which computes f .</p>
<p>Length Complexity</p>
<p>We showed that even simple classes like linear AR predictors can approximate any Turing computable function.Since linear predictors can be learned efficiently, we get a learning scheme that can efficiently learn virtually any function of interest.This is in contrast with the standard supervised learning setting, where efficiently learnable function classes are typically very limited in their expressive power.However, we note that the complexity of learning did not magically "disappear".To make learning possible, we require that the learner has, during learning, access to a sequence of tokens representing the internal CoT generated by the target it aims to imitate.While the length of this sequence is still reasonable (polynomial in the problem parameters), acquiring data with such long sequences might be costly, or even impossible.</p>
<p>In this section we introduce length complexity, a new notion of learning complexity that quantifies the number of intermediate tokens required for learning some concept class, i.e. the length of the CoT supervision provided to the model during training.The length complexity complements common complexity measures such as sample and run-time complexity, and we show that in some cases we can trade off sample/computational complexity for length complexity, and vice versa.</p>
<p>We begin with a formal definition of length complexity.Fix some distribution over D n , some AR hypothesis class H and some target concept class F of functions D n → D. The definition below extends Definition 3.5 to function classes, which allows an explicit discussion on length complexity.Definition 3.10.We say that H computes F with length complexity T , if T is the minimal number satisfying that for every f ∈ F there exists some h ∈ H such that, for all x ∈ D n we have h (T ) (x) = f (x).Additionally, we say that H ϵ-approximates F with length complexity T if for every f ∈ F there exists some h ∈ H s.t.
Pr D h (T ) (x) ̸ = f (x) ≤ ϵ.
From Theorem 3.8 we get that the class of linear threshold circuits of size T can be ϵ-approximated using linear AR functions with length complexity T .For small circuits this might not be an issue, but otherwise this dependence may be problematic.We expect that taking a richer AR hypothesis class H would result in reduction of the length complexity.In the rest of this section, we discuss the interplay between the choice of the AR hypothesis class and the different measures of complexity that it induces: sample complexity, computational complexity and length complexity.</p>
<p>Length Complexity of Parities</p>
<p>To demonstrate a concrete analysis of length complexity, we consider the well-studied problem of learning parities, a natural extension of the XOR problem (Minsky &amp; Papert, 2017).In the parity learning problem, the inputs are sequences of n bits, and the label is determined by the parity of the sum of an unknown subset of bits from the input.This problem is known to be computationally hard in some settings.For example, Statistical Query (SQ) algorithms and variants of gradient-descent need Ω(2 n ) steps to solve the problem (Kearns, 1998;Shalev-Shwartz et al., 2017;Abbe &amp; Sandon, 2018;Malach &amp; Shalev-Shwartz, 2022), and it is hard to solve with limited memory (Raz, 2018).</p>
<p>We now formally define the set of parity functions.Assume D = {0, 1} (Boolean inputs).For some subset A ⊆ [n], define the parity function over A by χ A (x) = i∈A x i mod 2. Let P n be the class of all parity functions, P n = {χ A : A ⊆ [n]}.It is known that parities can be computed using O(log n) size linear threshold circuit (Kautz, 1961).So, Theorem 3.8 implies that a linear AR model can compute any parity function with logarithmic length complexity:</p>
<p>Theorem 3.11.The class P n can be computed using H Lin , with length complexity O(log n).</p>
<p>Since we showed that linear AR functions are efficiently learnable (Theorem 3.3), the above theorem implies that parities become efficiently learnable given O(log n) intermediate tokens.This is in contrast to the standard supervised learning setting, where linear functions cannot approximate parities (Daniely &amp; Malach, 2020).We note that a similar result on learning parities with intermediate tokens appears in (Wies et al., 2022), but with O(n) length complexity (instead of O(log n)).</p>
<p>We next show that by taking more complex hypothesis classes we can reduce the length complexity of computing P n .However, this comes at a cost of increasing either the sample or the computational complexity.We define a sequence of AR classes of growing complexity for computing P n .For every k ≤ n, let P n,k be the class of parities over subsets of size ≤ k, namely P n,k = χ A : A ∈ [n]   ≤k .The larger n and k are, the harder it is to learn P n,k (via supervised learning).In particular, there are known lower bounds on learning P n,k using Statistical Query (SQ) algorithms, a large family of algorithms that include variants of gradient-based learning algorithms (Blum et al., 2003).Roughly speaking, learning P n,k using SQ algorithms requires run-time of n ≤k = O((n/k) k ), and the sample complexity of O(k log n).We define H (k) = P n,k × P n+1,k × . . ., and show the following: Theorem 3.12.H (k) can compute P n with length complexity Θ(n/k).</p>
<p>To prove the above result, we show that any parity over n bits can be computed by constructing a "tree" of k-order parities, which reduces the length complexity by a factor of k (see Appendix A).This decrease in length complexity comes at the cost of increasing the computational complexity of the learning exponentially with k (for SQ algorithms and variants of GD).While the exact interplay between computational and length complexity depends on the learning problem, this result shows that sometimes decreasing the length complexity makes the problem computationally hard to learn.We believe that a fundamental understanding of the length complexity of different problems will allow us to better understand AR predictors.For example, discovering an intrinsic complexity measure for hypothesis classes (analogous to VC dimension or SQ dimension) that can be used to derive length complexity bounds is of particular interest.We leave such an investigation to future research.</p>
<p>Experiments</p>
<p>We now turn to empirically validate our theoretical results, showing that very simple models perform surprisingly well when trained auto-regressively to perform next-token prediction.We start by training a simple linear model on a dataset of short stories, and then evaluate the performance of a small MLP on a task of arithmetic computations.</p>
<p>Tiny Stories</p>
<p>We test the efficiency of linear AR models on the TinyStories dataset (Eldan &amp; Li, 2023), a synthetic dataset of short stories containing simple words.We train a linear model with context length of T = 64 on this dataset.The model has only three layers: 1) a standard (linear) embedding layer,</p>
<p>Prompt #1: He was very scared Output:</p>
<p>He was very scared.He cried out loud, but then he heard a voice.He was afraid of the dark monster.He ran to the bear.He had a pain in his hand.He screamed and ran away.He was never seen again.He was scared of the monster and he went on.He never. . .</p>
<p>Prompt #2:</p>
<p>Adam was hungry, so Output:</p>
<p>Adam was hungry, so he wanted to eat the dessert.But he knew it was not a good at his sister .He thought, "Maybe I can eat this food, but I don't want to share with you."</p>
<p>Prompt #3: Alice was tired, so Output #1:</p>
<p>Alice was tired, so she decided to take a nap .She put a blanket on the ground and started to cry.Then, she heard a noise.It was a nearby tree.Output #2:</p>
<p>Alice was tired, so she decided to go on an adventure .She hopped on the way to go home and look for her...
MODEL GRAMMAR CREATIVITY CONSISTENCY PLOT GPT-4 / LT GPT-4 GPT-4 GPT-4
TS-33M 8.0 ± 0.8 / 62% 7.2 ± 0.5 7.0 ± 1.2 6.9 ± 0.8 TS-1M 6.9 ± 0.9 / 59% 6.7 ± 1.0 6.0 ± 1.5 5.6 ± 1.3 LINEAR 6.3 ± 2.0 / 64% 6.2 ± 1.8 5.9 ± 1.8 5.2 ± 1.8 mapping tokens into a vector of dimension d = 256; 2) a linear layer mapping d×T to d×T (using standard masking for next-token prediction during training); 3) an output embedding layer mapping vectors of dimension d = 256 back into the output space of all tokens (see Figure 1).To allow next-token prediction training, we apply masking on the second linear layer, so that each output token only has access to previous tokens in the sequence.While the resulting classifier is linear, we note that this model is not exactly the linear AR model analyzed previously, as we allow sharing some parameters (namely, the input/output embedding parameters) across the different sequence positions.However, this is a close proxy to the idealized linear model.The model is optimized with the cross-entropy loss, using a softmax operation applied to the outputs.Altogether, the resulting model has roughly 162M active parameters.The model is trained for 5 1 /2 hours on a single A100 machine.</p>
<p>We use our model to generate story paragraphs, given some initial sentence.Similarly to Eldan &amp; Li (2023), we use GPT-4 to grade 50 output examples based on grammar, creativity, consistency with the story's beginning and whether the plot makes sense (see Appendix C for further details).We also evaluate the grammar of the generated text using the LanguageTool (LanguageTool, 2024) grammar checker, and report the percentage of generations that had no gram-Prompt: 1394×8618= MLP: (4×1+9×10+3×100+1×1000)× (8×1+1×10+6×100+8×1000)= . . .</p>
<p>GPT-4:</p>
<p>The  (Liu &amp; Low, 2023).</p>
<p>matical errors (generating 5 outputs per story).While the results are certainly inferior in quality to transformer-based language models, we note that the linear predictor often does produce coherent and grammatically correct text.In Figure 2 we show some example of prompts and the resulting output of the model.We emphasize that our goal is not to claim that linear models are better or even comparable to transformers, but rather to show that these extremely simple models achieve non-trivial language modeling capabilities when trained on high-quality data.</p>
<p>Multiplication</p>
<p>We now turn to demonstrate the power of next-token prediction with CoT reasoning for arithmetic tasks.We focus on the task of multiplying two 4-digit numbers, which has been shown to be challenging even for huge language models such as GPT-4 (Liu &amp; Low, 2023).For this task, we train a simple Multi-Layered Perceptron (MLP) with four layers: 1) a standard (linear) embedding layer, from tokens to dimension d = 128; 2) a linear layer with a ReLU activation, applied across all the context window, mapping the input of d × T to an output of d × T (where we use a context length of T = 307); 3) a linear layer with a ReLU activation applied per token, mapping from d to d; 4) a final output embedding, mapping back to the space of all tokens (see Figure 1).Similarly to the linear network, we mask future positions in the second layer.We note that while this network has non-linearity (unlike the previous model), it is still very simple compared to standard transformer-based networks (e.g., we use no attention mechanism).Altogether, our MLP has 775M active parameters.</p>
<p>Recently, a paper by (Liu &amp; Low, 2023) instrodced Goat, a relatively small transformer fine-tuned from the LLaMA model that was able to outperform GPT-4 in various arithmetic tasks, when trained on data with intermediate calculations.We follow a similar procedure for training our model on 4-digit multiplication, with some key differences.First, we give more intermediate steps than in (Liu &amp; Low, 2023), essentially unfolding the multiplication algorithm in the training sequences (see Figure 4.1).Second, we use a custom tokenization scheme, where we tokenize separately single digits (1, 2, 3, . . .), signs (×, +, =) and also pairs of digits with multiplication sign (1 × 2, 3 × 5, etc).This tokenization allows the model to quickly solve the single-digit multiplication task (by mapping pairs of multiplied digits to their product), which is a crucial tool in the multiplication algorithm.Finally, we also add zero-padding to some of the numbers, to get all strings to have the same length.</p>
<p>We split all pairs of 4-digit numbers arbitrarily, use 75% for training, and keep the rest for validation.The network is trained from scratch for 17 hours on a single A100 GPU, going over 100M sequences (307M tokens) sampled uniformly from the training set.In Table 4.1 we compare the performance of our simple MLP (evaluated on 1000 validation examples) with GPT-3.5 (evaluated on the same examples), as well as to GPT-4 and Goat-7B on the same task (as reported in (Liu &amp; Low, 2023)).We report both accuracy of the exact match of the final answer, as well as accuracy of individual digits in the final number.We note that the performance of our MLP matches the performance of the much larger fine-tuned transformer in (Liu &amp; Low, 2023) 5 , and outperforms both GPT-3.5 and GPT-4 on this task.This demonstrates again that a lot of the power of language models can be attributed to the next-token autoregressive training on high-quality data, and not necessarily to a particular architectural choice.</p>
<p>Discussion</p>
<p>The emerging capabilities of large language models has triggered an ongoing debate about their potential and implications.Certain proponents assert that we are close to achieving Artificial General Intelligence (AGI), pointing to models such as GPT-4 which have already demonstrated per-ceived "sparks of AGI" (Bubeck et al., 2023).They argue that AGI is just a matter of scaling up-creating larger models, feeding them with more data, and increasing training time.In stark contrast, others dismiss these large models as merely sophisticated autocomplete systems, voicing concerns about their propensity to potentially absorb and perpetuate biased and harmful data (Bender et al., 2021).</p>
<p>While this debate is far from settled, we hope that our work sheds light on the theoretical possibilities inherent in training auto-regressive next-token predictors.However, we show that by modifying the choice of the hypothesis class we can possibly shorten the required sequence length, making our results more realistic.Therefore, we believe that our research can contribute towards a better, more nuanced understanding of both the capabilities and constraints associated with next-token predictors.</p>
<p>Impact Statement</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>A. Proofs</p>
<p>Proof of Theorem 3.3.Let D be some distribution over X × Z T realizable by H, and let D t be the distribution over (X × Z t−1 ) × D, where we sample (x, z) ∼ D and observe ((x, z &lt;t ), z t ).Therefore, D t is a labeled distribution realizable by H t , and so we can use a learner for H t to find using m(ϵ/T, δ/T ) samples, with probability 1 − δ/T , a hypothesis ĥt s.t.Pr D ĥt (x, z &lt;t ) ̸ = z t ≤ ϵ/T .Therefore, using the union bound, with probability at least 1 − δ, we get:
Pr ∃t ≤ T s.t. ĥt (x, z &lt;t ) ̸ = z t ≤ t≤T Pr ĥt (x, z &lt;t ) ̸ = z t ≤ ϵ
Proof of Theorem 3.6.By the definition of AR Learnability, we can find a hypothesis ĥ s.t., with probability at least 1 − ϵ over x ∼ D, we get ĥ(x, h (1) (x), . . ., h
(t) (x)) = h(x, h (1) (x), . . . , h (t) (x))
for all t.So, for such x we get ĥ(1) (x) = ĥ(x,
∅) = h(x, ∅) = h (1) (x)
and by induction: ĥ(t) (x) = ĥ(x, ĥ(1) (x), . . ., ĥ(t−1) (x))</p>
<p>= ĥ(x, h (1) (x), . . ., h
(t−1) (x)) = h(x, h (1) (x), . . . , h (t−1) (x)) = h (t) (x)
Proof of Theorem 3.8.Let f be some target circuit, and we define the depth of some gate in the circuit to be the maximal number of nodes in a path connecting the gate to some input variable.We sort the gates in the circuit by their depth, and let f (1) , . . ., f (T ) be the functions computed by the gates in the circuit (where f (T ) = f is the output function).Observe that every gate f (t) can be computed by the argmax of a linear function of the inputs and previous gates, and therefore we can define some linear hypothesis h s.t.h(x, f (1) (x), . . ., f (t−1) ) = f (t) (x).By induction, we get that for every t we have h (t) = f (t) and therefore the required follows.</p>
<p>Proof of Theorem 3.12.To show that the length complexity is O(n/k), observe that it is enough to construct a Boolean circuit of size O(n/k), where every gate computes a parity over at most k input bits (similarly to the proof of Theorem 3.8).This circuit has the structure of a tree, where each node has in-degree at most k.It is easy to see that such a tree, with depth log k (n) and O(n/k) internal nodes can compute the parity over any subset of bits from the input.</p>
<p>We now show that the length complexity is lower bounded by Ω(n/k).Assume, for the sake of contradiction, that P n can be computed with length complexity T ≤ n/2k, and particularly this implies that the parity over all input bits (namely, χ [n] ) can be computed with T ≤ n/2k.Observe that, by the choice of the function class, at every step t we have h (t) (x) = χ At (x) for some subset A t ⊆ [n].Additionally, at every step t, the size of A t can increase by at most k.Therefore, after T ≤ n/2k steps, h (T ) (x) = χ A T for some A T ⊊ [n], and therefore h (T ) (x) does not compute (or even approximate) χ [n] .</p>
<p>B. Additional Figures</p>
<p>C. TinyStories GPT-4 Evaluation</p>
<p>To evaluate the models trained on the TinyStories dataset, we give all models a beginning of a sentence from a list of 50 sentences (generated by GPT-4).We then let the models complete the sentence, generating up to 64 tokens.We sample all models with temperature 0.5.We use the following prompt to GPT-4 to grade the quality of the produced text (this prompt is taken, with small modifications, from (Eldan &amp; Li, 2023)):</p>
<p>The following exercise, the student is given a beginning of a sentence from a story.The student needs to complete it into a paragraph from the story.The exercise tests the student´s language abilities and creativity.The symbol *** marks the separator between the prescribed beginning and the student's completion:</p>
<p>Once upon a time, in an ancient house,*** there lived a little girl named Mia.Mia loved to play with her toys and have fun with her friends.One day, Mia found a big box in her room.It was very pretty and shiny.</p>
<p>Please provide your general assessment about the part written by the student (the one after the *** symbol).Is it gramatically correct?Is it consistent with the beginning of the story?Pay special attention to whether the student manages to complete the sentence which is split in the middle by the separator ***.The student's completion of the story is mostly consistent with the beginning of the story.It maintains the focus on Lily and her family, and the sentence split by the separator is completed correctly.However, the student's addition does not fully integrate the shiny decorations found in the attic, which were a significant part of the beginning.The grammar is generally correct, but there are a few minor errors: 〈list omitted〉.Overall, the student's completion of the story demonstrates adequate language abilities and creativity, but could benefit from better integration of the shiny decorations and minor grammar improvements.Now, grade the student's completion in terms of grammar, creativity, consistency with the story's beginning and whether the plot makes sense.Use numbers from 1 to 10.</p>
<p>Output example: Grammar: 8, Creativity: 7, Consistency:5, Plot: 6</p>
<p>We report the average score for all models.Below is the list of sentences that we use:</p>
<p>• "Once upon a time, in a colorful garden, there lived a tiny caterpillar named Charlie who..."</p>
<p>• "In the big, blue sky, Lucy the little bird was learning how to..."</p>
<p>Figure 2 .
2
Figure 2. Top: Example prompts and outputs for Linear model trained on TinyStories (grammatical/conceptual errors in red).Bottom: Comparison between Transformer-and Linear models, average grades from GPT-4.</p>
<p>Figure 4 .
4
Figure 4. Comparison between the output of our MLP, GPT-3.5 and GPT-4 on the 4-digit multiplication task.</p>
<p>Output of the MLP and GPT-4 on the 4-digit multiplication task (full output in Appendix B).Right: Performance of GPT vs. MLP model on the 4-digit multiplication task.<em>For GPT-4 and Goat-7B, we use the numbers as repored in
MODELACC. (EXACT/PER-DIGIT)MLP-775M96.9% / 99.5%GPT-3.51.2% / 61.9%multiplication of 1394 and 8618 equals 12 , 01 4 , 05 2 .GPT-4</em> GOAT-7B*5.3% / 61.8% 96.9% / 99.2%Answer: 12013492Figure 3. Left:
In our analysis we focus on the zero-temperature/argmax sampling, which acts as an explicit non-linearity.
In Section 3.3 we study how the choice of T affects the complexity of the learning problem, but for now we treat T as a fixed parameter of the learning problem.
Here we assume that f outputs a single token in D, and therefore observe only the last token produced by the auto-regression. However, we note that this can be extended to the case where f outputs multiple tokens, and we observe a sequence of tokens at the end of the auto-regression.
We also trained a small 70M transformer using our tokenization and CoT scheme. This transformer achieved only 72% perdigit accuracy, far worse than the MLP or the 7B transformer ofLiu &amp; Low (2023). That said, it is possible that a bigger transformer can achieve more competitive results, but needs far more compute compared to our MLP.
• "Under the warm, shining sun, Benny the playful puppy found a..."• "In the quiet, cozy barn, Millie the cow was dreaming about..."• "On a bright, sunny morning, Oliver the curious kitten saw a..."• "Deep in the green forest, Harry the little hedgehog was searching for..."• "Near the sparkling river, Daisy the duck was making friends with..."• "In the tall, whispering grass, Freddy the frog was hopping towards..."• "Up in the fluffy white clouds, Peter the plane was flying over..."• "Beneath the twinkling stars, Luna the owl was watching..."• "Along the sandy beach, Sammy the crab was building a..."• "In the busy, buzzing meadow, Bella the bee was collecting nectar from..."• "On top of the snowy mountain, Eddie the eagle was soaring above..."• "Inside the colorful coral reef, Wendy the fish was swimming with..."• "At the edge of the mysterious jungle, Zoe the zebra was looking at..."• "In the middle of the big city, Max the mouse was exploring..."• "Under the bright rainbow, Ruby the rabbit was playing with..."• "On the quiet farm, Ollie the ox was helping to..."• "In the vast, open field, Ellie the elephant was trumpeting to..."• "Next to the cool pond, Darcy the dragonfly was zooming around..."• "In the old, wise tree, Timmy the squirrel was collecting nuts for..."• "Around the busy bee hive, Polly the butterfly was fluttering near..."• "Underneath the cozy blanket, Lily the lamb was dreaming of..."• "Beside the gentle stream, Finley the fish was hiding from..."• "In the dark, spooky cave, George the bat was hanging upside down and..."• "Atop the ancient castle, Fiona the falcon was guarding..."• "Within the enchanted forest, Greta the gnome was casting spells to..."• "Behind the colorful rainbow, Nora the nymph was playing tricks on..."• "Among the tall sunflowers, Sunny the sunbird was singing to..."• "On the quiet moonlit night, Marvin the moth was flying towards..."• "In the golden wheat field, Will the weasel was sneaking through..."• "Along the sparkling coastline, Coral the seagull was searching for..."• "Under the large oak tree, Oakley the owl was preparing for..."• "In the middle of the pumpkin patch, Patty the pumpkin was waiting to..."• "At the bottom of the deep ocean, Oscar the octopus was discovering..."• "On the windy hilltop, Hannah the hawk was watching for..."• "Inside the bustling anthill, Andy the ant was working hard to..."• "Near the rosy apple tree, Amy the aardvark was sniffing around..."• "At the edge of the shimmering lake, Leah the loon was diving for..."
Abbe , E Sandon, C , arXiv:1812.06369Provable limitations of deep learning. 2018arXiv preprint</p>
<p>Computational complexity: a modern approach. S Arora, B Barak, 2009Cambridge University Press</p>
<p>A linear dynamical system model for text. D Belanger, S Kakade, International Conference on Machine Learning. PMLR2015</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Noise-tolerant learning, the parity problem, and the statistical query model. A Blum, A Kalai, H Wasserman, T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 5042003. 2020Journal of the ACM (JACM)</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Learning parities with neural networks. A Daniely, E Malach, Advances in Neural Information Processing Systems. 202033</p>
<p>Language modeling with gated convolutional networks. Y N Dauphin, A Fan, M Auli, D Grangier, International conference on machine learning. PMLR2017</p>
<p>Inductive biases and variable creation in self-attention mechanisms. B L Edelman, S Goel, S Kakade, C Zhang, International Conference on Machine Learning. PMLR2022</p>
<p>Tinystories: How small can language models be and still speak coherent english?. R Eldan, Y Li, arXiv:2305.077592023arXiv preprint</p>
<p>Towards revealing the mystery behind chain of thought: a theoretical perspective. G Feng, Y Gu, B Zhang, H Ye, D He, L Wang, arXiv:2305.154082023arXiv preprint</p>
<p>Looped transformers as programmable computers. A Giannou, S Rajput, J.-Y Sohn, K Lee, J D Lee, D Papailiopoulos, arXiv:2301.131962023arXiv preprint</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>Transformers are rnns: Fast autoregressive transformers with linear attention. A Katharopoulos, A Vyas, N Pappas, F Fleuret, International conference on machine learning. PMLR2020</p>
<p>The realization of symmetric switching functions with linear-input logical elements. W H Kautz, IRE Transactions on Electronic Computers. 31961</p>
<p>Efficient noise-tolerant learning from statistical queries. M Kearns, Journal of the ACM (JACM). 4561998</p>
<p>Large language models are zero-shot reasoners. Advances in neural information processing systems. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, 202235</p>
<p>. Languagetool, Languagetool, 2024</p>
<p>Teaching arithmetic to small transformers. N Lee, K Sreenivasan, J D Lee, K Lee, D Papailiopoulos, arXiv:2307.033812023arXiv preprint</p>
<p>Symbolic chain-of-thought distillation: Small models can also" think. L H Li, J Hessel, Y Yu, X Ren, K.-W Chang, Y Choi, arXiv:2306.140502023step-by-step. arXiv preprint</p>
<p>Explanations from large language models make small reasoners better. S Li, J Chen, Y Shen, Z Chen, X Zhang, Z Li, H Wang, J Qian, B Peng, Y Mao, arXiv:2210.067262022arXiv preprint</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.200502023arXiv preprint</p>
<p>B Liu, J T Ash, S Goel, A Krishnamurthy, C Zhang, arXiv:2210.10749Transformers learn shortcuts to automata. 2022arXiv preprint</p>
<p>Pay attention to mlps. H Liu, Z Dai, D So, Q V Le, Advances in Neural Information Processing Systems. 202134</p>
<p>T Liu, B K H Low, Goat, arXiv:2305.14201Fine-tuned llama outperforms gpt-4 on arithmetic tasks. 2023arXiv preprint</p>
<p>P Lu, L Qiu, W Yu, S Welleck, K.-W Chang, arXiv:2212.10535A survey of deep learning for mathematical reasoning. 2022arXiv preprint</p>
<p>Teaching small language models to reason. L C Magister, J Mallinson, J Adamek, E Malmi, A Severyn, arXiv:2212.084102022arXiv preprint</p>
<p>When hardness of approximation meets hardness of learning. E Malach, S Shalev-Shwartz, The Journal of Machine Learning Research. 2312022</p>
<p>Recurrent neural network based language model. T Mikolov, M Karafiát, L Burget, J Cernockỳ, S Khudanpur, In Interspeech. 22010Makuhari</p>
<p>Perceptrons, reissue of the 1988 expanded edition with a new foreword by Léon Bottou: an introduction to computational geometry. M Minsky, S A Papert, 2017MIT press</p>
<p>Evaluating transformer language models on arithmetic operations using number decomposition. M Muffo, A Cocco, E Bertino, R Nogueira, Z Jiang, J Lin, arXiv:2304.10977arXiv:2102.130192023. 2021arXiv preprintInvestigating the limitations of transformers with simple arithmetic tasks</p>
<p>Show your work: Scratchpads for intermediate computation with language models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, arXiv:2112.001142021arXiv preprint</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 2023</p>
<p>B Peng, E Alcaide, Q Anthony, A Albalak, S Arcadinho, H Cao, X Cheng, M Chung, M Grella, K K Gv, arXiv:2305.13048Reinventing rnns for the transformer era. 2023arXiv preprint</p>
<p>Limitations of language models in arithmetic and symbolic induction. J Qian, H Wang, Z Li, S Li, X Yan, arXiv:2208.050512022arXiv preprint</p>
<p>Fast learning requires good memory: A time-space lower bound for parity learning. R Raz, Journal of the ACM (JACM). 6612018</p>
<p>The perceptron: a probabilistic model for information storage and organization in the brain. F Rosenblatt, Psychological review. 6563861958</p>
<p>Solving general arithmetic word problems. S Roy, D Roth, arXiv:1608.014132016arXiv preprint</p>
<p>Understanding machine learning: From theory to algorithms. S Shalev-Shwartz, S Ben-David, 2014Cambridge university press</p>
<p>Failures of gradient-based deep learning. S Shalev-Shwartz, O Shamir, S Shammah, International Conference on Machine Learning. PMLR2017</p>
<p>On the computational power of neural nets. H T Siegelmann, E D Sontag, Proceedings of the fifth annual workshop on Computational learning theory. the fifth annual workshop on Computational learning theory1992</p>
<p>R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint</p>
<p>Mlp-mixer: An all-mlp architecture for vision. I O Tolstikhin, N Houlsby, A Kolesnikov, L Beyer, X Zhai, T Unterthiner, J Yung, A Steiner, D Keysers, J Uszkoreit, Advances in neural information processing systems. 202134</p>
<p>A theory of the learnable. L G Valiant, Communications of the ACM. 27111984</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730Attention is all you need</p>
<p>Statistically meaningful approximation: a case study on approximating turing machines with transformers. C Wei, Y Chen, T Ma, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Sub-task decomposition enables learning in sequence to sequence tasks. N Wies, Y Levine, A Shashua, arXiv:2204.028922022arXiv preprint</p>
<p>Are transformers universal approximators of sequence-to-sequence functions?. C Yun, S Bhojanapalli, A S Rawat, S J Reddi, S Kumar, arXiv:1912.100772019arXiv preprint</p>
<p>An attention free transformer. S Zhai, W Talbott, N Srivastava, C Huang, H Goh, R Zhang, J Susskind, arXiv:2105.141032021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8910 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8910</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8910</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-b5085bb6162fe503da62c2d14f7b73a8cfd06226</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b5085bb6162fe503da62c2d14f7b73a8cfd06226" target="_blank">Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization</a></p>
                <p><strong>Paper TL;DR:</strong> Mol-LLM is introduced, the first multimodal generalist model that handles a broad spectrum of molecular tasks among molecular LLMs, explicitly leverages molecular-structure information, and takes advantage of extensive instruction tuning.</p>
                <p><strong>Paper Abstract:</strong> Recent advances in large language models (LLMs) have led to models that tackle diverse molecular tasks, such as chemical reaction prediction and molecular property prediction. Large-scale molecular instruction-tuning datasets have enabled sequence-only (e.g., SMILES or SELFIES) generalist molecular LLMs, and researchers are now exploring multimodal approaches that incorporate molecular structural information for further gains. However, a genuinely multimodal, generalist LLM that covers a broad spectrum of molecular tasks has yet to be fully investigated. We observe that naive next token prediction training ignores graph-structural information, limiting an LLM's ability to exploit molecular graphs. To address this, we propose (i) Molecular structure Preference Optimization (MolPO), which facilitates graph usage by optimizing preferences between pairs of correct and perturbed molecular structures, and (ii) an advanced graph encoder with a tailored pre-training strategy to improve the effect of graph utilization by MolPO. Building on these contributions, we introduce Mol-LLM, the first multimodal generalist model that (a) handles a broad spectrum of molecular tasks among molecular LLMs, (b) explicitly leverages molecular-structure information, and (c) takes advantage of extensive instruction tuning. Mol-LLM attains state-of-the-art or comparable results across the most comprehensive molecular-LLM benchmark-even on out-of-distribution datasets for reaction and property prediction, where it surpasses prior generalist molecular LLMs by a large margin.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8910.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8910.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mol-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mol-LLM: Multimodal Generalist Molecular Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal generalist molecular LLM that fuses 1D molecular sequences (SELFIES) and 2D molecular graphs via a hybrid GNN encoder (GINE + TokenGT), a Q-Former cross-modal projector, and a backbone LLM; trained with extensive instruction-tuning and a novel Molecular structure Preference Optimization (MolPO) objective to improve graph utilization for generation, prediction, and captioning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mol-LLM (backbone: Mistral-7B-Instruct-v0.3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>multimodal transformer / GPT-style LLM with GNN encoders (hybrid GINE + TokenGT) and Q-Former cross-modal projector</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Mistral-7B backbone); overall multimodal architecture built on that backbone</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>3.3M instruction-tuning examples aggregated from Mol-Instructions, SMolInstruct, USPTO reaction datasets (forward/retrosynthesis/reagent), ChEBI-20, PubChem name conversion, MoleculeNet property datasets; GNN pre-training used 5M molecules sampled from PubChem with SELFIES strings and functional-group labels</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / molecular design (description-guided molecule generation), chemical reaction prediction (forward and retrosynthesis), property prediction, molecule captioning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct conditional generation of SELFIES (or other textual molecular outputs) by the LLM given a natural language description and optional 2D graph embeddings; cross-modal fusion via Q-Former; MolPO preference optimization trains the model on preference pairs (original vs perturbed graphs) by maximizing the log-probability gap for chosen vs rejected graphs to encourage reliance on graph structure</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Novelty assessed via exact-match to target molecules (EXACT) and molecular fingerprint similarity (MACCS, Morgan, RDKit FTS); reported metrics rather than explicit percent novel vs training: e.g., Mol-LLM EXACT on ChEBI-20 = 0.443, VALIDITY ≈ 1.00; no explicit fraction-of-novel-molecules-not-in-trainingset reported</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Generates molecules conditioned on textual descriptions and task instructions; uses graph inputs and MolPO to align generation with structural constraints and reaction contexts; for reaction tasks, model outputs target reactants/products directly and is evaluated on exact-match and fingerprint similarity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>EXACT (exact molecular string match), BLEU, fingerprint similarities (MACCS FTS, RDKit FTS, Morgan FTS), VALIDITY (chemical validity), classification/regression metrics for property tasks (ROC-AUC, RMSE/MAE), captioning metrics (BLEU-2/4, ROUGE, METEOR)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mol-LLM achieves state-of-the-art or comparable performance among generalist models across diversified molecular tasks. For description-guided molecule generation, Mol-LLM (ChEBI-20) EXACT = 0.443, MACCS FTS = 0.906, VALIDITY ≈ 1.00; for reaction prediction (Mol-Instructions) forward synthesis EXACT = 0.911 and MACCS FTS = 0.987; Mol-LLM also shows improved out-of-distribution generalization (AqSol LogS OOD RMSE = 1.02 vs baselines), and higher graph discrimination ratios (GDR) when trained with MolPO versus SFT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against generalist baselines (LlaSMol, Galactica, GPT-4, 3D-MoLM, ChemDFM), Mol-LLM outperforms or matches them on the reported benchmarks, particularly improving OOD and reaction prediction performance through explicit graph utilization and MolPO; omitting graph input (w/o Graph variant) reduces performance on tasks sensitive to structure, indicating advantage over sequence-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>MolPO can degrade classification performance when training molecular distributions are narrow or small (classification datasets often ≈1K samples), risk of treating OOD molecules as 'rejected' due to narrow training distributions; the paper notes potential misuse (harmful molecule generation), high compute/resource requirements for training, and that detailed qualitative analysis of which features benefit most from graph utilization remains incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8910.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8910.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The 7-billion-parameter instruction-tuned LLM adopted as Mol-LLM's backbone to generate textual and SELFIES outputs; its token codebook was extended for SELFIES and numeric tokens to support molecular generation and regression outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mistral 7b, 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-style transformer LLM (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained LLM weights (external) further fine-tuned (LoRA) on the 3.3M molecule-focused instruction-tuning dataset used by Mol-LLM during Stage 1; token codebook extended with ~3k SELFIES tokens</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>backbone for molecular generation, property regression, reaction prediction, and captioning within Mol-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive token generation producing SELFIES / textual outputs, conditioned on task instruction, SELFIES strings, and Q-Former graph tokens</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>As backbone component, novelty is a function of Mol-LLM fine-tuning; no standalone novelty metrics reported for Mistral in paper</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Extended token vocabulary (SELFIES, digits, symbols) and LoRA fine-tuning enable direct numeric regression outputs and heterogeneous task outputs (booleans, reaction routes)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used within Mol-LLM evaluations (EXACT, fingerprint FTS, validity, regression/classification metrics) rather than assessed standalone in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>When used as Mol-LLM backbone (with multimodal inputs and MolPO), achieves strong generation and prediction results reported for Mol-LLM; paper does not report isolated Mistral-7B-Instruct performance separate from Mol-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Chosen for strong instruction-following capacity; combined with graph encoder and Q-Former outperforms other generalist approaches on multimodal tasks in this work</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Backbone alone would ignore graph modality unless combined with GNN + Q-Former and MolPO; requires careful token-vocabulary extension and LoRA tuning for molecule tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8910.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8910.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's multimodal/large instruction-following model used as a few-shot (5-shot) baseline in property prediction and generation evaluations in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (5-shot evaluation as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large multimodal transformer LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; used as an external black-box baseline with 5-shot prompts for some tasks</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>baseline for property prediction and molecular generation/captioning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based few-shot generation (5-shot) to produce textual or SMILES/SELFIES-like molecule descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Paper reports baseline performance (e.g., property RMSE/MAE, generation BLEU/fingerprint scores) but does not report explicit novelty statistics for GPT-4 outputs</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Evaluated with few-shot prompts, not fine-tuned on the molecule instruction-tuning dataset used to train Mol-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same primary metrics as Mol-LLM for each task (RMSE, MAE, ROC-AUC, EXACT, BLEU, fingerprint FTS, VALIDITY), reported in tables for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 (5-shot) performs competitively on some property tasks but generally underperforms Mol-LLM on tasks where specialized multimodal training and graph utilization matter; used to contextualize Mol-LLM performance</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as a generalist LLM baseline; Mol-LLM surpasses GPT-4 on many molecular tasks in their benchmarks due to multimodal and domain-specific instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Few-shot prompt evaluation limits chemical specificity; lacks explicit multimodal (graph) inputs and domain instruction-tuning in this comparison</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8910.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8910.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior generalist molecular LLM trained on a large-scale (3.3M) instruction-tuning dataset (cited by the authors) used as a primary baseline; often strong in in-distribution tasks but Mol-LLM claims better OOD generalization via graph-aware training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction-tuned LLM (sequence-only) / transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>A large-scale chemical instruction-tuning dataset (3.3M examples as cited in the paper), sequence-only representations (SELFIES/SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>general molecular tasks (property prediction, reaction prediction, molecule generation, captioning)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequence-only instruction-tuned generation of molecular strings conditioned on prompts (no explicit graph modality reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Paper reports performance metrics (e.g., LlaSMol: strong in some in-distribution tasks and high fingerprint similarity on some reaction OOD tests) but does not provide explicit novelty percentages</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Task-conditioned generation via instruction-tuning; lacks explicit structural graph modality used by Mol-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>EXACT, fingerprint FTS, VALIDITY, regression/classification metrics when evaluated as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LlaSMol is a strong generalist baseline and rivals Mol-LLM on several in-distribution benchmarks; Mol-LLM surpasses LlaSMol on out-of-distribution reaction and property tasks by leveraging graph modality and MolPO</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly in tables; Mol-LLM often outperforms LlaSMol on OOD benchmarks and on tasks where graph structure matters</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Sequence-only approach may limit structural fidelity and OOD robustness compared to multimodal graph-aware models</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8910.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8910.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioT5+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioT5+: Towards generalized biological understanding with IUPAC integration and multi-task tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-generalist sequence-only model evaluated in the paper; two separate BioT5+ variants exist (classification/translation and regression/reaction) and are used as baselines for multiple tasks including molecule generation and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biot5+: Towards generalized biological understanding with iupac integration and multi-task tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioT5+ (two variants evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer sequence-to-sequence model (T5-derived) with task-specific tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Task-specific multi-task tuning datasets including IUPAC translation and molecular property/reaction datasets (as described in BioT5+ paper); used as semi-generalist baselines</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecular property prediction, translation tasks, reaction prediction, generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequence-conditioned generation (no explicit graph modality in variants used here)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Evaluated via standard metrics (EXACT, fingerprint FTS) in benchmarks; no explicit novelty percentages reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Task-specific model variants tuned for classification/translation or regression/reaction tasks; not a single unified multimodal generalist model</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>EXACT, fingerprint FTS, VALIDITY, regression/classification metrics (RMSE, ROC-AUC)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioT5+ shows competitive performance on some tasks but generally underperforms Mol-LLM on OOD and multimodal tasks where graph information is important; used to illustrate benefits of multimodal training</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>BioT5+ (regression & react) is competitive on some reaction tasks but lacks Mol-LLM's graph usage and broad single-model generality</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Two-model design (separate variants) limits cross-task generality; lacks graph modality which Mol-LLM uses to improve structural reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8910.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8910.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior multimodal molecular model that integrates 1D sequences and 2D graphs and is referenced as a baseline; the paper notes that naive SFT in such multimodal models can suffer graph-bypass unless preference optimization is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructMol</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Multimodal transformer with 1D sequence and 2D graph inputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Multimodal instruction-tuning datasets (referenced Mol-Instructions, SMolInstruct)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery assistant tasks including reaction prediction and generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multimodal conditional generation using sequence + graph inputs under supervised fine-tuning (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in this paper; InstructMol used as a comparative baseline in evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to follow instructions across molecular tasks with multimodal inputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used as baseline for generation and prediction metrics (EXACT, fingerprint FTS, ROC-AUC etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced as existing multimodal approach; Mol-LLM claims improved graph utilization and generalization relative to InstructMol when using MolPO and GNN pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper contrasts Mol-LLM's MolPO to naive SFT training approaches used in prior multimodal models like InstructMol, which exhibited limited graph discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Authors observe that naive SFT-trained multimodal models often fail to actually exploit graph structure (graph bypass phenomenon) without explicit preference objectives like MolPO</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8910.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8910.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialist sequence-to-sequence model designed for translation between molecular strings (SMILES/SELFIES) and natural language used as a specialist baseline for generation and captioning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>T5-based sequence-to-sequence transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Task-specific molecule-text translation datasets (ChEBI-20 and related corpora cited in MolT5 paper)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecule-text translation, description-guided molecule generation, molecule captioning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequence-to-sequence textual generation conditioned on descriptions to produce molecular strings</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Evaluated via EXACT, fingerprint FTS, VALIDITY in generation benchmarks; no explicit novelty percentages reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specialist design for translation/generation tasks yields strong performance on those tasks but lacks generalist scope across broad molecular tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>EXACT, MACCS FTS, VALIDITY, BLEU/ROUGE/METEOR for captioning and generation</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolT5 is presented in tables as a specialist strong performer on generation/captioning tasks; Mol-LLM aims to match or exceed such specialist performance while being a single generalist model</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>MolT5 performs well on generation/captioning (specialist) but does not cover the diverse task set that Mol-LLM targets</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Specialist focus limits broader applicability across all molecular task groups</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mol-Instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset <em>(Rating: 2)</em></li>
                <li>InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery <em>(Rating: 2)</em></li>
                <li>MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter <em>(Rating: 2)</em></li>
                <li>Biot5+: Towards generalized biological understanding with iupac integration and multi-task tuning <em>(Rating: 2)</em></li>
                <li>GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text <em>(Rating: 1)</em></li>
                <li>MolT5: Translation between molecules and natural language <em>(Rating: 1)</em></li>
                <li>MolXPT: Wrapping molecules with text for generative pre-training <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8910",
    "paper_id": "paper-b5085bb6162fe503da62c2d14f7b73a8cfd06226",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "Mol-LLM",
            "name_full": "Mol-LLM: Multimodal Generalist Molecular Large Language Model",
            "brief_description": "A multimodal generalist molecular LLM that fuses 1D molecular sequences (SELFIES) and 2D molecular graphs via a hybrid GNN encoder (GINE + TokenGT), a Q-Former cross-modal projector, and a backbone LLM; trained with extensive instruction-tuning and a novel Molecular structure Preference Optimization (MolPO) objective to improve graph utilization for generation, prediction, and captioning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mol-LLM (backbone: Mistral-7B-Instruct-v0.3)",
            "model_type": "multimodal transformer / GPT-style LLM with GNN encoders (hybrid GINE + TokenGT) and Q-Former cross-modal projector",
            "model_size": "7B (Mistral-7B backbone); overall multimodal architecture built on that backbone",
            "training_data": "3.3M instruction-tuning examples aggregated from Mol-Instructions, SMolInstruct, USPTO reaction datasets (forward/retrosynthesis/reagent), ChEBI-20, PubChem name conversion, MoleculeNet property datasets; GNN pre-training used 5M molecules sampled from PubChem with SELFIES strings and functional-group labels",
            "application_domain": "drug discovery / molecular design (description-guided molecule generation), chemical reaction prediction (forward and retrosynthesis), property prediction, molecule captioning",
            "generation_method": "Direct conditional generation of SELFIES (or other textual molecular outputs) by the LLM given a natural language description and optional 2D graph embeddings; cross-modal fusion via Q-Former; MolPO preference optimization trains the model on preference pairs (original vs perturbed graphs) by maximizing the log-probability gap for chosen vs rejected graphs to encourage reliance on graph structure",
            "novelty_of_chemicals": "Novelty assessed via exact-match to target molecules (EXACT) and molecular fingerprint similarity (MACCS, Morgan, RDKit FTS); reported metrics rather than explicit percent novel vs training: e.g., Mol-LLM EXACT on ChEBI-20 = 0.443, VALIDITY ≈ 1.00; no explicit fraction-of-novel-molecules-not-in-trainingset reported",
            "application_specificity": "Generates molecules conditioned on textual descriptions and task instructions; uses graph inputs and MolPO to align generation with structural constraints and reaction contexts; for reaction tasks, model outputs target reactants/products directly and is evaluated on exact-match and fingerprint similarity",
            "evaluation_metrics": "EXACT (exact molecular string match), BLEU, fingerprint similarities (MACCS FTS, RDKit FTS, Morgan FTS), VALIDITY (chemical validity), classification/regression metrics for property tasks (ROC-AUC, RMSE/MAE), captioning metrics (BLEU-2/4, ROUGE, METEOR)",
            "results_summary": "Mol-LLM achieves state-of-the-art or comparable performance among generalist models across diversified molecular tasks. For description-guided molecule generation, Mol-LLM (ChEBI-20) EXACT = 0.443, MACCS FTS = 0.906, VALIDITY ≈ 1.00; for reaction prediction (Mol-Instructions) forward synthesis EXACT = 0.911 and MACCS FTS = 0.987; Mol-LLM also shows improved out-of-distribution generalization (AqSol LogS OOD RMSE = 1.02 vs baselines), and higher graph discrimination ratios (GDR) when trained with MolPO versus SFT alone.",
            "comparison_to_other_methods": "Compared against generalist baselines (LlaSMol, Galactica, GPT-4, 3D-MoLM, ChemDFM), Mol-LLM outperforms or matches them on the reported benchmarks, particularly improving OOD and reaction prediction performance through explicit graph utilization and MolPO; omitting graph input (w/o Graph variant) reduces performance on tasks sensitive to structure, indicating advantage over sequence-only approaches.",
            "limitations_and_challenges": "MolPO can degrade classification performance when training molecular distributions are narrow or small (classification datasets often ≈1K samples), risk of treating OOD molecules as 'rejected' due to narrow training distributions; the paper notes potential misuse (harmful molecule generation), high compute/resource requirements for training, and that detailed qualitative analysis of which features benefit most from graph utilization remains incomplete.",
            "uuid": "e8910.0",
            "source_info": {
                "paper_title": "Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mistral-7B-Instruct",
            "name_full": "Mistral-7B-Instruct-v0.3",
            "brief_description": "The 7-billion-parameter instruction-tuned LLM adopted as Mol-LLM's backbone to generate textual and SELFIES outputs; its token codebook was extended for SELFIES and numeric tokens to support molecular generation and regression outputs.",
            "citation_title": "Mistral 7b, 2023.",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.3",
            "model_type": "GPT-style transformer LLM (instruction-tuned)",
            "model_size": "7B parameters",
            "training_data": "Pretrained LLM weights (external) further fine-tuned (LoRA) on the 3.3M molecule-focused instruction-tuning dataset used by Mol-LLM during Stage 1; token codebook extended with ~3k SELFIES tokens",
            "application_domain": "backbone for molecular generation, property regression, reaction prediction, and captioning within Mol-LLM",
            "generation_method": "Autoregressive token generation producing SELFIES / textual outputs, conditioned on task instruction, SELFIES strings, and Q-Former graph tokens",
            "novelty_of_chemicals": "As backbone component, novelty is a function of Mol-LLM fine-tuning; no standalone novelty metrics reported for Mistral in paper",
            "application_specificity": "Extended token vocabulary (SELFIES, digits, symbols) and LoRA fine-tuning enable direct numeric regression outputs and heterogeneous task outputs (booleans, reaction routes)",
            "evaluation_metrics": "Used within Mol-LLM evaluations (EXACT, fingerprint FTS, validity, regression/classification metrics) rather than assessed standalone in this paper",
            "results_summary": "When used as Mol-LLM backbone (with multimodal inputs and MolPO), achieves strong generation and prediction results reported for Mol-LLM; paper does not report isolated Mistral-7B-Instruct performance separate from Mol-LLM",
            "comparison_to_other_methods": "Chosen for strong instruction-following capacity; combined with graph encoder and Q-Former outperforms other generalist approaches on multimodal tasks in this work",
            "limitations_and_challenges": "Backbone alone would ignore graph modality unless combined with GNN + Q-Former and MolPO; requires careful token-vocabulary extension and LoRA tuning for molecule tasks",
            "uuid": "e8910.1",
            "source_info": {
                "paper_title": "Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4 (baseline)",
            "name_full": "Gpt-4 technical report",
            "brief_description": "OpenAI's multimodal/large instruction-following model used as a few-shot (5-shot) baseline in property prediction and generation evaluations in this paper.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4 (5-shot evaluation as reported)",
            "model_type": "Large multimodal transformer LLM",
            "model_size": null,
            "training_data": "Not specified in this paper; used as an external black-box baseline with 5-shot prompts for some tasks",
            "application_domain": "baseline for property prediction and molecular generation/captioning tasks",
            "generation_method": "Prompt-based few-shot generation (5-shot) to produce textual or SMILES/SELFIES-like molecule descriptions",
            "novelty_of_chemicals": "Paper reports baseline performance (e.g., property RMSE/MAE, generation BLEU/fingerprint scores) but does not report explicit novelty statistics for GPT-4 outputs",
            "application_specificity": "Evaluated with few-shot prompts, not fine-tuned on the molecule instruction-tuning dataset used to train Mol-LLM",
            "evaluation_metrics": "Same primary metrics as Mol-LLM for each task (RMSE, MAE, ROC-AUC, EXACT, BLEU, fingerprint FTS, VALIDITY), reported in tables for comparison",
            "results_summary": "GPT-4 (5-shot) performs competitively on some property tasks but generally underperforms Mol-LLM on tasks where specialized multimodal training and graph utilization matter; used to contextualize Mol-LLM performance",
            "comparison_to_other_methods": "Used as a generalist LLM baseline; Mol-LLM surpasses GPT-4 on many molecular tasks in their benchmarks due to multimodal and domain-specific instruction tuning",
            "limitations_and_challenges": "Few-shot prompt evaluation limits chemical specificity; lacks explicit multimodal (graph) inputs and domain instruction-tuning in this comparison",
            "uuid": "e8910.2",
            "source_info": {
                "paper_title": "Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "LlaSMol",
            "name_full": "LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset",
            "brief_description": "A prior generalist molecular LLM trained on a large-scale (3.3M) instruction-tuning dataset (cited by the authors) used as a primary baseline; often strong in in-distribution tasks but Mol-LLM claims better OOD generalization via graph-aware training.",
            "citation_title": "Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset.",
            "mention_or_use": "use",
            "model_name": "LlaSMol",
            "model_type": "Instruction-tuned LLM (sequence-only) / transformer",
            "model_size": null,
            "training_data": "A large-scale chemical instruction-tuning dataset (3.3M examples as cited in the paper), sequence-only representations (SELFIES/SMILES)",
            "application_domain": "general molecular tasks (property prediction, reaction prediction, molecule generation, captioning)",
            "generation_method": "Sequence-only instruction-tuned generation of molecular strings conditioned on prompts (no explicit graph modality reported in this paper)",
            "novelty_of_chemicals": "Paper reports performance metrics (e.g., LlaSMol: strong in some in-distribution tasks and high fingerprint similarity on some reaction OOD tests) but does not provide explicit novelty percentages",
            "application_specificity": "Task-conditioned generation via instruction-tuning; lacks explicit structural graph modality used by Mol-LLM",
            "evaluation_metrics": "EXACT, fingerprint FTS, VALIDITY, regression/classification metrics when evaluated as a baseline",
            "results_summary": "LlaSMol is a strong generalist baseline and rivals Mol-LLM on several in-distribution benchmarks; Mol-LLM surpasses LlaSMol on out-of-distribution reaction and property tasks by leveraging graph modality and MolPO",
            "comparison_to_other_methods": "Compared directly in tables; Mol-LLM often outperforms LlaSMol on OOD benchmarks and on tasks where graph structure matters",
            "limitations_and_challenges": "Sequence-only approach may limit structural fidelity and OOD robustness compared to multimodal graph-aware models",
            "uuid": "e8910.3",
            "source_info": {
                "paper_title": "Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "BioT5+",
            "name_full": "BioT5+: Towards generalized biological understanding with IUPAC integration and multi-task tuning",
            "brief_description": "A semi-generalist sequence-only model evaluated in the paper; two separate BioT5+ variants exist (classification/translation and regression/reaction) and are used as baselines for multiple tasks including molecule generation and property prediction.",
            "citation_title": "Biot5+: Towards generalized biological understanding with iupac integration and multi-task tuning",
            "mention_or_use": "use",
            "model_name": "BioT5+ (two variants evaluated)",
            "model_type": "Transformer sequence-to-sequence model (T5-derived) with task-specific tuning",
            "model_size": null,
            "training_data": "Task-specific multi-task tuning datasets including IUPAC translation and molecular property/reaction datasets (as described in BioT5+ paper); used as semi-generalist baselines",
            "application_domain": "molecular property prediction, translation tasks, reaction prediction, generation",
            "generation_method": "Sequence-conditioned generation (no explicit graph modality in variants used here)",
            "novelty_of_chemicals": "Evaluated via standard metrics (EXACT, fingerprint FTS) in benchmarks; no explicit novelty percentages reported in this paper",
            "application_specificity": "Task-specific model variants tuned for classification/translation or regression/reaction tasks; not a single unified multimodal generalist model",
            "evaluation_metrics": "EXACT, fingerprint FTS, VALIDITY, regression/classification metrics (RMSE, ROC-AUC)",
            "results_summary": "BioT5+ shows competitive performance on some tasks but generally underperforms Mol-LLM on OOD and multimodal tasks where graph information is important; used to illustrate benefits of multimodal training",
            "comparison_to_other_methods": "BioT5+ (regression & react) is competitive on some reaction tasks but lacks Mol-LLM's graph usage and broad single-model generality",
            "limitations_and_challenges": "Two-model design (separate variants) limits cross-task generality; lacks graph modality which Mol-LLM uses to improve structural reasoning",
            "uuid": "e8910.4",
            "source_info": {
                "paper_title": "Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "InstructMol",
            "name_full": "InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "brief_description": "A prior multimodal molecular model that integrates 1D sequences and 2D graphs and is referenced as a baseline; the paper notes that naive SFT in such multimodal models can suffer graph-bypass unless preference optimization is applied.",
            "citation_title": "Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "mention_or_use": "mention",
            "model_name": "InstructMol",
            "model_type": "Multimodal transformer with 1D sequence and 2D graph inputs",
            "model_size": null,
            "training_data": "Multimodal instruction-tuning datasets (referenced Mol-Instructions, SMolInstruct)",
            "application_domain": "drug discovery assistant tasks including reaction prediction and generation",
            "generation_method": "Multimodal conditional generation using sequence + graph inputs under supervised fine-tuning (SFT)",
            "novelty_of_chemicals": "Not quantified in this paper; InstructMol used as a comparative baseline in evaluations",
            "application_specificity": "Designed to follow instructions across molecular tasks with multimodal inputs",
            "evaluation_metrics": "Used as baseline for generation and prediction metrics (EXACT, fingerprint FTS, ROC-AUC etc.)",
            "results_summary": "Referenced as existing multimodal approach; Mol-LLM claims improved graph utilization and generalization relative to InstructMol when using MolPO and GNN pre-training",
            "comparison_to_other_methods": "Paper contrasts Mol-LLM's MolPO to naive SFT training approaches used in prior multimodal models like InstructMol, which exhibited limited graph discrimination",
            "limitations_and_challenges": "Authors observe that naive SFT-trained multimodal models often fail to actually exploit graph structure (graph bypass phenomenon) without explicit preference objectives like MolPO",
            "uuid": "e8910.5",
            "source_info": {
                "paper_title": "Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "Translation between molecules and natural language",
            "brief_description": "A specialist sequence-to-sequence model designed for translation between molecular strings (SMILES/SELFIES) and natural language used as a specialist baseline for generation and captioning tasks.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "mention",
            "model_name": "MolT5",
            "model_type": "T5-based sequence-to-sequence transformer",
            "model_size": null,
            "training_data": "Task-specific molecule-text translation datasets (ChEBI-20 and related corpora cited in MolT5 paper)",
            "application_domain": "molecule-text translation, description-guided molecule generation, molecule captioning",
            "generation_method": "Sequence-to-sequence textual generation conditioned on descriptions to produce molecular strings",
            "novelty_of_chemicals": "Evaluated via EXACT, fingerprint FTS, VALIDITY in generation benchmarks; no explicit novelty percentages reported in this paper",
            "application_specificity": "Specialist design for translation/generation tasks yields strong performance on those tasks but lacks generalist scope across broad molecular tasks",
            "evaluation_metrics": "EXACT, MACCS FTS, VALIDITY, BLEU/ROUGE/METEOR for captioning and generation",
            "results_summary": "MolT5 is presented in tables as a specialist strong performer on generation/captioning tasks; Mol-LLM aims to match or exceed such specialist performance while being a single generalist model",
            "comparison_to_other_methods": "MolT5 performs well on generation/captioning (specialist) but does not cover the diverse task set that Mol-LLM targets",
            "limitations_and_challenges": "Specialist focus limits broader applicability across all molecular task groups",
            "uuid": "e8910.6",
            "source_info": {
                "paper_title": "Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mol-Instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2
        },
        {
            "paper_title": "Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset",
            "rating": 2
        },
        {
            "paper_title": "InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "rating": 2
        },
        {
            "paper_title": "MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter",
            "rating": 2
        },
        {
            "paper_title": "Biot5+: Towards generalized biological understanding with iupac integration and multi-task tuning",
            "rating": 2
        },
        {
            "paper_title": "GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text",
            "rating": 1
        },
        {
            "paper_title": "MolT5: Translation between molecules and natural language",
            "rating": 1
        },
        {
            "paper_title": "MolXPT: Wrapping molecules with text for generative pre-training",
            "rating": 1
        }
    ],
    "cost": 0.0187255,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization</h1>
<p>Chanhui Lee ${ }^{1}$, Hanbum $\mathrm{Ko}^{1}$, Yuheon Song ${ }^{2}$, Yongjun Jeong ${ }^{1}$<br>Rodrigo Hormazabal ${ }^{3,4}$, Sehui Han ${ }^{4}$, Kyunghoon Bae ${ }^{4}$, Sungbin Lim ${ }^{4,5}$, Sungwoong Kim ${ }^{1 *}$<br>${ }^{1}$ Department of Artificial Intelligence, Korea University<br>${ }^{2}$ Department of Artificial Intelligence, UNIST<br>${ }^{3}$ Kim Jaechul Graduate School of AI, KAIST<br>${ }^{4}$ LG AI Research<br>${ }^{5}$ Department of Statistics, Korea University</p>
<h4>Abstract</h4>
<p>Recent advances in large language models (LLMs) have led to models that tackle diverse molecular tasks, such as chemical reaction prediction and molecular property prediction. Large-scale molecular instruction-tuning datasets have enabled sequence-only (e.g., SMILES or SELFIES) generalist molecular LLMs, and researchers are now exploring multimodal approaches that incorporate molecular structural information for further gains. However, a genuinely multimodal, generalist LLM that covers a broad spectrum of molecular tasks has yet to be fully investigated. We observe that naive next token prediction training ignores graph-structural information, limiting an LLM's ability to exploit molecular graphs. To address this, we propose (i) Molecular structure Preference Optimization (MolPO), which facilitates graph usage by optimizing preferences between pairs of correct and perturbed molecular structures, and (ii) an advanced graph encoder with a tailored pre-training strategy to improve the effect of graph utilization by MolPO. Building on these contributions, we introduce Mol-LLM, the first multimodal generalist model that (a) handles a broad spectrum of molecular tasks among molecular LLMs, (b) explicitly leverages molecular-structure information, and (c) takes advantage of extensive instruction tuning. Mol-LLM attains state-of-the-art or comparable results across the most comprehensive molecular-LLM benchmark-even on out-of-distribution datasets for reaction and property prediction, where it surpasses prior generalist molecular LLMs by a large margin. ${ }^{2}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) [1-4] have been widely used to tackle diverse tasks across multiple domains, such as mathematics and code generation, by leveraging their broad knowledge base. This achievement has recently motivated interest in applying LLMs to diverse molecular tasks-including molecular property prediction, chemical reaction prediction, description-guided molecule generation, and molecule captioning-all of which are essential in drug discovery and materials science [5-12]. In particular, most molecular LLMs tend to leverage only one of the two key components for improved molecular language modeling, either molecular structure information or multitask instruction-tuning, rather than combining both. Several studies [8, 10, 11] have moved away from conventional molecular language modeling based on 1D sequence such as SMILES [13] or SELFIES [14], and have instead developed multimodal LLMs that incorporate 2D molecular graphs as an additional input modality, thereby representing molecular structures and topologies more faithfully while achieving</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (Left) Performance comparison among generalist molecular LLMs with normalized primary metrics. (Right) Graph utilization comparison between SFT and proposed multimodal training (MolPO). Score closer to 1 indicate better use of graph, approaching 0.5 indicate less utilization.</p>
<p>better performance across diverse molecular tasks [15–17]. Meanwhile, other studies [5–7, 9] have constructed instruction-tuning datasets for multiple molecular tasks and fine-tuned LLMs on these datasets. This approach enables the models to acquire transferable and generalizable knowledge, allowing them to understand and perform various tasks based on natural language instructions.</p>
<p>However, it is uncertain whether the multimodal molecular LLMs effectively use molecular structural information when trained with naive supervised fine-tuning (SFT). To investigate this, we compare the likelihoods of the original and perturbed molecules, comparing how well the SFT model is at proper graph discrimination. Figure 1 shows that the SFT model hardly distinguishes between them on most molecular tasks, indicating that its molecular graph utilization is generally limited. Moreover, despite the potential for synergistic performance improvements by molecular graph structure utilization and multitask instruction-tuning, few studies have fully harnessed the benefits of both approaches, especially for a universal molecular LLM. Specifically, some recent studies [9, 11, 12, 18, 19] have attempted to combine molecule graph structure information with instruction-tuning, however, their instruction-tuning focuses solely on task-specific fine-tuning.</p>
<p>In this paper, we propose a generalist molecular LLM, called Mol-LLM, that leverages multimodal molecule and extensive instruction-tuning, addressing the broadest range of molecular tasks. In particular, while maintaining multimodal LLM architecture based on Q-Former [20], we introduce a novel multimodal instruction-tuning based on Molecular structure Preference Optimization (MolPO), where the molecular LLM learns to optimize the molecular structural preferences between the pairs of the correct (chosen) molecular graph and the perturbed (rejected) molecular graph. By creating rejected molecular graphs based on the substructures for molecular feature perturbation, the proposed MolPO mitigates the tendency to overlook graph information on various molecular tasks. Additionally, to further increase the effect of molecular graph utilization by advanced representation on a wide variety of molecular distributions, we introduce a new graph neural network (GNN) pre-training strategy and architecture. The proposed GNN pre-training framework combines two objectives: (i) functional group prediction, which teaches the model to accurately distinguish functional groups—the features that largely determine molecular properties—and (ii) SELFIES reconstruction, which helps the model preserve the molecular structure details from the molecular graph. Upon GINE [21], adopted by prior multimodal molecular LLMs [8, 9], we incorporate a transformer-based GNN named TokenGT [22], to enhance the expressive power. The resulting Mol-LLM shows strong performance and demonstrably better graph utilization on our benchmarks across a broad range of molecular tasks. To the best of our knowledge, Mol-LLM is not only the first versatile generalist multimodal molecular LLM on a wide range of tasks with a single generalist model, but it also surpasses other generalist models: LlaSMol [5], ChemDFM [23], 3D-MoLM [12] on most benchmarks as shown in Figure 1, highlighting the power of graph modality synergized with extensive instruction-tuning.</p>
<p>In summary, our contributions are:</p>
<ol>
<li><strong>Mol-LLM</strong>. We present Mol-LLM, which sets a new state-of-the-art on both in-distribution and out-of-distribution molecular benchmarks relative to existing generalist models.</li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (Left) Overall structure of Mol-LLM. Molecular graph is encoded into a fixed-length token sequence by a hybrid graph encoder, followed by a Q-Former that outputs query embeddings to feed LLM, with corresponding task instruction and molecular 1D sequence. (Right) Representative downstream molecular tasks.</p>
<ol>
<li><strong>Enhanced graph utilization</strong>. To exploit 2D molecular graphs more effectively, we propose MolPO—a fine-tuning strategy that leverages perturbed molecules—alongside a GNN pre-training method and a hybrid graph encoder augmented with transformer architecture.</li>
<li><strong>Extensive instruction-tuning</strong>. We construct a large, molecule-focused instruction-tuning dataset and employ multimodal training to build a generalist model with significantly enhanced molecular understanding.</li>
</ol>
<h2>2 Mol-LLM: Multimodal Generalist Molecular Large Language Model</h2>
<p>This section introduces the model architecture, training strategy, and instruction-tuning dataset of Mol-LLM, a multimodal generalist molecular LLM. As depicted in Figure 2, Mol-LLM comprises a hybrid molecular graph encoder, a Q-Former for cross-modal projection between molecular graph and text, and a backbone LLM. Utilizing the multimodal framework, the LLM addresses molecular task instructions and 1D molecular sequences directly, while feeding 2D molecular graph embeddings to the LLM through the hybrid graph encoder and Q-Former. Such multimodal architectures are trained through three training stages, as depicted in Figure 3.</p>
<h3>2.1 Model Architecture</h3>
<p><strong>Hybrid Graph Encoder</strong> Previous studies on multimodal molecular LLMs using 2D molecular graphs [8, 9] have adopted the GINE architecture [21], since it captures local graphical structure efficiently. However, addressing diverse molecular tasks across various data distributions requires the ability to process large molecules as well. This consideration led us to the simultaneous usage of TokenGT [22] as a graph encoder, which is designed to enhance global context understanding and mitigate over-smoothing [24] in large graphs via a transformer architecture. For a 2D molecular graph <em>G</em> = (<em>V, E</em>), the GINE encoder <em>f<sup>G</sup></em> outputs a graph embedding <em>h<sup>G</sup><sub>g</sub></em> ∈ ℝ<sup>1×d<sub>g</sub></sup> and node embeddings <em>h<sup>G</sup><sub>u</sub></em> ∈ ℝ<sup>|V|×d<sub>g</sub></sup>, where <em>d<sub>g</sub></em> is the embedding dimension. Otherwise, the TokenGT encoder <em>f<sup>T</sup></em> outputs not only a graph embedding <em>h<sup>T</sup><sub>g</sub></em> ∈ ℝ<sup>1×d<sub>g</sub></sup> and node embeddings <em>h<sup>T</sup><sub>u</sub></em> ∈ ℝ<sup>|V|×d<sub>g</sub></sup>, but also edge embeddings <em>h<sup>T</sup><sub>u</sub></em> ∈ ℝ<sup>|E|×d<sub>g</sub></sup>. We then concatenate all the embeddings obtained by both encoders <em>h<sup>G</sup><sub>g</sub></em>, <em>h<sup>G</sup><sub>u</sub></em>, <em>h<sup>T</sup><sub>g</sub></em>, <em>h<sup>T</sup><sub>u</sub></em>, and <em>h<sup>T</sup><sub>u</sub></em> along the first dimension to obtain <em>h</em> ∈ ℝ<sup>(2|V|+|E|+2)×d<sub>g</sub></sup>, which is then used as the key for the Q-Former.</p>
<p><strong>Cross-modal Projector (Q-Former)</strong> Querying transformer (Q-Former) [20] is a modality-bridging transformer that converts the varying number of concatenated embeddings for each molecular graph into a fixed-length token sequence, enabling efficient batch processing. Specifically, structural information is distilled via cross-attention between <em>N<sub>q</sub></em> = 32 learnable query vectors <em>l</em> ∈ ℝ<sup>32×d<sub>q</sub></sup>, initialized randomly, and the concatenated molecular embeddings <em>h</em> ∈ ℝ<sup>(2|V|+|E|+2)×d<sub>g</sub></sup>, producing 32 tokens aligned with the text modality. The 32 tokens are concatenated with the task instruction as well as the SELFIES string before being fed to the LLM.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overview of the three training stages and the loss function used at each stage. The training pipeline consists of a pre-training phase (Stages 1 and 2), followed by a fine-tuning phase (Stage 3). In Stage 1, all modules are trained independently and in parallel, whereas in Stages 2 and 3, the modules are trained in a unified architecture and loss function.</p>
<p>Backbone Large Language Model We adopt Mistral-7B-Instruct-v0.3 [25] as a backbone LLM, following Yu et al. [5]. In order to improve the efficiency in solving molecular tasks, we extend the token codebook with the 3K SELFIES vocabulary from BioT5+ [6] and add dedicated tokens for the digits 0–9, the decimal point, and the negative sign, thereby enabling direct number prediction for regression tasks. Additional task-specific vocabulary covers boolean labels, textual descriptions, and reaction routes, allowing Mol-LLM to natively produce the heterogeneous answer formats required by downstream applications. Examples of these extra tokens appear on the right side of Figure 2.</p>
<h3>2.2 Multimodal Training</h3>
<p>Stage 1 - Graph Encoder and LLM Pre-training The hybrid graph encoder comprises two GNNs, GINE and TokenGT. We pre-train these two GNNs in parallel with the LLM. The GNN pre-training comprises two complementary tasks: functional group prediction and SELFIES reconstruction. Functional group prediction strengthens representations of the functional groups that govern molecular properties, whereas SELFIES reconstruction encourages the encoder to preserve global structural information. Both tasks share the same graph-level embedding $h_{g}$ produced by the GNN, as illustrated on the left side of Figure 4. After discarding extremely common or extremely rare functional groups, we retain $K=72$ distinct groups (dataset construction details are given in Appendix B.1). For functional group prediction, $h_{g}$ is passed through a three-layer MLP ($1024 \rightarrow 1024 \rightarrow 72$ ) $f_{\theta}^{\text{MLP}}$ and trained with the binary cross-entropy loss $\mathcal{L}<em k="1">{\text{func}}=-\sum</em>}^{K}\left(y_{\text{func}}^{(k)} \log f_{\theta}^{\text{MLP}}\left(h_{g}\right)^{(k)}+\left(1-\right.\right.$ $\left.\left.y_{\text{func}}^{(k)}\right) \log \left(1-f_{\theta}^{\text{MLP}}\left(h_{g}\right)^{(k)}\right)\right)$, where superscript $(k)$ is the value for functional group $k$. SELFIES reconstruction reuses $h_{g}$ as a context for a GPT-2 decoder $\pi_{\theta}^{\text{GPT-2}}$ that learns to reproduce molecule's SELFIES string $s$ : $\mathcal{L<em t="t">{\text{recon}}=-\sum</em>} \log \pi_{\theta}^{\text{GPT-2}}\left(s_{t} \mid h_{g}, s_{&lt;t}\right)$. The graph encoder is optimized with the combined loss $\mathcal{L<em _text_func="\text{func">{\text{GNN}}=\mathcal{L}</em>$. Additional training procedures and hyperparameters are provided in Appendix B.2.}}+\mathcal{L}_{\text{recon}</p>
<p>The LLM pre-training serves two purposes: (i) injecting molecule-specific prior knowledge and (ii) reducing the compute required during later multimodal training. Accordingly, we pre-train the LLM on exactly the same dataset that will be used later for fine-tuning, optimizing a token-level cross-entropy objective. Given a training instance consisting of a task instruction $q$, a molecular SELFIES string $s$, and a ground truth answer $y$, we minimize $\mathcal{L}<em t="t">{\text{SFT}}=-\sum</em>\right)$ where $t$ indexes tokens.} \log \pi_{\theta}^{\text{LLM}}\left(y_{t} \mid s, q, y_{&lt;t</p>
<p>Stage 2 - Q-Former Pre-training In Stage 2, only the Q-Former is updated, while both the GNN and the LLM remain frozen. Following Liu et al. [26], we simply reuse the fine-tuning dataset, in which molecular representations and natural language tokens appear in an interleaved format. For each training instance $(s, q, y)$, the SELFIES string $s$ is converted into its corresponding molecular graph $g$. The combined model $\pi_{\theta}$ (GNN+Q-Former+LLM) is then trained for one epoch with the loss defined as $\mathcal{L}<em t="t">{\text{SFT}}=-\sum</em>\right)$.} \log \pi_{\theta}\left(y_{t} \mid s, q, g, y_{&lt;t</p>
<p>Stage 3 - MolPO: Molecular Structure Preference Optimization We observed that using only SFT training as in conventional multimodal Molecular LLMs [8, 9, 19, 12], result in a graph bypass phenomenon (Figure 1) in solving molecular tasks. To resolve the graph bypass issue, we propose Molecular structure Preference Optimization (MolPO). Rather than simply inputting multimodal molecules into the LLM without consideration for multimodal utilization, MolPO promotes the practical utilization of multimodal molecules by learning the preferences between an original (chosen)</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (Left) Overview of the two graph pre-training tasks for the proposed hybrid graph encoder. Two distinct GNN backbones, GINE and TokenGT, are trained independently. (Right) Illustration of the MolPO training objective, which contrasts a chosen molecule with a rejected molecule.</p>
<p>Graph <em>g<sub>w</sub></em> and a perturbed (rejected) graph <em>g<sub>ℓ</sub></em>, which is inspired by mDPO [27]. Constructing <em>g<sub>ℓ</sub></em>, it is crucial to introduce perturbations that alter the relationship between the graph and the target. Since molecular features can generally be identified on a substructure basis, we substitute the substructures found in the original <em>g<sub>w</sub></em> as in Figure 4 right panel. This approach offers the advantage of being applicable to any molecular task without significant computational costs or requiring task-specific graph perturbation design (details in Appendix B.4). Based on the reward formulation $$r_{w,i} = \frac{\beta}{|y|} \sum_{t} \log \pi_{\theta}(y_t \mid g_w, s, q_i, y_{&lt;t})$$ and $$r_{\ell,i} = \frac{\beta}{|y|} \sum_{t} \log \pi_{\theta}(y_t \mid g_{\ell}, s, q_i, y_{&lt;t})$$ motivated from SimPO [28], the MolPO objective is defined as follows:</p>
<p>$$\mathcal{L}<em _mathcal_D="\mathcal{D" _s_="(s," _sim="\sim" g_="g," q_i_="q_i," y_="y)">{\text{MolPO}} = \mathbb{E}</em><em w_i="w,i">{\text{tr}}} [-\log \sigma(\min(r</em>$$} - r_{\ell,i}, \lambda_{\text{clip}} |r_{w,i}|) - \gamma_i)],\tag{1</p>
<p>where <em>λ<sub>clip</sub></em> is coefficient, and <em>D<sub>tr</sub></em> denotes training dataset. <em>γ<sub>i</sub></em> = <em>λ<sub>margin</sub></em>|E<sub>(<em>g<sub>w</sub></em>, <em>s</em>, <em>q<sub>i</sub>, y)</em>|<em>r<sub>w,i</sub></em>|| is a task-adaptive target reward margin for each <em>i</em>-th molecular task, calculated during training. The entire training objective combined is <em>L<sub>SFT</sub> + cL<sub>MolPO</sub></em>, where <em>c</em> is a constant.</p>
<p>In developing a generalist over diverse molecular tasks, we experimentally observed that adopting SimPO's task-agnostic target reward margin <em>γ</em> results in inappropriate log-sigmoid values due to highly variant reward orders of magnitude across tasks. However, modeling the task-specific reward scale as a hyperparameter is not an ideal solution either, as it adds an additional burden of hyperparameter search for each molecular task. Instead, we introduce a task-adaptive target reward margin with only a task-agnostic hyperparameter <em>λ<sub>margin</sub></em>, where the expectation is estimated using an exponential moving average during training.</p>
<p>In addition, given that it is generally easier to lower the rejected reward than to increase the true chosen reward, the preference reward margin can be empirically manipulated by simply reducing <em>r<sub>ℓ,i</sub></em> without a corresponding enhancement of <em>r<sub>w,i</sub></em>. To fully harness the benefits of preference optimization without the drawbacks, we introduce margin clipping to appropriately control the influence of the reward margin on parameter updates. Specifically, the margin is constrained so that it cannot exceed a fraction, <em>λ<sub>clip</sub></em> of |<em>r<sub>w,i</sub></em>|. Through this simple margin clipping, the model is prevented from the circumventing unintended effect of preference optimization by solely reducing the rejected reward. Further details of MolPO training are provided in Appendix B.5.</p>
<h3>2.3 Extensive Instruction-tuning Dataset</h3>
<p>The instruction-tuning dataset for Mol-LLM spans five major molecular task groups: property regression, property classification, reaction prediction, description-guided molecule generation, and molecule captioning. Property regression consists of five tasks—LogS for water solubility (ESOL [5]), LogD for lipophilicity (Lipo [5]), HOMO [7], LUMO [7], and HOMO-LUMO gap [7], and property classification comprises BACE [6], BBBP [5], ClinTox [5], HIV [5], SIDER [5]. Reaction prediction covers forward synthesis (FS), retrosynthesis (RS), and reagent prediction (RP), with FS and RS each divided into Mol-Instructions [7] and SMolInstruct [5] subsets according to their dataset sources. The description-guided molecule generation and molecule captioning tasks are similarly split into ChEBI-20 [29] and SMolInstruct based on their origins. In addition, to enhance the understanding of IUPAC [30]—frequently used in molecular text captions—we incorporate an</p>
<p>Table 1: Performance comparison on molecular property prediction tasks from the MoleculeNet [41] benchmark. A superscript * indicates results evaluated with an official checkpoint, and "NA" denotes cases where no official checkpoint is available. Boldface highlights the best scores among generalist models. For semi-generalist models, each variant is annotated with the task group on which it is trained. GPT-4 is evaluated with 5-shots, except for classification performances borrowed from Zhao et al. [23] with zero-shot.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">LogS</th>
<th style="text-align: center;">LogD</th>
<th style="text-align: center;">HOMO</th>
<th style="text-align: center;">LUMO</th>
<th style="text-align: center;">Gap</th>
<th style="text-align: center;">BACE</th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">ClusTos</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">SIDER</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">RMSE $(\downarrow)$</td>
<td style="text-align: center;">RMSE $(\downarrow)$</td>
<td style="text-align: center;">MAE $(\downarrow)$</td>
<td style="text-align: center;">MAE $(\downarrow)$</td>
<td style="text-align: center;">MAE $(\downarrow)$</td>
<td style="text-align: center;">ROC-AUC $(\uparrow)$</td>
<td style="text-align: center;">ROC-AUC $(\uparrow)$</td>
<td style="text-align: center;">ROC-AUC $(\uparrow)$</td>
<td style="text-align: center;">ROC-AUC $(\uparrow)$</td>
<td style="text-align: center;">ROC-AUC $(\uparrow)$</td>
</tr>
<tr>
<td style="text-align: center;">Specialist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SourceMol</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">0.0048</td>
<td style="text-align: center;">0.0050</td>
<td style="text-align: center;">0.0061</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">MolCA</td>
<td style="text-align: center;">$\geq 100$</td>
<td style="text-align: center;">$\geq 100$</td>
<td style="text-align: center;">$\geq 1$</td>
<td style="text-align: center;">$\geq 1$</td>
<td style="text-align: center;">$\geq 1$</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">63.0</td>
</tr>
<tr>
<td style="text-align: center;">MolXPT</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: center;">Semi-Generalist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mol-Instructures</td>
<td style="text-align: center;">4.81</td>
<td style="text-align: center;">$\geq 100$</td>
<td style="text-align: center;">0.0210</td>
<td style="text-align: center;">0.0210</td>
<td style="text-align: center;">0.0203</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">48.2</td>
</tr>
<tr>
<td style="text-align: center;">BioT5+’(Ch. \&amp; Trans.)</td>
<td style="text-align: center;">$\geq 100$</td>
<td style="text-align: center;">$\geq 100$</td>
<td style="text-align: center;">$\geq 1$</td>
<td style="text-align: center;">$\geq 1$</td>
<td style="text-align: center;">$\geq 1$</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">43.7</td>
</tr>
<tr>
<td style="text-align: center;">BioT5+’(Reg. \&amp; React.)</td>
<td style="text-align: center;">$\geq 100$</td>
<td style="text-align: center;">$\geq 100$</td>
<td style="text-align: center;">0.0022</td>
<td style="text-align: center;">0.0024</td>
<td style="text-align: center;">0.0028</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">52.5</td>
</tr>
<tr>
<td style="text-align: center;">Generalist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (5-shot)</td>
<td style="text-align: center;">1.68</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">0.0227</td>
<td style="text-align: center;">0.0462</td>
<td style="text-align: center;">0.0395</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">40.5</td>
</tr>
<tr>
<td style="text-align: center;">Galactica</td>
<td style="text-align: center;">4.34</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">0.2329</td>
<td style="text-align: center;">0.0413</td>
<td style="text-align: center;">0.2497</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">55.9</td>
</tr>
<tr>
<td style="text-align: center;">3D-Mol.M ${ }^{+}$</td>
<td style="text-align: center;">3.41</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;">0.0299</td>
<td style="text-align: center;">0.0536</td>
<td style="text-align: center;">0.0673</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">49.7</td>
</tr>
<tr>
<td style="text-align: center;">ChemDPM ${ }^{+}$</td>
<td style="text-align: center;">8.19</td>
<td style="text-align: center;">6.21</td>
<td style="text-align: center;">0.1204</td>
<td style="text-align: center;">0.1262</td>
<td style="text-align: center;">0.1694</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">51.0</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol ${ }^{+}$</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">$\geq 1$</td>
<td style="text-align: center;">$\geq 1$</td>
<td style="text-align: center;">$\geq 1$</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">78.4</td>
</tr>
<tr>
<td style="text-align: center;">Mol-LLM (w/o Graph)</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.0044</td>
<td style="text-align: center;">0.0043</td>
<td style="text-align: center;">0.0055</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">76.1</td>
</tr>
<tr>
<td style="text-align: center;">Mol-LLM</td>
<td style="text-align: center;">1.28</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.0044</td>
<td style="text-align: center;">0.0043</td>
<td style="text-align: center;">0.0054</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">76.3</td>
</tr>
</tbody>
</table>
<p>IUPAC and SELFIES translation dataset [5] to construct an 3.3M extensive instruction-tuning dataset (details in Appendix C.1).</p>
<h1>3 Experiments</h1>
<h3>3.1 Experimental Setup</h3>
<p>Baseline Models We group the molecular LLMs compared with Mol-LLM into three broad categories. Specialist models are trained for a single molecular task; semi-generalist models cover a specific task group within one model but do not span all task groups; and generalist models are designed to handle every molecular task group. Representative examples are MolCA [8] for the specialist category, BioT5+ [6] for the semi-generalist category, and Galactica [31] and LlaSMol [5] for the generalist category. Comprehensive details on all baseline models can be found in Appendix D.2.</p>
<p>Evaluation Benchmark In addition to the molecular tasks described in Section 2.3, we evaluate molecular LLM robustness to out-of-distribution (OOD) by proposing two evaluation benchmarks. For LogS prediction, we retain high-confidence solubility labels from AqSol [32], exclude every molecule that also appears in ESOL, and collect molecules of high consistency among labels to construct the OOD evaluation versus ESOL. For reaction prediction, we gather 23K FS and 59K RS data instances from the ORDerly [33] repository except USPTO [34], apply a scaffold split to remove motif overlap with Mol-Instructions [7] and SMolInstruct [5], and reserve 5K examples for evaluation in each task. Full OOD dataset construction details are provided in Appendix C.2.</p>
<p>Evaluation Metrics For property prediction tasks, we report the root mean squared error (RMSE) or mean absolute error (MAE) in regression, and in classification tasks, receiver operating characteristic area under the curve (ROC-AUC) using the predicted probability of the positive class (i.e., True token). For reaction prediction and description-guided molecule generation, we evaluate exact match with the target molecule (EXACT), textual similarity (BLEU) [35], molecular fingerprint similarity based on RDKit [36], MACCS keys [37], and Morgan [38] fingerprints (RDK FTS, MACCS FTS, and MORGAN FTS, respectively), and the proportion of generated molecules that are chemically valid (VALIDITY). For the molecule captioning task, we measure similarity between the generated and reference descriptions using BLEU-2, BLEU-4, ROUGE-1 [39], ROUGE-2, ROUGE-L, and METEOR [40]. For a comprehensive assessment of the molecular LLM, we analyze a wide range of metrics for each molecular task. However, due to space limitations, the main paper reports only the primary metrics. The complete results are provided in Appendix D.3).</p>
<h3>3.2 Results</h3>
<p>Property Regression and Classification Table 1 summarizes the property regression and classification results. On most tasks, Mol-LLM outperforms every other generalist model, except for LogS,</p>
<p>Table 2: Performance comparison for reaction prediction tasks on Mol-Instructions [7] and SMoIInstruct [5] datasets.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Mol-Instructions / SMoIInstruct</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Task</td>
<td>Forward Synthesis</td>
<td></td>
<td>Retrosynthesis</td>
<td></td>
<td>Reagent Prediction</td>
<td></td>
</tr>
<tr>
<td>Metric</td>
<td>EXACT (↑)</td>
<td>MACCS FTS (↑)</td>
<td>EXACT (↑)</td>
<td>MACCS FTS (↑)</td>
<td>EXACT (↑)</td>
<td>MACCS FTS (↑)</td>
</tr>
<tr>
<td>Specialist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>InstructMol</td>
<td>0.536 / NA</td>
<td>0.878 / NA</td>
<td>0.407 / NA</td>
<td>0.852 / NA</td>
<td>0.129</td>
<td>0.539</td>
</tr>
<tr>
<td>MolCA</td>
<td>0.000 / 0.000</td>
<td>0.494 / 0.357</td>
<td>0.000 / 0.000</td>
<td>0.880 / 0.760</td>
<td>0.000</td>
<td>0.115</td>
</tr>
<tr>
<td>Semi-Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mol-Instructions</td>
<td>0.052 / 0.003</td>
<td>0.291 / 0.184</td>
<td>0.069 / 0.015</td>
<td>0.359 / 0.285</td>
<td>0.044</td>
<td>0.364</td>
</tr>
<tr>
<td>BioT5+ (Cls. &amp; Trans.)</td>
<td>0.000 / 0.000</td>
<td>0.152 / 0.187</td>
<td>0.001 / 0.000</td>
<td>0.195 / 0.170</td>
<td>0.000</td>
<td>0.056</td>
</tr>
<tr>
<td>BioT5+ (Reg. &amp; React.)</td>
<td>0.864 / 0.081</td>
<td>0.975 / 0.537</td>
<td>0.642 / 0.152</td>
<td>0.930 / 0.751</td>
<td>0.257</td>
<td>0.621</td>
</tr>
<tr>
<td>Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4 (5-shot)</td>
<td>0.021 / 0.011</td>
<td>0.728 / 0.634</td>
<td>0.012 / 0.013</td>
<td>0.716 / 0.686</td>
<td>0.000</td>
<td>0.228</td>
</tr>
<tr>
<td>Galactica</td>
<td>0.000 / 0.000</td>
<td>0.257 / 0.377</td>
<td>0.000 / 0.000</td>
<td>0.274 / 0.447</td>
<td>0.000</td>
<td>0.127</td>
</tr>
<tr>
<td>3D-MoLM</td>
<td>0.000 / 0.000</td>
<td>0.391 / 0.296</td>
<td>0.000 / 0.000</td>
<td>0.451 / 0.372</td>
<td>0.000</td>
<td>0.218</td>
</tr>
<tr>
<td>ChemDFM</td>
<td>0.000 / 0.002</td>
<td>0.142 / 0.178</td>
<td>0.000 / 0.000</td>
<td>0.440 / 0.443</td>
<td>0.000</td>
<td>0.099</td>
</tr>
<tr>
<td>LlaSMol</td>
<td>0.743 / 0.629</td>
<td>0.955 / 0.919</td>
<td>0.453 / 0.323</td>
<td>0.885 / 0.827</td>
<td>0.000</td>
<td>0.199</td>
</tr>
<tr>
<td>Mol-LLM (w/o Graph)</td>
<td>0.893 / 0.584</td>
<td>0.983 / 0.904</td>
<td>0.510 / 0.363</td>
<td>0.886 / 0.828</td>
<td>0.202</td>
<td>0.586</td>
</tr>
<tr>
<td>Mol-LLM</td>
<td>0.911 / 0.601</td>
<td>0.987 / 0.908</td>
<td>0.538 / 0.377</td>
<td>0.893 / 0.832</td>
<td>0.225</td>
<td>0.600</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance comparison for molecule generation and molecule captioning on ChEBI-20 [29] and SMoIInstruct [5] datasets.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>ChEBI-20 / SMoIInstruct</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Task</td>
<td>Molecule Generation</td>
<td></td>
<td></td>
<td></td>
<td>Molecule Captioning</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Metric</td>
<td>EXACT (↑)</td>
<td>MACCS FTS (↑)</td>
<td>VALIDITY (↑)</td>
<td>BLEU-4 (↑)</td>
<td>ROUGE-L (↑)</td>
<td>METEOR (↑)</td>
<td></td>
</tr>
<tr>
<td>Specialist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GIT-Mol</td>
<td>0.051 / NA</td>
<td>0.738 / NA</td>
<td>0.93 / NA</td>
<td>0.263 / NA</td>
<td>0.560 / NA</td>
<td>0.533 / NA</td>
<td></td>
</tr>
<tr>
<td>InstructMol</td>
<td>NA / NA</td>
<td>NA / NA</td>
<td>NA / NA</td>
<td>0.371 / NA</td>
<td>0.502 / NA</td>
<td>0.509 / NA</td>
<td></td>
</tr>
<tr>
<td>MolT5</td>
<td>0.311 / 0.317</td>
<td>0.834 / 0.879</td>
<td>0.91 / 0.95</td>
<td>0.508 / 0.366</td>
<td>0.594 / 0.501</td>
<td>0.614 / 0.515</td>
<td></td>
</tr>
<tr>
<td>MolCA</td>
<td>NA / NA</td>
<td>NA / NA</td>
<td>NA / NA</td>
<td>0.540 / 0.510</td>
<td>0.631 / 0.604</td>
<td>0.652 / 0.628</td>
<td></td>
</tr>
<tr>
<td>MolXPT</td>
<td>0.215 / NA</td>
<td>0.859 / NA</td>
<td>0.98 / NA</td>
<td>0.505 / NA</td>
<td>0.597 / NA</td>
<td>0.626 / NA</td>
<td></td>
</tr>
<tr>
<td>Text+Chem T5</td>
<td>0.322 / NA</td>
<td>0.901 / NA</td>
<td>0.94 / NA</td>
<td>0.542 / NA</td>
<td>0.622 / NA</td>
<td>0.648 / NA</td>
<td></td>
</tr>
<tr>
<td>Semi-Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mol-Instructions</td>
<td>0.016 / 0.045</td>
<td>0.167 / 0.475</td>
<td>1.00 / 1.00</td>
<td>0.171 / 0.020</td>
<td>0.289 / 0.217</td>
<td>0.271 / 0.124</td>
<td></td>
</tr>
<tr>
<td>BioT5+ (Cls. &amp; Trans.)</td>
<td>0.557 / 0.519</td>
<td>0.907 / 0.897</td>
<td>1.00 / 1.00</td>
<td>0.591 / 0.582</td>
<td>0.649 / 0.644</td>
<td>0.680 / 0.677</td>
<td></td>
</tr>
<tr>
<td>BioT5+ (Reg. &amp; React.)</td>
<td>0.537 / 0.416</td>
<td>0.897 / 0.867</td>
<td>1.00 / 1.00</td>
<td>0.216 / 0.221</td>
<td>0.364 / 0.364</td>
<td>0.323 / 0.321</td>
<td></td>
</tr>
<tr>
<td>Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4 (5-shot)</td>
<td>0.092 / 0.027</td>
<td>0.745 / 0.726</td>
<td>0.65 / 0.74</td>
<td>0.158 / 0.125</td>
<td>0.303 / 0.273</td>
<td>0.320 / 0.274</td>
<td></td>
</tr>
<tr>
<td>Galactica</td>
<td>0.000 / 0.000</td>
<td>0.264 / 0.271</td>
<td>0.70 / 0.61</td>
<td>0.000 / 0.000</td>
<td>0.006 / 0.006</td>
<td>0.004 / 0.005</td>
<td></td>
</tr>
<tr>
<td>3D-MoLM</td>
<td>0.000 / 0.000</td>
<td>0.000 / 0.000</td>
<td>0.00 / 0.00</td>
<td>0.171 / 0.167</td>
<td>0.287 / 0.285</td>
<td>0.326 / 0.329</td>
<td></td>
</tr>
<tr>
<td>ChemDFM</td>
<td>0.018 / 0.041</td>
<td>0.165 / 0.297</td>
<td>0.19 / 0.13</td>
<td>0.031 / 0.035</td>
<td>0.101 / 0.108</td>
<td>0.078 / 0.085</td>
<td></td>
</tr>
<tr>
<td>LlaSMol</td>
<td>0.274 / 0.180</td>
<td>0.871 / 0.845</td>
<td>0.95 / 0.93</td>
<td>0.333 / 0.328</td>
<td>0.464 / 0.465</td>
<td>0.466 / 0.470</td>
<td></td>
</tr>
<tr>
<td>Mol-LLM (w/o Graph)</td>
<td>0.431 / 0.362</td>
<td>0.903 / 0.888</td>
<td>1.00 / 1.00</td>
<td>0.482 / 0.477</td>
<td>0.509 / 0.490</td>
<td>0.587 / 0.585</td>
<td></td>
</tr>
<tr>
<td>Mol-LLM</td>
<td>0.443 / 0.368</td>
<td>0.906 / 0.887</td>
<td>1.00 / 0.99</td>
<td>0.493 / 0.482</td>
<td>0.439 / 0.433</td>
<td>0.599 / 0.589</td>
<td></td>
</tr>
</tbody>
</table>
<p>ClinTox, and SIDER. Notably, even Mol-LLM (w/o Graph) performs on a par with the full model. We attribute this behavior to the small molecular sizes in MoleculeNet [41], which allow the LLM to infer structural information directly from the SELFIES representation.</p>
<p>Reaction Prediction The reaction prediction results are reported in Table 2. Except for the FS task of SMoIInstruct dataset, Mol-LLM again leads all generalist models. Since successful reaction prediction depends on recognizing which functional groups can participate during a chemical reaction, these results suggest that pre-training of the GNN on functional group prediction helps Mol-LLM exploit structural cues more effectively. Consistent with this interpretation, omitting the graph input (w/o Graph variant) noticeably degrades performances.</p>
<p>Description-guided Molecule Generation Table 3 shows the results for description-guided molecule generation, whose input prompts contain no molecular graphs. Since both Mol-LLM and the w/o Graph variant receive identical inputs, their scores are nearly indistinguishable. This confirms that Mol-LLM's ability to use graphs does not impede its instruction-following ability when</p>
<p>Table 4: Evaluation of OOD generalization for reaction prediction on the ORDerly dataset, which is non-USPTO, and LogS on the AqSol dataset.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>AqSol</th>
<th>ORDerly</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Task</td>
<td>LogS</td>
<td>Forward Synthesis</td>
<td></td>
<td></td>
<td>Retrosynthesis</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Metric</td>
<td>RMSE $(\downarrow)$</td>
<td>EXACT $(\uparrow)$</td>
<td>MACCS FTS $(\uparrow)$</td>
<td>VALIDITY $(\uparrow)$</td>
<td>EXACT $(\uparrow)$</td>
<td>MACCS FTS $(\uparrow)$</td>
<td>VALIDITY $(\uparrow)$</td>
<td></td>
</tr>
<tr>
<td>Semi-Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BioT5+ ${ }^{\mathrm{T}}$ (Reg. \&amp; React.)</td>
<td>1.81</td>
<td>0.095</td>
<td>0.628</td>
<td>1.00</td>
<td>0.139</td>
<td>0.678</td>
<td>1.00</td>
<td></td>
</tr>
<tr>
<td>Generalist Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4</td>
<td>2.17</td>
<td>0.000</td>
<td>0.723</td>
<td>0.87</td>
<td>0.000</td>
<td>0.672</td>
<td>0.65</td>
<td></td>
</tr>
<tr>
<td>Galactica ${ }^{\mathrm{a}}$</td>
<td>3.20</td>
<td>0.000</td>
<td>0.322</td>
<td>0.49</td>
<td>0.000</td>
<td>0.398</td>
<td>0.38</td>
<td></td>
</tr>
<tr>
<td>3D-MoLM ${ }^{\mathrm{a}}$</td>
<td>2.72</td>
<td>0.000</td>
<td>0.288</td>
<td>0.01</td>
<td>0.000</td>
<td>0.396</td>
<td>0.01</td>
<td></td>
</tr>
<tr>
<td>ChemDFM ${ }^{\mathrm{a}}$</td>
<td>6.98</td>
<td>0.017</td>
<td>0.428</td>
<td>0.04</td>
<td>0.000</td>
<td>0.406</td>
<td>0.05</td>
<td></td>
</tr>
<tr>
<td>LlaSMol ${ }^{\mathrm{a}}$</td>
<td>1.32</td>
<td>0.350</td>
<td>0.881</td>
<td>1.00</td>
<td>0.473</td>
<td>0.875</td>
<td>0.99</td>
<td></td>
</tr>
<tr>
<td>Mol-LLM (w/o Graph)</td>
<td>1.10</td>
<td>0.394</td>
<td>0.900</td>
<td>1.00</td>
<td>0.727</td>
<td>0.936</td>
<td>1.00</td>
<td></td>
</tr>
<tr>
<td>Mol-LLM</td>
<td>1.02</td>
<td>0.401</td>
<td>0.877</td>
<td>1.00</td>
<td>0.738</td>
<td>0.939</td>
<td>1.00</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: An ablation study on MolPO's effect on graph utilization. We report RMSE $(\downarrow)$ for LogS and LogD, and EXACT $(\uparrow)$ for FS, RS, RP, and T2M, each representing forward reaction prediction, retrosynthesis, reagent prediction, and molecule generation. "Mol-Inst." and "SMol." denote the Mol-Instructions and SMolInstruct datasets, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LogS</th>
<th style="text-align: center;">LogD</th>
<th style="text-align: center;">FS (Mol-Inst.)</th>
<th style="text-align: center;">FS (SMol.)</th>
<th style="text-align: center;">RS (Mol-Inst.)</th>
<th style="text-align: center;">RS (SMol.)</th>
<th style="text-align: center;">RP (Mol-Inst.)</th>
<th style="text-align: center;">T2M (ChEBI-20)</th>
<th style="text-align: center;">T2M (SMol.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mol-LLM (w/o MolPO)</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.598</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.355</td>
</tr>
<tr>
<td style="text-align: left;">Mol-LLM</td>
<td style="text-align: center;">$\mathbf{1 . 2 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 1 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 6 8}$</td>
</tr>
</tbody>
</table>
<p>graphs are absent. On both the ChEBI-20 and SMolInstruct datasets, Mol-LLM nonetheless achieves the best results among generalist models.</p>
<p>Molecule Captioning As summarized in Table 3, Mol-LLM again surpasses all baselines. Compared with the w/o Graph variant, the full model obtains consistently higher BLEU and METEOR scores but slightly lower ROUGE scores on both ChEBI-20 and SMolInstruct. The pattern implies that Mol-LLM produces more concise captions: it captures the essential information while omitting peripheral details. We believe MolPO training encourages the model to rely on structural cues and focus on the core content.</p>
<p>Generalization Performance on Out-of-distribution Datasets Table 4 reports OOD results for AqSol. On the in-distribution training tasks (LogS and SIDER), Mol-LLM lags the generalist baseline LlaSMol only marginally. In contrast, it is markedly superior on the OOD AqSol benchmark, demonstrating stronger generalization. A similar trend appears in the reaction prediction FS and RS tasks: Mol-LLM is slightly weaker on in-distribution FS of SMolInstruct but outperforms competitors when evaluated OOD. These findings indicate that MolPO training confers broader generalization across both tasks and input distributions, whereas the semi-generalist BioT5+, which lacks large-scale instruction tuning, suffers a notable drop in performance.</p>
<h1>3.3 Ablation Study</h1>
<p>MolPO objective enhances molecular graph utilization and task performance. To examine whether incorporating the MolPO objective $\mathcal{L}<em i="i" w_="w,">{\text {MolPO }}$ during Mol-LLM training leads the model to exploit molecular graph information more effectively than training with SFT alone, we first compare, for each task $i$, the log-likelihood $r</em>}$ obtained when the model is given the chosen graph $g_{w}$ to the log-likelihood $r_{\ell, i}$ obtained when it is given the rejected graph $g_{\ell}$. We then compute the graph discrimination ratio $\operatorname{GDR}=\frac{1}{N_{i}} \sum_{n=1}^{N_{i}}\left|\left[r_{w, i}(n)&gt;r_{\ell, i}(n)\right]\right.$, where $N_{i}$ is the number of instances in task $i$, $\mathbb{I}$ is the indicator function, and $r_{w, i}(n)$ is the log-likelihood for the $n$-th instance in task $i$. A GDR close to 1 indicates that the model can clearly identify the correct molecular graph (i.e., it effectively exploits molecular graph information); a value near 0.5 indicates random guessing, and a value near 0 indicates systematic confusion. Figure 1 shows the per-task GDRs-green bars for MolPO-trained models and orange bars for models trained without $\mathcal{L<em _MolPO="{MolPO" _text="\text">{\text {MolPO }}$. The consistently higher GDRs in the MolPO setting confirm that this objective helps the model make better use of molecular graph information. We also compare the multitask fine-tuning performance obtained when $\mathcal{L}</em>}}$ is combined with $\mathcal{L<em _SFT="{SFT" _text="\text">{\text {SFT }}$ to that obtained when only $\mathcal{L}</em>$ is used. As shown in Table 5, leveraging the graph modality through the MolPO objective improves performances on most tasks.}</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of fine-tuning performances on three tasks under different GNN architectures and initialization with (or without) pre-trained parameters. Each line is labeled as ${$ GNN archi-tecture $}-{$ initialization $}$. Ours refers to models initialized with parameters obtained via our GNN pre-training method, whereas Scratch denotes models trained from random initialization. The x-axis denotes the number of training steps, and the y-axis shows the corresponding evaluation metric.</p>
<p>Our GNN pre-training improves molecular representation. To clearly demonstrate the effect of our GNN pre-training method, we frame the experiment as a single task setting and modify Mol-LLM so that, during fine-tuning, it receives only the task instruction and the 2D molecular graph as inputs, omitting the 1D sequence. The model is trained solely using the loss term $\mathcal{L}_{\text {SFT }}$, and its performance is then compared with different GNN architectures and weight initializations. Figure 5 presents the learning curves for property regression (HOMO) and reaction prediction (FS, RP). The GNN architectures (GINE, TokenGT) and their corresponding initializations are represented as ${$ GNN architecture $}-{$ initialization $}$. Scratch indicates that the GNN is trained from scratch without any pretrained weights. The model whose GNN is initialized with the proposed pre-training method (Ours) consistently outperforms the others, indicating that it learns higher quality molecular representations. Moreover, the existing pre-trained models-MoleculeSTM [15] and GraphCL [42]-perform either worse than or roughly on par with the non-pretrained baseline Scratch, which is a surprising outcome.</p>
<h1>4 Related Works</h1>
<p>Molecular Large Language Models MolT5 [29] extends T5 [43] to bidirectional translation between SMILES strings and natural language, whereas MolXPT [44], built on the GPT architecture [45], unifies text-molecule translation with property prediction. MolCA [8] and GIT-Mol [10] fuse 2D molecular graphs with text via a Q-Former [20], while MolLM [46] further injects 3D geometric cues. UniMoT [11] discretizes Q-Former outputs into graph tokens while 3D-MolT5 [19] introduces 3D structure tokens, enabling generative reasoning over conformers. Although these models exploit molecule structures, each is tailored to a narrow set of tasks. Mol-LLM tackles this limitation by jointly processing text and graphs and by performing translation, prediction, and generation within a single generalist framework.</p>
<p>Instruction-tuning on Molecular Tasks Mol-Instructions [7] introduced the first broad instructiontuning corpus, inspiring InstructMol [9] to fine-tune multimodal models with task-specific prompts and BioT5+ [6] to perform multitask tuning without structural inputs. LlaSMol [5] scales the idea to 3.3 M examples across ten tasks, yielding a single model that matches-or exceeds-specialists. Subsequent work, including UniMoT [11], 3D-MolT5 [19] and 3D-MoLM, couples instruction tuning with 2D/3D structure encoders, yet still lacks a systematic strategy for exploiting multimodal inputs. Consequently, models remain sensitive to task distribution shifts. Mol-LLM fills this gap by unifying instruction tuning with structure-aware training, thereby improving robustness across in-distribution and out-of-distribution tasks.</p>
<p>Preference Optimization on Different Modality DPO [47] aligns language models with human preferences by maximizing the log-probability gap between preferred and rejected outputs; SimPO [28] removes the expensive reference model for lighter training. As multimodal LLMs rise, mDPO [27] adapts the idea to vision-language models by corrupting images to build preference pairs, and numerous follow-ups [48-51] confirm its effectiveness. Yet no study has demonstrated comparable gains for molecular data. Mol-LLM is the first to apply preference optimization to molecular</p>
<p>graphs and text jointly, showing that structure-aware preferences yield stronger generalization than sequence-only tuning while keeping training costs manageable.</p>
<h1>5 Conclusion</h1>
<p>We introduced MolPO, a multimodal training objective that leverages perturbed molecules to enhance the utility of 2D molecular graphs, together with a hybrid graph encoder pre-training strategy. We also curated a large-scale molecule instruction tuning dataset and, using the proposed methods, developed Mol-LLM, a multimodal generalist molecular large language model. Mol-LLM achieved state-of-theart performances among generalist molecular models on property regression, property classification, reaction prediction, description-guided molecule generation, and molecule captioning tasks. We believe our approach can be extended beyond 2D molecular graphs to incorporate 3D structural information and molecular metadata, enabling real-world applications such as drug discovery and novel material discovery. A detailed discussion of the limitations are described in Appendix A.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>LG AI Research supported this work. This work was also supported by Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT(MSIT, Korea)\&amp;Gwangju Metropolitan City, the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. RS-2024-00410082), Institute of Information communications Technology Planning Evaluation (IITP) grant funded by the Korea government(MSIT) (No. RS-2019-II190079, Artificial Intelligence Graduate School Program(Korea University); No.RS-2020-II201336, Artificial Intelligence Graduate School Program(UNIST); No. 2022-0-00612, Geometric and Physical Commonsense Reasoning based Behavior Intelligence for Embodied AI), and partly supported by the Institute of Information \&amp; Communications Technology Planning \&amp; Evaluation(IITP)-ITRC(Information Technology Research Center) grant funded by the Korea government(MSIT)(IITP-2025-RS-2024-00436857, 15\%)</p>
<h2>References</h2>
<p>[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2023.
[2] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
[3] Gemini Team Google. Gemini: A family of highly capable multimodal models. ArXiv, abs/2312.11805, 2023.
[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288.
[5] Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, and Huan Sun. Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. ArXiv, abs/2402.09391, 2024. URL https://api.semanticscholar.org/ CorpusID:267657622.</p>
<p>[6] Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, and Rui Yan. Biot5+: Towards generalized biological understanding with iupac integration and multi-task tuning. ArXiv, abs/2402.17810, 2024. URL https://api.semanticscholar. org/CorpusID:268041632.
[7] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. ArXiv, abs/2306.08018, 2023. URL https://api.semanticscholar. org/CorpusID:259164901.
[8] Zhiyuan Liu, Sihang Li, Yancheng Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. ArXiv, abs/2310.12798, 2023. URL https://api.semanticscholar. org/CorpusID:264306303.
[9] He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. ArXiv, abs/2311.16208, 2023. URL https://api.semanticscholar.org/CorpusID:265466509.
[10] Pengfei Liu, Yiming Ren, Jun Tao, and Zhixiang Ren. Git-mol: A multi-modal large language model for molecular science with graph, image, and text. Computers in Biology and Medicine, 171:108073, March 2024. ISSN 0010-4825. doi: 10.1016/j.compbiomed.2024.108073. URL http://dx.doi.org/10.1016/j.compbiomed.2024.108073.
[11] Juzheng Zhang, Yatao Bian, Yongqiang Chen, and Quanming Yao. Unimot: Unified moleculetext language model with discrete token representation, 2024. URL https://arxiv.org/ abs/2408.00863.
[12] Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, and Qi Tian. 3d-molm: Towards 3d molecule-text interpretation in language models. In $I C L R, 2024$.
[13] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28(1):31-36, feb 1988. ISSN 0095-2338. doi: 10.1021/ci00057a005. URL https://doi.org/10.1021/ci00057a005.
[14] Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan AspuruGuzik. Self-referencing embedded strings (selfies): A 100string representation. Machine Learning: Science and Technology, 1(4):045024, October 2020. ISSN 2632-2153. doi: 10.1088/2632-2153/aba947. URL http://dx.doi.org/10.1088/2632-2153/aba947.
[15] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing, 2024. URL https://arxiv.org/abs/2212.10789.
[16] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence, 4:279-287, 2022.
[17] Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen. A molecular multimodal foundation model associating molecule graphs with natural language, 2022. URL https://arxiv.org/abs/2209.05481.
[18] Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie. Drugchat: Towards enabling chatgptlike capabilities on drug molecule graphs, 2023. URL https://arxiv.org/abs/2309. 03907 .
[19] Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, and Rui Yan. 3d-molt5: Towards unified 3d molecule-text modeling with 3d molecular tokenization, 2024. URL https://arxiv.org/ abs/2406.05797.
[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models, 2023. URL https: //arxiv.org/abs/2301.12597.</p>
<p>[21] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations, 2020.
[22] Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners. Advances in Neural Information Processing Systems, 35:14582-14595, 2022.
[23] Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Yi Xia, Bo Chen, Hongshen Xu, Zichen Zhu, Su Zhu, et al. Chemdfm: A large language foundation model for chemistry. arXiv preprint arXiv:2401.14818, 2024.
[24] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.
[25] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825.
[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485, 2023. URL https://api.semanticscholar.org/CorpusID: 258179774 .
[27] Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. mdpo: Conditional preference optimization for multimodal large language models. ArXiv, abs/2406.11839, 2024. URL https://api.semanticscholar.org/CorpusID: 270560448 .
[28] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. ArXiv, abs/2405.14734, 2024. URL https://api.semanticscholar. org/CorpusID:269983560.
[29] Carl N. Edwards, T. Lai, Kevin Ros, Garrett Honke, and Heng Ji. Translation between molecules and natural language. ArXiv, abs/2204.11817, 2022. URL https://api.semanticscholar. org/CorpusID:248376906.
[30] Henri A Favre and Warren H Powell. Nomenclature of organic chemistry: IUPAC recommendations and preferred names 2013. Royal Society of Chemistry, 2013.
[31] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony S. Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. ArXiv, abs/2211.09085, 2022. URL https://api.semanticscholar. org/CorpusID:253553203.
[32] Murat Cihan Sorkun, Abhishek Khetan, and Süleyman Er. Aqsoldb, a curated reference set of aqueous solubility and 2d descriptors for a diverse set of compounds. Scientific Data, 6, 2019. URL https://api.semanticscholar.org/CorpusID:199491456.
[33] Daniel S. Wigh, Joe Arrowsmith, Alexander Pomberger, Kobi C. Felton, and Alexei A. Lapkin. Orderly: Data sets and benchmarks for chemical reaction data. Journal of Chemical Information and Modeling, 64:3790 - 3798, 2024. URL https://api.semanticscholar.org/ CorpusID:269325115.
[34] Jinmao Wei, Xiao-Jie Yuan, Qinghua Hu, and Shuqin Wang. A novel measure for evaluating classifiers. Expert Syst. Appl., 37:3799-3809, 2010. URL https://api.semanticscholar. org/CorpusID:9240275.
[35] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318, 2002.
[36] Greg Landrum. Rdkit documentation. Release, 1(1-79):4, 2013.</p>
<p>[37] Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42 (6):1273-1280, 2002.
[38] Harry L Morgan. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Journal of chemical documentation, 5(2): $107-113,1965$.
[39] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81, 2004.
[40] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72, 2005.
[41] Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay S. Pande. Moleculenet: a benchmark for molecular machine learning. Chemical Science, 9:513 - 530, 2017. URL https://api.semanticscholar. org/CorpusID:217680306.
[42] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in neural information processing systems, 33:5812-5823, 2020.
[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.
[44] Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, and Tie-Yan Liu. Molxpt: Wrapping molecules with text for generative pre-training. In ACL, 2023.
[45] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[46] Xiangru Tang, Andrew Tran, Jeffrey Tan, and Mark B. Gerstein. Mollm: a unified language model for integrating biomedical text with 2d and 3d molecular representations. Bioinformatics, 40:i357 - i368, 2024. URL https://api.semanticscholar.org/CorpusID:265455405.
[47] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 53728-53741. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf.
[48] Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Hao Jiang, Fei Wu, and Linchao Zhu. Detecting and mitigating hallucination in large vision language models via fine-grained ai feedback. ArXiv, abs/2404.14233, 2024. URL https://api. semanticscholar.org/CorpusID:269293208.
[49] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. ArXiv, abs/2402.11411, 2024. URL https://api.semanticscholar.org/CorpusID:267750239.
[50] Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, and Tong Zhang. Strengthening multimodal large language model with bootstrapped preference optimization. ArXiv, abs/2403.08730, 2024. URL https://api.semanticscholar.org/CorpusID: 268379605 .
[51] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension. ArXiv, abs/2405.19716, 2024. URL https://api.semanticscholar.org/CorpusID: 270123045 .</p>
<p>[52] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. In Conference on Empirical Methods in Natural Language Processing, 2019. URL https://api.semanticscholar.org/CorpusID:202558505.
[53] Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 595-607, 2021.
[54] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A. Shoemaker, Paul A. Thiessen, Bo Yu, Leonid Y. Zaslavsky, Jian Zhang, and Evan E. Bolton. Pubchem 2023 update. Nucleic acids research, 2022. URL https://api. semanticscholar.org/CorpusID:253182955.</p>
<h1>A Limitation</h1>
<p>Performance Degradation from Limited Molecular Distribution in Classification Tasks When the training data lacks sufficient diversity, preference optimization approaches using input preference pairs could suffer performance degradation on test or out-of-distribution datasets. In the case of MolPO, if the training molecular distribution is too narrow or contains spurious patterns unrelated to the given molecular task, the model may inappropriately regard molecules in test set or out-of-distribution (OOD) dataset as rejected molecules, based solely on their non-in-distribution characteristics. This hypothesis is consistent with the observations in Table 1 for the classification datasets. The classification datasets are substantially smaller than the datasets in the other task groups. More than half of them contain only approximately 1 K samples, compared with 3.3 M samples in the entire training dataset, which explains why MolPO's performance either remained unchanged or slightly decreased. The principled and necessary solution to this issue is basically to procure more diverse molecular distributions. We anticipate that the research community will pay more attention to developing diverse and comprehensive property classification datasets.</p>
<p>In-depth Analysis across Molecular Tasks Beyond the overall improvement in benchmark performance, an in-depth analysis is needed to understand what qualitative changes occur for each molecular task from the improved graph utilization by MolPO. It is necessary to identify trends that cannot be determined by performance metrics alone, such as which molecular features are difficult to capture with sequence-only approaches, and whether these identified molecular features have strong practical impact. Such analysis could be particularly interesting for property prediction tasks where spatial recognition of molecules is important.</p>
<p>Multi-step Reasoning and Multi-turn Interaction As demonstrated by recent successful LLMs [2, 3], impactful real-world applications of LLMs critically depend on multi-step reasoning capabilities and multi-turn interactions between LLMs and users. However, research on these two aspects remains significantly underdeveloped in the field of molecular LLMs. Such research requires different considerations from single-turn instruction tuning, beginning with dataset construction, and necessitates appropriate training objectives and reward modeling. It is an interesting direction to extend molecular LLMs to multi-step reasoning and multi-turn interaction for practical applications.</p>
<h2>B Implementation Details</h2>
<p>This section discusses the details of the Mol-LLM implementation. All the necessary materials to reproduce the results through Tables 1 to 4 , including code, trained model, and test set, are available at https://anonymous.4open.science/r/mol-llm-neurips2025-93EB.</p>
<h2>B. 1 Functional Group Prediction Dataset for Graph Encoder Pre-training</h2>
<p>As explained in Section 2.2, the proposed graph encoder pre-training conducts functional group prediction of a given molecule, a kind of self-supervision task carried out only with the input molecule. The principal challenge in constructing the functional group prediction dataset is the severe class imbalance: some groups occur in most molecules, whereas others are exceedingly rare. Leveraging the RDKit Fragments module ${ }^{3}$, we enumerate 87 functional groups and quantify their occurrences across the entire PubChem database, as summarized in the top panel of Figure 6. Figure 6 illustrates functional group imbalance, for example, fr_NHO (tertiary amines) appears in many molecules, whereas fr_prisulfonamd (primary sulfonamides) are scarce. This imbalance can cause overfitting to dominant classes instead of learning general chemical knowledge. To alleviate the overfitting problem, we remove the 11 most prevalent groups (from fr_NHO to fr_aryl_methyl) and the rarest group (fr_prisulfonamd), retaining 72 functional groups. The middle panel of Figure 6 shows the reduced yet still skewed distribution. To adjust the skewed distribution, we apply sparsity-aware importance sampling as follows. Given $M$ molecules and $G$ retained groups, let $x_{i, g} \in{0,1}$ indicate the presence of group $g$ in molecule $i$. Then we can define group frequencies as $c_{g}=\sum_{i=1}^{M} x_{i, g}$. Here, we implement importance sampling that favors rarer groups by introducing the scaling factor</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (Top) Distribution of functional groups present in molecules from the PubChem database. (Middle) Distribution of functional groups in PubChem molecules after excluding groups that are either overly common or extremely rare. (Bottom) Distribution of functional groups obtained after sampling 5M molecules from PubChem database, considering functional group sparsity. Since the number of molecules differs among panels, the y-axis scale varies across plots for visualization purposes.
$s_{g}=1 /\left(c_{g}+\varepsilon\right)$ with $\varepsilon=10^{-6}$, resulting in the sparsity score of molecule $i$ as</p>
<p>$$
\sigma_{i}=\left(\sum_{g=1}^{G} x_{i, g} s_{g}\right)^{2}
$$</p>
<p>Normalizing the scores yields a categorical distribution $p_{i}=\sigma_{i} / \sum_{j=1}^{M} \sigma_{j}$, from which we sample 5M molecules. The resulting distribution (bottom panel of Figure 6) is flatter than before, which enables the graph encoder to learn more unbiased chemical knowledge than when trained on the raw PubChem molecule distribution.</p>
<h1>B. 2 Details of Graph Encoder and Pre-training</h1>
<p>Architecture Both GNN components of the hybrid graph encoder-GINE and TokenGT—use a hidden dimension $d_{g}=1024$ and five message-passing layers. We replace the original transformer blocks of TokenGT with a BERT encoder implemented in FlashAttention-2 and configured</p>
<p>Table 6: Comparison of MAE ( $\downarrow$ ) across different GNN training settings on the QM9 dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Property</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">GINE (Tuning)</th>
<th style="text-align: center;">GINE (Frozen)</th>
<th style="text-align: center;">GINE (Frozen MoleculeSTM)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mu$</td>
<td style="text-align: center;">Dipole moment</td>
<td style="text-align: center;">0.5247</td>
<td style="text-align: center;">0.9616</td>
<td style="text-align: center;">1.0927</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha$</td>
<td style="text-align: center;">Isotropic polarizability</td>
<td style="text-align: center;">1.026</td>
<td style="text-align: center;">3.1919</td>
<td style="text-align: center;">3.3589</td>
</tr>
<tr>
<td style="text-align: center;">$\epsilon_{\text {HOMO }}$</td>
<td style="text-align: center;">Highest occupied molecular orbital energy (HOMO)</td>
<td style="text-align: center;">0.1558</td>
<td style="text-align: center;">0.2864</td>
<td style="text-align: center;">0.357</td>
</tr>
<tr>
<td style="text-align: center;">$\epsilon_{\text {LUMO }}$</td>
<td style="text-align: center;">Lowest unoccupied molecular orbital energy (LUMO)</td>
<td style="text-align: center;">0.1428</td>
<td style="text-align: center;">0.3555</td>
<td style="text-align: center;">0.4581</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta \varepsilon$</td>
<td style="text-align: center;">Gap between $\epsilon_{\text {HOMO }}$ and $\epsilon_{\text {LUMO }}$ (Gap)</td>
<td style="text-align: center;">0.1817</td>
<td style="text-align: center;">0.4003</td>
<td style="text-align: center;">0.3997</td>
</tr>
<tr>
<td style="text-align: center;">$<R^{2}>$</td>
<td style="text-align: center;">Electronic spatial extent</td>
<td style="text-align: center;">24.7215</td>
<td style="text-align: center;">94.5913</td>
<td style="text-align: center;">103.2612</td>
</tr>
<tr>
<td style="text-align: center;">$Z P V E$</td>
<td style="text-align: center;">Zero point vibrational energy</td>
<td style="text-align: center;">0.0471</td>
<td style="text-align: center;">0.3056</td>
<td style="text-align: center;">0.234</td>
</tr>
<tr>
<td style="text-align: center;">$U_{0}$</td>
<td style="text-align: center;">Internal energy at 0 K</td>
<td style="text-align: center;">9,474.99</td>
<td style="text-align: center;">10,302.67</td>
<td style="text-align: center;">10,176.77</td>
</tr>
<tr>
<td style="text-align: center;">$U$</td>
<td style="text-align: center;">Internal energy at 298.15 K</td>
<td style="text-align: center;">10,160.12</td>
<td style="text-align: center;">10,550.07</td>
<td style="text-align: center;">10,134.84</td>
</tr>
<tr>
<td style="text-align: center;">$H$</td>
<td style="text-align: center;">Enthalpy at 298.15 K</td>
<td style="text-align: center;">10,295.32</td>
<td style="text-align: center;">10,466.08</td>
<td style="text-align: center;">10,057.47</td>
</tr>
<tr>
<td style="text-align: center;">$G$</td>
<td style="text-align: center;">Free energy at 298.15 K</td>
<td style="text-align: center;">9,596.59</td>
<td style="text-align: center;">10,278.72</td>
<td style="text-align: center;">10,142.24</td>
</tr>
<tr>
<td style="text-align: center;">$c_{v}$</td>
<td style="text-align: center;">Heat capacity at 298.15 K</td>
<td style="text-align: center;">0.5053</td>
<td style="text-align: center;">1.1965</td>
<td style="text-align: center;">1.4149</td>
</tr>
<tr>
<td style="text-align: center;">$U_{0}^{\text {ADIM }}$</td>
<td style="text-align: center;">Atomization energy at 0 K</td>
<td style="text-align: center;">0.9713</td>
<td style="text-align: center;">3.6615</td>
<td style="text-align: center;">3.2477</td>
</tr>
<tr>
<td style="text-align: center;">$U^{\text {ADIM }}$</td>
<td style="text-align: center;">Atomization energy at 298.15 K</td>
<td style="text-align: center;">0.8442</td>
<td style="text-align: center;">3.4631</td>
<td style="text-align: center;">3.309</td>
</tr>
<tr>
<td style="text-align: center;">$H^{\text {ADIM }}$</td>
<td style="text-align: center;">Atomization enthalpy at 298.15 K</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">3.6308</td>
<td style="text-align: center;">3.2287</td>
</tr>
<tr>
<td style="text-align: center;">$G^{\text {ADIM }}$</td>
<td style="text-align: center;">Atomization free energy at 298.15 K</td>
<td style="text-align: center;">1.0225</td>
<td style="text-align: center;">3.5317</td>
<td style="text-align: center;">3.4369</td>
</tr>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Rotational constant</td>
<td style="text-align: center;">0.9253</td>
<td style="text-align: center;">0.7283</td>
<td style="text-align: center;">1.0479</td>
</tr>
<tr>
<td style="text-align: center;">B</td>
<td style="text-align: center;">Rotational constant</td>
<td style="text-align: center;">0.1515</td>
<td style="text-align: center;">0.2428</td>
<td style="text-align: center;">0.2511</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">Rotational constant</td>
<td style="text-align: center;">0.0773</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.1368</td>
</tr>
</tbody>
</table>
<p>with eight attention heads, thereby maximizing GPU throughput. Within TokenGT, the node- and edge-projection dimensions are both 64 , and we adopt the graph laplacian eigenvector variant for node positional encoding.</p>
<p>Pre-training GINE and TokenGT are pre-trained with the same set of hyperparameters. For SELFIES reconstruction, sequences are truncated to a maximum length of 512 tokens; tokens beyond this limit do not contribute to the loss calculation. The GPT-2 decoder used for reconstruction consists of six layers, eight attention heads, and an embedding size of 1024 . We train for 50 epochs with a learning rate of $1 \times 10^{-4}$, a batch size of 64 , and the AdamW optimizer. Training is performed on the 5 M molecule dataset described in Appendix B.1, further augmented only by adding the corresponding SELFIES strings, and all experiments are run on four NVIDIA A100 GPUs.</p>
<h1>B. 3 Investigation of Graph Encoder Used in Prior Work</h1>
<p>In Section 3.3, we show that the downstream performance of the LLM integrated with pre-trained GNNs used in Liu et al. [8], Cao et al. [9], which are MoleculeSTM [15] and GraphCL [42], does not improve from that of random initialization. To further investigate the graph representation of MoleculeSTM, we conducted an additional experiment evaluating MoleculeSTM in isolation from the LLM on the QM9 datasets. In this experiment, the graph embedding $h_{g}$ is obtained by mean-pooling the node embeddings, and then passed to a simple MLP, a regression head, whose output is used for training over MSE minimization. While tuning the regression head, we compare three GNN tuning settings: tuning a randomly initialized GNN, freezing a randomly initialized GNN, and freezing a GNN initialized with MoleculeSTM. Table 6 reports the mean absolute error (MAE) for each property. It turns out that, when only the regression head is trained, the gap between random and MoleculeSTM initialization remains negligible w.r.t. the jointly training of GINE, reinforcing our observation that the current pre-trained GNN model fails to capture useful molecular representations. All models were trained for 1,500 epochs with a batch size of 128 using the Adam optimizer with a learning rate of $10^{-4}$ on four NVIDIA A100 GPUs.</p>
<h2>B. 4 Molecular Structure Preference Pair</h2>
<p>To improve the graph utilization of our model, we create molecular structural preference pairs, which are required for Molecular Structure Preference Optimization (MolPO). Specifically, as a generalist molecular LLM, it requires a preference pair generation method applicable across various molecular tasks. Therefore, we employed functional group-based substructure modification, which can alter molecular features based on only the input molecule, without requiring task-specific design. For this, we propose Molecular ACCess System (MACCS) [37] keys-based substructure modification method directly modifies molecular substructures by randomly removing and adding them. This approach first identifies the substructures of the molecule corresponding to MACCS keys, generating two lists: one containing the MACCS keys representing functional groups present in the molecule, and the other containing the keys for functional groups absent from the molecule. Then we sample random</p>
<p>Table 7: Model hyperparameters used for Mol-LLM architecture, evaluation, and training stages.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q-Former</td>
<td>768</td>
</tr>
<tr>
<td>bert_hidden_dim</td>
<td>52</td>
</tr>
<tr>
<td>bert_name</td>
<td>scibert [52]</td>
</tr>
<tr>
<td>num_query_token</td>
<td>32</td>
</tr>
<tr>
<td>bert_layers</td>
<td>5</td>
</tr>
<tr>
<td>LoRA</td>
<td>64</td>
</tr>
<tr>
<td>lora_r</td>
<td>32</td>
</tr>
<tr>
<td>lora_alpha</td>
<td>0.1</td>
</tr>
<tr>
<td>lora_dropout</td>
<td>256</td>
</tr>
<tr>
<td>Evaluation</td>
<td>1</td>
</tr>
<tr>
<td>gen_max_len</td>
<td>256</td>
</tr>
</tbody>
</table>
<p>(b) Training hyperparameters for each stage</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Stage 1</th>
<th>Stage 2</th>
<th>Stage 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>max_length</td>
<td>512</td>
<td></td>
<td></td>
</tr>
<tr>
<td>batch_size</td>
<td>968</td>
<td>1024</td>
<td>1024</td>
</tr>
<tr>
<td>optimizer</td>
<td>adamw</td>
<td></td>
<td></td>
</tr>
<tr>
<td>scheduler</td>
<td>linear_warmup_cosine_lr</td>
<td></td>
<td></td>
</tr>
<tr>
<td>weight_decay</td>
<td>0.05</td>
<td></td>
<td></td>
</tr>
<tr>
<td>min_lr</td>
<td>$10^{-5}$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>init_lr</td>
<td>$10^{-4}$</td>
<td>$10^{-4}$</td>
<td>$4 \times 10^{-5}$</td>
</tr>
<tr>
<td>warmup_lr</td>
<td>$10^{-5}$</td>
<td>$10^{-5}$</td>
<td>$4 \times 10^{-6}$</td>
</tr>
<tr>
<td>warmup_epochs</td>
<td>0.25</td>
<td></td>
<td></td>
</tr>
<tr>
<td>gradient_clip_val</td>
<td>0.5</td>
<td></td>
<td></td>
</tr>
<tr>
<td>precision</td>
<td>bf16-mixed</td>
<td></td>
<td></td>
</tr>
<tr>
<td>c</td>
<td>NA</td>
<td>NA</td>
<td>0.25</td>
</tr>
<tr>
<td>$\lambda_{\text {margin }}$</td>
<td>NA</td>
<td>NA</td>
<td>0.25</td>
</tr>
<tr>
<td>$\lambda_{\text {clip }}$</td>
<td>NA</td>
<td>NA</td>
<td>1.0</td>
</tr>
</tbody>
</table>
<p>keys from the present MACCS keys to remove from the original molecular graph. Subsequently, other random keys are chosen from the list of absent MACCS keys, and functional groups corresponding to the selected MACCS keys are attached at a random position in the molecule. We set the number of MACCS keys randomly selected to 30 percent of the number of each molecule's present MACCS keys. This method effectively alters molecular structural information without task-specific design, at the same time, it does not require heavy computation.</p>
<h1>B. 5 Details of Mol-LLM</h1>
<p>This section describes the details of the Mol-LLM architecture and training, including the hyperparameters listed in Table 7.</p>
<p>Architecture When using Q-Former as the cross-modal projector, instead of using randomly initialized weights, we initialize it, similarly to Liu et al. [8], using the parameters of a 12-layer pre-trained transformer encoder with an embedding dimension of 768. However, we observed that successful multi-task learning can be achieved without fully utilizing all 12 layers of the Q-Former while maintaining performance without significant performance degradation. Therefore, to reduce the pre-training cost of Q-Former, we use only 5 layers instead of all 12 layers. The number of Q-Former query tokens is set to 32 for multi-task learning, which is more than the eight used in prior work [8]. We set the LoRA rank to 64 , alpha to 32 , and the dropout rate to 0.1 .</p>
<p>Three Stage Training For component ablation, we maintain identical hyperparameters for MolLLM, Mol-LLM (w/o Graph), and Mol-LLM (w/o MolPO), as specified in Table 7. In Stage 1, along with the GNN pre-training described in Appendix B.2, we fine-tune only the LoRA parameters of the LLM for 12 epochs. In Stage 2, we train the Q-Former for a single epoch to align the LLM and GNN embeddings learned in Stage 1. Next, in Stage 3, as described in Section 2, we train using the combined objective $\mathcal{L}<em _MolPO="{MolPO" _text="\text">{\text {SFT }}+c \mathcal{L}</em>}}$, which combines both the SFT and MolPO objectives. Here, the scaling factor $c=0.25$ is adjusted to ensure that the scales between $\mathcal{L<em _MolPO="{MolPO" _text="\text">{\text {SFT }}$ and $c \mathcal{L}</em>}}$ do not differ significantly. For the hyperparameters used in $\mathcal{L<em _s_="(s," q__i="q_{i">{\text {MolPO }}=\mathbb{E}</em>}, y, y) \sim \mathcal{D<em i="i" w_="w,">{\mathrm{h}}}\left[-\log \sigma\left(\min \left(r</em>$ (half of the original value) without a warm-up epoch.}-\right.\right.\right.$ $\left.\left.r_{\ell, i}, \lambda_{\text {clip }}\left|r_{w, i}\right|\right)-\gamma_{i}\right)\right]$, we use $\lambda_{\text {margin }}=0.5$ and $\lambda_{\text {clip }}=1.0$, respectively. For Stage 3, we initially trained the model for 6 epochs using the hyperparameters specified in Table 7; however, we observed that performance had not fully converged on several tasks. Therefore, we report experimental results based on the model trained for one additional epoch using a reduced initial learning rate of $2 \times 10^{-5</p>
<p>Table 8: Details of Mol-LLM instruction-tuning training data and its sources.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Data Sources</th>
<th style="text-align: right;"># Train</th>
<th style="text-align: right;"># Test</th>
<th style="text-align: right;"># All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Property Prediction (Regression)</td>
<td style="text-align: left;">MoleculeNet [41]</td>
<td style="text-align: right;">359,556</td>
<td style="text-align: right;">2,519</td>
<td style="text-align: right;">362,075</td>
</tr>
<tr>
<td style="text-align: left;">Property Prediction (Classification)</td>
<td style="text-align: left;">MoleculeNet [41]</td>
<td style="text-align: right;">59,607</td>
<td style="text-align: right;">7,460</td>
<td style="text-align: right;">67,067</td>
</tr>
<tr>
<td style="text-align: left;">Forward Reaction Prediction</td>
<td style="text-align: left;">USPTO [34]</td>
<td style="text-align: right;">$1,079,379$</td>
<td style="text-align: right;">5,062</td>
<td style="text-align: right;">$1,084,441$</td>
</tr>
<tr>
<td style="text-align: left;">Retrosynthesis</td>
<td style="text-align: left;">USPTO 500MT</td>
<td style="text-align: right;">968,943</td>
<td style="text-align: right;">5,156</td>
<td style="text-align: right;">974,099</td>
</tr>
<tr>
<td style="text-align: left;">Reagent Prediction</td>
<td style="text-align: left;">USPTO 500K</td>
<td style="text-align: right;">121,896</td>
<td style="text-align: right;">1,000</td>
<td style="text-align: right;">122,896</td>
</tr>
<tr>
<td style="text-align: left;">Molecule Captioning</td>
<td style="text-align: left;">ChEBI-20 [53]</td>
<td style="text-align: right;">58,763</td>
<td style="text-align: right;">5,793</td>
<td style="text-align: right;">64,556</td>
</tr>
<tr>
<td style="text-align: left;">Description-Guided Molecule Generation</td>
<td style="text-align: left;">ChEBI-20</td>
<td style="text-align: right;">58,763</td>
<td style="text-align: right;">5,838</td>
<td style="text-align: right;">64,601</td>
</tr>
<tr>
<td style="text-align: left;">Name Conversion</td>
<td style="text-align: left;">PubChem [54]</td>
<td style="text-align: right;">599,767</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">599,767</td>
</tr>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: left;"></td>
<td style="text-align: right;">$3,306,674$</td>
<td style="text-align: right;">40,757</td>
<td style="text-align: right;">$3,347,431$</td>
</tr>
</tbody>
</table>
<h1>C Molecular Instruction-tuning Dataset</h1>
<p>This section describes the construction details of our molecular instruction-tuning dataset, whose statistics are described in Table 8. It covers 21 tasks grouped into eight categories, comprising about 3.3 M training and 40 K test instances.</p>
<h2>C. 1 In-distribution Dataset Construction</h2>
<p>We integrate molecules for each task from the molecule-oriented datasets Mol-Instructions [8] and SMolInstruct [5]. During this integration process, tasks present in both datasets, such as forward synthesis and molecule captioning, are deduplicated to ensure that molecules included in the test set of one dataset do not appear in the training set of the combined dataset. In this process, we exclude certain tasks that are not directly relevant (e.g., NC-I2F and NC-S2F). For tasks absent in both datasets, such as BACE, molecules are directly extracted from the original data sources to construct the dataset. Finally, we augment the resulting task-specific datasets with instructions using templates adopted and extended from SMolInstruct.</p>
<h2>C. 2 Out-of-distribution Dataset Construction</h2>
<p>LogS - AqSol Dataset To evaluate Mol-LLM on OOD LogS prediction, we use the AqSol dataset [32], which contains multiple water solubility datasets in addition to ESOL. The AqSol dataset is constructed by curating data from 9 different water solubility datasets for 9,982 unique molecules. For our out-of-distribution evaluation on the ESOL dataset, we removed instances from the AqSol dataset that overlap with the ESOL dataset based on the molecule's InChI. Notably, it is common for different prediction datasets to annotate different labels for the same molecule. This occurs due to experimental errors or when LogS labels are predicted based on different prediction models. To ensure high label reliability, we retain 925 molecules whose labels are either unique or have an inter-dataset standard deviation $&lt;0.1$.</p>
<p>Reaction Prediction - ORDerly Dataset From Open Reaction Database (ORD) [33], we collected non-USPTO reaction data relevant to forward synthesis and retrosynthesis. Since all reactions in our instruction-tuning dataset are derived from USPTO data, the reactions extracted from non-USPTO sources constitute out-of-distribution (OOD) samples. Then, to ensure no duplication between the collected reaction data and those in Mol-Instructions [7] and SMolInstruct [5], we filtered out reactions from these non-USPTO sources whose input molecule scaffolds overlap with molecules used for reaction prediction training. During this, we first extract data for the forward synthesis task and subsequently ensure that the retrosynthesis reaction data extraction does not duplicate entries already obtained for the forward synthesis. Finally, we apply scaffold splitting to each dataset, resulting in 18 K training samples and 5 K test samples for forward synthesis, and 54 K training samples and 5 K test samples for retrosynthesis.</p>
<h2>D Experimental Details</h2>
<p>This section provides supplementary information necessary for understanding and reproducing the main experiments. In Appendix D.1, we detail the resource requirements and execution times needed to reproduce the main results, followed by Appendix D. 2 where we define and categorize the baseline</p>
<p>Table 9: Numbers of training and evaluation of impactful molecular tasks, which consist of property classification, property regression, reaction prediction, molecule generation, and molecule captioning, of each model. BioT5+ comprises two separate models, each trained on a distinct group of tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;"># Train Tasks</th>
<th style="text-align: right;"># Eval Tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BioT5+ [6] (Mol-Instructions)</td>
<td style="text-align: center;">6</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">BioT5+ (ChEBI-20)</td>
<td style="text-align: center;">6</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">LlaSMol [5]</td>
<td style="text-align: center;">10</td>
<td style="text-align: right;">10</td>
</tr>
<tr>
<td style="text-align: left;">Mol-LLM (Ours)</td>
<td style="text-align: center;">23</td>
<td style="text-align: right;">15</td>
</tr>
</tbody>
</table>
<p>Table 10: Summary of baseline models categorized by their input modality and model type.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Input Modality</th>
<th style="text-align: left;">Task Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">InstructMol [9]</td>
<td style="text-align: left;">1D Sequence \&amp; 2D Graph</td>
<td style="text-align: left;">Specialist</td>
</tr>
<tr>
<td style="text-align: left;">MolCA [8]</td>
<td style="text-align: left;">1D Sequence \&amp; 2D Graph</td>
<td style="text-align: left;">Specialist</td>
</tr>
<tr>
<td style="text-align: left;">MolT5 [29]</td>
<td style="text-align: left;">1D Sequence Only</td>
<td style="text-align: left;">Specialist</td>
</tr>
<tr>
<td style="text-align: left;">MolXPT [44]</td>
<td style="text-align: left;">1D Sequence Only</td>
<td style="text-align: left;">Specialist</td>
</tr>
<tr>
<td style="text-align: left;">Mol-Instructions [7]</td>
<td style="text-align: left;">1D Sequence Only</td>
<td style="text-align: left;">Semi-Generalist</td>
</tr>
<tr>
<td style="text-align: left;">BioT5+ [6]</td>
<td style="text-align: left;">1D Sequence Only</td>
<td style="text-align: left;">Semi-Generalist</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (5-shot) [2]</td>
<td style="text-align: left;">1D Sequence Only</td>
<td style="text-align: left;">Generalist</td>
</tr>
<tr>
<td style="text-align: left;">Galactica [31]</td>
<td style="text-align: left;">1D Sequence Only</td>
<td style="text-align: left;">Generalist</td>
</tr>
<tr>
<td style="text-align: left;">3D-MolM [12]</td>
<td style="text-align: left;">1D Sequence \&amp; 3D Conformer</td>
<td style="text-align: left;">Generalist</td>
</tr>
<tr>
<td style="text-align: left;">ChemDFM [23]</td>
<td style="text-align: left;">1D Sequence Only</td>
<td style="text-align: left;">Generalist</td>
</tr>
<tr>
<td style="text-align: left;">LlaSMol [5]</td>
<td style="text-align: left;">1D Sequence Only</td>
<td style="text-align: left;">Generalist</td>
</tr>
<tr>
<td style="text-align: left;">Mol-LLM</td>
<td style="text-align: left;">1D Sequence \&amp; 2D Graph</td>
<td style="text-align: left;">Generalist</td>
</tr>
</tbody>
</table>
<p>molecular language models based on modality and task coverage. In Appendix D.3, we include full experimental results, whose evaluation metrics are skipped in the main body due to the page limit.</p>
<h1>D. 1 Resources</h1>
<p>All experiments, except for graph encoder pre-training, were conducted on 8 NVIDIA A100 80GB GPUs and an AMD EPYC 7713 64-Core processor with 512GB of RAM. Using this hardware configuration, Stage 1 required 6 days of training, Stage 2 required half a day, and Stage 3 required 12 days to complete. In Stage 1 graph encoder pre-training, GINE training took approximately 18 hours on 4 A100 GPUs, and TokenGT took 19 hours.</p>
<h2>D. 2 Baseline Models</h2>
<p>As described in Section 3.1, we categorize the baseline models into three groups: specialist models, semi-generalist models, and generalist models, based on their level of specialization and task coverage. In addition to the three model categories, we provide a classification based on the type of input modalities. These categorizations are summarized in Table 10.</p>
<h2>D.2.1 Categories by Input Modalities</h2>
<p>1D Sequence Only Models that rely solely on 1D sequences (e.g., SMILES or SELFIES), which address molecules as strings. This category include Galactica 6.7B [31], GPT-4 [2], Mol-Instructions [7], BioT5+[6], LlaSMol [5], MolT5 [29], MolXPT [44], and ChemDFM [23].</p>
<p>1D Sequence \&amp; 2D Graph Models integrate string-based and graph-based representations to capture 2D molecular structure. Representative examples are InstructMol [9], MolCA [8], and GIT-Mol [10]. GIT-Mol additionally exploits molecular images, providing another route to leverage structural information.</p>
<p>1D Sequence \&amp; 3D Conformer Models incorporate 3D conformers alongside sequence information to enrich molecular 3D spatial representations 3D-MoLM [12] belongs to this category.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparison of predicted outputs by generalists on forward synthesis (FS) and retrosynthesis (RS), both in Mol-Instructions and ORDerly dataset. The upper two rows represent forward synthesis in Mol-Instructions (InD) and ORDerly (OOD) Datasets, respectively, and the lower two rows represent the retrosynthesis task in the same dataset order.</p>
<h1>D.2.2 Categories by Task Coverage</h1>
<p>As described in Section 3.1, the baseline models are categorized as follows:
Specialist Models MolCA [8], InstructMol [9], MolXPT [44], GIT-Mol [10], and MolT5 [29] are optimized for individual molecular tasks without parameter or knowledge sharing across tasks.</p>
<p>Semi-Generalist Models BioT5+ [6] and Mol-Instructions [7] address related task groups within a single framework. For instance, BioT5+ trains two separate models: one for classification and translation, and the other for regression and reaction prediction, enabling knowledge sharing within each group while preserving task-specific optimization.</p>
<p>Generalist Models Galactica 6.7B [31], GPT-4 [2], LlaSMol [5], and ChemDFM [23] aim for broad generalization by simultaneously tackling all molecular task groups.</p>
<h2>D. 3 Full Experimental Results</h2>
<p>Table 11 presents the complete results corresponding to Table 2. Table 13 and Table 12 show the full results for Table 3. In Figure 7, we also visualize predicted outputs by generalists, including Mol-LLM, Galactica [31], and LlaSMol [5] on forward reaction prediction and retrosynthesis on both Mol-Instructions and ORDerly datasets.</p>
<h2>E Broader Impacts</h2>
<p>We currently anticipate no major negative social impacts from this research; nevertheless, there is a possibility that it could be used to generate molecules harmful to humans or the environment. At present, training is carried out on eight NVIDIA A100 GPUs, but scaling to larger LLMs would require additional GPUs and would therefore increase carbon emissions. On the positive side, MolLLM enables researchers performing chemical experiments to predict experimental outcomes in advance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://www.rdkit.org/docs/source/rdkit.Chem.Fragments.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8745 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8745</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8745</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-273812013</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.00855v1.pdf" target="_blank">Vision-Language Models Can Self-Improve Reasoning via Reflection</a></p>
                <p><strong>Paper Abstract:</strong> Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal LLMs has been largely overlooked. To this end, we propose a simple yet effective self-training framework, R3V, which iteratively enhances the model's Vision-language Reasoning by Reflecting on CoT Rationales. Our framework consists of two interleaved parts: (1) iteratively bootstrapping positive and negative solutions for reasoning datasets, and (2) reflection on rationale for learning from mistakes. Specifically, we introduce the self-refine and self-select losses, enabling the model to refine flawed rationale and derive the correct answer by comparing rationale candidates. Experiments on a wide range of vision-language tasks show that R3V consistently improves multimodal LLM reasoning, achieving a relative improvement of 23 to 60 percent over GPT-distilled baselines. Additionally, our approach supports self-reflection on generated solutions, further boosting performance through test-time computation.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8745.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8745.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R3V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection on bootstrapped Chain-of-Thought Rationales for Vision-language self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-training framework that bootstraps positive and negative multimodal Chain-of-Thought (CoT) solutions from an MLLM and trains the model via multitask losses (supervised fine-tuning, self-refine, self-select) to enable learning from mistakes and iterative reasoning improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL (primary), LLaVA-1.5 (also used), Qwen2-VL (generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal large language models (MLLMs) used as bases for self-training in the paper; exact parameter counts are not specified in the paper. Qwen-VL and LLaVA-1.5 were used for the main experiments; Qwen2-VL was used to show generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>R3V (self-reflective self-training with self-refine and self-select)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative self-training that (1) samples multiple CoT rationale-answer pairs per (image, question), labels samples positive/negative by answer correctness, (2) fine-tunes on positive CoTs (SFT), (3) trains a self-refine loss where the model conditions on a negative solution to produce the corrected positive solution, and (4) trains a self-select loss where the model selects the correct answer from a small set of candidate rationales (N=3 by default). The whole process is repeated for multiple iterations (4–5 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>A suite of six vision-language reasoning benchmarks (TabMWP, ChartQA, CLEVR-Math, MiniWob, GeoQA, M3CoT) and additional OOD benchmarks (e.g., MMMU)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multimodal reasoning tasks requiring integration of visual cues and multi-step chain-of-thought reasoning across domains: chart/table math, visual math, interactive web tasks, geometry, and multi-domain CoT benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Average accuracy 64.37% (Qwen-VL after R3V self-training). Paper reports R3V yields 23%–60% relative improvement across tasks over GPT-distilled baselines; specifically an average relative improvement of 32.8% (48.47 → 64.37) over the GPT-distilled baseline on Qwen-VL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>GPT-distilled baseline average 48.47% (used as warmup baseline). Zero-shot CoT often gave worse results than direct zero-shot QA. STaR baseline average ~59.28% (see comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Model-internal multitask training: supervised fine-tuning on positive CoTs (L_SFT), self-refine loss L_REF which conditions on a negative rationale to predict a positive rationale (log-probability objective), and self-select loss L_SEL where the model sees a candidate set of rationales and is trained to select the ground-truth answer; at inference the model can sample multiple candidates and apply the learned selection prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements reported across six benchmarks (average 48.47 → 64.37 for Qwen-VL). Ablations show removal of self-reflective components reduces accuracy (w/o self-refine avg 62.50; w/o self-select avg 60.78; w/o iteration avg 60.64). R3V also outperforms STaR and DPO baselines and improves OOD performance (e.g., MMMU improved via test-time selection).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Multimodal CoT is noisy (CoT fidelity in sampled positive-answer solutions ranges widely, authors report 8%–70% fully correct CoT across tasks), which limits preference-learning approaches; R3V's generated CoTs still contain noise and are limited by MLLM perception (OCR/recognition errors, symbol misinterpretation). Experiments run only on two main MLLMs (compute constraint). Test-time selection requires extra inference compute and may have diminishing returns with excessive samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared vs. GPT-distilled warmup, STaR (self-training on positives), and DPO (direct preference optimization). R3V substantially outperforms GPT-distilled and STaR baselines and yields clear advantages over DPO in noisy multimodal CoT settings; self-select test-time inference outperforms Test@1 and majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablation results: full R3V avg 64.37%; w/o self-refine 62.50%; w/o self-select 60.78%; w/o iteration 60.64%. These show each component (self-refine, self-select, iterative sampling) contributes to gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vision-Language Models Can Self-Improve Reasoning via Reflection', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8745.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8745.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-time Selection (TTS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-time self-select inference (selecting best answer from sampled candidate rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time reflection method where the model samples multiple candidate CoT solutions and is prompted (using the trained self-select behavior) to compare them and choose the most likely correct answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL / Qwen2-VL (and other MLLMs trained with R3V)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base MLLMs as used in self-training; test-time selection is applied at inference on models that were trained with the self-select objective.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Test-time selection (self-select at inference)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At inference, sample N candidate reasoning chains per question (N=3 by default), assemble them as a candidate set C and prompt the model to compare differences and select the final answer. This uses the self-select behavior learned during training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same vision-language benchmarks and OOD suites (e.g., MMMU) where inference selection is applied</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multimodal reasoning evaluation; test-time selection is evaluated on same benchmarks to measure gains from inference-time reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Example: on MMMU, sampling 3 candidates and applying test-time self-select improved accuracy from 35.63% → 38.48% (reported). Across tasks, test-time self-selection consistently outperformed Test@1 and majority voting when the model was trained with self-select.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Test@1 (single-sample) or majority voting baselines; e.g., MMMU Test@1 = 35.63% (before selection). Exact per-task baselines vary; majority voting generally underperforms self-select according to paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based comparison: candidates are provided in the input context and the model is prompted to analyze differences and eliminate incorrect options to select the best answer; no external verifier module is used — the trained model performs the comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical results and figures show test-time self-selection beats single-sample Test@1 and majority voting on all evaluated tasks; specific reported improvement on MMMU (35.63 → 38.48) and consistent aggregate improvements across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires additional inference compute proportional to sample count; effectiveness depends on quality/diversity of sampled candidates and on model's learned self-select ability (stronger base models scale better). Diminishing returns with excessive sample sizes are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Direct comparison: self-select outperforms Test@1 and majority voting. When combined with R3V training, test-time selection yields further gains beyond training-time SFT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper reports comparisons (figures/tables) showing consistent superiority of self-select over majority voting and Test@1; no separate ablation only for test-time selection beyond comparisons with those baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vision-Language Models Can Self-Improve Reasoning via Reflection', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8745.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8745.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine loss / mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training objective that teaches the model to correct flawed self-generated rationales by conditioning on a negative (incorrect) rationale and predicting a corresponding positive (correct) rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL, LLaVA-1.5 (MLLMs used in training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal LLMs fine-tuned with the self-refine objective as part of R3V.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Construct dataset D_REF of (image, question, y+, y-) pairs where y+ is a positive solution and y- a sampled negative; train with loss L_REF = -log M(y+ | y-, x, I), encouraging the model to analyze and correct mistakes in a negative rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Applied within the same vision-language reasoning benchmarks as an auxiliary training objective</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Helps the model correct errors found in its own sampled CoT rationales for multimodal reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Contributes to full R3V avg 64.37% (Qwen-VL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Removing self-refine (w/o self-refine) reduces average accuracy to 62.50% (from 64.37%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Multitask supervised objective where the model conditions on a negative rationale and input to generate a corrected rationale and answer; implemented as an additional supervised log-likelihood loss during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Ablation: w/o self-refine reduces average performance (64.37 → 62.50), indicating measurable contribution. Authors report that self-refine helps the model learn from mistakes and correct flawed reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires matched positive/negative pairs; depends on iterative sampling to obtain stronger/harder negatives; noisy CoT (e.g., perception errors) can limit the informativeness of negative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Complementary to SFT and self-select; outperforms preference-learning (DPO) approaches in noisy multimodal CoT settings because it avoids encouraging faulty rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>w/o self-refine average drops to 62.50% (from 64.37%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vision-Language Models Can Self-Improve Reasoning via Reflection', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8745.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8745.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Select</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-select loss / mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training objective that teaches the model to compare multiple sampled candidate rationales and select the correct answer from them, enabling inference-time elimination-style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL, LLaVA-1.5, Qwen2-VL (MLLMs used with this objective)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source MLLMs fine-tuned with the self-select objective; default candidate set size N=3 used during training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-select</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Create dataset D_SEL of (image, question, ground-truth answer, candidate set C) where C = {y1, y2, ..., yN} contains at least one positive solution; train with L_SEL = -log M(â | x, I, C) so the model learns to analyze candidate rationales and choose the correct answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same multimodal reasoning benchmarks; used both for training and test-time selection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Enables the model to compare multiple reasoning paths for the same multimodal question and select the most plausible answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Full R3V (including self-select) average 64.37% (Qwen-VL). Test-time self-select further improves some OOD results (e.g., MMMU 35.63 → 38.48).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Removing self-select (w/o self-select) reduces average accuracy to 60.78% (from 64.37%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based candidate-comparison training and inference: model receives multiple candidate rationales and is supervised to produce the ground-truth answer; at test-time the same prompt is used to choose among sampled candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Ablation shows w/o self-select drops average performance (64.37 → 60.78). Empirical comparisons show test-time self-select outperforms Test@1 and majority voting across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires candidate sets that include at least one positive solution (construction assumption); success depends on candidate diversity and model's ability to spot errors. Additional inference cost for multiple samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms majority voting and single-sample Test@1; more robust than DPO in noisy multimodal CoT because it trains explicit selection rather than preference scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>w/o self-select average drops to 60.78% (from 64.37%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vision-Language Models Can Self-Improve Reasoning via Reflection', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8745.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8745.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPO (failure case)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Preference Optimization (DPO) in multimodal CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preference-learning method evaluated by the authors and found to be ineffective in multimodal Chain-of-Thought self-training due to noisy CoT samples; DPO provided minimal improvement and underperformed R3V.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to MLLM self-training baselines (STaR+DPO experiments in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DPO is a preference-learning algorithm (external method) that trains a model using pairwise preference signals; the paper evaluated DPO applied to self-generated positive/negative multimodal CoT pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Direct Preference Optimization (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Preference-learning approach that uses positive/negative rationale pairs to optimize a model toward preferring better outputs; in this paper DPO was applied to self-generated multimodal CoT pairs but struggled.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Applied as an alternative self-training method on the same vision-language reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as a baseline to learn from positive/negative CoT samples for improving reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper reports minimal improvement when equipping STaR with DPO; DPO-based variant falls short of R3V (exact numbers not provided in main text, reported as much smaller gains).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline STaR (without DPO) performed better than DPO-enhanced STaR in the multimodal CoT tests according to the paper's Table 4 and discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Externally applied preference optimization on sampled positive/negative CoT pairs (not a prompt-based self-reflection mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical comparison (Table 4) shows DPO yields minimal improvement in multimodal settings and does not match R3V; authors attribute failure to noisy CoT where correct answers may still have flawed rationales that DPO misinterprets as better.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>DPO struggles in multimodal CoT because of high noise: many 'positive' (answer-correct) samples still contain incorrect or partially incorrect rationales (CoT fidelity across tasks reported between ~8% and 70% fully correct), so preference signals are unreliable and DPO can encourage faulty reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly and found inferior to R3V; DPO produced less improvement than STaR+R3V components in noisy multimodal CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper provides qualitative analysis and a case study indicating DPO's failure; no numeric DPO ablation table values are precisely quoted in the main text but the authors report minimal gains in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vision-Language Models Can Self-Improve Reasoning via Reflection', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8745.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8745.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STaR: Self-Training with Approximated Rationales (bootstrapping reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed iterative self-training method that fine-tunes on self-generated positive rationales only (used as a strong baseline for comparison in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to same MLLMs as a baseline (STaR-style iterative SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baseline self-training approach (Zelikman et al., 2022) that iteratively fine-tunes on sampled positive-chain-of-thought outputs; used here as a point of comparison to R3V.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>STaR (iterative fine-tuning on positive rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample multiple CoT outputs per example, filter by answer correctness, and fine-tune the model on positive CoT solutions only (no explicit learning-from-mistakes losses).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same multimodal reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as a baseline to compare effectiveness of learning from negative samples / reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>STaR baseline average reported around 59.28% (paper reports R3V improves from ~59.28 → 64.37 on average over STaR baseline in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>N/A (STaR does not include self-reflective components in its standard form).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Not a reflection method — iterative SFT only on positives; contrasted with R3V which uses negative samples and reflection losses.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>R3V outperforms STaR in iterative self-training experiments; authors show R3V achieves higher gains earlier in iterations and a higher final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>STaR ignores negative samples and thus can under-explore reasoning paths; in noisy multimodal CoT settings, only training on positives may limit robustness and cross-instance error-correction capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>R3V surpasses STaR by a large margin; STaR+ DPO underperformed compared to R3V in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Comparisons in iterative plots (Figure 3) and tables show STaR is outperformed by R3V across all tested tasks; exact per-task STaR numbers are reported in paper tables/figures (e.g., average ~59.28%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vision-Language Models Can Self-Improve Reasoning via Reflection', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bootstrapping reasoning with reasoning (STaR) <em>(Rating: 2)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 2)</em></li>
                <li>Learning from mistakes makes llm better reasoner <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-rewarding language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8745",
    "paper_id": "paper-273812013",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "R3V",
            "name_full": "Reflection on bootstrapped Chain-of-Thought Rationales for Vision-language self-improvement",
            "brief_description": "An iterative self-training framework that bootstraps positive and negative multimodal Chain-of-Thought (CoT) solutions from an MLLM and trains the model via multitask losses (supervised fine-tuning, self-refine, self-select) to enable learning from mistakes and iterative reasoning improvement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-VL (primary), LLaVA-1.5 (also used), Qwen2-VL (generalization)",
            "model_description": "Open-source multimodal large language models (MLLMs) used as bases for self-training in the paper; exact parameter counts are not specified in the paper. Qwen-VL and LLaVA-1.5 were used for the main experiments; Qwen2-VL was used to show generalization.",
            "reflection_method_name": "R3V (self-reflective self-training with self-refine and self-select)",
            "reflection_method_description": "Iterative self-training that (1) samples multiple CoT rationale-answer pairs per (image, question), labels samples positive/negative by answer correctness, (2) fine-tunes on positive CoTs (SFT), (3) trains a self-refine loss where the model conditions on a negative solution to produce the corrected positive solution, and (4) trains a self-select loss where the model selects the correct answer from a small set of candidate rationales (N=3 by default). The whole process is repeated for multiple iterations (4–5 reported).",
            "task_name": "A suite of six vision-language reasoning benchmarks (TabMWP, ChartQA, CLEVR-Math, MiniWob, GeoQA, M3CoT) and additional OOD benchmarks (e.g., MMMU)",
            "task_description": "Multimodal reasoning tasks requiring integration of visual cues and multi-step chain-of-thought reasoning across domains: chart/table math, visual math, interactive web tasks, geometry, and multi-domain CoT benchmarks.",
            "performance_with_reflection": "Average accuracy 64.37% (Qwen-VL after R3V self-training). Paper reports R3V yields 23%–60% relative improvement across tasks over GPT-distilled baselines; specifically an average relative improvement of 32.8% (48.47 → 64.37) over the GPT-distilled baseline on Qwen-VL.",
            "performance_without_reflection": "GPT-distilled baseline average 48.47% (used as warmup baseline). Zero-shot CoT often gave worse results than direct zero-shot QA. STaR baseline average ~59.28% (see comparison).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Model-internal multitask training: supervised fine-tuning on positive CoTs (L_SFT), self-refine loss L_REF which conditions on a negative rationale to predict a positive rationale (log-probability objective), and self-select loss L_SEL where the model sees a candidate set of rationales and is trained to select the ground-truth answer; at inference the model can sample multiple candidates and apply the learned selection prompt.",
            "number_of_iterations": 4,
            "evidence_for_improvement": "Quantitative improvements reported across six benchmarks (average 48.47 → 64.37 for Qwen-VL). Ablations show removal of self-reflective components reduces accuracy (w/o self-refine avg 62.50; w/o self-select avg 60.78; w/o iteration avg 60.64). R3V also outperforms STaR and DPO baselines and improves OOD performance (e.g., MMMU improved via test-time selection).",
            "limitations_or_failure_cases": "Multimodal CoT is noisy (CoT fidelity in sampled positive-answer solutions ranges widely, authors report 8%–70% fully correct CoT across tasks), which limits preference-learning approaches; R3V's generated CoTs still contain noise and are limited by MLLM perception (OCR/recognition errors, symbol misinterpretation). Experiments run only on two main MLLMs (compute constraint). Test-time selection requires extra inference compute and may have diminishing returns with excessive samples.",
            "comparison_to_other_methods": "Compared vs. GPT-distilled warmup, STaR (self-training on positives), and DPO (direct preference optimization). R3V substantially outperforms GPT-distilled and STaR baselines and yields clear advantages over DPO in noisy multimodal CoT settings; self-select test-time inference outperforms Test@1 and majority voting.",
            "ablation_study_results": "Ablation results: full R3V avg 64.37%; w/o self-refine 62.50%; w/o self-select 60.78%; w/o iteration 60.64%. These show each component (self-refine, self-select, iterative sampling) contributes to gains.",
            "uuid": "e8745.0",
            "source_info": {
                "paper_title": "Vision-Language Models Can Self-Improve Reasoning via Reflection",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Test-time Selection (TTS)",
            "name_full": "Test-time self-select inference (selecting best answer from sampled candidate rationales)",
            "brief_description": "An inference-time reflection method where the model samples multiple candidate CoT solutions and is prompted (using the trained self-select behavior) to compare them and choose the most likely correct answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-VL / Qwen2-VL (and other MLLMs trained with R3V)",
            "model_description": "Same base MLLMs as used in self-training; test-time selection is applied at inference on models that were trained with the self-select objective.",
            "reflection_method_name": "Test-time selection (self-select at inference)",
            "reflection_method_description": "At inference, sample N candidate reasoning chains per question (N=3 by default), assemble them as a candidate set C and prompt the model to compare differences and select the final answer. This uses the self-select behavior learned during training.",
            "task_name": "Same vision-language benchmarks and OOD suites (e.g., MMMU) where inference selection is applied",
            "task_description": "Multimodal reasoning evaluation; test-time selection is evaluated on same benchmarks to measure gains from inference-time reflection.",
            "performance_with_reflection": "Example: on MMMU, sampling 3 candidates and applying test-time self-select improved accuracy from 35.63% → 38.48% (reported). Across tasks, test-time self-selection consistently outperformed Test@1 and majority voting when the model was trained with self-select.",
            "performance_without_reflection": "Test@1 (single-sample) or majority voting baselines; e.g., MMMU Test@1 = 35.63% (before selection). Exact per-task baselines vary; majority voting generally underperforms self-select according to paper.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based comparison: candidates are provided in the input context and the model is prompted to analyze differences and eliminate incorrect options to select the best answer; no external verifier module is used — the trained model performs the comparison.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical results and figures show test-time self-selection beats single-sample Test@1 and majority voting on all evaluated tasks; specific reported improvement on MMMU (35.63 → 38.48) and consistent aggregate improvements across benchmarks.",
            "limitations_or_failure_cases": "Requires additional inference compute proportional to sample count; effectiveness depends on quality/diversity of sampled candidates and on model's learned self-select ability (stronger base models scale better). Diminishing returns with excessive sample sizes are noted.",
            "comparison_to_other_methods": "Direct comparison: self-select outperforms Test@1 and majority voting. When combined with R3V training, test-time selection yields further gains beyond training-time SFT alone.",
            "ablation_study_results": "Paper reports comparisons (figures/tables) showing consistent superiority of self-select over majority voting and Test@1; no separate ablation only for test-time selection beyond comparisons with those baselines.",
            "uuid": "e8745.1",
            "source_info": {
                "paper_title": "Vision-Language Models Can Self-Improve Reasoning via Reflection",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-refine loss / mechanism",
            "brief_description": "A training objective that teaches the model to correct flawed self-generated rationales by conditioning on a negative (incorrect) rationale and predicting a corresponding positive (correct) rationale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-VL, LLaVA-1.5 (MLLMs used in training)",
            "model_description": "Open-source multimodal LLMs fine-tuned with the self-refine objective as part of R3V.",
            "reflection_method_name": "Self-refine",
            "reflection_method_description": "Construct dataset D_REF of (image, question, y+, y-) pairs where y+ is a positive solution and y- a sampled negative; train with loss L_REF = -log M(y+ | y-, x, I), encouraging the model to analyze and correct mistakes in a negative rationale.",
            "task_name": "Applied within the same vision-language reasoning benchmarks as an auxiliary training objective",
            "task_description": "Helps the model correct errors found in its own sampled CoT rationales for multimodal reasoning tasks.",
            "performance_with_reflection": "Contributes to full R3V avg 64.37% (Qwen-VL).",
            "performance_without_reflection": "Removing self-refine (w/o self-refine) reduces average accuracy to 62.50% (from 64.37%).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Multitask supervised objective where the model conditions on a negative rationale and input to generate a corrected rationale and answer; implemented as an additional supervised log-likelihood loss during fine-tuning.",
            "number_of_iterations": 4,
            "evidence_for_improvement": "Ablation: w/o self-refine reduces average performance (64.37 → 62.50), indicating measurable contribution. Authors report that self-refine helps the model learn from mistakes and correct flawed reasoning paths.",
            "limitations_or_failure_cases": "Requires matched positive/negative pairs; depends on iterative sampling to obtain stronger/harder negatives; noisy CoT (e.g., perception errors) can limit the informativeness of negative examples.",
            "comparison_to_other_methods": "Complementary to SFT and self-select; outperforms preference-learning (DPO) approaches in noisy multimodal CoT settings because it avoids encouraging faulty rationales.",
            "ablation_study_results": "w/o self-refine average drops to 62.50% (from 64.37%).",
            "uuid": "e8745.2",
            "source_info": {
                "paper_title": "Vision-Language Models Can Self-Improve Reasoning via Reflection",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Self-Select",
            "name_full": "Self-select loss / mechanism",
            "brief_description": "A training objective that teaches the model to compare multiple sampled candidate rationales and select the correct answer from them, enabling inference-time elimination-style reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-VL, LLaVA-1.5, Qwen2-VL (MLLMs used with this objective)",
            "model_description": "Open-source MLLMs fine-tuned with the self-select objective; default candidate set size N=3 used during training and inference.",
            "reflection_method_name": "Self-select",
            "reflection_method_description": "Create dataset D_SEL of (image, question, ground-truth answer, candidate set C) where C = {y1, y2, ..., yN} contains at least one positive solution; train with L_SEL = -log M(â | x, I, C) so the model learns to analyze candidate rationales and choose the correct answer.",
            "task_name": "Same multimodal reasoning benchmarks; used both for training and test-time selection",
            "task_description": "Enables the model to compare multiple reasoning paths for the same multimodal question and select the most plausible answer.",
            "performance_with_reflection": "Full R3V (including self-select) average 64.37% (Qwen-VL). Test-time self-select further improves some OOD results (e.g., MMMU 35.63 → 38.48).",
            "performance_without_reflection": "Removing self-select (w/o self-select) reduces average accuracy to 60.78% (from 64.37%).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based candidate-comparison training and inference: model receives multiple candidate rationales and is supervised to produce the ground-truth answer; at test-time the same prompt is used to choose among sampled candidates.",
            "number_of_iterations": 4,
            "evidence_for_improvement": "Ablation shows w/o self-select drops average performance (64.37 → 60.78). Empirical comparisons show test-time self-select outperforms Test@1 and majority voting across tasks.",
            "limitations_or_failure_cases": "Requires candidate sets that include at least one positive solution (construction assumption); success depends on candidate diversity and model's ability to spot errors. Additional inference cost for multiple samples.",
            "comparison_to_other_methods": "Outperforms majority voting and single-sample Test@1; more robust than DPO in noisy multimodal CoT because it trains explicit selection rather than preference scoring.",
            "ablation_study_results": "w/o self-select average drops to 60.78% (from 64.37%).",
            "uuid": "e8745.3",
            "source_info": {
                "paper_title": "Vision-Language Models Can Self-Improve Reasoning via Reflection",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DPO (failure case)",
            "name_full": "Direct Preference Optimization (DPO) in multimodal CoT",
            "brief_description": "A preference-learning method evaluated by the authors and found to be ineffective in multimodal Chain-of-Thought self-training due to noisy CoT samples; DPO provided minimal improvement and underperformed R3V.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to MLLM self-training baselines (STaR+DPO experiments in paper)",
            "model_description": "DPO is a preference-learning algorithm (external method) that trains a model using pairwise preference signals; the paper evaluated DPO applied to self-generated positive/negative multimodal CoT pairs.",
            "reflection_method_name": "Direct Preference Optimization (DPO)",
            "reflection_method_description": "Preference-learning approach that uses positive/negative rationale pairs to optimize a model toward preferring better outputs; in this paper DPO was applied to self-generated multimodal CoT pairs but struggled.",
            "task_name": "Applied as an alternative self-training method on the same vision-language reasoning benchmarks",
            "task_description": "Used as a baseline to learn from positive/negative CoT samples for improving reasoning.",
            "performance_with_reflection": "Paper reports minimal improvement when equipping STaR with DPO; DPO-based variant falls short of R3V (exact numbers not provided in main text, reported as much smaller gains).",
            "performance_without_reflection": "Baseline STaR (without DPO) performed better than DPO-enhanced STaR in the multimodal CoT tests according to the paper's Table 4 and discussion.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Externally applied preference optimization on sampled positive/negative CoT pairs (not a prompt-based self-reflection mechanism).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical comparison (Table 4) shows DPO yields minimal improvement in multimodal settings and does not match R3V; authors attribute failure to noisy CoT where correct answers may still have flawed rationales that DPO misinterprets as better.",
            "limitations_or_failure_cases": "DPO struggles in multimodal CoT because of high noise: many 'positive' (answer-correct) samples still contain incorrect or partially incorrect rationales (CoT fidelity across tasks reported between ~8% and 70% fully correct), so preference signals are unreliable and DPO can encourage faulty reasoning.",
            "comparison_to_other_methods": "Compared directly and found inferior to R3V; DPO produced less improvement than STaR+R3V components in noisy multimodal CoT.",
            "ablation_study_results": "Paper provides qualitative analysis and a case study indicating DPO's failure; no numeric DPO ablation table values are precisely quoted in the main text but the authors report minimal gains in Table 4.",
            "uuid": "e8745.4",
            "source_info": {
                "paper_title": "Vision-Language Models Can Self-Improve Reasoning via Reflection",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "STaR (baseline)",
            "name_full": "STaR: Self-Training with Approximated Rationales (bootstrapping reasoning)",
            "brief_description": "A previously proposed iterative self-training method that fine-tunes on self-generated positive rationales only (used as a strong baseline for comparison in this paper).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to same MLLMs as a baseline (STaR-style iterative SFT)",
            "model_description": "Baseline self-training approach (Zelikman et al., 2022) that iteratively fine-tunes on sampled positive-chain-of-thought outputs; used here as a point of comparison to R3V.",
            "reflection_method_name": "STaR (iterative fine-tuning on positive rationales)",
            "reflection_method_description": "Sample multiple CoT outputs per example, filter by answer correctness, and fine-tune the model on positive CoT solutions only (no explicit learning-from-mistakes losses).",
            "task_name": "Same multimodal reasoning benchmarks",
            "task_description": "Used as a baseline to compare effectiveness of learning from negative samples / reflection.",
            "performance_with_reflection": "STaR baseline average reported around 59.28% (paper reports R3V improves from ~59.28 → 64.37 on average over STaR baseline in comparisons).",
            "performance_without_reflection": "N/A (STaR does not include self-reflective components in its standard form).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Not a reflection method — iterative SFT only on positives; contrasted with R3V which uses negative samples and reflection losses.",
            "number_of_iterations": null,
            "evidence_for_improvement": "R3V outperforms STaR in iterative self-training experiments; authors show R3V achieves higher gains earlier in iterations and a higher final performance.",
            "limitations_or_failure_cases": "STaR ignores negative samples and thus can under-explore reasoning paths; in noisy multimodal CoT settings, only training on positives may limit robustness and cross-instance error-correction capability.",
            "comparison_to_other_methods": "R3V surpasses STaR by a large margin; STaR+ DPO underperformed compared to R3V in the authors' experiments.",
            "ablation_study_results": "Comparisons in iterative plots (Figure 3) and tables show STaR is outperformed by R3V across all tested tasks; exact per-task STaR numbers are reported in paper tables/figures (e.g., average ~59.28%).",
            "uuid": "e8745.5",
            "source_info": {
                "paper_title": "Vision-Language Models Can Self-Improve Reasoning via Reflection",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bootstrapping reasoning with reasoning (STaR)",
            "rating": 2,
            "sanitized_title": "bootstrapping_reasoning_with_reasoning_star"
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Learning from mistakes makes llm better reasoner",
            "rating": 2,
            "sanitized_title": "learning_from_mistakes_makes_llm_better_reasoner"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-rewarding language models",
            "rating": 1,
            "sanitized_title": "selfrewarding_language_models"
        }
    ],
    "cost": 0.017887,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Vision-Language Models Can Self-Improve Reasoning via Reflection
30 Oct 2024</p>
<p>Kanzhi Cheng chengkz@smail.nju.edu 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Yantao Li li_yantao@smail.nju.edu 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Fangzhi Xu fangzhixu98@gmail.com 
Shanghai AI Lab</p>
<p>Jianbing Zhang 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Hao Zhou zhouhao@air.tsinghua.edu.cn 
Shanghai AI Lab</p>
<p>Institute for AI Industry Research (AIR)
Tsinghua University</p>
<p>Yang Liu 
Shanghai AI Lab</p>
<p>Institute for AI Industry Research (AIR)
Tsinghua University</p>
<p>Vision-Language Models Can Self-Improve Reasoning via Reflection
30 Oct 2024C597A695A71CE583EED32785E670C48AarXiv:2411.00855v1[cs.LG]
Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs).However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal LLMs has been largely overlooked.To this end, we propose a simple yet effective self-training framework, R 3 V, which iteratively enhances the model's Visionlanguage Reasoning by Reflecting on CoT Rationales.Our framework consists of two interleaved parts: (1) iteratively bootstrapping positive and negative solutions for reasoning datasets, and (2) reflection on rationale for learning from mistakes.Specifically, we introduce the self-refine and self-select losses, enabling the model to refine flawed rationale and derive the correct answer by comparing rationale candidates.Experiments on a wide range of vision-language tasks show that R 3 V consistently improves multimodal LLM reasoning, achieving a relative improvement of 23% to 60% over GPT-distilled baselines.Additionally, our approach supports self-reflection on generated solutions, further boosting performance through test-time computation.<em> † Equal contribution.</em> Our code is available at https://github.com/njucckevin/MM-Self-Improve.</p>
<p>Introduction</p>
<p>Humans often rely on intuitive Chain-of-Thought (CoT) to perform complex reasoning (Ericsson and Simon, 1980).Previous studies have shown that this CoT capacity also emerges in Large Language Models (LLMs) (Wei et al., 2022).Through simple prompting or fine-tuning (Cobbe et al., 2021;Kojima et al., 2022;Hsieh et al., 2023), CoT enhances the reasoning performance of LLMs while providing insights into their decision-making process.Recently, OpenAI o1 further advances reasoning by producing long internal CoT sequences, taking LLMs intelligence to a new level.</p>
<p>Figure 1: Results of Qwen-VL on TabMWP, a visual mathematical reasoning dataset.Qwen-VL exhibits weak zero-shot CoT reasoning performance, while our R 3 V iteratively self-improves, surpassing the GPTdistilled baseline by a large margin.</p>
<p>While CoT reasoning has significantly advanced LLMs in textual domains, extending CoT to multimodal settings remains an open problem.Unlike the abundant, unsupervised text-based CoT in pretraining corpora (Kojima et al., 2022;Wei et al., 2022), multimodal CoT resources are scarce in the text-dominated internet collections (Dai et al., 2023), hindering the full realization of Multimodal LLMs' (MLLMs) reasoning potential.</p>
<p>Recent studies show that open-sourced MLLMs struggle to integrate visual cues into their reasoning process, resulting in weak CoT performance (Zhang et al., 2024a;Shi et al., 2024).Consistent with our observations in Figure 1, CoT prompting provides minimal gains over direct prediction (Chen et al., 2024a) and falls far behind GPT-4o.One potential solution is to construct multimodal CoT annotations for post-training; however, manual annotation is prohibitively expensive and hard to scale.This raises our first research question: can MLLMs self-improve the reasoning capabilities through bootstrapping on CoT samples?</p>
<p>Orthogonal to fine-tuning on curated CoT annotations, relying solely on positive samples can lead to suboptimal policy due to insufficient explo-ration of reasoning paths.Inspired by human thinking, another promising direction involves learning from trial-and-errors (Yuan et al., 2024;Song et al., 2024), where mistakes are not failures but key opportunities to enhance reasoning.A few multimodal approaches use corrupted prompts to create negative samples for preference learning, aiming to improve image comprehension (Wang et al., 2023;Deng et al., 2024).However, these methods fail to generate reasoning-aligned positive and negative CoT solutions, making them unsuitable for complex multimodal reasoning tasks.Thus, it remains unaddressed: how can MLLMs efficiently learn from mistakes to improve their reasoning skills?</p>
<p>To address the above two questions, this paper proposes R 3 V, a self-training framework that enables the model to Reflect on bootstrapped CoT Rationales, thereby strengthening its Vision-Language Reasoning.</p>
<p>Firstly, we leverage MLLM's pre-existing but weak CoT ability to bootstrap both rationales and answers for a given question, enabling the collection of a large number of positive and negative solutions based on answer correctness.Secondly, we introduce a reflection mechanism on negative solutions to help the model learn from mistakes.Specifically, we design selfrefine and self-select losses that guide the model to correct flawed rationales and derive the correct answer by comparing rationale candidates, respectively.The above synergistic process can be repeated, with improved samples boosting MLLM's reasoning and the enhanced model further improving rationale generation.Additionally, through selfselect training, our model can derive the superior solution from multiple samples, further boosting performance via test-time computation.</p>
<p>We conduct experiments across a wide range of multimodal reasoning benchmarks, including charts, geometry, commonsense, science, mathematics, etc. R 3 V progressively enhances the reasoning ability of MLLMs, delivering a 23%-60% relative accuracy improvement compared to GPT distillation, and consistently outperforming the strong self-training baseline, STaR (Zelikman et al., 2022).Moreover, our test-time selection is robust and effective, consistently surpassing Pass@1 and majority voting, even in OOD scenarios.</p>
<p>Our main contributions are as follows:</p>
<p>• We introduce an iterative self-training framework R 3 V that leverages CoT bootstrapped by MLLM itself for self-improvement.To our knowledge, this is the first attempt to apply self-training in vision-language reasoning.</p>
<p>• We propose learning from mistakes through selfreflection, with support for test-time computation to further improve reasoning performance.</p>
<p>• We perform extensive evaluations across 6 different multimodal domains to validate the effectiveness of R 3 V.Our analysis reveals the key factors driving the success of multimodal self-training.</p>
<p>Related Work</p>
<p>Vision-Language Reasoning Beyond the extensively studied unimodal reasoning (Cobbe et al., 2021;Sun et al., 2023), multimodal reasoning has recently attracted significant interest as an essential part of human intelligence (Yue et al., 2024;Lu et al., 2023).Although MLLMs perform well on general vision-language benchmarks (Liu et al., 2024;Chen et al., 2024b), integrating visual cues into the reasoning process poses unique challenges, especially for open-source models (Zhang et al., 2024a;Chen et al., 2024a).Several studies have explored using rationale datasets to fine-tune models and enhance visual-language reasoning capabilities.For example, (Gao et al., 2023;Zhang et al., 2024b) augmented existing mathematical datasets with rationales using GPT distillation, while (Yang et al., 2024) enhanced performance through manually collected CoT annotations.In this work, we advocate for MLLMs to self-improve, reducing reliance on resource-heavy rationale annotations.</p>
<p>Self-Training Methods Self-training helps the model learn from its own generated outputs, reducing the need for labor-intensive human annotations (Yuan et al., 2024;Chen et al., 2024c).Prior works have focused on enhancing the reasoning capacity of LLM.The typical approach involves sampling multiple rationales and filtering positive and negative solutions based on the answers.The LLM is then fine-tuned on the positive samples (Zelikman et al., 2022;Hosseini et al., 2024;Yuan et al., 2023) or improved using preference learning (Wang et al., 2024b;Mitra et al., 2024), such as DPO (Rafailov et al., 2024).Recent advances have also extended self-training to agents (Song et al., 2024) and neural symbolic (Xu et al., 2024) scenarios.In this paper, we pioneer the exploration of self-training in vision-language reasoning, investigate the failure of DPO in multimodal settings, and address these challenges with our R 3 V framework.</p>
<p>Methodology</p>
<p>Our self-training framework consists of two alternating components: (1) bootstrapping a large number of positive and negative CoT solutions for multimodal questions (Section 3.1); (2) using the above-sampled solutions to reflect on rationales and learn from mistakes (Section 3.2).This iterative process turns the MLLM from weak to strong.The overall framework is illustrated in Figure 2.</p>
<p>Preliminaries</p>
<p>In visual-language reasoning, given an image I and a question x, a multimodal large language model is required to integrate information from both the image and the question for reasoning, generating a CoT rationale r and then deriving the final answer a.However, due to the difficulty in collecting high-quality rationale data, constructing large-scale (I, x, r, a) pairs presents significant challenges.This hinders the enhancement of MLLM reasoning capacities through fine-tuning.</p>
<p>To overcome this limitation, we propose leveraging the MLLM's pre-existing but weak CoT capability to iteratively augment (I, x, r, a) pairs from the widely available vision question answering data (I, x, a), enabling the model to self-improve.Following STaR (Zelikman et al., 2022), the MLLM self-training process involves iteratively fine-tuning on its self-generated rationale data.In each iteration t, given a question x from training set D = {(I, x, â)}, the MLLM M first generate a CoT rationale r along with an answer a , formulated as {(r i , a i )} |D| i=1 .These intermediate outputs are then combined with the original training set, resulting in an augmented dataset that includes rationales:
D r = {(I i , x i , r i , a i )} |D| i=1 (1)
Assuming that rationales leading to correct answers are of higher quality compared to those that do not, we can divide D r into positive and negative sample sets based on the correctness of the answers:
D + r = {(I i , x i , r i , a i ) | a i = âi } |D| i=1
(2)
D − r = {(I i , x i , r i , a i ) | a i ̸ = âi } |D| i=1(3)
We then fine-tune model M on the filtered positive CoT samples D + r using supervised fine-tuning (SFT) with a negative log-likelihood objective:
L SF T = − (I,x,y)∼Dt log M(y | x, I),(4)
where the y = (r, a) is the solution generated by the model.We continue repeating the above process, generating new rationales with the newly finetuned model, until performance plateaus.</p>
<p>R 3 V: Reflection on Rationales</p>
<p>The above self-improvement process strengthens the model using positive solutions, while negative ones are typically discarded.However, negative samples comprise a large portion of the sampled solutions and offer valuable insights for further model enhancement (An et al., 2023;Hosseini et al., 2024).In our preliminary experiments, we found that the noisy nature of CoT in multimodal scenarios leads to suboptimal performance when using DPO (Rafailov et al., 2024).Inspired by the error-driven learning of humans, we introduce reflection on rationales, teaching the model to correct its own mistakes and reflect on multiple reasoning paths to identify the correct solution.Specifically, we propose additional self-refine (Section 3.2.1)and self-select (Section 3.2.2) losses for multitask learning.Our framework harnesses the continuous production of positive and negative samples in selftraining, offering a robust and effective solution for learning from mistakes.Appendix E provides examples of different components in R 3 V.</p>
<p>Self-Refine</p>
<p>Upon failing to solve a problem, human students will analyze the errors in their solutions and reflect on how to correct them.Inspired by this, we designed the self-refine mechanism to encourage the model to correct flaws in its generated solutions.Multiple positive and negative solutions sampled during self-training can be viewed as the model's repeated reasoning on the same problem, making them well-suited for self-refine training.Specifically, we construct dataset for self-refine as follows:
D REF = {(I i , x i , y + i , y − i ) | ∃ y + i , y − i } |D| i=1 ,(5)
where y + i and y − i are positive and negative samples obtained from preceding iterations.Next, the self-refine loss is employed to guide the model in correcting errors in its self-generated answers:
L REF = − (I,x,y + ,y − )∼D REF log M(y + | y − , x, I) (6)
Throughout the self-training iterations, samples for self-refine are continuously updated to incorporate higher-quality positive solutions and harder negative solutions.</p>
<p>Self-Select</p>
<p>Our early explorations reveal a key challenge in MLLM reasoning: current MLLMs frequently make simple errors such as misreading chart numbers or calculation mistakes, however, the autoregressive model has no mechanism to correct them, leading to suboptimal performance.In contrast, human reasoners implicitly simulate multiple reasoning paths, check for errors, and select the best one.Inspired by this, we introduce the self-selection mechanism, guiding MLLMs to derive the correct answer from multiple candidate solutions.</p>
<p>Given a set of sampled rationales, the model is required to analyze their differences and finally select the correct answer.Specifically, we construct the self-select dataset as follows:
D SEL = {(I i , x i , âi , C i ) | ∃ C i } |D| i=1 ,(7)
where â is the ground truth and
C i = (y 1 i , y 2 i , ..., y N i
) is a set of N sampled rationaleanswer pair.In our experiments, N is set to 3 by default.We ensure that the candidate set C contains at least one positive solution y + , allowing the model to select the final correct answer.Then, the self-select loss is defined as:
L SEL = − (I,x,â,C)∼D SEL log M(â|x, I, C)(8)
Finally, our framework combines three loss functions in a multi-task training setup to enhance MLLM reasoning (see algorithm in Appendix D):
L R 3 V = L SF T + L REF + L SEL (9)
From another perspective, we argue that this multitask training enables MLLMs to learn reasoning from easy to hard: selecting the correct solution from multiple candidates, refining existing rationales, and eventually generating solutions directly.</p>
<p>Test-Time Selection</p>
<p>Through self-select training, our framework enables MLLMs to reflect on their self-generated solutions and select the final answer from multiple reasoning paths.During inference, given a question x and corresponding image I, we first sample multiple reasoning solutions to form the candidate set C. Next, the MLLM is prompted to select the best answer from these candidate solutions: a = M(x, I, C).</p>
<p>Test-time selection offers a novel approach for MLLMs to tackle complex multimodal reasoning.Instead of directly generating an answer, the model applies an elimination method by comparing different reasoning paths and checking for errors (e.g., visual recognition, calculation, or reasoning mistakes) to identify the most likely correct solution.In this way, our approach further boosts reasoning performance through test-time computation.</p>
<p>Experiments</p>
<p>In our experiments, we focus on a diverse and comprehensive set of vision-language reasoning tasks to demonstrate the effectiveness of R 3 V.We begin by outlining the benchmarks (Section 4.1) and experimental setup (Section 4.2), followed by the main results of R 3 V on six widely used datasets (Section 4.3).We also evaluated the improvements Table 1: Main results on six vision-language reasoning benchmarks.Is CoT? column indicates whether a CoT or a direct answer was generated.Avg.column reports the average performance across all tasks (-indicates MiniWob is not applicable to this setting and is excluded from the average).R 3 V significantly improves upon the GPT-distilled baseline without additional annotation costs, and surpasses the strong baseline STaR by a large margin.</p>
<p>achieved by our framework in out-of-distribution (OOD) scenarios (Section 4.4).</p>
<p>Datasets</p>
<p>We validate our framework's self-improvement on six vision-language reasoning benchmarks, which require integrating visual information into complex, multi-step reasoning.Refer to Appendix B for detailed information of these benchmarks.</p>
<p>TabMWP (Lu et al., 2022)</p>
<p>Experimental Setings</p>
<p>We primarily compare our framework with three categories of methods to comprehensively assess its effectiveness.All experiments are conducted under the same parameters to ensure a fair comparison.</p>
<p>Zero-shot Methods.We evaluated the MLLMs' zero-shot performance under the direct prompt (where the model tends to provide an immediate answer (Liu et al., 2024)) and the CoT prompt using "Let's think step by step."GPT-4o was also chosen as a strong baseline for comparison.Supervised Fine-tuning Baselines.Since the selftraining requires existing (I, x, a) datasets, we provide the results of fine-tuning MLLMs using direct prompts on these question-answer pairs.We also include a GPT distillation baseline, where GPT-4o annotates CoT rationales for a small subset of each dataset, and then the open-source MLLMs are fine-tuned for CoT reasoning.</p>
<p>Self-Training Methods.We employ the aforementioned GPT-distilled, warmed-up MLLM as the starting point for self-training, iteratively sampling positive and negative rationales from training samples for continuous self-improvement.We then compare R 3 V with the well-known self-training baselines STaR (Zelikman et al., 2022), which iteratively fine-tunes on self-generated positive solutions for model improvement.</p>
<p>We use two established MLLMs, Qwen-VL (Bai et al., 2023) and LLaVA-1.5 (Liu et al., 2024), as  base models for self-training.We sample three solutions per sample in each iteration by default.The total number of iterations for these tasks is set to 4-5, depending on the convergence speed.Further details can be found in Appendix C.</p>
<p>Main Results</p>
<p>Table 1 presents the evaluation results of R 3 V on various multimodal reasoning tasks, including logical and numerical reasoning, agentic tasks, geometry, and multi-domain scenarios.The evolution progress of self-training is illustrated in Figure 3.</p>
<p>Self-training effectively converts MLLMs from weak to strong.Open-source MLLMs struggle with complex vision-language reasoning tasks.CoT reasoning with the "Let's think step by step" prompt (Zero-shot CoT) proves ineffective, with performance even worse than direct prompting (Zero-shot QA).In this situation, the self-training method leverages MLLMs' pre-existing but weak CoT capabilities to bootstrap multimodal CoT data for self-improvement.This process progressively elevates MLLMs' CoT reasoning, as shown in Figure 3, taking it to the next level on top of the GPTdistilled baseline.As an example with Qwen-VL, our self-training framework R 3 V delivers an average 32.8% relative performance improvement over the GPT-distilled baseline (48.47 → 64.37).This result highlights the remarkable potential of MLLMs to enhance their reasoning capabilities through self-training on synthetic data.on Qwen-VL).As shown in Figure 3, R 3 V demonstrates swift adaptation across different multimodal scenarios, achieving notably higher gains in the first iteration compared to the STaR baseline, highlighting the efficiency of our method.These results underscore the value of learning from mistakes in multimodal reasoning and demonstrate the effectiveness of our reflection-based methodology.
R 3 V</p>
<p>Out-of-Distribution Evaluation</p>
<p>Beyond the success of the R 3 V framework on in-domain benchmarks, we are curious whether its reasoning improvements can generalize to outof-distribution (OOD) and more difficult visionlanguage tasks.To this end, we aggregated the CoT rationales self-generated by R 3 V across in-domain benchmarks and constructed positive and negative pairs for continual training on Qwen-VL.For a fair comparison, we also included a baseline that uses only GPT-distilled positive CoT annotations.We conducted evaluations on three challenging benchmarks: (1) MMMU (Yue et al., 2024)</p>
<p>M3CoT</p>
<p>STaR Test@1 ( Zellers et al., 2019), a cognition-level visual understanding benchmark that requires reasoning based on common sense and visual content.
R 3 V Test@1 R 3 V Majority Voting R 3 V Test-Time Selection
R 3 V also strengthens multimodal reasoning in OOD scenarios.As shown in Table 2, after incorporating R 3 V's self-generated CoT reasoning data, Qwen-VL significantly outperforms both the zero-shot and GPT-distilled baselines.This demonstrates that the CoT annotations synthesized by our framework not only enhance MLLM in-domain reasoning but also generalize to OOD and more challenging vision-language tasks.</p>
<p>Test-time selection generalizes to unseen tasks.Somewhat surprisingly, we found that the test-time selection ability does generalize to unseen tasks.For example, on MMMU, sampling three times during inference combined with our self-select mechanism (see Section 3.2.3)led to further improvement (35.63 → 38.48).This suggests that through our self-select training, the MLLM has learned to compare multiple reasoning paths, identify errors (e.g., recognition or calculation mistakes), and eliminate incorrect options to arrive at the correct answer.</p>
<p>Analysis</p>
<p>This section analyzes the key factors behind the success of the R 3 V, as well as the potential challenges of self-training in multimodal reasoning tasks.</p>
<p>Ablation Studies</p>
<p>Reflection on self-generated CoT facilitates learning from mistakes.To validate the effectiveness of each part of our framework, we independently ablated the self-refine and self-select losses, denoted as w/o self-refine and w/o self-select.As shown in Table 3, both self-refine and self-select play a crucial role in improving performance.This highlights the value of negative samples, while our R 3 V framework's reflection mechanism (i.e., selfrefine and self-select losses) serves as an effective method for learning from mistakes.</p>
<p>Iterative training process is crucial for selfimprovement.Next, we ablated iterative training as w/o iteration: instead of iteratively sampling and training, we sampled a large batch at once.For example, iterative self-training samples three times per round over four rounds, while w/o iteration samples 3 × 4 = 12 times in a single pass.This approach is similar to Rejection Sampling Finetuning (RFT; Yuan et al. (2023)), but includes our self-refine and self-select losses.The results in Table 3 demonstrate the importance of iteratation.</p>
<p>Although w/o iteration produces a large number of positive and negative samples (comparable to R 3 V by our statistics), the progressive training process yields higher-quality, more diverse samples, which boosts self-training performance.Table 4: Comparison between R 3 V and the reinforced baseline (DPO).Due to the noisy nature of CoT in multimodal scenarios, the DPO method struggles to efficiently learn from mistakes and improve performance.</p>
<p>Test-Time Scaling for Majority Voting and Self-Select</p>
<p>Test-time Compute</p>
<p>Test-time self-selection boosts performance through sampling.One key advantage of R 3 V framework lies in its capacity to enhance performance by scaling test-time computation: during inference, we sample multiple candidate solutions and apply self-select to choose the answer.Figure 4 compares self-selection with Test@1 and majority voting with a sample size of 3. Our self-selection method consistently outperforms Test@1 and majority voting across all tasks.While majority voting reduces noise by aggregating results, self-selection goes further by deeply comparing reasoning paths, eliminating incorrect options, and ultimately analyzing to reach the correct answer.excessive sample size, which we believe stronger base models could address.</p>
<p>Our self-training framework requires no manual annotation, instead synthesizing large-scale positive and negative CoT rationales through sampling, equipping the model with the capacity for selfreflection during reasoning.It also opens up new opportunities for boosting MLLM reasoning performance by scaling test-time computation.</p>
<p>The Noisy Nature of Multimodal CoT</p>
<p>In our preliminary study, we found that the widelyused preference learning method DPO (Rafailov et al., 2024) struggles to leverage positive and negative solutions for further improvement in multimodal settings.As shown in Table 4, equipping STaR with DPO training yields minimal improvement and falls short of our R 3 V.</p>
<p>To investigate DPO's failure, we closely examined the positive and negative samples selfgenerated by the MLLM (see details and case study in Appendix G).For each task, we randomly selected 100 positive solutions based on answer correctness and manually categorized their CoT fidelity as correct, partially correct, or incorrect.As shown in Figure 6, unlike natural language reasoning tasks (e.g., Logic, Math), multimodal CoT   contains significant noise, with the proportion of fully correct CoT ranging from 8% to 70%.This stems from MLLM's limited recognition capabilities, leading to flawed CoT despite correct answers, such as OCR errors.As a result, faulty reasoning in noisy CoT is often misjudged as better solutions, making it challenging for DPO to distinguish between correct and incorrect reasoning paths and ultimately reducing performance (Chowdhury et al., 2024).In contrast, our reflection method avoids encouraging the generation of faulty solutions, instead guiding the model to select the correct answer through elimination, demonstrating greater efficiency in noisy multimodal CoT scenarios.</p>
<p>Generalization to Stronger Backbone</p>
<p>To demonstrate generalizability, we applied R 3 V to the latest advanced MLLM, Qwen2-VL (Wang et al., 2024a) , evaluating its ability to selfimprove in solving geometric problems (Chen et al., 2021).As shown in Figure 7a, even without GPT-distilled warmup, R 3 V achieves significant self-improvement by leveraging the model's pre-existing CoT abilities, demonstrating the R 3 V's generalizability across backbones.More impressively, we found that test-time selection demonstrates superior scalability on Qwen2-VL, markedly outperforming majority voting, as illustrated in Figure 7b.We hypothesize that the enhanced general capabilities of the base model further amplify the effectiveness of self-select, which we leave for future exploration.</p>
<p>Conclusion</p>
<p>The scarcity of multimodal CoT data limits the reasoning capabilities of current MLLMs.In this paper, we take the first step toward enabling MLLMs to self-improve for better vision-language reasoning.We propose an iterative self-training framework, R 3 V, which continuously bootstraps positive and negative solutions and improves reasoning through reflection on self-generated CoT rationales.Meanwhile, R 3 V enables MLLMs to self-reflect on their generated solutions, offering new opportunities for boosting performance through test-time computation.Extensive experiments and analyses demonstrate the effectiveness of our framework and the key factors behind its success.</p>
<p>Limitations</p>
<p>As discussed in Section 5.3, due to the limitations of current MLLMs, the CoT annotations generated by R 3 V often contain noise.While our framework can self-improve performance on noisy multimodal CoT, we believe that higher-quality CoT will further enhance reasoning ability.Due to computational constraints, our main experiments were conducted on two well-known MLLMs, LLaVA and Qwen-VL.Expanding to larger and more advanced MLLMs could yield interesting results, which we plan to explore in future work.</p>
<p>C Training Details</p>
<p>We use the Qwen-VL and LLaVA-1.5 as the base model and conducted experiments on an MLLM training infrastructure * .We show the number of training and testing samples for each dataset, along with the amount of GPT annotations in Table 5.Our self-training process begins with the GPT-distilled warmup, where we fine-tune the base model using the training dataset augmented with GPT-4o CoT annotations.After this warm-up, the fine-tuned MLLM is employed to sample from the training dataset to build SFT, Self-Refine, and Self-Select data for our training in the R 3 V framework.We performed self-training in either four or five iterations, depending on performance saturates.The same training hyperparameters are used across all experiments, as detailed in Table 6.We employ DeepSpeed to train MLLM using the Zero2 strategy, maintaining a global batch size of 64.</p>
<p>D Algorithms</p>
<p>Algorithm 1 describes the overall process of R 3 V.The inner for-loop describes how we sample instances to build the proposed dataset, where we * https://github.com/TideDra/VL-RLHFalways select the most recent data.It is important to note that the sampled instances must be formatted as the data examples shown in Appendix E later.</p>
<p>E Examples of R 3 V Multi-task Learning</p>
<p>We illustrate examples of SFT, Self-Refine, and Self-Select in Figure 10.The input part in each subfigure shows the contexts such as question-choices pair and prompts used to guide the MLLM, and the output part shows the expected response from the MLLM.CoT prompt "Let's think step by step." will always be appended to the question-choices pairs.The prompts for self-refine and self-select vary slightly between multiple-choice and shortanswer tasks.Note that only the self-select prompt will be used for test-time reflection.</p>
<p>We add the sample generated by MLLM into both self-refine and self-select contexts using "Model Prediction" to divide with the questionchoices part.As illustrated in the figure, we use a green checkmark to indicate the positive solutions and a red cross to mark the negative ones.It highlights that R 3 V successfully builds negativepositive rationales pairs, from which the model can learn from mistakes in negative demonstrations.Additionally, R 3 V also builds diverse reasoning paths, ranging from completely wrong to correct rationales for the MLLM learning to choose from like human.</p>
<p>F Evolution Progress</p>
<p>Figure 11 shows the evolution progress of our R 3 V framework.</p>
<p>G Noisy Nature of Multimodal CoT</p>
<p>We manually reviewed the positive solutions generated by the Qwen-VL in our self-training process and evaluated the quality of its CoT reasoning.The CoT error in multimodal setting is significantly higher than samples from logical reasoning datasets (Liu et al., 2020;Yu et al., 2020) and math datasets (Cobbe et al., 2021;Hendrycks et al., 2021) in natural language setting.Multimodal CoT has considerable noise, such as visual perception error and symbol misinterpretation.We highlight this issue with case studies on the M 3 CoT and GeoQA dataset in Figure 9.</p>
<p>Figure 2 :
2
Figure 2: Overview of our multimodal self-training framework of R 3 V.It boosts vision-language reasoning by iteratively reflecting on bootstrapped CoT rationales and enables self-reflection through test-time computing.</p>
<p>Figure 3 :
3
Figure 3: Comparison of the iterative self-training process between R 3 V and STaR on Qwen-VL across four benchmarks.Full results are provided in Appendix F. R 3 V demonstrates higher efficiency in evolution and superior final performance.</p>
<p>Figure 4 :
4
Figure 4: Performance comparison of different test-time methods.Our test-time selection is robust and effective, consistently outperforming Test@1 and majority voting.</p>
<p>Figure 5 :
5
Figure 5: Comparison of scalability between test-time selection and majority voting.</p>
<p>R 3 VFigure 6 :
36
Figure 6: Proportion of correct rationales in solutions with correct answers.Multimodal CoT contains substantially more noise than text-based CoT.</p>
<p>(a) The effectiveness of our self-training framework R 3 V. × .× Test-Time Scaling for Majority Voting and Self-Select (b) Scalability between test-time selection and majority voting.</p>
<p>Figure 7 :
7
Figure 7: Evaluation result of Qwen2-VL on GeoQA.(a) shows that our self-training approach significantly enhances performance without GPT-distilled warmup.(b) demonstrates the superior scalability of test-time selection, which boosts performance through sampling.</p>
<p>Table 2 :
2further enhances self-training efficiency bylearning from mistakes. Instead of discardingvaluable negative samples, our R 3 V frameworkleverages carefully designed self-refine and self-select mechanisms to learn from negative solu-tions, surpassing the strong self-training baselineSTaR by a large margin (average 59.28 → 64.37
Evaluation results on OOD benchmarks.Ours (TTS) denotes Test-time Selection, a new feature introduced by our framework.The self-generated CoT data in R3V contributes to improving performance in more challenging scenarios.Test-time selection is also capable of generalizing to OOD settings.</p>
<p>Table 3 :
3
Ablation study of key components.w/o iteration refers to the ablation of iterative training, where we sample num_sample_per_iter * num_iter samples in a single pass.
MethodsLogical and Numerical reasoning TabMWP ChartQA CLEVR-Math MiniWob AgenticGeometry Multi-Domain GeoQA M 3 CoTAvgR 3 V83.2757.3668.8182.8939.2554.6664.37w/o self-refine80.8756.3264.5180.6738.3354.3162.50w/o self-select79.7255.3664.0079.1135.8150.6960.78w/o iteration78.5354.7264.5676.8736.0753.1160.6478 80 82 84 86 Performances (%)77.8481.33 81.77 TabMWP83.2752 54 56 58 6053.654.96 55.44 ChartQA57.3660 65 70 7561.45 CLEVR-Math 64.06 64.42 68.8134 36 38 40 4234.0838.46 UniGeo 35.4139.2548 50 52 54 5650.4751.8653.7154.667650553246
(Lu et al., 2023)ne dataset designed to evaluate various aspects of multimodal reasoning; (2) MathVista(Lu et al., 2023), which focuses specifically on mathematical reasoning in multimodal contexts; (3) VCR</p>
<p>A Additional Related WorkMultimodal Large Language Models and Multimodal Reasoning Driven by the advancement of Large Language Models (LLMs), the multimodal research community has recently witnessed a domain shift from Vision-Language Models (VLMs)(Radford et al., 2021;Li et al., 2022;Cheng et al., 2023)to Multimodal Large Language Models (MLLMs)(Achiam et al., 2023;Liu et al., 2023;Chen et al., 2023;Cheng et al., 2024).Unimodal reasoning has a strong research foundation, such as in mathematics(Hendrycks et al., 2021)and code generation(Sun et al., 2024).Multimodal reasoning requires models to integrate visual cues into the reasoning process(Zhang et al., 2023), presenting new challenges.Recent studies have explored synthesizing table or chart data and leveraging GPT to annotate CoT, aiming to enhance MLLM reasoning capabilities(Han et al., 2023;Jia et al., 2024).For instance,Huang et al. (2024)utilizes GPT to generate chart code and render it to obtain diverse chart reasoning samples.In this work, we do not rely on stronger models to synthesize new reasoning samples; instead, we enable MLLMs to achieve self-improvement from self-generated CoT data.Self-Training Methods Self-training, especially integrated with reinforcement learning from its own outputs, offers a promising avenue for model selfimprovement(Huang et al., 2022;Gulcehre et al., 2023).Recent studies have applied self-training to MLLMs with the goal of enhancing image comprehension, particularly in mitigating hallucinations(Zhou et al., 2024;Gunjal et al., 2024;Zhao et al., 2023).Deng et al. (2024)proposes constructing positive and negative sample pairs by perturbing images and prompts, and enhances alignment through DPO training.In contrast, this work focuses on complex reasoning in multimodal scenarios, which requires integrating visual cues to generate step-by-step reasoning CoT.To our knowledge, we are the first to explore self-training in the context of vision-language reasoning.B Vision-Language Reasoning BenchmarksTabMWP(Lu et al., 2022)ChartQA(Masry et al., 2022)We used the humanwritten version as the self-train benchmark, which contains more reasoning-intensive questions compared to the augmented split.This subset contains 7,398 chart figures and question pairs, comprising both free-text and multiple-choice questions.CLEVR-Math (Lindström and Abraham, 2022)The CLEVR-Math dataset consists of multimodal math word problems that combine text and images, where questions are posed about the state of the scene after a sequence of actions (like addition or subtraction of objects) have been applied.We randomly sampled 10000 instances for training and used the original test set.MiniWob(Shi et al., 2017)MiniWob asks MLLM to interact with a simulated Web environment.As shown in Figure8, the model is provided with an image of the web interface along with the html as input.It is then asked to generate Python code to simulate keyboard and mouse actions and complete the given task.GeoQA(Chen et al., 2021)GeoQA contains 4,998 multiple-choice geometric problems from Chinese middle school exams and annotated with solving programs.We use human translated English version provided by UniGeo(Chen et al., 2022).CoT is a manually verified multimodal, multi-domain, multi-step visual-language reasoning dataset.We use the official train/test splits in our R 3 V self-training process.Evaluation For structured outputs like GPTdistilled and self-train methods, we use the benchmark's default evaluation script to calculate metrics.For free-form outputs like the zero-shot CoT baseline, we employ GPT-4o-mini as the evaluator to assess accuracy.For MiniWob, the simulated web environment provides an automatic reward of Input:Web action simulation code:HTML Code: <div id="wrap">\n <div id="query">Select 129 with the slider and hit Submit.</div>\n<div id="area">\n <div id="slider" ><span tabindex="0" ></span></div>\n <div id="val">142</div>\n <button id="subbtn" >Submit</button>\n </div>\n</div> Task: Select 129 with the slider and hit Submit.if ∃ (s + 1 , s − 1 ) then 16:23:end if24:end for
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen, arXiv:2310.20689Learning from mistakes makes llm better reasoner. 2023arXiv preprint</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023arXiv preprint</p>
<p>UniGeo: Unifying geometry logical reasoning via reformulating mathematical expression. Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, Xiaodan Liang, 10.18653/v1/2022.emnlp-main.218Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P Xing, Liang Lin, arXiv:2105.14517Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning. 2021arXiv preprint</p>
<p>Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao, arXiv:2306.15195Shikra: Unleashing multimodal llm's referential dialogue magic. 2023arXiv preprint</p>
<p>M 3 CoT: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought. Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, Wanxiang Che, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand2024a1</p>
<p>How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Zhe Chen, Weiyun Wang, Shenglong Hao Tian, Zhangwei Ye, Erfei Gao, Wenwen Cui, Kongzhi Tong, Jiapeng Hu, Zheng Luo, Ma, arXiv:2404.168212024barXiv preprint</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, arXiv:2401.013352024carXiv preprint</p>
<p>Beyond generic: Enhancing image captioning with real-world knowledge using vision-language pre-training model. Kanzhi Cheng, Wenpo Song, Zheng Ma, Wenhao Zhu, Zixuan Zhu, Jianbing Zhang, Proceedings of the 31st ACM International Conference on Multimedia. the 31st ACM International Conference on Multimedia2023</p>
<p>Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, Zhiyong Wu, arXiv:2401.10935Seeclick: Harnessing gui grounding for advanced visual gui agents. 2024arXiv preprint</p>
<p>Ray Sayak, Anush Chowdhury, Nagarajan Kini, Natarajan, arXiv:2403.00409Provably robust dpo: Aligning language models with noisy feedback. 2024arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, arXiv:2305.065002023Preprint</p>
<p>Enhancing large vision language models with selftraining on image comprehension. Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, Wei Wang, arXiv:2405.197162024arXiv preprint</p>
<p>Anders Ericsson, Herbert A Simon, Verbal reports as data. 198087215</p>
<p>G-llava: Solving geometric problem with multi-modal large language model. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, arXiv:2312.113702023arXiv preprint</p>
<p>Reinforced selftraining (rest) for language modeling. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, arXiv:2308.089982023arXiv preprint</p>
<p>Detecting and preventing hallucinations in large vision language models. Anisha Gunjal, Jihan Yin, Erhan Bas, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, Hanwang Zhang, arXiv:2311.16483Chartllama: A multimodal llm for chart understanding and generation. 2023arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, arXiv:2402.06457Alessandro Sordoni, and Rishabh Agarwal. 2024. V-star: Training verifiers for self-taught reasoners. arXiv preprint</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister, The 61st Annual Meeting Of The Association For Computational Linguistics. 2023</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022arXiv preprint</p>
<p>Evochart: A benchmark and a self-training approach towards real-world chart understanding. Muye Huang, Lai Han, Xinyu Zhang, Wenjun Wu, Jie Ma, Lingling Zhang, arXiv:2409.01577Jun Liu. 2024arXiv preprint</p>
<p>Describe-then-reason: Improving multimodal mathematical reasoning through visual comprehension training. Mengzhao Jia, Zhihan Zhang, Wenhao Yu, Fangkai Jiao, Meng Jiang, arXiv:2404.146042024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. PMLR2022</p>
<p>Adam Dahlgren, Lindström , Savitha Sam, Abraham , arXiv:2208.05358Clevr-math: A dataset for compositional language, visual and mathematical reasoning. 2022arXiv preprint</p>
<p>Visual spatial reasoning. Fangyu Liu, Guy Emerson, Nigel Collier, Transactions of the Association for Computational Linguistics. 112023</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, arXiv:2007.081242020arXiv preprint</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, arXiv:2310.02255Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, arXiv:2209.146102022arXiv preprint</p>
<p>Ahmed Masry, Xuan Do, Jia Long, Shafiq Qing Tan, Enamul Joty, Hoque, arXiv:2203.10244Chartqa: A benchmark for question answering about charts with visual and logical reasoning. 2022arXiv preprint</p>
<p>Arindam Mitra, Hamed Khanpour, Corby Rosset, Ahmed Awadallah, arXiv:2402.14830Orca-math: Unlocking the potential of slms in grade school math. 2024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>World of bits: An open-domain platform for web-based agents. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang, International Conference on Machine Learning. PMLR2017</p>
<p>Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee, arXiv:2406.17294arXiv:2403.02502Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization for llm agents. 2024arXiv preprint</p>
<p>Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, arXiv:2403.14734A survey of neural code intelligence: Paradigms, advances and beyond. 2024arXiv preprint</p>
<p>Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, Lingpeng Kong, arXiv:2310.00280Corex: Pushing the boundaries of complex reasoning through multi-model collaboration. 2023arXiv preprint</p>
<p>Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, arXiv:2308.15126Evaluation and analysis of hallucination in large vision-language models. 2023arXiv preprint</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, arXiv:2409.121912024aarXiv preprint</p>
<p>Selftraining with direct preference optimization improves chain-of-thought reasoning. Tianduo Wang, Shichen Li, Wei Lu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024b1</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Interactive evolution: A neural-symbolic self-training framework for large language models. Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, Zhiyong Wu, arXiv:2406.117362024arXiv preprint</p>
<p>Mathglmvision: Solving mathematical problems with multimodal large language model. Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, Yuxiao Dong, Jie Tang, arXiv:2409.137292024arXiv preprint</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, arXiv:2002.04326Reclor: A reading comprehension dataset requiring logical reasoning. 2020arXiv preprint</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, arXiv:2401.10020Self-rewarding language models. 2024arXiv preprint</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Advances in Neural Information Processing Systems. 202235Jesse Mu, and Noah Goodman</p>
<p>From recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, arXiv:2403.146242024aarXiv preprint</p>
<p>Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, arXiv:2407.08739Mavis: Mathematical visual instruction tuning. 2024barXiv preprint</p>
<p>Multimodal chain-of-thought reasoning in language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, arXiv:2302.009232023arXiv preprint</p>
<p>Beyond hallucinations: Enhancing lvlms through hallucinationaware direct preference optimization. Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, Conghui He, arXiv:2311.168392023arXiv preprint</p>
<p>Calibrated self-rewarding vision language models. Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, Huaxiu Yao, arXiv:2405.146222024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
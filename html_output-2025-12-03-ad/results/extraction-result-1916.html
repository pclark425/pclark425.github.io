<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1916 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1916</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1916</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-279391992</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.11261v1.pdf" target="_blank">Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Robotic manipulation faces a significant challenge in generalizing across unseen objects, environments and tasks specified by diverse language instructions. To improve generalization capabilities, recent research has incorporated large language models (LLMs) for planning and action execution. While promising, these methods often fall short in generating grounded plans in visual environments. Although efforts have been made to perform visual instructional tuning on LLMs for robotic manipulation, existing methods are typically constrained by single-view image input and struggle with precise object grounding. In this work, we introduce Gondola, a novel grounded vision-language planning model based on LLMs for generalizable robotic manipulation. Gondola takes multi-view images and history plans to produce the next action plan with interleaved texts and segmentation masks of target objects and locations. To support the training of Gondola, we construct three types of datasets using the RLBench simulator, namely robot grounded planning, multi-view referring expression and pseudo long-horizon task datasets. Gondola outperforms the state-of-the-art LLM-based method across all four generalization levels of the GemBench dataset, including novel placements, rigid objects, articulated objects and long-horizon tasks.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1916.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1916.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gondola</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gondola: Grounded Vision-Language Planning for Generalizable Robotic Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-view grounded vision-language planning model that generates interleaved textual plans and per-view segmentation masks (via a <seg> token and SAM2) to produce next-step, visually-grounded high-level actions for robotic manipulation; trained on synthetic multi-view RLBench datasets (robot grounded planning, multi-view referring expressions, pseudo long-horizon) and integrated with a 3D motion planner for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gondola</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal model: frozen InternVL ViT encoder (InternVL-300M) producing per-view image patch embeddings + view tokens, frozen InternVL-4B LLM (fine-tuned with LoRA) that emits textual plans and a specialized <seg> token; a 2-layer MLP projects the <seg> hidden state to a prompt for SAM2 which decodes per-view segmentation masks. Outputs a grounded plan tuple (action name, object description, object masks across K views, location description, location masks).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining for InternVL components; pretrained segmentation model (SAM2); LLM pretrained as part of InternVL-4B (multimodal/vision-language).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Backbone InternVL and Sa2VA components were pretrained on large-scale visual-linguistic data (image-text and video-text grounding datasets) and SAM2 on large-scale segmentation data; the paper states these pretraining sources contain visual grounding and dense image/video grounding annotations (object descriptions, referring expressions and spatial language), but no explicit affordance/action-labelled robot data is claimed in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (vision-and-language conditioned high-level planning + 3D motion planning execution)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tabletop manipulation tasks from GemBench (RLBench simulator) across four generalization levels: L1 new placements, L2 novel rigid objects, L3 novel articulated objects, L4 unseen long-horizon tasks. High-level action space is discrete (actions like grasp, move grasped object, rotate grasped object, push down, push forward, release); low-level continuous motion is produced by an external 3D motion planning policy (3D-LOTUS++ policy used). Inputs: multi-view RGB (K=4) and history plans; evaluation in simulation (RLBench/GemBench) and limited real-robot fine-tuning/execution.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The paper explicitly fine-tunes the vision-language components on synthetic RLBench datasets to align language and perception for the target tasks; the constructed datasets include object names, multi-view segmentation masks and stepwise plans, so overlap in object descriptions and spatial referring expressions is created via supervised fine-tuning (but no formal quantification of corpus overlap between pretraining and RLBench is provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Grounded planning (multi-view+history, mask outputs) token accuracy and grounding (Table 1, best mask model with multi-view+history): L1 action/object acc 100%/100%, mask IoU ~87.9%; L2 action/object acc 99.0%/93.3%, mask IoU ~79.2%; L3 action/object acc 88.6%/83.9%, mask IoU ~66.4%; L4 action/object acc 79.4%/44.9%, mask IoU ~40.0%. End-to-end task execution (Gondola integrated with 3D motion policy, action chunk=5): success rates (SR) on GemBench test split: L1 87.3±1.9%, L2 74.8±1.8%, L3 52.4±2.1%, L4 19.0±1.0% (Table 3). The paper also reports that Gondola outperforms a leading LLM-based baseline (3D-LOTUS++) by absolute +10.3% (L2), +10.9% (L3) and +1.6% (L4) in success rate (text claim).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No direct 'no language pretraining' ablation is reported. The paper compares to non-LLM end-to-end policies (e.g., PolarNet, Hiveformer, PerAct variants) and to 3D-LOTUS++ (LLM-based baseline). It reports that non-LLM end-to-end policies perform well on seen tasks (L1) but generalize worse on L2-L4. Exact comparable metrics for a pure vision-only pretrained Gondola variant are not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper does not present a quantitative sample-efficiency comparison (e.g., number of demonstrations to reach a given performance) between language/vision-language pretraining vs non-pretrained baselines. Training details provided: ~15k robot-grounded planning tuples, ~15k multi-view examples and ~58k referring expressions; training used 10k iterations on 8 NVIDIA H100 GPUs (3 hours reported). No explicit sample-efficiency curves or demo-count comparisons are given.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention-map analysis or visualization of what the LLM or ViT attends to is provided in the paper. The grounding mechanism is evaluated via mask IoU metrics rather than attention visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No analysis of embedding space structure, clustering or representational geometry (e.g., PCA, t-SNE) is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes — concrete evidence: Gondola produces per-view segmentation masks tied to object/location mentions via a <seg> token and SAM2 prompting; mask IoU is measured quantitatively in grounded planning evaluation (reported per-level IoUs). Ablations show mask-based outputs outperform box-based outputs (box-based variant suffers from format errors and yields lower action/object accuracy and mask IoU). Multi-view inputs and the multi-view referring-expression dataset improve grounding quality (higher IoU). These are direct demonstrations of language-to-perception grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No multi-scale or hierarchical feature analyses (e.g., edge/texture vs object/scene level benefits) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer success is evaluated across GemBench L1-L4; positive factors: multi-view inputs (mitigate occlusion) and multi-view referring-expression fine-tuning improve grounding and transfer; history-plan context helps on L2/L3 but can harm L4 due to distribution shift in history plans; augmenting training with pseudo long-horizon concatenated tasks mitigates the history distribution shift and improves L4 performance. Domain shift to real robot showed decreased multi-view consistency and more segmentation errors; limited real-world data reduced performance.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Yes — evaluation explicitly across novel conditions: L1 (new placements), L2 (novel rigid objects), L3 (novel articulated objects), L4 (unseen long-horizon tasks). Grounded planning metrics drop from L1 to L2/L3/L4 (example best-mask multi-view+history numbers: L1 IoU ~87.9% -> L2 ~79.2% -> L3 ~66.4% -> L4 ~40.0%), showing reduced performance on novel objects/conditions versus familiar ones.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>The model is trained only on the GemBench training split and evaluated on held-out L2-L4 tasks (i.e., zero-shot generalization to unseen objects/long-horizon tasks). The reported success rates above reflect this (no few-shot adaptation at evaluation reported).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No per-layer importance or probing analysis is provided. Practical training setup: InternVL encoders and InternVL-4B LLM are frozen; the LLM is fine-tuned via LoRA (rank 128) and the SAM2 mask decoder is fine-tuned — but no ablations freezing/unfreezing other components on transfer are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — including history plans as input initially caused a significant performance drop on L4 (long-horizon) due to a distribution shift in history plans: the model tended to generate purely textual plans rather than grounded plans under that distribution. The issue was mitigated by adding pseudo long-horizon concatenated tasks to the fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>The paper compares to non-LLM vision-only or 3D-only policies (end-to-end methods) and reports that LLM-based planning (including Gondola) improves generalization on L2-L4, at the cost of sometimes reduced L1 performance; specific numbers: Gondola achieves higher success rates on novel object/generalization levels compared to prior non-LLM methods, but exact per-method deltas depend on the method (tableized in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit analysis of how representations or performance evolve during fine-tuning (early vs late training phases) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No measurements of representational dimensionality or intrinsic dimension are provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1916.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1916.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sa2VA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sa2VA (as used by Gondola)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense grounding vision-language model that integrates a strong VLM (InternVL) with SAM2 segmentation to produce dense visual grounding (masks) tied to language; Gondola builds upon and fine-tunes Sa2VA for multi-view robotic grounding and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sa2VA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dense grounding VLM that couples InternVL visual-language features with SAM2 segmentation via a specialized segmentation-token/prompting mechanism to produce region-level masks corresponding to language references.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining (InternVL) combined with segmentation pretraining (SAM2); Sa2VA itself was trained on large-scale image and video grounding data (per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>The paper states Sa2VA was trained on large-scale image and video grounding datasets (dense grounding annotations, referring expressions, video grounding). No robot-specific affordance/action labels are claimed for Sa2VA's pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Dense visual grounding / referring segmentation as subcomponent for robotic planning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used to produce per-view segmentation prompts/masks for objects and target locations in multi-view multi-step manipulation contexts (RLBench/GemBench simulated tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Sa2VA's pretraining on dense grounding datasets provides semantic region-language alignment that Gondola leverages and further adapts via RLBench fine-tuning to align with robot task vocabulary and multi-view inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Paper reports Sa2VA as state-of-the-art dense grounding prior to fine-tuning; Gondola fine-tunes Sa2VA for improved multi-view mask generation — concrete mask IoU gains for Gondola are reported in Tables 1–2 (see Gondola entry).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency comparisons for Sa2VA are provided in the Gondola paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in Gondola paper for Sa2VA.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided in Gondola paper for Sa2VA.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Sa2VA provides dense language-to-region grounding (masks) which is the core mechanism Gondola uses to ground object/location mentions to perception; Gondola demonstrates that mask-based grounding (via Sa2VA+SAM2) yields higher IoU and better downstream planning vs box-based outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Gondola fine-tunes Sa2VA on multi-view referring expressions and robot planning tuples to transfer dense grounding to multi-view robot scenes — multi-view fine-tuning improves robustness to occlusion.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not separately quantified for Sa2VA alone; Gondola fine-tuning shows results on novel objects (GemBench L2/L3).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not evaluated in isolation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported specifically for Sa2VA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Sa2VA is explicitly multimodal (vision+language). The paper argues mask-based VLM outputs (Sa2VA style) are superior to box-based textual numeric outputs for grounding in manipulation contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1916.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1916.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL (InternVL-300M visual encoder & InternVL-4B LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained vision-language foundation model family used as Gondola's visual encoder (InternVL-300M ViT) and LLM backbone (InternVL-4B); these components are frozen in Gondola and adapted via small MLPs and LoRA respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVL (InternVL-300M / InternVL-4B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InternVL-300M: ViT-based image encoder producing patch embeddings (input 448x448) followed by a small MLP adapter (trainable). InternVL-4B: multimodal LLM used to process concatenated multi-view image tokens and textual context; in Gondola the base parameters are frozen and LoRA adapters are added for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on large-scale visual-linguistic data (image-text/video-text).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>InternVL was trained on large visual-linguistic corpora; according to citations it covers diverse visual captioning and V+L tasks (object descriptions, scene descriptions, grounding style datasets). The Gondola paper uses these pretrained features as a starting point for robotic grounding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Perception and language processing for grounded planning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Produce multimodal tokens representing multiple camera views and language context to allow the LLM to output action and grounding tokens; used in RLBench/GemBench tabletop manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>InternVL provides generic visual-linguistic alignment; Gondola fine-tunes adapters (MLP for image features and LoRA for LLM) with RLBench-derived grounded planning data to align vocabulary (action/object names) and segmentation prompts to the robot domain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Gondola's reported grounded planning and task execution performance uses InternVL components as the pretrained backbone; see Gondola entry for task metrics (grounded planning accuracies and task SR).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No ablation where InternVL is trained from scratch is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency data comparing InternVL-pretrained vs non-pretrained is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No internal attention analyses of InternVL are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>InternVL features are used as the visual-linguistic substrate enabling the LLM to issue a <seg> hidden state that is converted into SAM2 prompts, giving operational evidence that InternVL-based multimodal features can be grounded to segmentation masks after task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>InternVL's generic pretraining plus Gondola's task-specific fine-tuning enable transfer to unseen GemBench tasks (zero-shot generalization). The paper highlights that frozen backbones with adapter tuning (LoRA/MLP) suffice to transfer to the embodied planning domain when combined with synthetic RLBench fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not separately reported for InternVL alone; the overall system generalization to novel objects is reported in Gondola metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>The system demonstrates zero-shot generalization to held-out GemBench levels after fine-tuning on training split (not few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layerwise ablations; only that backbones are frozen and LoRA/MLP adapters are used.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not specifically attributed to InternVL layers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>InternVL is multimodal (vision+language); no direct vision-only InternVL ablation is given.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1916.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1916.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAM2 (Segmentation model used for mask decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Segmentation model (SAM2) used by Gondola to decode per-view binary masks from a prompt embedding produced by projecting the LLM's <seg> hidden state; SAM2 encoder is frozen but the mask decoder is fine-tuned in Gondola.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Segment anything.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SAM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Promptable segmentation model: given a prompt embedding (from Gondola's LLM <seg> hidden state projection), SAM2 produces a binary segmentation mask per view. In Gondola the SAM2 encoder is frozen while its mask decoder is fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Segmentation pretraining on large-scale segmentation data (SAM family).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>SAM2 was pretrained for generic segmentation tasks across many images and videos (large-scale segmentation datasets) — used here as a general-purpose mask decoder that can be prompted by LLM-derived embedding vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Per-view instance/location segmentation for language-conditioned manipulation planning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Produce K binary masks for each referenced object/location across multi-view input images to enable 3D consolidation of segmented points for 3D motion planning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>SAM2 provides region-level segmentation which is coupled to language references via Gondola's learned projection from the LLM <seg> embedding; alignment is achieved through joint fine-tuning of the LLM LoRA layers and the SAM2 mask decoder on RLBench grounding data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Mask IoU improvement is reported when using end-to-end mask generation (SAM2 decoding from <seg>) vs box-based variants; Gondola's best mask IoUs per level (see Gondola entry) demonstrate effective grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>SAM2 is the mechanism that converts language-conditioned prompt embeddings to concrete per-view masks; improved downstream task success correlates with higher SAM2-produced mask IoU after fine-tuning, supporting that language tokens are grounded into perception via SAM2.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Fine-tuning SAM2 mask decoder on multi-view referring and planning data improves mask accuracy and transfer to unseen GemBench tasks; paper notes real-world multi-view consistency issues reduce SAM2 mask reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Mask IoU degrades for L2/L3/L4 relative to L1 indicating lower segmentation reliability on novel/unseen objects without further fine-tuning or more varied data.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Gondola uses SAM2 zero-shot (pretrained) then fine-tunes mask decoder; explicit few-shot numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Only mask decoder fine-tuned; no layerwise analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not specifically reported for SAM2 alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>SAM2 is vision-only segmentation backbone but is used here in a multimodal loop; no direct comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1916.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1916.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LOTUS++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LOTUS++ (LLM-guided 3D policy / prior SOTA LLM-based method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior LLM-based planning method used as a leading baseline; it uses extensive engineering and in-context learning to enable LLM-based planning without direct visual input and provides a 3D motion planning policy which Gondola reuses for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards generalizable vision-language robotic manipulation: A benchmark and LLM-guided 3D policy.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-LOTUS++</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-guided planning approach with engineered prompts and in-context learning used to produce high-level plans for driving a 3D motion planning policy; the original 3D-LOTUS family uses 3D point/voxel representations and LLM guidance but 3D-LOTUS++ in particular is described as using in-context learning without direct visual inputs for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>LLM pretraining plus engineered in-context learning (details from the referenced work); not a pure vision-language pretrained VLM in the same sense as InternVL/Sa2VA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this Gondola paper beyond being LLM-based and using in-context examples; the baseline relies on LLM textual capabilities rather than direct V+L pretraining for visual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>LLM-guided 3D robotic manipulation planning (GemBench / RLBench)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>High-level planning for manipulation tasks; in prior work the LLM is used to propose plans which are re-ranked or handed to a 3D motion policy. In Gondola experiments the 3D-LOTUS++ motion planning policy is reused to allow fair comparisons on task execution.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>3D-LOTUS++ performs planning largely via LLM capabilities with engineering and in-context examples; it lacks direct visual grounding in Gondola authors' description, and thus requires additional mechanisms (e.g., value predictors, affordance models) in prior literature to link text plans to perception.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Gondola reports absolute improvements over 3D-LOTUS++: +10.3% SR on L2 (novel rigid objects), +10.9% SR on L3 (novel articulated objects), and +1.6% SR on L4 (long-horizon) (text claim in paper). Exact base 3D-LOTUS++ SR numbers are given in the paper's tables (refer to the original for per-level values).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable in the same sense; 3D-LOTUS++ is itself an LLM-based method — comparisons to purely vision-only baselines are provided elsewhere in the paper (non-LLM policies do better on L1 but worse on L2-L4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The Gondola paper does not provide sample-efficiency numbers comparing 3D-LOTUS++ to Gondola (e.g., required fine-tuning examples).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not in Gondola paper (would be in original 3D-LOTUS++ paper if present).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not in Gondola paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Gondola argues that 3D-LOTUS++ relies more on textual planning and engineered prompts without direct visual mask grounding and that this leads to less precise grounding compared to Gondola's mask-based plans; Gondola's superior performance on novel-object tasks is presented as evidence that direct visual grounding (masks) helps.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported in Gondola paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>3D-LOTUS++ reportedly generalizes via LLM reasoning and in-context learning, but lacks visual grounding which the Gondola authors identify as a limitation for precise manipulation in 3D scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Gondola reports that it outperforms 3D-LOTUS++ notably on novel-object levels (L2/L3) as above; this implies 3D-LOTUS++ generalization is weaker for novel visual instances when not tightly grounded.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>3D-LOTUS++ relies on in-context learning and LLM capabilities (few-shot/in-context) in its original formulation; Gondola contrasts this with its grounded fine-tuning approach. Gondola does not provide exact in-paper counts of shots used by 3D-LOTUS++.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Gondola presents qualitative critique: LLM-only planning without grounded perception can be unstable and may produce ungrounded plans; no numeric negative-transfer analysis of 3D-LOTUS++ is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>The paper contrasts 3D-LOTUS++ (LLM-based planning with limited visual grounding) and pure vision-only end-to-end policies: vision-only methods do well on seen tasks but generalize worse to novel objects/environments than LLM-based approaches with grounding; Gondola (VLM-based grounded LLM) improves on prior LLM methods by providing explicit visual masks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported in Gondola paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. <em>(Rating: 2)</em></li>
                <li>Towards generalizable vision-language robotic manipulation: A benchmark and LLM-guided 3D policy. <em>(Rating: 2)</em></li>
                <li>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. <em>(Rating: 2)</em></li>
                <li>Segment anything. <em>(Rating: 2)</em></li>
                <li>3D-LOTUS [14] <em>(Rating: 1)</em></li>
                <li>PerAct: (PerAct is cited in related work as a 3D voxel/point-cloud based policy) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1916",
    "paper_id": "paper-279391992",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "Gondola",
            "name_full": "Gondola: Grounded Vision-Language Planning for Generalizable Robotic Manipulation",
            "brief_description": "A multi-view grounded vision-language planning model that generates interleaved textual plans and per-view segmentation masks (via a &lt;seg&gt; token and SAM2) to produce next-step, visually-grounded high-level actions for robotic manipulation; trained on synthetic multi-view RLBench datasets (robot grounded planning, multi-view referring expressions, pseudo long-horizon) and integrated with a 3D motion planner for execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gondola",
            "model_description": "Multimodal model: frozen InternVL ViT encoder (InternVL-300M) producing per-view image patch embeddings + view tokens, frozen InternVL-4B LLM (fine-tuned with LoRA) that emits textual plans and a specialized &lt;seg&gt; token; a 2-layer MLP projects the &lt;seg&gt; hidden state to a prompt for SAM2 which decodes per-view segmentation masks. Outputs a grounded plan tuple (action name, object description, object masks across K views, location description, location masks).",
            "pretraining_type": "Vision-language pretraining for InternVL components; pretrained segmentation model (SAM2); LLM pretrained as part of InternVL-4B (multimodal/vision-language).",
            "pretraining_data_description": "Backbone InternVL and Sa2VA components were pretrained on large-scale visual-linguistic data (image-text and video-text grounding datasets) and SAM2 on large-scale segmentation data; the paper states these pretraining sources contain visual grounding and dense image/video grounding annotations (object descriptions, referring expressions and spatial language), but no explicit affordance/action-labelled robot data is claimed in pretraining.",
            "target_task_name": "Robotic manipulation (vision-and-language conditioned high-level planning + 3D motion planning execution)",
            "target_task_description": "Tabletop manipulation tasks from GemBench (RLBench simulator) across four generalization levels: L1 new placements, L2 novel rigid objects, L3 novel articulated objects, L4 unseen long-horizon tasks. High-level action space is discrete (actions like grasp, move grasped object, rotate grasped object, push down, push forward, release); low-level continuous motion is produced by an external 3D motion planning policy (3D-LOTUS++ policy used). Inputs: multi-view RGB (K=4) and history plans; evaluation in simulation (RLBench/GemBench) and limited real-robot fine-tuning/execution.",
            "semantic_alignment": "The paper explicitly fine-tunes the vision-language components on synthetic RLBench datasets to align language and perception for the target tasks; the constructed datasets include object names, multi-view segmentation masks and stepwise plans, so overlap in object descriptions and spatial referring expressions is created via supervised fine-tuning (but no formal quantification of corpus overlap between pretraining and RLBench is provided).",
            "performance_with_language_pretraining": "Grounded planning (multi-view+history, mask outputs) token accuracy and grounding (Table 1, best mask model with multi-view+history): L1 action/object acc 100%/100%, mask IoU ~87.9%; L2 action/object acc 99.0%/93.3%, mask IoU ~79.2%; L3 action/object acc 88.6%/83.9%, mask IoU ~66.4%; L4 action/object acc 79.4%/44.9%, mask IoU ~40.0%. End-to-end task execution (Gondola integrated with 3D motion policy, action chunk=5): success rates (SR) on GemBench test split: L1 87.3±1.9%, L2 74.8±1.8%, L3 52.4±2.1%, L4 19.0±1.0% (Table 3). The paper also reports that Gondola outperforms a leading LLM-based baseline (3D-LOTUS++) by absolute +10.3% (L2), +10.9% (L3) and +1.6% (L4) in success rate (text claim).",
            "performance_without_language_pretraining": "No direct 'no language pretraining' ablation is reported. The paper compares to non-LLM end-to-end policies (e.g., PolarNet, Hiveformer, PerAct variants) and to 3D-LOTUS++ (LLM-based baseline). It reports that non-LLM end-to-end policies perform well on seen tasks (L1) but generalize worse on L2-L4. Exact comparable metrics for a pure vision-only pretrained Gondola variant are not reported in the paper.",
            "sample_efficiency_comparison": "The paper does not present a quantitative sample-efficiency comparison (e.g., number of demonstrations to reach a given performance) between language/vision-language pretraining vs non-pretrained baselines. Training details provided: ~15k robot-grounded planning tuples, ~15k multi-view examples and ~58k referring expressions; training used 10k iterations on 8 NVIDIA H100 GPUs (3 hours reported). No explicit sample-efficiency curves or demo-count comparisons are given.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No explicit attention-map analysis or visualization of what the LLM or ViT attends to is provided in the paper. The grounding mechanism is evaluated via mask IoU metrics rather than attention visualization.",
            "embedding_space_analysis": "No analysis of embedding space structure, clustering or representational geometry (e.g., PCA, t-SNE) is presented.",
            "action_grounding_evidence": "Yes — concrete evidence: Gondola produces per-view segmentation masks tied to object/location mentions via a &lt;seg&gt; token and SAM2 prompting; mask IoU is measured quantitatively in grounded planning evaluation (reported per-level IoUs). Ablations show mask-based outputs outperform box-based outputs (box-based variant suffers from format errors and yields lower action/object accuracy and mask IoU). Multi-view inputs and the multi-view referring-expression dataset improve grounding quality (higher IoU). These are direct demonstrations of language-to-perception grounding.",
            "hierarchical_features_evidence": "No multi-scale or hierarchical feature analyses (e.g., edge/texture vs object/scene level benefits) are reported.",
            "transfer_conditions": "Transfer success is evaluated across GemBench L1-L4; positive factors: multi-view inputs (mitigate occlusion) and multi-view referring-expression fine-tuning improve grounding and transfer; history-plan context helps on L2/L3 but can harm L4 due to distribution shift in history plans; augmenting training with pseudo long-horizon concatenated tasks mitigates the history distribution shift and improves L4 performance. Domain shift to real robot showed decreased multi-view consistency and more segmentation errors; limited real-world data reduced performance.",
            "novel_vs_familiar_objects": "Yes — evaluation explicitly across novel conditions: L1 (new placements), L2 (novel rigid objects), L3 (novel articulated objects), L4 (unseen long-horizon tasks). Grounded planning metrics drop from L1 to L2/L3/L4 (example best-mask multi-view+history numbers: L1 IoU ~87.9% -&gt; L2 ~79.2% -&gt; L3 ~66.4% -&gt; L4 ~40.0%), showing reduced performance on novel objects/conditions versus familiar ones.",
            "zero_shot_or_few_shot": "The model is trained only on the GemBench training split and evaluated on held-out L2-L4 tasks (i.e., zero-shot generalization to unseen objects/long-horizon tasks). The reported success rates above reflect this (no few-shot adaptation at evaluation reported).",
            "layer_analysis": "No per-layer importance or probing analysis is provided. Practical training setup: InternVL encoders and InternVL-4B LLM are frozen; the LLM is fine-tuned via LoRA (rank 128) and the SAM2 mask decoder is fine-tuned — but no ablations freezing/unfreezing other components on transfer are reported.",
            "negative_transfer_evidence": "Yes — including history plans as input initially caused a significant performance drop on L4 (long-horizon) due to a distribution shift in history plans: the model tended to generate purely textual plans rather than grounded plans under that distribution. The issue was mitigated by adding pseudo long-horizon concatenated tasks to the fine-tuning data.",
            "comparison_to_vision_only": "The paper compares to non-LLM vision-only or 3D-only policies (end-to-end methods) and reports that LLM-based planning (including Gondola) improves generalization on L2-L4, at the cost of sometimes reduced L1 performance; specific numbers: Gondola achieves higher success rates on novel object/generalization levels compared to prior non-LLM methods, but exact per-method deltas depend on the method (tableized in paper).",
            "temporal_dynamics": "No explicit analysis of how representations or performance evolve during fine-tuning (early vs late training phases) is provided.",
            "dimensionality_analysis": "No measurements of representational dimensionality or intrinsic dimension are provided.",
            "uuid": "e1916.0"
        },
        {
            "name_short": "Sa2VA",
            "name_full": "Sa2VA (as used by Gondola)",
            "brief_description": "A dense grounding vision-language model that integrates a strong VLM (InternVL) with SAM2 segmentation to produce dense visual grounding (masks) tied to language; Gondola builds upon and fine-tunes Sa2VA for multi-view robotic grounding and planning.",
            "citation_title": "Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos.",
            "mention_or_use": "use",
            "model_name": "Sa2VA",
            "model_description": "Dense grounding VLM that couples InternVL visual-language features with SAM2 segmentation via a specialized segmentation-token/prompting mechanism to produce region-level masks corresponding to language references.",
            "pretraining_type": "Vision-language pretraining (InternVL) combined with segmentation pretraining (SAM2); Sa2VA itself was trained on large-scale image and video grounding data (per paper).",
            "pretraining_data_description": "The paper states Sa2VA was trained on large-scale image and video grounding datasets (dense grounding annotations, referring expressions, video grounding). No robot-specific affordance/action labels are claimed for Sa2VA's pretraining.",
            "target_task_name": "Dense visual grounding / referring segmentation as subcomponent for robotic planning",
            "target_task_description": "Used to produce per-view segmentation prompts/masks for objects and target locations in multi-view multi-step manipulation contexts (RLBench/GemBench simulated tasks).",
            "semantic_alignment": "Sa2VA's pretraining on dense grounding datasets provides semantic region-language alignment that Gondola leverages and further adapts via RLBench fine-tuning to align with robot task vocabulary and multi-view inputs.",
            "performance_with_language_pretraining": "Paper reports Sa2VA as state-of-the-art dense grounding prior to fine-tuning; Gondola fine-tunes Sa2VA for improved multi-view mask generation — concrete mask IoU gains for Gondola are reported in Tables 1–2 (see Gondola entry).",
            "performance_without_language_pretraining": "Not reported in this paper.",
            "sample_efficiency_comparison": "No explicit sample-efficiency comparisons for Sa2VA are provided in the Gondola paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in Gondola paper for Sa2VA.",
            "embedding_space_analysis": "Not provided in Gondola paper for Sa2VA.",
            "action_grounding_evidence": "Sa2VA provides dense language-to-region grounding (masks) which is the core mechanism Gondola uses to ground object/location mentions to perception; Gondola demonstrates that mask-based grounding (via Sa2VA+SAM2) yields higher IoU and better downstream planning vs box-based outputs.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Gondola fine-tunes Sa2VA on multi-view referring expressions and robot planning tuples to transfer dense grounding to multi-view robot scenes — multi-view fine-tuning improves robustness to occlusion.",
            "novel_vs_familiar_objects": "Not separately quantified for Sa2VA alone; Gondola fine-tuning shows results on novel objects (GemBench L2/L3).",
            "zero_shot_or_few_shot": "Not evaluated in isolation in this paper.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported specifically for Sa2VA.",
            "comparison_to_vision_only": "Sa2VA is explicitly multimodal (vision+language). The paper argues mask-based VLM outputs (Sa2VA style) are superior to box-based textual numeric outputs for grounding in manipulation contexts.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1916.1"
        },
        {
            "name_short": "InternVL",
            "name_full": "InternVL (InternVL-300M visual encoder & InternVL-4B LLM)",
            "brief_description": "Pretrained vision-language foundation model family used as Gondola's visual encoder (InternVL-300M ViT) and LLM backbone (InternVL-4B); these components are frozen in Gondola and adapted via small MLPs and LoRA respectively.",
            "citation_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.",
            "mention_or_use": "use",
            "model_name": "InternVL (InternVL-300M / InternVL-4B)",
            "model_description": "InternVL-300M: ViT-based image encoder producing patch embeddings (input 448x448) followed by a small MLP adapter (trainable). InternVL-4B: multimodal LLM used to process concatenated multi-view image tokens and textual context; in Gondola the base parameters are frozen and LoRA adapters are added for fine-tuning.",
            "pretraining_type": "Vision-language pretraining on large-scale visual-linguistic data (image-text/video-text).",
            "pretraining_data_description": "InternVL was trained on large visual-linguistic corpora; according to citations it covers diverse visual captioning and V+L tasks (object descriptions, scene descriptions, grounding style datasets). The Gondola paper uses these pretrained features as a starting point for robotic grounding tasks.",
            "target_task_name": "Perception and language processing for grounded planning",
            "target_task_description": "Produce multimodal tokens representing multiple camera views and language context to allow the LLM to output action and grounding tokens; used in RLBench/GemBench tabletop manipulation tasks.",
            "semantic_alignment": "InternVL provides generic visual-linguistic alignment; Gondola fine-tunes adapters (MLP for image features and LoRA for LLM) with RLBench-derived grounded planning data to align vocabulary (action/object names) and segmentation prompts to the robot domain.",
            "performance_with_language_pretraining": "Gondola's reported grounded planning and task execution performance uses InternVL components as the pretrained backbone; see Gondola entry for task metrics (grounded planning accuracies and task SR).",
            "performance_without_language_pretraining": "No ablation where InternVL is trained from scratch is reported.",
            "sample_efficiency_comparison": "No explicit sample-efficiency data comparing InternVL-pretrained vs non-pretrained is provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No internal attention analyses of InternVL are reported in this paper.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "InternVL features are used as the visual-linguistic substrate enabling the LLM to issue a &lt;seg&gt; hidden state that is converted into SAM2 prompts, giving operational evidence that InternVL-based multimodal features can be grounded to segmentation masks after task-specific fine-tuning.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "InternVL's generic pretraining plus Gondola's task-specific fine-tuning enable transfer to unseen GemBench tasks (zero-shot generalization). The paper highlights that frozen backbones with adapter tuning (LoRA/MLP) suffice to transfer to the embodied planning domain when combined with synthetic RLBench fine-tuning data.",
            "novel_vs_familiar_objects": "Not separately reported for InternVL alone; the overall system generalization to novel objects is reported in Gondola metrics.",
            "zero_shot_or_few_shot": "The system demonstrates zero-shot generalization to held-out GemBench levels after fine-tuning on training split (not few-shot).",
            "layer_analysis": "No layerwise ablations; only that backbones are frozen and LoRA/MLP adapters are used.",
            "negative_transfer_evidence": "Not specifically attributed to InternVL layers.",
            "comparison_to_vision_only": "InternVL is multimodal (vision+language); no direct vision-only InternVL ablation is given.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1916.2"
        },
        {
            "name_short": "SAM2",
            "name_full": "SAM2 (Segmentation model used for mask decoding)",
            "brief_description": "Segmentation model (SAM2) used by Gondola to decode per-view binary masks from a prompt embedding produced by projecting the LLM's &lt;seg&gt; hidden state; SAM2 encoder is frozen but the mask decoder is fine-tuned in Gondola.",
            "citation_title": "Segment anything.",
            "mention_or_use": "use",
            "model_name": "SAM2",
            "model_description": "Promptable segmentation model: given a prompt embedding (from Gondola's LLM &lt;seg&gt; hidden state projection), SAM2 produces a binary segmentation mask per view. In Gondola the SAM2 encoder is frozen while its mask decoder is fine-tuned.",
            "pretraining_type": "Segmentation pretraining on large-scale segmentation data (SAM family).",
            "pretraining_data_description": "SAM2 was pretrained for generic segmentation tasks across many images and videos (large-scale segmentation datasets) — used here as a general-purpose mask decoder that can be prompted by LLM-derived embedding vectors.",
            "target_task_name": "Per-view instance/location segmentation for language-conditioned manipulation planning",
            "target_task_description": "Produce K binary masks for each referenced object/location across multi-view input images to enable 3D consolidation of segmented points for 3D motion planning.",
            "semantic_alignment": "SAM2 provides region-level segmentation which is coupled to language references via Gondola's learned projection from the LLM &lt;seg&gt; embedding; alignment is achieved through joint fine-tuning of the LLM LoRA layers and the SAM2 mask decoder on RLBench grounding data.",
            "performance_with_language_pretraining": "Mask IoU improvement is reported when using end-to-end mask generation (SAM2 decoding from &lt;seg&gt;) vs box-based variants; Gondola's best mask IoUs per level (see Gondola entry) demonstrate effective grounding.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "SAM2 is the mechanism that converts language-conditioned prompt embeddings to concrete per-view masks; improved downstream task success correlates with higher SAM2-produced mask IoU after fine-tuning, supporting that language tokens are grounded into perception via SAM2.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Fine-tuning SAM2 mask decoder on multi-view referring and planning data improves mask accuracy and transfer to unseen GemBench tasks; paper notes real-world multi-view consistency issues reduce SAM2 mask reliability.",
            "novel_vs_familiar_objects": "Mask IoU degrades for L2/L3/L4 relative to L1 indicating lower segmentation reliability on novel/unseen objects without further fine-tuning or more varied data.",
            "zero_shot_or_few_shot": "Gondola uses SAM2 zero-shot (pretrained) then fine-tunes mask decoder; explicit few-shot numbers not provided.",
            "layer_analysis": "Only mask decoder fine-tuned; no layerwise analysis.",
            "negative_transfer_evidence": "Not specifically reported for SAM2 alone.",
            "comparison_to_vision_only": "SAM2 is vision-only segmentation backbone but is used here in a multimodal loop; no direct comparison provided.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1916.3"
        },
        {
            "name_short": "3D-LOTUS++",
            "name_full": "3D-LOTUS++ (LLM-guided 3D policy / prior SOTA LLM-based method)",
            "brief_description": "A prior LLM-based planning method used as a leading baseline; it uses extensive engineering and in-context learning to enable LLM-based planning without direct visual input and provides a 3D motion planning policy which Gondola reuses for fair comparison.",
            "citation_title": "Towards generalizable vision-language robotic manipulation: A benchmark and LLM-guided 3D policy.",
            "mention_or_use": "use",
            "model_name": "3D-LOTUS++",
            "model_description": "LLM-guided planning approach with engineered prompts and in-context learning used to produce high-level plans for driving a 3D motion planning policy; the original 3D-LOTUS family uses 3D point/voxel representations and LLM guidance but 3D-LOTUS++ in particular is described as using in-context learning without direct visual inputs for planning.",
            "pretraining_type": "LLM pretraining plus engineered in-context learning (details from the referenced work); not a pure vision-language pretrained VLM in the same sense as InternVL/Sa2VA.",
            "pretraining_data_description": "Not specified in this Gondola paper beyond being LLM-based and using in-context examples; the baseline relies on LLM textual capabilities rather than direct V+L pretraining for visual grounding.",
            "target_task_name": "LLM-guided 3D robotic manipulation planning (GemBench / RLBench)",
            "target_task_description": "High-level planning for manipulation tasks; in prior work the LLM is used to propose plans which are re-ranked or handed to a 3D motion policy. In Gondola experiments the 3D-LOTUS++ motion planning policy is reused to allow fair comparisons on task execution.",
            "semantic_alignment": "3D-LOTUS++ performs planning largely via LLM capabilities with engineering and in-context examples; it lacks direct visual grounding in Gondola authors' description, and thus requires additional mechanisms (e.g., value predictors, affordance models) in prior literature to link text plans to perception.",
            "performance_with_language_pretraining": "Gondola reports absolute improvements over 3D-LOTUS++: +10.3% SR on L2 (novel rigid objects), +10.9% SR on L3 (novel articulated objects), and +1.6% SR on L4 (long-horizon) (text claim in paper). Exact base 3D-LOTUS++ SR numbers are given in the paper's tables (refer to the original for per-level values).",
            "performance_without_language_pretraining": "Not applicable in the same sense; 3D-LOTUS++ is itself an LLM-based method — comparisons to purely vision-only baselines are provided elsewhere in the paper (non-LLM policies do better on L1 but worse on L2-L4).",
            "sample_efficiency_comparison": "The Gondola paper does not provide sample-efficiency numbers comparing 3D-LOTUS++ to Gondola (e.g., required fine-tuning examples).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not in Gondola paper (would be in original 3D-LOTUS++ paper if present).",
            "embedding_space_analysis": "Not in Gondola paper.",
            "action_grounding_evidence": "Gondola argues that 3D-LOTUS++ relies more on textual planning and engineered prompts without direct visual mask grounding and that this leads to less precise grounding compared to Gondola's mask-based plans; Gondola's superior performance on novel-object tasks is presented as evidence that direct visual grounding (masks) helps.",
            "hierarchical_features_evidence": "Not reported in Gondola paper.",
            "transfer_conditions": "3D-LOTUS++ reportedly generalizes via LLM reasoning and in-context learning, but lacks visual grounding which the Gondola authors identify as a limitation for precise manipulation in 3D scenes.",
            "novel_vs_familiar_objects": "Gondola reports that it outperforms 3D-LOTUS++ notably on novel-object levels (L2/L3) as above; this implies 3D-LOTUS++ generalization is weaker for novel visual instances when not tightly grounded.",
            "zero_shot_or_few_shot": "3D-LOTUS++ relies on in-context learning and LLM capabilities (few-shot/in-context) in its original formulation; Gondola contrasts this with its grounded fine-tuning approach. Gondola does not provide exact in-paper counts of shots used by 3D-LOTUS++.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Gondola presents qualitative critique: LLM-only planning without grounded perception can be unstable and may produce ungrounded plans; no numeric negative-transfer analysis of 3D-LOTUS++ is provided in this paper.",
            "comparison_to_vision_only": "The paper contrasts 3D-LOTUS++ (LLM-based planning with limited visual grounding) and pure vision-only end-to-end policies: vision-only methods do well on seen tasks but generalize worse to novel objects/environments than LLM-based approaches with grounding; Gondola (VLM-based grounded LLM) improves on prior LLM methods by providing explicit visual masks.",
            "temporal_dynamics": "Not reported in Gondola paper.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1916.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos.",
            "rating": 2
        },
        {
            "paper_title": "Towards generalizable vision-language robotic manipulation: A benchmark and LLM-guided 3D policy.",
            "rating": 2
        },
        {
            "paper_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.",
            "rating": 2
        },
        {
            "paper_title": "Segment anything.",
            "rating": 2
        },
        {
            "paper_title": "3D-LOTUS [14]",
            "rating": 1
        },
        {
            "paper_title": "PerAct: (PerAct is cited in related work as a 3D voxel/point-cloud based policy)",
            "rating": 1
        }
    ],
    "cost": 0.023142,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Grounded Vision Language Planning for Generalizable Robotic Manipulation
12 Jun 2025</p>
<p>Shizhe Chen 
Inria
École normale supérieure
CNRS
PSL Research University</p>
<p>Ricardo Garcia 
Inria
École normale supérieure
CNRS
PSL Research University</p>
<p>Paul Pacaud 
Inria
École normale supérieure
CNRS
PSL Research University</p>
<p>Cordelia Schmid 
Inria
École normale supérieure
CNRS
PSL Research University</p>
<p>Grounded Vision Language Planning for Generalizable Robotic Manipulation
12 Jun 202523810B8B9B0DD82F3887E3A25256AA75arXiv:2506.11261v1[cs.RO]Robotic ManipulationTask PlanningVision-Language Model
Robotic manipulation faces a significant challenge in generalizing across unseen objects, environments and tasks specified by diverse language instructions.To improve generalization capabilities, recent research has incorporated large language models (LLMs) for planning and action execution.While promising, these methods often fall short in generating grounded plans in visual environments.Although efforts have been made to perform visual instructional tuning on LLMs for robotic manipulation, existing methods are typically constrained by single-view image input and struggle with precise object grounding.In this work, we introduce Gondola, a novel grounded vision-language planning model based on LLMs for generalizable robotic manipulation.Gondola takes multi-view images and history plans to produce the next action plan with interleaved texts and segmentation masks of target objects and locations.To support the training of Gondola, we construct three types of datasets using the RLBench simulator, namely robot grounded planning, multi-view referring expression and pseudo long-horizon task datasets.Gondola outperforms the state-of-the-art LLM-based method across all four generalization levels of the GemBench dataset, including novel placements, rigid objects, articulated objects and long-horizon tasks.</p>
<p>Introduction</p>
<p>Training robots to perform physical manipulation tasks following human instructions has been a long-term goal in robotics, enabling intuitive human-robot interaction in unstructured, dynamic environments such as homes and factories.Recently, end-to-end learning-based models [1,2,3], particularly Vision-Language-Action models (VLAs) [4,5,6,7,8] trained on real-world robot data, have achieved remarkable success in robotic manipulation.However, due to the scarcity and limited diversity of available robot datasets [9,10,11], these models still struggle to generalize beyond their training conditions, facing difficulties with novel objects, unfamiliar environments, and especially unseen long-horizon tasks [12,13,14].</p>
<p>To improve generalization, modular frameworks [15,16,14,17] have received increasing attention, separating high-level task planning from low-level action execution.As the planning component is less coupled to robot embodiments, it can leverage a broader range of internet-scale data for training to enhance its generalization capabilities.Inspired by the impressive zero-and few-shot reasoning and planning abilities of Large Language Models (LLMs) [18,19], researchers have begun exploring LLMs for task planning, such as decomposing a language instruction into substeps [14,20] or generating executable code [15,16].However, since LLMs lack direct grounding in the physical world, their ability to produce actionable and reliable plans remains limited.</p>
<p>Different methods have been proposed to ground LLM-generated plans in visual context.SayCan [21] trains an affordance score predictor based on visual input and candidate skills, allowing the system to rerank substeps proposed by the LLM.However, this is limited to evaluating a predefined set (a) Existing methods [24,23,17] using single-view input and producing various intermediate representations.</p>
<p>…</p>
<p>Gondola</p>
<p>Grounded Plans with Segmentation Masks</p>
<p>Screw in the rose light bulb.</p>
<p>(b) Our Gondola model with multi-view inputs generating grounded plans with segmentation masks.</p>
<p>Figure 1: Comparison of vision-language models for high-level planning in robotic manipulation.Multi-view inputs alleviate occlusions for improved 3D scene perception, while segmentation masks offer more precise and compact grounded plans.</p>
<p>of skills.ECoT [22] uses image captioning models to convert visual scenes into text descriptions, which are then fed to the LLM.Yet, captions can be less accurate and miss crucial details, leading to sub-optimal decision-making and raising the risk of error propagation.</p>
<p>More recently, a few works [23,24,17] have explored fine-tuning Vision-Language Models (VLMs) to generate visually grounded plans, using intermediate representations such as points [24], bounding boxes [23], or latent hidden states [17] as illustrated in Figure 1a.While promising, points and bounding boxes are often too coarse for precise robotic manipulation in 3D environments.Latent representations from VLMs, on the other hand, are difficult to interpret and may lack the conciseness needed for efficient execution.Furthermore, most existing methods rely on single-view images, which exacerbates planning challenges due to occlusions and limited fields of view.</p>
<p>To address these limitations, we propose Gondola -a grounded vision-language planning model to enhance generalization in robotic manipulation.As illustrated in Figure 1b, Gondola transforms language instructions and multi-view images into precisely grounded plans, consisting of interleaved actions and objects with accompanying segmentation masks for each referred object.The model builds upon a dense grounding VLM Sa2VA [25], leveraging a specialized segmentation token that enables object-specific mask generation across views.To improve planning consistency, we incorporate the textual history of previously generated plans as additional context to the model.For effectively training Gondola, we construct a robot grounded planning dataset using simulated environments from RLBench [26], supplemented with multi-view referring expression data to strengthen object grounding.To better handle long-horizon tasks, we create extended tasks by concatenating two short task sequences and using an LLM to generate instructions.We evaluate Gondola on both offline grounded planning and online task execution using the GemBench generalizable robotic manipulation benchmark [14].Comprehensive results demonstrate Gondola's superior performance, benefiting from multi-view inputs, history-aware planning, segmentation masks and diverse training data.It outperforms the state-of-the-art LLM-based method 3D-LOTUS++ [14] by absolute 10% on average.</p>
<p>To summarize, our contributions are three-fold:</p>
<p>• We propose Gondola to generate grounded vision-language plans with masks for generalizable robotic manipulation.It features multi-view image understanding and grounding.• We construct multi-view grounding and planning datasets using RLBench, and propose pseudo long-horizon task generation to improve long-term planning capabilities.• Our model sets a new state of the art on the generalization benchmark GemBench, and works reliably on a real robot.The code, models and datasets will be publicly released.</p>
<p>Related Work</p>
<p>Vision-and-language robotic manipulation.Learning robotic manipulation conditioned on vision and language has garnered significant interest [27,28,29].Due to the high dimensionality of the manipulation action space, directly applying reinforcement learning (RL) for training presents challenges [30].Therefore, most approaches employ imitation learning (IL) [31,4,32,33,34,35,36,37,38] using scripted trajectories [26] or tele-operation data [9].Visual representation plays a crucial role in policy learning.Existing works [31,4,2,32,34,39,40] rely on 2D images for action prediction, although recent work has begun to explore 3D visual representations [41,33,35,37,38,36,14,42]. Hiveformer [32] and RVT [34] utilize 2D images to predict a heatmap in 2D space, which is then combined with 3D point clouds to demermine the final 3D position.C2F-ARM [41] and PerAct [33] directly use 3D voxel representation as input, being less efficient due to encoding empty voxels.PolarNet [35] and 3D-LOTUS [14] improve efficiency by encoding only visible point clouds to predict actions, while SUGAR [36] further enhances point cloud representation through 3D pretraining.Given the superiority of current pre-trained 2D representations [43], works like Act3D [37] and 3D Diffuser Actor [38] lift pre-trained 2D features into 3D space, and then train 3D models to leverage the strengths of both.In this work, we leverage the strong generalization capabilities of pretrained 2D vision-language models (VLMs) for high-level task planning and integrate it with 3D-based motion planning policies for task execution.</p>
<p>Foundation models for robotics.Learning-based robotic policies struggle to generalize to novel scenarios [44].Inspired by generalization abilities of foundation models [43,45,19], recent research investigates ways to leverage these models for perception, reasoning and planning in robotics.Some methods [20,14] directly use LLMs to decompose high-level tasks into sub-steps.To better ground plans in visual world, SayCan [21] combines LLMs with value functions of pretrained skills given visual contexts.ViLa [46] replaces LLMs with a multimodal LLM GPT-4V [47].CaP [15] directs LLMs to generate code that invokes tools for visual perception and control, and VoxPoser [16] uses LLMs and VLMs to create 3D voxel maps indicating affordances, constraints, rotations, and velocities.These approaches rely on general-purpose pretrained models for task planning, but tend to be unstable in robotic settings and require heavy prompt engineering.To address this, a few recent methods fine-tune VLMs on robot datasets to generate grounded plans using intermediate representations such as points [24], bounding boxes [23], and latent vision-language embeddings [17].</p>
<p>In our work, we extend the VLM framework with multi-view inputs and finer grounding masks, and introduce synthetic robot data for long-horizon grounded planning.</p>
<p>Vision and language models for grounding.Early VLMs [48,49] are constrained to generating text outputs from multimodal inputs, such as image captioning and visual question answering.To enable VLMs to produce grounded outputs that align generated texts with specific image regions, existing methods can be broadly categorized into three types.The first category outputs box coordinates [50,51,52,53,54] or polygons of segmentation masks [55] as text.However, generating precise numerical outputs -especially when multiple grounding results are required -is difficult and prone to hallucination.The second category uses a proposal-based approach, where a separate module first generates candidate regions, and the VLM selects the one for each generated text [56,57].While more structured, this method is sensitive to proposal quality, suffers from error accumulation, and introduces more computation overhead.The third category decouples language and grounding by feeding the output of a VLM into a grounding model to produce boxes or masks [58,59,60,25].Among them, Sa2VA [25] achieves the state-of-the-art performance by integrating a strong VLM model InternVL [61] and a segmentation model SAM2 [62], as well as training on large-scale image and video grounding data.Our Gondola model fine-tunes Sa2VA on multi-view image grounding and planning datasets for robotic manipulation.</p>
<p>3 Gondola: Grounded Vision-Language Plan Generation</p>
<p>We formulate high-level task planning for robotic manipulation as a vision-language grounding problem, where the goal is to generate the next executable, visually-grounded action plan that accomplishes a natural language instruction in the observed environment.Formally, given a language instruction L and multi-view visual observations I = {I 1 , • • • , I K } from K cameras, the Gondola model produces a grounded vision-language plan P = (a, o, M o , l, M l ), where a represents the action name, o specifies the manipulated object description paired with corresponding segmentation masks
M o = {m 1 o , • • • , m K o }
across all views, and l denotes the target location description with associated location masks M l .Noting that either o or l may be empty if the particular action does not require an object or target location for execution.</p>
<p>Model Architecture</p>
<p>As illustrated in Figure 2, the Gondola architecture consists of three main components: an image encoder for tokenizing each view image, an LLM to process multimodal inputs and outputs, and a segmentation model for multi-view object grounding.Image encoder.We use a pretrained vision transformer (ViT) InternVL-300M [61] with an input image resolution of 448 × 448 to generate image patch embeddings, followed by a 2-layer multi-layer perceptron (MLP).The ViT is frozen, while the MLP is trained to adapt the visual features to the language space.The same image encoder is applied across all views, with view separation handled by a special token \n.Image tokens from all views are concatenated to form a single sequence.LLM.We adopt InternVL-4B [61] as our language model, keeping its base parameters frozen while adding LoRA [63] layers for fine-tuning.Building upon Sa2VA [25], we incorporate a specialized vocabulary that includes a dedicated <seg> token to signal mask generation, along with paired delimiter tokens <p> and </p> that precisely delineate object and location references requiring spatial grounding.To maintain contextual awareness across sequential steps in completing manipulation tasks, we further encode previously generated history plans H as compact text tokens to the model.The following example demonstrates input and output token formatting for the LLM:</p>
<p>User: <image>\n<image>\n<image>\n<image>\n You are a skilled assistant for robot task planning in tabletop environments.You can perform the following actions: grasp, move grasped object, rotate grasped object, push down, push forward, and release.Task: screw the light bulb from the rose holder into the lamp.You have completed the following action plans: grasp the rose light bulb.Please generate the next action plan.Gondola: Move the grasped object to <p> lamp </p><seg>.</p>
<p>Here, <image> represents placeholders for image tokens for each view, which are replaced by the actual visual embeddings.Segmentation model.We employ SAM2 [62] as the segmentation model.Given the hidden embedding h seg from the LLM that predicts the <seg> token, we project h seg with a 2-layer MLP to generate a prompt embedding.SAM2 uses this prompt to segment the corresponding object mask for each view image separately and thus generates K binary masks for each referred object or location.</p>
<p>Training Data</p>
<p>We construct three datasets to train Gondola using 31 task variations in GemBench training split [14] within the RLBench simulator [26], including robot grounded planning, multi-view referring expression, and pseudo long-horizon tasks.While this data construction approach can be extended to any task in RLBench, we restrict dataset construction to the GemBench training split for fair comparison with prior work [14] in evaluating generalization performance.Figure 3 illustrates examples from each of the three datasets.</p>
<p>Robot grounded planning</p>
<p>Multi-view referring expression</p>
<p>Please segment one of the violet jar  Robot grounded planning.In the RLBench simulator, each task is structured with semantically labeled objects and fixed procedure trajectories, enabling efficient grounded plan generation.First, we manually decompose the trajectory in each task into a sequence of plans, each step consisting of an action, object and placement location triplet.This only requires a single annotation effort per task with minimal annotation overhead.The corresponding segmentation masks for objects and locations are then automatically extracted given the annotated semantic labels.In this way, we create ground-truth plan {a t , o t , M t o , l t , M t l } for multi-view images I t at each keystep t1 in an episode per task variation.We use 100 episodes for each GemBench training task variation, where each episode contains randomized object placements (and optionally new distractor objects), resulting in approximately 15k training tuples for robot grounded planning.Multi-view referring expression.To strengthen Gondola's multi-view object grounding capabilities, we further create a multi-view referring expression dataset based on RLBench.Recognizing that the default semantic labels in RLBench contain noises and ambiguities, we implement an automatic preprocessing pipeline to standardize and refine object names in RLBench.More detail is presented in Appendix A. Similar to grounded planning generation, we automatically extract all object instances and their corresponding segmentation masks for each keystep in GemBench training split, yielding 15k multi-view image examples and 58k referring expressions.For each training example, we formulate the referring query as "Please segment one of the [object name]" with the expected output being the corresponding segmentation masks across all input view images.Pseudo long-horizon tasks.To enhance planning for long-horizon tasks, we propose to automatically generate pseudo long-horizon sequences.Specifically, we randomly concatenate pairs of different training task sequences from GemBench training split to create compositional tasks.We then use an LLM to create coherent joint instructions for the combined tasks.Despite the abrupt scene transitions between tasks, these pseudo long-horizon tasks still help the model learn to leverage history plans to track task progress, and predict the next step based on long-horizon context.</p>
<p>Training Objectives</p>
<p>Gondola is trained to jointly optimize plan generation and multi-view object grounding.For plan generation, we employ the cross-entropy loss for next token prediction:
L plan = − log p(y i |y &lt;i , I, L, H),(1)
where y i represents tokens in the generated plan including the special tokens.For multi-view object grounding, we adopt a joint loss of binary mask prediction and dice loss L grd = L bce + L dice :
L bce = − [M gt (p) • log(M pred (p)) + (1 − M gt (p)) • log(1 − M pred (p))],(2)L dice = 1 − 2 p M pred (p) • M gt (p) p M pred (p) + p M gt (p) + ϵ ,(3)
where p indexes over all pixels in the mask, M pred (p) is the predicted probability, M gt (p) is the ground-truth binary label, and ϵ is a small constant for numerical stability.</p>
<p>Experiments</p>
<p>Evaluation Datasets and Metrics</p>
<p>We evaluate Gondola on the GemBench benchmark [14] for robotic manipulation in RLBench [26] simulator.GemBench assesses models' generalization capabilities across four levels: Level 1 (L1) with new locations, Level 2 (L2) with novel rigid objects, Level 3 (L3) with new articulated objects, and Level 4 (L4) with unseen long-horizon tasks.To ensure a fair evaluation on generalization, tasks from L2 to L4 are excluded during training.The benchmark includes 31 task variations in L1, 28 in L2, 21 in L3 and 12 in L4.We conduct the following two types of evaluation:</p>
<p>• Grounded planning evaluation.This setup purely assesses models' grounded planning performance given instruction, multi-view images and ground-truth history plans.We construct a grounded planning evaluation set given the GemBench validation split.It contains 20 episodes per task variation in GemBench.For each keystep in an episode, we provide ground-truth annotations for the next action, object names and segmentation masks across all views.To evaluate the grounded planning performance, we measure the accuracy of action and object name predictions through exact text matches for each keystep.For grounding evaluation, we calculate the intersection over union (IoU) between predicted and ground-truth masks for each view.The averaged performances of each metric on all keysteps are reported for each generalization level of GemBench.</p>
<p>• Task completion evaluation.This setup integrates Gondola with low-level motion planning policies to execute the generated plans.We adopt the standard camera configuration in GemBench, using K = 4 cameras positioned at the front, left shoulder, right shoulder and wrist, each with an image resolution of 256 × 256.For evaluation, we use the GemBench test split across all four levels, conducting 20 episodes per task variation for 5 times, resulting in 20 × 5 × (31 + 28 + 21 + 12) evaluation episodes in total.Each episode is limited to a maximum of 25 steps.Task performance is measured by success rate (SR), where SR is 1 for a successful episode and 0 for failure.We report mean SR and standard deviations across the 5 runs.</p>
<p>Implementation Details</p>
<p>Training Gondola.We train the Gondola model using 8 NVIDIA H100 GPUs with training scripts built on the DeepSpeed engine [64].The image encoder and SAM2 encoder are kept frozen during training.We apply LoRA [63] with rank 128 for parameter-efficient fine-tuning of the LLM, and the SAM2 mask decoder is also fine-tuned.The model is optimized with AdamW, using a learning rate of 2 × 10 −5 for all trainable parameters.The batch size per device is set to 4, resulting in an effective batch size of 32.It takes 3 hours for training 10k iterations over the three constructed datasets.</p>
<p>Integrating Gondola with low-level policies.For fair comparison with prior work, we employ the same motion planning policy released by 3D-LOTUS++ [14].Unlike 3D-LOTUS++ [14] which performs task planning only once and then executes the plan, our approach runs the task and motion planning models iteratively in a feedback loop, enabling continuous re-planning and corrections as needed.Specifically, at each step, Gondola takes multi-view RGB images, instruction and previously executed history plans as input to produce the next plan, including the next action, the manipulated object, and/or the target location together with grounded masks on each view.Then we combine aligned depth images with these masks and unify the segmented objects across views into a consolidated 3D point cloud.Following [14], each point is assigned with one of four categories based on the segmentation results and robot proprioceptive information, namely target object, target location, robot, and obstacle.The predicted action name and the point cloud are fed into the 3D motion planning policy in [14] to generate a sequence of actions.We can either run the entire action sequence as in action chunking [3] or execute one action at a time before re-planning with Gondola.We compare the two strategies in Table 3.  1a.We use the same image encoder and LLM as Gondola (row 4) for fair comparison.To measure the mask IoU, the predicted boxes are fed into the SAM2 model to produce segmentation masks.We observe that the box-based model frequently suffers from format errors, as generating multiple numeric values for multiple images can be challenging.It performs worse across all levels in both action and object name accuracy, and shows significantly lower grounding quality in terms of mask IoU.These results highlight the advantages of end-to-end mask generation within VLMs, which provides more accurate and reliable grounding for robotic planning.</p>
<p>Multi-view inputs.The comparison between the 2nd and 3rd rows in Table 1 showcases the impact of multi-view image inputs for robot task planning.In the 2nd row, only the front-view image is provided to the model, whereas in the 3rd row, all four views are used.Multi-view images help mitigate occlusions and generally improve action, object and grounding prediction across levels, with only slight worse performance on a few metrics in L3 compared to the single-view setting.History plans.The last two rows in Table 1 compares the effect of incorporating history plans into task planning.Including history information boosts the performance on L2 and L3 by enabling more coherent and context-aware planning decisions.However, on L4, we observe a significant performance drop compared to the model without history.An in-depth analysis reveals that this decline is due to a distribution shift in history plans.As a result, the model tends to leverage its prior knowledge for generating purely textual plans rather than grounded plans.In contrast, the model without history does not suffer from this distribution shift.This issue can be addressed by training on our constructed pseudo long-horizon data, as shown in Table 2.</p>
<p>Fine-tuning datasets.Table 2 evaluates the contribution of each fine-tuning dataset.The multi-view referring expression dataset proves most effective in improving segmentation quality, leading to consistently better grounding performance across all four levels.The pseudo long-horizon task dataset is particularly beneficial for L4, as it mitigates the history plan shift issue and encourages the model to reason over extended plan histories when predicting subsequent actions.Action chunking.As shown in Table 3, when the action chunk size for running the motion planning policy is set to 1, Gondola replans at every step; when set to 5, it replans only after the motion planning policy completes the previous subplan.We observe that the impact of action chunk size varies depending on tasks.Detailed results and analysis are provided in Appendix B. In general, for tasks requiring fine-grained manipulation, frequent replanning (i.e., smaller action chunks) yields better performance.In contrast, for long-horizon tasks that benefit from consistent, highlevel planning, using a larger action chunk is more effective since the current Gondola model does not encode fine-grained history within subplans, which can limit coherent decision-making at this level.3D postprocessing.We ablate a postprocessing step that applies 3D point cloud filtering using the DBSCAN algorithm to remove outlier points in grounded masks.Results show that this step offers minimal improvement, indicating that Gondola already produces spatially coherent object masks.</p>
<p>Comparison with state of the art</p>
<p>Table 4 presents a comparison of our Gondola model with state-of-the-art methods on the GemBench test split.Gondola is combined with the same motion policy in 3D-LOTUS++ [14] with action chunking size of 5 and no 3D postprocessing.The methods in the upper section do not use LLMs for planning but rely on end-to-end policy training to predict actions directly.While these methods perform well on seen tasks in L1, they show limited generalization to unseen objects in L2 and L3 and struggle with unseen long-horizon tasks in L4.Models employing LLM-based planning demonstrate improved generalization from L2 to L4, despite reduced performance on familiar tasks in L1.In particular, compared to 3D-LOTUS++ [14], which uses extensive engineering and in-context learning to enable LLM-based planning without visual input, our Gondola model offers a straightforward approach to directly generate grounded plans for follow-up motion planning.Gondola outperforms 3D-LOTUS++ [14] by 10.3% on novel rigid objects in L2, 10.9% on novel articulated objects in L3 and 1.6% in L4 of long-horizon tasks.Detailed results per task and qualitative examples are included in Appendix B. We further deploy Gondola in a real robot with results detailed in Appendix C.</p>
<p>Conclusion</p>
<p>This paper presents Gondola, a grounded vision-language planning model to improve generalization in robotic manipulation.Gondola features with multi-view perception and the incorporation of planning history to generate fine-grained segmentation masks in the action plan.We construct three simulated datasets based on RLBench for model training, including robot grounded planning, multi-view referring expressions and pseudo long-horizon tasks datasets.Gondola demonstrates superior performance in both standalone planning and full execution on the GemBench benchmark, achieving stronger generalization abilities on novel rigid and articulated objects and long-horizon tasks.Our experiments highlight the importance of multi-view grounding, temporal reasoning, and end-to-end mask generation for effective robotic planning.</p>
<p>Limitations</p>
<p>First, data scarcity remains a major bottleneck for Gondola.Our current dataset is limited to shorthorizon tasks in controlled tabletop environments from the GemBench training split, which restricts generalization to unseen objects and more complex long-horizon tasks.Expanding the dataset with more diverse simulated and real-world data is essential.Second, visual history encoding can be improved.Multi-view inputs introduce many image tokens, making it difficult to include detailed historical context.A more efficient memory mechanism could support richer history representation without overwhelming the model.Lastly, our approach relies solely on imitation learning from successful episodes, making it challenging for the model to anticipate and correct errors.Introducing examples of failure and recovery could better equip Gondola for robust, real-world applications.</p>
<p>A Data Construction in RLBench</p>
<p>We detail the label construction from RLBench in Section 3.2.RLBench contains scripted trajectories for each task, and every object in the scene already has a name and an associated label id.We use regular expressions to filter out undesired objects and fix object names by removing undesired name parts, prefix numbers or words such as distractor or success.For the set of objects whose color changes from one task variation to the other, we prepend the color name whose RGB value is closest to the RGB value of the object color.We consider 20 different colors that appear in RLBench: red, maroon, lime, green, blue, navy, yellow, cyan, magenta, silver, gray, orange, olive, purple, teal, azure, violet, rose, black and white.We use 3100 episodes from the 31 GemBench training task variations.</p>
<p>For each keystep, we collect RGB images per camera viewpoint, the extracted list of object names in the scene, and object masks across views.</p>
<p>B Detailed Results on GemBench</p>
<p>Table 5 to 8 present the per-task results on four levels of GemBench benchmark, respectively.We observe that the impact of action chunk size varies depending on tasks.Frequent replanning (i.e., smaller action chunks) yields better performance for tasks requiring fine-grained and reactive actions such as 'SlideBlock' and 'PutInCupboard'.For example, in the 'PutInCupboard' task as shown in Figure 4, Step 2 involves moving the grasped object to the cupboard.Although the predictions of Gondola are correct, the partial observation of the cupboard makes it difficult to precisely localize its position for the motion planner, causing the object to fall outside the intended area.This issue could be mitigated by using smaller action chunks, which would allow the model to gradually get closer and acquire better views of the cupboard.While Gondola has replanning capabilities as shown in Step 5, the motion planner fails again due to the new object pose.Figure 5 shows a failure example for the 'SlideBlock' task.It is challenging for the motion planner to predict long-horizon action trajectories for this contact-rich task, though the initial plan generated from Gondola is correct.</p>
<p>In contrast, for long-horizon tasks in Level 4 that benefit from consistent, high-level planning, using a larger action chunk is more effective since the current Gondola model does not encode fine-grained history within subplans, which can limit coherent decision-making at this level.</p>
<p>Predicted plan grasp<p>crackers box</p> [SEG] Step 0      As illustrated in Figure 6, our real robot setup includes three RealSense d435 cameras attached to a table and a 6-DoF UR5 robotic arm equipped with an RG6 gripper.We collect 20 × 7 demonstrations via teleoperation for 7 variations across 5 tasks: stack cup (yellow in pink or navy in yellow), put fruit (strawberry or peach) in box, open drawer, put item in drawer and hang mug.Then, we fine-tune Gondola and the 3D-LOTUS [14] motion planning policy on a joint dataset of RLBench and the real robot demonstrations.We evaluate the fine-tuned models on the same 7 seen task variations with different objects placements and evaluate generalization capabilities on 7 new unseen task variations: put fruit (lemon and banana) in box, put food (tuna can then corn) in box and put food in plates (croissant in the yellow plate and grapes in the pink plate).For each task variation, we run models 10 times and report the success rate.</p>
<p>C.2 Real Robot Results</p>
<p>Table 9 and 10 show the performance on seen and unseen task variations, respectively.The final performance depends on both Gondola and the motion planning policy.Figure 7 illustrates a successful prediction by Gondola on a previously unseen task.However, we observe that Gondola performs worse in the real-world setting compared to simulation, primarily due to the limited amount of real robot data.As illustrated in Figure 8, the multiview consistency is significantly lower in the real world, leading to frequent segmentation errors.Increasing the availability of real-world multi-view images could help address this limitation, and we leave this direction for future work.The video in the supplementary material showcases more executions on the real robot.</p>
<p>Figure 2 :
2
Figure 2: Left: Gondola model architecture, consisting of a shared visual encoder for multi-view images, an LLM to generate action and object names along with segmentation tokens, and SAM2 to decode masks.Right: Integrating Gondola with a motion planning policy for task execution.</p>
<p>the maroon button first, then screw in the navy light bulb.You have completed the following actions: push down maroon button, grasp the navy light bulb.Ground-truth: move grasped object to <p>lamp</p><seg>Pseudo long-horizon taskTask 1 Task 2 Tasks: close the violet jar.Your are in the first step.Ground-truth: grasp <p> gray lid </p> <seg> View 1 View K Input Mask Ground-truth: <p> violet jar </p> <seg></p>
<p>Figure 3 :
3
Figure 3: Three types of datasets are constructed for model training: (1) robot grounded planning, (2) multi-view referring expressions for improved object grounding, and (3) pseudo long-horizon tasks for enhanced long-horizon planning.</p>
<p>Multi-view observationPredicted segmentation masks Point cloud and predicted actions black: obstacle green: robot blue: target object red: target location maroon dots: actions Predicted plan move grasped object to <p>cupboard</p>[SEG]</p>
<p>Figure 4 :
4
Figure 4: A failure example of the 'PutInCupboard' task using Gondola (AC=5).The instruction is 'put the crackers box in the cupboard'.The predictions of Gondola are correct, but the motion planner fails in Step 2 due to limited visual information of the cupboard from partial observations.Gondola replans in Step 5, but the motion planner fails due to the new object pose.</p>
<p>Figure 5 :
5
Figure5: A failure example of the 'SlideBlock' task using Gondola (AC=5).The instruction is 'slide the block towards the blue plane'.The prediction of Gondola at Step 0 is correct, but it is challenging for the motion planner to predict long-term actions for this contact-rich task.At Step 5, due to the wrong history plan fed into Gondola, Gondola fails to replan correctly.</p>
<p>Figure 6 :
6
Figure 6: Our setup includes three Re-alSense D435 cameras and a UR5 robotics arm equipped with a RG6 gripper.</p>
<ol>
<li>Grasp rose bulb [mask 1 ] … [mask K ] 2. Move grasped object to lamp [mask 1 ] … [mask K ] 3. Rotate grasped object
View 1View K</li>
</ol>
<p>Table 1 :
1
Performance on grounded planning evaluation.We measure the action (Act) and object (Obj) name prediction accuracy and grounding performance (Grd) on the four levels of GemBench validation split.All the models are fine-tuned on the robot grounded planning dataset.
GrdMulti-Hist-L1L2L3L4typevieworyAct Obj Grd Act Obj Grd Act Obj Grd Act Obj GrdBox✓✓95.1 93.7 62.8 97.4 89.0 58.7 69.3 53.8 46.2 70.0 36.8 16.6Mask××98.0 98.2 87.8 95.1 89.5 79.8 79.3 76.3 62.7 77.2 40.0 37.4Mask✓×100 100 88.6 98.0 91.3 81.2 85.6 75.6 61.4 89.9 50.1 46.5Mask✓✓100 100 87.9 99.0 93.3 79.2 88.6 83.9 66.4 79.4 44.9 40.0</p>
<p>Table 2 :
2
Performance on grounded planning evaluation.All the models use multi-view and history plans for mask generation, but are fine-tuned on different composition of datasets: robot grounded planning (Plan), multi-view referring expression (RefExp), and pseudo long-horizon tasks (Long).
Finetuning DataL1L2L3L4Plan RefExp Long Act Obj Grd Act Obj Grd Act Obj Grd Act Obj Grd✓××100 100 87.9 99.0 93.3 79.2 88.6 83.9 66.4 79.4 44.9 40.0✓✓×100 100 89.1 99.0 95.1 84.2 92.1 88.2 73.3 72.1 42.3 41.7✓✓✓100 100 89.5 99.7 95.3 85.2 88.5 82.2 73.8 93.9 51.2 53.84.3 Ablation Studies
Boxes vs. Masks.We compare Gondola's mask-based grounding approach with a box-based variant.The box variant (row 1) in Table1directly generates bounding boxes as textual outputs as illustrated in the middle of Figure</p>
<p>Table 4 :
4
Performance on four levels of GemBench testing split.
MethodL1L2L3L4Hiveformer [32]60.3±1.526.1±1.435.1±1.70.0±0.0PolarNet [35]77.7±0.937.1±1.438.5±1.70.1±0.2w/o LLM3D diffuser actor [38]91.9±0.843.4±2.837.0±2.20.0±0.0RVT-2 [39]89.1±0.851.0±2.336.0±2.20.0±0.03D-LOTUS [14]94.3±1.449.9±2.238.1±1.10.3±0.3w/ LLM3D-LOTUS++ [14] Gondola (Ours)68.7±0.6 87.3±1.964.5±0.9 74.8±1.841.5±1.8 52.4±2.117.4±0.4 19.0±1.0</p>
<p>Table 3 :
3
Success rate of task execution on four levels of GemBench testing split.The Gondola model is integrated with a 3D-based motion planning policy.
Act chunk3D filterL1L2L3L45× 87.3±1.9 74.8±1.8 52.4±2.1 19.0±1.0 ✓ 86.5±1.2 74.4±1.1 51.1±1.5 19.7±1.71× 90.8±1.2 78.2±1.4 49.5±0.5 14.9±2.2 ✓ 90.5±0.3 78.1±1.8 49.3±0.9 15.9±2.1</p>
<p>Table 5 :
5
Detailed performance on each task variation on GemBench Level 1. 'AC' denote action chunk size.
MethodAvg.Close Fridge+0Close Jar+15Close Jar+16CloseLap-topLid+0CloseMicro-wave+0LightBulb In+17LightBulb In+19Open Box+0Open Door+03D-LOTUS++ [14]68.79510099288755455579Gondola (AC=5)87.39699100968285816379Gondola (AC=1)90.897100100988380735573MethodOpen Drawer+0Open Drawer+2Pick&amp; Lift+0Pick&amp; Lift+2Pick&amp; Lift+7PickUp Cup+11PickUp Cup+8PickUp Cup+9Push Button+0Push Button+33D-LOTUS++ [14]6875979493918688100100Gondola (AC=5)8297979798889590100100Gondola (AC=1)84961001009796979497100MethodPush Button+4PutInCup-board+0PutInCup-board+3PutMoney InSafe+0PutMoney InSafe+1Reach&amp; Drag+14Reach&amp; Drag+18Slide Block+0Slide Block+1Stack Blocks+303D-LOTUS++ [14]10012221694621006586Gondola (AC=5)1005855897397981007476Gondola (AC=1)100817296941001001009589MethodStack Blocks+36Stack Blocks+393D-LOTUS++ [14]2028Gondola (AC=5)8181Gondola (AC=1)8484</p>
<p>Table 6 :
6
Detailed performance on each task variation on GemBench Level 2. 'AC' denote action chunk size.
MethodAvg.Close Jar+3Close Jar+4Lamp On+0LightBulb In+1LightBulb In+2Pick&amp; Lift+14Pick&amp; Lift+16Pick&amp; Lift+18Pick&amp;Lift Cylinder+03D-LOTUS++ [14]64.598962564394969591Gondola (AC=5)74.8991001818396989978Gondola (AC=1)78.299990837398969889MethodPick&amp;Lift Moon+0Pick&amp;Lift Star+0Pick&amp;Lift Toy+0PickUp Cup+10PickUp Cup+12PickUp Cup+13Push Button+13Push Button+15Push Button+17PutCube InSafe+03D-LOTUS++ [14]299471798984991009937Gondola (AC=5)919577849597999910042Gondola (AC=1)909583869698991009957MethodPutInCup-board+7PutInCup-board+8Reach&amp; Drag+5Reach&amp; Drag+7Slide Block+2Slide Block+3Stack Blocks+24Stack Blocks+27Stack Blocks+333D-LOTUS++ [14]109464275228359Gondola (AC=5)16197962610818072Gondola (AC=1)1521001002350918586
C.1 Experimental Setup</p>
<p>Table 9 :
9
Performance of 7 seen task variations with real robot.
TaskGondolaStack yellow cup in pink cup8/10Stack navy cup in yellow cup7/10Put strawberry in box4/10Put peach in box4/10Open drawer6/10Put item in drawer1/10Hang mug5/10Avg.5/10</p>
<p>Table 10 :
10
Performance of 7 unseen task variations with real robot.
TaskGondolaStack red cup in black cup7/10Stack black cup in orange cup2/10Place the yellow cup inside the red cup,then the cyan cup on top2/10Put lemon in box5/10Put banana in box6/10Put tuna can in box, then corn in box3/10Put croissant in yellow plate,then grapes in pink plate1/10Avg.3.7/10
The keystep is defined as step with significant motion change as in prior work[32,35,39,14,37], which helps avoid over-sampling similar images in training.
AcknowledgmentsThis work was partially supported by the HPC resources from GENCI-IDRIS (Grant 20XX-AD011012122 and AD011014846).It was funded in part by the French government under management of Agence Nationale de la Recherche as part of the "France 2030" program, reference ANR-23-IACL-0008 (PR[AI]RIE-PSAI projet), the ANR project VideoPredict (ANR-21-FAI1-0002-01), and the Paris Île-de-France Région in the frame of the DIM AI4IDF.Predicted plan and masks grasp<p>black cup</p>[SEG] Multi-view observationInstruction: place the black cup onto the orange cup.
Solving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R Ribas, arXiv:1910.071132019arXiv preprint</p>
<p>C Chi, S Feng, Y Du, Z Xu, E Cousineau, B Burchfiel, S Song, arXiv:2303.04137Diffusion policy: Visuomotor policy learning via action diffusion. 2023</p>
<p>Z Fu, T Z Zhao, C Finn, arXiv:2401.02117Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. 2024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.033782023arXiv preprint</p>
<p>O M Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, T Kreiman, C Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.09246An open-source vision-language-action model. 2024arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. Q Vuong, S Levine, H R Walke, K Pertsch, A Singh, R Doshi, C Xu, J Luo, L Tan, D Shah, CoRL. 2023</p>
<p>Droid: A large-scale in-the-wild robot manipulation dataset. A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, RSS 2024 Workshop: Data Generation for Robotics. </p>
<p>Q Agibot-World-Contributors, J Bu, L Cai, X Chen, Y Cui, S Ding, S Feng, X Gao, X He, X Hu, S Huang, Y Jiang, C Jiang, H Jing, J Li, C Li, Y Liu, Y Liu, J Lu, P Luo, Y Luo, Y Mu, Y Niu, J Pan, Y Pang, G Qiao, C Ren, J Ruan, Y Shan, C Shen, M Shi, M Shi, C Shi, J Sima, H Song, W Wang, D Wang, C Wei, G Xie, J Xu, C Yan, L Yang, S Yang, M Yang, J Yao, C Zeng, Q Zhang, B Zhang, C Zhao, J Zhao, J Zhao, Zhu, arXiv:2503.06669Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. 2025arXiv preprint</p>
<p>CALVIN: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, 2022IEEE RA-L</p>
<p>The colosseum: A benchmark for evaluating generalization for robotic manipulation. W Pumacay, I Singh, J Duan, R Krishna, J Thomason, D Fox, arXiv:2402.081912024arXiv preprint</p>
<p>Towards generalizable vision-language robotic manipulation: A benchmark and LLM-guided 3D policy. R Garcia, S Chen, C Schmid, ICRA. 2025</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, ICRA. 2023</p>
<p>W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023</p>
<p>J Bjorck, F Castañeda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025arXiv preprint</p>
<p>A I , Meta , Llama 3 model card. 2024</p>
<p>arXiv:2302.11550OpenAI. GPT-4 technical report. 2023</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, ICML. 2022</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, CoRL. 2023</p>
<p>Robotic control via embodied chain-of-thought reasoning. Z Michał, C William, P Karl, M Oier, F Chelsea, L Sergey, CORL. 2024</p>
<p>X Li, C Mata, J Park, K Kahatapitiya, Y S Jang, J Shang, K Ranasinghe, R Burgert, M Cai, Y J Lee, arXiv:2406.20095Supercharging robot learning data for vision-language policy. 2024arXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. W Yuan, J Duan, V Blukis, W Pumacay, R Krishna, A Murali, A Mousavian, D Fox, arXiv:2406.107212024arXiv preprint</p>
<p>H Yuan, X Li, T Zhang, Z Huang, S Xu, S Ji, Y Tong, L Qi, J Feng, M.-H Yang, arXiv:2501.04001Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. 2025arXiv preprint</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, 2020IEEE RA-L</p>
<p>L Shao, T Migimatsu, Q Zhang, K Yang, J Bohg, Concept2robot: Learning manipulation concepts from instructions and human demonstrations. IJRR. 2021</p>
<p>Interactive language: Talking to robots in real time. C Lynch, A Wahid, J Tompson, T Ding, J Betker, R Baruch, T Armstrong, P Florence, 2023IEEE RA-L</p>
<p>Languageconditioned imitation learning for robot manipulation tasks. S Stepputtis, J Campbell, M Phielipp, S Lee, C Baral, H Ben Amor, 2020NeurIPS</p>
<p>Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, CoRL. 2018</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, CoRL2022</p>
<p>Instruction-driven history-aware policies for robotic manipulations. P.-L Guhur, S Chen, R Garcia, M Pinel, I Tapaswi, C Laptev, Schmid, CoRL2023</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, CoRL2023</p>
<p>Rvt: Robotic view transformer for 3d object manipulation. A Goyal, J Xu, Y Guo, V Blukis, Y.-W Chao, D Fox, CoRL2023</p>
<p>Polarnet: 3d point clouds for language-guided robotic manipulation. S Chen, R Garcia, C Schmid, I Laptev, CoRL2023</p>
<p>S Chen, R Garcia, I Laptev, C Schmid, Sugar: Pre-training 3d visual representations for robotics. CVPR. 2024</p>
<p>Act3d: 3d feature field transformers for multi-task robotic manipulation. T Gervet, Z Xian, N Gkanatsios, K Fragkiadaki, CoRL2023</p>
<p>T.-W Ke, N Gkanatsios, K Fragkiadaki, arXiv:2402.108853d diffuser actor: Policy diffusion with 3d scene representations. 2024</p>
<p>Rvt2: Learning precise manipulation from few demonstrations. A Goyal, V Blukis, J Xu, Y Guo, Y.-W Chao, D Fox, RSS. 2024</p>
<p>G Tziafas, H Kasaei, arXiv:2406.18722Towards open-world grasping with large vision-language models. 2024arXiv preprint</p>
<p>Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. S James, K Wada, T Laidlow, A J Davison, CVPR. 2022</p>
<p>Learning robotic manipulation policies from point clouds with conditional flow matching. E Chisari, N Heppert, M Argus, T Welschehold, T Brox, A Valada, arXiv:2409.073432024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, ICML. 2021</p>
<p>Scaling robot learning with semantically imagined experience. T Yu, T Xiao, A Stone, J Tompson, A Brohan, S Wang, J Singh, C Tan, D M , J Peralta, B Ichter, K Hausman, F Xia, arXiv:2302.115502023</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollár, R Girshick, arXiv:2304.02643Segment anything. 2023</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Y Hu, F Lin, T Zhang, L Yi, Y Gao, arXiv:2311.178422023</p>
<p>4v (ision) system card. G Openai, 2023preprint</p>
<p>H Liu, C Li, Q Wu, Y J Lee, Visual instruction tuning. NeurIPS. 2024</p>
<p>Improved baselines with visual instruction tuning. H Liu, C Li, Y Li, Y J Lee, CVPR. 2024</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Z Peng, W Wang, L Dong, Y Hao, S Huang, S Ma, F Wei, arXiv:2306.148242023arXiv preprint</p>
<p>K Chen, Z Zhang, W Zeng, R Zhang, F Zhu, R Zhao, arXiv:2306.15195Shikra: Unleashing multimodal llm's referential dialogue magic. 2023arXiv preprint</p>
<p>J Chen, D Zhu, X Shen, X Li, Z Liu, P Zhang, R Krishnamoorthi, V Chandra, Y Xiong, M Elhoseiny, arXiv:2310.09478Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. 2023arXiv preprint</p>
<p>Ferret: Refer and ground anything anywhere at any granularity. H You, H Zhang, Z Gan, X Du, B Zhang, Z Wang, L Cao, S.-F Chang, Y Yang, arXiv:2310.077042023arXiv preprint</p>
<p>W Wang, Q Lv, W Yu, W Hong, J Qi, Y Wang, J Ji, Z Yang, L Zhao, X Song, arXiv:2311.03079Visual expert for pretrained language models. 2023arXiv preprint</p>
<p>Polyformer: Referring image segmentation as sequential polygon generation. J Liu, H Ding, Z Cai, Y Zhang, R K Satzoda, V Mahadevan, R Manmatha, CVPR. 2023</p>
<p>Groma: Localized visual tokenization for grounding multimodal large language models. C Ma, Y Jiang, J Wu, Z Yuan, X Qi, ECCV. 2025</p>
<p>Groundhog: Grounding large language models to holistic segmentation. Y Zhang, Z Ma, X Gao, S Shakiah, Q Gao, J Chai, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>Lisa: Reasoning segmentation via large language model. X Lai, Z Tian, Y Chen, Y Li, Y Yuan, S Liu, J Jia, CVPR. 2024</p>
<p>Llavagrounding: Grounded visual chat with large multimodal models. H Zhang, H Li, F Li, T Ren, X Zou, S Liu, S Huang, J Gao, C Li, J Yang, ECCV. Springer2025</p>
<p>Glamm: Pixel grounding large multimodal model. H Rasheed, M Maaz, S Shaji, A Shaker, S Khan, H Cholakkal, R M Anwer, E Xing, M.-H Yang, F S Khan, CVPR. 2024</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Z Chen, J Wu, W Wang, W Su, G Chen, S Xing, M Zhong, Q Zhang, X Zhu, L Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>N Ravi, V Gabeur, Y.-T Hu, R Hu, C Ryali, T Ma, H Khedr, R Rädle, C Rolland, L Gustafson, arXiv:2408.00714Segment anything in images and videos. 20242arXiv preprint</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. J Rasley, S Rajbhandari, O Ruwase, Y He, ACM SIGKDD. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-639 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-639</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-639</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-d049c0eb7aca40103b7bea484e7421892956b46e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d049c0eb7aca40103b7bea484e7421892956b46e" target="_blank">Quantifying Variance in Evaluation Benchmarks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work provides insights into variance in evaluation benchmarks, suggests LM-specific techniques to reduce variance, and more generally encourages practitioners to carefully factor in variance when comparing models.</p>
                <p><strong>Paper Abstract:</strong> Evaluation benchmarks are the cornerstone of measuring capabilities of large language models (LLMs), as well as driving progress in said capabilities. Originally designed to make claims about capabilities (or lack thereof) in fully pretrained models, evaluation benchmarks are now also extensively used to decide between various training choices. Despite this widespread usage, we rarely quantify the variance in our evaluation benchmarks, which dictates whether differences in performance are meaningful. Here, we define and measure a range of metrics geared towards measuring variance in evaluation benchmarks, including seed variance across initialisations, and monotonicity during training. By studying a large number of models -- both openly available and pretrained from scratch -- we provide empirical estimates for a variety of variance metrics, with considerations and recommendations for practitioners. We also evaluate the utility and tradeoffs of continuous versus discrete performance measures and explore options for better understanding and reducing this variance. We find that simple changes, such as framing choice tasks (like MMLU) as completion tasks, can often reduce variance for smaller scale ($\sim$7B) models, while more involved methods inspired from human testing literature (such as item analysis and item response theory) struggle to meaningfully reduce variance. Overall, our work provides insights into variance in evaluation benchmarks, suggests LM-specific techniques to reduce variance, and more generally encourages practitioners to carefully factor in variance when comparing models.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e639.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e639.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seed variance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seed variance (random initialization variance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The variability in benchmark scores across independently trained models that differ only in random initialization (seed), quantified as the standard deviation of a metric across seeds and averaged over checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-like (seed models trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / Language model evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Benchmark evaluation across 13 NLP benchmarks to quantify variance due to initialization seeds</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random initialization seed (primary); checkpoint timestep (metrics averaged over 21 checkpoints); (they controlled data ordering deterministically to isolate seed effects).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Standard deviation (across 10 seeds) of the preferred metric for each benchmark, averaged over checkpoints (21 timesteps); comparison to bootstrapped 95% confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Empirical seed-variance values (std. dev.) on final checkpoints: e.g. MMLU 0.57, AGIEval 0.77, ARC-C 0.80, HumanEval 1.11, COPA 2.15. Seed variance was generally smaller than per-model 95% CI but non-negligible for small benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of seed variance against bootstrapped 95% CIs; averaging variance over checkpoints; monotonicity across training (Kendall rank correlation).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Seed variance often < 95% CI but still large enough to confound small improvements; e.g., MMLU seed variance 0.57 vs 95% CI 0.72; COPA seed variance 2.15 vs 95% CI 8.30. They used 10 independent runs (seeds) producing 210 checkpoints to compute these numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Small test-set sizes (high CI), chance-level performance (scores ≈ chance reduce informative signal), sensitivity to prompt/formatting choices, and variance that persists across training.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Measure seed variance via multiple runs to contextualize small improvements; prefer continuous metrics; use cloze formulations for certain choice tasks (e.g., MMLU) during pretraining ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Providing seed-variance baseline allows proper comparison; using cloze formatting reduced MMLU seed variance from 0.57 to 0.22; continuous metrics improved SNR (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 independent pretraining runs (different seeds), 21 checkpoints each (210 checkpoints total)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random initialization seed produces measurable variance across benchmarks (std. dev. up to a few percentage points on accuracy), and this seed variance is typically smaller than but comparable to per-model 95% CIs; it must be accounted for when comparing small differences between setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Variance in Evaluation Benchmarks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e639.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e639.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Continuous vs Discrete Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous evaluation metrics (probability mass / NLL) versus discrete metrics (accuracy / EM / pass@1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison between discrete preferred metrics and continuous proxies (probability mass for choices, negative log-likelihood for generations) showing continuous metrics have much higher signal-to-noise ratio and better monotonicity during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-like seed models (and many other models across evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (primary seed experiments); also evaluated across many model sizes</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / Benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluate benchmarks using both discrete metrics (accuracy / EM / pass@1) and continuous metrics (prob mass / NLL) and compare variance, SNR, and monotonicity.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Discretization of outputs (rounding to correct/incorrect), low sample counts, and ties in argmax selection; continuous metrics capture graded model confidence, reducing apparent stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Signal-to-noise ratio (mean / std across seeds), standard deviation across seeds, monotonicity (Kendall rank correlation), and visualization of variance over time.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Continuous metrics have substantially higher SNR across all benchmarks (examples from Table 2: MMLU Disc SNR 52.45 vs Cont SNR 347.57; Hellaswag Disc SNR 608.23 vs Cont SNR 1921.15; PIQA Disc SNR 198.98 vs Cont SNR 1641.14). Monotonicity is consistently higher for continuous metrics (mon_cont > mon_disc for all benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>SNR comparison, monotonicity (Kendall correlation) across checkpoints, and visual inspection of performance development over time.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Continuous metrics provide more stable, monotonic and lower-noise signals during training, making them more reliable for comparing configurations and building scaling laws.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Some benchmarks (e.g., BigBench Hard, MATH) lack continuous metrics in this study; continuous metrics require defining a consistent continuous measure (prob mass, NLL) per task.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Adopt continuous metrics (probability mass for choices, target NLL for generation) when monitoring and comparing models.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Marked increase in SNR and monotonicity; e.g., for MMLU SNR improved from 52.45 (disc) to 347.57 (cont), and monotonicity values rose across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Computed across 10 seed models and 21 checkpoints (210 points) and across a larger set of models when comparing final-checkpoint SNRs</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Continuous evaluation metrics (prob mass / NLL) consistently reduce noise and improve monotonicity relative to discrete metrics, making them preferable for model comparisons and monitoring during training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Variance in Evaluation Benchmarks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e639.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e639.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrapped and Analytic 95% CI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrapped 95% confidence intervals and analytic CI formula for discrete metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Per-model uncertainty estimates computed using bootstrapping (empirical 95% CI) and an analytic approximation for discrete metrics: CI_analytic = 1.96 * sqrt(S*(1-S)/N).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-like seed models (and others for CI comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (primary seed experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation / statistical estimation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Estimate per-model uncertainty of discrete benchmark scores using bootstrapping and analytic formula, and compare those CIs to seed variance.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Finite test set size (N), discrete success/failure outcomes, and sampling variability at evaluation time.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Bootstrapped 95% confidence intervals (empirical) and analytic CI computed as 1.96*sqrt(S*(1-S)/N).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Reported 95% CI examples (avg across 210 checkpoints): COPA 8.30, HumanEval 3.98, MMLU 0.72, ARC-C 2.74, AGIEval 1.63. Bootstrapped and analytic CIs converge empirically when bootstrap samples are large.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of bootstrapped CI vs analytic CI and contrast with seed variance values.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Bootstrapped and analytic CIs provide a per-model uncertainty bound that is typically larger than seed variance; CIs can therefore dominate the uncertainty when comparing single-run models.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Bootstrapping is computationally expensive; analytic CI assumes binomial behavior which may not hold for all metrics or small N.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Compute bootstrapped or analytic CIs to contextualize observed score differences; where expensive, analytic CI provides a fast approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Analytic CI is a good approximation when bootstrap samples are large; using these CIs clarifies when score differences are statistically meaningful versus within sampling noise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>95% CIs computed over 210 checkpoints (10 seeds × 21 checkpoints); bootstrapping used with many samples (exact count not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Per-model 95% confidence intervals (bootstrapped or analytic) are often larger than seed variance and must be reported to assess statistical significance of score differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Variance in Evaluation Benchmarks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e639.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e639.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Monotonicity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monotonicity of benchmark scores during training (Kendall rank correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A measure of how stably a benchmark's score increases (or decreases) during training, computed as Kendall rank correlation between the sequence of checkpoint scores and a monotonically increasing array.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-like seed models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP training dynamics / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Quantify whether benchmark metrics show monotonic improvement over pretraining checkpoints for both discrete and continuous metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Non-monotonicity can arise from discrete metric saturation at chance or ceiling, item sampling noise, and instability in learning dynamics across checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Kendall rank correlation between checkpoint sequence and a monotonic sequence (mon_disc and mon_cont reported per benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Continuous metrics consistently show higher monotonicity than discrete ones across benchmarks (example: ARC-C mon_disc 0.88 vs mon_cont 0.91; MMLU mon_disc 0.09 vs mon_cont 0.15; MMLU-Cloze mon_disc 0.95 vs mon_cont 0.96).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Monotonicity values computed per seed across 21 checkpoints; aggregated across seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Higher monotonicity for continuous metrics indicates more stable and reproducible tracking of model improvement during training.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Benchmarks at chance or saturation provide poor monotonic signals; discrete metrics can oscillate more, reducing interpretability during ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Prefer continuous metrics or alternative task formulations (e.g., cloze) to improve monotonicity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Cloze formulation and continuous metrics yielded substantially higher monotonicity on affected tasks (e.g., MMLU-Cloze mon_disc 0.95 vs standard MMLU mon_disc 0.09).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Monotonicity computed across 10 seed runs and 21 checkpoints per run (210 datapoints per benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Monotonicity is generally higher for continuous metrics, making them better suited for tracking learning progress; discrete metrics can obscure improvement due to non-monotonic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Variance in Evaluation Benchmarks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e639.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e639.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Item analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Item analysis (difficulty and discrimination)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Per-item statistics adapted from human standardized testing: item difficulty (mean score across models) and item discrimination (correlation between per-item performance and overall model performance), used to assess and potentially prune evaluation items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Item analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (70 models across families including LLaMA, Mistral, Qwen, Pythia, DeepSeek, Falcon, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP benchmark design / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Compute item difficulty and discrimination across a train/test split of models (random and difficulty splits), test pruning low-discrimination items and measure effects on mean, standard error, and monotonicity.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Heterogeneity of items (some always right/wrong), model strength differences (train vs test splits), out-of-distribution splits causing discrimination to not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Item difficulty (average score), item discrimination (correlation across models), downstream effects on mean, standard error of mean, and monotonicity when pruning items.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Item discrimination computed showed poor transfer from weaker (train) to stronger (test) models; pruning up to 20% low-discrimination items led to modest decreases in std. err. and modest increases in monotonicity but also caused drift in the estimated mean (risking overestimation of capabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Correlation of item discrimination between train/test splits; changes in mean, std. err., and monotonicity after iterative pruning (plots shown up to 20% removal).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Train-set high discrimination often did not predict test-set discrimination (especially across difficulty splits), limiting the utility of item-analysis based pruning for future stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Discrimination depends on the distribution of model abilities; as models improve, items that discriminated earlier may saturate and lose informativeness; some items displayed negative discrimination without clear patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Prune low discrimination items based on train-set statistics; compare random vs targeted removal; inspect items qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Only modest improvements in std. err. and monotonicity; introduced non-negligible drift in mean score, so not recommended as a general reliability fix.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Item analysis run over 70 models (split into train/test: various splits described), with pruning experiments evaluated on test split.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Item analysis metrics (difficulty/discrimination) provide limited benefit for reducing variance in LM evaluations: pruning low-discrimination items gives modest variance reduction but risks biased mean estimates and does not generalize well across model strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Variance in Evaluation Benchmarks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e639.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e639.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Item Response Theory (IRT / IRT++)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Item Response Theory and IRT++ (tiny-benchmarks selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of IRT-based selection (including the IRT++ estimator from prior tiny-benchmarks work) to select small representative subsets of evaluation items; evaluated here for effects on mean estimate accuracy, seed variance, standard error, and ranking stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Item response theory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (70 models; also 7B seed models for tiny benchmark seed-variance analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (and 7B for seed experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP benchmark construction / evaluation efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Apply IRT and IRT++ subset selection (100 examples as in Polo et al.) and measure impact on mean estimate, seed variance, standard error, monotonicity, and model ranking fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Subsampling items (small tiny-benchmarks), instability of IRT-estimated parameters across model strengths, and small N leading to increased sampling noise.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Seed variance (std across seeds), deviation in mean estimate vs full dataset, standard error of the mean, monotonicity, and Kendall rank correlation / fraction of pairwise ranking flips between full dataset and subset estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Using 100-item IRT subset led to large deviations in mean (e.g., ARC-C full μ=39.71 vs IRT μ=46.21, an overestimation ≈7%), and greatly increased seed variance (ARC-C full E=0.80 vs IRT E=1.80 vs IRT++ E=1.86). Kendall correlation on 70 models dropped as low as 0.76, corresponding to ≈12% of pairwise model comparisons reversed when using IRT/IRT++ vs full dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Mean deviation, seed variance increase, monotonicity decrease (Table 4), Kendall rank correlation and percent of pairwise flips.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>IRT-based tiny-benchmarks can provide reasonable point estimates of the mean (IRT++ improved mean fidelity) but substantially increase variance and ranking instability, compromising their use for model comparisons and pretraining ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Small selected subsets amplify seed and sampling variance, and selection tuned on weaker models may not generalize to stronger models, causing flips in model rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use IRT++ estimator rather than naive IRT selection to reduce mean deviation; but overall caution against using small IRT-selected subsets for comparative evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>IRT++ reduced mean deviation compared to plain IRT but did not mitigate the increased seed variance and reduced monotonicity; practitioner caution advised.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Tiny-benchmark comparisons performed on 10 seed runs for 7B models and across 70 models for ranking analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRT-based tiny-benchmarks may estimate mean scores with modest bias correction (IRT++), but they substantially increase seed variance and reduce ranking stability (Kendall correlation down to ~0.76), making them ill-suited for model-comparison use in pretraining ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Variance in Evaluation Benchmarks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e639.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e639.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMLU-Cloze vs Standard MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MMLU-Cloze prompt formulation (cloze-style few-shots) compared to standard multiple-choice formatting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative prompt formatting for MMLU where few-shot examples present the gold choice text (cloze) instead of explicit choice letters; found to substantially reduce seed variance and greatly increase monotonicity during early training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-like seed models; also tested on larger LLaMA-family models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (primary seed experiments); examples on 13B and up referenced</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / prompt engineering for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Compare benchmark performance, seed variance, and monotonicity between standard MMLU multiple-choice prompt formatting and a cloze-style few-shot formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Prompt formatting (presence of choice letters and their placement), few-shot example formatting, and scoring method (lowest NLL over appended letters vs textual cloze).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Seed variance (std across seeds), monotonicity (Kendall), and raw accuracy comparisons across checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Cloze formulation (MMLU-Cloze) had lower seed variance (0.22) vs standard MMLU (0.57) and dramatically higher monotonicity (mon_disc 0.95 for cloze vs 0.09 for standard; mon_cont 0.96 vs 0.15). Large, fully-trained models may eventually perform better on standard MMLU in absolute terms, but cloze is more stable for early-stage comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Direct comparison of seed variance and monotonicity across the two prompt formats using the same seed models and checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Cloze format reduces evaluation noise and gives more monotonic progress signals during pretraining ablations, while correlating strongly with final standard-format performance for large fully-trained models (Pearson r ≈ 0.92 across 70 large models).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Absolute performance ordering can differ at final large-model scale (standard format may achieve higher absolute accuracy), so cloze is recommended primarily for early ablation and monitoring rather than final leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use cloze-style prompt formatting for MMLU when performing pretraining or ablation experiments to reduce variance and increase monotonicity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Substantial: seed variance reduced from 0.57 to 0.22 and monotonicity improved from 0.09 to 0.95 (discrete metric) in the 7B seed experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 seeded pretraining runs with 21 checkpoints (same as other seed experiments); correlation evaluated across 70 larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt formulation materially affects variability: MMLU-Cloze markedly reduces seed variance and yields a far more monotonic training signal, making it preferable for pretraining ablations and early-stage comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Variance in Evaluation Benchmarks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Item response theory <em>(Rating: 2)</em></li>
                <li>Item analysis <em>(Rating: 2)</em></li>
                <li>tinybenchmarks: evaluating llms with fewer examples <em>(Rating: 2)</em></li>
                <li>When benchmarks are targets: Revealing the sensitivity of large language model leaderboards <em>(Rating: 1)</em></li>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How I learned to start worrying about prompt formatting <em>(Rating: 1)</em></li>
                <li>Investigating data variance in evaluations of automatic machine translation metrics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-639",
    "paper_id": "paper-d049c0eb7aca40103b7bea484e7421892956b46e",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Seed variance",
            "name_full": "Seed variance (random initialization variance)",
            "brief_description": "The variability in benchmark scores across independently trained models that differ only in random initialization (seed), quantified as the standard deviation of a metric across seeds and averaged over checkpoints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-like (seed models trained from scratch)",
            "model_size": "7B",
            "scientific_domain": "NLP / Language model evaluation",
            "experimental_task": "Benchmark evaluation across 13 NLP benchmarks to quantify variance due to initialization seeds",
            "variability_sources": "Random initialization seed (primary); checkpoint timestep (metrics averaged over 21 checkpoints); (they controlled data ordering deterministically to isolate seed effects).",
            "variability_measured": true,
            "variability_metrics": "Standard deviation (across 10 seeds) of the preferred metric for each benchmark, averaged over checkpoints (21 timesteps); comparison to bootstrapped 95% confidence intervals.",
            "variability_results": "Empirical seed-variance values (std. dev.) on final checkpoints: e.g. MMLU 0.57, AGIEval 0.77, ARC-C 0.80, HumanEval 1.11, COPA 2.15. Seed variance was generally smaller than per-model 95% CI but non-negligible for small benchmarks.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of seed variance against bootstrapped 95% CIs; averaging variance over checkpoints; monotonicity across training (Kendall rank correlation).",
            "reproducibility_results": "Seed variance often &lt; 95% CI but still large enough to confound small improvements; e.g., MMLU seed variance 0.57 vs 95% CI 0.72; COPA seed variance 2.15 vs 95% CI 8.30. They used 10 independent runs (seeds) producing 210 checkpoints to compute these numbers.",
            "reproducibility_challenges": "Small test-set sizes (high CI), chance-level performance (scores ≈ chance reduce informative signal), sensitivity to prompt/formatting choices, and variance that persists across training.",
            "mitigation_methods": "Measure seed variance via multiple runs to contextualize small improvements; prefer continuous metrics; use cloze formulations for certain choice tasks (e.g., MMLU) during pretraining ablations.",
            "mitigation_effectiveness": "Providing seed-variance baseline allows proper comparison; using cloze formatting reduced MMLU seed variance from 0.57 to 0.22; continuous metrics improved SNR (see separate entry).",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 independent pretraining runs (different seeds), 21 checkpoints each (210 checkpoints total)",
            "key_findings": "Random initialization seed produces measurable variance across benchmarks (std. dev. up to a few percentage points on accuracy), and this seed variance is typically smaller than but comparable to per-model 95% CIs; it must be accounted for when comparing small differences between setups.",
            "uuid": "e639.0",
            "source_info": {
                "paper_title": "Quantifying Variance in Evaluation Benchmarks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Continuous vs Discrete Metrics",
            "name_full": "Continuous evaluation metrics (probability mass / NLL) versus discrete metrics (accuracy / EM / pass@1)",
            "brief_description": "Comparison between discrete preferred metrics and continuous proxies (probability mass for choices, negative log-likelihood for generations) showing continuous metrics have much higher signal-to-noise ratio and better monotonicity during training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-like seed models (and many other models across evaluation)",
            "model_size": "7B (primary seed experiments); also evaluated across many model sizes",
            "scientific_domain": "NLP / Benchmark evaluation",
            "experimental_task": "Evaluate benchmarks using both discrete metrics (accuracy / EM / pass@1) and continuous metrics (prob mass / NLL) and compare variance, SNR, and monotonicity.",
            "variability_sources": "Discretization of outputs (rounding to correct/incorrect), low sample counts, and ties in argmax selection; continuous metrics capture graded model confidence, reducing apparent stochasticity.",
            "variability_measured": true,
            "variability_metrics": "Signal-to-noise ratio (mean / std across seeds), standard deviation across seeds, monotonicity (Kendall rank correlation), and visualization of variance over time.",
            "variability_results": "Continuous metrics have substantially higher SNR across all benchmarks (examples from Table 2: MMLU Disc SNR 52.45 vs Cont SNR 347.57; Hellaswag Disc SNR 608.23 vs Cont SNR 1921.15; PIQA Disc SNR 198.98 vs Cont SNR 1641.14). Monotonicity is consistently higher for continuous metrics (mon_cont &gt; mon_disc for all benchmarks).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "SNR comparison, monotonicity (Kendall correlation) across checkpoints, and visual inspection of performance development over time.",
            "reproducibility_results": "Continuous metrics provide more stable, monotonic and lower-noise signals during training, making them more reliable for comparing configurations and building scaling laws.",
            "reproducibility_challenges": "Some benchmarks (e.g., BigBench Hard, MATH) lack continuous metrics in this study; continuous metrics require defining a consistent continuous measure (prob mass, NLL) per task.",
            "mitigation_methods": "Adopt continuous metrics (probability mass for choices, target NLL for generation) when monitoring and comparing models.",
            "mitigation_effectiveness": "Marked increase in SNR and monotonicity; e.g., for MMLU SNR improved from 52.45 (disc) to 347.57 (cont), and monotonicity values rose across datasets.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Computed across 10 seed models and 21 checkpoints (210 points) and across a larger set of models when comparing final-checkpoint SNRs",
            "key_findings": "Continuous evaluation metrics (prob mass / NLL) consistently reduce noise and improve monotonicity relative to discrete metrics, making them preferable for model comparisons and monitoring during training.",
            "uuid": "e639.1",
            "source_info": {
                "paper_title": "Quantifying Variance in Evaluation Benchmarks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Bootstrapped and Analytic 95% CI",
            "name_full": "Bootstrapped 95% confidence intervals and analytic CI formula for discrete metrics",
            "brief_description": "Per-model uncertainty estimates computed using bootstrapping (empirical 95% CI) and an analytic approximation for discrete metrics: CI_analytic = 1.96 * sqrt(S*(1-S)/N).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-like seed models (and others for CI comparisons)",
            "model_size": "7B (primary seed experiments)",
            "scientific_domain": "NLP evaluation / statistical estimation",
            "experimental_task": "Estimate per-model uncertainty of discrete benchmark scores using bootstrapping and analytic formula, and compare those CIs to seed variance.",
            "variability_sources": "Finite test set size (N), discrete success/failure outcomes, and sampling variability at evaluation time.",
            "variability_measured": true,
            "variability_metrics": "Bootstrapped 95% confidence intervals (empirical) and analytic CI computed as 1.96*sqrt(S*(1-S)/N).",
            "variability_results": "Reported 95% CI examples (avg across 210 checkpoints): COPA 8.30, HumanEval 3.98, MMLU 0.72, ARC-C 2.74, AGIEval 1.63. Bootstrapped and analytic CIs converge empirically when bootstrap samples are large.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of bootstrapped CI vs analytic CI and contrast with seed variance values.",
            "reproducibility_results": "Bootstrapped and analytic CIs provide a per-model uncertainty bound that is typically larger than seed variance; CIs can therefore dominate the uncertainty when comparing single-run models.",
            "reproducibility_challenges": "Bootstrapping is computationally expensive; analytic CI assumes binomial behavior which may not hold for all metrics or small N.",
            "mitigation_methods": "Compute bootstrapped or analytic CIs to contextualize observed score differences; where expensive, analytic CI provides a fast approximation.",
            "mitigation_effectiveness": "Analytic CI is a good approximation when bootstrap samples are large; using these CIs clarifies when score differences are statistically meaningful versus within sampling noise.",
            "comparison_with_without_controls": true,
            "number_of_runs": "95% CIs computed over 210 checkpoints (10 seeds × 21 checkpoints); bootstrapping used with many samples (exact count not specified).",
            "key_findings": "Per-model 95% confidence intervals (bootstrapped or analytic) are often larger than seed variance and must be reported to assess statistical significance of score differences.",
            "uuid": "e639.2",
            "source_info": {
                "paper_title": "Quantifying Variance in Evaluation Benchmarks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Monotonicity",
            "name_full": "Monotonicity of benchmark scores during training (Kendall rank correlation)",
            "brief_description": "A measure of how stably a benchmark's score increases (or decreases) during training, computed as Kendall rank correlation between the sequence of checkpoint scores and a monotonically increasing array.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-like seed models",
            "model_size": "7B",
            "scientific_domain": "NLP training dynamics / evaluation",
            "experimental_task": "Quantify whether benchmark metrics show monotonic improvement over pretraining checkpoints for both discrete and continuous metrics.",
            "variability_sources": "Non-monotonicity can arise from discrete metric saturation at chance or ceiling, item sampling noise, and instability in learning dynamics across checkpoints.",
            "variability_measured": true,
            "variability_metrics": "Kendall rank correlation between checkpoint sequence and a monotonic sequence (mon_disc and mon_cont reported per benchmark).",
            "variability_results": "Continuous metrics consistently show higher monotonicity than discrete ones across benchmarks (example: ARC-C mon_disc 0.88 vs mon_cont 0.91; MMLU mon_disc 0.09 vs mon_cont 0.15; MMLU-Cloze mon_disc 0.95 vs mon_cont 0.96).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Monotonicity values computed per seed across 21 checkpoints; aggregated across seeds.",
            "reproducibility_results": "Higher monotonicity for continuous metrics indicates more stable and reproducible tracking of model improvement during training.",
            "reproducibility_challenges": "Benchmarks at chance or saturation provide poor monotonic signals; discrete metrics can oscillate more, reducing interpretability during ablations.",
            "mitigation_methods": "Prefer continuous metrics or alternative task formulations (e.g., cloze) to improve monotonicity.",
            "mitigation_effectiveness": "Cloze formulation and continuous metrics yielded substantially higher monotonicity on affected tasks (e.g., MMLU-Cloze mon_disc 0.95 vs standard MMLU mon_disc 0.09).",
            "comparison_with_without_controls": true,
            "number_of_runs": "Monotonicity computed across 10 seed runs and 21 checkpoints per run (210 datapoints per benchmark).",
            "key_findings": "Monotonicity is generally higher for continuous metrics, making them better suited for tracking learning progress; discrete metrics can obscure improvement due to non-monotonic behavior.",
            "uuid": "e639.3",
            "source_info": {
                "paper_title": "Quantifying Variance in Evaluation Benchmarks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Item analysis",
            "name_full": "Item analysis (difficulty and discrimination)",
            "brief_description": "Per-item statistics adapted from human standardized testing: item difficulty (mean score across models) and item discrimination (correlation between per-item performance and overall model performance), used to assess and potentially prune evaluation items.",
            "citation_title": "Item analysis",
            "mention_or_use": "use",
            "model_name": "Various (70 models across families including LLaMA, Mistral, Qwen, Pythia, DeepSeek, Falcon, etc.)",
            "model_size": "various",
            "scientific_domain": "NLP benchmark design / evaluation",
            "experimental_task": "Compute item difficulty and discrimination across a train/test split of models (random and difficulty splits), test pruning low-discrimination items and measure effects on mean, standard error, and monotonicity.",
            "variability_sources": "Heterogeneity of items (some always right/wrong), model strength differences (train vs test splits), out-of-distribution splits causing discrimination to not generalize.",
            "variability_measured": true,
            "variability_metrics": "Item difficulty (average score), item discrimination (correlation across models), downstream effects on mean, standard error of mean, and monotonicity when pruning items.",
            "variability_results": "Item discrimination computed showed poor transfer from weaker (train) to stronger (test) models; pruning up to 20% low-discrimination items led to modest decreases in std. err. and modest increases in monotonicity but also caused drift in the estimated mean (risking overestimation of capabilities).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Correlation of item discrimination between train/test splits; changes in mean, std. err., and monotonicity after iterative pruning (plots shown up to 20% removal).",
            "reproducibility_results": "Train-set high discrimination often did not predict test-set discrimination (especially across difficulty splits), limiting the utility of item-analysis based pruning for future stronger models.",
            "reproducibility_challenges": "Discrimination depends on the distribution of model abilities; as models improve, items that discriminated earlier may saturate and lose informativeness; some items displayed negative discrimination without clear patterns.",
            "mitigation_methods": "Prune low discrimination items based on train-set statistics; compare random vs targeted removal; inspect items qualitatively.",
            "mitigation_effectiveness": "Only modest improvements in std. err. and monotonicity; introduced non-negligible drift in mean score, so not recommended as a general reliability fix.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Item analysis run over 70 models (split into train/test: various splits described), with pruning experiments evaluated on test split.",
            "key_findings": "Item analysis metrics (difficulty/discrimination) provide limited benefit for reducing variance in LM evaluations: pruning low-discrimination items gives modest variance reduction but risks biased mean estimates and does not generalize well across model strengths.",
            "uuid": "e639.4",
            "source_info": {
                "paper_title": "Quantifying Variance in Evaluation Benchmarks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Item Response Theory (IRT / IRT++)",
            "name_full": "Item Response Theory and IRT++ (tiny-benchmarks selection)",
            "brief_description": "Application of IRT-based selection (including the IRT++ estimator from prior tiny-benchmarks work) to select small representative subsets of evaluation items; evaluated here for effects on mean estimate accuracy, seed variance, standard error, and ranking stability.",
            "citation_title": "Item response theory",
            "mention_or_use": "use",
            "model_name": "Various (70 models; also 7B seed models for tiny benchmark seed-variance analysis)",
            "model_size": "various (and 7B for seed experiments)",
            "scientific_domain": "NLP benchmark construction / evaluation efficiency",
            "experimental_task": "Apply IRT and IRT++ subset selection (100 examples as in Polo et al.) and measure impact on mean estimate, seed variance, standard error, monotonicity, and model ranking fidelity.",
            "variability_sources": "Subsampling items (small tiny-benchmarks), instability of IRT-estimated parameters across model strengths, and small N leading to increased sampling noise.",
            "variability_measured": true,
            "variability_metrics": "Seed variance (std across seeds), deviation in mean estimate vs full dataset, standard error of the mean, monotonicity, and Kendall rank correlation / fraction of pairwise ranking flips between full dataset and subset estimates.",
            "variability_results": "Using 100-item IRT subset led to large deviations in mean (e.g., ARC-C full μ=39.71 vs IRT μ=46.21, an overestimation ≈7%), and greatly increased seed variance (ARC-C full E=0.80 vs IRT E=1.80 vs IRT++ E=1.86). Kendall correlation on 70 models dropped as low as 0.76, corresponding to ≈12% of pairwise model comparisons reversed when using IRT/IRT++ vs full dataset.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Mean deviation, seed variance increase, monotonicity decrease (Table 4), Kendall rank correlation and percent of pairwise flips.",
            "reproducibility_results": "IRT-based tiny-benchmarks can provide reasonable point estimates of the mean (IRT++ improved mean fidelity) but substantially increase variance and ranking instability, compromising their use for model comparisons and pretraining ablations.",
            "reproducibility_challenges": "Small selected subsets amplify seed and sampling variance, and selection tuned on weaker models may not generalize to stronger models, causing flips in model rankings.",
            "mitigation_methods": "Use IRT++ estimator rather than naive IRT selection to reduce mean deviation; but overall caution against using small IRT-selected subsets for comparative evaluations.",
            "mitigation_effectiveness": "IRT++ reduced mean deviation compared to plain IRT but did not mitigate the increased seed variance and reduced monotonicity; practitioner caution advised.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Tiny-benchmark comparisons performed on 10 seed runs for 7B models and across 70 models for ranking analyses.",
            "key_findings": "IRT-based tiny-benchmarks may estimate mean scores with modest bias correction (IRT++), but they substantially increase seed variance and reduce ranking stability (Kendall correlation down to ~0.76), making them ill-suited for model-comparison use in pretraining ablations.",
            "uuid": "e639.5",
            "source_info": {
                "paper_title": "Quantifying Variance in Evaluation Benchmarks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MMLU-Cloze vs Standard MMLU",
            "name_full": "MMLU-Cloze prompt formulation (cloze-style few-shots) compared to standard multiple-choice formatting",
            "brief_description": "An alternative prompt formatting for MMLU where few-shot examples present the gold choice text (cloze) instead of explicit choice letters; found to substantially reduce seed variance and greatly increase monotonicity during early training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-like seed models; also tested on larger LLaMA-family models",
            "model_size": "7B (primary seed experiments); examples on 13B and up referenced",
            "scientific_domain": "NLP / prompt engineering for evaluation",
            "experimental_task": "Compare benchmark performance, seed variance, and monotonicity between standard MMLU multiple-choice prompt formatting and a cloze-style few-shot formatting.",
            "variability_sources": "Prompt formatting (presence of choice letters and their placement), few-shot example formatting, and scoring method (lowest NLL over appended letters vs textual cloze).",
            "variability_measured": true,
            "variability_metrics": "Seed variance (std across seeds), monotonicity (Kendall), and raw accuracy comparisons across checkpoints.",
            "variability_results": "Cloze formulation (MMLU-Cloze) had lower seed variance (0.22) vs standard MMLU (0.57) and dramatically higher monotonicity (mon_disc 0.95 for cloze vs 0.09 for standard; mon_cont 0.96 vs 0.15). Large, fully-trained models may eventually perform better on standard MMLU in absolute terms, but cloze is more stable for early-stage comparisons.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Direct comparison of seed variance and monotonicity across the two prompt formats using the same seed models and checkpoints.",
            "reproducibility_results": "Cloze format reduces evaluation noise and gives more monotonic progress signals during pretraining ablations, while correlating strongly with final standard-format performance for large fully-trained models (Pearson r ≈ 0.92 across 70 large models).",
            "reproducibility_challenges": "Absolute performance ordering can differ at final large-model scale (standard format may achieve higher absolute accuracy), so cloze is recommended primarily for early ablation and monitoring rather than final leaderboards.",
            "mitigation_methods": "Use cloze-style prompt formatting for MMLU when performing pretraining or ablation experiments to reduce variance and increase monotonicity.",
            "mitigation_effectiveness": "Substantial: seed variance reduced from 0.57 to 0.22 and monotonicity improved from 0.09 to 0.95 (discrete metric) in the 7B seed experiments.",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 seeded pretraining runs with 21 checkpoints (same as other seed experiments); correlation evaluated across 70 larger models.",
            "key_findings": "Prompt formulation materially affects variability: MMLU-Cloze markedly reduces seed variance and yields a far more monotonic training signal, making it preferable for pretraining ablations and early-stage comparisons.",
            "uuid": "e639.6",
            "source_info": {
                "paper_title": "Quantifying Variance in Evaluation Benchmarks",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Item response theory",
            "rating": 2
        },
        {
            "paper_title": "Item analysis",
            "rating": 2
        },
        {
            "paper_title": "tinybenchmarks: evaluating llms with fewer examples",
            "rating": 2
        },
        {
            "paper_title": "When benchmarks are targets: Revealing the sensitivity of large language model leaderboards",
            "rating": 1
        },
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How I learned to start worrying about prompt formatting",
            "rating": 1
        },
        {
            "paper_title": "Investigating data variance in evaluations of automatic machine translation metrics",
            "rating": 2
        }
    ],
    "cost": 0.0209675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Quantifying Variance in Evaluation Benchmarks</h1>
<p>Lovish Madaan ${ }^{\alpha, \beta}$ Aaditya K. Singh ${ }^{\gamma}$ Rylan Schaeffer ${ }^{\delta}$ Andrew Poulton ${ }^{\epsilon}$<br>Sanmi Koyejo ${ }^{\delta}$ Pontus Stenetorp ${ }^{\beta}$ Sharan Narang ${ }^{\alpha}$ Dieuwke Hupkes ${ }^{\alpha}$<br>${ }^{\alpha}$ GenAI, Meta $\quad{ }^{\beta}$ UCL $\quad{ }^{\gamma}$ Gatsby Unit, UCL $\quad{ }^{\delta}$ Stanford University $\quad{ }^{\epsilon}$ Cohere<br>{lovish, dieuwkehupkes}@meta.com</p>
<h4>Abstract</h4>
<p>Evaluation benchmarks are the cornerstone of measuring capabilities of large language models (LLMs), as well as driving progress in said capabilities. Originally designed to make claims about capabilities (or lack thereof) in fully pretrained models, evaluation benchmarks are now also extensively used to decide between various training choices. Despite this widespread usage, we rarely quantify the variance in our evaluation benchmarks, which dictates whether differences in performance are meaningful. Here, we define and measure a range of metrics geared towards measuring variance in evaluation benchmarks, including seed variance across initialisations, and monotonicity during training. By studying a large number of models - both openly available and pretrained from scratch - we provide empirical estimates for a variety of variance metrics, with considerations and recommendations for practitioners. We also evaluate the utility and tradeoffs of continuous versus discrete performance measures and explore options for better understanding and reducing this variance. We find that simple changes, such as framing choice tasks (like MMLU) as completion tasks, can often reduce variance for smaller scale ( $\sim 7 \mathrm{~B}$ ) models, while more involved methods inspired from human testing literature (such as item analysis and item response theory) struggle to meaningfully reduce variance. Overall, our work provides insights into variance in evaluation benchmarks, suggests LM-specific techniques to reduce variance, and more generally encourages practitioners to carefully factor in variance when comparing models.</p>
<h2>1 Introduction</h2>
<p>Benchmark evaluation datasets are the cornerstone of establishing and defining progress with large language models (LLMs). Virtually any new model release is accompanied by a range of scores on common evaluation benchmarks, illustrating how the model tallies up against previous releases (Mesnard et al., 2024; AI@Meta, 2024; Achiam et al., 2023; Reid et al., 2024). As such, evaluation datasets play an important role in claiming progress and the title of state-of-the-art. Consequently, choices in model development are often based on how they impact performance on benchmarks considered important by the field, giving benchmarks a prominent role in model iteration as well. Yet, despite their importance, benchmark scores are often regarded as a one-dimensional number, and it is rare that they are given a more detailed consideration. While it is well known that benchmarks scores can be heavily influenced by the choice of prompt (Sclar et al., 2023), the distributions of labels in the provided few-shots (Weber et al., 2023) or even the symbols that are used for the different options in a multiple choice setup (Zheng et al., 2023; Alzahrani et al., 2024), papers rarely report more than a single number per benchmark, or specifics on how each number was computed. Furthermore, statistical significance values are scarcely reported on major release papers or leaderboards, or even in papers that study how scores vary across various dimensions. These issues muddy the power of</p>
<p>evaluation datasets, both during development and evaluation: if we cannot 'trust' our evaluation results or do not understand what improvements are statistically significant, we cannot make sound comparisons, thus making it more challenging to reliably use benchmarks during model development.
To address this, we present a deep dive into variance in benchmark scores, at much larger scale than any previous work. Across all our experiments, we consider 13 different popular benchmarks and compute their performance over 280 different models, including fully trained public models as well as a set of 7B models and their intermediate checkpoints that we trained from scratch, differing only in their initialisation random seed.
With this, our contributions are three-fold:</p>
<ul>
<li>We provide a comprehensive reference guide for what magnitudes of variance are expected for what benchmarks across various circumstances.</li>
<li>We make suggestions of how variance can be reduced for smaller scale models on choice tasks of important value (MMLU).</li>
<li>We caution against the use of methods from human standardised testing (item analysis, item response theory) as a means of reducing variance, finding them to be ineffective.
Our work brings to light the often overlooked problem of variance in evaluation benchmarks, quantifies its effects, and provides a set of positive and negative results on how to mitigate it.</li>
</ul>
<h1>2 Models and Benchmarks</h1>
<p>We run our analysis by comparing benchmark results across a large number of models trained across various setups. In this section, we describe these models and list the benchmarks we investigate.</p>
<h3>2.1 Models</h3>
<p>In our analysis, we use over 280 models for our analysis, including intermediate checkpoints. First, we train ten Llama-2-7B-architecture models from scratch on our own pre-training data mixture inspired by Touvron et al. (2023a) (See Appendix A). These 10 runs are identical, except for the model initialisation seed. The model hyper-parameters, the pre-training data mixture, and the data-loading mechanism is consistent across all these ten runs. We train these models for 210 billion tokens and store 21 checkpoints for each model, leaving us with 10 sets of 21 model snapshots. We refer to these 210 checkpoints as the "seed models." In addition, we use 41 intermediate and fully-trained models based on the Llama-1 and Llama-2 architecture pre-trained on the same data mixture used for training the seed models.
Finally, we use 32 publicly available models from Huggingface (Wolf et al., 2020): Meta-Llama-3 [8, 70]B (AI@Meta, 2024), Gemma [2, 7]B (Mesnard et al., 2024), DBRX-Base (Databricks, 2024), Mistral 7B (Jiang et al., 2023), Mixtral 8x[7, 22]B (Jiang et al., 2024), Qwen-1.5 [0.5, 1.8, 4, 7, 14, 32, 72, 110]B (Bai et al., 2023), Pythia [1, 1.4, 2.8, 6.9, 12]B (Biderman et al., 2023), Falcon [7, 40]B (Almazrouei et al., 2023), DeepSeek [7, 67]B (Bi et al., 2024), DeepSeek-MoE 16B (Bi et al., 2024), DeepSeek V2 (DeepSeek-AI, 2024), StableLM [1.6, 3, 7]B (StabilityAI, 2024), and MPT [7, 30]B (MosaicML NLP Team, 2023).
The set of models used for the analysis are diverse across architectures, data mixtures, and sizes ranging from 0.5 B to 236 B total parameters. Details of all models are presented in Table 6.</p>
<h3>2.2 Benchmarks</h3>
<p>We do a comprehensive analysis using 13 large-scale well-established NLP benchmarks: AGIEval (Zhong et al., 2023), AI2 Reasoning Challenge (ARC-C) (Clark et al., 2018), BIG Bench (Hard) (Srivastava et al., 2022; Suzgun et al., 2022), COPA (Roemmele et al., 2011), GSM8k (Cobbe et al., 2021), Hellaswag (Zellers et al., 2019), HumanEval (Chen et al., 2021), MATH (Hendrycks et al., 2021), MMLU (Hendrycks et al., 2020), Natural Questions (Kwiatkowski et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and TriviaQA (Joshi et al., 2017).
These benchmarks are a mix of choice- and generation-based benchmarks, that span various capabilities ranging from general knowledge to coding.</p>
<h1>3 How much variance do we observe?</h1>
<p>We first investigate how much variance there is across different models and datasets. We define a range of metrics for quantifying different kinds of variance.
First, using the 7B models we trained ourselves, we consider variance due to changes in seed, across otherwise identical setups. This seed variance gives us a metric useful for performing data ablations - to conclude that pretraining dataset or hyperparameter set B is better than pretraining dataset or hyperparameter set A , we would want the performance increase to be larger than that due to random seed variance across different models trained in setup A. To this end, we also compute a dataset's monotonicity, quantifying how stably it develops during training.
To ground the seed variance numbers, we compare them with bootstrapped $95 \%$ confidence intervals on individual models, as well as observed variance across different setups. In all our experiments, we consider both the (discrete) metric preferred for the benchmark and a more continuous representations for the same task.</p>
<h3>3.1 Analysis Methodology</h3>
<p>For our initial variance analysis, we use both benchmark-level scores (to compute variance and monotonicity) and sample level scores (to estimate $95 \%$ confidence intervals). Here, we provide a brief description of the metrics we compute.</p>
<p>Seed Mean $(\mu(\mathcal{S}, \mathbb{M})) \quad$ We compute the performance using metric $\mathcal{S}$ of the final checkpoint (at 210B tokens) of each of the 10 "fully trained" models in $\mathbb{M}$ (one for each seed).</p>
<p>Seed variance $(\mathbb{E}(\mathcal{S}, \mathbb{M})) \quad$ Given a benchmark, a preferred metric $\mathcal{S}$, and a set of models $\mathbb{M}=$ $\left{\mathrm{M}<em 2="2">{1}, \mathrm{M}</em>}, \ldots \mathrm{M<em _mathbb_M="\mathbb{M">{n}\right}$, we define the benchmark seed variance $\mathbb{E}(\mathcal{S}, \mathbb{M})$ as the standard deviation of the metric $\mathcal{S}$ scores $\left{\mathbb{S}</em>}}=\mathcal{S<em 1="1">{M</em>}}, \mathcal{S<em 2="2">{\mathrm{M}</em>}} \ldots \mathcal{S<em n="n">{\mathrm{M}</em>$.
To estimate the variance expected due only to random seed changes, we take the average of this metric over all checkpoint timesteps $\mathbb{E}(\mathcal{S}, \mathbb{M})=\frac{1}{21} \sum_{\text {time }={10.210 B}} \mathbb{E}\left(\mathcal{S}, \mathbb{M}^{(\text {time })}\right)$, where for example $\mathbb{E}\left(\mathcal{S}, \mathbb{M}^{(\text {time })}\right)$ corresponds to the standard deviation of performance of the 10 model checkpoints (across seeds) after 200B tokens of training. For each benchmark, we consider both a discrete and a continuous metric. ${ }^{1}$ The benchmark details are provided in Table 5 of Appendix A.}}\right}$ for each of the models in $\mathbb{M</p>
<p>Confidence intervals ( $95 \%$ CI) We use the bootstrapped library ${ }^{2}$ to compute $95 \%$ bootstrapped confidence interval (CI) values for each of the benchmarks on all 210 checkpoints from our 10 random seeded pretraining runs. Since bootstrapping is expensive, we also compute analytic interval (for discrete metrics) using the formula:</p>
<p>$$
C I_{\text {analytic }}(\mathrm{M})=1.96 * \sqrt{\frac{\mathcal{S}<em _mathrm_M="\mathrm{M">{\mathrm{M}} \times\left(1-\mathcal{S}</em>
$$}}\right)}{N}</p>
<p>where $\mathcal{S}_{\mathrm{M}}$ is the obtained preferred metric score for model M on a given benchmark and $N$ is the number of test instances present in that benchmark. Empirically, we observe that, for the distributions we consider, bootstrapped and Analytic CIs converge when the number of bootstrap samples is large.</p>
<p>Monotonicity values (mon $\mathbf{m o n}<em _cont="{cont" _text="\text">{\text {disc }} / \mathbf{m o n}</em>}}$ ) We compute the extent to which the scores for a benchmark develop monotonically during training. We define monotonicity for seed $i$ as the Kendall Rank correlation between the list of scores $\left[\mathcal{S<em _mathrm_M="\mathrm{M">{\mathrm{M}^{10 B}}, \mathcal{S}</em><em _mathrm_M="\mathrm{M">{i}^{20 B}}, \ldots, \mathcal{S}</em>\right]$ and a monotonically increasing or decreasing array of the same length, for discrete and continuous metrics, respectively.}_{i}^{210 B}</p>
<h3>3.2 Results</h3>
<p>In this section, we present our comprehensive analysis for two scenarios.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Variance values on 7B seed models. Benchmarks are listed in alphabetical order. We report means - $\mu(\mathcal{S}, \mathbb{M})$, standard deviations - $\mathbb{E}(\mathcal{S}, \mathbb{M})$, confidence intervals - $95 \%$ CI, and monotonicities mon ${ }<em _cont="{cont" _text="\text">{\text {disc }}$, mon $</em>$. We also observe that mon $}}$. We also report size and chance level performance for reference-note all generative tasks have a chance level performance of $0 . \mathbb{E}(\mathcal{S}, \mathbb{M})$ is generally lower than $95 \% \mathrm{CI<em _disc="{disc" _text="\text">{\text {cont }}&gt;\operatorname{mon}</em>$ for all benchmarks.}</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Chance</th>
<th style="text-align: center;">$\mu(\mathcal{S}, \mathbb{M})$</th>
<th style="text-align: center;">$\mathbb{E}(\mathcal{S}, \mathbb{M})$</th>
<th style="text-align: center;">$95 \%$ CI</th>
<th style="text-align: center;">mon $_{\text {disc }}$</th>
<th style="text-align: center;">mon $_{\text {cont }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AGIEval</td>
<td style="text-align: center;">2546</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">23.44</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: center;">ARC-C ${ }^{3}$</td>
<td style="text-align: center;">1165</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">39.71</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">2.74</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: center;">Big Bench (Hard)</td>
<td style="text-align: center;">6511</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">29.10</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">78.80</td>
<td style="text-align: center;">2.15</td>
<td style="text-align: center;">8.30</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;">GSM8k</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4.10</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;">Hellaswag</td>
<td style="text-align: center;">10042</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">70.08</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">11.89</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">3.98</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">14042</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25.86</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: center;">MMLU-Cloze</td>
<td style="text-align: center;">14042</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">37.47</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: center;">Natural Questions</td>
<td style="text-align: center;">3610</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16.43</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PIQA</td>
<td style="text-align: center;">1838</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">76.93</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">1.99</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: center;">SIQA</td>
<td style="text-align: center;">1954</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">46.69</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">2.21</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: center;">TriviaQA</td>
<td style="text-align: center;">11313</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">42.69</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: 7B seed models. Comparison between discrete (Disc) and continuous (Cont) metrics along with the signal to noise ratio (SNR). The means - $\mu(\mathcal{S}=\operatorname{Disc}, \mathbb{M}), \mu(\mathcal{S}=\operatorname{Cont}, \mathbb{M})$ and standard deviations (Disc Std, Cont Std) reported here (and used to calculate SNR) are computed across the final checkpoints across the 10 seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">$\mu(\mathcal{S}=\mathbf{D i s c}, \mathbb{M})$</th>
<th style="text-align: center;">Disc Std</th>
<th style="text-align: center;">Disc SNR</th>
<th style="text-align: center;">$\mu(\mathcal{S}=\mathbf{C o n t}, \mathbb{M})$</th>
<th style="text-align: center;">Cont Std</th>
<th style="text-align: center;">Cont SNR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AGIEval</td>
<td style="text-align: center;">23.44</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">25.20</td>
<td style="text-align: center;">0.2267</td>
<td style="text-align: center;">0.0009</td>
<td style="text-align: center;">254.93</td>
</tr>
<tr>
<td style="text-align: center;">ARC-C</td>
<td style="text-align: center;">39.71</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">45.89</td>
<td style="text-align: center;">0.2684</td>
<td style="text-align: center;">0.0007</td>
<td style="text-align: center;">381.64</td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">78.80</td>
<td style="text-align: center;">2.04</td>
<td style="text-align: center;">38.63</td>
<td style="text-align: center;">0.5376</td>
<td style="text-align: center;">0.0008</td>
<td style="text-align: center;">662.41</td>
</tr>
<tr>
<td style="text-align: center;">GSM8k</td>
<td style="text-align: center;">4.10</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">7.88</td>
<td style="text-align: center;">0.9948</td>
<td style="text-align: center;">0.0653</td>
<td style="text-align: center;">15.24</td>
</tr>
<tr>
<td style="text-align: center;">Hellaswag</td>
<td style="text-align: center;">70.08</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">608.23</td>
<td style="text-align: center;">0.2833</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">1921.15</td>
</tr>
<tr>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">11.89</td>
<td style="text-align: center;">1.75</td>
<td style="text-align: center;">6.79</td>
<td style="text-align: center;">0.2186</td>
<td style="text-align: center;">0.0018</td>
<td style="text-align: center;">124.08</td>
</tr>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">25.86</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">52.45</td>
<td style="text-align: center;">0.2511</td>
<td style="text-align: center;">0.0007</td>
<td style="text-align: center;">347.57</td>
</tr>
<tr>
<td style="text-align: center;">MMLU-Cloze</td>
<td style="text-align: center;">37.47</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">302.73</td>
<td style="text-align: center;">0.2698</td>
<td style="text-align: center;">0.0004</td>
<td style="text-align: center;">678.42</td>
</tr>
<tr>
<td style="text-align: center;">PIQA</td>
<td style="text-align: center;">76.93</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">198.98</td>
<td style="text-align: center;">0.5168</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">1641.14</td>
</tr>
<tr>
<td style="text-align: center;">SIQA</td>
<td style="text-align: center;">46.69</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">91.87</td>
<td style="text-align: center;">0.3656</td>
<td style="text-align: center;">0.0009</td>
<td style="text-align: center;">387.11</td>
</tr>
</tbody>
</table>
<p>Seed variance In Table 1, we report the observed variance across our 7B seed models in which the training setup is same across all init seeds, including a deterministic data ordering. We contextualise these numbers with the per-model $95 \%$ confidence interval, reported in the form of an average of 210 (one for each model) confidence interval sizes. The latter is easily computable from a single training run, whereas the former requires multiple (expensive) training runs with different seeds.
For some benchmarks (e.g. AGIEval, MMLU), scores are around chance accuracy ( $\sim 25 \%$ ) even after training for 210B tokens. Benchmarks with few test examples (like COPA and HumanEval) exhibit high variance (both seed variance and $95 \%$ CIs). Generally, the 7B seed variance is well below the $95 \%$ CI for the same benchmark, though the ratio of the two is quite variable. Having access to the former value, which is smaller but closer to what would be needed to, for instance, compare two data mixes, may allow practitioners to make more fine-grained decisions during model development.
Motivated by prior work which suggests a move to continuous metrics (Srivastava et al., 2022; Schaeffer et al., 2023; Du et al., 2024; Schaeffer et al., 2024), we show a comparison of discrete and continuous metrics along with their signal to noise ratios (SNR) in Table 2. To maintain consistency, we used probability mass of the predicted answer for all choice-based benchmarks and NLL of the correct answer for generation-based benchmarks; more details are provided in Appendix A. We observe that the SNR is considerably higher for continuous metrics for all benchmarks, suggesting that they may be better when comparing models in the sense that they are less confounded by noise. These results may thus help in building better scaling laws for downstream evaluation tasks (Achiam et al., 2023), along with accurate comparisons between two models that have performances lying within the confidence interval for the discrete metric.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Development of model performance over time. Boxplots for both discrete and continous metrics depicting the model improvement over time for ARC-C, GSM8k, and HumanEval. Top row depicts discrete metrics for each of the benchmarks, and the bottom row is composed of the continuous metrics.</p>
<p>Monotonicity In Table 1, we list the monotonicity values for each of the continuous and discrete metrics listed in Table 5. Higher monotonicity values are indicative of evaluations that more stably represent model improvement. In almost all cases, the mononicity is better for the continuous metrics than for the discrete metrics, mirroring our findings with SNR above. However, for some benchmarks, such as HellaSwag and TriviaQA, the difference is minimal, likely since these benchmarks saturate earlier in training. Likewise, for benchmarks where performance remains at chance level we observe very low monotonicities.</p>
<p>In Figure 1, we visualise the development of discrete and continuous metrics and their seed variance during training, for ARC-C, GSM8k, and HumanEval. Generally (with the exception of GSM8k), continuous metrics have better predictive scaling compared to the discrete metrics because they have higher monotonicity and SNR. Interestingly, we see that the variance remains relatively constant as performance increases, suggesting that the estimates may extrapolate well to models trained for longer. Overall, these results suggest that monitoring continuous metrics could be more fruitful during model development than tracking discrete metrics.</p>
<h3>3.3 The curious case of MMLU</h3>
<p>Motivated by prior work considering the inconsistency of multiple choice benchmarks (Wang et al., 2024; Alzahrani et al., 2024), we examined two formulations of MMLU: (Standard) MMLU and MMLU-Cloze.</p>
<p>Standard MMLU refers to the prompting format where the choices along with the choice texts are present for the few-shot examples as well as the question in the prompt text. To evaluate the sample, we append the choice letters ("A", "B", "C", or "D") at the end of the prompt text, and pick the choice that has the lowest negative log-likelihood (NLL). For MMLU-Cloze, just the correct choice's text is present for the few-shots, and we pick the choice that gives the lowest NLL after appending the choice texts at the end of the prompt. The complete prompts used for the two cases are detailed in Appendix B.</p>
<p>In Figure 2, we plot performance over training and see that standard MMLU is at chance performance even after training on 210B tokens. The cloze formulation performs better, and importantly has lower seed variance and much higher monotonicity (0.95 instead of 0.09, see Table 1). This result seems surprising, given that the cloze format is not standard. Further investigation yields that fully-trained large models tend to have better performance on standard MMLU compared to MMLU-cloze (e.g.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Development of model performance over time. In $(a)$, we show the boxplots for the two MMLU variants. The top row is for the discrete metric (accuracy) and bottom row for the continuous metric (probability mass of the correct answer). In (b), we show the comparison of the standard (choice) and cloze variants on a Llama-2 13B model trained from scratch.
$78.7 \%$ on standard MMLU vs. $60.6 \%$ for MMLU-Cloze for LLaMa 3 70B). Despite this difference in absolute performance, we find the performance on standard and cloze formats is highly correlated for fully trained large models (Pearson correlation of 0.92 on the 70 models listed in § 2.1).
To understand this dichotomy better, we train a Llama-2-13B-like model from scratch on our pretraining mix. We observe a sudden jump in performance at around 800B tokens (for both discrete and continuous metrics), after which standard MMLU performs better than MMLU-cloze (see Figure 2).
Given these results, we encourage researchers to use cloze formulations when doing pretraining ablations, as they are less confounded by noise during early stages of training, but still seem predictive of final performance on the standard MMLU format.</p>
<h1>4 Understanding variance through the lens of item analysis</h1>
<p>In the previous section, we computed the empirically occurring variances for commonly used evaluation benchmarks, considering benchmark-level scores, and we showed how looking at continuous metrics or cloze formulations of tasks can boost SNR.
As another avenue of possibly reducing variance, and to better understand it, we take inspiration from item analysis, a common method used to assess the usefulness of individual test questions on standardised tests administered to humans (Livingston, 2011; University of Washington, 2024). Item analysis focuses on metrics of individual samples (e.g. difficulty) to understand the types of questions on tests in terms of how individuals (in our case, models) perform on them.</p>
<h3>4.1 Method</h3>
<p>In applying item analysis to benchmarks, we consider two metrics. Item difficulty refers to the average score on an item across models; Item discrimination refers to the correlation between models' performances on a single data point and models' overall performances. Intuitively, items with either high or low difficulty will have low discrimination (as all models will be wrong or right, respectively).
As we wish to make recommendations about evaluation datasets that extend to future models, we split our 70 models into train and test sets. We consider two splits: "random" and "difficulty". As the name suggests, in the random split, we split models randomly; In the difficulty split, we hold out the best performing 14 models. The full lists of models in each split can be found in Appendix D.1. We then calculate item analysis metrics on individual data points for the train and test sets. As is often done with human testing, we also consider the use of removing data points with low item discrimination,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Item analysis results on GSM8k and ARC-C. Results on additional benchmarks provided in Appendix D.2. First column shows a scatter plot of item difficulty (x-axis) vs item discrimination (y-axis). Second column shows a scatter plot of item discrimination calculated over models from the train or test set of the difficulty split. Third column is the same as the second, except on the random split. As expected (since train and test splits come from the same distribution), discrimination on train models for this split is positively correlated to discrimination on test models. Fourth, fifth, and sixth columns show the effects of iteratively removing up to $20 \%$ of items (based on discrimination) on the mean (fourth column), standard error (fifth column) of model performance on the test set from the difficulty split by looking at the delta. Error bars indicate $95 \%$ confidence intervals in the delta. Monotonicity (sixth column) is calculated over the 10 runs from $\S$ 2.1. Orange curves show effects from randomly removing points, as a baseline.
and observe the effects this has on evaluation metrics such as mean, standard error of the mean (std. err.), ${ }^{4}$ and monotonicity.</p>
<h1>4.2 Results</h1>
<p>In Figure 3, we show results for two illustrative benchmarks: ARC-C and GSM8k. Full results across other benchmarks can be found in Appendix D.2. Overall, we find that item discrimination scores may not provide much useful signal for the field of language model evaluations (unlike their widespread usage in human standardised testing). This is especially true given that state-of-theart models perform better and better, and we would like tests to stay informative when models improve. To illustrate this, we show how high discrimination on train (weaker) models often does not correspond to high discrimination on test (stronger) models (Figure 3, second column). Striping around $x=0$ corresponds to items that train set models always get wrong (yielding 0 discrimination) but are informative on test set models. Similarly, striping around $y=0$ corresponds to items that test models always get right (yielding 0 discrimination) but are informative on the train set. If we instead consider item discriminations on a random split of models (Figure 3, third column), we see stronger correlations, indicating that the low correlation is in fact due to the difference in item discrimination on weaker and stronger models.
In Appendix D.3, we qualitatively inspect examples with negative item discrimination (which are thus anti-correlated with overall model performance), but are not able to discern any clear patterns for most benchmarks (a notable exception being Hellaswag, see Figure 8). While these negative results suggest item discriminations may not be the most informative means of understanding (or reducing) variance on stronger models, we consider further application to explore the causal effect.
Specifically, we consider pruning data points with low item discrimination, with the hopes that this will reduce variance or improve monotonicity. More precisely, we prune data points with low item discrimination on the train set of models from the difficulty split and we visualise metrics calculated using the pruned subset on the test set of models from the difficulty split. Results are presented in the three rightmost columns of Figure 3. Overall, while we find modest improvements in both standard error (a decrease) and monotonicity (an increase), the drift in the estimated accuracy is mildly concerning. It may be acceptable for the purpose of comparing models, but may also provide an overestimate of capabilities if considering the absolute score. One hypothesis for this discrepancy with human testing could be that item discrimination for human tests typically does not consider</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Variance values for Tiny Benchmark (across seeds). Full represents the full benchmark, and IRT/IRT++ use the 100 examples proposed in Polo et al. (2024). $\mathbb{E}(\mathcal{S}, \mathbb{M})$ is the seed variance defined in $\S 3.1$, which is represented as $\mathbb{E}$ in this table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Full $\mu$</th>
<th style="text-align: center;">IRT $\mu$</th>
<th style="text-align: center;">IRT++ $\mu$</th>
<th style="text-align: center;">Full $\mathbb{E}$</th>
<th style="text-align: center;">IRT $\mathbb{E}$</th>
<th style="text-align: center;">IRT++ $\mathbb{E}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ARC-C</td>
<td style="text-align: center;">39.71</td>
<td style="text-align: center;">46.21</td>
<td style="text-align: center;">42.32</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">1.80</td>
<td style="text-align: center;">1.86</td>
</tr>
<tr>
<td style="text-align: center;">GSM8k</td>
<td style="text-align: center;">4.10</td>
<td style="text-align: center;">3.21</td>
<td style="text-align: center;">4.62</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">1.49</td>
</tr>
<tr>
<td style="text-align: center;">Hellaswag</td>
<td style="text-align: center;">70.08</td>
<td style="text-align: center;">71.80</td>
<td style="text-align: center;">68.81</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">2.06</td>
<td style="text-align: center;">2.42</td>
</tr>
</tbody>
</table>
<p>Table 4: Monotonicity values for Tiny Benchmark. We list the monotonicity values for both discrete ( $\operatorname{mon}<em _cont="{cont" _text="\text">{\text {disc }}$ ) and continuous ( $\operatorname{mon}</em>$ ) metrics for the 7B seed models from $\S 3.2$. Full represents the full benchmark, and IRT/IRT++ use the 100 examples proposed in Polo et al. (2024).}</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">mon $\mathbf{m o n}_{\text {disc }}$ (Full/IRT/IRT++)</th>
<th style="text-align: center;">mon $_{\text {cont }}$ (Full/IRT/IRT++)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ARC-C</td>
<td style="text-align: center;">$0.88 / 0.64 / 0.63$</td>
<td style="text-align: center;">$0.91 / 0.78 / 0.82$</td>
</tr>
<tr>
<td style="text-align: center;">GSM8k</td>
<td style="text-align: center;">$0.74 / 0.32 / 0.30$</td>
<td style="text-align: center;">$0.30 / 0.24 / 0.24$</td>
</tr>
<tr>
<td style="text-align: center;">Hellaswag</td>
<td style="text-align: center;">$0.99 / 0.84 / 0.80$</td>
<td style="text-align: center;">$0.99 / 0.93 / 0.94$</td>
</tr>
</tbody>
</table>
<p>out-of-distribution splits - it takes into account the entire spectrum of scores. However, even beyond the difficulty split, we similarly find little-to-no benefits on the random split (see Figure 7). As a result, we overall would not suggest the use of item analysis-based methods for understanding variance in language model evaluations, though the underlying cause for this mismatch remains an open question for future work.</p>
<h1>5 The false promise of item response theory for LLMs</h1>
<p>In a similar category to item analysis, item response theory (Cai et al., 2016; van der Linden, 2018; Brzezińska, 2020; Lord and Novick, 1968) describes a set of statistical models used to analyse human abilities on standardised test data. In the recent past, the method has become popular as a means of understanding model performance on a set of evaluation samples (Lalor et al., 2016; Vania et al., 2021; Rodriguez et al., 2021). Most recently, Polo et al. (2024) used IRT to cluster evaluation points with the aims of reducing eval benchmark size (and thus, cost of running).</p>
<p>Following our mixed findings applying item analysis, we apply the IRT method from Polo et al. (2024) to our models and the overlapping set of evaluation benchmarks. For a brief summary of the IRT method, we refer to Appendix E.1. Specifically, we go beyond the comparisons drawn in prior work and consider how our defined variance metrics (§ 3) change under this model. We believe the application to evaluating intermediate checkpoints during pretraining is especially relevant, as that's the application where smaller evaluation datasets could have the most efficiency gains (as opposed to one-time evaluations of larger models).</p>
<p>In Tables 3 and 4, we report various metrics on the discrete performance measure for GSM8k, Hellaswag, and ARC-C. We find that simply using the performance on the 100 datapoints selected by Polo et al. (2024) for each benchmark can lead to quite large deviations in the mean (an overestimation by $7 \%$ for ARC-C). The full IRT++ method obtains less deviation, replicating prior findings (Polo et al., 2024). However, both methods suffer from greatly increased seed variance (final two columns, Table 3), indicating that the tiny-benchmarks method may have limited use during pretraining ablations as it makes model comparisons more likely to be confounded by randomness from the initialisation and data ordering seed. This increased variance is also reflected in the monotonicity metrics - we see a decrease in monotonicity in Table 4, indicating that performance oscillates more during training (see Figure 9).</p>
<p>Beyond the smaller scale models, we also considered the use of tiny-benchmarks for evaluating larger models, like the ones used for item analysis in Section 4. In Figure 4, we find that IRT-based methods generalise relatively well when it comes to the average performance metric (with the IRT++ estimator performing better), but have much larger standard error of the mean. This increased error cautions against the use of IRT-based subsets for model evaluations that will be used to compare different models. To quantify how this increased standard error of the mean may affect model rankings, we also compute the Kendall rank correlation on our 70 models using the performance estimate obtained</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Tiny Benchmarks Means and Standard Errors of the mean (proportional to 95\% CI).
from using the full dataset, as well as the IRT and IRT++ methods. In Table 7, we find that the correlation can drop as low as 0.76 , corresponding to $12 \%$ of model pairwise comparisons giving the opposite result when using the IRT or IRT++ method (versus the full dataset mean estimate). Furthermore, we find that the number of flips is relatively higher on models that perform better, suggesting that IRT-based methods may not scale well (similar to item analysis). These findings reinforce the promise of IRT-based methods for a point estimate of the mean (relatively low error, Figure 4), but caution against the use of IRT-based methods when comparing models due to the increased variance of the estimate.</p>
<h1>6 Related work</h1>
<p>While a significant body of work exists proposing natural language processing (NLP) benchmarks to evaluate the capabilities of models, there is comparatively less work studying the benchmarks themselves. Before the era of chat large language models, Marie et al. (2021) conducted a large scale meta-analysis of 769 research papers published from 2010 to 2020 and identified troubling trends, including one that partially motivates our work: models are frequently declared superior to competitors based on small differences in performance scores, without proper hypothesis testing that takes into account natural fluctuations in benchmark scores. Spiritually similar claims were made by Dehghani et al. (2021) in their provocatively titled paper "The Benchmark Lottery". Kocmi et al. (2021) further leveraged large-scale human experiments to evaluate benchmarks with automated metrics and concluded that commonly used metrics such as BLEU score had led to poor deployment decisions. Their conclusion was echoed by a meta-analysis of 3500 NLP benchmark scores published on Papers with Code (Blagec et al., 2022).
More recently, with accelerating progress in NLP, researchers have begun to study benchmarks in earnest to understand their properties and limitations (Gehrmann et al., 2023). Von Werra et al. (2022) proposed a framework to evaluate benchmarks themselves and provided a mechanism for researchers to share their benchmarking analyses. Certain papers have studied specific aspects of benchmarks, focusing on the sensitivity of language models to various factors. Sclar et al. (2023) tested how sensitive language models are to differently formatted prompts, while Wang et al. (2024) and Alzahrani et al. (2024) find that models are inconsistent across changes in the format of MultipleChoice Question Answering (MCQA) benchmarks. Our work builds on these works by focusing on the inherent variance in benchmarks (e.g. due to model seed) that pracitioners should consider when making decisions, and suggesting minor modifications (e.g. in how a task is scored or formulated) that can reduce this variance.
With the aims of improving efficiency in model development cycles, recent work proposes reducing the size of evaluation benchmarks by picking representative samples (Vivek et al., 2023; Polo et al., 2024). Polo et al. (2024) show that methods from human standardised testing (specifically, item</p>
<p>response theory; Lord and Novick, 1968) can be combined with clustering to subselect evaluation benchmarks without incurring too much deviation from the mean. However, they do not consider the increased variance from their method nor how small deviations in means can compound when comparing multiple models. We go beyond their work by considering the use of additional methods from human standardised testing literature (item analysis; Livingston, 2011), as well as showing that such methods generally do not meaningfully reduce variance.</p>
<p>Perhaps most similar to ours is the work of Xiang et al. (2022), who study different sources of variance in NLP benchmarks and offers cautionary advice about when one should (not) be confident in benchmark scores. Their approach is limited to the machine translation setting; here we quantify and study variance in 13 different NLP benchmarks (covering general knowledge, reasoning, coding, and math) across 280 models, including many frontier LLMs.</p>
<h1>7 Conclusion</h1>
<p>As language models become more and more prevalent, it has become increasingly important to get a sense of their capabilities. One of the primary ways to assess these capabilities is through the use of evaluation benchmarks, where a model is scored on a series of examples. These scores are often directly compared, without consideration of the variance. This obscures the interpretation of evaluation results, in assessing final models as well as making decisions during model development. In this work, we aimed to quantify evaluation benchmark variance across a range of settings (from pretraining intermediate checkpoints, to the largest frontier LLMs) using a diverse set of metrics (seed variance, confidence intervals, and monotonicity). Beyond quantifying variance, we also experimented with various techniques used in human standardised testing (item analysis; University of Washington (2024), item response theory; Cai et al. (2016)), but generally found these methods to be ineffective on the models and benchmarks we considered, in terms of reducing variance. Future work could explore such avenues further, and it is possible that as models reach closer and closer to human-level performance these methods will provide more useful insights. On the other hand, in line with recent work advocating for a teleological approach to measuring capabilities (McCoy et al., 2023), we demonstrated LLM-specific techniques (e.g. the use of continuous metrics or clozeformatted tasks) can improve the signal-to-noise ratio in our evals. Such techniques are not available when assessing humans, but provide a unique opporutnity for LLM evaluations, especially when performing pretraining ablations. We hope our work spurs future work in this direction of reducing variance, in addition to serving as an empirical guide for model practitioners to use when comparing models and assessing performance.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md.</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.</p>
<p>Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, Nora Altwairesh, Areeb Alowisheq, et al. When benchmarks are targets: Revealing the sensitivity of large language model leaderboards. arXiv preprint arXiv:2402.01781, 2024.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng</p>
<p>Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.</p>
<p>Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.</p>
<p>Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. 2020.</p>
<p>Kathrin Blagec, Georg Dorffner, Milad Moradi, Simon Ott, and Matthias Samwald. A global analysis of metrics used for measuring performance in natural language processing, 2022.</p>
<p>Justyna Brzezińska. Item response theory models in the measurement theory. Communications in Statistics - Simulation and Computation, 49(12):3299-3313, 2020. doi: 10.1080/03610918.2018. 1546399. URL https://doi.org/10.1080/03610918.2018.1546399.</p>
<p>Li Cai, Kilchan Choi, Mark Hansen, and Lauren Harrell. Item response theory. Annual Review of Statistics and Its Application, 3(Volume 3, 2016):297-321, 2016. ISSN 2326-831X. doi: https://doi.org/10.1146/annurev-statistics-041715-033702. URL https://www.annualreviews. org/content/journals/10.1146/annurev-statistics-041715-033702.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Databricks. Dbrx technical blog. 2024. URL https://www.databricks.com/blog/ introducing-dbrx-new-state-art-open-llm.</p>
<p>DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.</p>
<p>Mostafa Dehghani, Yi Tay, Alexey A. Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler, and Oriol Vinyals. The benchmark lottery, 2021.</p>
<p>Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of language models from the loss perspective, 2024.</p>
<p>Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text. Journal of Artificial Intelligence Research, 77:103-166, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. 2021.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017.</p>
<p>Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation, 2021.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.</p>
<p>John P Lalor, Hao Wu, and Hong Yu. Building an evaluation scale using item response theory. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, volume 2016, page 648. NIH Public Access, 2016.</p>
<p>Samuel A Livingston. Item analysis. In Handbook of test development, pages 435-456. Routledge, 2011.
F. M. Lord and M. R. Novick. Statistical theories of mental test scores. Addison-Wesley, 1968.</p>
<p>Benjamin Marie, Atsushi Fujita, and Raphael Rubino. Scientific credibility of machine translation research: A meta-evaluation of 769 papers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 2021.
R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve, 2023.</p>
<p>Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Cristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, and et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.</p>
<p>MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.</p>
<p>Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer examples, 2024.</p>
<p>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.</p>
<p>Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P Lalor, Robin Jia, and Jordan Boyd-Graber. Evaluation examples are not equally informative: How should that change nlp leaderboards? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4486-4503, 2021.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.</p>
<p>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019.</p>
<p>Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage?, 2023.</p>
<p>Rylan Schaeffer, Hailey Schoelkopf, Brando Miranda, Gabriel Mukobi, Varun Madan, Adam Ibrahim, Herbie Bradley, Stella Biderman, and Sanmi Koyejo. Why has predicting downstream capabilities of frontier ai models with scale remained elusive?, 2024.</p>
<p>Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324, 2023.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>StabilityAI. Stablelm technical report. 2024. URL https://stability.wandb.io/stability-llm/stable-lm/ reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken= u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vpOwyfo.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>University of Washington. Understanding item analyses, 2024. URL https://www.washington. edu/assessment/scanning-scoring/scoring/reports/item-analysis/.</p>
<p>Wim J. van der Linden, editor. Handbook of Item Response Theory: Three Volume Set. CRC Press, 2018.</p>
<p>Clara Vania, Phu Mon Htut, William Huang, Dhara Mungra, Richard Yuanzhe Pang, Jason Phang, Haokun Liu, Kyunghyun Cho, and Samuel R Bowman. Comparing test sets with item response theory. arXiv preprint arXiv:2106.00840, 2021.</p>
<p>Rajan Vivek, Kawin Ethayarajh, Diyi Yang, and Douwe Kiela. Anchor points: Benchmarking models with much fewer examples, 2023.</p>
<p>Leandro Von Werra, Lewis Tunstall, Abhishek Thakur, Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani, Victor Mustar, and Helen Ngo. Evaluate \&amp; evaluation on the hub: Better best practices for data and model measurements. In Wanxiang Che and Ekaterina Shutova, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 128-136, Abu Dhabi, UAE, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-demos.13. URL https://aclanthology.org/2022.emnlp-demos.13.</p>
<p>Haochun Wang, Sendong Zhao, Zewen Qiang, Bing Qin, and Ting Liu. Beyond the answers: Reviewing the rationality of multiple choice question answering for the evaluation of large language models. arXiv preprint arXiv:2402.01349, 2024.</p>
<p>Lucas Weber, Elia Bruni, and Dieuwke Hupkes. Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning. arXiv preprint arXiv:2310.13486, 2023.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38-45, 2020.</p>
<p>Jiannan Xiang, Huayang Li, Yahui Liu, Lemao Liu, Guoping Huang, Defu Lian, and Shuming Shi. Investigating data variance in evaluations of automatic machine translation metrics. arXiv preprint arXiv:2203.15858, 2022.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.</p>
<p>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models, 2023.</p>
<h1>A Models and Benchmarks Details</h1>
<p>For pre-training the 7B Llama-2 like checkpoints, we use a pre-training mix of publicly available data. We apply filtering to remove documents containing a high amount of personal information. We use a learning rate of $3.0 \times 10^{4}$, sequence length of 4096 , and a batch size of $4.1 M$ tokens to train the 7B models for 50000 steps. We use 256 80GiB A100 GPUs for a single pre-training run for 50k steps on our internal cluster. We do 10 such runs with different seeds. Each step takes 4.3 seconds.</p>
<p>For running the evaluations, we use 8 GPUs for each evaluation job comprising multiple evaluation datasets in a single job. A single evaluation job takes on average takes 3.5 hours for 13 benchmarks.</p>
<p>In Table 5, we provide the discrete metric (preferred), the continuous metric, and the number of samples for each of the benchmarks we consider. We can choose any continuous metric like character NLL, raw NLL, probability mass, log of probabilities, etc. for the benchmarks, but to maintain consistency, we choose probability mass of the predicted answer for choice-based tasks and negative log likelihood (NLL) of the target answer for generation-based benchmarks. Choice-based benchmarks are evaluated by appending the possible option choice letters or choice texts and then choosing the option with the lowest NLL. Generation-based benchmarks involve free-form generation, where the answer is extracted from the model's response using various post-processing techniques.</p>
<p>Table 5: Benchmark Details Details of all benchmarks used in the paper alphabetically. Exact Match (EM) is computed for 1 generation (maj@1). Prob Mass is the probability mass of the predicted answer and Target NLL represents the NLL of the target answer. CoT represents chain of thought prompting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">License</th>
<th style="text-align: center;"># samples</th>
<th style="text-align: center;"># few-shot</th>
<th style="text-align: center;">Disc Metric</th>
<th style="text-align: center;">Cont Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AGIEval <br> (Zhong et al., 2023)</td>
<td style="text-align: center;">MIT</td>
<td style="text-align: center;">2546</td>
<td style="text-align: center;">$3-5$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Prob Mass</td>
</tr>
<tr>
<td style="text-align: center;">ARC-C <br> (Clark et al., 2018)</td>
<td style="text-align: center;">Apache 2.0</td>
<td style="text-align: center;">1165</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Prob Mass</td>
</tr>
<tr>
<td style="text-align: center;">Big Bench Hard <br> (Srivastava et al., 2022)</td>
<td style="text-align: center;">Apache 2.0</td>
<td style="text-align: center;">6511</td>
<td style="text-align: center;">3 (CoT)</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">COPA <br> (Roemmele et al., 2011)</td>
<td style="text-align: center;">BSD 2-Clause</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Prob Mass</td>
</tr>
<tr>
<td style="text-align: center;">GSM8k <br> (Cobbe et al., 2021)</td>
<td style="text-align: center;">MIT</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">8 (CoT)</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Target NLL</td>
</tr>
<tr>
<td style="text-align: center;">Hellaswag <br> Zellers et al. (2019)</td>
<td style="text-align: center;">MIT</td>
<td style="text-align: center;">10042</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Prob Mass</td>
</tr>
<tr>
<td style="text-align: center;">HumanEval <br> (Chen et al., 2021)</td>
<td style="text-align: center;">MIT</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Target NLL</td>
</tr>
<tr>
<td style="text-align: center;">MATH <br> (Hendrycks et al., 2021)</td>
<td style="text-align: center;">MIT</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">4 (CoT)</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MMLU <br> (Hendrycks et al., 2020)</td>
<td style="text-align: center;">MIT</td>
<td style="text-align: center;">14042</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Prob Mass</td>
</tr>
<tr>
<td style="text-align: center;">Natural Questions <br> (Kwiatkowski et al., 2019)</td>
<td style="text-align: center;">MIT</td>
<td style="text-align: center;">3610</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PIQA <br> (Bisk et al., 2020)</td>
<td style="text-align: center;">Academic Free</td>
<td style="text-align: center;">1838</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Prob Mass</td>
</tr>
<tr>
<td style="text-align: center;">SIQA <br> Sap et al. (2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1954</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Prob Mass</td>
</tr>
<tr>
<td style="text-align: center;">TriviaQA <br> Joshi et al. (2017)</td>
<td style="text-align: center;">Apache 2.0</td>
<td style="text-align: center;">11313</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 6: Model Details Details of all models in the paper categorized by model family along with the number of parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Family</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Model Sizes (# params)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Meta-Llama <br> (AI@Meta, 2024; Touvron et al., 2023b,a)</td>
<td style="text-align: center;">Llama-1, Llama-2, <br> Llama-3</td>
<td style="text-align: center;">$7-70$ B</td>
</tr>
<tr>
<td style="text-align: left;">Google <br> (Mesnard et al., 2024)</td>
<td style="text-align: center;">Gemma</td>
<td style="text-align: center;">$2-7$ B</td>
</tr>
<tr>
<td style="text-align: left;">Databricks <br> (Databricks, 2024)</td>
<td style="text-align: center;">DBRX-Base</td>
<td style="text-align: center;">132B</td>
</tr>
<tr>
<td style="text-align: left;">Mistral <br> (Jiang et al., 2023, 2024)</td>
<td style="text-align: center;">Mistral, Mixtral</td>
<td style="text-align: center;">$7-141$ B</td>
</tr>
<tr>
<td style="text-align: left;">Qwen <br> (Bai et al., 2023)</td>
<td style="text-align: center;">Qwen-1.5</td>
<td style="text-align: center;">$0.5-110$ B</td>
</tr>
<tr>
<td style="text-align: left;">EleutherAI <br> (Biderman et al., 2023)</td>
<td style="text-align: center;">Pythia</td>
<td style="text-align: center;">$1-12$ B</td>
</tr>
<tr>
<td style="text-align: left;">TII-UAE <br> (Almazrouei et al., 2023)</td>
<td style="text-align: center;">Falcon</td>
<td style="text-align: center;">$7-40$ B</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek <br> (Bi et al., 2024; DeepSeek-AI, 2024)</td>
<td style="text-align: center;">DeepSeek, DeepSeek-MoE, <br> DeepSeek-V2</td>
<td style="text-align: center;">$7-236$ B</td>
</tr>
<tr>
<td style="text-align: left;">StabilityAI <br> (StabilityAI, 2024)</td>
<td style="text-align: center;">StableLM</td>
<td style="text-align: center;">$1.6-7$ B</td>
</tr>
<tr>
<td style="text-align: left;">MosaicML <br> (MosaicML NLP Team, 2023)</td>
<td style="text-align: center;">MPT</td>
<td style="text-align: center;">$7-30$ B</td>
</tr>
</tbody>
</table>
<h1>B MMLU prompt formats</h1>
<p>We use the following prompt variations for the standard and cloze versions of MMLU. We list down the preamble and the shot formatting for both cases. The final question is formatted like the few shot examples without the gold choice letter or text.</p>
<h2>B. 1 MMLU</h2>
<p>Preamble:
The following are multiple choice questions (with answers) about <subject>.
Shot formatting:
<question>
A. <choice A text>
B. <choice B text>
C. <choice C text>
D. <choice D text></p>
<p>Answer: <gold choice letter></p>
<h2>B. 2 MMLU-cloze</h2>
<h2>Preamble:</h2>
<p>Shot formatting:
<question>
Answer: <gold choice text></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Development of model performance over time. Boxplots for both discrete and continous metrics depicting the model improvement over time for COPA, Hellaswag, PIQA, and SIQA. Top row depicts discrete metrics for each of the benchmarks, and the bottom row is composed of the continuous metrics.</p>
<h1>C Variance Analysis Additional Results</h1>
<p>In this section, we present additional results on model performance development for the remaining benchmarks - COPA, Hellaswag, PIQA, and SIQA (see Figure 5). This supplements the results presented in Figure 1 and Figure 2. We observe similar trends except for SIQA. The error bars for both discrete and continuous metrics are similar, however, the continuous metric plot has less number of outliers.</p>
<h2>D Item analysis additional results</h2>
<h2>D. 1 Splits</h2>
<p>We used 70 base models for the item analysis results. We provide the splits used below.
Difficulty split (train): LLaMa 3 8B, Mistral 7B, Qwen ${0.5,1.8,4}$ B, LLaMa 2 7B, LLaMa 2 13B, LLaMa 2 70B, DeepSeek 7B, DeepSeek MoE 16B, Falcon 7B, Falcon 40B, Gemma 2B, Gemma 7B, LLaMa $1{7,13,33,65}$ B, MPT 30B, Pythia ${1,1.4,2.8,6.9,12}$ B, StableLM ${3,7}$ B. In addition to these open source models, we use 30 internal checkpoints from LLaMa-architecture models we pre-trained on our interal data mix.
Difficulty split (test): LLaMa 3 70B, Mixtral 8x{7,22}B, Qwen $1.5{7,13,32,72,110}$ B, DBRX, DeepSeek 67B, and 4 internal held out models.
Random split (train): LLaMa $3{8,70}$ B, Mistral 7B, Mixtral 8x{7,22}B, Qwen $1.5{0.5,1.8,4,7$, 13, 32, 72}B, LLaMa 2 7B, LLaMa 2 13B, LLaMa 2 70B, DBRX, DeepSeek MoE 16B, DeepSeek 67B, Falcon 40B, Gemma 2B, Gemma 7B, LLaMa $1{7,33,65}$ B, MPT 30B, Pythia ${1,1.4,2.8$, $6.9,12}$ B, StableLM 3B. In addition to these open source models, we use 25 internal checkpoints from LLaMa-architecture models we pre-trained on our interal data mix.
Random split (test): DeepSeek 7B, Falcon 7B, Qwen 1.5 110B, LLaMa 1 13B, StableLM 7B, and 9 internal checkpoints.</p>
<h2>D. 2 Additional results</h2>
<p>We present results on additional benchmarks, in a similar format to Figure 3, in Figure 6. Furthermore, we provide extended results on the random split of models in Figure 7.</p>
<h2>D. 3 Inspection of samples with low item discrimination</h2>
<p>We provide the 3 items from GSM8k, ARC-C and Hellaswag with the lowest item discrimination.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Item analysis results on six additional benchmarks, in the same format as Figure 3.</p>
<h1>For GSM8k:</h1>
<h2>Question:</h2>
<p>Aaron and Vanessa were relay race partners on a running team. Aaron was able to run each mile twice as fast as Vanessa, but Vanessa was able to run twice as far as Aaron did. If Vanessa ran 4 miles and Aaron completed his part of the race in 16 minutes, how long in minutes did Vanessa take to complete her part?
Answer: 64
Item Discrimination: -0.264
Item Difficulty: 0.1</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Results on 8 benchmarks when removing points based on item discrimination on the random split. These plots are similar to the final 3 columns in Figure 3 and Figure 6. Specifically, we show the effects of iteratively removing up to $20 \%$ of items (based on discrimination) on the mean (first column), standard error (second column) of model performance on the test set from the random split by looking at the delta. Error bars indicate $95 \%$ confidence intervals in the delta. Monotonicity (sixth column) is calculated over the 10 runs from Section 2.1. Orange curves show effects from randomly removing points, as a baseline. As we can see, these plots look qualitatively similar to Figure 3 and Figure 6 indicating that the observed lack of benefit from pruning based on item discrimination is not simply due to using the difficulty split of models.</p>
<h1>Question:</h1>
<p>Suzie loves to chew fruit-flavored gum. She bought four packs of gum the last time she was at the store. She got two packs of her favorite flavor, strawberry. She paid $\$ 2$ for a pack of grape gum that she also liked. She wanted to try something new, so she paid half as much for a small pack of green apple gum. If she paid $\$ 7$ in all, how many dollars did each pack of strawberry gum cost?
Answer: 2
Item Discrimination: -0.198
Item Difficulty: 0.229</p>
<h2>Question:</h2>
<p>John brings his dog to the vet. His dog needs 2 vaccines, which are $\$ 20$ each, and a heartworm check. The heartworm check is $60 \%$ of his total bill. If he brought $\$ 125$ with him, how much does he leave with?
Answer: 25
Item Discrimination: -0.196
Item Difficulty: 0.057</p>
<p>For ARC-challenge (correct answer is italicized):</p>
<h1>Question:</h1>
<p>Wolves, which are top predators, were eliminated from Yellowstone National Park in the 1930s. In 1995, wolves were reintroduced into Yellowstone. During the period in which wolves were absent from Yellowstone, which most likely occurred?
A. an increase in competition for food resources among small prey
B. a greater opportunity for primary producers to flourish
C. an increase in the population of tertiary consumers
D. a greater balance of predator-prey relationships</p>
<p>Item Discrimination: -0.689
Item Difficulty: 0.2</p>
<h2>Question:</h2>
<p>Which of these traits is inherited but greatly influenced by the environment?
A. tongue rolling ability
B. athletic performance
C. language
D. color of eyes</p>
<p>Item Discrimination: -0.574
Item Difficulty: 0.443</p>
<h2>Question:</h2>
<p>Organisms interact in the flow of energy in an ecosystem. Carnivores and omnivores are classified as consumers. Which two organisms are also classified as consumers?
A. bacteria and fungi
B. fungi and scavengers
C. parasites and herbivores
D. decomposers and herbivores</p>
<p>Item Discrimination: -0.539
Item Difficulty: 0.071</p>
<h2>For Hellaswag:</h2>
<h2>Question:</h2>
<p>The sunburned man is taking his shirt off and laying it on the bed. His friends help him with cream on his sunburn. the woman
A. places orange-colored tissue paper onto the sunburn.
B. is helping him putting on sunscreen.
C. is getting massage by a man.
D. is sitting at the table eating.</p>
<p>Item Discrimination: -0.637
Item Difficulty: 0.057</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Scatter plots of two features correlated with item discrimination (calculated on the train set of models from the difficulty split). Low item discrimination tends to correspond to short prompts that do not contain '[header]' tags.</p>
<h1>Question:</h1>
<p>A person is seen playing an accordion on a busy street while many people walk around him and watch. the man
A. continue playing with others in the street and ends with him walking away.
B. continues to play the instrument and ends by stopping to laugh and smile at others.
C. continues to play behind a set of drums while people walk in and out of frame.
D. continues to play while looking out at people and pans back to the camera.</p>
<p>Item Discrimination: -0.551
Item Difficulty: 0.086</p>
<h2>Question:</h2>
<p>A female weight lifter bends at the knees. She lifts a barbell to her chest. she
A. then lifts it over her head before dropping it heavily to the ground.
B. lowers the barbell and stands, then sways.
C. lifts it over her head.
D. then lifts it over her head to her body.</p>
<p>Item Discrimination: -0.488
Item Difficulty: 0.229</p>
<p>Note that for Hellaswag, we did find some correlations to item discrimination in terms of features of the problems. Specifically, as shown in Figure 8, we found that items with low discrimination tend to feature shorter prompts and do not contain tags such as '[header]' in the prompt.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Note that the confidence intervals of $\S 3.1$ are 1.96 times the standard error.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1536 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1536</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1536</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-92c89db048fb825d2c81b086d7bd82ed230f685b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/92c89db048fb825d2c81b086d7bd82ed230f685b" target="_blank">gradSim: Differentiable simulation for system identification and visuomotor control</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> We consider the problem of estimating an object's physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present gradSim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph -- spanning from the dynamics and through the rendering process -- enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1536.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1536.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>∇Sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>∇Sim (Differentiable Simulation for System Identification and Visuomotor Control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified differentiable simulator that couples a differentiable physics engine (rigid bodies, tetrahedral FEM deformable solids, thin-shell cloth, simple fluids, compliant contact) with differentiable rendering (SoftRas / DIB-R) to enable backpropagation from pixels to physical parameters and to train visuomotor controllers using image-space supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>∇Sim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>End-to-end differentiable simulator combining: (1) a differentiable physics engine supporting rigid bodies (Newton-Euler), compliant contact (penalty-based with friction/damping), tetrahedral FEM hyperelastic solids (Neo-Hookean, per-element Lamé parameters), thin-shell cloth with bending and lift/drag, simple incompressible-fluid (smoke) and pendula; and (2) a differentiable renderer (SoftRas / DIB-R) that provides smooth, differentiable image formation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics (rigid/deformable/cloth) and basic fluid dynamics (smoke); image formation/rendering</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-to-high fidelity for dynamics (mesh-based FEM with Neo-Hookean constitutive model, per-element material parameters, compliant contact with friction and damping); medium-fidelity for rendering (differentiable rasterizers that approximate visibility by smoothing triangle edges, do not perform full photorealistic light transport).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes friction, elastic and damping contact terms, per-element Lamé parameters (λ, μ), per-element mass, bending stiffness for thin-shells, lift/drag model for cloth, semi-implicit Euler integration; rendering uses SoftRas/DIB-R (sigmoid-based smoothing of visibility, differentiable UV sampling for foreground); does not use full Monte-Carlo ray-traced global illumination in the default differentiable renderer (but photorealistic renderings were used as targets in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>MPC-style neural control policy (control-walker, control-fem, control-cloth) and parameter-estimation optimization pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small neural network controllers: either a simple single fully-connected layer with tanh (for 2D walker) or a 3-layer MLP-style architecture that maps phase-shifted sinusoidal inputs to per-element activations; training performed by gradient-based optimization (Adam / momentum) through the differentiable simulator. Parameter estimation uses gradient descent on pixelwise MSE between rendered and target images.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>System identification from video (estimate mass, friction, elasticity, per-element Lamé parameters, cloth velocities) and visuomotor control of deformable objects to reach an image-specified target pose.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Mass estimation (rigid objects): mean absolute error 2.36e-5 kg, absolute relative error 9.01e-5 (Table 1). Deformable objects (per-particle mass): relative MAE 0.048; Lamé parameters λ,μ relative MAE ~0.0054-0.0056 (Table 3). Cloth per-particle velocity relative MAE 0.026 (Table 3). Visuomotor control: ∇Sim solved control-walker within 3 gradient-descent iterations and solved control-fem/control-cloth using image supervision (took more iterations than a 3D-supervised baseline but achieved the visual goal); qualitative convergence shown in Fig. 6.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Target images rendered with a high-fidelity photorealistic renderer and test of robustness to dynamics-model mismatch (unmodeled friction/elasticity, wrong object modeling such as rigid-as-deformable or deformable-as-rigid).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>When target images were generated with a photorealistic renderer, mean relative absolute mass-estimation error increased to 0.1793 (Table 4) compared to a perfect-model scenario 0.1071; dynamics-model mismatches led to larger degradations (e.g., unmodeled friction 0.1860, unmodeled elasticity 0.2281, rigid-as-deformable 0.3462, deformable-as-rigid 0.4574; Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The paper directly compares dynamics-model fidelity (perfect model vs unmodeled friction/elasticity vs incorrect object-type assumptions) and rendering fidelity (differentiable rasterizer vs photorealistic render). Key result: errors due to imperfect dynamics modelling are substantially larger than those due to rendering mismatch. Specifically, mis-modeling object type (rigid-as-deformable or deformable-as-rigid) produced the largest increases in estimation error (up to rel. abs. est. ~0.3462 and 0.4574), whereas using photorealistic target images produced more modest increases (0.1793). Shading and texture in the differentiable renderer improved convergence speed.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors conclude accurate dynamics modelling is more crucial than photorealistic rendering for successful parameter estimation and visuomotor control from images; differentiable rendering (with shading and texture) speeds convergence but is less critical than correct dynamics. They explicitly note that imperfect dynamics (unmodeled contact/friction or wrong rigidity assumptions) has a more profound impact than imperfect rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported failure modes include severe degradation when a deformable object is modeled as rigid (''deformable-as-rigid''), making deformation parameters irrecoverable; PyBullet+REINFORCE (non-differentiable baseline) shows narrow convergence basins and sensitivity to simulation hyperparameters; diffphysics (3D-supervised) underperforms on cloth tasks due to COM ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'gradSim: Differentiable simulation for system identification and visuomotor control', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1536.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1536.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiffPhysics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiffPhysics (differentiable physics-only baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable-physics-only baseline used in the paper that uses the differentiable dynamics component but does not include differentiable rendering; requires precise 3D ground-truth states (3D supervision) for training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>DiffPhysics (differentiable physics-only)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A differentiable physics engine that outputs 3D state trajectories and is trained/supervised using precise 3D state labels (e.g., center-of-mass trajectories sampled at 30 FPS); it lacks an image-formation/differentiable-rendering pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics (rigid/deformable/cloth)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High fidelity in state-space (accurate, dense 3D state outputs and gradients) but does not model image formation; relies on full 3D supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Accurate dynamics and contact modelling sufficient to produce reliable 3D state trajectories; assumes access to precise 3D ground-truth at training frequency (30 FPS); does not model pixels or rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Privileged baseline controllers / parameter estimation pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same or comparable control network architectures and parameter-estimation optimizers, but trained with dense 3D-state supervision rather than image-based losses; e.g., minimizing MSE between predicted and target center-of-mass (COM).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>System identification and visuomotor control when dense 3D state supervision is available (parameter estimation, control via state-space rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Extremely low numerical errors when given dense 3D supervision: reported mass-estimation errors near numerical precision (e.g., mean abs error ~1.35e-9 kg and abs.rel ~5.17e-9 in Table 1), and deformable per-particle mass/material rel MAEs: 0.032 (mass), 0.0025 (λ), 0.0024 (μ) (Table 3). On control-fem it converged faster than the image-supervised ∇Sim baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Not used for cross-renderer transfer; serves as privileged, high-information baseline rather than a transfer target.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared 3D-supervised differentiable physics (DiffPhysics) vs image-supervised ∇Sim: DiffPhysics performs very well when dense 3D ground-truth is available and converges faster in control tasks, but for some problems (cloth) using only COM supervision is ambiguous and ∇Sim's image supervision can be preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Implicitly demonstrates that high-fidelity 3D state supervision yields strong performance, but the paper argues this is often infeasible in practice and that differentiable rendering + image supervision can achieve competitive performance without 3D labels.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>DiffPhysics (3D-com supervision) performs poorly on cloth tasks where COM is an ambiguous goal signal (leading to suboptimal solutions) and requires dense 3D ground-truth that is impractical for many real systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'gradSim: Differentiable simulation for system identification and visuomotor control', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1536.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1536.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet (+ REINFORCE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyBullet, a python module for physics simulation for games, robotics and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used non-differentiable physics engine used here as a black-box simulator baseline; combined with REINFORCE (black-box gradient estimator) to perform parameter estimation/control without analytic gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PyBullet, a python module for physics simulation for games, robotics and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Non-differentiable, real-time physics simulator for rigid-body dynamics and contact used in robotics and ML; used in the paper as a black-box baseline combined with REINFORCE for gradient estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics (rigid-body dynamics and contact)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Industry-grade real-time fidelity for rigid-body dynamics and contacts; medium-to-high fidelity dynamics but non-differentiable.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Simulates rigid dynamics, contacts, penalty and constraint-based collision handling; fast/real-time but gradients must be estimated with black-box methods (e.g., REINFORCE) which are sample-inefficient and noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>PyBullet + REINFORCE baseline (Ehsani et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box baseline using REINFORCE (policy gradient) to estimate gradients through non-differentiable simulator for parameter estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Parameter estimation from video-like observations (baseline) using black-box gradient estimators; used to test whether analytical differentiability is required.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Worse and more sensitive than differentiable approaches: reported mean absolute mass-estimation error 0.0926 kg (Table 1) and larger absolute relative errors; narrow convergence basins were observed in loss-landscape analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Used to generate ground-truth and as a baseline; not used for transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors observe that black-box gradient estimation with non-differentiable simulators is sensitive to simulator parameters, has narrow convergence regions and is less data-efficient than analytic differentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>PyBullet+REINFORCE exhibited narrow convergence regions and sensitivity leading to worse estimation performance compared to ∇Sim; multiple local minima and sample inefficiency were noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'gradSim: Differentiable simulation for system identification and visuomotor control', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1536.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1536.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoftRas</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft Rasterizer (SoftRas)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable rasterization-based renderer that replaces hard triangle edges with smooth sigmoids to produce differentiable image formation; used inside ∇Sim as one differentiable rendering option.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Soft rasterizer: A differentiable renderer for image-based 3d reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>SoftRas (differentiable renderer)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Differentiable rasterizer that smooths triangle edges using sigmoids to remove nondifferentiable visibility discontinuities, supports textured and shaded outputs for gradient-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>rendering / image formation (used to enable image-to-physics gradients)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-fidelity rendering: captures geometry, textures, and approximate shading cues, but intentionally smooths visibility to remain differentiable; does not model full physical light transport (no full ray-traced global illumination by default).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Smooths triangle edges into semi-transparent boundaries (sigmoid), supports texture sampling and shading approximations, differentiable UV sampling; trades exact visibility correctness for differentiability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Differentiable renderer used inside training pipeline (component)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Rasterization-based differentiable renderer module used to render predicted states to pixels for MSE loss against target images.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Enables backpropagation from pixel-wise losses to scene geometry and physical parameters, facilitating system identification and visuomotor policy training from images.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Contributes to smooth loss landscapes that allow successful gradient-based parameter estimation; including shading and texture in the differentiable renderer improved convergence speed (qualitative and shown in Fig. 7).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Compared against photorealistic renderer targets in robustness experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>When ∇Sim used differentiable renderers as training and targets were photorealistic, estimation error increased but remained usable (photorealistic target rel. abs. est. 0.1793 vs perfect-model 0.1071).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Differentiable rasterizers (SoftRas) with shading/textures improved convergence speed; however, mismatch to photorealistic rendering increases error modestly compared to dynamics-model mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Shading and texture cues in the differentiable renderer are beneficial for faster convergence, but perfect photorealism is not strictly required for reasonable parameter recovery; accurate dynamics remain more important.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Because SoftRas smooths visibility, it may not exactly match sharp visibility changes in photorealistic images, causing some degradation when transferring to photorealistic targets (observed modestly higher estimation errors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'gradSim: Differentiable simulation for system identification and visuomotor control', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1536.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1536.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIB-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DIB-R (Interpolation-based Differentiable Renderer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interpolation-based differentiable renderer that distinguishes foreground and background pixels and uses differentiable interpolation for foreground, used as an alternative differentiable rendering backend in ∇Sim.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to predict 3d objects with an interpolation-based differentiable renderer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>DIB-R (differentiable renderer)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Differentiable rendering approach that treats foreground pixels by differentiable bilinear sampling of textures and processes background pixels similarly to SoftRas; used to render differentiable images from mesh states in ∇Sim.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>rendering / image formation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-fidelity differentiable rendering (foreground/background treatment, differentiable texture sampling), not a full forward photorealistic light transport renderer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Differentiable bilinear texture sampling for foreground, sigmoid-smoothed treatment for background visibility; supports textures and basic shading; designed for gradient propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Differentiable rendering backend within ∇Sim</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Component that transforms mesh geometry, textures, and camera parameters into images with gradients usable by the simulator's optimization loop.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Enables gradient-based system identification and control by providing differentiable image outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Used interchangeably with SoftRas; contributes to smooth loss landscapes and successful parameter estimation/control from images; shading and texture improved convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Used in experiments where ∇Sim trained with DIB-R is evaluated against photorealistic-renderer targets.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Performance decreased modestly when target images were photorealistic (see ∇Sim transfer_performance), but DIB-R-enabled training still produced usable gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Acts similarly to SoftRas: differentiable rasterization with foreground interpolation yields stable gradients; photorealistic mismatch causes modest error increase, while dynamics mismatch causes larger errors.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Same conclusions as SoftRas: differentiable rendering with shading and texture helps convergence but is secondary to accurate dynamics modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Same as SoftRas — mismatch to high-fidelity photorealistic targets leads to modest degradation in estimation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'gradSim: Differentiable simulation for system identification and visuomotor control', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1536.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1536.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Photorealistic renderer (PBRT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physically Based Rendering (photorealistic renderer used as ground-truth)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-fidelity, non-differentiable photorealistic renderer (cited to Pharr et al., 2016) used to generate target images for robustness / transfer experiments to evaluate sensitivity to rendering mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physically Based Rendering: From Theory to Implementation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Photorealistic renderer (PBRT)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>High-fidelity forward renderer implementing physically based light transport (shading, textures, possibly multi-bounce lighting) used to synthesize target frames while ∇Sim uses differentiable rasterizers for training.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>rendering / image formation (used to test transfer from differentiable renderers to photorealistic images)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity photorealistic rendering (accurate shading, textures, and advanced light transport) but not differentiable in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes Phong-style shading, textures, and physically based rendering effects; produces realistic images used as ground-truth targets to evaluate the reality-gap caused by renderer mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Target image generator for transfer experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Non-differentiable photorealistic renderer used to create ground-truth images and object masks to test sensitivity of ∇Sim parameter estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Serves as the target domain to which ∇Sim-trained estimators/controllers must transfer when trained with differentiable rasterizers.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>When ∇Sim was trained with differentiable renderers and tested on photorealistic targets, estimation error increased: mean relative absolute estimation ~0.1793 (Table 4) versus perfect-model case 0.1071.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Represents a higher-fidelity target domain used to evaluate transfer from differentiable-renderer training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Quantified degradation: rel. abs. est. increased to 0.1793 for photorealistic targets (Table 4), indicating modest domain gap but not catastrophic failure.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Using photorealistic target images causes a modest increase in parameter estimation error relative to perfect matched differentiable-renderer training, but is less damaging than dynamics-model mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors note that photorealistic target images cause modest degradation and that shading/texture cues help; however, they emphasize that accurate dynamics modelling is a higher priority for transfer than perfect photorealistic rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Photorealistic rendering mismatch alone did not cause catastrophic failures but did slow convergence and increased estimation error compared to perfectly matched rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'gradSim: Differentiable simulation for system identification and visuomotor control', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Use the Force, Luke! learning to predict physical forces by simulating effects <em>(Rating: 2)</em></li>
                <li>DiffTaichi: Differentiable programming for physical simulation <em>(Rating: 2)</em></li>
                <li>ChainQueen: A real-time differentiable physical simulator for soft robotics <em>(Rating: 2)</em></li>
                <li>A differentiable physics engine for deep learning in robotics <em>(Rating: 2)</em></li>
                <li>Differentiable cloth simulation for inverse problems <em>(Rating: 2)</em></li>
                <li>PyBullet, a python module for physics simulation for games, robotics and machine learning <em>(Rating: 2)</em></li>
                <li>Physically Based Rendering: From Theory to Implementation <em>(Rating: 2)</em></li>
                <li>Tiny Differentiable Simulator <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1536",
    "paper_id": "paper-92c89db048fb825d2c81b086d7bd82ed230f685b",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "∇Sim",
            "name_full": "∇Sim (Differentiable Simulation for System Identification and Visuomotor Control)",
            "brief_description": "A unified differentiable simulator that couples a differentiable physics engine (rigid bodies, tetrahedral FEM deformable solids, thin-shell cloth, simple fluids, compliant contact) with differentiable rendering (SoftRas / DIB-R) to enable backpropagation from pixels to physical parameters and to train visuomotor controllers using image-space supervision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "∇Sim",
            "simulator_description": "End-to-end differentiable simulator combining: (1) a differentiable physics engine supporting rigid bodies (Newton-Euler), compliant contact (penalty-based with friction/damping), tetrahedral FEM hyperelastic solids (Neo-Hookean, per-element Lamé parameters), thin-shell cloth with bending and lift/drag, simple incompressible-fluid (smoke) and pendula; and (2) a differentiable renderer (SoftRas / DIB-R) that provides smooth, differentiable image formation.",
            "scientific_domain": "mechanics (rigid/deformable/cloth) and basic fluid dynamics (smoke); image formation/rendering",
            "fidelity_level": "Medium-to-high fidelity for dynamics (mesh-based FEM with Neo-Hookean constitutive model, per-element material parameters, compliant contact with friction and damping); medium-fidelity for rendering (differentiable rasterizers that approximate visibility by smoothing triangle edges, do not perform full photorealistic light transport).",
            "fidelity_characteristics": "Includes friction, elastic and damping contact terms, per-element Lamé parameters (λ, μ), per-element mass, bending stiffness for thin-shells, lift/drag model for cloth, semi-implicit Euler integration; rendering uses SoftRas/DIB-R (sigmoid-based smoothing of visibility, differentiable UV sampling for foreground); does not use full Monte-Carlo ray-traced global illumination in the default differentiable renderer (but photorealistic renderings were used as targets in experiments).",
            "model_or_agent_name": "MPC-style neural control policy (control-walker, control-fem, control-cloth) and parameter-estimation optimization pipelines",
            "model_description": "Small neural network controllers: either a simple single fully-connected layer with tanh (for 2D walker) or a 3-layer MLP-style architecture that maps phase-shifted sinusoidal inputs to per-element activations; training performed by gradient-based optimization (Adam / momentum) through the differentiable simulator. Parameter estimation uses gradient descent on pixelwise MSE between rendered and target images.",
            "reasoning_task": "System identification from video (estimate mass, friction, elasticity, per-element Lamé parameters, cloth velocities) and visuomotor control of deformable objects to reach an image-specified target pose.",
            "training_performance": "Mass estimation (rigid objects): mean absolute error 2.36e-5 kg, absolute relative error 9.01e-5 (Table 1). Deformable objects (per-particle mass): relative MAE 0.048; Lamé parameters λ,μ relative MAE ~0.0054-0.0056 (Table 3). Cloth per-particle velocity relative MAE 0.026 (Table 3). Visuomotor control: ∇Sim solved control-walker within 3 gradient-descent iterations and solved control-fem/control-cloth using image supervision (took more iterations than a 3D-supervised baseline but achieved the visual goal); qualitative convergence shown in Fig. 6.",
            "transfer_target": "Target images rendered with a high-fidelity photorealistic renderer and test of robustness to dynamics-model mismatch (unmodeled friction/elasticity, wrong object modeling such as rigid-as-deformable or deformable-as-rigid).",
            "transfer_performance": "When target images were generated with a photorealistic renderer, mean relative absolute mass-estimation error increased to 0.1793 (Table 4) compared to a perfect-model scenario 0.1071; dynamics-model mismatches led to larger degradations (e.g., unmodeled friction 0.1860, unmodeled elasticity 0.2281, rigid-as-deformable 0.3462, deformable-as-rigid 0.4574; Table 4).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "The paper directly compares dynamics-model fidelity (perfect model vs unmodeled friction/elasticity vs incorrect object-type assumptions) and rendering fidelity (differentiable rasterizer vs photorealistic render). Key result: errors due to imperfect dynamics modelling are substantially larger than those due to rendering mismatch. Specifically, mis-modeling object type (rigid-as-deformable or deformable-as-rigid) produced the largest increases in estimation error (up to rel. abs. est. ~0.3462 and 0.4574), whereas using photorealistic target images produced more modest increases (0.1793). Shading and texture in the differentiable renderer improved convergence speed.",
            "minimal_fidelity_discussion": "Authors conclude accurate dynamics modelling is more crucial than photorealistic rendering for successful parameter estimation and visuomotor control from images; differentiable rendering (with shading and texture) speeds convergence but is less critical than correct dynamics. They explicitly note that imperfect dynamics (unmodeled contact/friction or wrong rigidity assumptions) has a more profound impact than imperfect rendering.",
            "failure_cases": "Reported failure modes include severe degradation when a deformable object is modeled as rigid (''deformable-as-rigid''), making deformation parameters irrecoverable; PyBullet+REINFORCE (non-differentiable baseline) shows narrow convergence basins and sensitivity to simulation hyperparameters; diffphysics (3D-supervised) underperforms on cloth tasks due to COM ambiguity.",
            "uuid": "e1536.0",
            "source_info": {
                "paper_title": "gradSim: Differentiable simulation for system identification and visuomotor control",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "DiffPhysics",
            "name_full": "DiffPhysics (differentiable physics-only baseline)",
            "brief_description": "A differentiable-physics-only baseline used in the paper that uses the differentiable dynamics component but does not include differentiable rendering; requires precise 3D ground-truth states (3D supervision) for training and evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "DiffPhysics (differentiable physics-only)",
            "simulator_description": "A differentiable physics engine that outputs 3D state trajectories and is trained/supervised using precise 3D state labels (e.g., center-of-mass trajectories sampled at 30 FPS); it lacks an image-formation/differentiable-rendering pipeline.",
            "scientific_domain": "mechanics (rigid/deformable/cloth)",
            "fidelity_level": "High fidelity in state-space (accurate, dense 3D state outputs and gradients) but does not model image formation; relies on full 3D supervision.",
            "fidelity_characteristics": "Accurate dynamics and contact modelling sufficient to produce reliable 3D state trajectories; assumes access to precise 3D ground-truth at training frequency (30 FPS); does not model pixels or rendering.",
            "model_or_agent_name": "Privileged baseline controllers / parameter estimation pipelines",
            "model_description": "Same or comparable control network architectures and parameter-estimation optimizers, but trained with dense 3D-state supervision rather than image-based losses; e.g., minimizing MSE between predicted and target center-of-mass (COM).",
            "reasoning_task": "System identification and visuomotor control when dense 3D state supervision is available (parameter estimation, control via state-space rewards).",
            "training_performance": "Extremely low numerical errors when given dense 3D supervision: reported mass-estimation errors near numerical precision (e.g., mean abs error ~1.35e-9 kg and abs.rel ~5.17e-9 in Table 1), and deformable per-particle mass/material rel MAEs: 0.032 (mass), 0.0025 (λ), 0.0024 (μ) (Table 3). On control-fem it converged faster than the image-supervised ∇Sim baseline.",
            "transfer_target": "Not used for cross-renderer transfer; serves as privileged, high-information baseline rather than a transfer target.",
            "transfer_performance": null,
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Compared 3D-supervised differentiable physics (DiffPhysics) vs image-supervised ∇Sim: DiffPhysics performs very well when dense 3D ground-truth is available and converges faster in control tasks, but for some problems (cloth) using only COM supervision is ambiguous and ∇Sim's image supervision can be preferable.",
            "minimal_fidelity_discussion": "Implicitly demonstrates that high-fidelity 3D state supervision yields strong performance, but the paper argues this is often infeasible in practice and that differentiable rendering + image supervision can achieve competitive performance without 3D labels.",
            "failure_cases": "DiffPhysics (3D-com supervision) performs poorly on cloth tasks where COM is an ambiguous goal signal (leading to suboptimal solutions) and requires dense 3D ground-truth that is impractical for many real systems.",
            "uuid": "e1536.1",
            "source_info": {
                "paper_title": "gradSim: Differentiable simulation for system identification and visuomotor control",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "PyBullet (+ REINFORCE)",
            "name_full": "PyBullet, a python module for physics simulation for games, robotics and machine learning",
            "brief_description": "A widely used non-differentiable physics engine used here as a black-box simulator baseline; combined with REINFORCE (black-box gradient estimator) to perform parameter estimation/control without analytic gradients.",
            "citation_title": "PyBullet, a python module for physics simulation for games, robotics and machine learning",
            "mention_or_use": "use",
            "simulator_name": "PyBullet",
            "simulator_description": "Non-differentiable, real-time physics simulator for rigid-body dynamics and contact used in robotics and ML; used in the paper as a black-box baseline combined with REINFORCE for gradient estimation.",
            "scientific_domain": "mechanics (rigid-body dynamics and contact)",
            "fidelity_level": "Industry-grade real-time fidelity for rigid-body dynamics and contacts; medium-to-high fidelity dynamics but non-differentiable.",
            "fidelity_characteristics": "Simulates rigid dynamics, contacts, penalty and constraint-based collision handling; fast/real-time but gradients must be estimated with black-box methods (e.g., REINFORCE) which are sample-inefficient and noisy.",
            "model_or_agent_name": "PyBullet + REINFORCE baseline (Ehsani et al. style)",
            "model_description": "Black-box baseline using REINFORCE (policy gradient) to estimate gradients through non-differentiable simulator for parameter estimation.",
            "reasoning_task": "Parameter estimation from video-like observations (baseline) using black-box gradient estimators; used to test whether analytical differentiability is required.",
            "training_performance": "Worse and more sensitive than differentiable approaches: reported mean absolute mass-estimation error 0.0926 kg (Table 1) and larger absolute relative errors; narrow convergence basins were observed in loss-landscape analysis.",
            "transfer_target": "Used to generate ground-truth and as a baseline; not used for transfer experiments.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors observe that black-box gradient estimation with non-differentiable simulators is sensitive to simulator parameters, has narrow convergence regions and is less data-efficient than analytic differentiation.",
            "failure_cases": "PyBullet+REINFORCE exhibited narrow convergence regions and sensitivity leading to worse estimation performance compared to ∇Sim; multiple local minima and sample inefficiency were noted.",
            "uuid": "e1536.2",
            "source_info": {
                "paper_title": "gradSim: Differentiable simulation for system identification and visuomotor control",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "SoftRas",
            "name_full": "Soft Rasterizer (SoftRas)",
            "brief_description": "A differentiable rasterization-based renderer that replaces hard triangle edges with smooth sigmoids to produce differentiable image formation; used inside ∇Sim as one differentiable rendering option.",
            "citation_title": "Soft rasterizer: A differentiable renderer for image-based 3d reasoning",
            "mention_or_use": "use",
            "simulator_name": "SoftRas (differentiable renderer)",
            "simulator_description": "Differentiable rasterizer that smooths triangle edges using sigmoids to remove nondifferentiable visibility discontinuities, supports textured and shaded outputs for gradient-based optimization.",
            "scientific_domain": "rendering / image formation (used to enable image-to-physics gradients)",
            "fidelity_level": "Medium-fidelity rendering: captures geometry, textures, and approximate shading cues, but intentionally smooths visibility to remain differentiable; does not model full physical light transport (no full ray-traced global illumination by default).",
            "fidelity_characteristics": "Smooths triangle edges into semi-transparent boundaries (sigmoid), supports texture sampling and shading approximations, differentiable UV sampling; trades exact visibility correctness for differentiability.",
            "model_or_agent_name": "Differentiable renderer used inside training pipeline (component)",
            "model_description": "Rasterization-based differentiable renderer module used to render predicted states to pixels for MSE loss against target images.",
            "reasoning_task": "Enables backpropagation from pixel-wise losses to scene geometry and physical parameters, facilitating system identification and visuomotor policy training from images.",
            "training_performance": "Contributes to smooth loss landscapes that allow successful gradient-based parameter estimation; including shading and texture in the differentiable renderer improved convergence speed (qualitative and shown in Fig. 7).",
            "transfer_target": "Compared against photorealistic renderer targets in robustness experiments.",
            "transfer_performance": "When ∇Sim used differentiable renderers as training and targets were photorealistic, estimation error increased but remained usable (photorealistic target rel. abs. est. 0.1793 vs perfect-model 0.1071).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Differentiable rasterizers (SoftRas) with shading/textures improved convergence speed; however, mismatch to photorealistic rendering increases error modestly compared to dynamics-model mismatches.",
            "minimal_fidelity_discussion": "Shading and texture cues in the differentiable renderer are beneficial for faster convergence, but perfect photorealism is not strictly required for reasonable parameter recovery; accurate dynamics remain more important.",
            "failure_cases": "Because SoftRas smooths visibility, it may not exactly match sharp visibility changes in photorealistic images, causing some degradation when transferring to photorealistic targets (observed modestly higher estimation errors).",
            "uuid": "e1536.3",
            "source_info": {
                "paper_title": "gradSim: Differentiable simulation for system identification and visuomotor control",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "DIB-R",
            "name_full": "DIB-R (Interpolation-based Differentiable Renderer)",
            "brief_description": "An interpolation-based differentiable renderer that distinguishes foreground and background pixels and uses differentiable interpolation for foreground, used as an alternative differentiable rendering backend in ∇Sim.",
            "citation_title": "Learning to predict 3d objects with an interpolation-based differentiable renderer",
            "mention_or_use": "use",
            "simulator_name": "DIB-R (differentiable renderer)",
            "simulator_description": "Differentiable rendering approach that treats foreground pixels by differentiable bilinear sampling of textures and processes background pixels similarly to SoftRas; used to render differentiable images from mesh states in ∇Sim.",
            "scientific_domain": "rendering / image formation",
            "fidelity_level": "Medium-fidelity differentiable rendering (foreground/background treatment, differentiable texture sampling), not a full forward photorealistic light transport renderer.",
            "fidelity_characteristics": "Differentiable bilinear texture sampling for foreground, sigmoid-smoothed treatment for background visibility; supports textures and basic shading; designed for gradient propagation.",
            "model_or_agent_name": "Differentiable rendering backend within ∇Sim",
            "model_description": "Component that transforms mesh geometry, textures, and camera parameters into images with gradients usable by the simulator's optimization loop.",
            "reasoning_task": "Enables gradient-based system identification and control by providing differentiable image outputs.",
            "training_performance": "Used interchangeably with SoftRas; contributes to smooth loss landscapes and successful parameter estimation/control from images; shading and texture improved convergence.",
            "transfer_target": "Used in experiments where ∇Sim trained with DIB-R is evaluated against photorealistic-renderer targets.",
            "transfer_performance": "Performance decreased modestly when target images were photorealistic (see ∇Sim transfer_performance), but DIB-R-enabled training still produced usable gradients.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Acts similarly to SoftRas: differentiable rasterization with foreground interpolation yields stable gradients; photorealistic mismatch causes modest error increase, while dynamics mismatch causes larger errors.",
            "minimal_fidelity_discussion": "Same conclusions as SoftRas: differentiable rendering with shading and texture helps convergence but is secondary to accurate dynamics modeling.",
            "failure_cases": "Same as SoftRas — mismatch to high-fidelity photorealistic targets leads to modest degradation in estimation accuracy.",
            "uuid": "e1536.4",
            "source_info": {
                "paper_title": "gradSim: Differentiable simulation for system identification and visuomotor control",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Photorealistic renderer (PBRT)",
            "name_full": "Physically Based Rendering (photorealistic renderer used as ground-truth)",
            "brief_description": "A high-fidelity, non-differentiable photorealistic renderer (cited to Pharr et al., 2016) used to generate target images for robustness / transfer experiments to evaluate sensitivity to rendering mismatches.",
            "citation_title": "Physically Based Rendering: From Theory to Implementation",
            "mention_or_use": "use",
            "simulator_name": "Photorealistic renderer (PBRT)",
            "simulator_description": "High-fidelity forward renderer implementing physically based light transport (shading, textures, possibly multi-bounce lighting) used to synthesize target frames while ∇Sim uses differentiable rasterizers for training.",
            "scientific_domain": "rendering / image formation (used to test transfer from differentiable renderers to photorealistic images)",
            "fidelity_level": "High-fidelity photorealistic rendering (accurate shading, textures, and advanced light transport) but not differentiable in the experiments.",
            "fidelity_characteristics": "Includes Phong-style shading, textures, and physically based rendering effects; produces realistic images used as ground-truth targets to evaluate the reality-gap caused by renderer mismatch.",
            "model_or_agent_name": "Target image generator for transfer experiments",
            "model_description": "Non-differentiable photorealistic renderer used to create ground-truth images and object masks to test sensitivity of ∇Sim parameter estimation.",
            "reasoning_task": "Serves as the target domain to which ∇Sim-trained estimators/controllers must transfer when trained with differentiable rasterizers.",
            "training_performance": "When ∇Sim was trained with differentiable renderers and tested on photorealistic targets, estimation error increased: mean relative absolute estimation ~0.1793 (Table 4) versus perfect-model case 0.1071.",
            "transfer_target": "Represents a higher-fidelity target domain used to evaluate transfer from differentiable-renderer training.",
            "transfer_performance": "Quantified degradation: rel. abs. est. increased to 0.1793 for photorealistic targets (Table 4), indicating modest domain gap but not catastrophic failure.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Using photorealistic target images causes a modest increase in parameter estimation error relative to perfect matched differentiable-renderer training, but is less damaging than dynamics-model mismatch.",
            "minimal_fidelity_discussion": "Authors note that photorealistic target images cause modest degradation and that shading/texture cues help; however, they emphasize that accurate dynamics modelling is a higher priority for transfer than perfect photorealistic rendering.",
            "failure_cases": "Photorealistic rendering mismatch alone did not cause catastrophic failures but did slow convergence and increased estimation error compared to perfectly matched rendering.",
            "uuid": "e1536.5",
            "source_info": {
                "paper_title": "gradSim: Differentiable simulation for system identification and visuomotor control",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Use the Force, Luke! learning to predict physical forces by simulating effects",
            "rating": 2
        },
        {
            "paper_title": "DiffTaichi: Differentiable programming for physical simulation",
            "rating": 2
        },
        {
            "paper_title": "ChainQueen: A real-time differentiable physical simulator for soft robotics",
            "rating": 2
        },
        {
            "paper_title": "A differentiable physics engine for deep learning in robotics",
            "rating": 2
        },
        {
            "paper_title": "Differentiable cloth simulation for inverse problems",
            "rating": 2
        },
        {
            "paper_title": "PyBullet, a python module for physics simulation for games, robotics and machine learning",
            "rating": 2
        },
        {
            "paper_title": "Physically Based Rendering: From Theory to Implementation",
            "rating": 2
        },
        {
            "paper_title": "Tiny Differentiable Simulator",
            "rating": 2
        }
    ],
    "cost": 0.02233425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>$\nabla$ Sim: DIFFERENTIABLE SIMULATION FOR SYSTEM IDENTIFICATION AND VISUOMOTOR CONTROL</h1>
<p>https://gradsim.github.io</p>
<p>Krishna Murthy Jatavallabhula ${ }^{\star 1,3,4}$, Miles Macklin ${ }^{\star 2}$, Florian Golemo ${ }^{1,3}$, Vikram Voleti ${ }^{3,4}$, Linda Petrini ${ }^{3}$, Martin Weiss ${ }^{3,4}$, Breandan Considine ${ }^{3,5}$, Jérôme Parent-Lévesque ${ }^{3,5}$, Kevin Xie ${ }^{2,6,7}$, Kenny Erleben ${ }^{8}$, Liam Paull ${ }^{1,3,4}$, Florian Shkurti ${ }^{6,7}$, Derek Nowrouzezahrai ${ }^{3,5}$, and Sanja Fidler ${ }^{2,6,7}$<br>${ }^{1}$ Montreal Robotics and Embodied AI Lab, ${ }^{2}$ NVIDIA, ${ }^{3}$ Mila, ${ }^{4}$ Université de Montréal, ${ }^{5}$ McGill, ${ }^{6}$ University of Toronto, ${ }^{7}$ Vector Institute, ${ }^{8}$ University of Copenhagen</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: $\nabla \boldsymbol{S i m}$ is a unified differentiable rendering and multiphysics framework that allows solving a range of control and parameter estimation tasks (rigid bodies, deformable solids, and cloth) directly from images/video.</p>
<h4>Abstract</h4>
<p>We consider the problem of estimating an object's physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present $\nabla \operatorname{Sim}$, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph - spanning from the dynamics and through the rendering process - enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.</p>
<h2>1 INTRODUCTION</h2>
<p>Accurately predicting the dynamics and physical characteristics of objects from image sequences is a long-standing challenge in computer vision. This end-to-end reasoning task requires a fundamental understanding of both the underlying scene dynamics and the imaging process. Imagine watching a short video of a basketball bouncing off the ground and ask: "Can we infer the mass and elasticity of the ball, predict its trajectory, and make informed decisions, e.g., how to pass and shoot?" These seemingly simple questions are extremely challenging to answer even for modern computer vision models. The underlying physical attributes of objects and the system dynamics need to be modeled and estimated, all while accounting for the loss of information during 3D to 2D image formation.</p>
<p>Depending on the assumptions on the scene structre and dynamics, three types of solutions exist: black, grey, or white box. Black box methods (Watters et al., 2017; Xu et al., 2019b; Janner et al., 2019; Chang et al., 2016) model the state of a dynamical system (such as the basketball's trajectory in time) as a learned embedding of its states or observations. These methods require few prior assumptions about the system itself, but lack interpretability due to entangled variational factors (Chen et al., 2016) or due to the ambiguities in unsupervised learning (Greydanus et al., 2019; Cranmer et al., 2020b). Recently, grey box methods (Mehta et al., 2020) leveraged partial knowledge about the system dynamics to improve performance. In contrast, white box methods (Degrave et al., 2016;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: $\nabla$ Sim: Given video observations of an evolving physical system (e), we randomly initialize scene object properties (a) and evolve them over time using a differentiable physics engine (b), which generates states. Our renderer (c) processes states, object vertices and global rendering parameters to produce image frames for computing our loss. We backprop through this computation graph to estimate physical attributes and controls. Existing methods rely solely on differentiable physics engines and require supervision in state-space (f), while $\nabla$ Sim only needs image-space supervision (g).</p>
<p>Liang et al., 2019; Hu et al., 2020; Qiao et al., 2020) impose prior knowledge by employing explicit dynamics models, reducing the space of learnable parameters and improving system interpretability.
Most notably in our context, all of these approaches require precise 3D labels - which are laborintensive to gather, and infeasible to generate for many systems such as deformable solids or cloth.</p>
<h1>We eliminate the dependence of white box dynamics methods on 3D supervision by coupling explicit (and differentiable) models of scene dynamics with image formation (rendering) ${ }^{\dagger}$.</h1>
<p>Explicitly modeling the end-to-end dynamics and image formation underlying video observations is challenging, even with access to the full system state. This problem has been treated in the vision, graphics, and physics communities (Pharr et al., 2016; Macklin et al., 2014), leading to the development of robust forward simulation models and algorithms. These simulators are not readily usable for solving inverse problems, due in part to their non-differentiability. As such, applications of black-box forward processes often require surrogate gradient estimators such as finite differences or REINFORCE (Williams, 1992) to enable any learning. Likelihood-free inference for black-box forward simulators (Ramos et al., 2019; Cranmer et al., 2020a; Kulkarni et al., 2015; Yildirim et al., 2017; 2015; 2020; Wu et al., 2017b) has led to some improvements here, but remains limited in terms of data efficiency and scalability to high dimensional parameter spaces. Recent progress in differentiable simulation further improves the learning dynamics, however we still lack a method for end-to-end differentiation through the entire simulation process (i.e., from video pixels to physical attributes), a prerequisite for effective learning from video frames alone.</p>
<p>We present $\nabla$ Sim, a versatile end-to-end differentiable simulator that adopts a holistic, unified view of differentiable dynamics and image formation(cf. Fig. 1,2). Existing differentiable physics engines only model time-varying dynamics and require supervision in state space (usually 3D tracking). We additionally model a differentiable image formation process, thus only requiring target information specified in image space. This enables us to backpropagate (Griewank \&amp; Walther, 2003) training signals from video pixels all the way to the underlying physical and dynamical attributes of a scene.</p>
<p>Our main contributions are:</p>
<ul>
<li>$\nabla$ Sim, a differentiable simulator that demonstrates the ability to backprop from video pixels to the underlying physical attributes (cf. Fig. 2).</li>
<li>We demonstrate recovering many physical properties exclusively from video observations, including friction, elasticity, deformable material parameters, and visuomotor controls (sans 3D supervision)</li>
<li>A PyTorch framework facilitating interoperability with existing machine learning modules.</li>
</ul>
<p>We evaluate $\nabla$ Sim's effectiveness on parameter identification tasks for rigid, deformable and thinshell bodies, and demonstrate performance that is competitive, or in some cases superior, to current physics-only differentiable simulators. Additionally, we demonstrate the effectiveness of the gradients provided by $\nabla$ Sim on challenging visuomotor control tasks involving deformable solids and cloth.</p>
<h2>$2 \nabla$ Sim: A UNIFIED DIFFERENTIABLE SIMULATION ENGINE</h2>
<p>Typically, physics estimation and rendering have been treated as disjoint, mutually exclusive tasks. In this work, we take on a unified view of simulation in general, to compose physics estimation</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and rendering. Formally, simulation is a function Sim : $\mathbb{R}^{P} \times[0,1] \mapsto \mathbb{R}^{H} \times \mathbb{R}^{W} ; \operatorname{Sim}(\mathbf{p}, t)=\mathcal{I}$. Here $\mathbf{p} \in \mathbb{R}^{P}$ is a vector representing the simulation state and parameters (objects, their physical properties, their geometries, etc.), $t$ denotes the time of simulation (conveniently reparameterized to be in the interval $[0,1]$ ). Given initial conditions $\mathbf{p}_{0}$, the simulation function produces an image $\mathcal{I}$ of height $H$ and width $W$ at each timestep $t$. If this function Sim were differentiable, then the gradient of $\operatorname{Sim}(\mathbf{p}, t)$ with respect to the simulation parameters $\mathbf{p}$ provides the change in the output of the simulation from $\mathcal{I}$ to $\mathcal{I}+\nabla \operatorname{Sim}(\mathbf{p}, t) \delta \mathbf{p}$ due to an infinitesimal perturbation of $\mathbf{p}$ by $\delta \mathbf{p}$. This construct enables a gradient-based optimizer to estimate physical parameters from video, by defining a loss function over the image space $\mathcal{L}(\mathcal{I},$.$) , and descending this loss landscape along a$ direction parallel to $-\nabla \operatorname{Sim}($.$) . To realise this, we turn to the paradigms of computational graphs$ and differentiable programming.
$\nabla$ Sim comprises two main components: a differentiable physics engine that computes the physical states of the scene at each time instant, and a differentiable renderer that renders the scene to a 2D image. Contrary to existing differentiable physics (Toussaint et al., 2018; de Avila Belbute-Peres et al., 2018; Song \&amp; Boularias, 2020b;a; Degrave et al., 2016; Wu et al., 2017a; Research, 2020 (accessed May 15, 2020; Hu et al., 2020; Qiao et al., 2020) or differentiable rendering (Loper \&amp; Black, 2014; Kato et al., 2018; Liu et al., 2019; Chen et al., 2019) approaches, we adopt a holistic view and construct a computational graph spanning them both.</p>
<h1>2.1 DIFFERENTIABLE PHYSICS ENGINE</h1>
<p>Under Lagrangian mechanics, the state of a physical system can be described in terms of generalized coordinates $\mathbf{q}$, generalized velocities $\dot{\mathbf{q}}=\mathbf{u}$, and design/model parameters $\theta$. For the purpose of exposition, we make no distinction between rigid bodies, or deformable solids, or thin-shell models of cloth, etc. Although the specific choices of coordinates and parameters vary, the simulation procedure is virtually unchanged. We denote the combined state vector by $\mathbf{s}(t)=[\mathbf{q}(t), \mathbf{u}(t)]$.
The dynamic evolution of the system is governed by second order differential equations (ODEs) of the form $\mathbf{M}(\mathbf{s}, \theta) \dot{\mathbf{s}}=\mathbf{f}(\mathbf{s}, \theta)$, where $\mathbf{M}$ is a mass matrix that depends on the state and parameters. The forces on the system may be parameterized by design parameters (e.g. Young's modulus). Solutions to these ODEs may be obtained through black box numerical integration methods, and their derivatives calculated through the continuous adjoint method (Chen et al., 2018). However, we instead consider our physics engine as a differentiable operation that provides an implicit relationship between a state vector $\mathbf{s}^{-}=\mathbf{s}(t)$ at the start of a time step, and the updated state at the end of the time step $\mathbf{s}^{+}=\mathbf{s}(t+\Delta t)$. An arbitrary discrete time integration scheme can be then be abstracted as the function $\mathbf{g}\left(\mathbf{s}^{-}, \mathbf{s}^{+}, \theta\right)=\mathbf{0}$, relating the initial and final system state and the model parameters $\theta$.</p>
<p>Gradients through this dynamical system can be computed by graph-based autodiff frameworks (Paszke et al., 2019; Abadi et al., 2015; Bradbury et al., 2018), or by program transformation approaches (Hu et al., 2020; van Merriënboer et al., 2018). Our framework is agnostic to the specifics of the differentiable physics engine, however in Appendices A through D we detail an efficient approach based on the source-code transformation of parallel kernels, similar to DiffTaichi (Hu et al., 2020). In addition, we describe extensions to this framework to support mesh-based tetrahedral finite-element models (FEMs) for deformable and thin-shell solids. This is important since we require surface meshes to perform differentiable rasterization as described in the following section.</p>
<h3>2.2 DIFFERENTIABLE RENDERING ENGINE</h3>
<p>A renderer expects scene description inputs and generates color image outputs, all according to a sequence of image formation stages defined by the forward graphics pipeline. The scene description includes a complete geometric descriptor of scene elements, their associated material/reflectance properties, light source definitions, and virtual camera parameters. The rendering process is not generally differentiable, as visibility and occlusion events introduce discontinuities. Most interactive renderers, such as those used in real-time applications, employ a rasterization process to project 3D geometric primitives onto 2D pixel coordinates, resolving these visibility events with nondifferentiable operations.</p>
<p>Our experiments employ two differentiable alternatives to traditional rasterization, SoftRas (Liu et al., 2019) and DIB-R (Chen et al., 2019), both of which replace discontinuous triangle mesh edges with smooth sigmoids. This has the effect of blurring triangle edges into semi-transparent</p>
<p>boundaries, thereby removing the non-differentiable discontinuity of traditional rasterization. DIBR distinguishes between foreground pixels (associated to the principal object being rendered in the scene) and background pixels (for all other objects, if any). The latter are rendered using the same technique as SoftRas while the former are rendered by bilinearly sampling a texture using differentiable UV coordinates.
$\nabla$ Sim performs differentiable physics simulation and rendering at independent and adjustable rates, allowing us to trade computation for accuracy by rendering fewer frames than dynamics updates.</p>
<h1>3 EXPERIMENTS</h1>
<p>We conducted multiple experiments to test the efficacy of $\nabla$ Sim on physical parameter identification from video and visuomotor control, to address the following questions:</p>
<ul>
<li>Can we accurately identify physical parameters by backpropagating from video pixels, through the simulator? (Ans: Yes, very accurately, cf. 3.1)</li>
<li>What is the performance gap associated with using $\nabla$ Sim (2D supervision) vs. differentiable physics-only engines (3D supervision)? (Ans: $\nabla$ Sim is competitive/superior, cf. Tables 1, 2, 3)</li>
<li>How do loss landscapes differ across differentiable simulators ( $\nabla \operatorname{Sim}$ ) and their non-differentiable counterparts? (Ans: Loss landscapes for $\nabla$ Sim are smooth, cf. 3.1.3)</li>
<li>Can we use $\nabla$ Sim for visuomotor control tasks? (Ans: Yes, without any 3D supervision, cf. 3.2)</li>
<li>How sensitive is $\nabla$ Sim to modeling assumptions at system level? (Ans: Moderately, cf. Table 4)</li>
</ul>
<p>Each of our experiments comprises an environment $\mathcal{E}$ that applies a particular set of physical forces and/or constraints, a (differentiable) loss function $\mathcal{L}$ that implicitly specifies an objective, and an initial guess $\theta_{0}$ of the physical state of the simulation. The goal is to recover optimal physics parameters $\theta^{*}$ that minimize $\mathcal{L}$, by backpropagating through the simulator.</p>
<h3>3.1 PHYSICAL PARAMETER ESTIMATION FROM VIDEO</h3>
<p>First, we assess the capabilities of $\nabla$ Sim to accurately identify a variety of physical attributes such as mass, friction, and elasticity from image/video observations. To the best of our knowledge, $\nabla$ Sim is the first study to jointly infer such fine-grained parameters from video observations. We also implement a set of competitive baselines that use strictly more information on the task.</p>
<h3>3.1.1 RIGID BODIES (RIGID)</h3>
<p>Our first environment-rigid-evaluates the accuracy of estimating of physical and material attributes of rigid objects from videos. We curate a dataset of 10000 simulated videos generated from variations of 14 objects, comprising primitive shapes such as boxes, cones, cylinders, as well as non-convex shapes from ShapeNet (Chang et al., 2015) and DexNet (Mahler et al., 2017). With uniformly sampled initial dimensions, poses, velocities, and physical properties (density, elasticity, and friction parameters), we apply a known impulse to the object and record a video of the resultant trajectory. Inference with $\nabla$ Sim is done by guessing an initial mass (uniformly random in the range $[2,12] \mathrm{kg} / \mathrm{m}^{2}$ ), unrolling a differentiable simulation using this guess, comparing the rendered out video with the true video (pixelwise mean-squared error - MSE), and performing gradient descent updates. We refer the interested reader to the appendix (Sec. G) for more details.</p>
<p>Table 1 shows the results for predicting the mass of an object from video, with a known impulse applied to it. We use EfficientNet (B0) (Tan \&amp; Le, 2019) and resize input frames to $64 \times 64$. Feature maps at a resoluition of $4 \times 4 \times 32$ are concatenated for all frames and fed to an MLP with 4 linear</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Mean abs.</th>
<th style="text-align: center;">Abs. Rel.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">err. (kg)</td>
<td style="text-align: center;">err.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">0.2022</td>
<td style="text-align: center;">0.1031</td>
<td style="text-align: center;">Approach</td>
<td style="text-align: center;">m</td>
<td style="text-align: center;">$\xi_{2}$</td>
<td style="text-align: center;">$\xi_{1}$</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">0.2032</td>
<td style="text-align: center;">0.1341</td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">1.7713</td>
<td style="text-align: center;">3.7145</td>
<td style="text-align: center;">2.2410</td>
</tr>
<tr>
<td style="text-align: left;">ConvLSTM Xu et al. (2019b)</td>
<td style="text-align: center;">0.1247</td>
<td style="text-align: center;">0.0091</td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">10.0007</td>
<td style="text-align: center;">4.18</td>
<td style="text-align: center;">2.5454</td>
</tr>
<tr>
<td style="text-align: left;">PyBullet + REINFORCE Ehsani et al. (2020)</td>
<td style="text-align: center;">0.0926</td>
<td style="text-align: center;">0.5666</td>
<td style="text-align: center;">ConvLSTM Xu et al. (2019b)</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: left;">DiffPhysics (3D sup.)</td>
<td style="text-align: center;">1.35e-9</td>
<td style="text-align: center;">5.17e-9</td>
<td style="text-align: center;">DiffPhysics (3D sup.)</td>
<td style="text-align: center;">1.70e-8</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.0020</td>
</tr>
<tr>
<td style="text-align: left;">$\nabla$ Sim</td>
<td style="text-align: center;">2.36e-5</td>
<td style="text-align: center;">9.01e-5</td>
<td style="text-align: center;">$\nabla$ Sim</td>
<td style="text-align: center;">2.87e-4</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.0026</td>
</tr>
</tbody>
</table>
<p>Table 1: Mass estimation: $\nabla$ Sim obtains precise mass estimates, comparing favourably even with approaches that require 3D supervision (diffphysics). We report the mean abolute error and absolute relative errors for all approaches evaluated.</p>
<p>Table 2: Rigid-body parameter estimation: $\nabla$ Sim estimates contact parameters (elasticity, friction) to a high degree of accuracy, despite estimating them from video. Diffphys. requires accurate 3D ground-truth at 30 FPS. We report absolute relative errors for each approach evaluated.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Deformable solid FEM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Thin-shell (cloth)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Per-particle mass</td>
<td style="text-align: center;">Material properties</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Per-particle velocity</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">m</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">v</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Approach</td>
<td style="text-align: center;">Rel. MAE</td>
<td style="text-align: center;">Rel. MAE</td>
<td style="text-align: center;">Rel. MAE</td>
<td style="text-align: center;">Rel. MAE</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DiffPhysics (3D Sup.)</td>
<td style="text-align: center;">0.032</td>
<td style="text-align: center;">0.0025</td>
<td style="text-align: center;">0.0024</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\nabla$ Sim</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">0.0054</td>
<td style="text-align: center;">0.0056</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Parameter estimation of deformable objects: We estimate per-particle masses and material properties (for solid def. objects) and per-particle velocities for cloth. In the case of cloth, there is a perceivable performance drop in diffphysics, as the center of mass of a cloth is often outside the body, which results in ambiguity.
layers, and trained with an MSE loss. We compare $\nabla$ Sim with three other baselines: PyBullet + REINFORCE (Ehsani et al., 2020; Wu et al., 2015), diff. physics only (requiring 3D supervision), and a ConvLSTM baseline adopted from Xu et al. (2019b) but with a stronger backbone. The DiffPhysics baseline is a strict subset of $\nabla$ Sim, it only inolves the differentiable physics engine. However, it needs precise 3D states as supervision, which is the primary factor for its superior performance. Nevertheless, $\nabla$ Sim is able to very precisely estimate mass from video, to a absolute relative error of 9.01e-5, nearly two orders of magnitude better than the ConvLSTM baseline. Two other baselines are also used: the "Average" baseline always predicts the dataset mean and the "Random" baseline predicts a random parameter value from the test distribution. All baselines and training details can be found in Sec. H of the appendix.</p>
<p>To investigate whether analytical differentiability is required, our PyBullet + REINFORCE baseline applies black-box gradient estimation (Williams, 1992) through a non-differentiable simulator (Coumans \&amp; Bai, 2016-2019), similar to Ehsani et al. (2020). We find this baseline particularly sensitive to several simulation parameters, and thus worse-performing. In Table 2, we jointly estimate friction and elasticity parameters of our compliant contact model from video observations alone. The trend is similar to Table 1, and $\nabla$ Sim is able to precisely recover the parameters of the simulation. A few examples can be seen in Fig. 3.</p>
<h1>3.1.2 Deformable Bodies (Deformable)</h1>
<p>We conduct a series of experiments to investigate the ability of $\nabla$ Sim to recover physical parameters of deformable solids and thin-shell solids (cloth). Our physical model is parameterized by the perparticle mass, and Lamé elasticity parameters, as described in in Appendix C.1. Fig. 3 illustrates the recovery of the elasticity parameters of a beam hanging under gravity by matching the deformation given by an input video sequence. We found our method is able to accurately recover the parameters of 100 instances of deformable objects (cloth, balls, beams) as reported in Table 3 and Fig. 3.</p>
<h3>3.1.3 SMOOTHNESS OF THE LOSS LANDSCAPE IN $\nabla$ Sim</h3>
<p>Since $\nabla$ Sim is a complex combination of differentiable non-linear components, we analyze the loss landscape to verify the validity of gradients through the system. Fig. 4 illustrates the loss landscape when optimizing for the mass of a rigid body when all other physical properties are known.</p>
<p>We examine the image-space mean-squared error (MSE) of a unit-mass cube ( 1 kg ) for a range of initializations $(0.1 \mathrm{~kg}$ to 5 kg$)$. Notably, the loss landscape of $\nabla$ Sim is well-behaved and conducive to momentum-based optimizers. Applying MSE to the first and last frames of the predicted and true videos provides the best gradients. However, for a naive gradient estimator applied to a nondifferentiable simulator (PyBullet + REINFORCE), multiple local minima exist resulting in a very narrow region of convergence. This explains $\nabla$ Sim's superior performance in Tables 1, 2, 3.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Loss landscapes when optimizing for physical attributes using $\nabla$ Sim. (Left) When estimating the mass of a rigid-body with known shape using $\nabla$ Sim, despite images being formed by a highly nonlinear process (simulation), the loss landscape is remarkably smooth, for a range of initialization errors. (Right) when optimizing for the elasticity parameters of a deformable FEM solid. Both the Lamé parameters $\lambda$ and $\mu$ are set to 1000, where the MSE loss has a unique, dominant minimum. Note that, for fair comparison, the ground-truth for our PyBullet+REINFORCE baseline was generated using the PyBullet engine.</p>
<h1>3.2 VISUOMOTOR CONTROL</h1>
<p>To investigate whether the gradients computed by $\nabla$ Sim are meaningful for vision-based tasks, we conduct a range of visuomotor control experiments involving the actuation of deformable objects towards a visual target pose (a single image). In all cases, we evaluate against diffphysics, which uses a goal specification and a reward, both defined over the 3D state-space.</p>
<h3>3.2.1 DEFORMABLE SOLIDS</h3>
<p>The first example (control-walker) involves a 2D walker model. Our goal is to train a neural network (NN) control policy to actuate the walker to reach a target pose on the right-hand side of an image. Our NN consists of one fully connected layer and a tanh activation. The network input is a set of 8 time-varying sinusoidal signals, and the output is a scalar activation value per-tetrahedron. $\nabla$ Sim is able to solve this environment within three iterations of gradient descent, by minimizing a pixelwise MSE between the last frame of the rendered video and the goal image as shown in Fig. 5 (lower left).</p>
<p>In our second test, we formulate a more challenging 3D control problem (control-fem) where the goal is to actuate a soft-body FEM object (a gear) consisting of 1152 tetrahedral elements to move to a target position as shown in Fig. 5 (center). We use the same NN architecture as in the 2D walker example, and use the Adam (Kingma \&amp; Ba, 2015) optimizer to minimize a pixelwise MSE loss. We also train a privileged baseline (diffphysics) that uses strong supervision and minimizes the MSE between the target position and the precise 3D location of the center-of-mass (COM) of the FEM model at each time step (i.e. a dense reward). We test both diffphysics and $\nabla$ Sim against a naive baseline that generates random activations and plot convergence behaviors in Fig. 6a.
While diffphysics appears to be a strong performer on this task, it is important to note that it uses explicit 3D supervision at each timestep (i.e. 30 FPS). In contrast, $\nabla$ Sim uses a single image as an implicit target, and yet manages to achieve the goal state, albeit taking a longer number of iterations.</p>
<h3>3.2.2 Cloth (CONTROL-CLOTH)</h3>
<p>We design an experiment to control a piece of cloth by optimizing the initial velocity such that it reaches a pre-specified target. In each episode, a random cloth is spawned, comprising between 64 and 2048 triangles, and a new start/goal combination is chosen.</p>
<p>In this challenging setup, we notice that state-based MPC (diffphysics) is often unable to accurately reach the target. We believe this is due to the underdetermined nature of the problem, since, for objects such as cloth, the COM by itself does not uniquely determine the configuration of the object. Visuomotor control on the other hand, provides a more well-defined problem. An illustration of the task is presented in Fig. 5 (column 3), and the convergence of the methods shown in Fig. 6(b).
<img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Results of various approaches on the control-fem environment ( 6 randomseeds; each randomseed corresponds to a different goal configuration). While diffphysics performs well, it assumes strong 3D supervision. In contrast, $\nabla$ Sim is able to solve the task by using just a single image of the target configuration.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Results on control-cloth environment (5 randomseeds; each controls the dimensions and initial/target poses of the cloth). diffphysics converges to a suboptimal solution due to ambiguity in specifying the pose of a cloth via its center-of-mass. $\nabla$ Sim solves the environment using a single target image.</p>
<p>Figure 6: Convergence Analysis: Performance of $\nabla$ Sim on visuomotor control using image-based supervision, 3D supervision, and random policies.</p>
<h1>3.3 IMPACT OF IMPERFECT DYNAMICS AND RENDERING MODELS</h1>
<p>Being a white box method, the performance of $\nabla$ Sim relies on the choice of dynamics and rendering models employed. An immediate question that arises is "how would the performance of $\nabla \mathrm{Sim}$ be impacted (if at all) by such modeling choices." We conduct multiple experiments targeted at investigating modelling errors and summarize them in Table 4 (left).</p>
<p>We choose a dataset comprising 90 objects equally representing rigid, deformable, and cloth types. By not modeling specific dynamics and rendering phenomena, we create the following 5 variants of our simulator.</p>
<ol>
<li>Unmodeled friction: We model all collisions as being frictionless.</li>
<li>Unmodeled elasticity: We model all collisions as perfectly elastic.</li>
<li>Rigid-as-deformable: All rigid objects in the dataset are modeled as deformable objects.</li>
<li>Deformable-as-rigid: All deformable objects in the dataset are modeled as rigid objects.</li>
<li>Photorealistic render: We employ a photorealistic renderer-as opposed to $\nabla$ Sim's differentiable rasterizers-in generating the target images.</li>
</ol>
<p>In all cases, we evaluate the accuracy with which the mass of the target object is estimated from a target video sequence devoid of modeling discrepancies. In general, we observe that imperfect dynamics models (i.e. unmodeled friction and elasticity, or modeling a rigid object as deformable or vice-versa) have a more profound impact on parameter identification compared to imperfect renderers.</p>
<h3>3.3.1 UNMODELED DYNAMICS PHENOMENON</h3>
<p>From Table 4 (left), we observe a noticeable performance drop when dynamics effects go unmodeled. Expectedly, the repurcussions of incorrect object type modeling (Rigid-as-deformable, Deformable-as-rigid) are more severe compared to unmodeled contact parameters (friction, elasticity). Modeling a deformable body as a rigid body results in irrecoverable deformation parameters and has the most severe impact on the recovered parameter set.</p>
<h3>3.3.2 UNMODELED RENDERING PHENOMENON</h3>
<p>We also independently investigate the impact of unmodeled rendering effects (assuming perfect dynamics). We indepenently render ground-truth images and object foreground masks from a photorealistic renderer (Pharr et al., 2016). We use these photorealistic renderings for ground-truth</p>
<p>and perform physical parameter estimation from video. We notice that the performance obtained under this setting is superior compared to ones with dynamics model imperfections.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Mean Rel. Abs. Est.</th>
<th style="text-align: center;">Tetrahedra ( $\boldsymbol{\theta}$ )</th>
<th style="text-align: center;">Forward (DP)</th>
<th style="text-align: center;">Forward (DR)</th>
<th style="text-align: center;">Backward (DP)</th>
<th style="text-align: center;">Backward (DP + DR)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Unmodelled friction</td>
<td style="text-align: center;">0.18601</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">9057 Hz</td>
<td style="text-align: center;">3504 Hz</td>
<td style="text-align: center;">3721 Hz</td>
<td style="text-align: center;">3057 Hz</td>
</tr>
<tr>
<td style="text-align: left;">Unmodelled elasticity</td>
<td style="text-align: center;">0.2281</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">9057 Hz</td>
<td style="text-align: center;">3478 Hz</td>
<td style="text-align: center;">3780 Hz</td>
<td style="text-align: center;">2963 Hz</td>
</tr>
<tr>
<td style="text-align: left;">Rigid-as-deformable</td>
<td style="text-align: center;">0.3462</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">8751 Hz</td>
<td style="text-align: center;">3357 Hz</td>
<td style="text-align: center;">3750 Hz</td>
<td style="text-align: center;">1360 Hz</td>
</tr>
<tr>
<td style="text-align: left;">Deformable-as-rigid</td>
<td style="text-align: center;">0.4574</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">4174 Hz</td>
<td style="text-align: center;">1690 Hz</td>
<td style="text-align: center;">1644 Hz</td>
<td style="text-align: center;">1041 Hz</td>
</tr>
<tr>
<td style="text-align: left;">Photorealistic render</td>
<td style="text-align: center;">0.1793</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">3967 Hz</td>
<td style="text-align: center;">1584 Hz</td>
<td style="text-align: center;">1655 Hz</td>
<td style="text-align: center;">698 Hz</td>
</tr>
<tr>
<td style="text-align: left;">Perfect model</td>
<td style="text-align: center;">$\mathbf{0 . 1 0 7 1}$</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">3871 Hz</td>
<td style="text-align: center;">1529 Hz</td>
<td style="text-align: center;">1553 Hz</td>
<td style="text-align: center;">424 Hz</td>
</tr>
</tbody>
</table>
<p>Table 4: (Left) Impact of imperfect models: The accuracy of physical parameters estimated by $\nabla$ Sim is impacted by the choice of dynamics and graphics (rendering) models. We find that the system is more sensitive to the choice of dynamics models than to the rendering engine used. (Right) Timing analysis: We report runtime in simulation steps / second (Hz). $\nabla$ Sim is significantly faster than real-time, even for complex geometries.</p>
<h1>3.3.3 IMPACT OF SHADING AND TEXTURE CUES</h1>
<p>Although our work does not attempt to bridge the reality gap, we show early prototypes to assess phenomena such as shading/texture. Fig. 7 shows the accuracy over time for mass estimation from video. We evaluate three variants of the renderer - "Only color", "Shading", and "Texture". The "Only color" variant renders each mesh element in the same color regardless of the position and orientation of the light source. The "Shading" variant implements a Phong shading model and can model specular and diffuse reflections. The "Texture" variant also applies a nonuniform texture sampled from ShapeNet (Chang et al., 2015). We notice that shading and texture cues significantly improve convergence speed. This is expected, as vertex colors often have very little appearance cues inside the object boundaries, leading to poor correspondences between the rendered and ground-truth images. Furthermore, textures seem to offer slight improvements in convergence speed over shaded models, as highlighted by the inset (log scale) plot in Fig. 7.</p>
<h3>3.3.4 Timing analysis</h3>
<p>Table 4 (right) shows simulation rates for the forward and backward passes of each module. We report forward and backward pass rates separately for the differentiable physics (DP) and the differentiable rendering (DR) modules. The time complexity of $\nabla$ Sim is a function of the number of tetrahedrons and/or triangles. We illustrate the arguably more complex case of deformable object simulation for varying numbers of tetrahedra (ranging from 100 to 10000). Even in the case of 10000 tetrahedra-enough to contruct complex mesh models of multiple moving objects- $\nabla$ Sim enables faster-than-real-time simulation ( 1500 steps/second).</p>
<h2>4 RELATED WORK</h2>
<p>Differentiable physics simulators have seen significant attention and activity, with efforts centered around embedding physics structure into autodifferentiation frameworks. This has enabled differentiation through contact and friction models (Toussaint et al., 2018; de Avila Belbute-Peres et al., 2018; Song \&amp; Boularias, 2020b;a; Degrave et al., 2016; Wu et al., 2017a; Research, 2020 (accessed May 15, 2020), latent state models (Guen \&amp; Thome, 2020; Schenck \&amp; Fox, 2018; Jaques et al., 2020; Heiden et al., 2019), volumetric soft bodies (Hu et al., 2019; 2018; Liang et al., 2019; Hu et al., 2020), as well as particle dynamics (Schenck \&amp; Fox, 2018; Li et al., 2019; 2020; Hu et al., 2020). In contrast, $\nabla$ Sim addresses a superset of simulation scenarios, by coupling the physics simulator with a differentiable rendering pipeline. It also supports tetrahedral FEM-based hyperelasticity models to simulate deformable solids and thin-shells.</p>
<p>Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al.,</p>
<p>2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with ground truth supervision (Asenov et al., 2019; Wu et al., 2016; Xu et al., 2019b).</p>
<p>Sensor readings have been used to predicting the effects of forces applied to an object in models of learned (Fragkiadaki et al., 2016; Byravan \&amp; Fox, 2017) and intuitive physics (Ehsani et al., 2020; Mottaghi et al., 2015; 2016; Gupta et al., 2010; Ehrhardt et al., 2018; Yu et al., 2015; Battaglia et al., 2013; Mann et al., 1997; Innamorati et al., 2019; Standley et al., 2017). This also includes approaches that learn to model multi-object interactions (Watters et al., 2017; Xu et al., 2019b; Janner et al., 2019; Ehrhardt et al., 2017; Chang et al., 2016; Agrawal et al., 2016). In many cases, intuitive physics approaches are limited in their prediction horizon and treatment of complex scenes, as they do not sufficiently accurately model the 3D geometry nor the object properties. System identification based on parameterized physics models (Salzmann \&amp; Urtasun, 2011; Brubaker et al., 2010; Kozlowski, 1998; Wensing et al., 2018; Brubaker et al., 2009; Bhat et al., 2003; 2002; Liu et al., 2005; Grzeszczuk et al., 1998; Sutanto et al., 2020; Wang et al., 2020; 2018a) and inverse simulation (Murray-Smith, 2000) are closely related areas.</p>
<p>There is a rich literature on neural image synthesis, but we focus on methods that model the 3D scene structure, including voxels (Henzler et al., 2019; Paschalidou et al., 2019; Smith et al., 2018b; Nguyen-Phuoc et al., 2018; Gao et al., 2020), meshes (Smith et al., 2020; Wang et al., 2018b; Groueix et al., 2018; Alhaija et al., 2018; Zhang et al., 2021), and implicit shapes (Xu et al., 2019a; Chen \&amp; Zhang, 2019; Michalkiewicz et al., 2019; Niemeyer et al., 2020; Park et al., 2019; Mescheder et al., 2019; Takikawa et al., 2021). Generative models condition the rendering process on samples of the 3D geometry (Liao et al., 2019). Latent factors determining 3D structure have also been learned in generative models (Chen et al., 2016; Eslami et al., 2018). Additionally, implicit neural representations that leverage differentiable rendering have been proposed (Mildenhall et al., 2020; 2019) for realistic view synthesis. Many of these representations have become easy to manipulate through software frameworks like Kaolin (Jatavallabhula et al., 2019), Open3D (Zhou et al., 2018), and PyTorch3D (Ravi et al., 2020).
Differentiable rendering allows for image gradients to be computed w.r.t. the scene geometry, camera, and lighting inputs. Variants based on the rasterization paradigm (NMR (Kato et al., 2018), OpenDR (Loper \&amp; Black, 2014), SoftRas (Liu et al., 2019)) blur the edges of scene triangles prior to image projection to remove discontinuities in the rendering signal. DIB-R (Chen et al., 2019) applies this idea to background pixels and proposes an interpolation-based rasterizer for foreground pixels. More sophisticated differentiable renderers can treat physics-based light transport processes ( Li et al., 2018; Nimier-David et al., 2019) by ray tracing, and more readily support higher-order effects such as shadows, secondary light bounces, and global illumination.</p>
<h1>5 CONCLUSION</h1>
<p>We presented $\nabla$ Sim, a versatile differentiable simulator that enables system identification from videos by differentiating through physical processes governing dyanmics and image formation. We demonstrated the benefits of such a holistic approach by estimating physical attributes for timeevolving scenes with complex dynamics and deformations, all from raw video observations. We also demonstrated the applicability of this efficient and accurate estimation scheme on end-to-end visuomotor control tasks. The latter case highlights $\nabla$ Sim's efficient integration with PyTorch, facilitating interoperability with existing machine learning modules. Interesting avenues for future work include extending our differentiable simulation to contact-rich motion, articulated bodies and higher-fidelity physically-based renderers - doing so takes us closer to operating in the real-world.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>KM and LP thank the IVADO fundamental research project grant for funding. FG thanks CIFAR for project funding under the Catalyst program. FS and LP acknowledge partial support from NSERC.</p>
<h1>REFERENCES</h1>
<p>Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org. 3,17</p>
<p>Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. Neural Information Processing Systems, 2016. 9</p>
<p>Hassan Abu Alhaija, Siva Karthik Mustikovela, Andreas Geiger, and Carsten Rother. Geometric image synthesis. In Proceedings of Computer Vision and Pattern Recognition, 2018. 9</p>
<p>Martin Asenov, Michael Burke, Daniel Angelov, Todor Davchev, Kartic Subr, and Subramanian Ramamoorthy. Vid2Param: Modelling of dynamics parameters from video. IEEE Robotics and Automation Letters, 2019. 9</p>
<p>Peter W. Battaglia, Jessica B. Hamrick, and Joshua B. Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):18327-18332, 2013. ISSN 0027-8424. doi: 10.1073/pnas. 1306572110.9</p>
<p>Kiran S Bhat, Steven M Seitz, Jovan Popović, and Pradeep K Khosla. Computing the physical parameters of rigid-body motion from video. In Proceedings of the European Conference on Computer Vision, 2002. 9</p>
<p>Kiran S Bhat, Christopher D Twigg, Jessica K Hodgins, Pradeep Khosla, Zoran Popovic, and Steven M Seitz. Estimating cloth simulation parameters from video. In ACM SIGGRAPH/Eurographics Symposium on Computer Animation, 2003. 9</p>
<p>James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. 3, 17</p>
<p>Robert Bridson, Sebastian Marino, and Ronald Fedkiw. Simulation of clothing with folds and wrinkles. In ACM SIGGRAPH 2005 Courses, 2005. 17</p>
<p>Marcus Brubaker, David Fleet, and Aaron Hertzmann. Physics-based person tracking using the anthropomorphic walker. International Journal of Computer Vision, 87:140-155, 03 2010. 9</p>
<p>Marcus A Brubaker, Leonid Sigal, and David J Fleet. Estimating contact dynamics. In Proceedings of International Conference on Computer Vision, 2009. 9</p>
<p>Arunkumar Byravan and Dieter Fox. SE3-Nets: Learning rigid body motion using deep neural networks. IEEE International Conference on Robotics and Automation (ICRA), 2017. 9</p>
<p>Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. ShapeNet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 4, 8</p>
<p>Michael B. Chang, Tomer Ullman, Antonio Torralba, and Joshua B. Tenenbaum. A compositional object-based approach to learning physical dynamics. International Conference on Learning Representations, 2016. 1, 9</p>
<p>Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In Neural Information Processing Systems, 2018. 3, 17</p>
<p>Wenzheng Chen, Jun Gao, Huan Ling, Edward Smith, Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learning to predict 3d objects with an interpolation-based differentiable renderer. Neural Information Processing Systems, 2019. 3, 9, 19</p>
<p>Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. Neural Information Processing Systems, 2016. 1, 9</p>
<p>Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and Léon Bottou. Symplectic recurrent neural networks. In International Conference on Learning Representations, 2020. 8</p>
<p>Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. Proceedings of Computer Vision and Pattern Recognition, 2019. 9</p>
<p>Erwin Coumans and Yunfei Bai. PyBullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2019. 5, 22</p>
<p>Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. In National Academy of Sciences (NAS), 2020a. 2</p>
<p>Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho. Lagrangian neural networks. In ICLR Workshops, 2020b. 1, 9, 20</p>
<p>Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, and J. Zico Kolter. End-to-end differentiable physics for learning and control. In Neural Information Processing Systems, 2018. 3, 8, 19, 20, 25</p>
<p>Jonas Degrave, Michiel Hermans, Joni Dambre, and Francis Wyffels. A differentiable physics engine for deep learning in robotics. Neural Information Processing Systems, 2016. 1, 3, 8, 20</p>
<p>Sébastien Ehrhardt, Aron Monszpart, Niloy J. Mitra, and Andrea Vedaldi. Learning a physical long-term predictor. arXiv, 2017. 9</p>
<p>Sébastien Ehrhardt, Aron Monszpart, Niloy J. Mitra, and Andrea Vedaldi. Unsupervised intuitive physics from visual observations. Asian Conference on Computer Vision, 2018. 9</p>
<p>Kiana Ehsani, Shubham Tulsiani, Saurabh Gupta, Ali Farhadi, and Abhinav Gupta. Use the Force, Luke! learning to predict physical forces by simulating effects. In Proceedings of Computer Vision and Pattern Recognition, 2020. 5, 9, 23</p>
<p>Tom Erez, Yuval Tassa, and Emanuel Todorov. Simulation tools for model-based robotics: Comparison of Bullet, Havok, MuJoCo, ODE, and PhysX. In IEEE International Conference on Robotics and Automation (ICRA), 2015. 17, 18, 19
S. M. Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S. Morcos, Marta Garnelo, Avraham Ruderman, Andrei A. Rusu, Ivo Danihelka, Karol Gregor, David P. Reichert, Lars Buesing, Theophane Weber, Oriol Vinyals, Dan Rosenbaum, Neil Rabinowitz, Helen King, Chloe Hillier, Matt Botvinick, Daan Wierstra, Koray Kavukcuoglu, and Demis Hassabis. Neural scene representation and rendering. Science, 2018. 9</p>
<p>Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, and Jitendra Malik. Learning visual predictive models of physics for playing billiards. In International Conference on Learning Representations, 2016. 9</p>
<p>Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Learning deformable tetrahedral meshes for 3d reconstruction. In Neural Information Processing Systems, 2020. 9</p>
<p>Sam Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Neural Information Processing Systems, 2019. 1, 8, 20</p>
<p>Andreas Griewank and Andrea Walther. Introduction to automatic differentiation. PAMM, 2(1): $45-49,2003.2$</p>
<p>Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, and Mathieu Aubry. Atlasnet: A papier-mâché approach to learning 3d surface generation. In Proceedings of Computer Vision and Pattern Recognition, 2018. 9</p>
<p>Radek Grzeszczuk, Demetri Terzopoulos, and Geoffrey Hinton. Neuroanimator: Fast neural network emulation and control of physics-based models. In Proceedings of the 25th annual conference on Computer graphics and interactive techniques, 1998. 9</p>
<p>Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for unsupervised video prediction. In Proceedings of Computer Vision and Pattern Recognition, 2020. 8</p>
<p>Abhinav Gupta, Alexei A. Efros, and Martial Hebert. Blocks world revisited: Image understanding using qualitative geometry and mechanics. In Proceedings of the European Conference on Computer Vision, 2010. 9</p>
<p>Ernst Hairer, Christian Lubich, and Gerhard Wanner. Geometric numerical integration: structurepreserving algorithms for ordinary differential equations, volume 31. Springer Science \&amp; Business Media, 2006. 18</p>
<p>Eric Heiden, David Millard, Hejia Zhang, and Gaurav S. Sukhatme. Interactive differentiable simulation. In arXiv, 2019. 8</p>
<p>Philipp Henzler, Niloy J. Mitra, and Tobias Ritschel. Escaping plato's cave using adversarial training: 3d shape from unstructured 2d image collections. In Proceedings of International Conference on Computer Vision, 2019. 9</p>
<p>Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu, Andre Pradhana, and Chenfanfu Jiang. A moving least squares material point method with displacement discontinuity and two-way rigid body coupling. ACM Transactions on Graphics, 37(4), 2018. 8</p>
<p>Yuanming Hu, Jiancheng Liu, Andrew Spielberg, Joshua B. Tenenbaum, William T. Freeman, Jiajun Wu, Daniela Rus, and Wojciech Matusik. Chainqueen: A real-time differentiable physical simulator for soft robotics. In IEEE International Conference on Robotics and Automation (ICRA), 2019. 8, 17</p>
<p>Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Frédo Durand. DiffTaichi: Differentiable programming for physical simulation. International Conference on Learning Representations, 2020. 2, 3, 8, 17, 19, 20</p>
<p>Carlo Innamorati, Bryan Russell, Danny Kaufman, and Niloy Mitra. Neural re-simulation for generating bounces in single images. In Proceedings of International Conference on Computer Vision, 2019. 9</p>
<p>Michael Janner, Sergey Levine, William T. Freeman, Joshua B. Tenenbaum, Chelsea Finn, and Jiajun Wu. Reasoning about physical interactions with object-oriented prediction and planning. International Conference on Learning Representations, 2019. 1, 9</p>
<p>Miguel Jaques, Michael Burke, and Timothy M. Hospedales. Physics-as-inverse-graphics: Joint unsupervised learning of objects and physics from video. International Conference on Learning Representations, 2020. 8</p>
<p>Krishna Murthy Jatavallabhula, Edward Smith, Jean-Francois Lafleche, Clement Fuji Tsang, Artem Rozantsev, Wenzheng Chen, Tommy Xiang, Rev Lebaredian, and Sanja Fidler. Kaolin: A pytorch library for accelerating 3d deep learning research. In arXiv, 2019. 9</p>
<p>Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In Proceedings of Computer Vision and Pattern Recognition, 2018. 3, 9</p>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. 6, 24</p>
<p>David Kirk et al. Nvidia cuda software and gpu parallel computing architecture. In ISMM, volume 7, pp. 103-104, 2007. 17</p>
<p>Krzysztof Kozlowski. Modelling and Identification in Robotics. Advances in Industrial Control. Springer, London, 1998. ISBN 978-1-4471-1139-9. 9</p>
<p>T. D. Kulkarni, P. Kohli, J. B. Tenenbaum, and V. Mansinghka. Picture: A probabilistic programming language for scene perception. In Proceedings of Computer Vision and Pattern Recognition, 2015. 2</p>
<p>Tzu-Mao Li, Miika Aittala, Frédo Durand, and Jaakko Lehtinen. Differentiable monte carlo ray tracing through edge sampling. SIGGRAPH Asia, 37(6):222:1-222:11, 2018. 9</p>
<p>Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenenbaum, and Antonio Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. In International Conference on Learning Representations, 2019. 8</p>
<p>Yunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel L. K. Yamins, Jiajun Wu, Joshua B. Tenenbaum, and Antonio Torralba. Visual grounding of learned physical models. In International Conference on Machine Learning, 2020. 8</p>
<p>Junbang Liang, Ming Lin, and Vladlen Koltun. Differentiable cloth simulation for inverse problems. In Neural Information Processing Systems, 2019. 2, 8</p>
<p>Yiyi Liao, Katja Schwarz, Lars Mescheder, and Andreas Geiger. Towards unsupervised learning of generative models for 3d controllable image synthesis. In Proceedings of Computer Vision and Pattern Recognition, 2019. 9</p>
<p>C Karen Liu, Aaron Hertzmann, and Zoran Popović. Learning physics-based motion style with nonlinear inverse optimization. ACM Transactions on Graphics (TOG), 24(3):1071-1081, 2005. 9</p>
<p>Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for image-based 3d reasoning. Proceedings of International Conference on Computer Vision, 2019. 3, 9,19</p>
<p>Matthew M. Loper and Michael J. Black. Opendr: An approximate differentiable renderer. In Proceedings of the European Conference on Computer Vision, 2014. 3, 9</p>
<p>Miles Macklin, Matthias Müller, Nuttapong Chentanez, and Tae-Yong Kim. Unified particle physics for real-time applications. ACM Transactions on Graphics (TOG), 33(4):1-12, 2014. 2</p>
<p>Dougal Maclaurin, David Duvenaud, Matt Johnson, and Jamie Townsend. Autograd, 2015. URL https://github.com/HIPS/autograd. 20</p>
<p>Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. In Robotics Science and Systems, 2017. 4</p>
<p>Richard Mann, Allan Jepson, and Jeffrey Mark Siskind. The computational perception of scene dynamics. Computer Vision and Image Understanding, 65(2):113 - 128, 1997. 9</p>
<p>Charles C Margossian. A review of automatic differentiation and its efficient implementation. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(4):e1305, 2019. 20</p>
<p>Viraj Mehta, Ian Char, Willie Neiswanger, Youngseog Chung, Andrew Oakleigh Nelson, Mark D Boyer, Egemen Kolemen, and Jeff Schneider. Neural dynamical systems: Balancing structure and flexibility in physical prediction. ICLR Workshops, 2020. 1</p>
<p>Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of Computer Vision and Pattern Recognition, 2019. 9</p>
<p>Mateusz Michalkiewicz, Jhony K. Pontes, Dominic Jack, Mahsa Baktashmotlagh, and Anders Eriksson. Implicit surface representations as layers in neural networks. In Proceedings of International Conference on Computer Vision, 2019. 9</p>
<p>Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 2019. 9</p>
<p>Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision, 2020. 9</p>
<p>Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, and Ali Farhadi. Newtonian image understanding: Unfolding the dynamics of objects in static images. Proceedings of Computer Vision and Pattern Recognition, 2015. 9</p>
<p>Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, and Ali Farhadi. "what happens if..." learning to predict the effect of forces in images. In Proceedings of the European Conference on Computer Vision, 2016. 9
D.J. Murray-Smith. The inverse simulation approach: a focused review of methods and applications. Mathematics and Computers in Simulation, 53(4):239 - 247, 2000. ISSN 0378-4754. 9</p>
<p>Thu Nguyen-Phuoc, Chuan Li, Stephen Balaban, and Yong-Liang Yang. Rendernet: A deep convolutional network for differentiable rendering from 3d shapes. Neural Information Processing Systems, 2018. 9</p>
<p>Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of Computer Vision and Pattern Recognition, 2020. 9</p>
<p>Merlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wenzel Jakob. Mitsuba 2: A retargetable forward and inverse renderer. Transactions on Graphics (Proceedings of SIGGRAPH Asia), 38(6), 2019. 9</p>
<p>Jeong Joon Park, Peter Florence, Julian Straub, Richard A. Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of Computer Vision and Pattern Recognition, 2019. 9</p>
<p>Despoina Paschalidou, Ali Osman Ulusoy, Carolin Schmitt, Luc van Gool, and Andreas Geiger. Raynet: Learning volumetric 3d reconstruction with ray potentials. In Proceedings of Computer Vision and Pattern Recognition, 2019. 9</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems, 2019. 3, 17</p>
<p>Matt Pharr, Wenzel Jakob, and Greg Humphreys. Physically Based Rendering: From Theory to Implementation. Morgan Kaufmann Publishers Inc., 2016. ISBN 0128006455. 2, 7</p>
<p>Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, and Ming C Lin. Scalable differentiable physics for learning and control. International Conference on Machine Learning, 2020. 2, 3</p>
<p>Fabio Ramos, Rafael Carvalhaes Possas, and Dieter Fox. Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators. Robotics Science and Systems, 2019. 2</p>
<p>Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3d deep learning with pytorch3d. arXiv preprint arXiv:2007.08501, 2020. 9</p>
<p>Google Research. Tiny Differentiable Simulator, 2020 (accessed May 15, 2020). URL https : //github.com/google-research/tiny-differentiable-simulator. 3, 8</p>
<p>Danilo Jimenez Rezende, SM Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, and Nicolas Heess. Unsupervised learning of 3d structure from images. In Neural Information Processing Systems, 2016. 23</p>
<p>Mathieu Salzmann and Raquel Urtasun. Physically-based motion models for 3d tracking: A convex formulation. In Proceedings of International Conference on Computer Vision, 2011. 9</p>
<p>Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian graph networks with ode integrators. In arXiv, 2019. 9, 20</p>
<p>Connor Schenck and Dieter Fox. Spnets: Differentiable fluid dynamics for deep neural networks. In International Conference on Robot Learning, 2018. 8</p>
<p>Eftychios Sifakis and Jernej Barbic. Fem simulation of 3d deformable solids: a practitioner's guide to theory, discretization and model reduction. In ACM SIGGRAPH 2012 courses, 2012. 17</p>
<p>Breannan Smith, Fernando De Goes, and Theodore Kim. Stable neo-hookean flesh simulation. ACM Transactions on Graphics, 37(2):1-15, 2018a. 17, 19</p>
<p>Edward Smith, Scott Fujimoto, and David Meger. Multi-view silhouette and depth decomposition for high resolution 3d object representation. In Neural Information Processing Systems, 2018b. 9</p>
<p>Edward J. Smith, Scott Fujimoto, Adriana Romero, and David Meger. Geometrics: Exploiting geometric structure for graph-encoded objects. International Conference on Machine Learning, 2020. 9</p>
<p>Changkyu Song and Abdeslam Boularias. Identifying mechanical models through differentiable simulations. In Learning for Dynamical Systems and Control (L4DC), 2020a. 3, 8</p>
<p>Changkyu Song and Abdeslam Boularias. Learning to slide unknown objects with differentiable physics simulations. In Robotics Science and Systems, 2020b. 3, 8</p>
<p>Jos Stam. Stable fluids. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pp. 121-128, 1999. 18, 20</p>
<p>Trevor Standley, Ozan Sener, Dawn Chen, and Silvio Savarese. image2mass: Estimating the mass of an object from its image. In International Conference on Robot Learning, 2017. 9</p>
<p>Giovanni Sutanto, Austin S. Wang, Yixin Lin, Mustafa Mukadam, Gaurav S. Sukhatme, Akshara Rai, and Franziska Meier. Encoding physical constraints in differentiable newton-euler algorithm. In Learning for Dynamical systems and Control (L4DC), 2020. 9</p>
<p>Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. Proceedings of Computer Vision and Pattern Recognition, 2021. 9</p>
<p>Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, 2019. 4, 24</p>
<p>Emanuel Todorov. Convex and analytically-invertible dynamics with contacts and constraints: Theory and implementation in mujoco. In IEEE International Conference on Robotics and Automation (ICRA), 2014. 18</p>
<p>Peter Toth, Danilo Jimenez Rezende, Andrew Jaegle, Sébastien Racanière, Aleksandar Botev, and Irina Higgins. Hamiltonian generative networks. In International Conference on Learning Representations, 2020. 8, 20</p>
<p>Marc Toussaint, Kelsey Allen, Kevin Smith, and Joshua Tenenbaum. Differentiable physics and stable modes for tool-use and manipulation planning. In Robotics Science and Systems, 2018. 3, 8</p>
<p>Bart van Merriënboer, Alexander B Wiltschko, and Dan Moldovan. Tangent: automatic differentiation using source code transformation in python. In Neural Information Processing Systems, 2018. 3, 17</p>
<p>Bin Wang, Paul G. Kry, Yuanmin Deng, Uri M. Ascher, Hui Huang, and Baoquan Chen. Neural material: Learning elastic constitutive material and damping models from sparse data. arXiv, 2018a. 9</p>
<p>Kun Wang, Mridul Aanjaneya, and Kostas Bekris. A first principles approach for data-efficient system identification of spring-rod systems via differentiable physics engines. In arXiv, 2020. 9</p>
<p>Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European Conference on Computer Vision, 2018b. 9</p>
<p>Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, and Andrea Tacchetti. Visual interaction networks: Learning a physics simulator from video. In Neural Information Processing Systems, 2017. 1, 9
P. M. Wensing, S. Kim, and J. E. Slotine. Linear matrix inequalities for physically consistent inertial parameter identification: A statistical perspective on the mass distribution. IEEE Robotics and Automation Letters, 3(1):60-67, 2018. 9</p>
<p>Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229-256, 1992. ISSN 0885-6125. 2, 5, 22</p>
<p>Jiajun Wu, Ilker Yildirim, Joseph J Lim, William T Freeman, and Joshua B Tenenbaum. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. In Neural Information Processing Systems, 2015. 5, 23</p>
<p>Jiajun Wu, Joseph J Lim, Hongyi Zhang, Joshua B Tenenbaum, and William T Freeman. Physics 101: Learning physical object properties from unlabeled videos. In British Machine Vision Conference, 2016. 9</p>
<p>Jiajun Wu, Erika Lu, Pushmeet Kohli, William T Freeman, and Joshua B Tenenbaum. Learning to see physics via visual de-animation. In Neural Information Processing Systems, 2017a. 3, 8</p>
<p>Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In Proceedings of Computer Vision and Pattern Recognition, 2017b. 2</p>
<p>Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. In Neural Information Processing Systems, 2019a. 9</p>
<p>Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B. Tenenbaum, and Shuran Song. Densephysnet: Learning dense physical object representations via multi-step dynamic interactions. In Robotics Science and Systems, 2019b. 1, 5, 9</p>
<p>Ilker Yildirim, Tejas Kulkarni, Winrich Freiwald, and Joshua Tenenbaum. Efficient analysis-bysynthesis in vision: A computational framework, behavioral tests, and comparison with neural representations. In $\operatorname{CogSci}, 2015 .2$</p>
<p>Ilker Yildirim, Michael Janner, Mario Belledonne, Christian Wallraven, W. A. Freiwald, and Joshua B. Tenenbaum. Causal and compositional generative models in online perception. In CogSci, 2017. 2</p>
<p>Ilker Yildirim, Mario Belledonne, Winrich Freiwald, and Josh Tenenbaum. Efficient inverse graphics in biological face processing. Science Advances, 6(10), 2020. doi: 10.1126/sciadv.aax5979. 2
L. Yu, N. Duncan, and S. Yeung. Fill and transfer: A simple physics-based approach for containability reasoning. In Proceedings of International Conference on Computer Vision, 2015. 9</p>
<p>Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio Torralba, and Sanja Fidler. Image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering. In International Conference on Learning Representations, 2021. 9</p>
<p>Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ode-net: Learning hamiltonian dynamics with control. In International Conference on Learning Representations, 2020. 9</p>
<p>Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A modern library for 3D data processing. arXiv:1801.09847, 2018. 9</p>
<h1>A APPENDIX</h1>
<h2>A DIFFERENTIABLE PHYSICS ENGINE</h2>
<p>Under Lagrangian mechanics, the state of a physical system can be described in terms of generalized coordinates $\mathbf{q}$, generalized velocities $\dot{\mathbf{q}}=\mathbf{u}$, and design, or model parameters $\theta$. For the purposes of exposition, we make no distinction between rigid-bodies, deformable solids, or thin-shell models of cloth and other bodies. Although the specific choices of coordinates and parameters vary, the simulation procedure is virtually unchanged. We denote the combined state vector by $\mathbf{s}(t)=$ $[\mathbf{q}(t), \mathbf{u}(t)]$.</p>
<p>The dynamic evolution of the system is governed by a second order differential equations (ODE) of the form $\mathbf{M} \ddot{\mathbf{s}}=\mathbf{f}(\mathbf{s})$, where $\mathbf{M}$ is a mass matrix that may also depend on our state and design parameters $\theta$. Solutions to ODEs of this type may be obtained through black box numerical integration methods, and their derivatives calculated through the continuous adjoint method Chen et al. (2018). However, we instead consider our physics engine as a differentiable operation that provides an implicit relationship between a state vector $\mathbf{s}^{-}=\mathbf{s}(t)$ at the start of a time step, and the updated state at the end of the time step $\mathbf{s}^{+}=\mathbf{s}(t+\Delta t)$. An arbitrary discrete time integration scheme can be then be abstracted as the function $\mathbf{g}\left(\mathbf{s}^{-}, \mathbf{s}^{+}, \theta\right)=\mathbf{0}$, relating the initial and final system state and the model parameters $\theta$. By the implicit function theorem, if we can specify a loss function $l$ at the output of the simulator, we can compute $\frac{\partial l}{\partial \mathbf{s}^{-}}$as $\mathbf{c}^{T} \frac{\partial \mathbf{g}}{\partial \mathbf{s}^{+}}$, where $\mathbf{c}$ is the solution to the linear system $\frac{\partial \mathbf{g}}{\partial \mathbf{s}^{+}} T_{\mathbf{c}}=-\frac{\partial l}{\partial \mathbf{s}^{+}} T$, and likewise for the model parameters $\theta$.
While the partial derivatives $\frac{\partial \mathbf{g}}{\partial \mathbf{s}^{-}}, \frac{\partial \mathbf{g}}{\partial \mathbf{s}^{+}}, \frac{\partial \mathbf{g}}{\partial \theta}$ can be computed by graph-based automatic differentation frameworks Paszke et al. (2019); Abadi et al. (2015); Bradbury et al. (2018), program transformation approaches such as DiffTaichi, and Google Tangent Hu et al. (2020); van Merriënboer et al. (2018) are particularly well-suited to simulation code. We use an embedded subset of Python syntax, which computes the adjoint of each simulation kernel at runtime, and generates C++/CUDA Kirk et al. (2007) code. Kernels are wrapped as custom autograd operations on PyTorch tensors, which allows users to focus on the definition of physical models, and leverage the PyTorch tape-based autodiff to track the overall program flow. While this formulation is general enough to represent explicit, multi-step, or fully implicit time-integration schemes, we employ semi-implicit Euler integration, which is the preferred integration scheme for most simulators Erez et al. (2015).</p>
<h2>A. 1 PHYSICAL MODELS</h2>
<p>We now discuss some of the physical models available in $\nabla$ Sim.
Deformable Solids: In contrast with existing simulators that use grid-based methods for differentiable soft-body simulation Hu et al. (2019; 2020), we adopt a finite element (FEM) model with constant strain tetrahedral elements common in computer graphics Sifakis \&amp; Barbic (2012). We use the stable Neo-Hookean constitutive model of Smith et al. Smith et al. (2018a) that derives per-element forces from the following strain energy density:</p>
<p>$$
\Psi(\mathbf{q}, \theta)=\frac{\mu}{2}\left(I_{C}-3\right)+\frac{\lambda}{2}(J-\alpha)^{2}-\frac{\mu}{2} \log \left(I_{C}+1\right)
$$</p>
<p>where $I_{C}, J$ are invariants of strain, $\theta=[\mu, \lambda]$ are the Lamé parameters, and $\alpha$ is a per-element actuation value that allows the element to expand and contract.
Numerically integrating the energy density over each tetrahedral mesh element with volume $V_{e}$ gives the total elastic potential energy, $U(\mathbf{q}, \theta)=\sum V_{e} \Psi_{e}$. The forces due to this potential $\mathbf{f}<em _mathbf_q="\mathbf{q">{e}(\mathbf{s}, \theta)=$ $-\nabla</em>, \theta)$, can computed analytically, and their gradients obtained using the adjoint method (cf. Section 2.1).
Deformable Thin-Shells: To model thin-shells such as clothing, we use constant strain triangular elements embedded in 3D. The Neo-Hookean constitutive model above is applied to model in-plane elastic deformation, with the addition of a bending energy $\mathbf{f}}} U(\mathbf{q<em b="b">{b}(\mathbf{s}, \theta)=k</em>$ is the force direction given by Bridson} \sin \left(\frac{\phi}{2}+\alpha\right) \mathbf{d}$, where $k_{b}$ is the bending stiffness, $\phi$ is the dihedral angle between two triangular faces, $\alpha$ is a per-edge actuation value that allows the mesh to flex inwards or outwards, and $\mathbf{d</p>
<p>et al. (2005). We also include a lift/drag model that approximates the effect of the surrounding air on the surface of mesh.</p>
<p>Rigid Bodies: We represent the state of a 3D rigid body as $\mathbf{q}<em b="b">{b}=[\mathbf{x}, \mathbf{r}]$ consisting of a position $\mathbf{x} \in \mathbb{R}^{3}$, and a quaternion $\mathbf{r} \in \mathbb{R}^{4}$. The generalized velocity of a body is $\mathbf{u}</em>, \omega]$ and the dynamics of each body is given by the Newton-Euler equations,}=[\mathbf{v</p>
<p>$$
\left[\begin{array}{cc}
m &amp; \mathbf{0} \
\mathbf{0} &amp; \mathbf{I}
\end{array}\right]\left[\begin{array}{c}
\dot{\mathbf{v}} \
\dot{\omega}
\end{array}\right]=\left[\begin{array}{c}
\mathbf{f} \
\tau
\end{array}\right]-\left[\begin{array}{c}
\mathbf{0} \
\omega \times \mathbf{I} \omega
\end{array}\right]
$$</p>
<p>where the mass $m$ and inertia matrix $\mathbf{I}$ (expressed at the center of mass) are considered design parameters $\theta$.
Contact: We adopt a compliant contact model that associates elastic and damping forces with each nodal contact point. The model is parameterized by four scalars $\theta=\left[k_{e}, k_{d}, k_{f}, \mu\right]$, corresponding to elastic stiffness, damping, frictional stiffness, and friction coefficient respectively. To prevent interpenetration we use a proportional penalty-based force, $\mathbf{f}<em e="e">{n}(\mathbf{s}, \theta)=-\mathbf{n}\left[k</em>} C(\mathbf{q})+k_{d} \dot{C}(\mathbf{u})\right]$, where $\mathbf{n}$ is a contact normal, and $C$ is a gap function measure of overlap projected on to $\mathbb{R}^{+}$. We model friction using a relaxed Coulomb model Todorov (2014) $\mathbf{f<em n="n">{f}(\mathbf{s}, \theta)=-\mathbf{D}\left[\min \left(\mu\left|\mathbf{f}</em>}\right|, k_{f} \mathbf{u<em s="s">{s}\right)\right]$, where $\mathbf{D}$ is a basis of the contact plane, and $\mathbf{u}</em>$ continuous, we found that this was sufficient for optimization over a variety of objectives.
More physical simulations: We also implement a number of other differentiable simulations such as pendula, mass-springs, and incompressible fluids Stam (1999). We note these systems have already been demonstrated in prior art, and thus focus on the more challenging systems in our paper.}=\mathbf{D}^{T} \mathbf{u}$ is the sliding velocity at the contact point. While these forces are only $C^{0</p>
<h1>B Discrete Adjoint Method</h1>
<p>Above, we presented a formulation of time-integration using the discrete adjoint method that represents an arbitrary time-stepping scheme through the implicit relation,</p>
<p>$$
\mathbf{g}\left(\mathbf{s}^{-}, \mathbf{s}^{+}, \theta\right)=\mathbf{0}
$$</p>
<p>This formulation is general enough to represent both explicit or implicit time-stepping methods. While explicit methods are often simple to implement, they may require extremely small time-steps for stability, which is problematic for reverse-mode automatic differentiation frameworks that must explicitly store the input state for each discrete timestep invocation of the integration routine. On the other hand, implicit methods can introduce computational overhead or unwanted numerical dissipation Hairer et al. (2006). For this reason, many real-time physics engines employ a semiimplicit (symplectic) Euler integration scheme Erez et al. (2015), due to its ease of implementation and numerical stability in most meaningful scenarios (conserves energy for systems where the Hamiltonian is time-invariant).
We now give a concrete example of the discrete adjoint method applied to semi-implicit Euler. For the state variables defined above, the integration step may be written as follows,</p>
<p>$$
\mathbf{g}\left(\mathbf{s}^{-}, \mathbf{s}^{+}, \theta\right)=\left[\begin{array}{l}
\mathbf{u}^{+}-\mathbf{u}^{-}-\Delta t \mathbf{M}^{-1} \mathbf{f}\left(\mathbf{s}^{-}\right) \
\mathbf{q}^{+}-\mathbf{q}^{-}-\Delta t \mathbf{u}^{+}
\end{array}\right]=\mathbf{0}
$$</p>
<p>Note that in general, the mass matrix $\mathbf{M}$ is a function of $\mathbf{q}$ and $\theta$. For conciseness we only consider the dependence on $\theta$, although the overall procedure is unchanged in the general case. We provide a brief sketch of computing the gradients of $\mathbf{g}\left(\mathbf{s}^{-}, \mathbf{s}^{+}, \theta\right)$. In the case of semi-implicit integration above, these are given by the following equations:</p>
<p>$$
\frac{\partial \mathbf{g}}{\partial \mathbf{s}^{-}}=\left[\begin{array}{cc}
-\Delta t \mathbf{M}^{-1} \frac{\partial \mathbf{f}}{\partial \mathbf{q}(t)} &amp; -\mathbf{I}-\Delta t \mathbf{M}^{-1} \frac{\partial \mathbf{f}}{\partial \mathbf{u}(t)} \
-\mathbf{I} &amp; 0
\end{array}\right] \quad \frac{\partial \mathbf{g}}{\partial \mathbf{s}^{+}}=\left[\begin{array}{cc}
0 &amp; \mathbf{I} \
\mathbf{I} &amp; -\Delta t \mathbf{I}
\end{array}\right] \quad \frac{\partial \mathbf{g}}{\partial \theta}=\left[\begin{array}{c}
-\Delta t \frac{\partial \mathbf{M}^{-1}}{\partial \theta} \
\mathbf{0}
\end{array}\right]
$$</p>
<p>In the case of semi-implicit Euler, the triangular structure of these Jacobians allows the adjoint variables to be computed explicitly. For fully implicit methods such as backwards Euler, the Jacobians may create a linear system that must be first solved to generate adjoint variables.</p>
<h1>C Physical Models</h1>
<p>We now undertake a more detailed discussion of the physical models implemented in $\nabla$ Sim.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(a) Triangular FEM element
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) Tetrahedral FEM element</p>
<p>Figure 8: Mesh Discretization: We use triangular (a) and tetrahedral (b) FEM models with angle-based and volumetric activation parameters, $\alpha$. These mesh-based discretizations are a natural fit for our differentiable rasterization pipeline, which is designed to operate on triangles.</p>
<h2>C. 1 Finite element MEthod</h2>
<p>As described in section 3.2 ("Physical models"), we use a hyperelastic constitutive model based on the neo-Hookean model of Smith et al. Smith et al. (2018a):</p>
<p>$$
\Psi(\mathbf{q}, \theta)=\frac{\mu}{2}\left(I_{C}-3\right)+\frac{\lambda}{2}(J-\alpha)^{2}-\frac{\mu}{2} \log \left(I_{C}+1\right)
$$</p>
<p>The Lamé parameters, $\lambda, \mu$, control the element's resistance to shearing and volumetric strains. These may be specified on a per-element basis, allowing us to represent heterogeneous materials. In contrast to other work using particle-based models Hu et al. (2020), we adopt a mesh-based discretization for deformable shells and solids. For thin-shells, such as cloth, the surface is represented by a triangle mesh as in Figure 8a, enabling straightforward integration with our triangle mesh-based differentiable rasterizer Liu et al. (2019); Chen et al. (2019). For solids, we use a tetrahedral FEM model as illustrated in Figure 8b. Both these models include a per-element activation parameter $\alpha$, which for thin-shells, allows us to control the relative dihedral angle between two connected faces. For tetrahedral meshes, this enables changing the element's volume, enabling locomotion, as in the control-fem example.</p>
<h2>C. 2 CONTACT</h2>
<p>Implicit contact methods based on linear complementarity formulations (LCP) of contact may be used to maintain hard non-penetration constraints de Avila Belbute-Peres et al. (2018). However, we found relaxed models of contact-used in typical physics engines Erez et al. (2015)—were sufficient for our experiments. In this approach, contact forces are derived from a one-sided quadratic potential, giving rise to penalty forces of the form 9a. While Coulomb friction may also be modeled as an LCP, we use a relaxed model where the stick regime is represented by a stiff quadratic potential around the origin, and a linear portion in the slip regime, as shown in Figure 9b. To generate contacts, we test each vertex of a mesh against a collision plane and introduce a contact within some distance threshold $d$.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Contact Model: To model non-interpenetration constraints we use a relaxed model of contact that replaces a delta function with a linear hinge corresponding to a quadratic penalty energy (a). To model friction we use a relaxed Coulomb model, that replaces the step function with a symmetric hinge (b).</p>
<h1>C. 3 Pendula</h1>
<p>We also implement simple and double pendula, as toy examples of well-behaved and chaotic systems respectively, and estimate the parameters of the system (i.e., the length(s) of the rod(s) and initial angular displacement(s)), by comparing the rendered videos (assuming uniformly random initial guesses) with the true videos. As pendula have extensively been studied in the context of differentiable physics simulation Degrave et al. (2016); de Avila Belbute-Peres et al. (2018); Cranmer et al. (2020b); Toth et al. (2020); Greydanus et al. (2019); Sanchez-Gonzalez et al. (2019), we focus on more challenging systems which have not been studied in prior art.</p>
<h2>C. 4 INCOMPRESSIBLE FLUIDS</h2>
<p>As an example of incompressible fluid simulation, we implement a smoke simulator following the popular semi-Lagrangian advection scheme of Stam et al. Stam (1999). At 2:20 in our supplementary video attachment, we show an experiment which optimizes the initial velocities of smoke particles to form a desired pattern. Similar schemes have already been realized differentiably, e.g. in DiffTaichi Hu et al. (2020) and autograd Maclaurin et al. (2015).</p>
<h2>D SOURCE-CODE TRANSFORMATION FOR AUTOMATIC DIFFERENTIATION</h2>
<p>The discrete adjoint method requires computing gradients of physical quantities with respect to state and design parameters. To do so, we adopt a source code transformation approach to perform reverse mode automatic differentiation Hu et al. (2020); Margossian (2019). We use a domain-specific subset of the Python syntax extended with primitves for representing vectors, matrices, and quaternions. Each type includes functions for acting on them, and the corresponding adjoint method. An example simulation kernel is then defined as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="err">@</span><span class="n">kernel</span>
<span class="n">def</span><span class="w"> </span><span class="n">integrate_particles</span><span class="p">(</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">(</span><span class="n">float3</span><span class="p">),</span>
<span class="w">    </span><span class="n">v</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">(</span><span class="n">float3</span><span class="p">),</span>
<span class="w">    </span><span class="n">f</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">(</span><span class="n">float3</span><span class="p">),</span>
<span class="w">    </span><span class="n">w</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">(</span><span class="nb nb-Type">float</span><span class="p">),</span>
<span class="w">    </span><span class="n">gravity</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">(</span><span class="n">float3</span><span class="p">),</span>
<span class="w">    </span><span class="n">dt</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="nb nb-Type">float</span><span class="p">,</span>
<span class="w">    </span><span class="n">x_new</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">(</span><span class="n">float3</span><span class="p">),</span>
<span class="w">    </span><span class="n">v_new</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">(</span><span class="n">float3</span><span class="p">)</span>
<span class="p">):</span>
<span class="c1"># Get thread ID</span>
<span class="n">thread_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">()</span>
<span class="c1"># Load state variables and parameters</span>
<span class="n">x0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">load</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">thread_id</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">v0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">load</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">thread_id</span><span class="p">)</span>
<span class="n">f0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">thread_id</span><span class="p">)</span>
<span class="n">inv_mass</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">load</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="n">thread_id</span><span class="p">)</span>
<span class="c1"># Load external forces</span>
<span class="n">g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">load</span><span class="p">(</span><span class="n">gravity</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Semi-implicit Euler</span>
<span class="n">v1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">v0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">f0</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">inv_mass</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">g</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">step</span><span class="p">(</span><span class="n">inv_mass</span><span class="p">))</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dt</span>
<span class="n">x1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">v1</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dt</span>
<span class="c1"># Store results</span>
<span class="n">store</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span><span class="w"> </span><span class="n">thread_id</span><span class="p">,</span><span class="w"> </span><span class="n">x1</span><span class="p">)</span>
<span class="n">store</span><span class="p">(</span><span class="n">v_new</span><span class="p">,</span><span class="w"> </span><span class="n">thread_id</span><span class="p">,</span><span class="w"> </span><span class="n">v1</span><span class="p">)</span>
</code></pre></div>

<p>Listing 1: Particle Integration Kernel</p>
<p>At runtime, the kernel's abstract syntax tree (AST) is parsed using Python's built-in ast module. We then generate C++ kernel code for forward and reverse mode, which may be compiled to a CPU or GPU executable using the PyTorch torch.utils.cpp_extension mechanism.</p>
<p>This approach allows writing imperative code, with fine-grained indexing and implicit operator fusion (since all operations in a kernel execute as one GPU kernel launch). Each kernel is wrapped as a PyTorch autograd operation so that it fits natively into the larger computational graph.</p>
<h1>E MPC Controller Architecture</h1>
<p>For our model predictive control examples, we use a simple 3-layer neural network architecture illustrated in Figure 10. With simulation time $t$ as input we generate $N$ phase-shifted sinusoidal signals which are passed to a fully-connected layer (zero-bias), and a final activation layer. The output is a vector of per-element activation values as described in the previous section.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Our simple network architecture used the for control-walker and control-fem tasks.</p>
<h2>F LOSS LANDSCAPES FOR PARAMETER ESTIMATION OF DEFORMABLE SOLIDS</h2>
<p>$\nabla$ Sim integrates several functional blocks, many of which contain nonlinear operations. Furthermore, we employ a pixelwise mean-squared error (MSE) loss function for estimating physical parameters from video. To demonstrate whether the gradients obtained from $\nabla \operatorname{Sim}$ are relevant for the task of physical parameter estimation, in Figure 2 of the main paper, we present an analysis of the MSE loss landscape for mass estimation.</p>
<h2>F. 1 ELASTICITY PARAMETER</h2>
<p>We now present a similar analysis for elasticity parameter estimation in deformable solids. Figure 11a shows the loss landscape when optimizing for the Lamé parameters of a deformable solid FEM. In this case, both parameters $\lambda$ and $\mu$ are set to 1000 . As can be seen in the plot, the loss landscape has</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ Dynamics refers to the laws governing the motion and deformation of objects over time. Rendering refers to the interaction of these scene elements - including their material properties - with scene lighting to form image sequences as observed by a virtual camera. Simulation refers to a unified treatment of these two processes.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
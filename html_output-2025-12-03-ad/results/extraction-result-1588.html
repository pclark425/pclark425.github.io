<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1588 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1588</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1588</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-fee5088cc8242189f298be604114e8b8e9f7ce28</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fee5088cc8242189f298be604114e8b8e9f7ce28" target="_blank">Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a representation learning framework called breakpoint modeling that allows for efficient and robust learning of this type of model, and shows the benefit of the T5-based breakpoint transformer over strong conventional representation learning approaches in terms of processing efficiency, belief accuracy, and belief consistency.</p>
                <p><strong>Paper Abstract:</strong> Can we teach models designed for language understanding tasks to track and improve their beliefs through intermediate points in text? Besides making their inner workings more transparent, this would also help make models more reliable and consistent. To this end, we propose a representation learning framework called breakpoint modeling that allows for efficient and robust learning of this type. Given any text encoder and data marked with intermediate states (breakpoints) along with corresponding textual queries viewed as true/false propositions (i.e., the candidate intermediate beliefs of a model), our approach trains models in an efficient and end-to-end fashion to build intermediate representations that facilitate direct querying and training of beliefs at arbitrary points in text, alongside solving other end-tasks. We evaluate breakpoint modeling on a diverse set of NLU tasks including relation reasoning on Cluttr and narrative understanding on bAbI. Using novel proposition prediction tasks alongside these end-tasks, we show the benefit of our T5-based breakpoint transformer over strong conventional representation learning approaches in terms of processing efficiency, belief accuracy, and belief consistency, all with minimal to no degradation on the end-task. To show the feasibility of incorporating our belief tracker into more complex reasoning pipelines, we also obtain state-of-the-art performance on the three-tiered reasoning challenge for the recent TRIP benchmark (23-32% absolute improvement on Tasks 2-3).</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1588.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1588.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Warmup multitask schedule</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Initial warmup before enabling proposition-prediction loss in multi-task training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training schedule used in the paper where proposition-prediction loss is turned on only after an initial warmup period (typically 5–10 epochs) while the model is first trained on other tasks (QA/generation); loss weights (λ) are hand-tuned (λ_prop=1.0, λ_QA often 0.1) to form a multi-task curriculum-like procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Breakpoint Transformer (BPT) — T5-based</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Single-read, late-interaction representation model built on T5 encoder(s); breakpoint tokens mark intermediate story positions and pooled breakpoint embeddings are scored against separately encoded textual propositions via a bilinear classifier. Models are trained multi-task (proposition classification, optional QA generation, and auxiliary generation losses).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td>T5-base (220M) and T5-large (770M) used depending on dataset</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>bAbI, CLUTRR, TRIP (text-story benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Benchmarks of story/narrative text: bAbI (synthetic micro-world stories, object/location/possession questions), CLUTRR (synthetic family-relation stories with proofs; variable story length k), TRIP (human-authored short stories for physical commonsense, 3-tier evaluation: plausibility, conflict sentence identification, state verification). Interaction is via text stories and text queries/propositions (non-interactive episodic stories rather than game-like actions).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense and relational story procedures (physical commonsense, object/location/possession tracking, relational inference)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Tracking object locations (John is in the kitchen), possession (John has the apple), family relation deductions (X is the grandfather of Y), physical state preconditions/effects (oven was open / oven is open).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are compositional in story length and in deduction depth (e.g., CLUTRR k-step relation chains); the multi-task setup composes proposition prediction, QA, and auxiliary generation losses into a single training objective.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>warmup-before-proposition-loss (a staged multi-task schedule)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Training begins with one or more primary tasks (QA and/or auxiliary generation) and an initial warmup period (usually 5–10 epochs) during which proposition-prediction loss is not applied. After warmup the proposition loss (λ1) is enabled; loss weights are hand-tuned (paper reports λ_prop = 1.0 for proposition tasks; λ_QA often set to 0.1). This creates a staged learning regime (first focus on generation/QA, then add supervised belief-tracking).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>training phase ordering (warmup before enabling auxiliary supervised proposition loss)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Applied across datasets: CLUTRR stories k=2..8 (training on k=2..5 in reported setups), bAbI stories with 20 events/breakpoints, TRIP short human-authored stories (pairwise comparisons and sentence-level state changes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Reported multi-task models trained with the warmup schedule achieved high proposition accuracy and competitive QA and benchmark results in the paper's experiments. Examples: CLUTRR proposition prediction improved from baseline ~81.9% to BPT ~85.2% (dev, top of Table 1); bAbI BPT-base proposition accuracy 98.5% and joint BPT-base+QA QA exact-match 94.9% (i.i.d) and 70.51% (hardQA); TRIP (3-tier dev) BPT-base: Task1 (Plausibility) 81.99% ±0.91, Task2 (Consistency) 58.07% ±0.76, Task3 (Verifiability) 36.44% ±0.53, substantially above the prior RoBERTa-based RoB (dev: 73.6 / 22.4 / 10.6). The paper also reports the model requires less training data to reach comparable performance (e.g., comparable to baselines with ~60% of training data) and faster training (single-read BPT reduced training time by ~54% vs multi-pass on long stories).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Paper reports compositional generalization experiments (CLUTRR): BPT generalization performance (best run) 69.2% vs baseline 61.7% on generalization split; i.i.d performance was high (BPT 95.69% i.i.d). These numbers are reported for models trained with the described multi-task setup and warmup, but there is no explicit ablation that isolates warmup vs no-warmup scheduling for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Staging training by warming up on QA/generation before enabling proposition supervision allows integrating explicit belief tracking without significant degradation of end-task QA: joint training had minimal to modest impact on QA. The multi-task/warmup setup coincides with strong proposition accuracy and improved belief consistency and enables state-of-the-art performance on TRIP's multi-tier evaluation; paper does not provide an explicit ablation isolating warmup timing, but reports that initial warmup before turning on proposition loss (5–10 epochs) was part of their best setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1588.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1588.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Length-based training (CLUTRR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training on short-to-medium relation-chain stories (CLUTRR k-length splits) and evaluating compositional generalization to longer stories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper trains breakpoint models on CLUTRR stories sampled from lengths k=2..5 (13k training stories) and evaluates both in-domain (i.i.d) and compositional generalization to longer relation-chain lengths (up to k=8 or k'=2..10 in standard comparisons), using proposition annotations at breakpoints derived from proof trees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Breakpoint Transformer (BPT) — T5-large for CLUTRR experiments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>T5-based breakpoint model with single-read encoding of stories, [B] tokens used for breakpoint embeddings, self-attention pooling over breakpoint embeddings, bilinear proposition scorer; for CLUTRR they used the larger T5-large model (770M).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td>T5-large (770M) used for CLUTRR experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CLUTRR (synthetic relational-reasoning stories)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Synthetic stories describing family relations/events, diagnostics for inductive/relational reasoning where queries/proofs can be derived; environment is text stories with breakpoints after each sentence and proposition annotations derived from proof trees (entailed/contradicted/unknown labels).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>relational reasoning / logical inference (structured deductive procedures over family relations)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Deducing family relations such as 'X is the grandfather of Y' from a chain of binary relation facts revealed sentence-by-sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Compositional via relation-chain length k (longer chains require composing more binary relation steps); training on short chains tests ability to generalize to longer compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Stories characterized by k (number of events): training drawn from k=2..5; evaluation includes k up to 8 (mix dev/test) and standard splits for generalization (train k=2,3 and test on k'=2..10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Models trained on k=2..5 (BPT best run) achieved i.i.d proposition accuracy 95.69% (global constraint violation ρ=25.9) and generalization accuracy 69.2% (ρ=97.6) on compositional splits; baseline best run: i.i.d 94.54% (ρ=32.1) and generalization 61.7% (ρ=90.2). The paper highlights a gap between i.i.d and generalization (69.2% vs 95.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Explicitly evaluated: BPT improves over baselines on compositional generalization (about +7.5% absolute in generalization accuracy vs baseline in reported comparison), but absolute generalization performance remains far lower than i.i.d performance (69.2% generalization vs 95.7% i.i.d for their best run).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training on shorter relation-chain stories yields high in-domain proposition accuracy but substantially reduced compositional generalization; BPT outperforms baselines on generalization but still shows a major gap, indicating that richer intermediate supervision helps but does not solve compositional generalization. The paper reports that auxiliary modeling choices (breakpoint self-attention, auxiliary generation losses) improve accuracy and consistency, but compositional generalization remains a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dyna-bAbl: unlocking bAbl's potential with dynamic synthetic benchmarking <em>(Rating: 2)</em></li>
                <li>CLUTRR: A diagnostic benchmark for inductive reasoning from text <em>(Rating: 2)</em></li>
                <li>Tiered reasoning for intuitive physics: Toward verifiable commonsense language understanding <em>(Rating: 2)</em></li>
                <li>Measuring systematic generalization in neural proof generation with transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1588",
    "paper_id": "paper-fee5088cc8242189f298be604114e8b8e9f7ce28",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "Warmup multitask schedule",
            "name_full": "Initial warmup before enabling proposition-prediction loss in multi-task training",
            "brief_description": "A training schedule used in the paper where proposition-prediction loss is turned on only after an initial warmup period (typically 5–10 epochs) while the model is first trained on other tasks (QA/generation); loss weights (λ) are hand-tuned (λ_prop=1.0, λ_QA often 0.1) to form a multi-task curriculum-like procedure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Breakpoint Transformer (BPT) — T5-based",
            "agent_description": "Single-read, late-interaction representation model built on T5 encoder(s); breakpoint tokens mark intermediate story positions and pooled breakpoint embeddings are scored against separately encoded textual propositions via a bilinear classifier. Models are trained multi-task (proposition classification, optional QA generation, and auxiliary generation losses).",
            "agent_size": "T5-base (220M) and T5-large (770M) used depending on dataset",
            "environment_name": "bAbI, CLUTRR, TRIP (text-story benchmarks)",
            "environment_description": "Benchmarks of story/narrative text: bAbI (synthetic micro-world stories, object/location/possession questions), CLUTRR (synthetic family-relation stories with proofs; variable story length k), TRIP (human-authored short stories for physical commonsense, 3-tier evaluation: plausibility, conflict sentence identification, state verification). Interaction is via text stories and text queries/propositions (non-interactive episodic stories rather than game-like actions).",
            "procedure_type": "commonsense and relational story procedures (physical commonsense, object/location/possession tracking, relational inference)",
            "procedure_examples": "Tracking object locations (John is in the kitchen), possession (John has the apple), family relation deductions (X is the grandfather of Y), physical state preconditions/effects (oven was open / oven is open).",
            "compositional_structure": "Tasks are compositional in story length and in deduction depth (e.g., CLUTRR k-step relation chains); the multi-task setup composes proposition prediction, QA, and auxiliary generation losses into a single training objective.",
            "uses_curriculum": true,
            "curriculum_name": "warmup-before-proposition-loss (a staged multi-task schedule)",
            "curriculum_description": "Training begins with one or more primary tasks (QA and/or auxiliary generation) and an initial warmup period (usually 5–10 epochs) during which proposition-prediction loss is not applied. After warmup the proposition loss (λ1) is enabled; loss weights are hand-tuned (paper reports λ_prop = 1.0 for proposition tasks; λ_QA often set to 0.1). This creates a staged learning regime (first focus on generation/QA, then add supervised belief-tracking).",
            "curriculum_ordering_principle": "training phase ordering (warmup before enabling auxiliary supervised proposition loss)",
            "task_complexity_range": "Applied across datasets: CLUTRR stories k=2..8 (training on k=2..5 in reported setups), bAbI stories with 20 events/breakpoints, TRIP short human-authored stories (pairwise comparisons and sentence-level state changes).",
            "performance_with_curriculum": "Reported multi-task models trained with the warmup schedule achieved high proposition accuracy and competitive QA and benchmark results in the paper's experiments. Examples: CLUTRR proposition prediction improved from baseline ~81.9% to BPT ~85.2% (dev, top of Table 1); bAbI BPT-base proposition accuracy 98.5% and joint BPT-base+QA QA exact-match 94.9% (i.i.d) and 70.51% (hardQA); TRIP (3-tier dev) BPT-base: Task1 (Plausibility) 81.99% ±0.91, Task2 (Consistency) 58.07% ±0.76, Task3 (Verifiability) 36.44% ±0.53, substantially above the prior RoBERTa-based RoB (dev: 73.6 / 22.4 / 10.6). The paper also reports the model requires less training data to reach comparable performance (e.g., comparable to baselines with ~60% of training data) and faster training (single-read BPT reduced training time by ~54% vs multi-pass on long stories).",
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Paper reports compositional generalization experiments (CLUTRR): BPT generalization performance (best run) 69.2% vs baseline 61.7% on generalization split; i.i.d performance was high (BPT 95.69% i.i.d). These numbers are reported for models trained with the described multi-task setup and warmup, but there is no explicit ablation that isolates warmup vs no-warmup scheduling for transfer.",
            "key_findings": "Staging training by warming up on QA/generation before enabling proposition supervision allows integrating explicit belief tracking without significant degradation of end-task QA: joint training had minimal to modest impact on QA. The multi-task/warmup setup coincides with strong proposition accuracy and improved belief consistency and enables state-of-the-art performance on TRIP's multi-tier evaluation; paper does not provide an explicit ablation isolating warmup timing, but reports that initial warmup before turning on proposition loss (5–10 epochs) was part of their best setup.",
            "uuid": "e1588.0",
            "source_info": {
                "paper_title": "Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Length-based training (CLUTRR)",
            "name_full": "Training on short-to-medium relation-chain stories (CLUTRR k-length splits) and evaluating compositional generalization to longer stories",
            "brief_description": "The paper trains breakpoint models on CLUTRR stories sampled from lengths k=2..5 (13k training stories) and evaluates both in-domain (i.i.d) and compositional generalization to longer relation-chain lengths (up to k=8 or k'=2..10 in standard comparisons), using proposition annotations at breakpoints derived from proof trees.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Breakpoint Transformer (BPT) — T5-large for CLUTRR experiments",
            "agent_description": "T5-based breakpoint model with single-read encoding of stories, [B] tokens used for breakpoint embeddings, self-attention pooling over breakpoint embeddings, bilinear proposition scorer; for CLUTRR they used the larger T5-large model (770M).",
            "agent_size": "T5-large (770M) used for CLUTRR experiments",
            "environment_name": "CLUTRR (synthetic relational-reasoning stories)",
            "environment_description": "Synthetic stories describing family relations/events, diagnostics for inductive/relational reasoning where queries/proofs can be derived; environment is text stories with breakpoints after each sentence and proposition annotations derived from proof trees (entailed/contradicted/unknown labels).",
            "procedure_type": "relational reasoning / logical inference (structured deductive procedures over family relations)",
            "procedure_examples": "Deducing family relations such as 'X is the grandfather of Y' from a chain of binary relation facts revealed sentence-by-sentence.",
            "compositional_structure": "Compositional via relation-chain length k (longer chains require composing more binary relation steps); training on short chains tests ability to generalize to longer compositions.",
            "uses_curriculum": null,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Stories characterized by k (number of events): training drawn from k=2..5; evaluation includes k up to 8 (mix dev/test) and standard splits for generalization (train k=2,3 and test on k'=2..10).",
            "performance_with_curriculum": "Models trained on k=2..5 (BPT best run) achieved i.i.d proposition accuracy 95.69% (global constraint violation ρ=25.9) and generalization accuracy 69.2% (ρ=97.6) on compositional splits; baseline best run: i.i.d 94.54% (ρ=32.1) and generalization 61.7% (ρ=90.2). The paper highlights a gap between i.i.d and generalization (69.2% vs 95.7%).",
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Explicitly evaluated: BPT improves over baselines on compositional generalization (about +7.5% absolute in generalization accuracy vs baseline in reported comparison), but absolute generalization performance remains far lower than i.i.d performance (69.2% generalization vs 95.7% i.i.d for their best run).",
            "key_findings": "Training on shorter relation-chain stories yields high in-domain proposition accuracy but substantially reduced compositional generalization; BPT outperforms baselines on generalization but still shows a major gap, indicating that richer intermediate supervision helps but does not solve compositional generalization. The paper reports that auxiliary modeling choices (breakpoint self-attention, auxiliary generation losses) improve accuracy and consistency, but compositional generalization remains a limitation.",
            "uuid": "e1588.1",
            "source_info": {
                "paper_title": "Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dyna-bAbl: unlocking bAbl's potential with dynamic synthetic benchmarking",
            "rating": 2
        },
        {
            "paper_title": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
            "rating": 2
        },
        {
            "paper_title": "Tiered reasoning for intuitive physics: Toward verifiable commonsense language understanding",
            "rating": 2
        },
        {
            "paper_title": "Measuring systematic generalization in neural proof generation with transformers",
            "rating": 1
        }
    ],
    "cost": 0.015009249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs</h1>
<p>Kyle Richardson ${ }^{2 \dagger}$ Ronen Tamari ${ }^{1 \dagger *}$ Oren Sultan ${ }^{1}$<br>Reut Tsarfaty ${ }^{2,3}$ Dafna Shahaf ${ }^{1}$ Ashish Sabharwal ${ }^{2}$<br>${ }^{1}$ The Hebrew University of Jerusalem ${ }^{2}$ Allen Institute for AI ${ }^{3}$ Bar-Ilan University<br>{ronent, orens, dshahaf}@cs.huji.ac.il, {kyler, reutt, ashishs}@allenai.org</p>
<h4>Abstract</h4>
<p>Can we teach natural language understanding models to track their beliefs through intermediate points in text? We propose a representation learning framework called breakpoint modeling that allows for learning of this type. Given any text encoder and data marked with intermediate states (breakpoints) along with corresponding textual queries viewed as true/false propositions (i.e., the candidate beliefs of a model, consisting of information changing through time) our approach trains models in an efficient and end-to-end fashion to build intermediate representations that facilitate teaching and direct querying of beliefs at arbitrary points alongside solving other end tasks. To show the benefit of our approach, we experiment with a diverse set of NLU tasks including relational reasoning on CLUTRR and narrative understanding on bAbI. Using novel belief prediction tasks for both tasks, we show the benefit of our main breakpoint transformer, based on T5, over conventional representation learning approaches in terms of processing efficiency, prediction accuracy and prediction consistency, all with minimal to no effect on corresponding QA endtasks. To show the feasibility of incorporating our belief tracker into more complex reasoning pipelines, we also obtain SOTA performance on the three-tiered reasoning challenge for the TRIP benchmark (around 23-32\% absolute improvement on Tasks 2-3). ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Despite considerable progress made recently in natural language understanding (NLU), driven largely by advances in language model pre-training (Devlin et al., 2019; Raffel et al., 2020) and the development of large-scale NLU benchmarks (Wang et al., 2018), understanding the behavior of models remains a formidable and highly consequential</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Deep narrative understanding in natural language (bottom) involves the ability to answer queries about arbitrary intermediate points in a given story. We liken this task to breakpoints in programming (top), or reporting the state of a program at different stages of execution, facilitating human inspection of model beliefs and consistency with end-task behavior (bottom).
challenge for model safety. Such a challenge is particularly acute in tasks such as narrative understanding, where one must piece together many individual (possibly implicit) facts through time in order to solve problems. For example, in the story in Figure 1, answering the question Where is the apple? requires knowing how to track objects through time (e.g., knowing the location of the John and Mary and their interaction) and how to compartmentalize other types of knowledge across the story. In such a setting, where models are trained to narrowly answer questions, a natural question arises: do models acquire the kind of requisite background knowledge and world tracking abilities, and ultimately learn representations that give rise to correct beliefs ${ }^{2}$ about intermediate states?</p>
<p>A chief difficulty in answering such questions is</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A high-level view of our modeling approach. For a given story and a set of textual queries corresponding to intermediate points in the story (breakpoints), truth assignments are assigned to queries to form belief states based on a projection over encodings of breakpoints and individual proposition encodings using a single task-specific encoder.
that directly inspecting the propositional attitudes of our current models remains a formidable challenge due to the latent nature of their knowledge. Such a complication also makes it unclear what the right interface should be for eliciting beliefs in the first place (e.g., how can we determine if a model believes a proposition John is in the kitchen at an arbitrary point in text?). In addition, for tasks such as QA, story contexts and questions are usually encoded jointly (often with full attention over context and query), which makes it difficult to tease apart a model's understanding of a story independent of each question. Entangled story and question representations can be inefficient when scaling to a large space of questions, particularly for novel combinations of questions and stories (Tamari et al., 2022). Such entangled representations also allow models to exploit spurious patterns in questions that inflate performance (Kaushik and Lipton, 2018) and hinder interpretability.</p>
<p>We present a model-agnostic representation learning framework called breakpoint modeling that facilitates teaching models to have propositional beliefs at arbitrary points in stories (or breakpoints) using ordinary textual queries as our interface language. Our general modeling approach is illustrated in Figure 2. Given any task-specific encoder and data marked with the intermediate state of interest (or breakpoints, denoted throughout as [B]) along with a set of textual queries (i.e., the candidate beliefs provided in training as auxiliary
intermediate supervision), models are trained in an end-to-end fashion to learn intermediate taskspecific representations (pooled from single encodings of stories) that jointly facilitate making correct and consistent belief predictions efficiently across a large space of queries. Making an analogy with breakpoints in programming (see top of Figure 1), we aim to simulate stopping execution at intermediate points during a story to inspect the model's belief state (e.g., checking that a model's answers for QA are consistent with their beliefs and satisfy certain high-level constraints), as well as teach the model to have certain beliefs learned through intermediate supervision at training time.</p>
<p>Using a state-of-the-art pretrained model, T5 (Raffel et al., 2020), we develop and investigate a breakpoint transformer to do belief prediction on three categories of tasks: narrative understanding on bAbI (Weston et al., 2016; Tamari et al., 2022), relational reasoning on CLUTRR (Sinha et al., 2019) and physical commonsense reasoning over human authored stories on TRIP (Storks et al., 2021). In the former two cases, we focus on training and evaluating models on a novel belief prediction task. We report improvements over a conventional transformer-based representation learning approach (Reimers and Gurevych, 2019) both in terms of prediction accuracy ( $4 \%$ to $8 \%$ absolute improvement on CLUTRR dev) and belief consistency, all with significantly improved processing efficiency (i.e., minimal forward calls to the full transformer) and minimal effect on endtask performance when jointly trained with QA. In the latter case for TRIP, we show how to integrate our modeling approach into a more complex transformer pipeline and report state-of-the-art results on the three-tiered reasoning task (with 23-32\% absolute improvement on two component tasks) over existing task-specific architectures.</p>
<p>Taken together, our results show the viability of building an end-to-end trainable belief tracking mechanism and integrating it within existing transformer-based reasoning systems. To our knowledge, our work is among the first to look at at general-purpose sentence representation learning for intermediate states in text as a way to facilitate complex situation reasoning.</p>
<h2>2 Related Work</h2>
<p>Our work brings together two recent areas that aim to understand model behavior (broadly model prob-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Example Stories</th>
<th style="text-align: center;">Breakpoint Propositions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Relational Reasoning (CLI/TRR)</td>
<td style="text-align: center;">John is the brother of Susan [B]; Susan's mother is Janice [B];...</td>
<td style="text-align: center;">$\mathbf{P}<em 2="2">{1} \quad \mid$ 'Susan is the sister of John' true, 'Susan is the sister-in-law of Janice' false, 'Janice is the mother of John' $\left.\mathrm{sch} \mid\right}$ <br> $\mathbf{P}</em> \quad \mid$ 'Janice is the mother of John' true, 'John is the father of Janice' false, ... $\mid$</td>
</tr>
<tr>
<td style="text-align: center;">Story</td>
<td style="text-align: center;">John moved to the kitchen [B]; He picked up an apple [B]; John then gave the apple to Mary [B];...</td>
<td style="text-align: center;">$\mathbf{P}_{1} \quad \mid$ 'John has the apple' false, 'John is in the kitchen' true,...</td>
</tr>
<tr>
<td style="text-align: center;">(bAbl)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Commonsense (TRIP)</td>
<td style="text-align: center;">Tom dropped his radio ...carpet. [B]; The radio broke ...[B]; Tom turned on the radio ...[B];...</td>
<td style="text-align: center;">$\mathbf{P}<em 3="3">{2} \quad \mid$ 'radio is in pieces' true, 'radio is powered' false], ... <br> $\mathbf{P}</em> \quad \mid$ 'radio was powered' true $\mid$</td>
</tr>
</tbody>
</table>
<p>Figure 3: Three tasks rendered as stories with special breakpoint tokens [B] $]<em j="j">{j}$ (for convenience, marked with an index $j$ ). Each intermediate breakpoint is aligned to a set of propositions $\mathbf{P}</em>$ marked with truth conditions (i.e., true, false, unknown) corresponding to the truth value of each proposition at that breakpoint.
ing): probing of the type that includes finding neural correlates of high-level behavioral phenomena, modular structure in networks (Tenney et al., 2019; Hewitt and Manning, 2019) on the one hand, as well as diagnostic testing, which aims to understand model competence through controlled input-output testing (Lake and Baroni, 2018; Richardson et al., 2020), or post-hoc consistency analysis (Kassner et al., 2021). Our work is more closely related to Li et al. (2021), who show that partial world state information can be decoded from NLMs even without explicit supervision. In that work, state information is roughly localized to entity mentions, but varies across different datasets. Differently from such probing work, our breakpoint models are trained in a supervised manner to localize particular propositional information at particular locations (similar to Geiger et al. (2021)).</p>
<p>Our breakpoint model closely relates to lateinteraction encoder architectures that tease apart the encoding of problems and solutions. This includes the sentence transformer from Reimers and Gurevych (2019), which we compare against in our experiments, as well as read once transformers (Lin et al., 2021), colBERT (Khattab and Zaharia, 2020) and others. Given that the types of narrative tasks we focus on require modeling many intermediate points, we follow this work in putting an emphasis on representation and encoding efficiency. In contrast to this, and other related work on sentence representation learning (Gao et al., 2021; Ni et al., 2022), we uniquely focus on learning representations of intermediate states in text for complex situational reasoning.</p>
<p>We are also inspired by the situation modeling literature in cognitive science (Golden and Rumelhart, 1993; Frank et al., 2003; Venhuizen et al., 2019), and proposals for their integration with NLP research (Tamari et al., 2020). These works also studied neural models of narrative comprehension in carefully controlled micro-worlds, but typically
focused on relatively short sentence-level inputs.
Our work also relates to efforts on building interpretable models by making the underlying reasoning processes transparent, either through explicit decomposition (Andreas et al., 2016; Khot et al., 2021; Bostrom et al., 2022) or generation of rationales (Camburu et al., 2018; Wiegreffe and Marasovic, 2021) and other reasoning structures (Tafjord et al., 2021; Dalvi et al., 2021; Gontier et al., 2020). In contrast, we focus on belief representations that are ultimately faithful (Jacovi and Goldberg, 2020) to end-tasks by training knowledge directly into a model's task-specific representations.</p>
<h2>3 Breakpoint Modeling</h2>
<p>The goal of breakpoint modeling is to capture the intermediate states and beliefs of models at arbitrary positions in text. Our models take stories as inputs, or pieces of text containing one or more intermediate positions (breakpoints), as well as sets of text propositions that align to certain intermediate points (see Figure 3). Such propositions play the role of auxiliary supervision if provided at training time or as queries to the model for performing probing; when coupled with predictions they constitute the beliefs of the model.</p>
<p>While breakpoint models can technically take different forms, their basic function is to assign encodings to intermediate states in text and their corresponding propositions (§ 3.1) and to make predictions about the truth/falsity of each proposition (§ 3.2). Learning (§ 3.3) reduces to the problem of teaching a model to have a correct and consistent set of beliefs for each target task given a set of representative intermediate propositions and beliefs provided at training time (§ 3.4).</p>
<h3>3.1 Breakpoint and Proposition Encoding</h3>
<p>As illustrated in Figure 3, stories are texts consisting of $n$ tokens within which there can exist $m \geq 1$ arbitrarily selected intermedi-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: What is the best way to model intermediate states and beliefs with existing encoder models? An illustration of two late-interaction architectures we investigate (Single Read model, our main model described in § 3 and a Multi-pass Late Interaction model)
ate points or breakpoints. For convenience, we will render a story $s$ in the following way: $s:=w_{1, b_{1}} \ldots w_{. b_{1}}[\mathbf{B}] \ldots w_{. b_{j}} \ldots[\mathbf{B}] \ldots w_{n, b_{m}}[\mathbf{B}]$ where [B] is a special token used to explicitly mark position of each breakpoint $b_{j}$. Intuitively, a breakpoint token represents all of the information in the story relevant to building an accurate belief state at the corresponding (intermediate) point in the text. Associated with each $b_{j}$ is a set of text propositions $\mathbf{P}<em 1="1">{j}=\left{p</em>$ (in the sense of Footnote 2).}, p_{2}, \ldots, p_{t}\right}$. Truth assignments to these text propositions constitute the candidate beliefs at breakpoint $b_{j</p>
<p>At the core of any breakpoint model are two encoders, $\mathbf{e n c}<em _prop="{prop" _text="\text">{\text {story }}$, $\mathbf{e n c}</em>}}$, that are used to generate a representation or embedding for each breakpoint in the story and each proposition, respectively. Representations of breakpoints $\mathbf{b} \in \mathbb{R}^{d}$ are pooled from a single encoding of an input story $s: \mathbf{c<em _story="{story" _text="\text">{s} \leftarrow \mathbf{e n c}</em>}}(s) \in \mathbb{R}^{|s| \times d}$ and representations for propositions $\mathbf{c<em _prop="{prop" _text="\text">{\text {prop }} \in \mathbb{R}^{d}$ are obtained in a similar fashion using enc ${ }</em>}}$. While the choice of the encoder and the details of how pooling is done can vary (see details in $\S 5.1$ ), in all of our models breakpoint representations $\mathbf{b}$ are obtained by taking projections of the hidden states of the [B] tokens from $\mathbf{c<em _story="{story" _text="\text">{s}$. We also investigate models that assume a siamese architecture (Reimers and Gurevych, 2019) where $\mathbf{e n c}</em>$ are the same encoder.}}$ and $\mathbf{e n c}_{\mathrm{p}</p>
<p>An important property of breakpoint models is that all breakpoints representations $\mathbf{b}_{j}$ are obtained from a single read encoding of each target story. We later compare this against a much less efficient approach that requires multiple forward passes through the story to obtain intermediate encodings (i.e., the multi-pass approach shown in Figure 4). Our model therefore stays within the spirit of a late-interaction architecture (Khattab and Zaharia, 2020) by using separate encodings of
breakpoints and propositions, which allows us to scale to large sets of propositional queries.</p>
<h3>3.2 Proposition Scoring and Semantics</h3>
<p>Given a breakpoint encoding $\mathbf{b}$ and an aligned proposition encoding $\mathbf{c}_{\text {prop }}$, a proposition scorer makes a prediction about a proposition at that breakpoint. As mentioned, our aim is to predict the truth value of a proposition at an intermediate state, which we take to be the model's belief in that proposition. Our scorer takes the form of a classifier that maps a breakpoint encoding and proposition encoding to the discrete space ${$ true, false, unknown $}$, following Li et al. (2021) and the annotation scheme from NLI (Dagan et al., 2005; Bowman et al., 2015).</p>
<p>To make clear that the interpretation of each proposition is tied to a specific breakpoint, we will use the symbolic notation from Li et al. (2019) and introduce three binary logical predicates $\mathbf{E}, \mathbf{C}$, and $\mathbf{U}$. For each $b_{j}$ and $p \in \mathbf{P}<em j="j">{j}$, these predicates capture whether $p$ is entailed by, is contradicted by, or has an unknown relation to the information in the text at breakpoint $b</em>$.}$, respectively. For instance, $\mathbf{E}\left(b_{j}, p\right)$ is true if the text proposition $p$ is entailed by the story at breakpoint $b_{j</p>
<h3>3.3 Learning</h3>
<p>Suppose we have a dataset $D$ consisting of $n$ stories $\left{s^{(i)}\right}<em j="j">{i=1}^{n}$ along with the following additional information. For each story $s^{(i)}$, we have $m$ breakpoints $B^{(i)} .^{3}$ For each such breakpoint $b</em>}$, we have $t$ labeled text propositions ${ }^{4} \mathbf{P<em k="k">{j}^{(i)}$, where each proposition $p</em>} \in \mathbf{P<em j_="j," k="k">{j}^{(i)}$ is labeled with $y</em>}^{(i)} \in{\mathbf{t r u e}$, false, unknown $}$ indicating $p_{k}$ 's truth value at breakpoint $b_{j}$. Using the above predicate logic notation, we can equivalently think of having, for each $p_{k} \in \mathbf{P<em j_="j," k="k">{j}^{(i)}$, exactly one predicate $Y</em>$ are False).}^{(i)} \in{\mathbf{E}, \mathbf{C}, \mathbf{U}}$ annotated in $D$, with the semantics that $Y_{j, k}^{(i)}\left(b_{j}, p_{k}\right)$ is True (and the other two predicates for $b_{j}$ and $p_{k</p>
<p>The goal here is to learn a model that assigns truth values to all text propositions across all breakpoints-equivalently, truth values for all three logical predicates-in a way that maximally aligns with $D$. Semantically, this can be expressed as</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>satisfying the logical formula (Li et al., 2019):</p>
<p>$$
\bigwedge_{s^{(i)} \in D} \bigwedge_{b_{j} \in B^{(i)}} \bigwedge_{p_{k} \in \mathbf{P}<em j_="j," k="k">{j}^{(i)}} Y</em>\right)
$$}^{(i)}\left(b_{j}, p_{k</p>
<p>with the added constraint that for each story $s^{(i)}$ and all $j, k$, exactly one of $\mathbf{E}\left(b_{j}, p_{k}\right), \mathbf{C}\left(b_{j}, p_{k}\right)$, and $\mathbf{U}\left(b_{j}, p_{k}\right)$ is True.</p>
<p>Using $\operatorname{Pr}\left[y_{j, k}^{(i)}\right]$ to denote the model's probability corresponding to the predicate $\mathbf{Y}<em j="j">{j, k}^{(i)}\left(b</em>\right)$, this formula can be translated into the following loss using the product translation from Li et al. (2019):}, p_{k</p>
<p>$$
\mathcal{L}<em i="1">{\text {prop }}=\sum</em>\right]
$$}^{n} \sum_{j=1}^{m} \sum_{k=1}^{t}-\log \operatorname{Pr}\left[y_{j, k}^{(i)</p>
<p>which yields the common cross-entropy loss that we use in our experiments.</p>
<h3>3.4 Proposition Sampling</h3>
<p>Propositions in breakpoint models have a dual role: when given at training time, they provide intermediate supervision for training models across different situation states. When given at inference time they allow for post-hoc probing of a model's beliefs. As shown in the Figure 3, propositions, in virtue of being ordinary text, can express many different types of information and thus provide an unbounded source of semantic supervision (Hanjie et al., 2022), e.g., for expressing fluents, or conditions that change through time in a story (e.g., John is in the kitchen, or event pre/post-conditions (e.g., The radio was powered via English tense).</p>
<p>For training models to have beliefs, a necessary first step is to devise a sampling policy for generating these intermediate annotations. While such a strategy needs to be tailored to each target task, we experiment with a combination of extracting propositions from existing task annotations (Figure 5) and generating propositions based on a set of domain constraints using the semantics of each target domain (details in the next section).</p>
<h2>4 Proposition Prediction Tasks</h2>
<p>We focus on three categories of tasks: text-based relational reasoning, story understanding and commonsense reasoning, each considered in turn. In the former two cases, we devise new proposition and belief prediction tasks that involve training on intermediate belief state annotations. We also include out-of-distribution (o.o.d) generalization</p>
<p>Qiana is Lisa's mother
Derrick is Lisa's father Qiana is Derick's wife</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Story</th>
<th style="text-align: left;">Lisa is Jerry's granddaughter [B] Derrick is Lisa's <br> father [B] Qiana is Derrick's wife [B]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Atomic Belief</td>
<td style="text-align: left;">E(1, 'Jerry is the grandfather of Lisa')</td>
</tr>
<tr>
<td style="text-align: left;">Annotations:</td>
<td style="text-align: left;">E(1, 'Derick is the father of Lisa'),</td>
</tr>
<tr>
<td style="text-align: left;">The basic facts</td>
<td style="text-align: left;">C(1, 'Lisa ,father of Derick'),</td>
</tr>
<tr>
<td style="text-align: left;">that should be</td>
<td style="text-align: left;">E(3, 'Qiana, wife of Derick'),</td>
</tr>
<tr>
<td style="text-align: left;">predicted</td>
<td style="text-align: left;">E(3,Qiana is the mother of Lisa)</td>
</tr>
<tr>
<td style="text-align: left;">Knowledge:</td>
<td style="text-align: left;">(3,Qiana is the mother of Lisa')</td>
</tr>
<tr>
<td style="text-align: left;">Constraints to</td>
<td style="text-align: left;">E(3, 'Qiana is the wife of Derick'))</td>
</tr>
<tr>
<td style="text-align: left;">satisfy</td>
<td style="text-align: left;">E(3, 'Qiana is the mother of Lisa'))</td>
</tr>
</tbody>
</table>
<p>Figure 5: How are intermediate propositions collected? An illustration of constructing intermediate propositions from CLUTRR proof trees (above) in Gontier et al. (2020). BOTTOM: An example ground constraint, which we use for analyzing consistency.
tests beyond standard i.i.d (independent and identically distributed) evaluation. In the latter case, we recast an existing task in terms of breakpoint models to show the versatility of our approach in a more complex multi-task setting.</p>
<h3>4.1 Relational Reasoning</h3>
<p>CLUTRR Sinha et al. (2019) focuses on QA over synthetic stories about family relations as shown in Figure 3, and has more recently been extended to focus on proof generation (Gontier et al., 2020). As illustrated in Figure 5, we use the proof annotations in the latter work to generate intermediate propositions that track the time-course of family relations as they emerge at each new sentence.</p>
<p>Relying on the clean subset of CLUTRR stories Sinha et al. (2019) and proof annotations, breakpoints are added after each sentence. Propositional renderings of the explicit story facts, as well as intermediate propositions revealed in the proof annotations, were then added to each corresponding breakpoint in the story and serve as the base proposition set. From these base propositions, additional propositions, including negative and unknown propositions, were added using the following general constraints: monotonicity, that beliefs, once established to be true /false, cannot change; the mutually exclusivity of certain relations (e.g., $X$ is the grandfather of $Y$ is mutually exclusive with $X$ is the grandmother of $Y$ ); inverse relations between certain relations (e.g., that $X_{f e m}$ is a sister of $Y_{f e m}$ means that $Y$ is a sister of $X$ ), and that all non-deductively valid propositions are unknown (i.e., with label $\mathbf{U}$ ). ${ }^{5}$ Such ground propositions con-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>straints are included in the breakpoint annotations as symbolic expressions (see again Figure 5) to allow for measuring model consistency at inference time (later in Figure 6. See details in § A.3.
o.o.d evaluation. Stories in CLUTRR are characterized by their length $k$ (number of events) and generalization testing is usually performed to measure generalization. Our main datasets (later seen in Table 1) consists of 13 k training stories drawn from stories from $k=2, . ., 5$. We tune our models and evaluate on a mixture of in-domain and generalization stories of lengths $k=2, . .8$ each containing around 1.5 k stories (containing (avg) 10 propositions per breakpoint and 15 constraints per story). While these splits deviate from standard uses of CLUTRR, we also compare against standard splits (i.e., training on $k=2,3$ and testing on $k^{\prime}=2, . .10$ ) to look at the ability of training joint belief prediction and QA models on the original QA task.</p>
<h3>4.2 Story Understanding</h3>
<p>We experiment with the bAbI QA benchmark (Weston et al., 2016), which contains questions over stories about agents in controlled micro-worlds (see Figure 3). As with CLUTRR, the synthetic nature of domain makes it possible to automatically extract proposition annotations that express object location (e.g., PersonX/ObjectX' is in Y), object possession (PersonX has ObjectY), abstractions of event post-conditions (e.g., PersonX took something for the event PersonX grabbed the ball) and pronoun references (e.g., He refers to John). We use the Dyna-bAbI task generator (Tamari et al., 2022) to generate initial base propositions and, similar with CLUTRR, heuristically add more propositions using domain constraints (see § A. 2 for more details). ${ }^{6}$ We use propositional versions of the 7 task set introduced in Tamari et al. (2022). We specifically use the long-form version of this set, where stories all contain 20 events/breakpoints, and train on 500 examples per task (totaling $3.5 \mathrm{k}+1.4 \mathrm{k}$ training/evaluation stories, with an average of 10 propositions per breakpoint and 123 constraints per story).
o.o.d evaluation. In addition to training and testing on this set, we also look at joint training on proposition prediction and the original QA task. For evaluation we also consider a more challeng-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ing hardQA generalization task from Tamari et al. (2022), where the test set features compositions of concepts seen at training time. Appendix A. 2 contains example inputs and further task details.</p>
<h3>4.3 Physical Commonsense Reasoning</h3>
<p>We apply our approach to the recently introduced Tiered Reasoning for Intuitive Physics (TRIP) dataset (Storks et al., 2021). TRIP features a story plausibility end task, similar in scope to our proposition task, as well as a multi-tiered evaluation of models' reasoning process. Given a pair of highly similar human-authored short stories about everyday activities, models must jointly identify (1) the implausible story (task1) (2) a pair of conflicting sentences in the implausible story (task2) (3) the underlying physical states in those sentences causing the conflict (task 3). While task3 takes the form of a breakpoint modeling task, where physical states are rendered as textual propositions, we model the first two tasks as text2text tasks using multi-task breakpoint models (details in the appendix and in $\S 5.1$ ). We use the original splits, consisting of 675 plausible stories and 1472 implausible stories. While we focus on the multi-tiered evaluation, we devised a small filtered dev set (644 stories) for later model analysis (Table 5).</p>
<h2>5 Modeling Details and Metrics</h2>
<p>Here we detail our main breakpoint transformer (§5.1) following the framework in $\S 3$ and all metrics used in our experiments (§5.2).</p>
<h3>5.1 Modeling</h3>
<p>Encoder We experimented with the T5 model (Raffel et al., 2020) using the implementation from Wolf et al. (2020). T5's bi-directional encoder was used for both our story encoder $\mathbf{e n c}<em _prop="{prop" _text="\text">{\text {story }}$ and proposition encoder $\mathbf{e n c}</em>$. While any comparable encoder would suffice, we chose T5 due its common use in NLU and ability to perform generation, which we used to implement other components in the multi-task models discussed below. For efficiency reasons, we experimented with a combination of the smaller T5-base model (with 220M parameters) for datasets with long stories and many propositions (TRIP, bAbI) and T5-large (with 770M parameters) for CLUTRR.
Breakpoint and Proposition Embeddings For each story, individual breakpoint representations are first pooled from the $[\mathbf{B}]$ token hidden states in}</p>
<p>the story encodings $\mathbf{c}_{s}$ (see again Figure 4). Following Ni et al. (2022), a linear projection and L2 normalization is applied to each representation to construct initial breakpoint embeddings. To allow for information transfer between different breakpoints, we then apply an additional self-attention layer (sit-self) over these resulting representations to obtain a self-attention breakpoint representation (see Fan et al. (2020) for a similar idea), which gets concatenated with the initial representation to create the final breakpoint embedding. Operationally, the self-attention layer takes the form of a standard transformer block (Vaswani et al., 2017) with a single attention head.</p>
<p>One subtlety in using a standard bi-directional encoder such as T5 is that each breakpoint token can look at future parts of the story. While the content of a breakpoint is often determined by the preceding sentence, in some cases it is important to have information about the future to obtain an accurate representation. For example, for the story John has the apple.[B] He then moved to the kitchen [B] ${ }<em 1="1">{2}$, knowing that John can't be in the kitchen at $[\mathbf{B}]</em>$ (a pre-condition of move events) requires looking into the future. To limit the amount of future information in part of our breakpoint representations, however, future masking is applied in the breakpoint self-attention layer described above.</p>
<p>To obtain a proposition embedding, we use the same T5 encoder over each text proposition prefixed with a special token, then take the hidden state of the target proposition. A final proposition representation is then similarly obtained using the same linear projection and normalization layers.
Proposition Classifier As in Li et al. (2021), we use a bilinear layer for proposition classification $(\operatorname{score}(\cdot))$. Using the notation from $\S 3.3$, probabilities $\hat{\mathbf{y}}\left(\mathbf{b}<em j="j">{j}, \mathbf{p}\right)=$ $\left\langle\operatorname{Pr}\left[\mathbf{E}\left(b</em>}, p\right)\right], \operatorname{Pr}\left[\mathbf{C}\left(b_{j}, p\right)\right], \operatorname{Pr}\left[\mathbf{U}\left(b_{j}, p\right)\right]\right\rangle$ for the 3 truth values of a proposition $p$ are computed in the following way using the final breakpoint representation $\mathbf{b<em p="p">{j}$ and proposition encoding $\mathbf{c}</em>$ :</p>
<p>$$
\begin{aligned}
\operatorname{score}\left(b_{j}, p\right) &amp; =\mathbf{b}<em p="p">{j}^{T} \cdot \mathbf{M} \cdot \mathbf{c}</em> \
\hat{\mathbf{y}}\left(b_{j}, p\right) &amp; =\operatorname{softmax}(\operatorname{score}\left(b_{j}, p\right))
\end{aligned}
$$}+\mathbf{a</p>
<p>Learning In addition to optimizing for the objective described in $\S 3.3\left(\mathcal{L}<em _gen="{gen" _text="\text">{\text {prop }}\right)$, we also experiment with multi-task models trained to do generation $\left(\mathcal{L}</em>\right)$, both of which are formulated as text2text tasks and optimized using standard cross-entropy-based training. In the former
case, we investigate two analogues to the unsupervised denoising objectives from (Raffel et al., 2020), which aim to increase the amount of local information contained in breakpoint representations.}}\right)$ and $\mathrm{QA}\left(\mathcal{L}_{q a</p>
<p>The first is an event generation task that involves generating randomly chosen events from their right-most breakpoint encodings (e.g., generating the text Susan's mother is Janice from the encoding of $[\mathbf{B}]<em 1="1">{2}$ in Figure 3). The second, which is inspired by Gontier et al. (2022), generates textual abstractions either of random events from breakpoints (in the case of TRIP, e.g., generating the abstracted text PERSON dropped his OBJ... from $[\mathbf{B}]</em>]}$ in Figure 3) or random pairs of events in a story (e.g., generating the text A person received an apple from the an encoding averaged from the two breakpoints $[\mathbf{B<em 3="3">{2}$ and $[\mathbf{B}]</em>$ in Figure 3) (see additional details in § B.2).</p>
<p>Taken together, our full multi-task model's loss is: $\mathcal{L}=\lambda_{1} \mathcal{L}<em 2="2">{\text {prop }}+\lambda</em>} \mathcal{L<em 3="3">{\text {qa }}+\lambda</em>} \mathcal{L<em _123_1_2_3_125_="{1,2,3}">{\text {gen }}$ where $\lambda</em>$}$ are task weights manually tuned during training. We used ADAM as our optimizer (Kingma and Ba, 2014). Standardly, hyper-parameter tuning and model selection was performed via a random search search in the style of Devlin et al. (2019) on heldout dev sets (see details in § B.1). Unless stated otherwise, we report the average of three random restarts for all models and their standard deviations. Baselines We compare against two standard sentence representation learning approaches based on transformers and LSTMs. For the former we use the sentence transformer approach (Reimers and Gurevych, 2019) applied to our task, and for the latter we use a model close to Conneau et al. (2017). The set up is standard: stories and propositions are encoded separately using a single encoder and collected via mean (transformer) and max (BILSTM) pooling then aggregated via concatenation (in the style of InferSent Conneau et al. (2017)) and fed into a softmax classifier to make a belief prediction. Importantly, these baselines models are much less efficient compared with our single read breakpoint model, in that they require making multiple (multi-pass late interaction) forward passes through stories to create intermediate representations as illustrated in Figure 4. For the transformer models, with use the same T5 encoder as in the breakpoint models throughout all experiments. ${ }^{7</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Given that our breakpoint models take full story texts as input, to make the baselines fully comparable, we similarly feed in the full story on each read with a similar special token (#) to mark the target intermediate point (e.g., In the story John went to the store. He bought an apple we feed the text John went to the store. # He bought an apple when modeling the first breakpoint).
Joint Modeling For CLUTRR and bAbI, we also compare our multi-task breakpoint model trained for QA against T5 and Bart (Lewis et al., 2020), both fine-tuned solely for QA.</p>
<h3>5.2 Metrics</h3>
<p>For proposition prediction tasks we measure overall proposition accuracy (\%). Similarly for QA experiments, we follow other work in measuring exact match EM accuracy (\%) against a model's generated output. For some of our analysis on CLUTRR (Figure 5), we measure the consistency of belief prediction using the global consistency metric $\rho$ from Li et al. (2019), which measures the fraction of stories containing one or more constraint violation using the constraint annotations described in $\S 4$. For example, using the constraint on the bottom Figure 5, we first have the model make predictions about the constituent propositions (1. Derick is the father of Lisa, 2. Qiana is the wife of Derick. 3. etc..) and see if those predictions symbolically satisfy the constraint.</p>
<p>For TRIP, we follow exactly the 3-tiered evaluation of Storks et al. (2021). We calculate: Plausibility (task 1): \% of instances where the implausible story was correctly identified. Consistency (task 2): \% of correctly identified implausible stories where the conflicting sentences were correctly identified. Verifiability (task 3): \% of instances with correct plausibility/consistency predictions, where all relevant physical states are also identified.</p>
<h2>6 Results and Discussion</h2>
<p>We focus on the following questions: 1. Can our main model effectively and efficiently solve our new belief proposition prediction tasks (introduced in § 4) and model intermediate state? 2. Can we effectively integrate our breakpoint model into joint models for solving more complex tasks?
Proposition Prediction We found breakpoint models to be effective at our proposition prediction tasks, most notably improving on the transformer</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Table 1: TOP: Proposition prediction results on CLUTRR on the main mix dev and test sets comparing our breakpoint model (BPT) with baselines. BOTTOM: Evaluation on standard CLUTRR QA $(k=2,3)$ comparing our breakpoint model trained joint with QA to fined-tuned (FT) T5 and Bart models.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Effect of training data size on proposition prediction (left) and global consistency $\rho$ (right, lower is better), on CLUTRR dev (best of 3 random runs).</p>
<p>Multi-pass baselines for CLUTRR prediction from 81.9 to 85.2 (top of Table 1, both an over $23 \%$ improvement over our BILSTM baseline, suggesting task difficulty). Based on the plots in Figure 6, we also found our models to be more efficient learners (e.g., achieving comparable performance to baselines using only $60 \%$ training data) and to exhibit less global constraint violations in the i.i.d setting (with a $6 \%$ reduction in constraint violations $\rho$ ), thus leading to more consistent belief states.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">i.i.d</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">hard QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prop.\%</td>
<td style="text-align: center;">QA\%</td>
<td style="text-align: center;">QA\%</td>
</tr>
<tr>
<td style="text-align: center;">Majority</td>
<td style="text-align: center;">65.87</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">FT-T5-base (QA)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">97.29 (20.14)</td>
<td style="text-align: center;">69.09 (20.79)</td>
</tr>
<tr>
<td style="text-align: center;">FT-Bart-base</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">97.57 (20.31)</td>
<td style="text-align: center;">67.21 (20.80)</td>
</tr>
<tr>
<td style="text-align: center;">BILSTM (Multi)</td>
<td style="text-align: center;">80.2 (20.18)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">T5-base (Multi)</td>
<td style="text-align: center;">99.1 (20.21)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BPT-base</td>
<td style="text-align: center;">98.5 (20.10)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BPT-base + QA</td>
<td style="text-align: center;">98.5 (20.10)</td>
<td style="text-align: center;">94.9 (20.60)</td>
<td style="text-align: center;">70.51 (20.20)</td>
</tr>
</tbody>
</table>
<p>Table 2: bAbI proposition prediction (Prop. \%) and QA performance on the main i.i.d and hardQA test sets.</p>
<p>For bAbI (Table 2) all transformer-based models achieve near perfect accuracy (and significantly outperform our BILSTM model); as such, models have near perfect consistency on the underlying constraints (not shown). Given that bAbI stories are considerably longer than CLUTRR stories</p>
<p>(each containing 20 events/breakpoints), these results show the feasibility of modeling long contexts with our model and representing complex state information with individual breakpoints. In contrast to the baseline transformers, here we also see considerable practical improvements in training time efficiency due to our single read architecture, resulting in a $54 \%$ reduction in training time (from around 63 hours for multi-pass models to around 34 for ours on a single RTX A6000 GPU).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">CLUTRR i.i.d vs. generalization (gener.) splits</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">i.i.d $(E=2, .5)$</td>
<td style="text-align: left;">gener.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Baseline (best run)</td>
<td style="text-align: left;">94.54 ( $\rho=32.1$ )</td>
<td style="text-align: left;">61.7 ( $\rho=90.2$ )</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BPT (best run)</td>
<td style="text-align: left;">95.69 ( $\rho=25.9$ )</td>
<td style="text-align: left;">$\mathbf{6 9 . 2}(\rho=97.6)$</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison between i.i.d and compositional settings for CLUTRR.</p>
<p>Our model's proposition prediction consistency is $7.5 \%$ higher than that of the baseline, in terms of the $\rho$ metric reported in Table 3. As an important caveat, however, in absolute terms, even our breakpoint model has much lower consistency on generalizations tasks (69.2\%) than in the i.i.d. setting ( $95.7 \%$ ). We discuss this further in $\S 8$.
Joint Training When trained jointly for both proposition prediction and QA, we found minimal to no impact on end-task performance, as shown on the bottom of Table 1 for CLUTRR and in Table 2 for bAbl (with a small improvement on the generalization QA task at the cost of a mere $2 \%$ degradation in i.i.d. QA performance). This shows the viability of integrating our belief tracking mechanism into existing transformer pipelines without significant performance drops. As first motivated in Figure 1, it also permits the development of more debuggable systems where the results of QA can be checked against the model's beliefs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Split</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Task 1(Plaus.)</th>
<th style="text-align: center;">Task 2(Consist.)</th>
<th style="text-align: center;">Task 3 (Verif.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">RoB</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">10.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BPT-base</td>
<td style="text-align: center;">81.99 ( $\pm 0.91$ )</td>
<td style="text-align: center;">58.07 ( $\pm 0.76$ )</td>
<td style="text-align: center;">36.44 ( $\pm 0.53$ )</td>
</tr>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">RoB</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">9.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BPT-base</td>
<td style="text-align: center;">80.55 ( $\pm 1.20$ )</td>
<td style="text-align: center;">53.83 ( $\pm 1.65$ )</td>
<td style="text-align: center;">32.37 ( $\pm 0.27$ )</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on the TRIP 3-tiered physical commonsense reasoning benchmark, our main breakpoint model (BPT) compared against the RoBERTa-based approach (RoB) of Storks et al. (2021).</p>
<p>Through our results on TRIP (Table 4), we also see the viability of adding our belief tracking mechanism into more complex modeling pipelines. We were specifically able to obtain SOTA performance on this task and outperform the larger and highly tailored task-specific model architecture based on</p>
<p>RoBERTa-large used by Storks et al. (2021).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">CLUTRR (mix dev)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prop. Acc\%</td>
<td style="text-align: center;">Global Violations $\rho$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BPT-large (best run)</td>
<td style="text-align: center;">85.5 ( $\Delta$ )</td>
<td style="text-align: center;">36.7 ( $\Delta$ )</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- brk self-attn</td>
<td style="text-align: center;">77.3 ( 8.12 )</td>
<td style="text-align: center;">54.3 ( 37.61 )</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- event generation</td>
<td style="text-align: center;">82.1 ( 3.36 )</td>
<td style="text-align: center;">41.8 ( 5.07 )</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- abstraction</td>
<td style="text-align: center;">82.1 ( 3.35 )</td>
<td style="text-align: center;">42.8 ( 6.06 )</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BPT-base</td>
<td style="text-align: center;">81.8 ( 3.62 )</td>
<td style="text-align: center;">44.3 ( 7.61 )</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TRIP (tiered dev)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BPT-base (best run)</td>
<td style="text-align: center;">92.8 ( $\Delta$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- brk self-attn</td>
<td style="text-align: center;">89.43 ( 3.36 )</td>
<td style="text-align: center;">- - - - - - - - -</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- event generation</td>
<td style="text-align: center;">89.43 ( 3.36 )</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- abstraction</td>
<td style="text-align: center;">92.9 ( $\pm 0.10$ )</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Breakpoint model feature ablations.
Additional Analysis We see in Table 5 for CLUTRR that having an additional self-attention aggregation layer when constructing breakpoint representations (-brk self-attn, § 5.1) is very important for accuracy and consistency (we find similar results for TRIP, bottom). This suggests that further improvements might be achieved through improved pooling and masking strategies for constructing breakpoint representations. We also see the advantages of having auxiliary generation losses (event generation, abstraction) for improving accuracy and performance.</p>
<h2>7 Conclusion</h2>
<p>Being able to track the beliefs of models remains a formidable challenge at the forefront of model interpretability. In this paper, we presented a new representation learning framework, breakpoint modeling, that facilitates end-to-end learning and tracking of beliefs at intermediate states in narrative text. On a diverse set of NLU tasks, we show the benefit of our approach (based on T5) over conventional learning approaches in terms of improved belief prediction performance on new belief tracking tasks and processing efficiency. We also show the feasibility of recasting existing tasks into our framework and integrating our approach into existing transformer-based NLU pipelines, which we believe can help to improve the interpretability of these models as part of this larger challenge.</p>
<h2>Acknowledgements</h2>
<p>The authors thank the Aristo team for valuable feedback. This work was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant no. 852686, SIAM, Shahaf). Part of this research was also supported by the European Research Council, ERC-StG grant no. 677352 (Tsarfaty), which is gratefully acknowledged.</p>
<h2>8 Limitations</h2>
<p>Below we summarize the main limitations of our current breakpoint models and the techniques pursued in this study.</p>
<p>Compositional Generalization Despite richer supervision over intermediate states, compositional generalization performance remains a significant challenge (on bAbI and CLUTRR generalization splits, see §6) for future work, which shows that our approach inherits many of the limitations in the generalization ability of large-scale LMs more broadly. Following Kim et al. (2021) and others, we hypothesize that the all-to-all attention employed by Transformers in creating token encodings (including the breakpoint tokens) is a factor in non-compositional behavior; such attention is more vulnerable to overfitting spurious patterns. Accordingly, more advanced attention masking (Kim et al., 2021) and supervision (Yin et al., 2021) approaches are promising directions to explore.</p>
<p>Our notion of "belief" While breakpoints provide an indication of intermediate model "beliefs", they are also different from beliefs in important ways. In particular, the causal relation between information represented in breakpoints and generated model outputs is unclear (see also Li et al. (2021) for similar caveats in standard NLMs). For example, models may generate outputs that are inconsistent with their own breakpoint belief states. Interestingly, breakpoint models may offer new ways to address these limitations by more explicitly representing intermediate reasoning steps; neural logic losses (Li et al., 2019) can help enforce belief consistency between sets of propositions (§3.3).</p>
<p>Task and domain limitations Finally, our experiments are still limited to datasets involving relatively short (TRIP) and synthetic (bAbI, CLUTRR) inputs with limited semantics. Further work is needed to address more natural and complex language to ultimately develop more robust breakpoint models. In contrast to standard end-to-end QA methods, breakpoint modeling requires more costly annotation, as training currently requires some form of supervision on intermediate states, beyond the final target output. Thus, developing new methods for collecting such annotations with minimal engineering effort remains a challenge.</p>
<h2>References</h2>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Learning to compose neural networks for question answering. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1545-1554, San Diego, California. Association for Computational Linguistics.</p>
<p>Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and Greg Durrett. 2022. Natural language deduction through search over statement compositions. arXiv preprint arXiv:2201.06028.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31.</p>
<p>Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670-680, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer.</p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358-7370, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. 2020. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402.</p>
<p>Stefan L. Frank, Mathieu Koppen, Leo G.M. Noordman, and Wietske Vonk. 2003. Modeling knowledgebased inferences in story comprehension. Cognitive Science, 27(6):875-910.</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D Goodman, and Christopher Potts. 2021. Inducing causal structure for interpretable neural networks. arXiv preprint arXiv:2112.00826.</p>
<p>Richard M. Golden and David E. Rumelhart. 1993. A parallel distributed processing model of story comprehension and recall. Discourse Processes, 16(3):203237.</p>
<p>Nicolas Gontier, Siva Reddy, and Christopher Pal. 2022. Does entity abstraction help generative transformers reason? arXiv preprint arXiv:2201.01787.</p>
<p>Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal. 2020. Measuring systematic generalization in neural proof generation with transformers. In Advances in Neural Information Processing Systems, volume 33, page 22231-22242. Curran Associates, Inc.</p>
<p>Andrew R Haas. 1987. The case for domain-specific frame axioms. In The Frame Problem in Artificial Intelligence, pages 343-348. Elsevier.</p>
<p>Austin W Hanjie, Ameet Deshpande, and Karthik Narasimhan. 2022. Semantic supervision: Enabling generalization over output spaces. arXiv preprint arXiv:2202.13100.</p>
<p>John Hewitt and Christopher D Manning. 2019. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129-4138.</p>
<p>Alon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4198-4205, Online. Association for Computational Linguistics.</p>
<p>Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and Peter Clark. 2021. BeliefBank: Adding memory to a pre-trained language model for a systematic notion of belief. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8849-8861, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Divyansh Kaushik and Zachary C. Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 50105015, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 3948.</p>
<p>Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2021. Text modular networks: Learning to decompose tasks in the language of existing models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1264-1279, Online. Association for Computational Linguistics.</p>
<p>Juyong Kim, Pradeep Ravikumar, Joshua Ainslie, and Santiago Ontanon. 2021. Improving compositional generalization in classification tasks via structure annotations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 637-645, Online. Association for Computational Linguistics.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Brenden Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pages 2873-2882. PMLR.</p>
<p>Fred Landman. 2012. Structures for semantics, volume 45. Springer Science \&amp; Business Media.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021. Implicit representations of meaning in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1813-1827, Online. Association for Computational Linguistics.</p>
<p>Tao Li, Vivek Gupta, Maitrey Mehta, and Vivek Srikumar. 2019. A logic-driven framework for consistency of neural models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3924-3935, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Shih-Ting Lin, Ashish Sabharwal, and Tushar Khot. 2021. ReadOnce transformers: Reusable representations of text for transformers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7129-7141, Online. Association for Computational Linguistics.</p>
<p>Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022. Sentence-t5: Scalable sentence encoders from pretrained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1864-1874, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1).</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Kyle Richardson, Hai Hu, Lawrence Moss, and Ashish Sabharwal. 2020. Probing natural language inference models through semantic fragments. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 8713-8721.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4506-4515, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Shane Storks, Qiaozi Gao, Yichi Zhang, and Joyce Chai. 2021. Tiered reasoning for intuitive physics: Toward verifiable commonsense language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4902-4918, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Online. Association for Computational Linguistics.</p>
<p>Ronen Tamari, Kyle Richardson, Noam Kahlon, Aviad Sar-shalom, Nelson F. Liu, Reut Tsarfaty, and Dafna Shahaf. 2022. Dyna-bAbl: unlocking bAbl's potential with dynamic synthetic benchmarking. In Proceedings of the 11th Joint Conference on Lexical and Computational Semantics, pages 101-122, Seattle, Washington. Association for Computational Linguistics.</p>
<p>Ronen Tamari, Chen Shani, Tom Hope, Miriam R L Petruck, Omri Abend, and Dafna Shahaf. 2020. Language (re)modelling: Towards embodied language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6268-6281, Online. Association for Computational Linguistics.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Noortje J. Venhuizen, Matthew W. Crocker, and Harm Brouwer. 2019. Expectation-based comprehension: Modeling the interaction of world knowledge and linguistic experience. Discourse Processes, 56(3):229255.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomás Mikolov. 2016. Towards ai-complete question answering: A set of prerequisite toy tasks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Sarah Wiegreffe and Ana Marasovic. 2021. Teach me to explain: A review of datasets for explainable natural language processing. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1).</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Pengcheng Yin, Hao Fang, Graham Neubig, Adam Pauls, Emmanouil Antonios Platanios, Yu Su, Sam Thomson, and Jacob Andreas. 2021. Compositional generalization for neural semantic parsing via spanlevel supervised attention. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2810-2823, Online. Association for Computational Linguistics.</p>
<h2>A Dataset Details</h2>
<p>In this section, we provide additional details about all datasets.</p>
<h2>A. 1 TRIP</h2>
<p>As described in $\S 4$, the TRIP benchmark consists of 3 tiered tasks: (1) plausibility (2) consistency (3) verifiability. To apply our model to TRIP, we convert the first two tasks to a text2text format: the first task involves taking two stories (A) storyA (B) storyB \$plaus as input text and producing a text label ${A, B}$ to identify the implausible story; task 2 involves taking a labeled story sentence1 1 sentence2 2. \$conflict and generating the labels identifying the problematic sentences. ${ }^{8}$ We convert the third task to breakpoint format by converting state change labels to textual propositions associated with the corresponding timesteps. Figure 8 shows an example instance from the TRIP development set. Note that each task is effectively rendered as two instances: the first instance addresses task 1 (as QA), and the second jointly addresses tasks 2 (QA) and 3 (proposition prediction).</p>
<p>State changes in TRIP are defined either as effects or preconditions (Storks et al., 2021) and this information must be preserved in the conversion to breakpoint format. Preconditions are propositions that hold before a described event; for example, the proposition "oven was open" should be true before the sentence "John closed the oven." Effect propositions are propositions that hold after a described event; the proposition "oven is open" should be false after "John closed the oven." We represent precondition and effect propositions simply by modifying the proposition tense. Given breakpoint $b_{t}$, for associated precondition propositions at time $t$, we use past tense ("oven was open"). For effect propositions at time $t$, we use present tense ("oven is open").</p>
<p>While the TRIP data includes state information for all time steps and entities, we follow the official evaluation procedure ${ }^{9}$ and only score the subset of state changes defined to be relevant in the pair of conflict sentences. At training time, we use all available state change information for training.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Finally, while most state changes in TRIP are attributes that can be true, false or unknown (and thus can be directly converted to proposition form), location attributes are formulated as $k$-way classification problems. For example, an object location attribute change is represented by 1 of 9 possible classes (see Table 5 in Storks et al. (2021) and blue propositions in Fig. 8). To facilitate equivalent evaluation of $k$-class predictions with our breakpoint model, we consider the predicted true score for each of the possible $k$ propositions and take the maximum scoring proposition to be the predicted value. ${ }^{10}$</p>
<h2>A. 2 bAbI</h2>
<h2>A.2.1 Proposition generation</h2>
<p>As detailed in $\S 4$, base propositions for bAbI are generated using the Dyna-bAbI tool (Tamari et al., 2022). From this, new propositions are derived from the following general constraints: location/ possession uniqueness that dictate that objects can only be in one location at a time and possessed by a single agent (e.g., John cannot simultaneously be in the kitchen and living room), mutually exclusivity between event types (e.g., that dropping a ball is the opposite of picking up a ball); explanation frame rules (Haas, 1987) that dictate that objects, when left unchanged, maintain their location and their possession through time (e.g., John is in the kitchen or John has the apple stays true until there is an explicit event that changes this).</p>
<h2>A.2.2 Task details</h2>
<p>The training data includes 500 samples per task type, where the tasks follow the same structure as the concat(T7) dataset described in (Tamari et al., 2022) (Table 6 in that work), with the only difference being the story length which was fixed to 20 sentences to match the test data. The hardQA generalization task was generated using the same settings as the $\operatorname{mix}(T 7)$ evaluation set from (Tamari et al., 2022), including the same 3 question types with 1,000 samples for each type (also Table 6 in Tamari et al. (2022)). Figure 7 shows example stories from the training and hardQA test splits.</p>
<h2>A. 3 CLUTRR</h2>
<p>We note that all of the underlying story data was generated from scratch and relies on the publicly available task generators from Sinha et al. (2019)</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Examples from the bAbI story understanding task. The train set includes 7 sub-tasks, such as co-reference and object tracking (left). The hardQA sample (right) incorporates novel compositions of concepts seen separately at training time. Beyond the question answering task, each example also includes proposition prediction at each time step (not shown here, see Figure 1) for example.
and Gontier et al. (2020) ${ }^{11}$. As detailed in Gontier et al. (2020), leakage among the proofs and propositions in stories of the same $k$ can be a problem. Using some of their ideas, we avoided this by expanding the inventory of names used in training and abstracted names for parts of the training. We verified the hardness of our data by training a nostory proposition-only baseline an found it to have low performance, and also manually verified all inference rules used for generating propositions.</p>
<h2>B Training details</h2>
<h2>B. 1 Hyper-parameters</h2>
<p>All hyper-parameter tuning for our main models was performed via a random search in the style of Devlin et al. (2019). Model selection was performed by selecting models with the highest validation accuracy for each task (e.g., proposition accuracy for our proposition tasks, exact match for the QA experiments). Unless noted otherwise, we report the average of models with the optimal hyperparameters based on 3 random re-starts; early stopping was applied throughout. All experiments were performed on NVIDIA AX6000 GPU hardware on a single GPU.
breakpoint models: learning rate (we experimented in the range of $1 \mathrm{e}-3$ to $5 \mathrm{e}-6$, we generally found $5 \mathrm{e}-5$ to be optimal for most experi-</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ments), number of epoch (up to 35 for CLUTRR, TRIP and 150 for bAbI), batch size (in the range of 2 to 16 , memory permitting, we found 2 to be optimal for bAbI and TRIP experiments, and 4 for CLUTRR) and weight decay (set to 0.001 ) and warmup steps (from 500 to 1 k steps). See the project repository for further details
joint models For multi-task training, parameters $\lambda_{{1,2,3}}$ were hand tuned, with $\lambda_{1}$ set to 1.0 for all proposition prediction tasks (with $\lambda_{2}=0.1$ for most tasks). For joint QA tasks, we found setting $\lambda_{1}=1.0$ and $\lambda_{1}=1.0$ to be optimal, with an initial warmup before turning on the proposition prediction loss (usually between $5-10$ epochs). Given the high cost of training the bAbI breakpoint QA model in Table 2, the joint QA + prop models described on the last row start training from the BPT-base checkpoints described in the row above.</p>
<h2>B. 2 Auxiliary Generation Losses</h2>
<p>As detailed in $\S 5.1$, we jointly trained our breakpoint models with additional generation losses that aim to mimic some of the unsupervised denoising objectives used in Raffel et al. (2020). Whereas in standard denoising you might try to generate from a text input A dog <mask> while running the output text <mask> barked loudly <mask>, from an original text A dog barked loudly while running (with full attention over the input text), in our case we try to generate from a story John went to the store [B]; He then picked up the</p>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">(</span><span class="n">plausibility</span><span class="p">)</span>
<span class="err">{</span>
<span class="w">    </span><span class="ss">&quot;example id&quot;</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;414-C0-a&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="ss">&quot;question&quot;</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;(A) John turned on the oven [B] John put the cake in the oven [</span>
<span class="ss">        B] John got the ice cream out [B] John put some ice cream in a red bowl</span>
<span class="ss">            [B] John put the red bowl in the oven [B] (B) John turned on the oven [B</span>
<span class="ss">            ] John put the cake in the oven [B] John got the ice cream out [B] John</span>
<span class="ss">            put some ice cream in a red bowl [B] John put the rest of the ice cream</span>
<span class="ss">            in the fridge [B] $plaus&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="ss">&quot;answer&quot;</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;B&quot;</span>
<span class="err">}</span>
<span class="err">#</span><span class="w"> </span><span class="n">Tasks</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="p">(</span><span class="n">consistency</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">verifiability</span><span class="p">)</span>
<span class="err">{</span>
<span class="w">    </span><span class="ss">&quot;example id&quot;</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;414-C0-b&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="ss">&quot;question&quot;</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;John turned on the oven 0 [B] John put the cake in the oven 1 [</span>
<span class="ss">        B] John got the ice cream out 2 [B] John put some ice cream in a red</span>
<span class="ss">            bowl 3 [B] John put the red bowl in the oven 4 [B] $conflict&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="ss">&quot;answer&quot;</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;3,4&quot;</span>
<span class="w">    </span><span class="ss">&quot;proposition_lists&quot;</span><span class="err">:</span><span class="w"> </span><span class="o">[</span>
<span class="n">            [...</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">0</span>
<span class="w">            </span><span class="o">[</span><span class="n">...</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">1</span>
<span class="w">            </span><span class="o">[</span><span class="n">...</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">2</span>
<span class="w">        </span><span class="o">[</span>
<span class="n">        &quot;red bowl is occupied&quot;,</span>
<span class="n">        &quot;ice cream is put into a container&quot;,</span>
<span class="n">        &quot;ice cream does not move to a new location&quot;,</span>
<span class="n">        &quot;ice cream disappears&quot;,</span>
<span class="n">        &quot;ice cream is picked up&quot;,</span>
<span class="n">        &quot;ice cream is put down&quot;,</span>
<span class="n">        &quot;ice cream is put on&quot;, &quot;ice cream is removed&quot;,</span>
<span class="n">        &quot;ice cream is taken out of a container&quot;,</span>
<span class="n">        &quot;ice cream moved somewhere new&quot;,...</span>
<span class="n">            </span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">3</span>
<span class="w">        </span><span class="o">[</span>
<span class="n">    &quot;red bowl is put into a container&quot;, &quot;oven was powered&quot;,</span>
<span class="n">    &quot;oven was open&quot;, &quot;oven was turned on&quot;,...</span>
<span class="n">        </span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">4</span>
<span class="w">        </span><span class="err">]</span><span class="p">,</span>
<span class="w">    </span><span class="ss">&quot;labels&quot;</span><span class="err">:</span><span class="w"> </span><span class="o">[</span>
<span class="n">        [...</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">0</span>
<span class="w">        </span><span class="o">[</span><span class="n">...</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">1</span>
<span class="w">        </span><span class="o">[</span><span class="n">...</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">2</span>
<span class="w">        </span><span class="o">[</span><span class="n">&quot;true&quot;, &quot;true&quot;, &quot;false&quot;, &quot;false&quot;, &quot;false&quot;, &quot;false&quot;, &quot;false&quot;, &quot;false&quot;,&quot;</span>
<span class="n">            false&quot;, &quot;false&quot;,...</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">3</span>
<span class="w">        </span><span class="o">[</span><span class="n">&quot;true&quot;, &quot;true&quot;,&quot;true&quot;, &quot;true&quot;,...</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">sent</span><span class="p">.</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">4</span>
<span class="w">    </span><span class="err">]</span>
<span class="err">}</span>
</code></pre></div>

<p>Figure 8: Rendering of TRIP instance in breakpoint format. Breakpoint models can operate in standard text-to-text mode, generating output answers in response to questions, and additionally they can provide joint predictions over propositions associated with each sentence. Propositions in blue indicate location attributes which are evaluated as $k$-class predictions. See Appendix A. 1 for further details on instance construction.</p>
<p>apple $[\mathbf{B}]<em 1="1">{2}$ the raw event text John went to the store from the corresponding raw breakpoint hidden state for the special token $[\mathbf{B}]</em>]}$ alone. In addition to this event generation task, we also experimented with a abstraction generation task: given two stories in a batch and two random breakpoints within those stories, e.g., John went to the kitchen $[\mathbf{B<em 2_1="2,1">{1,1}$, and Mary went to the kitchen $[\mathbf{B}]</em>]}$, we ask the model to generate an abstract textual description of the two events only from the mean of the two breakpoint hidden states, i.e., abstraction $\left([\mathbf{B<em 2_1="2,1">{1,1},[\mathbf{B}]</em>\right)=A$ person went to the kitchen. (This was inspired by the abstraction generation ideas from Gontier et al. (2022)).</p>
<p>During training, both forms of generation were done by randomly selecting a single breakpoint example and abstraction pair for each story in the batch and computing a standard loss over the generated texts and abstractions. Using symbolic annotations of both the CLUTRR and bAbI training events, a deterministic algorithm was implemented for creating abstracted texts on the fly for training. For TRIP, where logical annotations are not available, the abstraction task was replaced by the task of generating versions of text replaced with POS tags (e.g., John turned off the stove would be turned into PER turned off the NOUN).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ See full details at: https://github.com/ facebookresearch/clutrr and https: //github.com/NicolasAG/SGinPG&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{10}$ Inspired by a similar method in Li et al. (2021).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
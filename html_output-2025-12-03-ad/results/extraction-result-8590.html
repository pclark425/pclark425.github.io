<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8590 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8590</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8590</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-265128609</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.06158v1.pdf" target="_blank">Language Models can be Logical Solvers</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions. In this paper, we introduce LoGiPT, a novel language model that directly emulates the reasoning processes of logical solvers and bypasses the parsing errors by learning to strict adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning datasets demonstrate that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8590.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8590.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOGIPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOGIPT (Language model fine-tuned to emulate logical solvers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of open-source LMs (fine-tuned from Vicuna/CodeLlama) trained on solver-derived instruction-tuning data that reveal symbolic solver reasoning traces so the LM can act as a deductive solver and output implied facts directly from NL contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LOGIPT (fine-tuned Vicuna / CodeLlama variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLMs (Vicuna-13B and CodeLlama-13B variants) fine-tuned on a constructed instruction-tuning dataset derived from the internal reasoning traces of a Prolog-like solver (pyke). The dataset contains: predicate definitions, facts, rules, explicit solver actions (bind/unbind, fail & backtrack) and enumerated implied facts; greedy decoding, temperature 0 used at eval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deductive reasoning benchmarks: ProofWriter requires deriving implied facts and answering True/False/Unknown under open-world assumption; PrOntoQA (CWA subset) requires proving/disproving queries under closed-world assumptions, both expressed as natural-language facts/rules mapped to Prolog-like symbolic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning LMs on solver-derived, instruction-formatted traces that make the solver's invisible chain-of-thought explicit (distillation of solver behavior into LM). The LM is trained to output all implied facts given context and then answer queries (bypassing NL→SL parsing and external solver invocation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported best LOGIPT (CodeLlama-13b-hf) exceeds state-of-the-art solver-augmented LogicLM (GPT-4) by +9.84 percentage points on ProofWriter; LOGIPT (Vicuna-13b) exceeds LogicLM (GPT-4) by +13.20 points on PrOntoQA. Also outperforms or matches closed-source few-shot/CoT baselines in aggregate (exact per-model accuracies in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against solver-augmented LogicLM (uses external solver with GPT-style problem formulation), closed-source LMs (ChatGPT gpt-3.5-turbo, text-davinci-003, GPT-4) with standard and CoT prompting, and open-source LMs (Vicuna-13B, CodeLlama-13B base & instruct). LOGIPT substantially outperforms solver-augmented and few-shot prompting baselines; CodeLlama-13B-Base with standard prompting scored 0.00 accuracy prior to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Depends on filtered training data: training excludes cases that could not be parsed/executed by the solver (parser failures removed). Performance is sensitive to the exact format of solver traces (ablation shows mixed effects when removing 'Unbind' or 'Fail & backtrack'). Merging datasets with different world-assumption styles (OWA vs CWA) without reformatting degrades performance; replacement of symbolic-language (SL) traces with natural language significantly reduces accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Distilling explicit solver reasoning traces into LMs enables them to act as faithful deductive solvers and avoid NL→SL parsing failures; symbolic representations and explicit solver actions (bind/unbind, backtrack) are important signals; fine-tuning on solver-derived data can allow 13B open models to match or exceed much larger/closed models on structured deductive tasks; representation style and dataset genre (rule syntax) matter for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8590.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8590.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicLM (solver-augmented LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A solver-augmented approach that uses an LLM to parse NL logical problems to symbolic form and then invokes an external logical solver to perform deterministic symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogicLM (solver-augmented pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline method: use an LLM (e.g., ChatGPT / GPT-4) to convert NL contexts into symbolic (Prolog-like) facts and rules, then dispatch these to an external symbolic solver (pyke or similar) which deterministically derives implications and answers queries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same deductive benchmarks as above; LogicLM formulates problems into symbolic logic then relies on solver for exact deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neural-to-symbolic pipeline: NL→SL parsing by LLM + external logical solver for trustworthy symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as state-of-the-art prior to LOGIPT; in this paper LOGIPT (CodeLlama-13b-hf) outperforms LogicLM (GPT-4) by 9.84% on ProofWriter and LOGIPT (Vicuna) outperforms LogicLM (GPT-4) by 13.20% on PrOntoQA.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared as main SOTA baseline against LOGIPT and few-shot CoT methods (LogicLM used GPT-4/ChatGPT as the underlying LLM in the referenced implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Vulnerable to NL-to-SL parsing errors: if the LLM parser fails to produce syntactically valid symbolic input, the solver cannot run and returns no answer. Paper reports low parsing-success rates for open-source models (e.g., Vicuna had ~17% parsing-success on ProofWriter in preliminary tests).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>While solver-augmentation yields faithful symbolic answers when parsing is correct, the pipeline is brittle to parsing failures; distilling solver traces into LMs (LOGIPT) can avoid that brittleness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8590.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8590.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-13B (vicuna-13b-v1.5-16k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source chatbot model finetuned from LLaMA-2 on user-shared conversations, used here both as a baseline and as an underlying model for LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter conversational model derived by fine-tuning LLaMA-2 on ShareGPT-collected dialogues (vicuna-13b-v1.5-16k). Used for few-shot prompting baselines and as a base for LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>NL deductive reasoning requiring conversion to symbolic facts/rules and multi-step inference to prove/disprove queries; evaluated under OWA (ProofWriter) and CWA (PrOntoQA subset).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with standard few-shot prompting and chain-of-thought; also used as base model for LOGIPT fine-tuning on solver-derived traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>As a few-shot/CoT baseline, open-source Vicuna showed poor deductive performance (near-random); when fine-tuned into LOGIPT (vicuna-13b), it became a top-performing LOGIPT variant and achieved the paper's best improvement on PrOntoQA (+13.20% vs LogicLM GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed by LOGIPT when not fine-tuned; as LOGIPT variant, outperforms other baselines including closed-source LMs in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Low NL→SL parsing success in preliminary experiments (~17% parsing-success on ProofWriter) and poor few-shot compliance prior to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Conversational fine-tuning alone is insufficient for strict deductive reasoning; solver-derived fine-tuning meaningfully improves Vicuna's deductive abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8590.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8590.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama-13B-Base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama-13B-Base (CodeLlama-13b-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B code-focused foundation model evaluated for NL-to-SL parsing and reasoning; its base variant struggled with few-shot prompts but benefited from LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-13B-Base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B parameter foundation model optimized for code tasks (base HF release). Evaluated with standard and CoT prompting and used as an underlying LM for LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same deductive benchmarks requiring symbolic reasoning; CodeLlama evaluated both as-is and after LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Standard few-shot prompting, chain-of-thought, and LOGIPT fine-tuning on solver-derived traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With standard prompting CodeLlama-13B-Base achieved 0.00 accuracy (model failed to follow few-shot format); CoT close to random. After LOGIPT fine-tuning, CodeLlama variants showed large performance gains and in some cases produced the best LOGIPT result (CodeLlama-13b-hf LOGIPT beat LogicLM GPT-4 by +9.84% on ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Significant improvement after LOGIPT fine-tuning compared to base few-shot/CoT behavior; outperformed Vicuna baseline once fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Base model often failed to follow few-shot demonstrations resulting in no answer output; required instruct variant or fine-tuning to mitigate.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Code-focused foundation models can be highly effective for deductive reasoning once fine-tuned on explicit solver traces; their code-oriented pretraining appears useful after proper instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8590.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8590.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama-13B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama-13B-Instruct (CodeLlama-13b-Instruct-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned variant of CodeLlama-13B that better follows instructions and few-shot demonstrations; used as both baseline and LOGIPT base.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-13B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned 13B code foundation model (additional ~5B finetuning tokens for instruction-following properties) used for NL→SL parsing baselines and LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deductive reasoning benchmarks requiring exact symbolic inferences; tested with standard/CoT prompting and LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with few-shot prompting / CoT and fine-tuned on solver-derived traces to create LOGIPT variant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Instruction variant mitigates the few-shot compliance issue seen in the base version; after LOGIPT fine-tuning achieved much higher accuracy than base prompting and in some LOGIPT experiments surpassed Vicuna-based LOGIPT.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Better few-shot behavior than CodeLlama base; achieves strong performance as a LOGIPT base relative to other open-source LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still sensitive to representation format; overall gains depend on solver-derived training data quality and format.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Instruction-tuning improves the model's ability to follow the multi-turn instruction format of solver-derived traces, facilitating successful distillation of solver behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8590.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8590.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source conversational LLM from OpenAI used as a baseline for few-shot and solver-augmented pipelines (e.g., LogicLM underlying LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source chat-oriented LLM from OpenAI; used in paper as a baseline for standard prompting and chain-of-thought, and as the underlying LLM in solver-augmented baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tested on deductive benchmarks using standard few-shot and CoT prompting; also used as the problem-formulator in solver-augmented baselines in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting and chain-of-thought prompts; used in LogicLM-style NL→SL parsing pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With few-shot prompting ChatGPT's performance on these deductive reasoning tasks was close to random (paper states standard prompting of ChatGPT/gpt-3.5-turbo performed similar to random answering).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>LOGIPT outperforms ChatGPT few-shot/CoT prompting baselines; LogicLM implementations using ChatGPT as problem-formulator are compared in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Standard few-shot prompting failed to reliably solve strict deductive problems; NL→SL parsing remains a brittle step for many LLMs, including ChatGPT, when used in pipeline form without solver-trace fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Off-the-shelf ChatGPT few-shot/CoT does not reliably handle strict symbolic deductive tasks; augmentation or targeted fine-tuning (solver-derived traces) is needed for faithful performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8590.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8590.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-quality closed-source text-generation model used as a baseline with standard prompting and chain-of-thought evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's older text-completion model used here as a closed-source baseline for few-shot and CoT prompting on deductive reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tested with few-shot and chain-of-thought prompting on NL deductive reasoning tasks requiring exact symbolic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Standard few-shot prompting and chain-of-thought evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Standard prompting performance similar to random answering, per paper summary (explicit numeric accuracies not shown in text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed by LOGIPT fine-tuned models; served as baseline showing that generic few-shot LLM prompting is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Failed to reliably parse or deduce strict symbolic conclusions with basic prompting; exhibited near-random accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Generic text-completion models without solver-trace fine-tuning do not perform well on strict deductive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8590.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8590.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source state-of-the-art OpenAI model used both as a few-shot/CoT baseline and as the underlying LLM in the LogicLM solver-augmented baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's high-capacity multimodal/LLM model used here as a strong few-shot and chain-of-thought baseline and as the problem-formulator in LogicLM-style solver-augmented systems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluated on deductive reasoning benchmarks with standard prompting and chain-of-thought; used in prior solver-augmented SOTA (LogicLM).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting, chain-of-thought, and as the NL→SL problem formulator paired with an external solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Strong baseline overall; however LOGIPT (CodeLlama-13b-hf) reportedly surpasses LogicLM (GPT-4) by 9.84% on ProofWriter and LOGIPT (Vicuna) surpasses LogicLM (GPT-4) by 13.20% on PrOntoQA. On PrOntoQA GPT-4 with CoT was comparable to LOGIPT in one experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as SOTA baseline (LogicLM) and compared to LOGIPT; LOGIPT outperforms GPT-4-based solver-augmented pipeline in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Even GPT-4's standard and CoT prompts were outperformed by LOGIPT in some ProofWriter settings; vulnerable to NL→SL parsing brittleness when used in pipeline without solver-trace fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Very strong general reasoning LLMs can still be improved upon for strict symbolic deductive tasks by targeted fine-tuning on explicit solver traces; token-level solver actions and symbolic representations provide crucial structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Evaluating the logical reasoning ability of chatgpt and gpt-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8590",
    "paper_id": "paper-265128609",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "LOGIPT",
            "name_full": "LOGIPT (Language model fine-tuned to emulate logical solvers)",
            "brief_description": "A family of open-source LMs (fine-tuned from Vicuna/CodeLlama) trained on solver-derived instruction-tuning data that reveal symbolic solver reasoning traces so the LM can act as a deductive solver and output implied facts directly from NL contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LOGIPT (fine-tuned Vicuna / CodeLlama variants)",
            "model_description": "Open-source LLMs (Vicuna-13B and CodeLlama-13B variants) fine-tuned on a constructed instruction-tuning dataset derived from the internal reasoning traces of a Prolog-like solver (pyke). The dataset contains: predicate definitions, facts, rules, explicit solver actions (bind/unbind, fail & backtrack) and enumerated implied facts; greedy decoding, temperature 0 used at eval.",
            "model_size": "13B",
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Deductive reasoning benchmarks: ProofWriter requires deriving implied facts and answering True/False/Unknown under open-world assumption; PrOntoQA (CWA subset) requires proving/disproving queries under closed-world assumptions, both expressed as natural-language facts/rules mapped to Prolog-like symbolic representations.",
            "method_or_approach": "Fine-tuning LMs on solver-derived, instruction-formatted traces that make the solver's invisible chain-of-thought explicit (distillation of solver behavior into LM). The LM is trained to output all implied facts given context and then answer queries (bypassing NL→SL parsing and external solver invocation).",
            "performance": "Reported best LOGIPT (CodeLlama-13b-hf) exceeds state-of-the-art solver-augmented LogicLM (GPT-4) by +9.84 percentage points on ProofWriter; LOGIPT (Vicuna-13b) exceeds LogicLM (GPT-4) by +13.20 points on PrOntoQA. Also outperforms or matches closed-source few-shot/CoT baselines in aggregate (exact per-model accuracies in paper tables).",
            "baseline_comparison": "Compared against solver-augmented LogicLM (uses external solver with GPT-style problem formulation), closed-source LMs (ChatGPT gpt-3.5-turbo, text-davinci-003, GPT-4) with standard and CoT prompting, and open-source LMs (Vicuna-13B, CodeLlama-13B base & instruct). LOGIPT substantially outperforms solver-augmented and few-shot prompting baselines; CodeLlama-13B-Base with standard prompting scored 0.00 accuracy prior to fine-tuning.",
            "limitations_or_failures": "Depends on filtered training data: training excludes cases that could not be parsed/executed by the solver (parser failures removed). Performance is sensitive to the exact format of solver traces (ablation shows mixed effects when removing 'Unbind' or 'Fail & backtrack'). Merging datasets with different world-assumption styles (OWA vs CWA) without reformatting degrades performance; replacement of symbolic-language (SL) traces with natural language significantly reduces accuracy.",
            "insights_or_conclusions": "Distilling explicit solver reasoning traces into LMs enables them to act as faithful deductive solvers and avoid NL→SL parsing failures; symbolic representations and explicit solver actions (bind/unbind, backtrack) are important signals; fine-tuning on solver-derived data can allow 13B open models to match or exceed much larger/closed models on structured deductive tasks; representation style and dataset genre (rule syntax) matter for transfer.",
            "uuid": "e8590.0",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LogicLM",
            "name_full": "LogicLM (solver-augmented LM)",
            "brief_description": "A solver-augmented approach that uses an LLM to parse NL logical problems to symbolic form and then invokes an external logical solver to perform deterministic symbolic reasoning.",
            "citation_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "mention_or_use": "use",
            "model_name": "LogicLM (solver-augmented pipeline)",
            "model_description": "Pipeline method: use an LLM (e.g., ChatGPT / GPT-4) to convert NL contexts into symbolic (Prolog-like) facts and rules, then dispatch these to an external symbolic solver (pyke or similar) which deterministically derives implications and answers queries.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Same deductive benchmarks as above; LogicLM formulates problems into symbolic logic then relies on solver for exact deduction.",
            "method_or_approach": "Neural-to-symbolic pipeline: NL→SL parsing by LLM + external logical solver for trustworthy symbolic reasoning.",
            "performance": "Reported as state-of-the-art prior to LOGIPT; in this paper LOGIPT (CodeLlama-13b-hf) outperforms LogicLM (GPT-4) by 9.84% on ProofWriter and LOGIPT (Vicuna) outperforms LogicLM (GPT-4) by 13.20% on PrOntoQA.",
            "baseline_comparison": "Compared as main SOTA baseline against LOGIPT and few-shot CoT methods (LogicLM used GPT-4/ChatGPT as the underlying LLM in the referenced implementation).",
            "limitations_or_failures": "Vulnerable to NL-to-SL parsing errors: if the LLM parser fails to produce syntactically valid symbolic input, the solver cannot run and returns no answer. Paper reports low parsing-success rates for open-source models (e.g., Vicuna had ~17% parsing-success on ProofWriter in preliminary tests).",
            "insights_or_conclusions": "While solver-augmentation yields faithful symbolic answers when parsing is correct, the pipeline is brittle to parsing failures; distilling solver traces into LMs (LOGIPT) can avoid that brittleness.",
            "uuid": "e8590.1",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Vicuna-13B",
            "name_full": "Vicuna-13B (vicuna-13b-v1.5-16k)",
            "brief_description": "An open-source chatbot model finetuned from LLaMA-2 on user-shared conversations, used here both as a baseline and as an underlying model for LOGIPT fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vicuna-13B",
            "model_description": "13B-parameter conversational model derived by fine-tuning LLaMA-2 on ShareGPT-collected dialogues (vicuna-13b-v1.5-16k). Used for few-shot prompting baselines and as a base for LOGIPT fine-tuning.",
            "model_size": "13B",
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "NL deductive reasoning requiring conversion to symbolic facts/rules and multi-step inference to prove/disprove queries; evaluated under OWA (ProofWriter) and CWA (PrOntoQA subset).",
            "method_or_approach": "Evaluated with standard few-shot prompting and chain-of-thought; also used as base model for LOGIPT fine-tuning on solver-derived traces.",
            "performance": "As a few-shot/CoT baseline, open-source Vicuna showed poor deductive performance (near-random); when fine-tuned into LOGIPT (vicuna-13b), it became a top-performing LOGIPT variant and achieved the paper's best improvement on PrOntoQA (+13.20% vs LogicLM GPT-4).",
            "baseline_comparison": "Outperformed by LOGIPT when not fine-tuned; as LOGIPT variant, outperforms other baselines including closed-source LMs in some settings.",
            "limitations_or_failures": "Low NL→SL parsing success in preliminary experiments (~17% parsing-success on ProofWriter) and poor few-shot compliance prior to fine-tuning.",
            "insights_or_conclusions": "Conversational fine-tuning alone is insufficient for strict deductive reasoning; solver-derived fine-tuning meaningfully improves Vicuna's deductive abilities.",
            "uuid": "e8590.2",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CodeLlama-13B-Base",
            "name_full": "CodeLlama-13B-Base (CodeLlama-13b-hf)",
            "brief_description": "A 13B code-focused foundation model evaluated for NL-to-SL parsing and reasoning; its base variant struggled with few-shot prompts but benefited from LOGIPT fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CodeLlama-13B-Base",
            "model_description": "13B parameter foundation model optimized for code tasks (base HF release). Evaluated with standard and CoT prompting and used as an underlying LM for LOGIPT fine-tuning.",
            "model_size": "13B",
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Same deductive benchmarks requiring symbolic reasoning; CodeLlama evaluated both as-is and after LOGIPT fine-tuning.",
            "method_or_approach": "Standard few-shot prompting, chain-of-thought, and LOGIPT fine-tuning on solver-derived traces.",
            "performance": "With standard prompting CodeLlama-13B-Base achieved 0.00 accuracy (model failed to follow few-shot format); CoT close to random. After LOGIPT fine-tuning, CodeLlama variants showed large performance gains and in some cases produced the best LOGIPT result (CodeLlama-13b-hf LOGIPT beat LogicLM GPT-4 by +9.84% on ProofWriter).",
            "baseline_comparison": "Significant improvement after LOGIPT fine-tuning compared to base few-shot/CoT behavior; outperformed Vicuna baseline once fine-tuned.",
            "limitations_or_failures": "Base model often failed to follow few-shot demonstrations resulting in no answer output; required instruct variant or fine-tuning to mitigate.",
            "insights_or_conclusions": "Code-focused foundation models can be highly effective for deductive reasoning once fine-tuned on explicit solver traces; their code-oriented pretraining appears useful after proper instruction tuning.",
            "uuid": "e8590.3",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CodeLlama-13B-Instruct",
            "name_full": "CodeLlama-13B-Instruct (CodeLlama-13b-Instruct-hf)",
            "brief_description": "Instruction-tuned variant of CodeLlama-13B that better follows instructions and few-shot demonstrations; used as both baseline and LOGIPT base.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CodeLlama-13B-Instruct",
            "model_description": "Instruction-finetuned 13B code foundation model (additional ~5B finetuning tokens for instruction-following properties) used for NL→SL parsing baselines and LOGIPT fine-tuning.",
            "model_size": "13B",
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Deductive reasoning benchmarks requiring exact symbolic inferences; tested with standard/CoT prompting and LOGIPT fine-tuning.",
            "method_or_approach": "Evaluated with few-shot prompting / CoT and fine-tuned on solver-derived traces to create LOGIPT variant.",
            "performance": "Instruction variant mitigates the few-shot compliance issue seen in the base version; after LOGIPT fine-tuning achieved much higher accuracy than base prompting and in some LOGIPT experiments surpassed Vicuna-based LOGIPT.",
            "baseline_comparison": "Better few-shot behavior than CodeLlama base; achieves strong performance as a LOGIPT base relative to other open-source LMs.",
            "limitations_or_failures": "Still sensitive to representation format; overall gains depend on solver-derived training data quality and format.",
            "insights_or_conclusions": "Instruction-tuning improves the model's ability to follow the multi-turn instruction format of solver-derived traces, facilitating successful distillation of solver behavior.",
            "uuid": "e8590.4",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ChatGPT (gpt-3.5-turbo)",
            "name_full": "ChatGPT (GPT-3.5 Turbo)",
            "brief_description": "A closed-source conversational LLM from OpenAI used as a baseline for few-shot and solver-augmented pipelines (e.g., LogicLM underlying LLM).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Closed-source chat-oriented LLM from OpenAI; used in paper as a baseline for standard prompting and chain-of-thought, and as the underlying LLM in solver-augmented baselines.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Tested on deductive benchmarks using standard few-shot and CoT prompting; also used as the problem-formulator in solver-augmented baselines in related work.",
            "method_or_approach": "Few-shot prompting and chain-of-thought prompts; used in LogicLM-style NL→SL parsing pipelines.",
            "performance": "With few-shot prompting ChatGPT's performance on these deductive reasoning tasks was close to random (paper states standard prompting of ChatGPT/gpt-3.5-turbo performed similar to random answering).",
            "baseline_comparison": "LOGIPT outperforms ChatGPT few-shot/CoT prompting baselines; LogicLM implementations using ChatGPT as problem-formulator are compared in paper.",
            "limitations_or_failures": "Standard few-shot prompting failed to reliably solve strict deductive problems; NL→SL parsing remains a brittle step for many LLMs, including ChatGPT, when used in pipeline form without solver-trace fine-tuning.",
            "insights_or_conclusions": "Off-the-shelf ChatGPT few-shot/CoT does not reliably handle strict symbolic deductive tasks; augmentation or targeted fine-tuning (solver-derived traces) is needed for faithful performance.",
            "uuid": "e8590.5",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-3.5 (text-davinci-003)",
            "name_full": "GPT-3.5 (text-davinci-003)",
            "brief_description": "A high-quality closed-source text-generation model used as a baseline with standard prompting and chain-of-thought evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5 family)",
            "model_description": "OpenAI's older text-completion model used here as a closed-source baseline for few-shot and CoT prompting on deductive reasoning benchmarks.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Tested with few-shot and chain-of-thought prompting on NL deductive reasoning tasks requiring exact symbolic inference.",
            "method_or_approach": "Standard few-shot prompting and chain-of-thought evaluation.",
            "performance": "Standard prompting performance similar to random answering, per paper summary (explicit numeric accuracies not shown in text excerpt).",
            "baseline_comparison": "Outperformed by LOGIPT fine-tuned models; served as baseline showing that generic few-shot LLM prompting is insufficient.",
            "limitations_or_failures": "Failed to reliably parse or deduce strict symbolic conclusions with basic prompting; exhibited near-random accuracy.",
            "insights_or_conclusions": "Generic text-completion models without solver-trace fine-tuning do not perform well on strict deductive tasks.",
            "uuid": "e8590.6",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A closed-source state-of-the-art OpenAI model used both as a few-shot/CoT baseline and as the underlying LLM in the LogicLM solver-augmented baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's high-capacity multimodal/LLM model used here as a strong few-shot and chain-of-thought baseline and as the problem-formulator in LogicLM-style solver-augmented systems.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Evaluated on deductive reasoning benchmarks with standard prompting and chain-of-thought; used in prior solver-augmented SOTA (LogicLM).",
            "method_or_approach": "Few-shot prompting, chain-of-thought, and as the NL→SL problem formulator paired with an external solver.",
            "performance": "Strong baseline overall; however LOGIPT (CodeLlama-13b-hf) reportedly surpasses LogicLM (GPT-4) by 9.84% on ProofWriter and LOGIPT (Vicuna) surpasses LogicLM (GPT-4) by 13.20% on PrOntoQA. On PrOntoQA GPT-4 with CoT was comparable to LOGIPT in one experiment.",
            "baseline_comparison": "Used as SOTA baseline (LogicLM) and compared to LOGIPT; LOGIPT outperforms GPT-4-based solver-augmented pipeline in the reported experiments.",
            "limitations_or_failures": "Even GPT-4's standard and CoT prompts were outperformed by LOGIPT in some ProofWriter settings; vulnerable to NL→SL parsing brittleness when used in pipeline without solver-trace fine-tuning.",
            "insights_or_conclusions": "Very strong general reasoning LLMs can still be improved upon for strict symbolic deductive tasks by targeted fine-tuning on explicit solver traces; token-level solver actions and symbolic representations provide crucial structure.",
            "uuid": "e8590.7",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
            "rating": 1,
            "sanitized_title": "evaluating_the_logical_reasoning_ability_of_chatgpt_and_gpt4"
        }
    ],
    "cost": 0.014374999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models can be Logical Solvers
10 Nov 2023</p>
<p>Jiazhan Feng fengjiazhan@pku.edu.cn 
Peking University
Beijing</p>
<p>Ruochen Xu 
Microsoft Azure AI
Redmond</p>
<p>Junheng Hao junhenghao@microsoft.com 
Microsoft Azure AI
Redmond</p>
<p>Hiteshi Sharma 
Microsoft Azure AI
Redmond</p>
<p>Yelong Shen 
Microsoft Azure AI
Redmond</p>
<p>Dongyan Zhao zhaody@pku.edu.cn 
Peking University
Beijing</p>
<p>Weizhu Chen wzchen@microsoft.com 
Microsoft Azure AI
Redmond</p>
<p>Language Models can be Logical Solvers
10 Nov 202303ED0BE6B08ACDCDB5429424B2795E1FarXiv:2311.06158v1[cs.CL]Green('Charlie'True) Green('Charlie'False)
Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decisionmaking.Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge.The stateof-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers.Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions.In this paper, we introduce LOGIPT, a novel language model that directly emulates the reasoning processes of logical solvers and bypasses the parsing errors by learning to strict adherence to solver syntax and grammar.LOGIPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers.Experimental results on two public deductive reasoning datasets demonstrate that LOGIPT outperforms state-of-the-art solver-augmented LMs and fewshot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
<p>Introduction</p>
<p>Logical reasoning is a foundational element of human intelligence, holding a pivotal role in tasks like problem-solving, decision-making, and critical thinking (Huang and Chang, 2023).Recently, substantial advancements have been achieved in the field of NLP through the development of large language models (LLMs) (OpenAI, 2022(OpenAI, , 2023;;Google, 2023;Touvron et al., 2023a,b).It has been noted that language models (LMs) could potentially display reasoning capabilities when they reach a certain scale threshold (e.g., training compute, model parameters, etc.) (Kaplan et al., 2020;Wei et al., 2022a;Hoffmann et al., 2022).To this end, LLMs can answer logical questions with explicit reasoning steps when prompted with a simple snippet: "Let's think step by step."(Kojima et al., 2022) or step-wise explanations of reasoning (i.e., "chain of thoughts") (Wei et al., 2022b).</p>
<p>While LLMs have made significant progress, complex logical reasoning remains challenging (Valmeekam et al., 2022;Liu et al., 2023b).Some prior work (Tafjord et al., 2022;Ling et al., 2023) aimed to enable LMs to perform logical reasoning via specialized module fine-tuning, where reasoning is in natural language (NL).However, the ambiguity and complexity of NL can lead to undesired issues like hallucinations and unfaithful reasoning (Saparov and He, 2023;Gao et al., 2023).To this end, recent work has begun to augment LLMs with access to external Solvers (Chen et al., 2022;Ye et al., 2023;Pan et al., 2023).In this paper, we focus on the logical solvers, which are theorem provers that can be any automated reasoning tool for checking the truth value of logical formulas in symbolic language (SL).Invoking logical solvers can guarantee the accuracy of logical reasoning and relieve the burden of LLMs to execute intricate and precise deductive reasoning.</p>
<p>The data flow of the aforementioned solveraugmented LMs is depicted in Figure 1(a).At the outset, the information of logical questions is stored in NL.It is subsequently fed into a LM for parsing into a symbolic representation suitable for solver-input format.Finally, the SL information is dispatched to a symbolic solver, which yields the truth value of the logical question.However, during this process, any NL-to-SL parsing errors will inevitably result in the failure of the reasoning process and no answer to the question.In our pre- liminary experiments, we observed that the parsing successful rate (i.e., percentage of executable logical formulations) of Vicuna-13B (Chiang et al., 2023) on ProofWriter (Tafjord et al., 2021) is only 17%, significantly below the expected performance.</p>
<p>In addressing parsing failures, current methods either directly use LLMs to reason in NL solely or rely on the solver's erroneous message to regenerate parsing results, but these approaches don't fundamentally resolve the problem.</p>
<p>In this paper, we introduce LOGIPT, a novel LM designed to mimic the reasoning process of logical solvers, enabling it to solve deductive reasoning tasks.We first construct an instruction-tuning dataset containing NL logical questions and their corresponding solver's symbolic reasoning process.After filtering out cases having invalid syntax, we fine-tune open-source LMs like Vicuna or CodeLlama (Roziere et al., 2023) with this data to create LOGIPT.Then, LOGIPT can generate all implied facts given premises and rules, allowing us to determine the truth value of a logical query by matching it with implied facts or outputting 'unknown' if it cannot be determined.The data flow of our pipeline is presented in Figure 1(b,c).We can bypass the syntax or grammatical errors derived from NL-to-SL parsing by directly outputting the answers with a fine-tuned LOGIPT.</p>
<p>Our approach is akin to the process of distillation, whereby we distill knowledge from a symbolic model (i.e., solver) into a neural network (i.e., LM).However, the reasoning process of solvers is invisible to users and we can only obtain the answers without intermediate reasoning steps.We design a pipeline to reveal and formalize solvers' invisible reasoning processes, creating instructiontuning datasets with visible and interpretable sym-bolic reasoning steps (see Figure 3).</p>
<p>Our main contributions are three-fold:</p>
<p>• To the best of our knowledge, we are the first to propose empowering LLMs to directly learn the reasoning process of logical solvers, thereby acquiring similar reasoning capability for addressing deductive reasoning tasks.• Our proposed LOGIPT, can directly act as a deductive solver and output all Facts implied from NL logical questions while bypassing the syntax or grammatical errors derived from NL-to-SL parsing of solver-augmented LMs.• Evaluation results on two public deductive reasoning datasets show that LOGIPT can outperform state-of-the-art solver-augmented LMs, and few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
<p>Preliminary</p>
<p>Deductive Reasoning</p>
<p>Deductive reasoning is an essential type of logical reasoning problem.It typically commences with known facts and rules from logical context, then proceeds through a series of inference steps until the query can be proved or disproved (Poole and Mackworth, 2010).In this paper, we consider the Prolog logic programming language (Clocksin and Mellish, 2003;Körner et al., 2022), which stands as the most prominent symbolic language for describing deductive reasoning problems.We showcased a deductive reasoning question along with its corresponding Prolog syntax representation in Figure 2.</p>
<p>For each question, we denote the NL description as Context.The Context can further be parsed into Facts, Rules, and Query1 .Specifically, a Fact F = P (a 1 , • • • , a t ) is a symbolic statement with a predicate P and t arguments {a 1 , • • • , a t } where a i can be a variable, entity, number or bool.For example, Green('Charlie', True) means "Charlie is green"; Rules are presented in the form of clauses
F 1 ∧ • • • ∧ F m → F m+1 ∧ • • • ∧ F n , where F i is a Fact. The Rule means "if each F i ∈ {F 1 , • • • , F m }
is true, then we can imply that all Facts in {F m+1 , • • • , F n } are also true."For example, Furry($x, True) → Quiet($x, True) indicates if variable $x is furry, then $x is quiet; a Query Q is also in the format of a Fact that needs to be proved based on Facts and Rules.</p>
<p>Solver-augmented LMs</p>
<p>Solver-augmented LMs have demonstrated remarkable performance in deductive reasoning tasks.As shown in Figure 1(a), these model can be generally divided into two stages: Problem Formulation (from LMs to Symbolic Solvers) and Symbolic Reasoning (from Symbolic Solvers to Answers).</p>
<p>In Problem Formulation stage, an LM is used to parse an NL logical question into symbolic representation (Figure 2).The process can be accomplished by providing LM with detailed instructions about the grammar of Prolog, alongside a few demonstrations as in-context examples (Ouyang et al., 2022).The LM is expected to identify the symbolic Facts, Rules, and Query from the NL logical question following the instructions; In Symbolic Reasoning stage, a solver takes in the symbolic representation obtained in the previous stage and conducts symbolic reasoning.The reasoning process of external off-the-shelf solver, e.g., pyke expert system (Frederiksen, 2008), is deterministic and invisible.Then, the truth value of the parsed Query, which is the only output of the solver, can be treated as the answer to the given question.</p>
<p>Analysis on the Parsing Successful Rate</p>
<p>Through the aforementioned two phases, once the solver-augmented LMs correctly formulate the problem, the answers obtained through symbolic reasoning will be faithful, attributed to the deterministic nature of the solver.However, this heavily relies on the in-context learning capabilities of LMs.Therefore, we first calculate the parsing successful rate of three selected open-source LLMs on two deductive reasoning datasets in Table 1.Firstly, we observe that CodeLlama-13B-Base (CodeLlama-13b-hf) is unable to effectively conduct NL-to-SL parsing due to the limited in-context learning capabilities in natural languages.Then we can find that replacing the Base model with the Instruct version (CodeLlama-13b-Instruct-hf) can alleviate this issue, which may be attributed to the fact that the Instruct version is further finetuned with an additional approx.5B tokens to better follow human instructions.Overall, open-source LLMs still exhibit parsing performance significantly lower than expected in some cases.</p>
<p>LoGiPT</p>
<p>In this paper, we aim to mitigate the parsing issue and present a novel LM, LOGIPT instructed to imitate the logical reasoning process of Solvers for deductive reasoning tasks.To achieve this, we first reveal the solver reasoning process when solving logical problems ( §3.1).Then, we construct a solver-derived instruction-tuning dataset, comprising NL logical questions and corresponding SL reasoning steps ( §3.2).Finally, we fine-tune open-source LLMs using this dataset to develop LOGIPT ( §3.3).</p>
<p>Revealing the Solver Reasoning Process</p>
<p>Before operating on the solvers, we first adopt gpt-4 as the problem formulator for NL-to-SL parsing with instructions about the grammar and</p>
<p>Turn-1 (from human):</p>
<p>Task Description: Given a statement containing logical facts and rules in natural languages as context, the task is to:</p>
<ol>
<li>define all the predicates in logic forms from the context 2. parse the statement into known facts and rules based on the defined predicates 3. begin reasoning to obtain all implied facts Context: Charlie is green.Charlie is white.Dave is blue.Dave is furry.Dave is white.Fiona is blue.Fiona is green.Fiona is nice.Fiona is rough.Fiona is white.All furry people are quiet.All green, white people are nice.If someone is blue and quiet then they are green.</li>
</ol>
<p>Reasoning:</p>
<p>Turn-2 (from LM):</p>
<p>We first define following predicates and corresponding natural language explanations: Green($x, bool) ::: Is x green?(... more predicates here ...)</p>
<p>We have following known facts from the context: Green('Charlie', True) (... more facts here ...)</p>
<p>We have following known rules from the context: rule1:
Furry($x, True) → Quiet($x, True) rule2: Green($x, True) ∧ White($x, True) → Nice($x, True) rule3: Blue($x, True) ∧ Quiet($x, True) → Green($x,</p>
<p>Turn-3 (from human):</p>
<p>Question: Based on the above information, is the following comment true, false, or unknown?Dave is not green.
Options: A) True B) False C) Unknown
Now you should:</p>
<ol>
<li>reason first carefully with the above known facts and implied facts 2. then output your answer strictly in the format of "So, the correct option is: X".The X should be replaced by "A) True", "B) False" or "C) Unknown"</li>
</ol>
<p>Turn-4 (from LM):</p>
<p>The comment can be parsed into: Green('Dave', False) According to the implied fact: Green('Dave', True)</p>
<p>The comment contradicts the fact.So, the correct option is: B) False</p>
<p>Figure 3: A comprehensive 4-turn training example of our instruction-tuning data.We highlight the initial occurrences of each functionality described in §3.1 using the corresponding colors.We omit some predicates and Facts in Turn-2 due to limited space.Hint: this figure is color-sensitive.</p>
<p>few-shot demonstrations 2 , and obtain the SL representations of all training logical questions of the given logical datasets.Then, consistent with solveraugmented methods, we adopt pyke expert system as the symbolic solver in this work that can make inferences using the Prolog symbolic language.Given a logical question, pyke first sets up a knowledge base and injects all known Facts and Rules (Figure 2) from solver's inputs.Then, it iteratively applies Rules on already known or implied Facts, aiming at obtaining more implied Facts until the Query is proved or disproved.The reasoning process executed by pyke solver is invisible to users and solver-augmented LMs use the solver as a black-box.We hypothesis the 'chain-of-thought' reasoning process of the solver is valuable and LLMs are able to learn from it.To this end, we first modify the source code of the pyke 3 to achieve the following functionalities: With the aforementioned instructions, we can obtain the revealed solver's reasoning process for the construction of training data.We also high-lighted the initial occurrences of each functionality using the corresponding colors in Figure 3 (Turn-2), where a case will be described in detail in the next section.</p>
<p>Constructing the Instruction-tuning Data</p>
<p>However, as previously mentioned, we cannot guarantee that LMs can definitely complete the NL-to-SL parsing on arbitrary questions.To this end, we first filter out all unsuccessfully parsed training cases that cannot be executed by pyke.Then we reorganize and refine the filtered training data to enhance the interpretability of the solver-derived reasoning steps.For each case, we divide the reasoning process into four conversational turns (Turn-1&amp;3 for human and Turn-2&amp;4 for LM), which will be described elaborately in the following paragraphs.We also provide a comprehensive training example of our instruction-tuning data4 in Figure 3, and the full version is also included in Appendix C.</p>
<p>Turn-1: Instructions &amp; NL logical Context.</p>
<p>For each NL logical question within the training set, we begin by stripping away the specific Query statement while retaining the question Context and subsequently integrating it with elaborately crafted instructions.Taking the case in Figure 3 as an example, we temporarily exclude the Query 'Dave is not green' from the 'Context' field.Here, we only consider Query-agnostic question description to ensure that LMs initially focus on the logical background itself.This is because sometimes the ground-truth answer is 'Unknown' (e.g., cases in ProofWriter).The truth value of the Query cannot be inferred from the Context, and therefore we need to deduce all implied Facts first.</p>
<p>Turn-2: Query-agnostic Solver-derived Reasoning.As we have acquired the solver's symbolic reasoning data in the revealing phase, our goal in Turn-2 is to further refine and enhance the reasoning process to achieve a more readable form of the solver's reasoning process.Specifically, for each logical question, we first define all necessary predicates and append the corresponding natural language explanations.Then we list the known Facts and Rules extracted from the Context with interleaved NL instructions.</p>
<p>After that, we represent the application of each Rule by utilizing separate blocks, line by line.We strive to preserve as many solver actions as possible, such as 'Binding' and 'Unbinding', as well as the acquisition of new implied Facts, and so forth.Noting that this information has already been obtained during the revealing phase, we focus on the refinement of the solver-derived reasoning process.Finally, we enumerate all newly implied Facts to enable the model to perform an interim review.</p>
<p>Turn-3: Query &amp; Answering Instructions.In Turn-3, we present instructions for answering a given Query.Following prior works (Ceri et al., 1989;Tafjord et al., 2021), a Query can be considered true within a certain logical context if it is explicitly mentioned or if it can be implied through several Rule applications.To handle negation, we consider two distinct assumptions: 1) the open-world assumption (OWA) that treats any fact that cannot be provable as special truth value 'unknown'; 2) the closed-world assumption (CWA) where any fact not provable is assumed 'false'.Following both assumptions, we adjust the answering instructions, particularly the 'Options' part.</p>
<p>Turn-4: Query-based Reasoning &amp; Formatted Answer.In the final Turn-4, we compare the parsed Query with all the known Facts and implied Facts, expecting the model to perform basic language inference and generate answer options in the desired format.</p>
<p>Fine-tuning Open-source LLMs</p>
<p>After obtaining the refined deductive reasoning instruction-tuning dataset, we can perform finetuning on open-source LLMs with the expectation that the trained model (i.e., LOGIPT) can possess reasoning abilities similar to those of solvers.Consequently, for any given Query, we can bypass the syntax or grammatical errors derived from NL-to-SL parsing by directly generating the answer with a fine-tuned LOGIPT.</p>
<p>Experiments</p>
<p>We construct our solver-derived instruction-tuning data on two public deductive reasoning datasets and evaluate LOGIPT on corresponding test sets.</p>
<p>Datasets</p>
<p>ProofWriter (Tafjord et al., 2021)</p>
<p>Baselines</p>
<p>We consider comparing LOGIPT with following groups of baselines:</p>
<p>Closed-source LMs: We include the Chat-GPT (gpt-3.5-turbo)(OpenAI, 2022), GPT-3.5 (text-davinci-003) (Ouyang et al., 2022) and GPT-4 (gpt-4) (OpenAI, 2023) as closed-source LMs for evaluation following Pan et al. (2023).</p>
<p>Open-source LMs:</p>
<p>We also evaluate opensource LMs for research community.Specifically, we choose Vicuna-13B (vicuna-13b-v1.5-16k)(Chiang et al., 2023), a chatbot trained by finetuning LLaMA-2 (Touvron et al., 2023b) on usershared conversations collected from ShareGPT5 , and CodeLlama-13B (Roziere et al., 2023), foundation models for code tasks.We select the base version (CodeLlama-13b-hf), and instruction finetuned version (CodeLlama-13b-Instruct-hf).</p>
<p>Solver-argumented LMs: Finally, we compare our model against the solver-argumented LMs.We focus on the representative LogicLM (Pan et al., 2023) with underlying LLMs ChatGPT (gpt-3.5-turbo),and , which serve as the state-of-theart deductive reasoning methods.</p>
<p>Apart from the LMs, we also analyze two types of prompting methods: i) Standard prompting that uses in-context learning with few-shot demonstrations to directly answer the given question; ii) Chain-of-Thought (CoT) that utilizes step-by-step problem-solving process to generate explanations where few-shot demonstrations are also provided, and then outputs the final answer.For a fair comparison, we use the same in-context examples, shown in Appendix A and B, for NL-to-SL parsing when evaluating all models on the same dataset, consistent with Pan et al. (2023).To enhance the clarification, we also provide a specific baseline 'Random Answering' that randomly outputs answer options.</p>
<p>Implementation Details</p>
<p>During the fine-tuning phase, we use a batch size of 32 per GPU and a learning rate of 1e-5 for all opensource LMs.We train our model on 8 Nvidia A100-80G GPUs with DeepSpeed ZeRO-3 (Rasley et al., 2020) for 12 hours on 2 epochs.For reproducibility, we use greedy decoding and set the temperature to 0 and the maximum context length to 8192.As for baselines, we strictly follow the setting of Pan et al. (2023).Given that all instances are presented in the form of multiple-choice questions, we assess the model's performance by the accuracy of selecting the correct answer option.</p>
<p>Main Results</p>
<p>We report the results of LOGIPT and baselines on Table 2 and have following main findings:</p>
<p>1) When prompting with few-shot examples, open-source LMs exhibit notably poor deductive reasoning capabilities, with their outputs closed to random answering.Even the Standard prompting models of ChatGPT (gpt-3.5-turbo)and GPT-3.5 (text-davinci-003) exhibit a similar performance to random answering.This once again demonstrates that it is considerably difficult for many LLMs to solve logical reasoning tasks.</p>
<p>2) LOGIPT is significantly superior to the state-of-the-art solver-augmented LMs by a large margin on both deductive reasoning benchmarks.In ProofWriter, our best-performing model, LOGIPT (CodeLlama-13b-hf), outperforms the currently state-of-the-art LogicLM (gpt-4) by an absolute improvement of 9.84%.Mean-while, in PrOntoQA, our best-performing model LOGIPT (vicuna-13b-v1.5-16k)exhibits an even higher absolute improvement of 13.20% than LogicLM (gpt-4).This indicates that our approach is better than the pipeline of problem formulation first and then reasoning with solvers, and finetuning with solver-derived reasoning data can facilitate the deductive reasoning capacity of LMs.</p>
<p>3) LOGIPT significantly outperforms all selected open/closed-source LMs on both datasets, except for the CoT experiment on the PrOntoQA data where LOGIPT achieves comparable results with GPT-4 CoT.This is surprising considering that our underlying open-source LMs are merely 13B parameters in size.As for the baseline experiments of GPT-4, our performance on ProofWriter also significantly surpasses that of GPT-4's Standard and CoT prompting versions, as well as the Standard version of PrOntoQA.These results further demonstrate that open-source LMs, when coupled with solver-simulated reasoning capacity, can achieve performance on par with or even superior to closedsource GPT models.</p>
<p>4) The accuracy of CodeLlama-13B-Base (CodeLlama-13b-hf) with Standard prompting was 0.00, and the performance of the CoT version was close to random answering.By examining the outputs, we found that this is due to the CodeLlama-13B-Base's inability to follow the provided fewshot demonstrations, resulting in outputting no answering options.The introduction of the Instruct version of CodeLlama-13B mitigates this issue to some extent.However, after training with LOGIPT, the CodeLlama models far less encounter this issue (i.e., following the right answering format in both test sets) and even achieve better performance than the Vicuna version of LOGIPT.This demonstrates the potential of code foundation models in logical reasoning tasks, consistent with the finding on prior work (Yue et al., 2023).</p>
<p>Further Analysis</p>
<p>Impact of Solver-derived Reasoning Formats</p>
<p>We further investigate the impact of different solverderived reasoning formats on the model's performance.Specifically, we consider the following format variations: 1) w/o 'unbind' statements that we remove all 'Unbind' statements from Turn-2 to investigate the utility of the explicit retention of this action from the solver; 2) w/o 'fail &amp; back-   track' statements that we removing all 'Fail &amp; backtrack' statements from Turn-2.During the solver's reasoning process, it is expected to encounter situations in which, after binding a value, the solver realizes that not all premises are satisfied (e.g., 'Fiona is blue' but 'Fiona is not quiet' for application of Rule3 in Figure 3).Consequently, a 'Fail &amp; backtrack' operation occurs (highlighted in color in Figure 3).We explore the effectiveness of explicitly stating these operations.</p>
<p>We present the accuracy of the variations on solver-derived reasoning format on ProofWriter in Table 3 where several observations can be made: 1) regardless of using the default format, removing 'Unbind' statements, or removing 'Fail &amp; backtrack' statements, it can not be determined which format guarantees the optimal results.To retain the maximum amount of action information that the solver can provide, we still adopt the de-fault settings in LOGIPT; 2) whether 'Unbind' statements are removed or 'Fail &amp; backtrack' statements are removed, there is always an experiment under each open-source LMs that can surpass the default LOGIPT results.This further enhances the best performance of LOGIPT shown in Table 2.</p>
<p>Impact of SL Reasoning Representations</p>
<p>We are also curious about the impact of SL reasoning representations.Therefore, we include additional experiments in Table 3, denoted as w/ NL representation that we re-translate the symbolic representation (e.g., Green('Charlie', True)) back to its original NL version (e.g., Charlie is green.)and replace the original symbolic representation in Turn-2.From the table, we can find that replacing SL representations with NL results in a significant decrease in model performance, further emphasizing that symbolic representations are superior to NL representations in deductive reasoning tasks.</p>
<p>Effectiveness of Merging Data from Different Reasoning Assumptions</p>
<p>Since ProofWriter is an open-world assumption and PrOntoQA is labeled within a closed-world assumption, we also perform a further investigation on whether both reasoning assumptions can benefit each other.Specifically, we first merge both constructed training data and then test LOGIPT on each test set.The experimental results are shown in Table 4.We can conclude that if we directly mix the two types of data for training, the results on their respective test sets will be slightly lower than those obtained from training solely on their respective datasets.Therefore, we conducted an in-depth analysis of the underlying reasons and observed that in PrOntoQA, the majority of Rules are in the format of 'Every/Each A is (not) B' or 'A are (not) B'.While in ProofWriter, the predominant structure of Rules consists of: 'If someone is A, then they are B' or 'If something is A, then it is B'.Therefore, we conducted an additional set of experiments in which the Rule format of two training sets was randomly reformatted into the four aforementioned types using regular expression (denoted as 'Both (Reformat)').Then, we test the model on the original test sets.We can observe that by employing this approach, the code models yield improved performance on ProofWriter.Thus, the style/genre of logical context must also be taken into consideration to maximize the efficacy of transfer learning in logical reasoning.</p>
<p>Related Work</p>
<p>Logical Reasoning with LMs.Recent efforts in adapting Large Language Models (LLMs) for logical reasoning tasks generally adopt direct finetuning specialized modules (Clark et al., 2020;Tafjord et al., 2021Tafjord et al., , 2022;;Yang et al., 2022) or in-context learning (Zhou et al., 2022;Lyu et al., 2023;Ling et al., 2023), where reasoning in NL is used by both groups of methods.Fine-tuning approaches involve training the full model or specialized modules, enhancing LLMs with module-level logical reasoning skills like proof, enumeration, and abduction (Tafjord et al., 2021).The in-context learning approaches create specific prompts to encourage LLMs' step-by-step reasoning skills.Common methods encompass chain-of-thought prompting (Wei et al., 2022b;Chen et al., 2023), which produces explanations before delivering a final answer, and least-to-most prompting (Zhou et al., 2022), which deconstructs a problem into simpler components that can be resolved individually.Some recent work has focused on combining neural networks with symbolic reasoning (Tian et al., 2022;Pryor et al., 2022;Pan et al., 2023), especially the solver-augmented LMs that parse NL logical questions into symbolic representations, then utilizing external logical solvers for answering.Despite their impressive performance, parsing errors can lead to solver execution failure and logical question-answering issues.To address this, we propose LOGIPT, which directly imitates the solver's reasoning ability and outputs the answer.</p>
<p>Augmented LMs for Reasoning.Recent work has begun to augment LMs to overcome their inherent limitations such as the incapacity to access up-to-date information or conduct accurate mathematical reasoning.They augment with external tools and resources, such as the information retriever (Shi et al., 2023;Lazaridou et al., 2022), planner (Liu et al., 2023a) and other pre-trained models (Shen et al., 2023).Specifically, to enhance the reasoning capacity, recent work resort to external off-the-shelf Solvers including programmatic interpreters (Chen et al., 2022;Gao et al., 2023), satisfiability solvers (Ye et al., 2023), logical solvers (Pan et al., 2023) or their hybrids (Poesia et al., 2023).Most of them utilize the LMs to parse the NL question to symbolic representations and then invoke solvers to reason in SL.In this paper, we concentrate on logical solvers, automated tools for validating the truth value of logical formulas.</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel LOGIPT that can directly act as a logical solver for deductive reasoning tasks.LOGIPT can output all facts implied from NL logical questions, while bypassing the syntax or grammatical errors derived from NL-to-SL parsing of solver-augmented LMs.We conducted numerous analytical experiments on two public deductive reasoning benchmarks.Evaluation results show that LOGIPT can significantly outperform state-of-the-art solver-augmented LMs, and surpass or be comparable with few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
<p>Turn-4 (from LM):</p>
<p>The comment can be parsed into: Green('Dave', False) According to the implied fact: Green('Dave', True)</p>
<p>The comment contradicts the fact.So, the correct option is: B) False</p>
<p>Figure 1 :
1
Figure 1: Data flow of current solver-augmented LMs for inference (a), and our pipeline for LOGIPT (b,c).</p>
<p>Figure 2 :
2
Figure 2: A deductive reasoning question derived from ProofWriter and its parsed Facts, Rules, and Query.</p>
<p>True) Now begin reasoning to obtain all implied facts: Use rule1: Furry($x, True) → Quiet($x, True) Bind $x to 'Dave' Obtain a new implied fact: Quiet('Dave', True) Unbind $x Finish implied with rule1 Use rule2: Green($x, True) ∧ White($x, True) → Nice($x, True) Bind $x to 'Charlie' Obtain a new implied fact: Nice('Charlie', True) Unbind $x Bind $x to 'Fiona' Obtain an already known or implied fact: Nice('Fiona', True) Unbind $x Finish implied with rule2 Use rule3: Blue($x, True) ∧ Quiet($x, True) → Green($x, True) Bind $x to 'Dave' Obtain a new implied fact: Green('Dave', True) Reuse rule2: Green($x, True) ∧ White($x, True) → Nice($x, True) Bind $x to 'Dave' Obtain a new implied fact: Nice('Dave', True) Finish implied with rule2 Unbind $x Bind $x to 'Fiona' Unbind $x Finish implied with rule3 Finally, we obtain following implied facts: Green('Dave', True) (... more facts here ...) Finish reasoning</p>
<p>Figure 4 :
4
Figure 4: The full version of the comprehensive 4-turn training example of our instruction-tuning data shown in Figure 3.</p>
<p>Table 2 :
2
Main results on two evaluation datasets.The best results of LOGIPT are in bold and the best results within each dataset are underlined.
is a commonly
of each example is one of {True, False, Unknown}.to ProofWriter, PrOntoQA is in a closed-world assumption (CWA) subset where the answer of each example is one of {True, False}.For training, we merely merge all subsets with fictional characters and obtained 15,940 training cases after filtering out syntax-invalid ones.</p>
<p>Table 3 :
3
The accuracy of the variations on solver-derived reasoning format, and replacing SL representations with NL on ProofWriter.The best results on each underlying LMs are underlined.
Train setTest SetVCN CLBCLIPrOntoQAPrOntoQA96.40 95.60 96.20BothPrOntoQA91.00 87.00 89.00Both (Reformat) PrOntoQA90.00 87.00 77.80ProofWriterProofWriter 81.17 89.50 81.67BothProofWriter 79.33 87.17 79.67Both (Reformat) ProofWriter 79.00 90.83 84.50</p>
<p>Table 4 :
4
The accuracy of LOGIPT trained with merged data and tested on single data with different underlying
LMs. 'VCN', 'CLB', and 'CLI' respectively representVicuna-13B, CodeLlama-13B-Base, and CodeLlama-13B-Instruct. 'Both' means 'ProofWriter + PrOntoQA'.
In this paper, the term 'Query' refers to a specific sentence of statement or comment, while 'question' is used in a broader sense to denote the description of a logical problem.
In the original case, the Query is 'Charlie is not green.'. We replace it with 'Dave is not green.' for better illustration.
https://sharegpt.com/
A Instructions for NL-to-SL Parsing on ProofWriterTask Description: You are given a problem description and a question.The task is to: 1) define all the predicates in the problem 2) parse the problem into logic rules based on the defined predicates 3) write all the facts mentioned in the problem 4) parse the question into the logic formProblem:Anne is quiet.Erin is furry.(... more context here ...) All red people are young.Question:Based on the above information, is the following statement true, false, or unknown?Anne is white.Predicates:
What you always wanted to know about datalog(and never dared to ask). Stefano Ceri, Georg Gottlob, Letizia Tanca, IEEE transactions on knowledge and data engineering. 111989</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models. Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.143232023arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-202020</p>
<p>Programming in PROLOG. Christopher S William F Clocksin, Mellish, 2003Springer Science &amp; Business Media</p>
<p>Applying expert system technology to code reuse with pyke. Bruce Frederiksen, 2008PyCon: Chicago</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. 2023PMLR. Google. 2023. Google bard</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Fifty years of prolog and beyond. Philipp Körner, Michael Leuschel, João Barbosa, Santos Vítor, Verónica Costa, Manuel V Dahl, Jose F Hermenegildo, Jan Morales, Daniel Wielemaker, Salvador Diaz, Abreu, Theory and Practice of Logic Programming. 2262022</p>
<p>Internetaugmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, arXiv:2203.051152022arXiv preprint</p>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, arXiv:2306.03872Deductive verification of chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023aarXiv preprint</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023barXiv preprint</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-ofthought reasoning. 2023arXiv preprint</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>ArXiv, abs/2303.08774Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D Goodman, arXiv:2306.04031Certified reasoning with language models. 2023arXiv preprint</p>
<p>L David, Alan K Poole, Mackworth, Artificial Intelligence: foundations of computational agents. Cambridge University Press2010</p>
<p>Connor Pryor, Charles Dickens, Eriq Augustine, Alon Albalak, William Wang, Lise Getoor, arXiv:2205.14268Neupsl: Neural probabilistic soft logic. 2022arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrievalaugmented black-box language models. 2023arXiv preprint</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Entailer: Answering questions with faithful and truthful chains of reasoning. Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, 10.18653/v1/2022.emnlp-main.134Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Weakly supervised neural symbolic learning for cognitive tasks. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Prajjwal Soumya, Shruti Bhargava, Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.104982022arXiv preprint</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Transactions on Machine Learning Research. 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, 10.18653/v1/2022.emnlp-main.7Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Xi Ye, Qiaochu Chen, arXiv:2305.09656Isil Dillig, and Greg Durrett. 2023. Satisfiability-aided language models using declarative prompting. arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The Eleventh International Conference on Learning Representations. 2022</p>
<p>statement into known facts and rules based on the defined predicates 3. begin reasoning to obtain all implied facts Context: Charlie is green. Charlie is white. Dave is blue. Dave is furry. Dave is white. Fiona is blue. Fiona is green. Fiona is nice. Fiona is rough. Fiona is white. All furry people are quiet. All green, white people are nice. If someone is blue and quiet then they are green. Reasoning: Turn-2 (from LM): We first define following predicates and corresponding natural language explanations: Green($x, bool) ::: Is x green? White($x, bool) ::: Is x white? Blue($x, bool) ::: Is x blue? Furry($x, bool) ::: Is x furry? Nice($x, bool) ::: Is x nice?. Truefrom human): Task Description: Given a statement containing logical facts and rules in natural languages as context, the task is to: 1. define all the predicates in logic forms from the context 2. parse the. Rough($x, bool) ::: Is x rough? Quiet($x, bool) ::: Is x quiet? We have following known facts from the context: Green('Charlie</p>
<p>Quiet($x, True) → Green($x, True) Now begin reasoning to obtain all implied facts: Use rule1: Furry($x, True) → Quiet($x, True) Bind $x to 'Dave' Obtain a new implied fact: Quiet('Dave', True) Unbind $x Finish implied with rule1 Use rule2: Green($x, True) ∧ White($x, True) → Nice($x, True) Bind $x to 'Charlie' Obtain a new implied fact: Nice('Charlie', True) Unbind $x Bind $x to 'Fiona' Obtain an already known or implied fact: Nice('Fiona', True) Unbind $x Finish implied with rule2 Use rule3: Blue($x, True) ∧ Quiet($x, True) → Green($x, True) Bind $x to 'Dave' Obtain a new implied fact: Green('Dave', True) Reuse rule2: Green($x, True) ∧ White($x, True) → Nice($x, True) Bind $x to 'Dave' Obtain a new implied fact: Nice('Dave', True) Finish implied with rule2 Unbind $x Bind $x to 'Fiona' Unbind $x Finish implied with rule3 Finally, we obtain following implied facts: Green('Dave', True) Nice('Charlie', True) Nice('Dave', True) Quiet('Dave', True) Finish reasoning Turn-3 (from human): Question: Based on the above information, is the following comment true, false, or unknown? Dave is not green. Options: A) True B) False C) Unknown Now you should: 1. reason first carefully with the above known facts and implied facts 2. ( Green, ' Fiona, ' True) White('charlie, ' True) White('dave, ) True, ( White, ' Fiona, ) True, ' Blue('dave, ) True, ( Blue, ' Fiona, ) True, ( Furry, ' 'dave, ) True, ( Nice, ' Fiona, ) True, Rough, We have following known rules from the context: rule1: Furry($x, True) → Quiet($x, True) rule2: Green($x, True) ∧ White($x, True) → Nice($x, True) rule3: Blue($x, True) ∧. then output your answer strictly in the format of "So, the correct option is: X". The X should be replaced by "A) True", "B) False" or "C) Unknown"</p>            </div>
        </div>

    </div>
</body>
</html>
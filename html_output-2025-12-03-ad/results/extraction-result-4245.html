<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4245 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4245</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4245</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-281843726</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.04749v1.pdf" target="_blank">LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows</a></p>
                <p><strong>Paper Abstract:</strong> The increasing volume of scholarly publications requires advanced tools for efficient knowledge discovery and management. This paper introduces ongoing work on a system using Large Language Models (LLMs) for the semantic extraction of key concepts from scientific documents. Our research, conducted within the German National Research Data Infrastructure for and with Computer Science (NFDIxCS) project, seeks to support FAIR (Findable, Accessible, Interoperable, and Reusable) principles in scientific publishing. We outline our explorative work, which uses in-context learning with various LLMs to extract concepts from papers, initially focusing on the Business Process Management (BPM) domain. A key advantage of this approach is its potential for rapid domain adaptation, often requiring few or even zero examples to define extraction targets for new scientific fields. We conducted technical evaluations to compare the performance of commercial and open-source LLMs and created an online demo application to collect feedback from an initial user-study. Additionally, we gathered insights from the computer science research community through user stories collected during a dedicated workshop, actively guiding the ongoing development of our future services. These services aim to support structured literature reviews, concept-based information retrieval, and integration of extracted knowledge into existing knowledge graphs.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4245.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4245.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based Concept Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows (NFDIxCS demo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses large language models via in-context learning (zero-shot and few-shot) to extract semantic concepts (research questions, methods, findings, categorical/binary labels and free-text fields) from full-text scientific papers to populate structured templates and support literature comparison and knowledge-graph prefilling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen 2.5-72B instruct; Llama 3.3-70B instruct; Gemini 1.5 Flash (002 and 8B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B, 70B, 8B (Gemini full size unknown)</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>In-context concept extraction (zero-shot & few-shot) with chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The system processes uploaded full-text PDFs with an LLM using in-context learning. Two operation modes were used: zero-shot (instruction + full text) and few-shot (three in-domain examples). Each few-shot example contained: (1) full text of a document, (2) the domain-specific extraction questions, (3) the instructions, and (4) a manually crafted ideal answer. Prompts employed chain-of-thought style instructions to elicit stepwise reasoning and returned a structured JSON with fields: reasoning (short stepwise reasoning), context (relevant text passages), and answer (concise extracted value). An explicit zero-shot prompt example required the model to produce a JSON object with reasoning, context and answer and limited each field to 500 words. Models were accessed through openrouter.ai (Qwen, Llama) and the Google API (Gemini). The pipeline includes a Gradio demo UI; future/ongoing work explores retrieval-augmented approaches (RAG) and embedding-based retrieval to avoid arbitrary chunking.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>122 papers</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Business Process Management (Computer Science)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Semantic and empirical contributions: explicitly stated research questions, methodological descriptions, findings and categorical/binary attributes (i.e., research concepts and empirical generalizations and patterns for literature comparison), rather than formal mechanistic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Example extraction given in the paper: For the query 'What is the explicitly stated research question for the paper?' the system returned 'How to decide which processes need to be analyzed in detail to determine if changes are necessary.' Other extracted item types: research questions, methods, findings, categorical labels and binary indicators used to prefill KG templates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison to a gold-standard of manual annotations (human-coded ground truth) on 122 BPM papers; metrics chosen per target type: ExactAcc for categorical targets, macro-averaged F1 for binary targets, and BERTScore F1 (semantic F1 via BERT embeddings) for free-text targets. Additionally, a pilot user study and workshop collected qualitative user feedback (satisfaction, needs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported per-model table (values are unitless proportions): qwen-2.5-72b (3-shot): ExactAcc=0.249, BinF1=0.594, BERT_F1=0.863, Overall=0.569; qwen-2.5-72b (0-shot): ExactAcc=0.219, BinF1=0.514, BERT_F1=0.887, Overall=0.540; llama-3.3-70b (0-shot): ExactAcc=0.212, BinF1=0.556, BERT_F1=0.877, Overall=0.548; gemini-1.5-flash-002 (3-shot): ExactAcc=0.246, BinF1=0.330, BERT_F1=0.893, Overall=0.490; gemini-1.5-flash-002 (0-shot): ExactAcc=0.183, BinF1=0.390, BERT_F1=0.883, Overall=0.486; gemini-1.5-flash-8b-001 (0-shot): ExactAcc=0.171, BinF1=0.345, BERT_F1=0.881, Overall=0.466; gemini-1.5-flash-8b-001 (3-shot): ExactAcc=0.180, BinF1=0.148, BERT_F1=0.897, Overall=0.408. The paper notes BERT_F1 scores near 0.90 for free-text fields and best BinF1 ~0.59; ExactAcc remained <0.25 overall.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Zero-shot mode served as an internal baseline; the study compares zero-shot vs few-shot and compares multiple LLMs (open-weight Qwen and Llama vs commercial Gemini variants). Gold-standard manual annotations provided ground truth for metric computation, but no separate non-LLM automated baseline method was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) In-context learning with modern LLMs can be quickly adapted to a specific scientific domain to extract semantic concepts. 2) Few-shot examples improved performance for categorical outputs and free-text extraction formats (BERT_F1), but degraded binary classification performance (likely due to class-bias introduced by examples). 3) Open-weight larger models (Qwen 2.5-72B and Llama 3.3-70B) outperformed the commercial Gemini variants tested, likely because of model size differences. 4) Returning reasoning and context alongside answers (chain-of-thought + highlighted source context) was desirable for traceability. 5) Using full-text examples in few-shot is costly and may introduce noise; selecting shorter/more targeted examples is an avenue for improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limited exact categorical accuracy (ExactAcc <0.25); class-bias introduced by few-shot examples affecting binary tasks; need for improved traceability (linking extracted items back to highlighted source passages); hallucination and correctness concerns (hence requirement for traceability and verification); cost and latency from including full-text few-shot examples; chunking issues limiting semantic relevance in vector searches (motivating embedding-based retrieval); domain-limited evaluation (122 BPM papers only) so generalization unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering <em>(Rating: 2)</em></li>
                <li>Elicit: AI literature review research assistant <em>(Rating: 2)</em></li>
                <li>ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4245",
    "paper_id": "paper-281843726",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "LLM-based Concept Extraction",
            "name_full": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows (NFDIxCS demo)",
            "brief_description": "A system that uses large language models via in-context learning (zero-shot and few-shot) to extract semantic concepts (research questions, methods, findings, categorical/binary labels and free-text fields) from full-text scientific papers to populate structured templates and support literature comparison and knowledge-graph prefilling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen 2.5-72B instruct; Llama 3.3-70B instruct; Gemini 1.5 Flash (002 and 8B variants)",
            "model_size": "72B, 70B, 8B (Gemini full size unknown)",
            "method_name": "In-context concept extraction (zero-shot & few-shot) with chain-of-thought prompting",
            "method_description": "The system processes uploaded full-text PDFs with an LLM using in-context learning. Two operation modes were used: zero-shot (instruction + full text) and few-shot (three in-domain examples). Each few-shot example contained: (1) full text of a document, (2) the domain-specific extraction questions, (3) the instructions, and (4) a manually crafted ideal answer. Prompts employed chain-of-thought style instructions to elicit stepwise reasoning and returned a structured JSON with fields: reasoning (short stepwise reasoning), context (relevant text passages), and answer (concise extracted value). An explicit zero-shot prompt example required the model to produce a JSON object with reasoning, context and answer and limited each field to 500 words. Models were accessed through openrouter.ai (Qwen, Llama) and the Google API (Gemini). The pipeline includes a Gradio demo UI; future/ongoing work explores retrieval-augmented approaches (RAG) and embedding-based retrieval to avoid arbitrary chunking.",
            "number_of_papers": "122 papers",
            "domain_or_field": "Business Process Management (Computer Science)",
            "type_of_laws_extracted": "Semantic and empirical contributions: explicitly stated research questions, methodological descriptions, findings and categorical/binary attributes (i.e., research concepts and empirical generalizations and patterns for literature comparison), rather than formal mechanistic laws.",
            "example_laws_extracted": "Example extraction given in the paper: For the query 'What is the explicitly stated research question for the paper?' the system returned 'How to decide which processes need to be analyzed in detail to determine if changes are necessary.' Other extracted item types: research questions, methods, findings, categorical labels and binary indicators used to prefill KG templates.",
            "evaluation_method": "Comparison to a gold-standard of manual annotations (human-coded ground truth) on 122 BPM papers; metrics chosen per target type: ExactAcc for categorical targets, macro-averaged F1 for binary targets, and BERTScore F1 (semantic F1 via BERT embeddings) for free-text targets. Additionally, a pilot user study and workshop collected qualitative user feedback (satisfaction, needs).",
            "performance_metrics": "Reported per-model table (values are unitless proportions): qwen-2.5-72b (3-shot): ExactAcc=0.249, BinF1=0.594, BERT_F1=0.863, Overall=0.569; qwen-2.5-72b (0-shot): ExactAcc=0.219, BinF1=0.514, BERT_F1=0.887, Overall=0.540; llama-3.3-70b (0-shot): ExactAcc=0.212, BinF1=0.556, BERT_F1=0.877, Overall=0.548; gemini-1.5-flash-002 (3-shot): ExactAcc=0.246, BinF1=0.330, BERT_F1=0.893, Overall=0.490; gemini-1.5-flash-002 (0-shot): ExactAcc=0.183, BinF1=0.390, BERT_F1=0.883, Overall=0.486; gemini-1.5-flash-8b-001 (0-shot): ExactAcc=0.171, BinF1=0.345, BERT_F1=0.881, Overall=0.466; gemini-1.5-flash-8b-001 (3-shot): ExactAcc=0.180, BinF1=0.148, BERT_F1=0.897, Overall=0.408. The paper notes BERT_F1 scores near 0.90 for free-text fields and best BinF1 ~0.59; ExactAcc remained &lt;0.25 overall.",
            "comparison_baseline": "Zero-shot mode served as an internal baseline; the study compares zero-shot vs few-shot and compares multiple LLMs (open-weight Qwen and Llama vs commercial Gemini variants). Gold-standard manual annotations provided ground truth for metric computation, but no separate non-LLM automated baseline method was reported.",
            "key_findings": "1) In-context learning with modern LLMs can be quickly adapted to a specific scientific domain to extract semantic concepts. 2) Few-shot examples improved performance for categorical outputs and free-text extraction formats (BERT_F1), but degraded binary classification performance (likely due to class-bias introduced by examples). 3) Open-weight larger models (Qwen 2.5-72B and Llama 3.3-70B) outperformed the commercial Gemini variants tested, likely because of model size differences. 4) Returning reasoning and context alongside answers (chain-of-thought + highlighted source context) was desirable for traceability. 5) Using full-text examples in few-shot is costly and may introduce noise; selecting shorter/more targeted examples is an avenue for improvement.",
            "challenges_limitations": "Limited exact categorical accuracy (ExactAcc &lt;0.25); class-bias introduced by few-shot examples affecting binary tasks; need for improved traceability (linking extracted items back to highlighted source passages); hallucination and correctness concerns (hence requirement for traceability and verification); cost and latency from including full-text few-shot examples; chunking issues limiting semantic relevance in vector searches (motivating embedding-based retrieval); domain-limited evaluation (122 BPM papers only) so generalization unknown.",
            "uuid": "e4245.0",
            "source_info": {
                "paper_title": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows",
                "publication_date_yy_mm": "2025-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering",
            "rating": 2,
            "sanitized_title": "coregpt_combining_open_access_research_and_large_language_models_for_credible_trustworthy_question_answering"
        },
        {
            "paper_title": "Elicit: AI literature review research assistant",
            "rating": 2,
            "sanitized_title": "elicit_ai_literature_review_research_assistant"
        },
        {
            "paper_title": "ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System",
            "rating": 2,
            "sanitized_title": "orkg_ask_a_neurosymbolic_scholarly_search_and_exploration_system"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "rating": 1,
            "sanitized_title": "gemini_15_unlocking_multimodal_understanding_across_millions_of_tokens_of_context"
        }
    ],
    "cost": 0.008094,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows ⋆
6 Oct 2025</p>
<p>Samy Ateia samy.ateia@ur.de 0009-0000-2622-9194
University of Regensburg
Universitätsstraße 3193053RegensburgGermany</p>
<p>Udo Kruschwitz udo.kruschwitz@ur.de 0000-0002-5503-0341
University of Regensburg
Universitätsstraße 3193053RegensburgGermany</p>
<p>Melanie Scholz melanie.scholz@uni-bayreuth.de 0000-0002-5503-0341
University of Bayreuth
Universitätsstraße 3095447BayreuthGermany</p>
<p>Agnes Koschmider agnes.koschmider@uni-bayreuth.de 0000-0001-8206-7636
University of Bayreuth
Universitätsstraße 3095447BayreuthGermany</p>
<p>Moayad Almohaishi moayad.almohaishi@uni-bayreuth.de 0009-0002-1758-3153
University of Bayreuth
Universitätsstraße 3095447BayreuthGermany</p>
<p>LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows ⋆
6 Oct 2025AC0C24461439B6430FA196E65643357510.1007/978-arXiv:2510.04749v1[cs.DL]sponding to the accepted manuscript and supersedes the submitted version that was inadvertently published as the version of record.Large Language ModelsInformation ExtractionScientific PublishingDigital LibrariesFAIR PrinciplesKnowledge Graphs
The increasing volume of scholarly publications requires advanced tools for efficient knowledge discovery and management.This paper introduces ongoing work on a system using Large Language Models (LLMs) for the semantic extraction of key concepts from scientific documents.Our research, conducted within the German National Research Data Infrastructure for and with Computer Science (NFDIxCS) project, seeks to support FAIR (Findable, Accessible, Interoperable, and Reusable) principles in scientific publishing.We outline our explorative work, which uses in-context learning with various LLMs to extract concepts from papers, initially focusing on the Business Process Management (BPM) domain.A key advantage of this approach is its potential for rapid domain adaptation, often requiring few or even zero examples to define extraction targets for new scientific fields.We conducted technical evaluations to compare the performance of commercial and open-source LLMs and created an online demo application to collect feedback from an initial user-study.Additionally, we gathered insights from the computer science research community through user stories collected during a dedicated workshop, actively guiding the ongoing development of our future services.These services aim to support structured literature reviews, concept-based information retrieval, and integration of extracted knowledge into existing knowledge graphs.</p>
<p>Introduction</p>
<p>The scientific publication landscape is booming with global annual publication growing by 59% according to the NSF 3 and more than one million articles being published per year in biomedicine and life sciences alone [6].This rise in publications makes it harder for scientists to stay on top of their field, while also limiting the discoverability of their publications as they have to compete with others for visibility.Digital transformation and new tools such as LLMs can accelerate that problem, but also have the potential to assist scientists and publishing platforms in managing these challenges.</p>
<p>In computer science, publications are often accompanied by software artifacts and datasets for reproducibility, but their management frequently lacks standardization and fails to meet FAIR principles.The National Research Data Infrastructure for and with Computer Science (NFDIxCS) project 4 addresses this by creating an infrastructure to implement FAIR principles [21] for CS research outputs in Germany [5].But to make these artifacts findable it is necessary to link them to relevant semantic information from the publication text itself, for example, research questions or methods so that they are discoverably as related work by other scientists.</p>
<p>Our project, situated within the NFDIxCS initiative, aims to develop tools to exactly address this problem.We leverage Large Language Models (LLMs) for the semantic analysis of scientific text, with the goal of enhancing the FAIR principles for scholarly literature.Specifically, we aim to:</p>
<p>-Develop robust methods for automatically extracting key semantic concepts (e.g., research questions, methodologies, findings) from scientific papers.-Explore mechanisms for structuring these extracted concepts to improve the organization and distribution of digital content, potentially linking them to knowledge graphs.-Design and prototype services, informed by community needs, that use this structured information to support researchers in their workflows.</p>
<p>This short paper presents our preliminary findings and outlines how user-driven requirements are shaping the trajectory of our research towards practical applications.We publish a demo system alongside its source code under a permissive license5 alongside the results of our user workshops, and plan to maintain this practice for future services.</p>
<p>Related Work</p>
<p>In this work, we explore the use of LLMs for knowledge extraction to improve scientific workflows within digital libraries and beyond.We review related efforts in three key areas: platforms for scientific literature analysis, knowledge graphs for structuring scientific information, and the use of LLMs for information extraction.</p>
<p>Platforms for Scientific Literature Analysis</p>
<p>Several platforms exist that use natural language processing (NLP) based on language models to highlight relevant information from scientific text, therefore assisting in navigating the considerable volume of publications.Semantic Scholar [2], for example, employs AI to provide summaries (TLDRs) and identify influential citations, while Elicit uses a systematic review inspired workflow, leveraging LLMs to synthesize findings from multiple papers in response to a user's query [20].Scite.aifocuses specifically on citation context, classifying whether a citation supports, disputes, or merely mentions a claim [9].While these platforms offer similar flexible LLM-based question answering tools, they do not currently offer the use of predefined domain-specific questions and mostly require a paid subscription to be fully utilized.With our approach, we want to potentially offer higher accuracy and user guidance through curated extraction targets and examples serving specialized communities.</p>
<p>Knowledge Graphs for Structuring Scientific Knowledge</p>
<p>Structuring scientific knowledge in a machine-readable format has long been a goal of the scientific community.The Open Research Knowledge Graph (ORKG) [8] is a prominent initiative aiming to represent the content of research papers as structured data.By describing papers through their contributions, methods, and findings, the ORKG facilitates systematic comparisons and reviews.Other notable examples are the discontinued Microsoft Academic Graph (MAG) [18] that was succeeded by OpenAlex [13] or SciKGraph [17].However, curating such knowledge graphs often requires significant manual effort from researchers.Most recently, ORKG Ask was introduced, which offers the possibility to create ad-hoc comparison tables using information extraction with LLMs [11].In our work, we want to build on that approach and take it a step further.Instead of just having users query questions on a set of retrieved papers, we explore how curated questions from domain-experts can be leveraged to prefill knowledge graph input templates for users.This complements the KG vision by lowering the barrier to entry and scaling up content acquisition.Embedding and indexing the extracted information in separate fields could lead to improved semantic search, by enabling users to search for papers with similar research questions or algorithms.</p>
<p>LLMs for Information Extraction in Science</p>
<p>In-context learning with LLMs describes the ability of these models to solve problems that they have not explicitly been trained on, by just giving the model an abstract description of the problem (Zero-Shot) or several examples (Few-Shot) in their input context.These approaches were first popularized with LLMs like GPT-3 [3] and enable their use in domains where limited, or no training data is available.Recent LLMs such as the Google Gemini series 6 or OpenAIs GPT-4.1 7 have pushed the size of the available context up to 1 million input tokens.Which makes it feasible to extract information from large text sources in a single step.These properties can be used in retrieval augmented generation (RAG) [16] systems that ground the knowledge of these models in relevant texts.Systems such as CORE-GPT have shown the usefulness of such approaches in questions answering across multiple scientific domains [12].</p>
<p>In our work, we explore both zero-and few-shot learning for extracting predefined semantic information from scientific texts, that can then be used to facilitate scientific knowledge discovery and publication workflows.</p>
<p>Methodology: LLM-Based Concept Extraction</p>
<p>Our system uses an LLM to extract semantic information from scientific documents.The demo UI allows a user to upload a paper and pose predefined or custom questions.The LLM then processes the document and a prompt to identify and return relevant information or synthesized answers (Figure 1).</p>
<p>In-context Learning for Rapid Domain Adaptation</p>
<p>We leverage the in-context learning of modern LLMs for rapid domain adaptation.By providing instructions and a few examples to guide extraction, our method avoids the need for the extensive, domain-specific datasets required by traditional supervised techniques.</p>
<p>In our evaluations we tested two modes: A few-shot mode where we supplied three in-domain examples each consisting of 1. the full text of the document; 2. the domain-specific information extraction questions, 3. the instructions, 4. the manually crafted ideal answer from the document.In addition, we tested a zeroshot mode where we only supplied the instruction and the full text of the PDF.</p>
<p>The zero-shot mode formed our baseline for evaluation, but could also be used to offer the flexibility to the user to pose their own extraction questions against a document or a set of retrieved documents.The few-shot mode aligns the model with the style of the manual annotators, overcoming limited or missing instructions in the way the question was posed.This mode could be used in settings where predefined questions and predictable answer formats are important, for example when prefilling forms for later knowledge graph mapping.</p>
<p>We tested four different models: Qwen 2.5 72B instruct [14], Llama 3.3 70B instruct 8 [7], Gemini 1.5 Flash 002, Gemini 1.5 Flash 8B 001 [4].Llama and Qwen were accessed via openrouter.aiwhile the Gemini models were accessed via the official Google API.</p>
<p>The instructions used chain of thought prompting [19] to generate a reasoning beside relevant context and the final answer.</p>
<p>The exact zero-shot prompt can be seen in Listing 1.1.</p>
<p>Listing 1.1. Zero-shot prompt example in Python</p>
<p>Extract the information answering the following question from the text : Question : ' ' '{ question } ' ' ' Text : ' ' '{ text } ' ' ' Return a JSON object in the following format : {{ " reasoning ": " &lt; think step by step and write down your reasoning &gt;" , " context ": " &lt; contains all relevant context from the text &gt;" , " answer ": " &lt; one concise answer to the question for example : yes / no / none , or a word or multiple words &gt;" }} Try to be concise and limit your reasoning , answer , and the extracted context to max 500 words .</p>
<p>Dataset and Domain</p>
<p>The initial development and a preliminary evaluation were carried out on a corpus of 122 scientific papers from the Business Process Management (BPM) conferences (2019-2023) 9 .This domain was selected due to the availability of domain experts who are actively constructing a knowledge graph in this area.Key concepts were manually annotated in the papers to establish a gold standard for evaluating extraction performance.</p>
<p>Extraction Example</p>
<p>An example of the extraction process for the Target Concept of a "Research Question" would involve posing the Query: "What is the explicitly stated research question for the paper?".The system might then return an Example Extracted Answer like: "How to decide which processes need to be analyzed in detail to determine if changes are necessary."</p>
<p>This methodology is highly flexible, allowing us to target a wide array of semantic information within scientific texts with high adaptability, as only limited domain expert involvement is needed to create a few examples for each information item.</p>
<p>Demo System</p>
<p>To showcase the ability of the tool and collect initial user feedback, we built a demo UI using the Gradio framework [1] around our approach.The demo system is available online 10 (user:demo, pw:demo) and the source code for this system is available on GitHub 11 .</p>
<p>User Feedback</p>
<p>In a pilot user study, we collected initial user-feedback with the demo system through a questionnaire after instructing a panel of users to choose one paper from a selection of business processing domain papers, upload it to the tool and select any questions that they were interested in.</p>
<p>At a separate workshop with around 30 participants from different computerscience fields, we collected user-stories that they would like to be solved by the offered and demonstrated technology.</p>
<ol>
<li>For free-text targets (4 targets, 488 annotations), we assessed semantic equivalence with the F1-score from BERTScore [22].</li>
</ol>
<p>The Overall score in Table 1 is the unweighted arithmetic mean of these three metrics.BERT_F1 scores near 0.90 indicate strong semantic alignment on free-text fields, whereas binary indicators show moderate performance (best BinF1 = 0.59) and exact categorical extraction remains limited (ExactAcc &lt; 0.25).</p>
<p>User Study &amp; Workshop</p>
<p>Feedback from our pilot user study on the prototype demo UI was positive (88% satisfaction with extracted concepts), indicating the potential utility of the approach.While some feedback was UI related (hiding advanced configuration like few-shot examples, wanting more expert configuration), a main point was that the traceability of the extracted information should be improved.</p>
<p>In a separate workshop with around 30 computer-science researchers, we collected 56 user-stories.38 of these focused on the task of literature research and comparison, 8 on assistance while writing papers, 3 on support in the review process, 3 were directed towards software development and 4 were unique.Overall, it became clear that the users want to go beyond just extracting concepts from one specific paper and compare the extracted information from multiple papers instead.The full categorized list is available in our repository 12 .</p>
<p>Discussion</p>
<p>The results of our technical evaluation and user-studies, while preliminary, provide valuable direction for the development of our future services.The open-weight models Qwen 2.5-72B-Instruct and Llama 3.3-70B-Instruct seemed to perform better than the commercial models that we tested.For the Gemini 1.5-flash-8b model, this is most likely explained by the difference in size.The size of the normal Gemini 1.5-flash model is unknown but given our results and the cost and speed we suspect that it is also smaller than the 70 billion parameter models that we compared them to.</p>
<p>From the feedback that we collected through our pilot user study and the discussion in a later workshop, it became clear that there is a need for better transparency and traceability.Ideally, highlighting the text passages that inform an extracted information items in the source document.</p>
<p>Even though there are commercial services available that are similar to our tool, our contribution can inform researchers and professionals that want to offer customizable domain-specific services to their users.We demonstrate the feasibility of in-context learning and open-weights models for these use-cases and publish our code to boost independent development of transparent services.</p>
<p>Conclusion and Ongoing Work</p>
<p>We confirmed the potential of current LLMs to summarize and extract domainspecific information from scientific text.Through in-context learning, these models can be quickly adapted to specific scientific domains and facilitate the transfer of expert knowledge between researchers by highlighting and comparing key aspects of their work.</p>
<p>Through our user-centric approach, we collected valuable feedback and userstories that can guide the development of current and future services.Notable transparency and the need that services enable the user to verify LLM generated output by tracing summarized information back to the source text.</p>
<p>Our ongoing work will focus on exploring embedding-based retrieval on the extracted structured information, therefore overcoming the arbitrary chunking issue that limits semantic relevance in vector search.We're also exploring how our approach can be integrated in the publishing process, prefilling templates for knowledge graph mapping e.g., for ORKG.Making it easier for authors to fill out forms that facilitate the discoverability of their work.</p>
<p>Overall, our work highlights the potential of LLMs to improve the publishing process and discoverability of scientific information in digital libraries and beyond.Its main contribution is demonstrating the practical integration of these models into a user-focused, open-source system designed to tackle real-world challenges, rather than proposing a novel extraction algorithm itself.</p>
<p>Fig. 1 .
1
Fig. 1.LLM-based demo extraction pipeline.</p>
<p>Few-shot examples seemed to improve the performance of the models in tasks where specific categorial answers were needed and on the free-text extractions measured by BERTScore.But on the binary classification task, the performance decreased.This could be explained by a class bias introduced via the few-shot examples, while on the textual extractions the examples might have informed the model better about the expected format of the answers.Using the full-text of documents in few-shot examples is costly and potentially increases noise.We are working on exploring the impact of more and shorter examples and selecting ideal examples for specific extraction target types.</p>
<p>Table 1 .
1
Model comparison on the paper-coding benchmark (higher = better)
ModelExactAcc BinF1 BERT_F1 Overallqwen-2.5-72b (3-shot)0.2490.5940.8630.569qwen-2.5-72b (0-shot)0.2190.5140.8870.540llama-3.3-70b (0-shot)0.2120.5560.8770.548gemini-1.5-flash-002 (3-shot)0.2460.3300.8930.490gemini-1.5-flash-002 (0-shot)0.1830.3900.8830.486gemini-1.5-flash-8b-001 (0-shot)0.1710.3450.8810.466gemini-1.5-flash-8b-001 (3-shot)0.1800.1480.8970.408
https://web.archive.org/web/20250507134337/https://www.ncses.nsf.gov/ pubs/nsb202333/executive-summary
https://nfdixcs.org/
CC BY 4.0
https://web.archive.org/web/20250607225206/https://blog.google/ technology/ai/google-gemini-next-generation-model-february-2024/
https://web.archive.org/web/20250612080402/https://openai.com/index/ gpt-4-1/
https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/
https://bpm-conference.org/conferences/
https://github.com/SamyAteia/nfdixcs-d3-knowledge-extraction-demo
ResultsWe evaluated model performance against a gold-standard dataset of 122 manually annotated papers.We used three metrics based on the target type:Acknowledgments.We thank the anonymous reviewers for their valuable feedback.This work is funded by the German Research Foundation (DFG) as part of the NFDIxCS consortium (Grant number: 501930651).Disclosure of Interests.The authors have no competing interests to declare that are relevant to the content of this article.
For categorical targets (15 targets, 1121 annotations), we used ExactAcc. 15</p>
<p>For binary targets (13 targets, 984 annotations), we used the macro-averaged F1-score. BinF1</p>
<p>Gradio: Hasslefree sharing and testing of ml models in the wild. A Abid, A Abdalla, A Abid, D Khan, A Alfozan, J Zou, 2019</p>
<p>W Ammar, D Groeneveld, C Bhagavatula, I Beltagy, M Crawford, D Downey, J Dunkelberger, A Elgohary, S Feldman, V Ha, arXiv:1805.02262Construction of the literature graph in semantic scholar. 2018arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. P Georgiev, 2024</p>
<p>Research Data Management in Computer Science-NFDIxCS Approach. M Goedicke, U Lucke, INFORMATIK 20222022BonnGesellschaft für Informatik</p>
<p>The landscape of biomedical research. R González-Márquez, L Schmidt, B M Schmidt, P Berens, D Kobak, 10.1016/j.patter.2024.100968Patterns. 561009682024</p>
<p>A Grattafiori, The Llama 3 Herd of Models. 2024</p>
<p>Open research knowledge graph: a system walkthrough. M Y Jaradeh, A Oelen, M Prinz, M Stocker, S Auer, Digital Libraries for Open Knowledge: 23rd International Conference on Theory and Practice of Digital Libraries, TPDL 2019. Proceedings. Oslo, NorwaySpringerSeptember 9-12, 2019. 201923</p>
<p>Examining the use of supportive and contrasting citations in different disciplines: a brief study using Scite (scite. ai) data. B Lund, A Shamsi, Scientometrics. 12882023</p>
<p>An empirical evaluation of set similarity join techniques. W Mann, N Augsten, P Bouros, Proceedings of the VLDB Endowment. the VLDB Endowment20169</p>
<p>A Oelen, M Y Jaradeh, S Auer, arXiv:2412.04977ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System. 2024arXiv preprint</p>
<p>CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering. D Pride, M Cancellieri, P Knoth, International Conference on Theory and Practice of Digital Libraries. Springer2023</p>
<p>OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. J Priem, H Piwowar, R Orr, arXiv:2205.018332022arXiv preprint</p>
<p>. Yang Qwen, A Yang, B Zhang, B Hui, B Zheng, B Yu, B Li, C Liu, D Huang, F Wei, H Lin, H Yang, J Tu, J Zhang, J Yang, J Yang, J Zhou, J Lin, J Dang, K Lu, K Bao, K Yang, K Yu, L Li, M Xue, M Zhang, P Zhu, Q Men, R Lin, R Li, T Tang, T Xia, T Ren, X Ren, X Fan, Y Su, Y Zhang, Y Wan, Y Liu, Y Cui, Z Zhang, Z Qiu, Z , 2025Qwen2.5 Technical Report</p>
<p>Data extraction methods for systematic review (semi) automation: Update of a living systematic review. L Schmidt, A N F Mutlu, R Elmore, B K Olorisade, J Thomas, J P Higgins, F1000Research. 202510401</p>
<p>Retrieval Augmentation Reduces Hallucination in Conversation. K Shuster, S Poff, M Chen, D Kiela, J Weston, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>SciKGraph: A knowledge graph approach to structure a scientific field. M D L Tosi, J C Reis, 10.1016/j.joi.2020.101109Journal of Informetrics. 1511011092021</p>
<p>Microsoft academic graph: When experts are not enough. K Wang, Z Shen, C Huang, C H Wu, Y Dong, A Kanakia, Quantitative Science Studies. 112020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems. NIPS '22. the 36th International Conference on Neural Information Processing Systems. NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>Elicit: AI literature review research assistant. S Whitfield, M A Hofmann, Public Services Quarterly. 1932023</p>
<p>The FAIR Guiding Principles for scientific data management and stewardship. M D Wilkinson, M Dumontier, I J Aalbersberg, G Appleton, M Axton, A Baak, N Blomberg, J W Boiten, L B Da Silva Santos, P E Bourne, Scientific data. 312016</p>
<p>T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
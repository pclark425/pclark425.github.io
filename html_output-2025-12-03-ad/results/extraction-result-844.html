<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-844 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-844</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-844</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-266998862</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.07128v2.pdf" target="_blank">EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records</a></p>
                <p><strong>Paper Abstract:</strong> Clinicians often rely on data engineers to retrieve complex patient information from electronic health record (EHR) systems, a process that is both inefficient and time-consuming. We propose EHRAgent, a large language model (LLM) agent empowered with accumulative domain knowledge and robust coding capability. EHRAgent enables autonomous code generation and execution to facilitate clinicians in directly interacting with EHRs using natural language. Specifically, we formulate a multi-tabular reasoning task based on EHRs as a tool-use planning process, efficiently decomposing a complex task into a sequence of manageable actions with external toolsets. We first inject relevant medical information to enable EHRAgent to effectively reason about the given query, identifying and extracting the required records from the appropriate tables. By integrating interactive coding and execution feedback, EHRAgent then effectively learns from error messages and iteratively improves its originally generated code. Experiments on three real-world EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate, verifying its strong capacity to tackle complex clinical tasks with minimal demonstrations.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e844.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e844.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EHRAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EHRAgent: Code-Empowered LLM Agent for EHR Multi-Tabular Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based autonomous agent that generates, executes, debugs, and refines Python code plans to answer complex multi-table EHR questions by integrating query-specific medical knowledge, long-term memory of successful cases, interactive coding with execution feedback, and a 'rubber duck' error-tracing debugger.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>EHRAgent</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent uses a base LLM (GPT-4 / GPT-3.5-turbo) as planner; produces executable Python code plans; interacts iteratively with a code executor; components include Medical Information Integration, Long-Term Memory (few-shot example retrieval), Interactive Coding (multi-turn code generation + execution), and Rubber Duck Debugging (error tracing).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>EHR multi-tabular question answering (MIMIC-III, eICU, TREQS)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Multi-tabular EHR reasoning formulated as code-based tool-use planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; planning; multi-step reasoning; sequential action plan execution</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported success rate (SR) on MIMIC-III: overall ~58.97% SR and completion rate (CR) 85.86% (per paper tables); outperforms strongest baseline by up to 29.6% SR across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>code interface (Python), interactive coding with execution feedback, long-term memory for few-shot retrieval, medical information integration (domain knowledge injection), rubber-duck error tracing/debugging, SQLInterpreter tool available</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot prompting / in-context learning (K=4 demonstrations), iterative execution-feedback loops (multiple LLM calls); no supervised fine-tuning reported</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change; prompting strategy; hybrid approach (memory + debugging + tooling)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Transforms EHR QA into an executable-code planning process: (1) injects query-specific medical metadata into prompts; (2) retrieves the most relevant few-shot examples from a long-term memory of prior successful cases; (3) uses a code interface (Python) to generate executable plans and a code executor to run them; (4) iteratively refines code with execution feedback and an error-tracing 'rubber duck' LLM that hypothesizes root causes of errors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial positive effect: EHRAgent achieves much higher success/completion rates than baselines (up to +29.6% SR on some datasets). Ablation: removing interactive coding drops overall SR dramatically (paper reports e.g., overall SR from ~58.97% to ~24.55% and CR from 85.86% to 62.14% on MIMIC-III); removing medical information, long-term memory, or rubber-duck debugging also reduces SR (examples: w/o medical info overall SR ~33.66%; w/o LTM overall SR ~51.73%; w/o rubber-duck debugging overall SR ~42.86%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper attributes the gap to (1) lack of environmental (execution) feedback in open-loop QA methods, (2) absence of a code interface that enables efficient iterative action planning and compact representations (e.g., loops), (3) insufficient domain-specific knowledge/schema understanding for EHRs, (4) context-length limits that hinder many-shot demonstrations, and (5) difficulty of generating high-quality SQL directly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e844.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e844.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs to improve complex problem solving, typically in a single open-loop generation pass.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt engineering method to elicit step-by-step reasoning (natural-language chain-of-thought) from LLMs; no built-in tool execution or code interface.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Applied here to EHR multi-table question answering</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Applied (open-loop) to multi-tabular EHR reasoning without execution feedback</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning (open-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Paper reports open-loop methods like CoT have low success rates on EHR multi-table tasks (all below 40% SR across the three datasets); completion rates are low on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought prompting (no tool interface, no execution loop)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / few-shot in-context; no interactive execution feedback</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used as an open-loop baseline: generate natural-language reasoning steps for the EHR task without code execution or iterative debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Per paper: neglecting environmental feedback prevents adaptive refinement; results in low SR (<40%) on EHR multi-tabular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Open-loop natural-language plans cannot leverage execution feedback or program-like constructs (loops, APIs), making them brittle for tool-use / multi-table EHR tasks and causing poor performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e844.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e844.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (CoT ensemble decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding strategy that samples multiple chain-of-thoughts and aggregates answers to improve reasoning robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregation over multiple sampled chain-of-thought outputs from an LLM to choose the most consistent final answer; remains open-loop without execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Applied here to EHR multi-table question answering</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Open-loop multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning (open-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported to perform poorly on EHR multi-table tasks (below 40% SR across datasets), per paper summary.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>ensemble decoding over CoT samples (no tool-use or execution loop)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / few-shot in-context only</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting/decoding strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used as baseline; does not use environment feedback or code execution.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Does not close the gap in interactive/procedural EHR tasks; low SR reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Lack of execution feedback and inability to adapt plans based on environment/tool errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e844.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e844.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chameleon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chameleon: Plug-and-play compositional reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compositional reasoning baseline that composes reasoning modules; used as an open-loop baseline without integrated execution feedback in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Chameleon</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A compositional reasoning approach that composes subroutines or modules via prompting; in this work used as an open-loop baseline lacking environment feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>EHR multi-tabular question answering (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Applied as open-loop reasoning (no execution-feedback loop)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning (open-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported to perform poorly on EHR multi-table tasks (below 40% SR in aggregate), per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>compositional reasoning via prompting (no interactive code execution)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Baseline that lacks code execution and environment-driven debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Low success/completion rates on complex EHR multi-table tasks compared to EHRAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Absence of environmental feedback and inability to iteratively refine executable plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e844.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e844.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework combining reasoning (chain-of-thought) and actions (tool calls) in a single agent, enabling some environment interaction, but in this paper limited by lack of code-interface and shallow handling of error messages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent that interleaves natural-language reasoning traces with discrete actions (tool calls); enables environment interaction but often uses natural-language planning rather than executable code.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>EHR multi-tabular question answering (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool-augmented question answering with action planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; planning; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported moderate-to-low success on EHR tasks; paper notes ReAct and Reflexion consider environment feedback but are restricted to tool-generated error messages and lack code interface, leading to longer contexts and lower completion rates (numerical SRs for open-loop baselines typically <40% on datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>interleaved reasoning + action (tool calls), natural-language plans (no code interface in baseline usage)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting with tool-use (few-shot/in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>agent framework (tool-use integration)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Uses environment/tool calls and receives tool error messages for refinement but lacks a code interface and deep error-tracing.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Some environment feedback improves over pure open-loop prompting, but constrained by lack of code interface and shallow debugging, yielding lower completion and success rates than EHRAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e844.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e844.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent method that uses episodic verbal reflection (rewards/feedback) for iterative improvement; in this paper evaluated as a baseline with environment-feedback but limited debugging depth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent incorporates verbal reflections (rewards, short summaries) to refine behavior over episodes; receives environment signals but typically limited to high-level rewards rather than deep error traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>EHR multi-tabular question answering (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool-augmented iterative agent refinement</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; multi-step reasoning; learning from feedback</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Per paper, Reflexion considers environmental feedback but is limited and inferior to EHRAgent due to lack of a code interface and detailed error tracing; lower SR/CR than EHRAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>episodic verbal reflection / reward-based refinement (no code interface in this evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>iterative prompting with reward signals / reflection</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>agent-level behavioral training/strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Uses verbalized feedback/reward signals to alter future planning; does not deeply parse execution error traces.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves over purely open-loop prompting in some cases but remains weaker than code-execution + debugging approaches like EHRAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e844.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e844.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM2SQL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM2SQL (text-to-SQL direct generation baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that prompt LLMs to directly generate SQL queries to answer database questions, without interactive code-execution debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLM2SQL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Direct text-to-SQL generation by LLMs to retrieve answers from relational databases; typically open-loop generation of SQL strings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>EHR text-to-SQL question answering / EHRQA</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Direct SQL generation for database QA (not interactive coding)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>database query generation (single-step execution)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Paper reports limited gains from direct SQL generation methods on complex EHR tasks because LLMs struggle to generate high-quality SQL; performance inferior to EHRAgent which uses code + debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>text-to-SQL output; no iterative execution-driven debugging in baseline</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / few-shot text-to-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Direct SQL generation approach; lacks a debugging module and interactive code execution to iteratively fix SQL errors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Limited improvement on complex multi-table EHR questions; lower success rates compared to code-execution + debugging approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e844.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e844.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIN-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DIN-SQL: Decomposed In-Context Learning of text-to-SQL with self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-to-SQL method that decomposes generation and applies rule-based self-correction to improve SQL validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DIN-SQL: Decomposed in-context learning of text-to-SQL with self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DIN-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decomposes SQL generation and applies a set of heuristic/rule-based corrections to self-correct SQL outputs prior to execution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>EHR text-to-SQL question answering</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>SQL generation with automatic self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>database querying; self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Paper notes DIN-SQL improves SQL validity but remains rigid and less adaptive than interactive code-debugging agents; lower success rate than EHRAgent on EHR tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>decomposed in-context learning + rule-based self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>in-context learning with engineered self-correction rules</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy + heuristic self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Applies deterministic rules to correct generated SQL; lacks nuanced error-tracing and domain-aware debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Some increase in SQL validity but insufficiently adaptive for complex EHR multi-table queries compared to interactive code-debugging approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e844.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e844.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Debugging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Debugging (code-execution feedback refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that generates code, executes it, and uses execution errors plus an explanation of the code for iterative refinement, improving completion and success over open-loop methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Debugging</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Self-Debugging</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses code interface and execution feedback: the agent sends execution results and an explanation of the code back to the LLM to refine future generations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>EHR multi-tabular question answering (evaluated as a strong baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive code generation + execution with feedback-driven refinement</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; multi-step reasoning; debugging</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported to have notable performance gains over many baselines (higher SR/CR than open-loop methods). In MIMIC-III comparisons it is one of the stronger baselines (paper shows Self-Debugging SR notably below EHRAgent but higher than many open-loop baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>code interface; execution feedback loop; explanation-based debugging</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot / prompting with iterative execution-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural/training strategy (debugging via execution feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Leverages code generation and execution; returns execution results and code explanation to LLM for refinement, but does not perform deeper error-trace analysis or domain-knowledge injection as EHRAgent does.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves completion and success relative to open-loop baselines; nevertheless EHRAgent outperforms it by adding domain knowledge integration and more advanced debugging ('rubber duck') and long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e844.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e844.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGen: Multi-agent conversation framework for LLM applications</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM framework that enables multi-turn structured conversations (including code execution agents); used as a competitive baseline employing environment feedback but without EHRAgent's domain-specific modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autogen: Enabling next-gen llm applications via multi-agent conversation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-agent conversation framework where agents (assistant, user, proxies) can coordinate to produce code and interact with executors; supports environment feedback but baseline lacks domain injection and specialized debugging used by EHRAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>EHR multi-tabular QA (evaluated as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive multi-agent code generation + execution</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; multi-step reasoning; multi-agent coordination</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>AutoGen achieves notable gains over many baselines but remains below EHRAgent on EHR multi-table tasks (paper reports AutoGen as a strong baseline but EHRAgent still surpasses it).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>multi-agent conversation / code-execution interface; environment feedback</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting with multi-agent orchestration; in-context few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>agent/architectural framework</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Uses multi-agent conversation to manage planning and execution; does not incorporate domain metadata injection, long-term memory retrieval of successful cases, or deep error-tracing debugging as in EHRAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves over simple baselines but EHRAgent's tailored modules (knowledge injection, LTM, rubber-duck debugging) yield superior SR/CR on EHR tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>DIN-SQL: Decomposed in-context learning of text-to-SQL with self-correction <em>(Rating: 2)</em></li>
                <li>Autogen: Enabling next-gen llm applications via multi-agent conversation <em>(Rating: 2)</em></li>
                <li>Self-Debugging <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Self-Consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>Chameleon: Plug-and-play compositional reasoning with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-844",
    "paper_id": "paper-266998862",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "EHRAgent",
            "name_full": "EHRAgent: Code-Empowered LLM Agent for EHR Multi-Tabular Reasoning",
            "brief_description": "An LLM-based autonomous agent that generates, executes, debugs, and refines Python code plans to answer complex multi-table EHR questions by integrating query-specific medical knowledge, long-term memory of successful cases, interactive coding with execution feedback, and a 'rubber duck' error-tracing debugger.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "EHRAgent",
            "model_description": "Agent uses a base LLM (GPT-4 / GPT-3.5-turbo) as planner; produces executable Python code plans; interacts iteratively with a code executor; components include Medical Information Integration, Long-Term Memory (few-shot example retrieval), Interactive Coding (multi-turn code generation + execution), and Rubber Duck Debugging (error tracing).",
            "model_size": null,
            "qa_task_name": "EHR multi-tabular question answering (MIMIC-III, eICU, TREQS)",
            "qa_performance": null,
            "interactive_task_name": "Multi-tabular EHR reasoning formulated as code-based tool-use planning",
            "interactive_task_type": "tool use; planning; multi-step reasoning; sequential action plan execution",
            "interactive_performance": "Reported success rate (SR) on MIMIC-III: overall ~58.97% SR and completion rate (CR) 85.86% (per paper tables); outperforms strongest baseline by up to 29.6% SR across datasets.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "code interface (Python), interactive coding with execution feedback, long-term memory for few-shot retrieval, medical information integration (domain knowledge injection), rubber-duck error tracing/debugging, SQLInterpreter tool available",
            "training_method": "few-shot prompting / in-context learning (K=4 demonstrations), iterative execution-feedback loops (multiple LLM calls); no supervised fine-tuning reported",
            "intervention_type": "architectural change; prompting strategy; hybrid approach (memory + debugging + tooling)",
            "intervention_description": "Transforms EHR QA into an executable-code planning process: (1) injects query-specific medical metadata into prompts; (2) retrieves the most relevant few-shot examples from a long-term memory of prior successful cases; (3) uses a code interface (Python) to generate executable plans and a code executor to run them; (4) iteratively refines code with execution feedback and an error-tracing 'rubber duck' LLM that hypothesizes root causes of errors.",
            "intervention_effect": "Substantial positive effect: EHRAgent achieves much higher success/completion rates than baselines (up to +29.6% SR on some datasets). Ablation: removing interactive coding drops overall SR dramatically (paper reports e.g., overall SR from ~58.97% to ~24.55% and CR from 85.86% to 62.14% on MIMIC-III); removing medical information, long-term memory, or rubber-duck debugging also reduces SR (examples: w/o medical info overall SR ~33.66%; w/o LTM overall SR ~51.73%; w/o rubber-duck debugging overall SR ~42.86%).",
            "hypothesized_cause_of_gap": "The paper attributes the gap to (1) lack of environmental (execution) feedback in open-loop QA methods, (2) absence of a code interface that enables efficient iterative action planning and compact representations (e.g., loops), (3) insufficient domain-specific knowledge/schema understanding for EHRs, (4) context-length limits that hinder many-shot demonstrations, and (5) difficulty of generating high-quality SQL directly.",
            "uuid": "e844.0",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs to improve complex problem solving, typically in a single open-loop generation pass.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Chain-of-Thought (CoT) prompting",
            "model_description": "Prompt engineering method to elicit step-by-step reasoning (natural-language chain-of-thought) from LLMs; no built-in tool execution or code interface.",
            "model_size": null,
            "qa_task_name": "Applied here to EHR multi-table question answering",
            "qa_performance": null,
            "interactive_task_name": "Applied (open-loop) to multi-tabular EHR reasoning without execution feedback",
            "interactive_task_type": "multi-step reasoning (open-loop)",
            "interactive_performance": "Paper reports open-loop methods like CoT have low success rates on EHR multi-table tasks (all below 40% SR across the three datasets); completion rates are low on complex tasks.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "chain-of-thought prompting (no tool interface, no execution loop)",
            "training_method": "prompting / few-shot in-context; no interactive execution feedback",
            "intervention_type": "prompting strategy",
            "intervention_description": "Used as an open-loop baseline: generate natural-language reasoning steps for the EHR task without code execution or iterative debugging.",
            "intervention_effect": "Per paper: neglecting environmental feedback prevents adaptive refinement; results in low SR (&lt;40%) on EHR multi-tabular tasks.",
            "hypothesized_cause_of_gap": "Open-loop natural-language plans cannot leverage execution feedback or program-like constructs (loops, APIs), making them brittle for tool-use / multi-table EHR tasks and causing poor performance.",
            "uuid": "e844.1",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (CoT ensemble decoding)",
            "brief_description": "Decoding strategy that samples multiple chain-of-thoughts and aggregates answers to improve reasoning robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Self-Consistency",
            "model_description": "Aggregation over multiple sampled chain-of-thought outputs from an LLM to choose the most consistent final answer; remains open-loop without execution feedback.",
            "model_size": null,
            "qa_task_name": "Applied here to EHR multi-table question answering",
            "qa_performance": null,
            "interactive_task_name": "Open-loop multi-step reasoning",
            "interactive_task_type": "multi-step reasoning (open-loop)",
            "interactive_performance": "Reported to perform poorly on EHR multi-table tasks (below 40% SR across datasets), per paper summary.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "ensemble decoding over CoT samples (no tool-use or execution loop)",
            "training_method": "prompting / few-shot in-context only",
            "intervention_type": "prompting/decoding strategy",
            "intervention_description": "Used as baseline; does not use environment feedback or code execution.",
            "intervention_effect": "Does not close the gap in interactive/procedural EHR tasks; low SR reported.",
            "hypothesized_cause_of_gap": "Lack of execution feedback and inability to adapt plans based on environment/tool errors.",
            "uuid": "e844.2",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Chameleon",
            "name_full": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "brief_description": "A compositional reasoning baseline that composes reasoning modules; used as an open-loop baseline without integrated execution feedback in this paper's experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Chameleon",
            "model_description": "A compositional reasoning approach that composes subroutines or modules via prompting; in this work used as an open-loop baseline lacking environment feedback.",
            "model_size": null,
            "qa_task_name": "EHR multi-tabular question answering (evaluated as baseline)",
            "qa_performance": null,
            "interactive_task_name": "Applied as open-loop reasoning (no execution-feedback loop)",
            "interactive_task_type": "multi-step reasoning (open-loop)",
            "interactive_performance": "Reported to perform poorly on EHR multi-table tasks (below 40% SR in aggregate), per the paper.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "compositional reasoning via prompting (no interactive code execution)",
            "training_method": "prompting / few-shot",
            "intervention_type": "prompting strategy",
            "intervention_description": "Baseline that lacks code execution and environment-driven debugging.",
            "intervention_effect": "Low success/completion rates on complex EHR multi-table tasks compared to EHRAgent.",
            "hypothesized_cause_of_gap": "Absence of environmental feedback and inability to iteratively refine executable plans.",
            "uuid": "e844.3",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A framework combining reasoning (chain-of-thought) and actions (tool calls) in a single agent, enabling some environment interaction, but in this paper limited by lack of code-interface and shallow handling of error messages.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "model_or_agent_name": "ReAct",
            "model_description": "Agent that interleaves natural-language reasoning traces with discrete actions (tool calls); enables environment interaction but often uses natural-language planning rather than executable code.",
            "model_size": null,
            "qa_task_name": "EHR multi-tabular question answering (used as baseline)",
            "qa_performance": null,
            "interactive_task_name": "Tool-augmented question answering with action planning",
            "interactive_task_type": "tool use; planning; multi-step reasoning",
            "interactive_performance": "Reported moderate-to-low success on EHR tasks; paper notes ReAct and Reflexion consider environment feedback but are restricted to tool-generated error messages and lack code interface, leading to longer contexts and lower completion rates (numerical SRs for open-loop baselines typically &lt;40% on datasets).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "interleaved reasoning + action (tool calls), natural-language plans (no code interface in baseline usage)",
            "training_method": "prompting with tool-use (few-shot/in-context)",
            "intervention_type": "agent framework (tool-use integration)",
            "intervention_description": "Uses environment/tool calls and receives tool error messages for refinement but lacks a code interface and deep error-tracing.",
            "intervention_effect": "Some environment feedback improves over pure open-loop prompting, but constrained by lack of code interface and shallow debugging, yielding lower completion and success rates than EHRAgent.",
            "uuid": "e844.4",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "An agent method that uses episodic verbal reflection (rewards/feedback) for iterative improvement; in this paper evaluated as a baseline with environment-feedback but limited debugging depth.",
            "citation_title": "Reflexion: language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "model_or_agent_name": "Reflexion",
            "model_description": "Agent incorporates verbal reflections (rewards, short summaries) to refine behavior over episodes; receives environment signals but typically limited to high-level rewards rather than deep error traces.",
            "model_size": null,
            "qa_task_name": "EHR multi-tabular question answering (used as baseline)",
            "qa_performance": null,
            "interactive_task_name": "Tool-augmented iterative agent refinement",
            "interactive_task_type": "planning; multi-step reasoning; learning from feedback",
            "interactive_performance": "Per paper, Reflexion considers environmental feedback but is limited and inferior to EHRAgent due to lack of a code interface and detailed error tracing; lower SR/CR than EHRAgent.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "episodic verbal reflection / reward-based refinement (no code interface in this evaluation)",
            "training_method": "iterative prompting with reward signals / reflection",
            "intervention_type": "agent-level behavioral training/strategy",
            "intervention_description": "Uses verbalized feedback/reward signals to alter future planning; does not deeply parse execution error traces.",
            "intervention_effect": "Improves over purely open-loop prompting in some cases but remains weaker than code-execution + debugging approaches like EHRAgent.",
            "uuid": "e844.5",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLM2SQL",
            "name_full": "LLM2SQL (text-to-SQL direct generation baselines)",
            "brief_description": "Methods that prompt LLMs to directly generate SQL queries to answer database questions, without interactive code-execution debugging.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "LLM2SQL",
            "model_description": "Direct text-to-SQL generation by LLMs to retrieve answers from relational databases; typically open-loop generation of SQL strings.",
            "model_size": null,
            "qa_task_name": "EHR text-to-SQL question answering / EHRQA",
            "qa_performance": null,
            "interactive_task_name": "Direct SQL generation for database QA (not interactive coding)",
            "interactive_task_type": "database query generation (single-step execution)",
            "interactive_performance": "Paper reports limited gains from direct SQL generation methods on complex EHR tasks because LLMs struggle to generate high-quality SQL; performance inferior to EHRAgent which uses code + debugging.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "text-to-SQL output; no iterative execution-driven debugging in baseline",
            "training_method": "prompting / few-shot text-to-SQL",
            "intervention_type": "prompting strategy",
            "intervention_description": "Direct SQL generation approach; lacks a debugging module and interactive code execution to iteratively fix SQL errors.",
            "intervention_effect": "Limited improvement on complex multi-table EHR questions; lower success rates compared to code-execution + debugging approaches.",
            "uuid": "e844.6",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "DIN-SQL",
            "name_full": "DIN-SQL: Decomposed In-Context Learning of text-to-SQL with self-correction",
            "brief_description": "A text-to-SQL method that decomposes generation and applies rule-based self-correction to improve SQL validity.",
            "citation_title": "DIN-SQL: Decomposed in-context learning of text-to-SQL with self-correction",
            "mention_or_use": "use",
            "model_or_agent_name": "DIN-SQL",
            "model_description": "Decomposes SQL generation and applies a set of heuristic/rule-based corrections to self-correct SQL outputs prior to execution.",
            "model_size": null,
            "qa_task_name": "EHR text-to-SQL question answering",
            "qa_performance": null,
            "interactive_task_name": "SQL generation with automatic self-correction",
            "interactive_task_type": "database querying; self-correction",
            "interactive_performance": "Paper notes DIN-SQL improves SQL validity but remains rigid and less adaptive than interactive code-debugging agents; lower success rate than EHRAgent on EHR tasks.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "decomposed in-context learning + rule-based self-correction",
            "training_method": "in-context learning with engineered self-correction rules",
            "intervention_type": "prompting strategy + heuristic self-correction",
            "intervention_description": "Applies deterministic rules to correct generated SQL; lacks nuanced error-tracing and domain-aware debugging.",
            "intervention_effect": "Some increase in SQL validity but insufficiently adaptive for complex EHR multi-table queries compared to interactive code-debugging approaches.",
            "uuid": "e844.7",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Self-Debugging",
            "name_full": "Self-Debugging (code-execution feedback refinement)",
            "brief_description": "A baseline that generates code, executes it, and uses execution errors plus an explanation of the code for iterative refinement, improving completion and success over open-loop methods.",
            "citation_title": "Self-Debugging",
            "mention_or_use": "use",
            "model_or_agent_name": "Self-Debugging",
            "model_description": "Uses code interface and execution feedback: the agent sends execution results and an explanation of the code back to the LLM to refine future generations.",
            "model_size": null,
            "qa_task_name": "EHR multi-tabular question answering (evaluated as a strong baseline)",
            "qa_performance": null,
            "interactive_task_name": "Interactive code generation + execution with feedback-driven refinement",
            "interactive_task_type": "tool use; multi-step reasoning; debugging",
            "interactive_performance": "Reported to have notable performance gains over many baselines (higher SR/CR than open-loop methods). In MIMIC-III comparisons it is one of the stronger baselines (paper shows Self-Debugging SR notably below EHRAgent but higher than many open-loop baselines).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "code interface; execution feedback loop; explanation-based debugging",
            "training_method": "few-shot / prompting with iterative execution-feedback",
            "intervention_type": "architectural/training strategy (debugging via execution feedback)",
            "intervention_description": "Leverages code generation and execution; returns execution results and code explanation to LLM for refinement, but does not perform deeper error-trace analysis or domain-knowledge injection as EHRAgent does.",
            "intervention_effect": "Improves completion and success relative to open-loop baselines; nevertheless EHRAgent outperforms it by adding domain knowledge integration and more advanced debugging ('rubber duck') and long-term memory.",
            "uuid": "e844.8",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "AutoGen",
            "name_full": "AutoGen: Multi-agent conversation framework for LLM applications",
            "brief_description": "A multi-agent LLM framework that enables multi-turn structured conversations (including code execution agents); used as a competitive baseline employing environment feedback but without EHRAgent's domain-specific modules.",
            "citation_title": "Autogen: Enabling next-gen llm applications via multi-agent conversation",
            "mention_or_use": "use",
            "model_or_agent_name": "AutoGen",
            "model_description": "Multi-agent conversation framework where agents (assistant, user, proxies) can coordinate to produce code and interact with executors; supports environment feedback but baseline lacks domain injection and specialized debugging used by EHRAgent.",
            "model_size": null,
            "qa_task_name": "EHR multi-tabular QA (evaluated as a baseline)",
            "qa_performance": null,
            "interactive_task_name": "Interactive multi-agent code generation + execution",
            "interactive_task_type": "tool use; multi-step reasoning; multi-agent coordination",
            "interactive_performance": "AutoGen achieves notable gains over many baselines but remains below EHRAgent on EHR multi-table tasks (paper reports AutoGen as a strong baseline but EHRAgent still surpasses it).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "multi-agent conversation / code-execution interface; environment feedback",
            "training_method": "prompting with multi-agent orchestration; in-context few-shot",
            "intervention_type": "agent/architectural framework",
            "intervention_description": "Uses multi-agent conversation to manage planning and execution; does not incorporate domain metadata injection, long-term memory retrieval of successful cases, or deep error-tracing debugging as in EHRAgent.",
            "intervention_effect": "Improves over simple baselines but EHRAgent's tailored modules (knowledge injection, LTM, rubber-duck debugging) yield superior SR/CR on EHR tasks.",
            "uuid": "e844.9",
            "source_info": {
                "paper_title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "DIN-SQL: Decomposed in-context learning of text-to-SQL with self-correction",
            "rating": 2,
            "sanitized_title": "dinsql_decomposed_incontext_learning_of_texttosql_with_selfcorrection"
        },
        {
            "paper_title": "Autogen: Enabling next-gen llm applications via multi-agent conversation",
            "rating": 2,
            "sanitized_title": "autogen_enabling_nextgen_llm_applications_via_multiagent_conversation"
        },
        {
            "paper_title": "Self-Debugging",
            "rating": 2,
            "sanitized_title": "selfdebugging"
        },
        {
            "paper_title": "Chain-of-Thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "rating": 1,
            "sanitized_title": "chameleon_plugandplay_compositional_reasoning_with_large_language_models"
        }
    ],
    "cost": 0.0219175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records
4 Oct 2024</p>
<p>Wenqi Shi wqshi@gatech.edu 
Georgia Institute of Technology</p>
<p>Ran Xu ran.xu@emory.edu 
Emory University</p>
<p>Yuchen Zhuang yczhuang@gatech.edu 
Georgia Institute of Technology</p>
<p>Yue Yu yueyu@gatech.edu 
Jieyu Zhang jieyuz2@cs.washington.edu 
Georgia Institute of Technology</p>
<p>University of Washington</p>
<p>Hang Wu hangwu@gatech.edu 
Georgia Institute of Technology</p>
<p>Yuanda Zhu 
Joyce Ho joyce.c.ho@emory.edu 
Georgia Institute of Technology</p>
<p>Emory University</p>
<p>Carl Yang j.carlyang@emory.edu 
Emory University</p>
<p>May D Wang maywang@gatech.edu 
Georgia Institute of Technology</p>
<p>Lifan Yuan 
Yangyi Chen 
Xingyao Wang 
Yi R Fung 
Hao Peng 
Craft 
Siyu Yuan 
Kaitao Song 
Jiangjie Chen 
Xu Tan 
Yongliang Shen 
Ren Kan 
Dongsheng Li 
EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records
4 Oct 202431C3B0DF8D9B3D6A5F7F4EF1FD4E9B64arXiv:2401.07128v3[cs.CL]
Clinicians often rely on data engineers to retrieve complex patient information from electronic health record (EHR) systems, a process that is both inefficient and time-consuming.We propose EHRAgent 1 , a large language model (LLM) agent empowered with accumulative domain knowledge and robust coding capability.EHRAgent enables autonomous code generation and execution to facilitate clinicians in directly interacting with EHRs using natural language.Specifically, we formulate a multi-tabular reasoning task based on EHRs as a tool-use planning process, efficiently decomposing a complex task into a sequence of manageable actions with external toolsets.We first inject relevant medical information to enable EHRAgent to effectively reason about the given query, identifying and extracting the required records from the appropriate tables.By integrating interactive coding and execution feedback, EHRAgent then effectively learns from error messages and iteratively improves its originally generated code.Experiments on three real-world EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate, verifying its strong capacity to tackle complex clinical tasks with minimal demonstrations.* Equal contribution. 1 Our implementation of EHRAgent is available at https: //github.com/wshi83/EhrAgent.</p>
<p>Introduction</p>
<p>An electronic health record (EHR) is a digital version of a patient's medical history maintained by healthcare providers over time (Gunter and Terry, 2005).In clinical research and practice, clinicians actively interact with EHR systems to access and retrieve patient data, ranging from detailed individuallevel records to comprehensive population-level insights (Cowie et al., 2017).The reliance on pre-defined rule-based conversion systems in most EHRs often necessitates additional training or as-Figure 1: Simple and efficient interactions between clinicians and EHR systems with the assistance of LLM agents.Clinicians specify tasks in natural language, and the LLM agent autonomously generates and executes code to interact with EHRs (right) for answers.It eliminates the need for specialized expertise or extra effort from data engineers, which is typically required when dealing with EHRs in existing clinical settings (left).</p>
<p>sistance from data engineers for clinicians to obtain information beyond these rules (Mandel et al., 2016;Bender and Sartipi, 2013), leading to inefficiencies and delays that may impact the quality and timeliness of patient care.</p>
<p>Alternatively, an autonomous agent could facilitate clinicians to communicate with EHRs in natural languages, translating clinical questions into machine-interpretable queries, planning a sequence of actions, and ultimately delivering the final responses.Compared to existing EHR management that relies heavily on human effort, the adoption of autonomous agents holds great potential to efficiently simplify workflows and reduce workloads for clinicians (Figure 1).Although several supervised learning approaches (Lee et al., 2022;Wang et al., 2020) have been explored to automate the translation of clinical questions into corresponding machine queries, such systems require extensive training samples with fine-grained annotations, which are both expensive and challenging to obtain.</p>
<p>Large language models (LLMs) (OpenAI, 2023;Anil et al., 2023) bring us one step closer to autonomous agents with extensive knowledge and substantial instruction-following abilities from di-   et al., 2017) and SPIDER (Yu et al., 2018), multi-tabular reasoning tasks within EHRs (orange) typically involve a significantly larger number of records per table and necessitate querying multiple tables to answer each question, thereby requiring more advanced reasoning and problem-solving capabilities.</p>
<p>verse corpora during pretraining.LLM-based autonomous agents have demonstrated remarkable capabilities in problem-solving, such as reasoning (Wei et al., 2022), planning (Yao et al., 2023b), and memorizing (Wang et al., 2023b).One particularly notable capability of LLM agents is toolusage (Schick et al., 2023;Qin et al., 2023), where they can utilize external tools (e.g., calculators, APIs, etc.), interact with environments, and generate action plans with intermediate reasoning steps that can be executed sequentially towards a valid solution (Wu et al., 2023;Zhang et al., 2023).Despite their success in general domains, LLMs have encountered unique and significant challenges in the medical domain (Jiang et al., 2023;Yang et al., 2022;Moor et al., 2023), especially when dealing with individual EHR queries that require advanced reasoning across a vast number of records within multiple tables (Li et al., 2024;Lee et al., 2022) (Figure 2).First, given the constraints in both the volume and specificity of training data within the medical field (Thapa and Adhikari, 2023), LLMs still struggle to identify and extract relevant information from the appropriate tables and records within EHRs, due to insufficient knowledge and understanding of their complex structure and content.Second, EHRs are typically large-scale relational databases containing vast amounts of tables with comprehensive administrative and clinical information (e.g., 26 tables of 46K patients in MIMIC-III).Moreover, real-world clinical tasks derived from individual patients or specific groups are highly diverse and complex, requiring multi-step or complicated operations.</p>
<p>To address these limitations, we propose EHRAgent, an autonomous LLM agent with external tools and code interface for improved multi-tabular reasoning across EHRs.We translate the EHR question-answering problem into a tool-use planning process -generating, executing, debugging, and optimizing a sequence of code-based actions.Firstly, to overcome the lack of domain knowledge in LLMs, we instruct EHRAgent to integrate query-specific medical information for effectively reasoning from the given query and locating the query-related tables or records.Moreover, we incorporate long-term memory to continuously maintain a set of successful cases and dynamically select the most relevant few-shot examples, in order to effectively learn from and improve upon past experiences.Secondly, we establish an interactive coding mechanism, which involves a multiturn dialogue between the code planner and executor, iteratively refining the generated code-based plan for complex multi-hop reasoning.Specifically, EHRAgent optimizes the execution plan by incorporating environment feedback and delving into error messages to enhance debugging proficiency.</p>
<p>We conduct extensive experiments on three largescale real-world EHR datasets to validate the empirical effectiveness of EHRAgent, with a particular focus on challenging tasks that reflect diverse information needs and align with real-world application scenarios.In contrast to traditional supervised settings (Lee et al., 2022;Wang et al., 2020) that require over 10K training samples with manually crafted annotations, EHRAgent demonstrates its efficiency by necessitating only four demonstrations.Our findings suggest that EHRAgent improves multi-tabular reasoning on EHRs through autonomous code generation and execution, leveraging accumulative domain knowledge and interactive environmental feedback.</p>
<p>Our main contributions are as follows:</p>
<p> We propose EHRAgent, an LLM agent augmented with external tools and domain knowledge, to solve few-shot multi-tabular reasoning derived from EHRs with only four demonstrations;</p>
<p> Planning with a code interface, EHRAgent formulates a complex clinical problem-solving process as an executable code plan of action sequences, along with a code executor;</p>
<p> We introduce interactive coding between the LLM agent and code executor, iteratively refining plan generation and optimizing code execution by examining environmental feedback in depth;</p>
<p> Experiments on three EHR datasets show that EHRAgent improves the strongest baseline on multihop reasoning by up to 29.6% in success rate.def GetValue(ARGUMENT):</p>
<h1>Get the values of the selected columns </h1>
<p>-As comp-oth vasc dev/graft is a diagnose, the corresponding ICD9_CODE can be found in the d_icd_diagnoses database.</p>
<p>-The ICD9_CODE can be used to find the corresponding HADM_ID in the diagnoses_icd database.</p>
<p>-The HADM_ID can be used to find the corresponding COST in the cost database.</p>
<p>Med. Info.</p>
<p>... icd_code = GetValue("ICD9_CODE") diagnoses_icd_db = LoadDB("diagnoses_icd") filtered_icd_db = FilterDB("ICD9_CODE={icd_code}") hadm_id_list = GetValue("HADM_ID") max_cost = 0 for hadm_id in hadm_id_list: cost_db = LoadDB("cost") filtered_cost_db = FilterDB("HADM_ID={hadm_id}") ... EHRAgent date = Calendar("-1 year") diagnosis_db = LoadDB("d_icd_diagnoses") filtered_diagnosis_db = FilterDB("SHORT_TITLE=comp -oth vasc dev/graft") icd_code = GetValue("ICD9_CODE") max_cost = 0 for hadm_id in hadm_id_list: cost_db = LoadDB("cost") filtered_cost_db = FilterDB("ICD9_CODE={icd_code}") ...</p>
<p>EHRAgent</p>
<p>Runtime Error: There is not column named "ICD9_CODE" in the "cost" database.2 Preliminaries Problem Formulation.In this work, we focus on addressing health-related queries by leveraging information from structured EHRs.The reference EHR, denoted as
R = {R 0 , R 1 ,    }, comprises multiple tables, while C = {C 0 , C 1 ,    } corre-
sponds to the column descriptions within R. For each given query in natural language, denoted as q, our goal is to extract the final answer by utilizing the information within both R and C. LLM Agent Setup.We further formulate the planning process for LLMs as autonomous agents in EHR question answering.For initialization, the LLM agent is equipped with a set of pre-built tools M = {M 0 , M 1 ,    } to interact with and address queries derived from EHRs R. Given an input query q  Q from the task space Q, the objective of the LLM agent is to design a T -step execution plan P = (a 1 , a 2 ,    , a T ), with each action a t selected from the tool set a t  M. Specifically, we generate the action sequences (i.e., plan) by prompting the LLM agent following a policy Planning with Code Interface.To mitigate ambiguities and misinterpretations in plan generation, an increasing number of LLM agents (Gao et al., 2023;Liang et al., 2023;Sun et al., 2023;Chen et al., 2023;Zhuang et al., 2024) employ code prompts as planner interface instead of natural language prompts.The code interface enables LLM agents to formulate an executable code plan as action sequences, intuitively transforming natural language question-answering into iterative coding (Yang et al., 2023).Consequently, the planning policy () turns into a code generation process, with a code execution as the executor ().We then track the outcome of each interaction back to the LLM agent, which can be either a successful execution result or an error message, to iteratively refine the generated code-based plan.This interactive process, a multi-turn dialogue between the planner and executor, takes advantage of the advanced reasoning capabilities of LLMs to optimize plan refinement and execution.
p q  (a 1 ,    , a Tq |q; R, M) : Q  R  M  (M)
Algorithm 1: Overview of EHRAgent.</p>
<p>Input: q: input question; R: reference EHRs; Ci: column description of EHR Ri; D: descriptions of EHRs R; T : the maximum number of steps; T : definitions of tool function; L: long-term memory.
Initialize t  0, C (0) (q)  , O (0) (q)   // Medical Information Integration I = [D; C0; C1;    ] B(q) = LLM([I; q]) // Examples Retrieval from Long-Term Memory E(q) = arg TopK max (sim(q, qi|qi  L)) // Plan Generation C (0) (q) = LLM([I; T ; E(q); q; B(q)]) while t &lt; T &amp; TERMINATE /  O (t) (q) do // Code Execution O (t) (q) = EXECUTE(C (t) (q)) // Debugging and Plan Modification C (t+1) (q) = LLM(DEBUG(O (t) (q))) t  t + 1
Output: Final answer (solved) or error message (unsolved) from O (t) (q).</p>
<p>3 EHRAgent: LLMs as Medical Agents</p>
<p>In this section, we present EHRAgent (Figure 3), an LLM agent that enables multi-turn interactive coding to address multi-hop reasoning tasks on EHRs.EHRAgent comprises four key components:</p>
<p>(1) Medical Information Integration: We incorporate query-specific medical information for effective reasoning based on the given query, enabling EHRAgent to identify and retrieve the necessary tables and records for answering the question.</p>
<p>(2) Demonstration Optimization through Long-Term Memory: Using long-term memory, EHRAgent replaces original few-shot demonstrations with the most relevant successful cases retrieved from past experiences.(3) Interactive Coding with Execution Feedback: EHRAgent harnesses LLMs as autonomous agents in a multi-turn conversation with a code executor.(4) Rubber Duck Debugging via Error Tracing: Rather than simply sending back information from the code executor, EHRAgent thoroughly analyzes error messages to identify the underlying causes of errors through iterations until a final solution.We summarize the workflow of EHRAgent in Algorithm 1.</p>
<p>Medical Information Integration</p>
<p>Clinicians frequently pose complex inquiries that necessitate advanced reasoning across multiple tables and access to a vast number of records within a single query.To accurately identify the required tables, we first incorporate query-specific medical information (i.e., domain knowledge) into EHRAgent to develop a comprehensive understanding of the query within a limited context length.Given an EHR-based clinical question q and the reference EHRs R = {R 0 , R 1 ,    }, the objective of information integration is to generate the domain knowledge most relevant to q, thereby facilitating the identification and location of potential useful references within R. For example, given a query related to 'Aspirin', we expect LLMs to locate the drug 'Aspirin' at the PRESCRIPTION table, under the prescription_name column in the EHR.</p>
<p>To achieve this, we initially maintain a thorough metadata I of all the reference EHRs, including overall data descriptions D and the detailed column descriptions
C i for each individual EHR R i , expressed as I = [D; C 0 ; C 1 ;    ].
To further extract additional background knowledge essential for addressing the complex query q, we then distill key information from the detailed introduction I. Specifically, we directly prompt LLMs to generate the relevant information B(q) based on demonstrations, denoted as B(q) = LLM([I; q]).</p>
<p>Demonstration Optimization through</p>
<p>Long-Term Memory</p>
<p>Due to the vast volume of information within EHRs and the complexity of the clinical questions, there exists a conflict between limited input context length and the number of few-shot examples.Specifically, K-shot examples may not adequately cover the entire question types as well as the EHR information.To address this, we maintain a longterm memory L for storing past successful code snippets and reorganizing few-shot examples by retrieving the most relevant samples from L. Consequently, the LLM agent can learn from and apply patterns observed in past successes to current queries.The selection of K-shot demonstrations E(q) is defined as follows:
E(q) = arg TopK max (sim(q, q i |q i  L)),(1)
where arg TopK max() identifies the indices of the top K elements with the highest values from L, and sim(, ) calculates the similarity between two questions, employing negative Levenshtein distance as the similarity metric.Following this retrieval process, the newly acquired K-shot examples E(q) replace the originally predefined examples
E = {E 1 ,    , E K }.
This updated set of examples serves to reformulate the prompt, guiding EHRAgent in optimal demonstration selection by leveraging accumulative domain knowledge.</p>
<p>Interactive Coding with Execution</p>
<p>We then introduce interactive coding between the LLM agent (i.e., code generator) and code executor to facilitate iterative plan refinement.EHRAgent integrates LLMs with a code executor in a multi-turn conversation.The code executor runs the generated code and returns the results to the LLM.Within the conversation, EHRAgent navigates the subsequent phase of the dialogue, where the LLM agent is expected to either (1) continue to iteratively refine its original code in response to any errors encountered or (2) finally deliver a conclusive answer based on the successful execution outcomes.LLM Agent.To generate accurate code snippets C(q) as solution plans for the query q, we prompt the LLM agent with a combination of the EHR introduction I, tool function definitions T , a set of K-shot examples E(q) updated by long-term memory, the input query q, and the integrated medical information relevant to the query B(q): C(q) = LLM([I; T ; E(q); q; B(q)]).</p>
<p>(2)</p>
<p>We develop the LLM agent to (1) generate code within a designated coding block as required, (2) modify the code according to the outcomes of its execution, and (3) insert a specific code "TERMI-NATE" at the end of its response to indicate the conclusion of the conversation.</p>
<p>Code Executor.The code executor automatically extracts the code from the LLM agent's output and executes it within the local environment: O(q) = EXECUTE(C(q)).After execution, it sends back the execution results to the LLM agent for potential plan refinement and further processing.Given the alignment of empirical observations and Python's inherent modularity with tool functions2 , we select Python 3.9 as the primary coding language for interactions between the LLM agent and the code executor.</p>
<p>Rubber Duck Debugging via Error Tracing</p>
<p>Our empirical observations indicate that LLM agents tend to make slight modifications to the code snippets based on the error message without further debugging.In contrast, human programmers often delve deeper, identifying bugs or underlying causes by analyzing the code implementation against the error descriptions (Chen et al., 2024).Inspired by this, we integrate a 'rubber duck debugging' pipeline with error tracing to refine plans with the LLM agent.Specifically, we provide detailed trace feedback, including error type, message, and location, all parsed from the error information by the code executor.Subsequently, this error context is presented to a 'rubber duck' LLM, prompting it to generate the most probable causes of the error.The generated explanations are then fed back into the conversation flow, aiding in the debugging process.</p>
<p>For the t-th interaction between the LLM agent and the code executor, the process is as follows:
O (t) (q) = EXECUTE(C (t) (q)), C (t+1) (q) = LLM(DEBUG(O (t) (q))).(3)
The interaction ends either when a 'TERMINATE' signal appears in the generated messages or when t reaches a pre-defined threshold of steps T .(Wang et al., 2023d) 33.33 16.56 4.62 1.05 10.17 40.34 27.11 34.67 6.25 31.72 70.69 12.60 11.16 0.00 11.45 57.83 Chameleon (Lu et al., 2023) 38.67 14.11 4.62 4.21 12.77 42.76 31.09 34.68 16.67 35.06 83.41 13.58 12.72 4.55 12.25 60.34 ReAct (Yao et al., 2023b) 34.67 12.27 3.85 2.11 10.38 25.92 27.82 34.24 15.38 33.33 73.68 33.86 26.12 (Pourreza and Rafiei, 2023) 49.51 44.22 36.25 21.85 38.45 81.72 23.49 26.13 12.50 25.00 55.00 41.34 36.38 12.73  ).We categorize input queries into complexity levels (I-IV) based on the number of tables involved in solution generation.We include more details in Appendix A.2. Implementation Details.We employ GPT-4 (Ope-nAI, 2023) (version gpt-4-0613) as the base LLM model for all experiments.We set the temperature to 0 when making API calls to GPT-4 to eliminate randomness and set the pre-defined threshold of steps (T ) to 10. Due to the maximum length limitations of input context in baselines (e.g., Re-Act and Chameleon), we use the same initial fourshot demonstrations (K = 4) for all baselines and EHRAgent to ensure a fair comparison.Appendix E provides additional implementation details with prompt templates.</p>
<p>Experiments</p>
<p>Main Results</p>
<p>Table 1 summarizes the experimental results of EHRAgent and baselines on multi-tabular reasoning within EHRs.From the results, we have the following observations:</p>
<p>(1) EHRAgent significantly outperforms all the baselines on all three datasets with a performance gain of 19.92%, 12.41%, and 29.60%, respectively.This indicates the efficacy of our key designs, namely interactive coding with environment feedback and domain knowledge injection, as they gradually refine the generated code and provide sufficient back-ground information during the planning process.</p>
<p>Experimental results with additional base LLMs are available in Appendix F.1.</p>
<p>(2) CoT, Self-Consistency, and Chameleon all neglect environmental feedback and cannot adaptively refine their planning processes.Such deficiencies hinder their performance in EHR questionanswering scenarios, as the success rates for these methods on three datasets are all below 40%.</p>
<p>(3) ReAct and Reflexion both consider environment feedback but are restricted to tool-generated error messages.Thus, they potentially overlook the overall planning process.Moreover, they both lack a code interface, which prevents them from efficient action planning, and results in lengthy context execution and lower completion rates.</p>
<p>(4) LLM2SQL and DIN-SQL leverage LLM to directly generate SQL queries for EHR questionanswering tasks.However, the gain is rather limited, as the LLM still struggles to generate highquality SQL codes for execution.Besides, the absence of the debugging module further impedes its overall performance on this challenging task.</p>
<p>(5) Self-Debugging and AutoGen present a notable performance gain over other baselines, as they leverage code interfaces and consider the errors from the coding environment, leading to a large improvement in the completion rate.However, as they fail to model medical knowledge or identify underlying causes from error patterns, their success rates are still sub-optimal.</p>
<p>Ablation Studies</p>
<p>Our ablation studies on MIMIC-III ( nents in EHRAgent.Interactive coding 3 is the most significant contributor across all complexity levels, which highlights the importance of code generation in planning and environmental interaction for refinement.In addition, more challenging tasks benefits more from knowledge integration, indicating that comprehensive understanding of EHRs facilitates the complex multi-tabular reasoning in effective schema linking and reference (e.g., tables, columns, and condition values) identification.Detailed analysis with additional settings and results is available in Appendix F.2.</p>
<p>Quantitative Analysis</p>
<p>Effect of Question Complexity.We take a closer look at the model performance by considering multi-dimensional measurements of question complexity, exhibited in Figure 4.Although the performances of both EHRAgent and the baselines generally decrease with an increase in task complexity (either quantified as more elements in queries or more columns in solutions), EHRAgent consistently outperforms all the baselines at various levels of difficulty.Appendix G.1 includes additional analysis on the effect of various question complexities.Sample Efficiency. Figure 5 illustrates the model performance w.r.t.number of demonstrations for EHRAgent and the two strongest baselines, Au-toGen and Self-Debugging.Compared to supervised learning like text-to-SQL (Wang et al., 2020;Raghavan et al., 2021;Lee et al., 2022) that requires extensive training on over 10K samples with detailed annotations (e.g., manually generated corresponding code for each query), LLM agents enable complex tabular reasoning using a few demonstrations only.One interesting finding is that as the number of examples increases, both the success and completion rate of AutoGen tend to decrease, 3 For EHRAgent w/o interactive coding, we deteriorate from generating code-based to natural language-based plans and enable debugging based on error messages from tool execution.mainly due to the context limitation of LLMs.Notably, the performance of EHRAgent remains stable with more demonstrations, which may benefit from its integration of a 'rubber duck' debugging module and the adaptive mechanism for selecting the most relevant demonstrations.</p>
<p>Error Analysis</p>
<p>Figure 6 presents a summary of error types identified in the solution generation process of EHRAgent based on the MIMIC-III, as determined through manual examinations and analysis.The majority of errors occur because the LLM agent consistently fails to identify the underlying cause of these errors within T -step trails, resulting in plans that are either incomplete or inexcusable.Additional analysis of each error type is available in Appendix G.2.</p>
<p>Case Study</p>
<p>Figure 7 presents a case study of EHRAgent in interactive coding with environment feedback.The initial solution from LLM is unsatisfactory with multiple errors.Fortunately, EHRAgent is capable of identifying the underlying causes of errors by analyzing error messages and resolves multiple errors one by one through iterations.We have additional case studies in Appendix H.</p>
<p>Related Work</p>
<p>Augmenting LLMs with External Tools.LLMs have rapidly evolved from text generators into core computational engines of autonomous agents, with advanced planning and tool-use capabilities (Schick et al., 2023;Shen et al., 2023;Wang et al., 2024b;Yuan et al., 2024a,b;Zhuang et al., 2023).LLM agents equip LLMs with planning capabilities (Yao et al., 2023a;Gong et al., 2023) to decompose a large and hard task into multiple smaller and simpler steps for efficiently navigating complex real-world scenarios.By integrating with external tools, LLM agents access external APIs for additional knowledge beyond training data (Lu et al., 2023;Patil et al., 2023;Qin et al., 2024;Li et al., 2023b,a).The disconnection between plan generation and execution, however, prevents LLM agents from effectively and efficiently mitigating error propagation and learning from environmental feedback (Qiao et al., 2023;Shinn et al., 2023;Yang et al., 2023) In such cases, it is generally preferable to acknowledge failure rather than generate an incorrect answer, as this could lead to an inaccurate diagnosis.</p>
<p>We will explore stricter evaluation metrics to assess the cases of misinformation that could pose a risk within clinical settings in our future work.</p>
<p>Privacy and Ethical Statement</p>
<p>In compliance with the PhysioNet Credentialed Health Data Use Agreement 1.5.04 , we strictly prohibit the transfer of confidential patient data (MIMIC-III and eICU) to third parties, including through online services like APIs.To ensure responsible usage of Azure OpenAI Service based on the guideline5 , we have opted out of the human review process by requesting the Azure Ope-nAI Additional Use Case Form6 , which prevents third-parties (e.g., Microsoft) from accessing and processing sensitive patient information for any purpose.We continuously and carefully monitor our compliance with these guidelines and the relevant privacy laws to uphold the ethical use of data in our research and operations.</p>
<p>A.2 Question Complexity Level</p>
<p>We categorize input queries into various complexity levels (levels I-IV for MIMIC-III and levels I-III for eICU and TREQS) based on the number of tables involved in solution generation.For example, given the question 'How many patients were given temporary tracheostomy?', the complexity level is categorized as II, indicating that we need to extract information from two tables (admission and procedure) to generate the solution.Furthermore, we also conduct a performance analysis (see Figure 4) based on additional evaluation metrics related to question complexity, including (1) the number of elements (i.e., slots) in each question and (2) the number of columns involved in each solution.Specifically, elements refer to the slots within each template that can be populated with pre-defined values or database records.</p>
<p>A</p>
<p>D Selection of Primary Programming Language</p>
<p>In our main experiments, we concentrate on three SQL-based EHR QA datasets to assess EHRAgent in comparison with other baselines.Nevertheless, we have opted for Python as the primary programming language for EHRAgent, rather than SQL 13 . 13We include an empirical analysis in Appendix G.3 to further justify the selection of Python as the primary program-The primary reasons for choosing Python instead of SQL to address medical inquiries based on EHRs are outlined below:</p>
<p>Python Enables the External Tool-Use.Using alternative programming languages, such as SQL, can result in LLM-based agents becoming unavailable to external tools or functions.The primary contribution of EHRAgent is to develop a codeempowered agent capable of generating and executing code-based plans to solve complex real-world clinic tasks.In general, the SQL language itself is incapable of calling API functions.For example, EHRXQA (Bae et al., 2023) can be considered as an LLM agent that generates a solution plan in NeuralSQL (not SQL).This agent is equipped with two tools: a pre-trained Visual Question Answering (VQA) model called FUNC_VQA, and a SQL interpreter.Similar to EHRAgent, it also relies on a non-SQL language and includes an SQL interpreter as a tool.Compared with NeuralSQL in EHRXQA (Bae et al., 2023), Python in EHRAgent can be directly executed, while NeuralSQL requires additional parsing.</p>
<p>Python Enables the Integration of SQL Tool Function.Python provides excellent interoperability with various databases and data formats.It supports a wide range of database connectors, including popular relational databases such as PostgreSQL, MySQL, and SQLite, as well as non-relational databases like MongoDB.This interoperability ensures that EHRAgent can seamlessly interact with different EHR systems and databases.Although our proposed method primarily relies on generating and executing ming language for EHRAgent.</p>
<p>Python code, we do not prohibit EHRAgent from utilizing SQL to solve problems.In our prompts and instructions, we also provide the 'SQLInterpreter' tool function for the agent to perform relational database operations using SQL.Through our experiments, we have observed that EHRAgent is capable of combining results from Python code and SQL commands effectively.For instance, when presented with the question, "Show me patient 28020's length of stay of the last hospital stay.",EHRAgent will first generate SQL command admit_disch_tuple = SQLInterpreter (ELECT ADMITTIME, DISCHTIME FROM admissions WHERE SUBJECT_ID=28020 ORDER BY ADMITTIME DESC LIMIT 1 ) and execute it to obtain the tuples containing the patient's admission and discharge times.It will then employ Python code along with the built-in date-time function to calculate the duration of the last stay tuple.</p>
<p>Python Enables a More Generalizable Framework.EHRAgent is a generalizable LLM agent empowered with a code interface to autonomously generate and execute code as solutions to given While Section 4 focuses on the challenging multi-tabular reasoning task within EHRs for evaluation, the Python-based approach has the potential to be generalized to other tasks (e.g., risk prediction tasks based on EHRs) or even multi-modal clinical data and be integrated with additional toolsets in the future.In contrast, other languages like SQL are limited to database-related operations.</p>
<p>Python is More Flexible in Extension.Python is a general-purpose programming language that offers greater flexibility compared to SQL.It enables the implementation of complex logic and algorithms, which may be necessary for solving certain types of medical questions that require more than simple database queries.Python is also a highly flexible programming language that offers extensive capabilities through its libraries and frameworks, making it suitable for handling a wide range of programming tasks, including database operations.In contrast, SQL is only applicable within relational databases and does not provide the same level of flexibility and extension.This attribute is particularly important to LLM-based agents, as they can leverage both existing Python libraries and custom-defined functions as tools to solve complex problems that are inaccessible for and beyond the scope of SQL.</p>
<p>Python Includes More Extensive Resources for Pre-Training.Python has a large and active community of developers and researchers.This community contributes to the development of powerful libraries, frameworks, and tools that can be leveraged in EHRAgent.The extensive documentation, tutorials, and forums available for Python also provide valuable resources for troubleshooting and optimization.Github repositories are one of the most extensive sources of code data for state-ofthe-art language models (i.e., LLMs), such as GPTs.Python is the most widely used coding language on Github14 .In addition, Python is known for its readability and maintainability.The clean and expressive syntax of Python makes it easier for researchers and developers to understand, modify, and extend the codebase of EHRAgent.This is particularly important when extended to realworld clinical research and practice, where the system may need to be updated frequently to incorporate new knowledge and adapt to evolving requirements.</p>
<p>E Additional Implementation Details E.1 Hardware and Software Details</p>
<p>All experiments are conducted on CPU: Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz and GPU: NVIDIA GeForce RTX A5000 GPUs, using Python 3.9 and AutoGen 0.2.015 .</p>
<p>E.2 Data Preprocessing Details</p>
<p>During the data pre-processing stage, we create EHR question-answering pairs by considering text queries as questions and executing SQL commands in the database to automatically generate the corresponding ground-truth answers.We filter out samples containing unexecutable SQL commands or yielding empty results throughout this process.</p>
<p>E.3 Code Generation Details</p>
<p>Given that the majority of LLMs have been pretrained on Python code snippets (Gao et al., 2023), and Python's inherent modularity aligns well with tool functions, we choose Python 3.9 as the primary coding language for interaction coding and AutoGen 0.2.0 (Wu et al., 2023) as the interface for communication between the LLM agent and the code executor.</p>
<p>E.4 Selection of Initial Set of Demonstrations</p>
<p>The initial set of examples is collected manually, following four criteria: (1) using the same demonstrations across all the baselines; (2) utilizing all the designed tools; (3) covering as many distinct tables as possible; and ( 4) including examples in different styles of questions.With these criteria in mind, we manually crafted four demonstrations for each dataset.To ensure a fair comparison, we use the same initial four-shot demonstrations (K = 4) for all baselines and EHRAgent, considering the maximum length limitations of input context in baselines like ReAct (Yao et al., 2023b) and Chameleon (Lu et al., 2023).</p>
<p>E.5 Evaluation Metric Details</p>
<p>Our main evaluation metric is the success rate (SR), quantifying the percentage of queries that the model successfully handles.In addition, we leverage completion rate (CR) as a side evaluation metric to represent the percentage of queries for which the model is able to generate executable plans, regardless of whether the results are correct.Specifically, following existing LLM-based agent studies (Xu et al., 2023;Kirk et al., 2024), we use CR to assess the effectiveness of LLM-based agents in generating complete executable plans without execution errors.One of our key components in EHRAgent is interactive coding with environmental feedback.By using CR, we can demonstrate that our proposed EHRAgent, along with other baselines that incorporate environmental feedback (e.g., ReAct (Yao et al., 2023b), Reflexion (Shinn et al., 2023), Self-Debugging (Chen et al., 2024), andAutoGen (Wu et al., 2023)), has a stronger capability (higher CR) in generating complete executable plans without execution errors, compared to baselines without environmental feedback (e.g., CoT (Wei et al., 2022), Self-Consistency (Wang et al., 2023d), Chameleon (Lu et al., 2023), and LLM2SQL (Nan et al., 2023)).</p>
<p>E.6 EHR Metadata Details</p>
<p> MIMIC-III.</p>
<p><MIMIC_III> Metadata</p>
<p>Read the following data descriptions, generate the background knowledge as the context information that could be helpful for answering the question.(1) Tables are linked by identifiers which usually have the suffix 'ID'.For example, SUBJECT_ID refers to a unique patient, HADM_ID refers to a unique admission to the hospital, and ICUSTAY_ID refers to a unique admission to an intensive care unit.(2) Charted events such as notes, laboratory tests, and fluid balance are stored in a series of 'events' tables.For example the outputevents table contains all measurements related to output for a given patient, while the labevents table contains laboratory test results for a patient.(3) Tables prefixed with 'd_' are dictionary tables and provide definitions for identifiers.cFor example, every row of chartevents is associated with a single ITEMID which represents the concept measured, but it does not contain the actual name of the measurement.By joining chartevents and d_items on ITEMID, it is possible to identify the concept represented by a given ITEMID.(4) For the databases, four of them are used to define and track patient stays: admissions, patients,icustays, and transfers.Another four tables are dictionaries for crossreferencing codes against their respective definitions: d_icd_diagnoses, d_icd_procedures, d_items, and d_labitems.The remaining tables, including chartevents, cost, inputevents_cv, labevents, microbiologyevents, outputevents, prescriptions, procedures_icd, contain data associated with patient care, such as physiological measurements, caregiver observations, and billing information.</p>
<p> eICU.</p>
<p><eICU> Metadata Read the following data descriptions, generate the background knowledge as the context information that could be helpful for answering the question.(1) Data include vital signs, laboratory measurements, medications, APACHE components, care plan information, admission diagnosis, patient history, time-stamped diagnoses from a structured problem list, and similarly chosen treatments.(2) Data from each patient is collected into a common warehouse only if certain interfaces are available.Each interface is used to transform and load a certain type of data: vital sign interfaces incorporate vital signs, laboratory interfaces provide measurements on blood samples, and so on.(3) It is important to be aware that different care units may have different interfaces in place, and that the lack of an interface will result in no data being available for a given patient, even if those measurements were made in reality.The data is provided as a relational database, comprising multiple tables joined by keys.(4) All the databases are used to record information associated to patient care, such as allergy, cost, diagnosis, intakeoutput, lab, medication, microlab, patient, treatment, vitalperiodic.</p>
<p> TREQS.</p>
<p><TREQS> Metadata Read the following data descriptions, generate the background knowledge as the context information that could be helpful for answering the question.(1) The database contains five categories of information for patients, including demographics, laboratory tests, diagnosis, procedures and prescriptions, and prepared a specific table for each category separately.</p>
<p>(2) These tables compose a relational patient database where tables are linked through patient ID and admission ID.</p>
<p>E.7 Prompt Details</p>
<p>In the subsequent subsections, we detail the prompt templates employed in EHRAgent.The complete version of the prompts is available at our code repository due to space limitations. Prompt for Code Generation.We first present the prompt template for EHRAgent in code generation as follows:</p>
<p><LLM_Agent> Prompt Assume you have knowledge of several tables: {OVERALL_EHR_DESCRIPTIONS} Write a python code to solve the given question.</p>
<p>You can use the following functions: {TOOL_DEFINITIONS} Use the variable 'answer' to store the answer of the code.Here are some examples: {4-SHOT_EXAMPLES} (END OF EXAMPLES) Knowledge: {KNOWLEDGE} Question: {QUESTION} Solution:</p>
<p> Prompt for Knowledge Integration.We then present the prompt template for knowledge integration in EHRAgent as follows:</p>
<p><Medical_Knowledge> Prompt Read the following data descriptions, generate the background knowledge as the context information that could be helpful for answering the question.{OVERALL_EHR_DESCRIPTIONS} For different tables, they contain the following information: {COLUMNAR_DESCRIPTIONS}</p>
<p>F Additional Experimental Results</p>
<p>F.1 Effect of Base LLMs</p>
<p>Table 6 presents a summary of the experimental results obtained from EHRAgent and all baselines using a different base LLM, GPT-3.5-turbo(0613).The results clearly demonstrate that EHRAgent continues to outperform all the baselines, achieving a performance gain of 6.72%.This highlights the ability of EHRAgent to generalize across different base LLMs as backbone models.When comparing the experiments conducted with GPT-4 (Table 1), the performance of both the baselines and EHRAgent decreases.This can primarily be attributed to the weaker capabilities of instructionfollowing and reasoning in GPT-3.5-turbo.</p>
<p>w/o Code Interface</p>
<p>CoT (Wei et al., 2022) 23.16 10.40 2.99 1.71 8.62 41.55 Self-Consistency (Wang et al., 2023d) 25.26 11.88 4.19 2.56 10.52 47.59 Chameleon (Lu et al., 2023) 27.37 11.88 3.59 2.56 11.21 47.59 ReAct (Yao et al., 2023b) 26.32 10.89 3.59 3.42 9.66 61.21 Reflexion (Shinn et al., 2023) 30 Table 6: Experimental results of success rate (i.e., SR.) and completion rate (i.e., CR.) on MIMIC-III using GPT-3.5-turboas the base LLM.The complexity of questions increases from Level I (the simplest) to Level IV (the most difficult).</p>
<p>F.2 Additional Ablation Studies</p>
<p>We conduct additional ablation studies to evaluate the effectiveness of each module in EHRAgent on eICU in Table 7 and obtain consistent results.</p>
<p>From the results from both MIMIC-III and eICU, we observe that all four components contribute significantly to the performance gain. Medical Information Integration.Out of all the components, the medical knowledge injection module mainly exhibits its benefits in challenging tasks.These tasks often involve more tables and require a deeper understanding of domain knowledge to associate items with their corresponding tables. Long-term Memory.Following the reinforcement learning setting (Sun et al., 2023;Shinn et al., 2023), the long-term memory mechanism improves performance by justifying the necessity of selecting the most relevant demonstrations for planning.</p>
<p>In order to simulate the scenario where the ground truth annotations (i.e., rewards) are unavailable, we further evaluate the effectiveness of the long-term memory on the completed cases in Table 8, regardless of whether they are successful or not.The results indicate that the inclusion of long-term memory with completed cases increases the completion rate but tends to reduce the success rate across most difficulty levels, as some incorrect cases might be included as the few-shot demonstrations.We have also performed multi-round experiments with shuffled order and observed that the order had almost no influence on the final performance in all three datasets.Nonetheless, it still outperforms the per-formance without long-term memory, confirming the effectiveness of the memory mechanism. Interactive Coding.For the ablation study setting of EHRAgent w/o interactive coding, we directly chose CoT (Wei et al., 2022) as the backbone, where we deteriorate from generating code-based plans to natural language-based plans.Once the steps are generated, we execute them in a step-bystep manner and obtain error information from the tool functions.By combining the error messages with tool definitions and language-based plans, we are still able to prompt the LLMs to deduce the most probable underlying cause of the error.The medical information injection and long-term memory components remain unchanged from the original EHRAgent.From the ablation studies, we can observe that the interactive coding interface is the most significant contributor to the performance gain across all complexity levels.This verifies the importance of utilizing the code interface for planning instead of natural languages, which enables the model to avoid overly complex contexts and thus leads to a substantial increase in the completion rate.Additionally, the code interface also allows the debugging module to refine the planning with execution feedback, improving the efficacy of the planning process. Debugging Module.The 'rubber duck' debugging module enhances the performance by guiding the LLM agent to figure out the underlying reasons for the error messages.This enables EHRAgent to address the intrinsic error that occurs in the original reasoning steps.We then further illustrate the difference between debugging modules in EHRAgent and others.Self-debugging (Chen et al., 2024) that sends back the execution results with an explanation of the code for plan refinement.Reflexion (Shinn et al., 2023) sends the binary reward of whether it is successful or not back for refinement, which contains little information.In both cases, however, the error message is still information on the surface, like 'incorrect query', etc.This is aligned with our empirical observations that LLM agents tend to make slight modifications to the code snippets based on the error message without further debugging.Taking one step further, our debugging module in EHRAgent incorporates an error tracing procedure that enables the LLM to analyze potential causes beyond the current error message.Our debugging module aims to leverage the conversation format to think one step further about potential reasons, such as 'incorrect column names in the query' or 'incorrect values in the query'.</p>
<p>F.3 Cost Estimation</p>
<p>Using GPT-4 as the foundational LLM model, we report the average cost of EHRAgent for each query in the MIMIC-III, eICU, and TREQS datasets as $0.60, $0.17, and $0.52, respectively.The cost is mainly determined by the complexity of the question (i.e., the number of tables required to answer the question) and the difficulty in locating relevant information within each table.</p>
<p>G Additional Empirical Analysis</p>
<p>G.1 Additional Question Complexity Analysis</p>
<p>We further analyze the model performance by considering various measures of question complexity based on the number of elements in questions, and the number of columns involved in solutions, as shown in Figure 4. Incorporating more elements requires the model to either perform calculations or utilize domain knowledge to establish connections between elements and specific columns.Similarly, involving more columns also presents a challenge for the model in accurately locating and associating the relevant columns.We notice that both EHRAgent and baselines generally exhibit lower performance on more challenging tasks 16 .Notably, 16 Exceptions may exist when considering questions of seven elements in Figures 4(a) and 4(b), as it comprises only eight samples and may not be as representative.</p>
<p>our model consistently outperforms all the baseline models across all levels of difficulty.Specifically, for those questions with more than 10 columns, the completion rate of those open-loop baselines is very low (less than 20%), whereas EHRAgent can still correctly answer around 50% of queries, indicating the robustness of EHRAgent in handling complex queries with multiple elements.</p>
<p>G.2 Additional Error Analysis</p>
<p>We conducted a manual examination to analyze all incorrect cases generated by EHRAgent in MIMIC-III.Figure 6 illustrates the percentage of each type of error frequently encountered during solution generation:</p>
<p> Date/Time.When addressing queries related to dates and times, it is important for the LLM agent to use the 'Calendar' tool, which bases its calculations on the system time of the database.This approach is typically reliable, but there are situations where the agent defaults to calculating dates based on real-world time.Such instances may lead to potential inaccuracies. Context Length.This type of error occurs when the input queries or dialog histories are excessively long, exceeding the context length limit.</p>
<p> Incorrect Logic.When solving multi-hop reasoning questions across multiple databases, the LLM agent may generate executable plans that contain logical errors in the intermediate reasoning steps.For instance, in computing the total cost of a hospital visit, the LLM agent might erroneously generate a plan that filters the database using patient_id instead of the correct admission_id.</p>
<p> Incorrect SQL Command.This error type arises when the LLM agent attempts to integrate the SQLInterpreter into a Python-based plan to derive intermediate results.Typically, incorrect SQL commands result in empty responses from SQLInterpreter, leading to the failure of subsequent parts of the plan.</p>
<p> Fail to Follow Instructions.The LLM agent often fails to follow the instructions provided in the initial prompt or during the interactive debugging process.</p>
<p> Fail to Debug.Despite undertaking all T -step trials, the LLM agent consistently fails to identify the root cause of errors, resulting in plans that are either incomplete or inexcusable.</p>
<p>G.3 Additional Empirical Comparison of Primary Programming Languages</p>
<p>We conduct an additional analysis based on the empirical results (byond main results in Table 1) to further justify the selection of Python as our primary programming language.</p>
<p>Data Complexity.The SPIDER (Yu et al., 2018) dataset, which is commonly used in SQL baselines (Pourreza and Rafiei, 2023), typically only involves referencing information from an average of 1.1 tables per question.In contrast, the EHRQA datasets we utilized require referencing information from an average of 1.9 tables per question.This significant gap in # tablesquestions indicates that EHRQA requires more advanced reasoning across multiple tables.</p>
<p>Sample Efficiency.SQL-based methods require more demonstrations.As SQL occupies a relatively smaller proportion of training data, it is quite difficult for LLMs to generate valid SQL commands.Usually, the methods need at least tens of demonstrations to get the LLMs familiar with the data schema and SQL grammar.In EHRAgents, we only need four demonstrations as few-shot multitabular reasoning.</p>
<p>Environment Feedback.DIN-SQL (Pourreza and Rafiei, 2023) establishes a set of rules to automatically self-correct the SQL commands generated.Nevertheless, these rules are rigid and may not cover all potential scenarios.While it does contribute to enhancing the validity of the generated SQL commands to some extent, DIN-SQL lacks tailored information to optimize the code based on different circumstances, resulting in a lower success rate compared to self-debugging and EHRAgent, which provide error messages and deeper insights.</p>
<p>Execution Time Efficiency.We acknowledge that when handling large amounts of data, Python may experience efficiency issues compared to SQL commands.We have also observed similar challenges when working with the TREQS dataset, which contains a massive database with millions of records.However, in the MIMIC-III dataset, EHRAgent (avg.52.63 seconds per question) still demonstrates higher efficiency compared to the state-of-the-art LLM4SQL method, DIN-SQL (Pourreza and Rafiei, 2023) (avg.103.28 seconds per question).We will consider the efficiency of Python when dealing with large-scale databases as one of the important future directions.</p>
<p>H Additional Case Studies</p>
<p>We present additional case studies to showcase the effectiveness of each module in EHRAgent, including medical information integration (Section H.1), long-term memory (Section H.2), code interface (Section H.3), and rubber duck debugging module (Section H.4). In addition, we include Figures 13 and 14 to showcase the entire workflow of EHRAgent with accumulative domain knowledge and coding capability for EHR reasoning.</p>
<p>H.1 Case Study of Medical Information Integration</p>
<p>Figure 8 presents a case study of integrated medical knowledge based on a given query.When faced with a question related to 'aspirin ec' and 'venous cath nec', EHRAgent effectively summarizes and integrates relevant knowledge containing drug and procedure information, successfully reasoning and identifying the potential locations of the required information (tables and records) in the EHR database (e.g., 'prescriptions', 'd_icd_procedures'), along with the detailed identifiers (e.g., 'ICD9_CODE', 'HADM_ID') needed to associate them.</p>
<p>Question: count the number of patients who were prescribed aspirin ec within 2 month after having received a venous cath nec procedure until 4 year ago.</p>
<p>EHRAgent Knowledge:</p>
<p>-As aspirin ec is a drug, we can find the corresponding information in the prescriptions database.</p>
<p>-As venous cath nec is a procedure, we can find the corresponding ICD9_CODE in the d_icd_procedures database.</p>
<p>-The ICD9_CODE can be used to find the corresponding HADM_ID in the procedures_icd database.</p>
<p>-The HADM_ID can be used to find the corresponding SUBJECT_ID in the admissions database.</p>
<p>-We can then count the number of unique SUBJECT_IDs in the prescriptions database who were prescribed aspirin ec within 2 months after having received a venous cath nec procedure until 4 years ago.</p>
<p>Figure 2 :
2
Figure 2: Compared to general domain tasks (blue) such as WikiSQL(Zhong et al., 2017)  and SPIDER(Yu et al., 2018), multi-tabular reasoning tasks within EHRs (orange) typically involve a significantly larger number of records per table and necessitate querying multiple tables to answer each question, thereby requiring more advanced reasoning and problem-solving capabilities.</p>
<p>select max(t1.c1)from ( select sum(cost.cost)as c1 from cost where cost.hadm_id in  EHRAgent (Ours) EHRAgent Assume you have knowledge of following medical records: [EHR_metadata].Write a Python code to solve the given question.You can use the following functions: [api_name, api_description].Here are some examples: [k-shot demonstrations].The related knowledge to the question is given: [medical information].Question: [question].Solution: EHR Metadata (1) Charted events are stored in a series of 'events' tables (2) Tables prefixed with 'd_' are dictionary (3) Four databases are used to define and track patient stays Tool Set (API) def LoadDB(DBName): # Load the database DBName  def FilterDB(CONDITIONS):# Filter the data with CONDITIONS </p>
<p>Figure 3 :
3
Figure 3: Overview of our proposed LLM agent, EHRAgent, for complex few-shot tabular reasoning tasks on EHRs.Given an input clinical question based on EHRs, EHRAgent decomposes the task and generates a plan (i.e., code) based on (a) metadata (i.e., descriptions of tables and columns in EHRs), (b) tool function definitions, (c) few-shot examples, and (d) domain knowledge (i.e., integrated medical information).Upon execution, EHRAgent iteratively debugs the generated code following the execution errors and ultimately generates the final solution.</p>
<p>Figure 4 :Figure 5 :
45
Figure4: Success rate and completion rate under different question complexity, measured by the number of elements (i.e., slots) in each question (upper) and the number of columns involved in each solution (bottom).</p>
<p>Figure 6 :
6
Figure 6: Percentage of mistake examples in different categories on MIMIC-III dataset.</p>
<p></p>
<p>Prompt for 'Rubber Duck' Debugging.The prompt template used for debugging module in EHRAgent is shown as follows: code and point out the most possible reason to the error. Prompt for Few-Shot Examples.The prompt template used for few-shot examples in EHRAgent is shown as follows:</p>
<p>Figure 9 :
9
Figure9: Case study of long-term memory in EHRAgent on MIMIC-III dataset.From the original few-shot examples on the left, none of the questions related to either 'count the number' scenarios or procedure knowledge.In contrast, when we retrieve examples from the long-term memory, the new set is exclusively related to 'count the number' questions, thus providing a similar solution logic for reference.</p>
<p>Figure 10 :#
10
Figure10: Case study 1 of code interface in EHRAgent on MIMIC-III dataset.The baseline approach, ReAct, lacks a code interface and encounters limitations when performing identical operations on multiple sets of data.It resorts to generating repetitive action steps iteratively, leading to an extended solution trajectory that may exceed the context limitations.In contrast, EHRAgent leverages the advantages of code structures, such as the use of 'for loops', to address these challenges more efficiently and effectively.The steps marked in red on the left side indicate the repeated actions by ReAct, while the steps marked in green are the corresponding code snippets by EHRAgent.By comparing the length and number of steps, the code interface can help EHRAgent save much context space.</p>
<p>Figure 11 :
11
Figure11: Case study 2 of code interface in EHRAgent on MIMIC-III dataset.When encountering challenges in tool use, ReAct will keep making trials and can be stuck in the modification process.On the other hand, with code interface, EHRAgent can take advantage of Python built-in functions to help with debugging and code modification.</p>
<p>Table 1 :
1
38.05 82.73 Self-Debugging (Chen et al., 2024) 50.00 46.93 30.12 27.61 39.05 71.24 32.53 21.86 25.00 30.52 66.90 43.54 36.65 18.18 40.10 84.44 AutoGen (Wu et al., 2023) 36.00 28.13 15.33 11.11 22.49 61.47 42.77 40.70 18.75 40.69 86.21 46.65 19.42 0.00 33.13 85.38 EHRAgent (Ours) 71.58 66.34 49.70 49.14 58.97 85.86 54.82 53.52 25.00 53.10 91.72 78.94 61.16 27.27 69.70 88.02 Main results of success rate (i.e., SR.) and completion rate (i.e., CR.) on MIMIC-III, eICU, and TREQS datasets.The complexity of questions increases from Level I (the simplest) to Level IV (the most difficult).</p>
<p>Kirk et al. (2024) metric is success rate, quantifying the percentage of queries the model handles successfully.FollowingXu et al. (2023);Kirk et al. (2024), we further assess completion rate, which represents the percentage of queries that the model can generate executable plans (even not yield correct results</p>
<p>Table 2
2
EHRAgent 71.58 66.34 49.70 49.14 58.97 85.86 w/o medical information 68.42 33.33 29.63 20.00 33.66 69.22 w/o long-term memory 65.96 54.46 37.13 42.74 51.73 83.42 w/o interactive coding 45.33 23.90 20.97 13.33 24.55 62.14 w/o rubber duck debugging 55.00 38.46 41.67 35.71 42.86 77.19
Complexity Level ()IIIIIIIVAllMethods () /Metrics ()SR.SR. CR.
) demonstrate the effectiveness of all four compo-</p>
<p>Table 2 :
2
Ablation studies on success rate (i.e., SR.) and completion rate (i.e., CR.) under different question complexity (I-IV) on MIMIC-III dataset.</p>
<p>Xu et al. (2023);Kirk et al. (2024)arios.Besides success rate (SR) as our main evaluation metric, we followXu et al. (2023);Kirk et al. (2024)and employ completion rate (CR) to denote the percentage of queries for which the model can generate executable plans, irrespective of whether the results are accurate.However, it is important to note that a higher CR may not necessarily imply a superior outcome, especially in clinical settings.
emergent few-shot learning capabilities of LLMs,the adaptation and generalization of EHRAgentEHRAgent leverages autonomous code generationin low-resource languages is constrained by theand execution for direct communication betweenavailability of relevant resources and training data.clinicians and EHR systems. We also improveDue to limited access to LLMs' API services andEHRAgent by interactive coding with executionconstraints related to budget and computation re-feedback, along with accumulative medical knowl-sources, our current experiments are restricted toedge, thereby effectively facilitating plan optimiza-utilizing the Microsoft Azure OpenAI API ser-tion for multi-step problem-solving. Our exper-vice with the gpt-3.5-turbo (0613) and gpt-4iments demonstrate the advantages of EHRAgent(0613) models. As part of our important futureover baseline LLM agents in autonomous codingdirections, we plan to enhance EHRAgent by in-and improved medical reasoning.corporating fine-tuned white-box LLMs, such asLLaMA-2 (Touvron et al., 2023).Limitation and Future WorkEHRAgent holds considerable potential for positive social impact in a wide range of clinical tasks and applications, including but not limited to patient cohort definition, clinical trial recruitment, case review selection, and treatment decision-making support. Despite the significant improvement in model performance, we have identified several po-tential limitations of EHRAgent as follows:. To this end, we leverage inter-active coding to learn from dynamic interactions between the planner and executor, iteratively refin-ing generated code by incorporating insights from error messages. Furthermore, EHRAgent extends beyond the limitation of short-term memory ob-tained from in-context learning, leveraging long-term memory (Sun et al., 2023; Zhang et al., 2023) by rapid retrieval of highly relevant and successfulAdditional Execution Calls. We acknowledge that when compared to open-loop systemsexperiences accumulated over time. LLM Agents for Scientific Discovery. Augment-such as CoT, Self-Consistency, Chameleon, anding LLMs with domain-specific tools, LLM agentsLLM2SQL, which generate a complete problem-have demonstrated capabilities of autonomous de-solving plan at the beginning without any adapta-sign, planning, and execution in accelerating sci-tion during execution; EHRAgent, as well as other baselines that rely on environmental feedback likeentific discovery (Wang et al., 2023a,c, 2024a; Xi et al., 2023; Zhao et al., 2023; Cheung et al.,ReAct, Reflexion, Self-Debugging, and AutoGen,2024; Gao et al., 2024), including organic synthe-require additional LLM calls due to the multi-roundsis (Bran et al., 2023), material design (Boiko et al.,conversation. However, such open-loop systems2023), and gene prioritization (Jin et al., 2024). Inall overlook environmental feedback and cannotthe medical field, MedAgents (Tang et al., 2023),adaptively refine their planning processes. Thesea multi-agent collaboration framework, leveragesshortcomings largely hinder their performance forrole-playing LLM-based agents in a task-orientedthe challenging EHR question-answering task, asmulti-round discussion for multi-choice questionsthe success rates for these methods on all threein medical entrance examinations. Similarly, Ab-EHR datasets are all below 40%. We can clearlybasian et al. (2023) develop a conversational agentobserve the trade-off between performance andto enhance LLMs using external tools for gen-execution times. Although environmental feed-eral medical question-answering tasks. Differentback enhances performance, future work will focusfrom existing LLM agents in the medical domainson cost-effective improvements to balance perfor-mance and cost (Zhang et al., 2023).that focus on improving tasks like multiple-choice question-answering, EHRAgent integrates LLMswith an interactive code interface, exploring com-Translational Clinical Research Considerations.plex few-shot tabular reasoning tasks derived fromGiven the demands for privacy, safety, and ethi-real-world EHRs through autonomous code gener-cal considerations in real-world clinical researchation and execution.and practice settings, our goal is to further ad-vance EHRAgent by mitigating biases and address-6 Conclusioning ethical implications, thereby contributing tothe development of responsible artificial intelli-gence for healthcare and medicine. Furthermore,
In this study, we develop EHRAgent, an LLM agent with external tools for few-shot multi-tabular reasoning on real-world EHRs.Empowered by the</p>
<p>Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, and Chi Wang.2023.Ecoassistant: Using llm assistant more affordably and accurately.Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang.2023.Expel: Llm agents are experiential learners.
Victor Zhong, Caiming Xiong, and Richard Socher.2017. Seq2sql: Generating structured queries fromnatural language using reinforcement learning.Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra,Victor Bursztyn, Ryan A. Rossi, Somdeb Sarkhel,and Chao Zhang. 2024. Toolchain<em>: Efficient actionspace navigation in large language models with a</em>search. In The Twelfth International Conference onLearning Representations.Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun,and Chao Zhang. 2023. Toolqa: A dataset for llmquestion answering with external tools. Advances inNeural Information Processing Systems, 36:50117-50143.A Dataset and Task Details2022),EHRSQL 7 and TREQS 8 , built upon structuredEHRs from MIMIC-III and eICU. EHRSQL andTREQS serve as text-to-SQL benchmarks for as-sessing the performance of medical QA models,specifically focusing on generating SQL queriesfor addressing a wide range of real-world questionsgathered from over 200 hospital staff. Questionswithin EHRSQL and TREQS, ranging from simpledata retrieval to complex operations such as calcula-tions, reflect the diverse and complex clinical tasksencountered by front-line healthcare professionals.Dataset statistics are available in Table 3.Dataset# Examples # Table # Row/Table # Table/QMIMIC-III5801781k2.52eICU58010152k1.74TREQS9965498k1.48Average718.710.7243.7k1.91
A.1 Task DetailsWe evaluate EHRAgent on three publicly available EHR datasets from two text-to-SQL medical question answering (QA) benchmarks(Lee et al.,</p>
<p>Table 3 :
3
Dataset statistics.</p>
<p>Table 4 :
4
Comparison of baselines and EHRAgent on the inclusion of different components.
.3 MIMIC-IIIMIMIC-III (Johnson et al., 2016) 9 covers 38,597patients and 49,785 hospital admissions informa-tion in critical care units at the Beth Israel Dea-coness Medical Center ranging from 2001 to 2012.It includes deidentified administrative informationsuch as demographics and highly granular clini-cal information, including vital signs, laboratoryresults, procedures, medications, caregiver notes,imaging reports, and mortality.A.4 eICUSimilar to MIMIC-III, eICU (Pollard et al., 2018) 10includes over 200,000 admissions from multiplecritical care units across the United States in 2014and 2015. It contains deidentified administrative in-formation following the US Health Insurance Porta-bility and Accountability Act (HIPAA) standardand structured clinical data, including vital signs,laboratory measurements, medications, treatmentplans, admission diagnoses, and medical histories.A.5 TREQSTREQS (Wang et al., 2020) is a healthcare ques-tion and answering benchmark that is built uponthe MIMIC-III (Johnson et al., 2016) dataset. InTREQS, questions are generated automatically us-ing pre-defined templates with the text-to-SQL task.Compared to the MIMIC-III dataset within theEHRSQL (Lee et al., 2022) benchmark, TREQShas a narrower focus in terms of the types of ques-tions and the complexity of SQL queries. Specifi-cally, it is restricted to only five tables but includes
(Wei et al., 2022)2022): CoT enhances the complex reasoning capabilities of original LLMs by 11 https://products.wolframalpha.com/api in AssistantAgent and UserProxyAgent within AutoGen to serve as the LLM agent and the code executor, respectively.The AssistantAgent is</p>
<p>Table 5 :
5
Comparison of baselines and EHRAgent on the selection of primary programming languages.</p>
<p>Table 7 :
7
Additional ablation studies on success rate (i.e., SR.) and completion rate (i.e., CR.) under different question complexity (I-III) on eICU dataset.
Complexity levelIIIIIIAllMetricsSR.SR. CR.EHRAgent54.82 53.52 25.00 53.10 91.72w/o medical information36.75 28.39 6.25 30.17 47.24w/o long-term memory52.41 44.22 18.75 45.69 78.97w/o interactive coding46.39 44.97 6.25 44.31 65.34w/o rubber duck debugging 50.60 46.98 12.50 47.07 70.86Complexity levelIIIIIIIVAllMetricsSR.SR. CR.EHRAgent (LTM w/ Success) 71.58 66.34 49.70 49.14 58.97 85.86LTM w/ Completion76.84 60.89 41.92 34.48 53.24 90.05w/o LTM65.96 54.46 37.13 42.74 51.73 83.42Table 8: Comparison on long-term memory (i.e., LTM)design under different question complexity (I-IV) onMIMIC-III dataset.</p>
<p>count the number of times that patient 85895 received a ph lab test last month.: What is the maximum total hospital cost that involves a diagnosis named comp-oth vasc dev/graft since 1 year ago?
a given question, the initial set of examples is pre-defined and fixed, which may not cover the specificreasoning logic or knowledge required to solve it.Using long-term memory, EHRAgent replaces origi-Original ExamplesExamples from Long-Term Memorynal few-shot demonstrations with the most relevant Knowledge: {KNOWLEDGE} successful cases from past experiences for effec-tive plan refinement. For example, none of the Solution: {SOLUTION}Question: Count the number of times that patient 52898 were prescribed ns this month. Knowledge: {KNOWLEDGE} Solution: {SOLUTION}Question: Had any tpn w/lipids been given to patient 2238 in original few-shot examples relate to either 'count their last hospital visit?Question: Count the number of times that patient 14035 had a d10w intake.Knowledge: {KNOWLEDGE} the number' scenarios or procedure knowledge; af-Solution: {SOLUTION}Knowledge: {KNOWLEDGE} Solution: {SOLUTION}ter selecting from the long-term memory pool, we Question: What was the name of the procedure that was given twoQuestion: Count the number of times that patient 99791 received aor more times to patient 58730? successfully retrieve more relevant examples, thus Knowledge: {KNOWLEDGE}op red-int fix rad/ulna procedure. Knowledge: {KNOWLEDGE}providing a similar solution logic for reference. Solution: {SOLUTION}Solution: {SOLUTION}Question: What was the last time patient 4718 had a peripheralQuestion: Count the number of times that patient 54825 received ablood lymphocytes microbiology test in the last hospital visit? H.3 Case Study of Code Interface Knowledge: {KNOWLEDGE}rt/left heart card cath procedure last year. Knowledge: {KNOWLEDGE}Figures 10 and 11 present two case studies of har-Solution: {SOLUTION}Solution: {SOLUTION}nessing LLMs as autonomous agents in a multi-turnconversation for code generation, in comparisonto a natural language-based plan such as ReAct.From the case studies, we can observe that ReActlacks a code interface, which prevents it from uti-lizing code structures for efficient action planningand tool usage. This limitation often results in alengthy context for ReAct to execute, which even-tually leads to a low completion rate.H.4 Case Study of Rubber Duck DebuggingFigure 12 showcases a case study comparing theinteractive coding process between AutoGen andEHRAgent for the same given query. When exe-cuted with error feedback, AutoGen directly sendsback the original error messages, making slightmodifications (e.g., changing the surface string ofthe arguments) without reasoning the root cause ofthe error. In contrast, EHRAgent can identify theunderlying causes of the errors through interactivecoding and debugging processes. It successfullydiscovers the underlying error causes (taking intoaccount case sensitivity), facilitating accurate coderefinement.Figure 8: Case study of medical information injectionin EHRAgent on MIMIC-III dataset. Given a questionrelated to 'aspirin ec' and 'venous cath nec', EHRAgenteffectively integrates knowledge about their potentiallocation in the database and the identifiers required toassociate them.H.2 Case Study of Long-Term MemoryFigure 9 presents a case study of updating few-shotdemonstrations from long-term memory. Due tothe constraints of limited context length, we areonly able to provide a limited number of examplesto guide EHRAgent in generating solution code. For
Question:Question</p>
<p>We include additional analysis in Appendix D to further justify the selection of primary programming language.
https://physionet.org/about/licenses/ physionet-credentialed-health-data-license-150/
https://physionet.org/news/post/ gpt-responsible-use
https://aka.ms/oai/additionalusecase
https://github.com/glee4810/EHRSQL
https://github.com/wangpinggl/TREQS
https://physionet.org/content/mimiciii/1.4/
https://physionet.org/content/eicu-crd/2.0/
https://microsoft.github.io/autogen/docs/ Use-Cases/agent_chat/
https://madnight.github.io/githut/#/pull_ requests/2023/1
https://github.com/microsoft/autogen
AcknowledgmentsWe thank the anonymous reviewers and area chairs for their valuable feedback.This research was partially supported by Accelerate Foundation Models Academic Research Initiative from Microsoft Research.This research was also partially supported by the National Science Foundation under Award Number 2319449 and Award Number 2312502, the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under Award Number K25DK135913, the Emory Global Diabetes Center of the Woodruff Sciences Center, Emory University.Question: Was the rdw value of patient 273 second measured greater than that first measured on the last hospital visit?AutoGenError: The filtering query LABEL=RDW is incorrect.... # As RDW is a laboratory test, we can find the corresponding ITEMID in the d_labitems database.d_labitems_db = LoadDB('d_labitems') filtered_d_labitems_db = FilterDB(d_labitems_db, 'LABEL=RDW') item_id = GetValue(filtered_d_labitems_db, 'ITEMID') ...There is no RDW value in the column.Five example values in the column are rbc, cd7, i, cd8, rdw.Potential Reasons: The most possible reason for the error is that the filter condition for the 'LABEL' column in the 'd_labitems' database is case-sensitive.The error message indicates that there is a 'rdw' value in the column, but not 'RDW'.Therefore, the filter condition 'LABEL="RDW"' does not match any records.The filter condition should be changed to 'LABEL="rdw"'.  4Output: The answer is 4.# Calculate the number of days that have passed since the patient was admitted to the hospital answer = Calculate('({} -{}) / 86400'.format(in_time,out_time))Figure13: A complete version of case study in Figure7showcasing interactive coding with environment feedback.Question: count the number of patients who stayed in careunit csru until 2101?
Conversational health agents: A personalized llm-powered agent framework. Iman Mahyar Abbasian, Azimi, M Amir, Ramesh Rahmani, Jain, 2023</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report</p>
<p>EHRXQA: A multi-modal question answering dataset for electronic health records with chest x-ray images. Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho, Gyubok Lee, Sunjun Kweon, Jungwoo Oh, Lei Ji, I-Chao Eric, Tackeun Chang, Edward Kim, Choi, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Hl7 fhir: An agile and restful approach to healthcare information exchange. Duane Bender, Kamran Sartipi, Proceedings of the 26th IEEE international symposium on computer-based medical systems. the 26th IEEE international symposium on computer-based medical systemsIEEE2013</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Augmenting large language models with chemistry tools. Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew Baldassari, Philippe White, Schwaller, NeurIPS 2023 AI for Science Workshop. 2023</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, Transactions on Machine Learning Research. 2023</p>
<p>POLYIE: A dataset of information extraction from polymer material scientific literature. Xinyun Chen, Maxwell Lin, Nathanael Schrli, Denny Zhou, Jerry Cheung, Yuchen Zhuang, Yinghao Li, Pranav Shetty, Wantian Zhao, Sanjeev Grampurohit, Rampi Ramprasad, Chao Zhang, 10.18653/v1/2024.naacl-long.131The Twelfth International Conference on Learning Representations. Long Papers. Mexico City, MexicoAssociation for Computational Linguistics2024. 20241Proceedings of the 2024 Conference of the North American Chapter</p>
<p>Electronic health records to facilitate clinical research. Juuso I Martin R Cowie, Lesley H Blomster, Sylvie Curtis, Ian Duclaux, Fleur Ford, Samantha Fritz, Salim Goldman, Jrg Janmohamed, Mark Kreuzer, Leenay, 10.1007/s00392-016-1025-6Clinical Research in Cardiology. 1062017</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Empowering biomedical discovery with ai agents. Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik, 2024</p>
<p>Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, ArXiv preprint, abs/2309.09971Mindagent: Emergent gaming interaction. 2023</p>
<p>The emergence of national electronic health record architectures in the united states and australia: models, costs, and questions. D Tracy, Nicolas P Gunter, Terry, Journal of medical Internet research. 71e3832005</p>
<p>Health system-scale language models are all-purpose prediction engines. Yao Lavender, Xujin Jiang, Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton, Antony Howard, Ilya Riina, Paawan Laufer, Punjabi, Nature. 2023</p>
<p>GeneGPT: augmenting large language models with domain tools for improved access to biomedical information. Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu, 10.1093/bioinformatics/btae075Bioinformatics. 402e0752024</p>
<p>Mimic-iii, a freely accessible critical care database. Tom J Alistair Ew Johnson, Lu Pollard, Li-Wei H Shen, Mengling Lehman, Mohammad Feng, Benjamin Ghassemi, Peter Moody, Leo Szolovits, Roger G Anthony Celi, Mark, Scientific data. 312016</p>
<p>Improving knowledge extraction from llms for task learning through agent analysis. Robert E James R Kirk, Peter Wray, John E Lindes, Laird, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>EHRSQL: A practical text-to-SQL benchmark for electronic health records. Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon, Woncheol Shin, Seongjun Yang, Minjoon Seo, Jong-Yeup Kim, Edward Choi, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022</p>
<p>CAMEL: Communicative agents for "mind" exploration of large language model society. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Thirty-seventh Conference on Neural Information Processing Systems. 2023a</p>
<p>Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Advances in Neural Information Processing Systems. 202436</p>
<p>API-bank: A comprehensive benchmark for tool-augmented LLMs. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, 10.18653/v1/2023.emnlp-main.187Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023bAssociation for Computational Linguistics</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Smart on fhir: a standards-based, interoperable apps platform for electronic health records. Joshua C Mandel, David A Kreda, Kenneth D Mandl, Isaac S Kohane, Rachel B Ramoni, Journal of the American Medical Informatics Association. 2352016</p>
<p>Foundation models for generalist medical artificial intelligence. Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein, Harlan M Abad, Jure Krumholz, Eric J Leskovec, Pranav Topol, Rajpurkar, Nature. 61679562023</p>
<p>On evaluating the integration of reasoning and action in llm agents with database question answering. Linyong Nan, Ellen Zhang, Weijin Zou, Yilun Zhao, Wenfei Zhou, Arman Cohan, 2023</p>
<p>G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, Gorilla: Large language model connected with massive apis. 2023. 2023OpenAIGpt-4 technical report</p>
<p>The eICU collaborative research database, a freely available multi-center database for critical care research. Tom J Pollard, E W Alistair, Jesse D Johnson, Leo A Raffa, Roger G Celi, Omar Mark, Badawi, Scientific Data. 511801782018</p>
<p>DIN-SQL: Decomposed in-context learning of textto-SQL with self-correction. Mohammadreza Pourreza, Davood Rafiei, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Making language models better tool learners with execution feedback. Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, Ningyu Zhang, abs/2305.13068ArXiv preprint. 2023</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, 2023</p>
<p>ToolLLM: Facilitating large language models to master 16000+ real-world APIs. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Zhiyuan Liu, Maosong Sun, 2024In The Twelfth International Conference on Learning Representations</p>
<ol>
<li>emrkbqa: A clinical knowledge-base question answering dataset. Jennifer J Preethi Raghavan, Diwakar Liang, Rachita Mahajan, Peter Chandra, Szolovits, Proceedings of the 20th Workshop on Biomedical Language Processing. the 20th Workshop on Biomedical Language Processing</li>
</ol>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Hugging-GPT: Solving AI tasks with chatGPT and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Adaplanner: Adaptive planning from feedback with language models. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>Medagents: Large language models as collaborators for zero-shot medical reasoning. Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, 2023Arman Cohan, and Mark Gerstein</p>
<p>Chatgpt, bard, and large language models for biomedical research: opportunities and pitfalls. Surendrabikram Thapa, Surabhi Adhikari, 10.1007/s10439-023-03284-0Annals of Biomedical Engineering. 51122023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen, A survey on large language model based autonomous agents. 2023a</p>
<p>Text-to-sql generation for question answering on electronic medical records. Ping Wang, Tian Shi, Chandan K Reddy, 10.1145/3366423.3380120Proceedings of The Web Conference 2020. The Web Conference 20202020</p>
<p>Augmenting language models with long-term memory. Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei, Thirty-seventh Conference on Neural Information Processing Systems. 2023b</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, Wei Wang, 2023c</p>
<p>Executable code actions elicit better LLM agents. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, Forty-first International Conference on Machine Learning. 2024a</p>
<p>MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023d</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou ; Gagan, Jieyu Bansal, Yiran Zhang, Beibin Wu, Erkang Li, Li Zhu, Xiaoyun Jiang, Shaokun Zhang, Jiale Zhang, Ahmed Liu, Ryen W Hassan Awadallah, Doug White, Chi Burger, Wang, Advances in Neural Information Processing Systems. Curran Associates, Inc. Qingyun Wu2022. 202335Autogen: Enabling next-gen llm applications via multi-agent conversation</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, The rise and potential of large language model based agents: A survey. 2023</p>
<p>On the tool manipulation capability of open-source large language models. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang, 2023</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. John Yang, Akshara Prabhakar, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>A large language model for electronic health records. Xi Yang, Aokun Chen, Nima Pournejatian, Chang Hoo, Kaleb E Shin, Christopher Smith, Colin Parisien, Cheryl Compas, Anthony B Martin, Mona G Costa, Flores, NPJ Digital Medicine. 511942022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023a</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev, 10.18653/v1/D18-1425Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>            </div>
        </div>

    </div>
</body>
</html>
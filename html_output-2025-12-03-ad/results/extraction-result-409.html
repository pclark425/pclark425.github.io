<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-409 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-409</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-409</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-62617257</p>
                <p><strong>Paper Title:</strong> <a href="https://web.archive.org/web/20200209062018/http:/downloads.hindawi.com/archive/2012/363840.pdf" target="_blank">A Modular System Oriented to the Design of Versatile Knowledge Bases for Chatbots</a></p>
                <p><strong>Paper Abstract:</strong> The paper illustrates a system that implements a framework, which is oriented to the development of a modular knowledge base for a conversational agent. This solution improves the flexibility of intelligent conversational agents in managing conversations. The modularity of the system grants a concurrent and synergic use of different knowledge representation techniques. According to this choice, it is possible to use the most adequate methodology for managing a conversation for a specific domain, taking into account particular features of the dialogue or the user behavior. We illustrate the implementation of a proof-of-concept prototype: a set of modules exploiting different knowledge representation methodologies and capable of managing different conversation features has been developed. Each module is automatically triggered through a component, named corpus callosum, that selects in real time the most adequate chatbot knowledge module to activate.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e409.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e409.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modular Adaptive Chatbot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Modular System Oriented to the Design of Versatile Knowledge Bases for Chatbots</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular hybrid conversational-agent architecture that combines symbolic rule/ontology/AIML modules with subsymbolic semantic spaces (LSA) and a probabilistic planner (Bayesian network) called the 'corpus callosum' to dynamically select and activate modules according to dialogue context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Modular Architecture for Adaptive ChatBots (with Corpus Callosum)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular chatbot framework that splits a monolithic AIML knowledge base into multiple specialized modules (each with metadata, AIML categories extended with external-repository queries, and an inferential engine). A Dialogue Analyzer extracts context variables (topic, speech act, user type, goal) encoded as a temporal context matrix C_t. A central coordinator called the 'corpus callosum' maps context to module activation states S_t via a learned mapping function f(C_t), implemented in the prototype as a dynamic Bayesian network planner that selects the module with highest posterior probability. Modules can query declarative repositories (ontologies, AIML, semantic-space indexed documents) and subsymbolic resources (LSA semantic space) and are enabled/disabled at run-time using thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>AIML rule categories extended with ontology queries and explicit symbolic metadata for each module; ontologies/knowledge repositories for deterministic information; the planner is a probabilistic/declarative model implemented as a (dynamic) Bayesian network representing conditional dependencies between context variables and module states.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Subsymbolic semantic space constructed by Latent Semantic Analysis (LSA) for associative, data-driven semantic retrieval (folding-in technique used to encode incoming sentences); procedural inferential engines inside modules (plugin processors) implementing procedural selection, heuristics and rule-based speech-act classifier. No neural networks are used in the reported prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular, loosely-coupled integration: symbolic and subsymbolic components live in separate modules and external repositories; the Dialogue Analyzer produces context variables that feed the corpus callosum planner which probabilistically selects modules. Modules expose special AIML tags/processors to query ontologies and semantic spaces. The planner mapping f(C_t) is implemented as a Bayesian network (dynamic temporal arcs), and module outputs are combined by the dialogue engine after activation. Integration relies on run-time module activation, thresholding of activation probabilities, and possible learning to reconfigure the mapping function when modules are added/removed.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Context-sensitive, adaptive conversational behavior arising from dynamic activation of specialized modules; improved naturalness and flexibility relative to monolithic pattern-matching bots (can change conversational stance—friendly, determined, empathetic, submissive—based on speech-act sequences); concurrent symbolic deterministic reasoning (ontologies, AIML) and associative subsymbolic inferences (LSA) enabling analogical/associative responses and topic induction; modular extensibility allowing incremental addition of reasoning capabilities without changing core; interpretable module-selection decisions via the Bayesian planner probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Interactive conversational assistance (virtual departmental 'doorman' / secretary) — topic induction, speech-act driven dialog management, selection among behavioral/functional modules; evaluated via case-study dialogues rather than standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Claimed improved generalization in practice because the semantic-space (unsupervised LSA) provides data-driven associative knowledge that augments brittle pattern-matching rules, and the modular planner enables context-based reuse of modules; no quantitative OOD or compositional generalization evaluation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability for the declarative parts: AIML categories, module metadescriptions, and Bayesian planner activations are explicit and inspectable, allowing explanation of why a module was selected; subsymbolic LSA representations are less interpretable (vector similarities) but are accessed via explicit module queries so their contributions can be traced to the module that used them.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No numerical evaluation or benchmarked metrics reported; the speech-act classifier is rule-based and simple; thresholding mechanism for continuous module activations requires tuning; learning/updating of the planner mapping f is discussed but not fully described or quantitatively validated; potential conflicts between concurrently active modules and the details of output arbitration are not deeply analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Situated in the AGI paradigm emphasizing complementary strengths and division of labor between symbolic (explicit, rule-based, deterministic) and subsymbolic (associative, data-driven) components; brain-hemisphere analogy (left: symbolic; right: intuitive/holistic) and the corpus callosum as a coordinator implementing probabilistic mapping and module selection.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e409.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e409.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACT-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACT-R (Adaptive Control of Thought—Rational)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid cognitive architecture that combines symbolic production-rule modules with subsymbolic, parallel processes that modulate and control symbolic operations, intended to model human cognition and learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ACT-R</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in related work as an example of a hybrid cognitive system combining rule-based symbolic modules and subsymbolic parallel processes; used historically to model cognition by coordinating symbolic reasoning and graded/statistical control signals.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic production rules and module-level symbolic representations (rule-based modules for memory, retrieval, and symbolic manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Subsymbolic units implemented as parallel processes that control symbolic processes (parameterized activation values, learning on strengths, etc.); not specified as neural-network-based in the paper but as subsymbolic processes.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Hybrid cognitive integration where subsymbolic processes (parallel control) influence the firing and selection of symbolic production rules; complementary division of labor between layers.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Human-like cognitive phenomena such as graded retrieval, learning-driven changes in rule selection, and interaction between fast/explicit and slow/implicit processes (as described qualitatively in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General cognitive modeling tasks (human cognitive simulations); not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed to capture human generalization phenomena via interaction of symbolic and subsymbolic components; no quantitative claims provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic rules are interpretable; subsymbolic control parameters are less so but provide mechanistic explanations of graded behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in detail in this paper beyond being cited as an example of hybrid architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Hybrid cognitive architecture combining symbolic production rules and subsymbolic control processes; explicit emphasis on complementary roles.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e409.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e409.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLARION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLARION (Connectionist Learning with Adaptive Rule Induction ONline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-representation cognitive architecture that maintains both an explicit symbolic component for declarative knowledge and a low-level subsystem for tacit (subsymbolic) knowledge, organized into multiple interacting subsystems and a metacognitive component.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CLARION</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as a hybrid architecture using dual knowledge representations: symbolic explicit knowledge for rule-like processing and a low-level component for tacit/subsymbolic skills; composed of several subsystems (action-centered, non-centered action, motivational, and metacognitive) each using the dual representation.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic component representing explicit rules and knowledge, used for explicit reasoning and decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Low-level/subsymbolic component for tacit knowledge (connectionist or statistical mechanisms) that supports implicit skills and learning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Dual-representation within each subsystem with interactions between symbolic and subsymbolic layers and a metacognitive subsystem to monitor/manage operations.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines explicit, explainable rule-based behavior with implicit, learned tacit behavior yielding flexible skill acquisition and performance that mixes deliberative and automatic responses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General cognitive modeling and learning tasks; not evaluated within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Intended to capture both explicit rule generalization and implicit learning/generalization via subsymbolic components; no empirical results provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Explicit symbolic layer affords interpretability; implicit layer is less interpretable but the architecture includes metacognitive mechanisms to monitor operations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Only cited in related work; limitations are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Dual-representation principle: explicit symbolic knowledge + implicit tacit knowledge working in concert, with metacognition to reconcile and manage them.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e409.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e409.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenCog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenCog framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated hybrid AI framework combining a probabilistic reasoning engine, an evolutionary learning engine (Moses), and a knowledge representation that encodes both declarative hypergraph representations and procedural knowledge (Combo language), augmented with different weight types (probabilistic and Hebbian) to support diverse inference mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenCog</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a prominent hybrid architecture integrating probabilistic reasoning, evolutionary learning (Moses), procedural representations (Combo functional language), and a labeled hypergraph declarative memory with probabilistic and Hebbian weights enabling combined symbolic and subsymbolic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Hypergraph knowledge base with labeled nodes/edges and multiple weight types (probabilistic weights for uncertainty, Hebbian weights as neural-attractor-like associations) representing declarative knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural knowledge expressed in the Combo functional language; evolutionary learning engine Moses for program-search and learning; probabilistic reasoning engine for uncertain inference.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight integration of declarative hypergraph, probabilistic weights, Hebbian associations, and procedural Combo-code agents; multiple inference engines (probabilistic, evolutionary, associative) interact to produce emergent intelligent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Rich, multi-modal inference capacity: probabilistic reasoning, associative activation via Hebbian weights, and programmatic procedural generation via Moses lead to complex emergent cognitive behaviors (as described qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General AGI-style tasks and agent control; referenced but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed for broad generalization across tasks via complementary mechanisms; no quantitative generalization claims or benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Declarative hypergraph is inspectable; multiple weight types provide different interpretability lenses; procedural Combo code is explainable as programs though overall system dynamics can be complex.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Mentioned as an example of hybrid architectures; specific limitations not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>AGI-oriented cognitive synergy: combine probabilistic reasoning, associative memory (Hebbian), and programmatic learning/search to leverage complementary strengths.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Artificial General Intelligence (Cognitive Technologies) <em>(Rating: 2)</em></li>
                <li>Opencogbot: achieving generally intelligent virtual agentcontrol and humanoid robotics via cognitive synergy <em>(Rating: 2)</em></li>
                <li>Using brain imaging to guide the development of a cognitive architecture <em>(Rating: 2)</em></li>
                <li>A conversational agent based on a conceptual interpretation of a data driven semantic space <em>(Rating: 2)</em></li>
                <li>Subsymbolic semantic layer in cyc for intuitive chat-bots <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-409",
    "paper_id": "paper-62617257",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "Modular Adaptive Chatbot",
            "name_full": "A Modular System Oriented to the Design of Versatile Knowledge Bases for Chatbots",
            "brief_description": "A modular hybrid conversational-agent architecture that combines symbolic rule/ontology/AIML modules with subsymbolic semantic spaces (LSA) and a probabilistic planner (Bayesian network) called the 'corpus callosum' to dynamically select and activate modules according to dialogue context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Modular Architecture for Adaptive ChatBots (with Corpus Callosum)",
            "system_description": "A modular chatbot framework that splits a monolithic AIML knowledge base into multiple specialized modules (each with metadata, AIML categories extended with external-repository queries, and an inferential engine). A Dialogue Analyzer extracts context variables (topic, speech act, user type, goal) encoded as a temporal context matrix C_t. A central coordinator called the 'corpus callosum' maps context to module activation states S_t via a learned mapping function f(C_t), implemented in the prototype as a dynamic Bayesian network planner that selects the module with highest posterior probability. Modules can query declarative repositories (ontologies, AIML, semantic-space indexed documents) and subsymbolic resources (LSA semantic space) and are enabled/disabled at run-time using thresholds.",
            "declarative_component": "AIML rule categories extended with ontology queries and explicit symbolic metadata for each module; ontologies/knowledge repositories for deterministic information; the planner is a probabilistic/declarative model implemented as a (dynamic) Bayesian network representing conditional dependencies between context variables and module states.",
            "imperative_component": "Subsymbolic semantic space constructed by Latent Semantic Analysis (LSA) for associative, data-driven semantic retrieval (folding-in technique used to encode incoming sentences); procedural inferential engines inside modules (plugin processors) implementing procedural selection, heuristics and rule-based speech-act classifier. No neural networks are used in the reported prototype.",
            "integration_method": "Modular, loosely-coupled integration: symbolic and subsymbolic components live in separate modules and external repositories; the Dialogue Analyzer produces context variables that feed the corpus callosum planner which probabilistically selects modules. Modules expose special AIML tags/processors to query ontologies and semantic spaces. The planner mapping f(C_t) is implemented as a Bayesian network (dynamic temporal arcs), and module outputs are combined by the dialogue engine after activation. Integration relies on run-time module activation, thresholding of activation probabilities, and possible learning to reconfigure the mapping function when modules are added/removed.",
            "emergent_properties": "Context-sensitive, adaptive conversational behavior arising from dynamic activation of specialized modules; improved naturalness and flexibility relative to monolithic pattern-matching bots (can change conversational stance—friendly, determined, empathetic, submissive—based on speech-act sequences); concurrent symbolic deterministic reasoning (ontologies, AIML) and associative subsymbolic inferences (LSA) enabling analogical/associative responses and topic induction; modular extensibility allowing incremental addition of reasoning capabilities without changing core; interpretable module-selection decisions via the Bayesian planner probabilities.",
            "task_or_benchmark": "Interactive conversational assistance (virtual departmental 'doorman' / secretary) — topic induction, speech-act driven dialog management, selection among behavioral/functional modules; evaluated via case-study dialogues rather than standard benchmarks.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Claimed improved generalization in practice because the semantic-space (unsupervised LSA) provides data-driven associative knowledge that augments brittle pattern-matching rules, and the modular planner enables context-based reuse of modules; no quantitative OOD or compositional generalization evaluation provided.",
            "interpretability_properties": "High interpretability for the declarative parts: AIML categories, module metadescriptions, and Bayesian planner activations are explicit and inspectable, allowing explanation of why a module was selected; subsymbolic LSA representations are less interpretable (vector similarities) but are accessed via explicit module queries so their contributions can be traced to the module that used them.",
            "limitations_or_failures": "No numerical evaluation or benchmarked metrics reported; the speech-act classifier is rule-based and simple; thresholding mechanism for continuous module activations requires tuning; learning/updating of the planner mapping f is discussed but not fully described or quantitatively validated; potential conflicts between concurrently active modules and the details of output arbitration are not deeply analyzed.",
            "theoretical_framework": "Situated in the AGI paradigm emphasizing complementary strengths and division of labor between symbolic (explicit, rule-based, deterministic) and subsymbolic (associative, data-driven) components; brain-hemisphere analogy (left: symbolic; right: intuitive/holistic) and the corpus callosum as a coordinator implementing probabilistic mapping and module selection.",
            "uuid": "e409.0"
        },
        {
            "name_short": "ACT-R",
            "name_full": "ACT-R (Adaptive Control of Thought—Rational)",
            "brief_description": "A hybrid cognitive architecture that combines symbolic production-rule modules with subsymbolic, parallel processes that modulate and control symbolic operations, intended to model human cognition and learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ACT-R",
            "system_description": "Described in related work as an example of a hybrid cognitive system combining rule-based symbolic modules and subsymbolic parallel processes; used historically to model cognition by coordinating symbolic reasoning and graded/statistical control signals.",
            "declarative_component": "Symbolic production rules and module-level symbolic representations (rule-based modules for memory, retrieval, and symbolic manipulation).",
            "imperative_component": "Subsymbolic units implemented as parallel processes that control symbolic processes (parameterized activation values, learning on strengths, etc.); not specified as neural-network-based in the paper but as subsymbolic processes.",
            "integration_method": "Hybrid cognitive integration where subsymbolic processes (parallel control) influence the firing and selection of symbolic production rules; complementary division of labor between layers.",
            "emergent_properties": "Human-like cognitive phenomena such as graded retrieval, learning-driven changes in rule selection, and interaction between fast/explicit and slow/implicit processes (as described qualitatively in related work).",
            "task_or_benchmark": "General cognitive modeling tasks (human cognitive simulations); not evaluated in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Designed to capture human generalization phenomena via interaction of symbolic and subsymbolic components; no quantitative claims provided in this paper.",
            "interpretability_properties": "Symbolic rules are interpretable; subsymbolic control parameters are less so but provide mechanistic explanations of graded behavior.",
            "limitations_or_failures": "Not discussed in detail in this paper beyond being cited as an example of hybrid architectures.",
            "theoretical_framework": "Hybrid cognitive architecture combining symbolic production rules and subsymbolic control processes; explicit emphasis on complementary roles.",
            "uuid": "e409.1"
        },
        {
            "name_short": "CLARION",
            "name_full": "CLARION (Connectionist Learning with Adaptive Rule Induction ONline)",
            "brief_description": "A dual-representation cognitive architecture that maintains both an explicit symbolic component for declarative knowledge and a low-level subsystem for tacit (subsymbolic) knowledge, organized into multiple interacting subsystems and a metacognitive component.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "CLARION",
            "system_description": "Mentioned as a hybrid architecture using dual knowledge representations: symbolic explicit knowledge for rule-like processing and a low-level component for tacit/subsymbolic skills; composed of several subsystems (action-centered, non-centered action, motivational, and metacognitive) each using the dual representation.",
            "declarative_component": "Symbolic component representing explicit rules and knowledge, used for explicit reasoning and decision-making.",
            "imperative_component": "Low-level/subsymbolic component for tacit knowledge (connectionist or statistical mechanisms) that supports implicit skills and learning.",
            "integration_method": "Dual-representation within each subsystem with interactions between symbolic and subsymbolic layers and a metacognitive subsystem to monitor/manage operations.",
            "emergent_properties": "Combines explicit, explainable rule-based behavior with implicit, learned tacit behavior yielding flexible skill acquisition and performance that mixes deliberative and automatic responses.",
            "task_or_benchmark": "General cognitive modeling and learning tasks; not evaluated within this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Intended to capture both explicit rule generalization and implicit learning/generalization via subsymbolic components; no empirical results provided here.",
            "interpretability_properties": "Explicit symbolic layer affords interpretability; implicit layer is less interpretable but the architecture includes metacognitive mechanisms to monitor operations.",
            "limitations_or_failures": "Only cited in related work; limitations are not detailed in this paper.",
            "theoretical_framework": "Dual-representation principle: explicit symbolic knowledge + implicit tacit knowledge working in concert, with metacognition to reconcile and manage them.",
            "uuid": "e409.2"
        },
        {
            "name_short": "OpenCog",
            "name_full": "OpenCog framework",
            "brief_description": "An integrated hybrid AI framework combining a probabilistic reasoning engine, an evolutionary learning engine (Moses), and a knowledge representation that encodes both declarative hypergraph representations and procedural knowledge (Combo language), augmented with different weight types (probabilistic and Hebbian) to support diverse inference mechanisms.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "OpenCog",
            "system_description": "Cited as a prominent hybrid architecture integrating probabilistic reasoning, evolutionary learning (Moses), procedural representations (Combo functional language), and a labeled hypergraph declarative memory with probabilistic and Hebbian weights enabling combined symbolic and subsymbolic inference.",
            "declarative_component": "Hypergraph knowledge base with labeled nodes/edges and multiple weight types (probabilistic weights for uncertainty, Hebbian weights as neural-attractor-like associations) representing declarative knowledge.",
            "imperative_component": "Procedural knowledge expressed in the Combo functional language; evolutionary learning engine Moses for program-search and learning; probabilistic reasoning engine for uncertain inference.",
            "integration_method": "Tight integration of declarative hypergraph, probabilistic weights, Hebbian associations, and procedural Combo-code agents; multiple inference engines (probabilistic, evolutionary, associative) interact to produce emergent intelligent behavior.",
            "emergent_properties": "Rich, multi-modal inference capacity: probabilistic reasoning, associative activation via Hebbian weights, and programmatic procedural generation via Moses lead to complex emergent cognitive behaviors (as described qualitatively).",
            "task_or_benchmark": "General AGI-style tasks and agent control; referenced but not evaluated in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Designed for broad generalization across tasks via complementary mechanisms; no quantitative generalization claims or benchmarks in this paper.",
            "interpretability_properties": "Declarative hypergraph is inspectable; multiple weight types provide different interpretability lenses; procedural Combo code is explainable as programs though overall system dynamics can be complex.",
            "limitations_or_failures": "Mentioned as an example of hybrid architectures; specific limitations not discussed here.",
            "theoretical_framework": "AGI-oriented cognitive synergy: combine probabilistic reasoning, associative memory (Hebbian), and programmatic learning/search to leverage complementary strengths.",
            "uuid": "e409.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Artificial General Intelligence (Cognitive Technologies)",
            "rating": 2,
            "sanitized_title": "artificial_general_intelligence_cognitive_technologies"
        },
        {
            "paper_title": "Opencogbot: achieving generally intelligent virtual agentcontrol and humanoid robotics via cognitive synergy",
            "rating": 2,
            "sanitized_title": "opencogbot_achieving_generally_intelligent_virtual_agentcontrol_and_humanoid_robotics_via_cognitive_synergy"
        },
        {
            "paper_title": "Using brain imaging to guide the development of a cognitive architecture",
            "rating": 2,
            "sanitized_title": "using_brain_imaging_to_guide_the_development_of_a_cognitive_architecture"
        },
        {
            "paper_title": "A conversational agent based on a conceptual interpretation of a data driven semantic space",
            "rating": 2,
            "sanitized_title": "a_conversational_agent_based_on_a_conceptual_interpretation_of_a_data_driven_semantic_space"
        },
        {
            "paper_title": "Subsymbolic semantic layer in cyc for intuitive chat-bots",
            "rating": 1,
            "sanitized_title": "subsymbolic_semantic_layer_in_cyc_for_intuitive_chatbots"
        }
    ],
    "cost": 0.0120495,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Modular System Oriented to the Design of Versatile Knowledge Bases for Chatbots</p>
<p>Giovanni Pilato 
Istituto di Calcolo e Reti ad Alte Prestazioni (ICAR)
Consiglio Nazionale delle Ricerche (ICAR)
Viale delle Scienze, Edificio 1190128PalermoItaly</p>
<p>Agnese Augello 
Dipartimento de Ingegneria Chimica
Informatica e Meccanica
Università di Palermo
Viale delle Scienze, Edificio 690128Gestionale, PalermoItaly</p>
<p>Salvatore Gaglio 
Dipartimento de Ingegneria Chimica
Informatica e Meccanica
Università di Palermo
Viale delle Scienze, Edificio 690128Gestionale, PalermoItaly</p>
<p>A Modular System Oriented to the Design of Versatile Knowledge Bases for Chatbots</p>
<p>International Scholarly Research Network ISRN Artificial Intelligence
201210.5402/2012/363840Received 9 October 2011; Accepted 22 November 2011Research Article Correspondence should be addressed to Agnese Augello, augello@dinfo.unipa.it Academic Editor: K. T. Atanassov
The paper illustrates a system that implements a framework, which is oriented to the development of a modular knowledge base for a conversational agent. This solution improves the flexibility of intelligent conversational agents in managing conversations. The modularity of the system grants a concurrent and synergic use of different knowledge representation techniques. According to this choice, it is possible to use the most adequate methodology for managing a conversation for a specific domain, taking into account particular features of the dialogue or the user behavior. We illustrate the implementation of a proof-of-concept prototype: a set of modules exploiting different knowledge representation methodologies and capable of managing different conversation features has been developed. Each module is automatically triggered through a component, named corpus callosum, that selects in real time the most adequate chatbot knowledge module to activate.</p>
<p>Introduction</p>
<p>Research on intelligent systems in last years has been characterized by the growth of the Artificial General Intelligence (AGI) paradigm [1]. This paradigm focuses attention more on learning processes than on the formalization of the domain. According to this theory, an intelligent system should not only solve a specific problem, but it should be designed as a hybrid architecture that integrates different approaches in order to emulate features of human intelligence, like flexibility and generalization capability.</p>
<p>At the same time, particular interest has been specifically addressed in the development of functional and accessible interfaces between intelligent systems and their users, in order to obtain a satisfactory man-machine interaction.</p>
<p>In this context, an ambitious goal is the creation of intelligent systems with conversational skills. The implementation of a conversational agent, however, is a complex task since it involves language understanding and dialogue management [2,3].</p>
<p>The simplest way to implement a conversational agent is to use chatbots [4], which are dialogue systems based on a pattern matching mechanism between user queries and a set of rules defined in their knowledge base.</p>
<p>In this work we show the evolution of a previously developed model of conversational agent [5,6]. The cognitive architecture evolved into a modular knowledge representation framework for the realization of smart and versatile conversational agents. A particular module, named "corpus callosum," is dedicated to dynamically switch the different modules. Moreover, it manages their mutual interaction, in order to activate different cognitive skills of the chatbot. This solution provides intelligent conversational agents with a dynamic and flexible behavior that better fits the context of the dialogue [7].</p>
<p>We have modified the ALICE (Artificial Linguistic Internet Computer Entity) [4] core and realized the implementation of a proof-of-concept chatbot prototype, which uses a set of modules exploiting different knowledge representation techniques.</p>
<p>ISRN Artificial Intelligence</p>
<p>Each module provides specific capabilities: to induce the conversation topic, to analyze the semantics of user requests, and to make semantic associations between dialogue topics.</p>
<p>The dynamic activation of the most adequate modules, capable to manage specific aspects of the conversation with the user, gives to the conversational agent more naturalness of interaction.</p>
<p>The proposed solution tries to overcome the main limits of pattern-matching-based chatbot architectures, which are based on a rigid knowledge base, time consuming to establish and maintain, and a limited dialogue engine which does not take into account the semantic content, the context, and the evolution of the dialogue.</p>
<p>The modularity of the architecture makes it possible to use in a concurrent and synergic way specific methodologies and techniques, choosing and using the most adequate methodology for a specific characteristic of the domain (e.g., an ontology to represent deterministic information, Bayesian Networks to represent uncertainty, and semantic spaces to encode subsymbolic relationships between concepts).</p>
<p>The remaining of the paper is organized as follows: Section 2 gives an overview of related works; Section 3 illustrates the modular architecture; in Section 4 a case of study is described; in Section 5 dialogue examples are reported; finally Section 6 contains the conclusions.</p>
<p>Related Work</p>
<p>Several systems oriented to the artificial general intelligence approach have been illustrated in the literature. They combine different methodologies of representation, reasoning, and learning.</p>
<p>As an example, ACT-R [8] is a hybrid cognitive system which combines rule-based modules with subsymbolic units represented by parallel processes that control many of the symbolic processes. CLARION [9] uses a dual representation of knowledge, consisting of a symbolic component to manage explicit knowledge and a low-level component to manage tacit knowledge. CLARION consists of several subsystems, each one is based on this dual representation. Subsystems are an action centered system to control actions, a noncentered action system to maintain the general knowledge, a motivational subsystem for perception cognition and action, and a metacognitive subsystem to monitor and manage the operations of other subsystems. The most significant example of hybrid architecture is the OpenCog framework [10]. It is based on a probabilistic reasoning engine and an evolutionary learning engine called Moses. These mechanisms are integrated with a representation of knowledge both in declarative and procedural form. Procedural knowledge is represented by using a functional programming language called Combo. Declarative knowledge is represented in a hypergraph labeled with different types of weights: probabilistic weights representing values of the semantic uncertainty and Hebbian weights acting as attractors of neural networks, allowing the system to make inferences about concepts which are simultaneously activated.</p>
<p>In the field of conversational agents, two different knowledge representation approaches are generally used by intelligent systems to extract and manage semantics in natural language: symbolic and subsymbolic.</p>
<p>Symbolic paradigms provide a rigorous description of the world in which the conversational agent works, exploiting ad hoc rules and grammars to make agents able to understand and generate natural language sentences. These paradigms are limited by the difficulty of defining rules and grammars that must consider all the different ways of expression of people. Subsymbolic approaches analyze text documents and chunks of conversations to infer statistic and probabilistic rules that model the language.</p>
<p>In last years we worked on the construction of a system that implements an hybrid cognitive architecture for conversational agents, going to the direction of the AGI paradigm. The cognitive architecture integrates both symbolic and subsymbolic approaches for knowledge representation and reasoning.</p>
<p>The symbolic approach is used to define the agent's background knowledge and to make it capable of reasoning about a specific domain, both in terms of deterministic and uncertain reasoning [11].</p>
<p>The subsymbolic approach, based on the creation of data-driven semantic spaces, makes the conversational agent capable to infer data-driven knowledge through machine learning techniques. This choice improves the agent competences in an unsupervised manner, and allows it to perform associative reasoning about conversation concepts [5].</p>
<p>A Modular Architecture for</p>
<p>Adaptive ChatBots</p>
<p>The illustrated system implements a framework oriented to the design and implementation of conversational agents characterized by a dynamic behavior, that is, capable to adapt their interaction with the user according to the current context of the conversation. With context we mean a set of conditions characterizing the interaction with the user, like the topic and the goal of the conversation, the profile of the user, and her speech act. The proposed work has been realized according to the AGI paradigm: a modular and easily manageable and upgradable architecture, which integrates different knowledge representation and reasoning capabilities.</p>
<p>In the specific case illustrated in this paper, the architecture integrates symbolic and subsymbolic reasoning capabilities. The system architecture is shown in Figure 1, and it is constituted by different components. The proposed architecture is quite general and it can be particularized defining specific implementations for each component: for example, it is possible to use different models of knowledge representation, to change the planning module, and to consider different kinds of context variables. Moreover, the proposed architecture is characterized by an adaptive behavior and generic adaptability. In fact, the dynamic activation of the modules makes it possible to obtain a behavior of the chatbot capable to adapt itself to the current context. The behavior depends on the modules specific functionalities and of the corpus callosum planner.</p>
<p>Dialogue Engine.</p>
<p>The dialogue engine improves the standard Alice [4] dialogue mechanism. The standard Alice dialogue engine is based on a pattern-matching approach that compares from time to time the sentences written by the user and a set of elements named "categories" defined through the AIML (Artificial Intelligence Markup Language) language. This language defines the rules for the lexical management and understanding of sentences [4]. Each category is composed of a pattern, which is compared with the user query, and a template, which constitutes the answer of the chatbot. The main drawbacks of this approach are (a) the time-expensive designing process of the AIML Knowledge Base (KB), because it is necessary to consider all the possible user requests and (b) the dialogue management mechanism, which is too rigid.</p>
<p>In previous works [5,6] many approaches have been proposed in order to overcome these disadvantages. The traditional KB has been extended through the use of ad hoc tags capable to query external knowledge repositories, like ontologies or semantic spaces, enhancing as a consequence the inferential capabilities of the chatbots. In fact, incomplete generic patterns can be defined and completed through a search of concepts related to a given topic of conversation in an ontology in order to dynamically build appropriate answers. Furthermore, the KB has been modeled, in an unsupervised manner, in a semantic space, starting form a statistical analysis of documents and dialogue chunks.</p>
<p>The contribute of the new architecture presented in this paper is to enhance the knowledge management capabilities of the chatbot both by declarative and procedural points of view. The goal is reached by splitting the traditional monolithic knowledge base of the chatbot in different components, named modules, that are perfectly suited to deal with particular characteristics of dialogue. Besides, a coordination mechanism has been provided in order to select and trigger, from time to time, the most adequate modules to manage the conversation with the user for efficiency reasons.</p>
<p>Modules.</p>
<p>Each module of the dialogue engine has its own specific features, that make it different from the other modules. For example, the differentiation can be done on functionality, on topics, on mood recognition or emulation, on specific goals to reach, on management of specific user profiles, or on a particular combination of them.</p>
<p>The trivial case is to organize specific modules for determines topics: each module is suited to deal with a particular subject and from time to time the corpus callosum evaluates which are the best modules to deal with the current state of the conversation.</p>
<p>Even if AIML has the topic tag, the proposed approach has the advantage to separate the KB of the chatbot at module level instead of AIML level and, most important, the recognition of the topic can be realized through a semantic process instead of a lexical, pattern-matching guided, approach.  We have defined a module as an ALICE extension, obtained through the insertion of specific plugins in the ALICE architecture, importing the necessary libraries for the module execution. Each module is characterized by the definition of specific AIML tags and processors to query external repositories like ontologies, linguistic dictionaries, semantic spaces, and so on. Each module (see Figure 2) is composed of (a) a metadescription (metadata that semantically characterize the module), (b) a knowledge base, composed of the standard Alice AIML categories, which can be extended with other repositories, like ontologies or semantic spaces, and (c) an inferential engine capable to retrieve information, to select the chatbot answers or to perform actions.</p>
<p>The modular knowledge base is easier to define, design, and maintain. Each module has its own inference engine whose complexity is variable and defined by the module designer. The framework is general purpose: any new module, designed according to the rules of the architecture can be connected or disconnected without affecting the source code, the core of the chatbot or the behavior of any other module. It is possible to create a module at any time in order to manage specific cognitive activities, that mix "memory-oriented elements" (modules specifically suited to manage a specific topic of the conversation) with elements oriented to specific reasoning capabilities (modules oriented to semantic retrieval of information, modules oriented to the lexical analysis of the dialogue, modules oriented at inferring concepts from an ontology-based knowledge representation, modules oriented to evaluation of decisional processes, and so on). The new tags defined in new modules can be used to reach higher complexity levels and add new reasoning capability with the aim of enhancing the interaction characteristics of the conversational agent.</p>
<p>Dialogue</p>
<p>Analyzer. This component is capable of capturing particular features related to the context of the dialogue and to manage them as variables. It can analyze the whole dialogue using syntactic, semantic, or pragmatics rules. The dialogue analyzer extracts from the current conversation what we define as "context variables." Possible variables are the topic of the dialogue, the goal of the conversation, the speech act, the mood of the user, the kind of user, the kind of dialogue (e.g., formal, informal), particular keywords, and so on.</p>
<p>Corpus Callosum.</p>
<p>The corpus callosum is equipped with a module selector and a planner. The module selector enables or disables the dialogue engine modules at runtime by selecting the most appropriate modules from time to time. Modules are disabled as soon as they are not useful. The planner uses the context variables in order to define the temporal evolution of the state of each module.</p>
<p>Let C t be the set of m context variables C jt ( j = 1, 2, . . . m) at time t; the planner maps the context representation on the module states. The state s it of the ith module at time t can be a binary value (e.g., active, not active) or a real value in [0, 1] representing its probability of activation. C t contains all the past values for each one of the variables:
C t = ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ C 1,t C 2,t . . . C m,t C 1,(t−1) C 2,(t−1) . . . C m,(t−1) . . . . . . . . . . . . C 1,0 C 2,0 . . . C m,0 ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ .(1)
The planner is characterized by the mapping function:
S t = f (C t )(2)
with S t = [s 1t , s 2t , . . . , s mt ].</p>
<p>Given the metadescription of the modules, the corpus callosum must determine the function f . The corpus callosum reconfigures the mapping function when new modules are enabled/connected or disabled/disconnected. It modifies the mapping using a learning algorithm that enhances the chatbot behavior in terms of activation/deactivation of the most fitting modules. It is possible to define a training set and an evaluation feedback mechanism of the chatbot's answers. The module selector activates or deactivates the chatbot modules, checking the value of the s it state of each module i at time t: for binary values the activation is straightforward; for continuous values it is necessary to use a thresholding mechanism that can be the same for all modules or a specific threshold for each module, or dynamic, computed from time to time according to specific constraints.</p>
<p>A Case Study</p>
<p>As a proof of concept of the proposed system, we have implemented a conversational agent oriented at assisting people, typically students, in the Computer Science Engineering Department of the University of Palermo, Italy.</p>
<p>The conversational agent plays the role of a virtual doorman, or secretary, who is also capable of showing a different behavior according to the current dialogue context. Possible users are students, professors, researchers, or other people. Requests can vary from generic information to particular questions regarding specific people.</p>
<p>In the following subsections we will describe the key implemented components. For the dialogue analyzer we will describe the extracted contextual variables, and the modality of extraction. Particular emphasis will be given to the extraction of variables like speech acts, which are a fundamental characteristic that conduct conversations.</p>
<p>Dialogue Analyzer Component.</p>
<p>In the particular implementation we have chosen to extract the following context variables: the speech act, the kind of user, the goal of the user, and the topic of the conversation. Speech acts derive from John Austin studies [12] and characterize the evolution of a conversation. According to Austin each utterance can be considered as an action of the human being in the world. Specific sequences of interaction are common in spoken language (e.g., question-answer), and they can be recognized and evaluated in order to better understand the meaning of sentences and generating the most appropriate answer to a user question. A conversation is affected also by the kind of interlocutor. As an example a simple language would be adopted to speak with a child, while a refined style is more adequate to speak to a well-educated person. Also the kind of terms between people is important during a dialogue: a conversation can be more or less formal. An agent can also have a thorough knowledge or not of a specific topic of conversation.</p>
<p>In consideration of this, we have identified four main variables that characterize the dialogue:</p>
<p>(i) Topic: it is the topic of the conversation; it can be artificial intelligence, image processing, computer languages, administration questions, and generic.</p>
<p>(ii) Speech act: the kind of speech act characterizing the dialogue at a given time; it can assume the values assertive, directive, commissive, declarative, expressive with positive, neutral, or negative connotation.</p>
<p>(iii) User: the kind of user that is interacting with the chatbot. According to the definition of the scenario users are student, professor, and other.</p>
<p>(iv) Goal: the goal of the user; it can be "looking for people" or "looking for information".</p>
<p>Topic Extraction.</p>
<p>In order to detect the topic of the conversation, we semantically compare the requests of the users with a set of documents, which have been previously classified according to the possible topics of conversation. The comparison is based on the induction of a semantic space. A semantic space is a vector model based on the principle that it is possible to know the meaning of a word analyzing its context. The building of a semantic space is usually an unsupervised process that consists of the analysis of the distribution of words in the documents corpus.</p>
<p>The result of the process is the coding of words and documents as numerical vectors, whose respective distance reflects their semantic similarity.</p>
<p>In particular, a large text corpus composed of microdocuments classified according to the possible topics of conversation has been analyzed. Each document used to create the space has been then associated with a very specific topic. A semantic space has been therefore built according to an approach based on latent semantic analysis (LSA) [13], reported in [5].</p>
<p>Given N documents of a text corpus, let M be the number of unique words occurring in the documents set. Let A = {a} im be an M × N matrix whose (i, j)th entry is the square root of the sample probability of finding the ith word in the vocabulary in the jth document. According to the Truncated Singular Value Decomposition technique, a matrix A can be approximated, given a truncation integer K &lt; min{M, N} as a matrix A k given by the product
A k = U k Σ k V T k , where U k is a column-orthonormal M × K matrix, V k is a column- orthonormal M × K matrix, and Σ k is a K × K diagonal matrix, whose elements are called singular values of A k .
After the application of LSA the corpus documents have been coded as vectors in the semantic space.</p>
<p>Let N be the number of documents used to build the semantic space, d i and s the numerical vectors associated respectively, to the ith document of the corpus and to the current sentence of conversation, and let T(d i ) be the topic associated to d i . During the conversation, to evaluate the topic associated to the current sentence, we encode it as a vector in space, by means of the folding its technique [14], obtaining the vector s, as reported below.</p>
<p>Let v be a vector representing the current sentence. The ith entry of v is the square root of the sample probability of finding the ith word in the vocabulary in the sentence, then:
x = v T U k Σ −1 k , s = U k Σ k x.(4)
Then we compare the obtained vector with all the documents encoded in the space by using an appropriate geometric similarity measure sim defined in [5]. The topic of the sentence T(s) is equal to the topic of the closer document T(d k ):
T(s) = T(d k )(5)
according to the following similarity measure [5]:
sim(s, d k ) = cos 2 (s, d k ) if cos(s, d k ) ≥ 0, 0, otherwise,(6)
where cos(s, d k ) is the cosine between the vectors s and d k .</p>
<p>Speech Acts.</p>
<p>Exploiting the support of a linguist, we have adopted two elements of the speech act theory: the illocutionary point and the illocutionary force. An illocutionary point is the basic purpose of a speaker in making an utterance. It is a component of illocutionary force. The illocutionary force of an utterance is the speaker's intention in producing that utterance. An illocutionary act is an instance of a culturally defined speech act type, characterized by a particular illocutionary force, for example, promising, advising, warning, and so forth. Recalling the Searle definition [15], we have five kinds of illocutionary points:</p>
<p>(i) assertive: to assert something;</p>
<p>(ii) directive: to commit to doing something;</p>
<p>(iii) commissive: to attempt to get someone to do something;</p>
<p>(iv) declarative: to bring about a state of affairs by the utterance;</p>
<p>(v) expressive: to express an attitude or emotion.</p>
<p>We have simplified the concept of illocutionary force introducing three kinds of act connotation:
(i) positive connotation, (ii) neutral connotation, (iii) negative connotation.
In our approach a directive act has a negative connotation when it is conducted with some sort of coercion. An explanation request conducted in a polite manner has a positive connotation; in other cases it will be labeled as a "neutral" connotation. For assertive acts we have a negative connotation when the assertion induces an adverse mood of the interlocutor, while it will be labelled as "positive" on the contrary. Commissive acts will be basically "neutral," apart from promises (positive connotation), and threats (negative connotation). Expressive acts, like wishes, have a positive connotation; greetings have a neutral connotation, and complaints have a negative connotation. Declarative acts have not been used so far, since in dialogues schema that we have considered they are substantially absent (characteristic that is reported also in the literature [16,17]).</p>
<p>This classification is not obvious and clear; therefore we have used some heuristics that associate a positive connotation to acts that cause in the agent a more favorable behavior (e.g., empathy and understanding are kinds of favorable behaviors). Illocutionary point and illocutionary force are variables of a speech act in an absolute sense, since they do not have any reference to a previous act.</p>
<p>Kind of User and Goal of the Dialogue.</p>
<p>At present these kind of variables are extracted trough ad hoc AIML categories suited to capture the information.</p>
<p>Created Modules.</p>
<p>We have realized five kinds of modules.</p>
<p>(i) Module 1: it is aimed at characterizing a friendly behavior of the chatbot.</p>
<p>(ii) Module 2: it is oriented to characterize the chatbot with though behavior as a consequence of a negative evolution of the dialogue.</p>
<p>(iii) Module 3: it makes the chatbot empathetic with the user.</p>
<p>(iv) Module 4: this module is designed to induce a situation of submissiveness of the chatbot with respect to the interlocutor.</p>
<p>(v) Module 5: it is built to make the chatbot capable to manage informative tasks.</p>
<p>Each module is activated by the corpus callosum, according to specific thresholds.</p>
<p>Corpus Callosum Implementation.</p>
<p>In this section we illustrate the corpus callosum component and its relative mapping function realized with a Bayesian network. The network is shown in Figure 3.</p>
<p>The network makes inference on the variables extracted from the context in order to trigger the activation/deactivation of a module.</p>
<p>The status of a module is directly influenced by variables such as the conversation goal, the topic and the kind of user, while it is indirectly influenced by speech acts sequences (see Figure 3).</p>
<p>Since specific sequences of speech acts can imply a mood change in the chatbot, we have defined a "chatbot-induced behavior" variable, representing the chatbot behavior, with the aim of realizing a more realistic and plausible conversational agent. Speech acts are detected through a simple rulebased speech act classifier, whose description goes beyond the scope of this paper. Once recognized, speech acts relating both to current and past time slices are used to code the current behavior of the chatbot. The corpus callosum selects therefore the most appropriate module to activate by analyzing the chatbot behavior induced by the speech acts sequence and the other context variables.</p>
<p>The module with the highest probability is then selected, according to the causal inference schema coded in a dynamic bayesian network.</p>
<p>In particular, a variable "Modules" is defined in the network, whose states represent the possible modules to activate. The probability value associated with each state will determine whether that particular module must be turned on or off. The mapping function is then obtained by evaluating the conditional probability of the variable "Modules," given its parents (see Figure 3)
S t = f (C t ) =⇒ Modules = P(Modules | Parents(Modules)) = P ModulesGoal, Topic, User, ChatbotInducedBehavior .(7)
The relationship between the variables follows the Bayes' rule:
p yx = p xy · p y p(x) .(8)
The labeled arcs are temporal arcs, for example, the arc labeled as "1" represents the influence of the variable at the former instant of time. The behavior of the chatbot is influenced by a triplet of speech acts (see (ii) the act of the chatbot at time t − 1;</p>
<p>(iii) the act of the user at time t − 1.</p>
<p>The acts sequencing is therefore given by the last three speech acts in the dialogue. Of course, it is possible to extend the temporal window to more than three steps of speech acts. We have restricted the behaviors of the chatbot to 4:</p>
<p>(i) friendly, (ii) determined, (iii) submissive, (iv) reassuring.</p>
<p>In Figure 4 we show an example of a possible sequence that induces a positive behavior in the chatbot, that can be mapped as a friendly behavior in our domain.</p>
<p>Dialogue Examples</p>
<p>In this section we show some samples of dialogues obtained during the interaction of users with a proof of concept of our prototype that we have realized at Department of Ingegneria Informatica of the University of Palermo, Italy.</p>
<p>The first example illustrates how the behavior of the chatbot changes according to different sequences of speech acts.  Table 1 shows the triplets of speech acts for the previous dialogue. Time starts from "−1" since the first triplet is characterized only by the speech act of the user. As a consequence the starting of the dialogue is in the last column at time t − 1, where an expressive act of the user is detected. With this kind of dialogue we expect the activation of Module 1, which has been designed to manage friendly communications with students and have the goal of looking for people. At step 3 it is detected the goal of the user, and at step 5 the kind of user. Figure 5 shows the evolution of the Bayesian network for each step of the dialogue. In this picture the probability of activation of each module is shown during the temporal evolution of the dialogue, which consists of 6 time slots. On the x-axis time slots are reported, while y-axis shows the probability of activation for each module. The Module 1 is the prevalent one.     In the following dialogue we show how the system detects a change of goal of the user: from a search for information, managed by Module 5, to the search for a professor, managed by Module 1. The shape of the probabilities of activation of the modules is shown in Figure 6: at time 1 the user is looking for information, and module 5 is activated; at time 3 the goal of the user becomes the search of a professor (step 6) and Module 1 becomes the most probable.</p>
<p>Conclusion</p>
<p>We have presented a system which implements a framework capable to dynamically manage knowledge in conversational agents. As a proof of concept we have illustrated a prototype, which is characterized by a Bayesian network planner. The planner is aimed at selecting, from time to time, the most adequate knowledge modules for managing specific features of the conversation with the user. As a result, the architecture is capable to generate complex behaviors of the agent. The architecture is analogous to the structure of the human brain: two hemispheres cooperate for the management and understanding of a dialogue, through a connection element like the corpus callosum. The left hemisphere is specialized in logic, reasoning, linguistic, ruleoriented, and syntactic processing of language; the right hemisphere is more oriented to intuition and emotions and processes information holistically. The corpus callosum is the bridge that connects the two hemispheres. It makes possible their mutual interaction through the migration of information between them. The dialogue analyzer extracts context information (the topic, the goal, the linguistic act, etc.) and the planner exploits these information to select and activate only the most appropriate modules, that incorporate the most adequate rules to process the specific sentences typed by the user. The conversational agent is also capable to perform analogical reasoning in order to understand the context and the structures in a general manner; the corpus callosum analyzes this information and selects the most appropriate module that is capable to properly process the specific sentence. However, it is worthwhile to point out that it is possible to realize other modules that fulfill other kind of specific functions.</p>
<p>Figure 1 :
1(i) A dialogue engine manages the interaction between the user and the chatbot. In particular it is composed of a set of modules defining the knowledge and reasoning capabilities of the chatbot. Each module is interconnected with several information repositories of declarative knowledge and defines a set of rules modeling the procedural knowledge. A component, called collector links the dialogue interface with the modules of the knowledge base. It sends the request System architecture.of the user to the currently active modules and gives her a proper answer through the interface; (ii) A dialogue analyzer extracts a set of variables that characterize the context of the conversation; (iii) A module named "corpus callosum" plans the activations and the deactivations of the dialogue engine modules according to the values of the context variables.</p>
<p>Figure 2 :
2Chatbot module.</p>
<p>Figure 3 Figure 3 :
33Bayesian planner of the corpus callosum.</p>
<p>User: Buongiorno! (Good morning!) Chatbot: Buongiorno, cosa desidera? (Good morning, may I help you?) User: Sto cercando il professore Gaglio. (I'm looking for professor Gaglio) Chatbot: Chi lo desidera? (Who are you?) User: Sono un suo studente (I'm one of his students.) Chatbot: Aspetti che verifico (Please wait, I'll check.) User: Grazie (Thank you.) Chatbot: Mi dispiace, il professore non c'e' (I apologize, the professor is not here at the moment.) User: Grazie lo stesso. (Thanks anyway.) Chatbot: Mi dispiace, arrivederci. (I am sorry, goodbye.)</p>
<p>Figure 4 :
4An example of speech acts sequence inducing a positive behavior in the chatbot.</p>
<p>Figure 5 :
5Bayesian network behavior evolution for dialogue 1.</p>
<dl>
<dt>Figure 6 :</dt>
<dt>6Bayesian network behavior evolution for dialogue 2.</dt>
<dd>
<p>Sto cercando informazioni sulle tesi. (I am looking for information about degree theses.) 3 Chatbot: Mi dica pure. (Yes, tell me please.) 4 User: Quale professore si occupa di I.A.? (Which professor deals with A.I.?) 5 Chatbot: Il professore Gaglio. (Professor Gaglio.) 6 User: Il Professore Gaglio si trova in dipartimento? (Is professor Gaglio at the Department right now?) 7 Chatbot: Chi lo desidera? (Who is looking for him?) 8. . .</p>
</dd>
</dl>
<p>Table 1 :
1Speech act triplets for dialogue 1.Tempo t 
User speech act (t) 
Chatbot speech act (t + 1) 
User speech act (t + 1) </p>
<p>−1 </p>
<p>Expressive-Neutral 
0 
Expressive-Neutral 
Expressive-Neutral 
Directive-Neutral 
1 
Directive-Neutral 
Directive-Neutral 
Assertive-Neutral 
2 
Assertive-Neutral 
Commissive-Neutral 
Expressive-Neutral 
3 
Expressive-Neutral 
Assertive-Neutral 
Expressive-Neutral 
4 
Expressive-Neutral 
Expressive-Neutral </p>
<p>AcknowledgmentsThis work has been partially supported by Italian MIUR (Ministero dell'Istruzione, dell'Universita' e della Ricerca) within the SINTESYS "Security and INTelligence SYStem" project. The authors would thank Mr. Giuseppe Miceli for his partial contribution to the experimental trials.
. B Goertzel, C Pennachin, Artificial General Intelligence (CognitiveTechnologies). SpringerB. Goertzel and C. Pennachin, Artificial General Intelligence (CognitiveTechnologies), Springer, Secaucus, NJ, USA, 2007.</p>
<p>D Jurafsky, J H Martin, Speech and Language Processing: AnIntroduction to Natural Language Processing, Computational Linguistics,and Speech Recognition. Prentice HallD. Jurafsky and J. H. Martin, Speech and Language Processing: AnIntroduction to Natural Language Processing, Computational Linguistics,and Speech Recognition, Prentice Hall, 2000, ISBN0- 13-095069-6.</p>
<p>Dialogue systems: simulations or interfaces. S Larsson, Proceedings of the 9th workshop on the semanticsand pragmatics of dialogue. Gardent and Gaiffethe 9th workshop on the semanticsand pragmatics of dialogueS. Larsson, "Dialogue systems: simulations or interfaces?" in Proceedings of the 9th workshop on the semanticsand pragmatics of dialogue, In Gardent and Gaiffe (eds.), 2005.</p>
<p>. A.l.i.c.e.chatbot. A.l.i.c.e.chatbot, http://www.alicebot.org/.</p>
<p>A conversational agent based on a conceptual interpretation of a data driven semantic space. F Agostaro, A Augello, G Pilato, G Vassallo, S Gaglio, Proceedings of the Advances in Artificial Intelligence: 9th Congress of the Italian Association for Artificial Intelligence (AIIA '05. S. Bandini and S. Manzonithe Advances in Artificial Intelligence: 9th Congress of the Italian Association for Artificial Intelligence (AIIA '05Milan, Italy3673F. Agostaro, A. Augello, G. Pilato, G. Vassallo, and S. Gaglio, "A conversational agent based on a conceptual interpretation of a data driven semantic space," in Proceedings of the Advances in Artificial Intelligence: 9th Congress of the Italian Association for Artificial Intelligence (AIIA '05), S. Bandini and S. Manzoni, Eds., vol. 3673 of Lecture Notes in Artificial Intelligence, pp. 381-392, Milan, Italy, September 2005.</p>
<p>A semantic layer onsemi-structured data sources for intuitive chatbots. A Augello, G Pilato, G Vassallo, S Gaglio, Proceedings of the 2nd International Workshop On Intelligent Interfaces For Human-Computer Interaction (IIHCI '09). the 2nd International Workshop On Intelligent Interfaces For Human-Computer Interaction (IIHCI '09)A. Augello, G. Pilato, G. Vassallo, and S. Gaglio, "A semantic layer onsemi-structured data sources for intuitive chatbots," in Proceedings of the 2nd International Workshop On Intelligent Interfaces For Human-Computer Interaction (IIHCI '09), pp. 760-765, 2009.</p>
<p>A modular architecture for adaptivechatbots. G Pilato, A Augello, S Gaglio, Proceedings of the 5th IEEE International Conference onSemantic Computing. the 5th IEEE International Conference onSemantic ComputingPaloAlto, Calif, USAStanford UniversityG. Pilato, A. Augello, and S. Gaglio, "A modular architecture for adaptivechatbots," in Proceedings of the 5th IEEE Interna- tional Conference onSemantic Computing, Stanford University, PaloAlto, Calif, USA, September 2011.</p>
<p>Using brain imaging to guide the development of acognitive architecture. J R Anderson, Integrated Models of Cognitive Systems. W. D. GrayNew York, NY, USAOxford University PressJ. R. Anderson, "Using brain imaging to guide the devel- opment of acognitive architecture," in Integrated Models of Cognitive Systems, W. D. Gray, Ed., pp. 49-62, Oxford University Press, New York, NY, USA, 2007.</p>
<p>Incubation, insight, and creative problem solving: a unified theory and a connectionist model. S Hélie, R Sun, Psychological Review. 1173S. Hélie and R. Sun, "Incubation, insight, and creative problem solving: a unified theory and a connectionist model," Psycho- logical Review, vol. 117, no. 3, pp. 994-1024, 2010.</p>
<p>Opencogbot: achieving generally intelligent virtual agentcontrol and humanoid robotics via cognitive synergy. B Goertzel, Proceedings of the (ICAI '10). the (ICAI '10)Beijing, ChinaB. Goertzel, "Opencogbot: achieving generally intelligent vir- tual agentcontrol and humanoid robotics via cognitive syn- ergy," in Proceedings of the (ICAI '10), Beijing, China, 2010.</p>
<p>Subsymbolicsemantic layer in cyc for intuitive chat-bots. G Pilato, A Augello, G Vassallo, S Gaglio, Proceedings of the 1st IEEE InternationalConference on Semantic Computing. the 1st IEEE InternationalConference on Semantic ComputingG. Pilato, A. Augello, G. Vassallo, and S. Gaglio, "Sub- symbolicsemantic layer in cyc for intuitive chat-bots," in Pro- ceedings of the 1st IEEE InternationalConference on Semantic Computing, pp. 121-128, September 2007.</p>
<p>How to do things with words. J L Austin, Harvard University PressCambridge, Mass, USAJ. L. Austin, How to do things with words, Harvard University Press, Cambridge, Mass, USA, 1975.</p>
<p>An introduction to latentsemantic analysis. T K Landauer, P Foltz, D Laham, Discourse Processes. 25T. K. Landauer, P. Foltz, and D. Laham, "An introduction to latentsemantic analysis," Discourse Processes, vol. 25, pp. 259- 284, 1998.</p>
<p>Using linear algebra for intelligent information retrieval. M W Berry, S T Dumais, G W O&apos;brien, SIAM Review. 374M. W. Berry, S. T. Dumais, and G. W. O'Brien, "Using linear algebra for intelligent information retrieval," SIAM Review, vol. 37, no. 4, pp. 573-595, 1995.</p>
<p>A taxonomy of illocutionary acts. J R Searle, Language, Mind, and Knowledge, K. Gnderson7J. R. Searle, "A taxonomy of illocutionary acts," in Language, Mind, and Knowledge, K. Gnderson, Ed., vol. 7, 1975.</p>
<p>Speech act profiling: a probabilisticmethod for analyzing persistent conversations and their participants. D Twitchell, J F Nunamaker, Proceedings of the 37th Annual Hawaii International Conference onSystem Sciences. the 37th Annual Hawaii International Conference onSystem SciencesD. Twitchell and J. F. Nunamaker, "Speech act profiling: a probabilisticmethod for analyzing persistent conversations and their participants," in Proceedings of the 37th Annual Hawaii International Conference onSystem Sciences, 2004.</p>
<p>Usingspeech act theory to model conversations for automated classificationand retrieval. D P Twitchell, M Adkins, J F Nunamaker, J , Proceedings of the 9th International Working Conferenceon the Language-Action Perspective on Communication Modeling. the 9th International Working Conferenceon the Language-Action Perspective on Communication ModelingBurgoonD. P. Twitchell, M. Adkins, J. F. Nunamaker, and J. K. Bur- goon, "Usingspeech act theory to model conversations for automated classificationand retrieval," in Proceedings of the 9th International Working Conferenceon the Language-Action Perspective on Communication Modeling, 2004.</p>            </div>
        </div>

    </div>
</body>
</html>
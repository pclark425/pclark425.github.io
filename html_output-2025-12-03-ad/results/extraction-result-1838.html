<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1838 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1838</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1838</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7" target="_blank">ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A multi-agent referee team called ChatEval is constructed to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks, offering a human-mimicking evaluation process for reliable assessments.</p>
                <p><strong>Paper Abstract:</strong> Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1838.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1838.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatEval (Multi-Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatEval multi-agent referee team (multi-agent LLM debate evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM-based evaluation framework introduced in this paper where multiple role-differentiated LLM agents debate using various communication strategies (one-by-one, simultaneous-talk, summarizer) and produce a final judgment by majority vote or averaged score; shown to improve alignment with human judgments on NLG tasks versus single-agent LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (multi-agent debate / aggregated ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 and ChatGPT (GPT-3.5-turbo) (homogeneous groups used: either all GPT-4 or all ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>pairwise comparison / numeric scoring (agents given two assistants' responses, asked to score 1-10 and discuss); multi-agent debate with role-based personas and aggregation via majority vote or mean score</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>generated text artifacts: open-ended question answers and dialogue responses</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-purpose NLG (open-ended QA and dialogue response generation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>helpfulness, relevance, accuracy, level of detail (open-ended); naturalness, coherence, engagingness, groundedness (dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Reused published human annotations as gold: (a) FairEval open-ended dataset: three human annotators per item with majority-vote aggregation; (b) Topical-Chat dialogue: human judgments from Mehri & Eskenazi (2020) used as reference</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 (open-ended / FairEval) ; not specified for Topical-Chat in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>crowd/human annotators used in referenced benchmarks (not described as expert software developers)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy, Cohen's Kappa (Kap.) on FairEval; Spearman's rho and Kendall's tau on Topical-Chat (turn-level correlations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>FairEval (open-ended): ChatGPT Multi-Agent Acc=60.0%, Kap=0.33; ChatGPT Single-Agent Acc=53.8%, Kap=0.27; GPT-4 Multi-Agent Acc=63.8%, Kap=0.40; GPT-4 Single-Agent Acc=61.3%, Kap=0.36. Topical-Chat (dialogue, average across aspects): ChatGPT MA avg Spearman=0.552, Kendall-tau=0.510; ChatGPT SA avg Spearman=0.544, Kendall-tau=0.503; GPT-4 MA avg Spearman=0.684, Kendall-tau=0.632; GPT-4 SA avg Spearman=0.658, Kendall-tau=0.611</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Diverse role prompts (distinct personas) across agents; one-by-one communication strategy (for ChatGPT in experiments); moderate number of diverse roles (performance improved up to 3-4 distinct roles); using GPT-4 as backbone yields higher correlations</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Homogeneous/same-role prompts (all agents with identical role descriptions) degraded performance and failed to beat single-agent baseline; simultaneous-talk and simultaneous-talk-with-summarizer performed worse than one-by-one in ChatGPT setting; excessive role number (e.g., 5) caused accuracy decline; increasing discussion turns did not improve and sometimes degraded performance</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not framed as software complexity, but task complexity matters: open-ended items and nuanced responses are challenging; multi-agent improved alignment more for complex, subjective text evaluation than n-gram metrics; increasing discussion turns (longer debate/context) can cause stagnation or degradation (likely due to long context effects)</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Design choices that increase evaluation perspective (diverse role prompts) improved alignment; lack of role diversity (ambiguous/identical roles) reduced agreement — indicating clarity and diversity of evaluation perspectives affects alignment</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>80 open-ended questions (FairEval set used) ; 60 dialogue contexts (Topical-Chat used)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Reported human annotator kappas on FairEval: Annotator1 Kap=0.50, Annotator2 Kap=0.62, Annotator3 Kap=0.50; paper reports average human-annotation accuracy ~71.7% (indicating some inter-individual discrepancy)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>LLM-human alignment improved with ChatEval but did not fully reach inter-human agreement: e.g., average human accuracy ~71.7% (open-ended) vs GPT-4 Multi-Agent Acc=63.8%; human kappas ~0.5-0.62 vs GPT-4 Multi-Agent Kap=0.40, indicating gap remains though ChatEval narrows it compared to single-agent</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No fine-tuning of LLMs reported; paper used prompting, role/persona design, position calibration techniques (position calibration applied in both single- and multi-agent experiments); compared to other works that apply MEC (Multiple Evidence Calibration) and BPC (Balanced Position Calibration) but ChatEval itself is a prompt/protocol-level approach rather than model fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-agent LLM debate (ChatEval) yields higher agreement with human judgments than single-agent LLM evaluations across both open-ended QA and dialogue response tasks; diverse role prompts are essential — identical-role configurations nullify benefits; one-by-one communication strategy outperformed simultaneous strategies for ChatGPT; performance increases with number of distinct roles up to a point (3-4) while more discussion turns do not help and can hurt due to context-length effects; GPT-4 multi-agent produced the best correlations (Topical-Chat average Spearman 0.684 vs GPT-4 single-agent 0.658).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Study focused on NLG/text artifacts (not software development artifacts); homogeneous-agent groups only (no heterogeneous-model agent experiments); multi-agent debate can stagnate or degrade with excessive turns or overly long context; ChatEval improves but does not fully reach inter-human agreement; dependency on expensive proprietary LLMs (GPT family) and on prompt/role design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1838.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1838.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Agent LLM-as-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-agent LLM-based evaluator (LLM-as-a-judge, single prompted model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline evaluation approach where a single LLM (GPT-4 or ChatGPT) is prompted to evaluate generated text (using the same prompt template but without chat-history/role discussion), producing a score or preference; used here as the principal baseline against multi-agent ChatEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging llm-as-a-judge with mt-bench and chatbot arena.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (single-agent numeric scoring / comparative judgment)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 and ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>single rating with provided rubric-like instructions (same template as multi-agent but without chat history / role prompts); numeric scoring (1-10) or direct preference</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>generated text: open-ended answers and dialogue responses</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general NLG (open-ended QA and dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>helpfulness, relevance, accuracy, detail; for dialogue: naturalness, coherence, engagingness, groundedness</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared against the same human-annotated benchmarks (FairEval and Topical-Chat) used in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 for FairEval open-ended (as in referenced data); unspecified for Topical-Chat within this paper</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>benchmark human annotators (not characterized as software experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy, Cohen's Kappa (FairEval); Spearman's rho and Kendall's tau (Topical-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>FairEval: ChatGPT Single-Agent Acc=53.8%, Kap=0.27; GPT-4 Single-Agent Acc=61.3%, Kap=0.36. Topical-Chat average: ChatGPT SA avg Spearman=0.544, Kendall=0.503; GPT-4 SA avg Spearman=0.658, Kendall=0.611</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Use of stronger LLMs (GPT-4 outperforms ChatGPT); applying position calibration techniques (used in experiments) helps compared to naive prompting</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>No multi-agent discussion (single perspective), lack of role diversity, simpler prompting leads to lower agreement especially for ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Single-agent methods less able to capture nuanced distinctions in open-ended/subjective items compared to multi-agent debate; no explicit software-artifact complexity analysis</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Not directly quantified, but single-agent performance benefits from prompt structure and calibration; less robust to subjective, ambiguous criteria</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>80 open-ended (FairEval); 60 dialogue contexts (Topical-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>See ChatEval entry for reported human kappas; single-agent agreement scores are lower than inter-human kappas reported (e.g., human kappas ~0.5-0.62 vs single-agent kappas 0.27-0.36)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Single-agent LLM-human agreement lags behind inter-human agreement and behind multi-agent ChatEval (e.g., ChatGPT SA Acc 53.8% vs human avg accuracy ~71.7%; GPT-4 SA 61.3% vs human ~71.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No model fine-tuning; used prompt template and position calibration; compared to external MEC/BPC techniques but single-agent baseline here was not fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Single-agent LLM-as-judge provides reasonable alignment but is outperformed by multi-agent debate (ChatEval); stronger models (GPT-4) give better alignment than weaker (ChatGPT), but still remain below average human agreement</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Single perspective can introduce bias and instability; lower accuracy and kappa versus human annotators and multi-agent setups; depends heavily on prompting and calibration heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1838.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1838.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FairEval (MEC+BPC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FairEval: calibration techniques (Multiple Evidence Calibration + Balanced Position Calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed approach (FairEval) that applies calibration strategies to improve LLM evaluation (MEC and BPC) and is used as a comparison baseline in the paper; aims to mitigate fairness/position biases in LLM-as-judge evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are not fair evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge with calibration heuristics (MEC + BPC)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Applied to ChatGPT and GPT-4 in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Calibration-based evaluation (Multiple Evidence Calibration, Balanced Position Calibration) — strategies to improve single-LM scoring</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>open-ended question answers (FairEval benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general NLG (open-ended QA)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>same as FairEval benchmark criteria used by human annotators (helpfulness/relevance/accuracy as defined in dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>FairEval uses human annotation results as gold; this paper compares FairEval's best published results with ChatEval</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 annotators per example in referenced human annotation (as used by FairEval data in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>benchmark human annotators (not specified as expert software developers)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy, Cohen's Kappa (as reported in FairEval results)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>FairEval reported (as cited in this paper): ChatGPT MEC+BPC Acc=58.7%, Kap=0.31; GPT-4 MEC+BPC Acc=62.5%, Kap=0.37</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Calibration (MEC + BPC) improves single-agent alignment compared to naive prompting</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Calibration helps but still below best multi-agent (ChatEval) results in this paper; limited by single-evaluator perspective</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Designed to handle open-ended QA but still shows lower alignment than human annotators in subjective tasks</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Calibration reduces some biases but does not substitute for multi-perspective debate when evaluation criteria are subjective or nuanced</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>80 open-ended questions (FairEval benchmark used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>See ChatEval entry; FairEval's human gold uses 3 annotators per item</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>FairEval calibration improves single-agent agreement but in this paper ChatEval multi-agent surpasses FairEval's best published numbers (e.g., GPT-4 ChatEval Acc 63.8% > FairEval GPT-4 62.5%; Kap 0.40 > 0.37).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Yes — MEC (Multiple Evidence Calibration) and BPC (Balanced Position Calibration) are calibration strategies applied to LLM outputs (no model fine-tuning reported here)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Calibration strategies (MEC+BPC) raise single-agent evaluator reliability but multi-agent debate (ChatEval) further improves alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Calibration reduces some biases but still cannot fully match multi-agent performance or inter-human agreement; efficacy depends on model strength (better with GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1838.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1838.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-EVAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-EVAL (probability-weighted LLM evaluation with chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based evaluator that prompts models to produce chain-of-thought (CoT) or intermediate reasoning and computes probability-weighted aggregated scores to improve alignment with human judgments; used as a baseline on dialogue evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpteval: Nlg evaluation using gpt-4 with better human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge with chain-of-thought and probability-weighted score aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>G-EVAL reported for GPT-3.5 (G-EVAL-3.5) and GPT-4 (G-EVAL-4) in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Chain-of-thought prompting (generate intermediate thoughts) and probability-weighted summation of output scores</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>dialogue response evaluation (Topical-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>dialogue NLG</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>naturalness, coherence, engagingness, groundedness (Topical-Chat dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Evaluated against human judgments from Topical-Chat / Mehri & Eskenazi (2020)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>Not specified in this paper for Topical-Chat reference</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>benchmark human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Turn-level Spearman's rho and Kendall's tau correlations against human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>G-EVAL-3.5 average Spearman=0.585, Kendall-tau=0.574; G-EVAL-4 average Spearman=0.588, Kendall-tau=0.575 (as cited in this paper's Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Using chain-of-thought style prompting and probability-weighted aggregation improved alignment over simple automated metrics</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Still outperformed by ChatEval (GPT-4 multi-agent) in this paper; method depends on reliable intermediate reasoning generation</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Improves alignment for dialogue aspects relative to n-gram/embedding metrics, but multi-agent debate further improves results</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Using intermediate reasoning helps capture nuanced criteria, increasing correlation with human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>60 dialogue contexts (Topical-Chat used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>G-EVAL achieves substantially higher correlations than ROUGE/BLEU/BERTScore but is slightly below GPT-4 multi-agent ChatEval results (e.g., ChatEval GPT-4 MA avg Spearman 0.684 > G-EVAL-4 0.588).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No model fine-tuning reported; method relies on CoT prompting and probability weighting</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chain-of-thought + probability-weighted scoring (G-EVAL) yields stronger alignment than traditional metrics, but multi-agent debate with GPT-4 can further raise correlations with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on quality of generated intermediate thoughts; still lags behind multi-agent debate results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1838.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1838.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional Automated Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>n-gram and embedding-based automated metrics (ROUGE, BLEU, BERTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical automatic evaluation metrics: ROUGE and BLEU (n-gram overlap) and BERTScore (embedding similarity); shown here to have low correlation with human judgments on open-ended and dialogue tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated metrics (n-gram overlap and embedding similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>generated text: summaries, answers, dialogue responses</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general NLG</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>surface overlap and embedding similarity (proxy for semantic similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared to human judgments on Topical-Chat (dialogue) dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho and Kendall's tau (Topical-Chat correlations reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Topical-Chat average (as seen in Table 2): ROUGE-L avg rho=0.244, tau=0.244; BLEU-4 avg rho=0.259, tau=0.189; BERTScore avg rho=0.274, tau=0.262 — substantially lower than LLM-based evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Perform better when surface overlap is a valid proxy (e.g., tasks with limited variation and high reference overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Open-ended generation and dialogue where acceptable responses vary widely — these metrics show poor correlation with human preference</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Agreement is low for complex, open-ended, or subjective artifacts; metrics miss semantic and pragmatic adequacy</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>They assume clear reference-based similarity; unclear or subjective criteria reduce their usefulness</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>60 dialogue contexts (Topical-Chat results reported)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Traditional metrics substantially underperform compared to LLM-based evaluators and human judgments (much lower Spearman/Kendall correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>n-gram and embedding metrics have weak correlation with human judgments on open-ended and dialogue tasks and are outperformed by LLM-based evaluators and multi-agent debate approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Cannot capture semantic adequacy, nuance, or diverse valid responses; poor for open-ended generation and domain-specific/expert tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1838.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1838.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Annotators (benchmark gold)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human annotators / human evaluation (benchmark reference gold labels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human judgments used as the gold standard in the experiments: three annotators for FairEval open-ended questions (majority vote), and human judgments from Mehri & Eskenazi for Topical-Chat dialogue; reported inter-annotator kappas and average accuracy provide a reference for LLM alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>human expert/crowd evaluation (majority vote aggregation used in FairEval), human rating scales for dialogue aspects</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>generated text artifacts: open-ended answers and dialogue responses</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general NLG</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>dataset-specific human annotation guidelines (helpfulness/relevance/accuracy; naturalness/coherence/engagingness/groundedness)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>FairEval: three annotators evaluated replies per item and majority vote produced ground truth (as used by Wu et al. 2023); Topical-Chat: human evaluators annotated 60 dialogue contexts per Mehri & Eskenazi (2020)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 (FairEval open-ended); unspecified for Topical-Chat within this paper</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Human annotators (crowd/benchmark annotators; not explicitly characterized as domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (for pairwise correctness on FairEval), Cohen's Kappa (inter-annotator reliability) reported for individual annotators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>FairEval human annotators (per-table): Annotator1 Acc=68.8%, Kap=0.50; Annotator2 Acc=76.3%, Kap=0.62; Annotator3 Acc=70.0%, Kap=0.50; paper reports average human annotation accuracy ~71.7%</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Multiple annotators and majority voting produce higher reliability; clear annotation rules in benchmark datasets</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Subjective items and open-ended generation yield discrepancies across annotators (evidenced by varying accuracies and kappas), indicating inherent difficulty in tasks</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Human agreement decreases for highly subjective or ambiguous responses; reported annotator variance reflects this</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear annotation rules and multiple annotators (majority vote) increase inter-annotator agreement</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>80 open-ended items (FairEval) ; 60 dialogue contexts (Topical-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Annotator kappas: 0.50, 0.62, 0.50 on FairEval (per-table); average human accuracy ~71.7%</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Human agreement provides an upper-bound reference; multi-agent and single-agent LLM methods approach but do not reach human consistency in these experiments (ChatEval narrows the gap).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human judgments exhibit non-negligible variance even among annotators (kappas ~0.5-0.62), underscoring that text evaluation is inherently challenging and that LLM-based evaluators should be compared to this imperfect human standard.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human evaluation is costly and time-consuming and still inconsistent for subjective tasks; benchmark human labels are not infallible and contain disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena. <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators. <em>(Rating: 2)</em></li>
                <li>Gpteval: Nlg evaluation using gpt-4 with better human alignment. <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1838",
    "paper_id": "paper-ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "ChatEval (Multi-Agent)",
            "name_full": "ChatEval multi-agent referee team (multi-agent LLM debate evaluator)",
            "brief_description": "A multi-agent LLM-based evaluation framework introduced in this paper where multiple role-differentiated LLM agents debate using various communication strategies (one-by-one, simultaneous-talk, summarizer) and produce a final judgment by majority vote or averaged score; shown to improve alignment with human judgments on NLG tasks versus single-agent LLM evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (multi-agent debate / aggregated ratings)",
            "llm_judge_model": "GPT-4 and ChatGPT (GPT-3.5-turbo) (homogeneous groups used: either all GPT-4 or all ChatGPT)",
            "llm_judge_prompt_approach": "pairwise comparison / numeric scoring (agents given two assistants' responses, asked to score 1-10 and discuss); multi-agent debate with role-based personas and aggregation via majority vote or mean score",
            "artifact_type": "generated text artifacts: open-ended question answers and dialogue responses",
            "artifact_domain": "general-purpose NLG (open-ended QA and dialogue response generation)",
            "evaluation_criteria": "helpfulness, relevance, accuracy, level of detail (open-ended); naturalness, coherence, engagingness, groundedness (dialogue)",
            "human_evaluation_setup": "Reused published human annotations as gold: (a) FairEval open-ended dataset: three human annotators per item with majority-vote aggregation; (b) Topical-Chat dialogue: human judgments from Mehri & Eskenazi (2020) used as reference",
            "human_expert_count": "3 (open-ended / FairEval) ; not specified for Topical-Chat in this paper",
            "human_expert_expertise": "crowd/human annotators used in referenced benchmarks (not described as expert software developers)",
            "agreement_metric": "Accuracy, Cohen's Kappa (Kap.) on FairEval; Spearman's rho and Kendall's tau on Topical-Chat (turn-level correlations)",
            "agreement_score": "FairEval (open-ended): ChatGPT Multi-Agent Acc=60.0%, Kap=0.33; ChatGPT Single-Agent Acc=53.8%, Kap=0.27; GPT-4 Multi-Agent Acc=63.8%, Kap=0.40; GPT-4 Single-Agent Acc=61.3%, Kap=0.36. Topical-Chat (dialogue, average across aspects): ChatGPT MA avg Spearman=0.552, Kendall-tau=0.510; ChatGPT SA avg Spearman=0.544, Kendall-tau=0.503; GPT-4 MA avg Spearman=0.684, Kendall-tau=0.632; GPT-4 SA avg Spearman=0.658, Kendall-tau=0.611",
            "high_agreement_conditions": "Diverse role prompts (distinct personas) across agents; one-by-one communication strategy (for ChatGPT in experiments); moderate number of diverse roles (performance improved up to 3-4 distinct roles); using GPT-4 as backbone yields higher correlations",
            "low_agreement_conditions": "Homogeneous/same-role prompts (all agents with identical role descriptions) degraded performance and failed to beat single-agent baseline; simultaneous-talk and simultaneous-talk-with-summarizer performed worse than one-by-one in ChatGPT setting; excessive role number (e.g., 5) caused accuracy decline; increasing discussion turns did not improve and sometimes degraded performance",
            "artifact_complexity_effect": "Not framed as software complexity, but task complexity matters: open-ended items and nuanced responses are challenging; multi-agent improved alignment more for complex, subjective text evaluation than n-gram metrics; increasing discussion turns (longer debate/context) can cause stagnation or degradation (likely due to long context effects)",
            "criteria_clarity_effect": "Design choices that increase evaluation perspective (diverse role prompts) improved alignment; lack of role diversity (ambiguous/identical roles) reduced agreement — indicating clarity and diversity of evaluation perspectives affects alignment",
            "sample_size": "80 open-ended questions (FairEval set used) ; 60 dialogue contexts (Topical-Chat used)",
            "inter_human_agreement": "Reported human annotator kappas on FairEval: Annotator1 Kap=0.50, Annotator2 Kap=0.62, Annotator3 Kap=0.50; paper reports average human-annotation accuracy ~71.7% (indicating some inter-individual discrepancy)",
            "proxy_vs_human_comparison": "LLM-human alignment improved with ChatEval but did not fully reach inter-human agreement: e.g., average human accuracy ~71.7% (open-ended) vs GPT-4 Multi-Agent Acc=63.8%; human kappas ~0.5-0.62 vs GPT-4 Multi-Agent Kap=0.40, indicating gap remains though ChatEval narrows it compared to single-agent",
            "calibration_or_training": "No fine-tuning of LLMs reported; paper used prompting, role/persona design, position calibration techniques (position calibration applied in both single- and multi-agent experiments); compared to other works that apply MEC (Multiple Evidence Calibration) and BPC (Balanced Position Calibration) but ChatEval itself is a prompt/protocol-level approach rather than model fine-tuning",
            "key_findings": "Multi-agent LLM debate (ChatEval) yields higher agreement with human judgments than single-agent LLM evaluations across both open-ended QA and dialogue response tasks; diverse role prompts are essential — identical-role configurations nullify benefits; one-by-one communication strategy outperformed simultaneous strategies for ChatGPT; performance increases with number of distinct roles up to a point (3-4) while more discussion turns do not help and can hurt due to context-length effects; GPT-4 multi-agent produced the best correlations (Topical-Chat average Spearman 0.684 vs GPT-4 single-agent 0.658).",
            "limitations_noted": "Study focused on NLG/text artifacts (not software development artifacts); homogeneous-agent groups only (no heterogeneous-model agent experiments); multi-agent debate can stagnate or degrade with excessive turns or overly long context; ChatEval improves but does not fully reach inter-human agreement; dependency on expensive proprietary LLMs (GPT family) and on prompt/role design choices.",
            "uuid": "e1838.0",
            "source_info": {
                "paper_title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Single-Agent LLM-as-Judge",
            "name_full": "Single-agent LLM-based evaluator (LLM-as-a-judge, single prompted model)",
            "brief_description": "A baseline evaluation approach where a single LLM (GPT-4 or ChatGPT) is prompted to evaluate generated text (using the same prompt template but without chat-history/role discussion), producing a score or preference; used here as the principal baseline against multi-agent ChatEval.",
            "citation_title": "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (single-agent numeric scoring / comparative judgment)",
            "llm_judge_model": "GPT-4 and ChatGPT (GPT-3.5-turbo)",
            "llm_judge_prompt_approach": "single rating with provided rubric-like instructions (same template as multi-agent but without chat history / role prompts); numeric scoring (1-10) or direct preference",
            "artifact_type": "generated text: open-ended answers and dialogue responses",
            "artifact_domain": "general NLG (open-ended QA and dialogue)",
            "evaluation_criteria": "helpfulness, relevance, accuracy, detail; for dialogue: naturalness, coherence, engagingness, groundedness",
            "human_evaluation_setup": "Compared against the same human-annotated benchmarks (FairEval and Topical-Chat) used in the paper",
            "human_expert_count": "3 for FairEval open-ended (as in referenced data); unspecified for Topical-Chat within this paper",
            "human_expert_expertise": "benchmark human annotators (not characterized as software experts)",
            "agreement_metric": "Accuracy, Cohen's Kappa (FairEval); Spearman's rho and Kendall's tau (Topical-Chat)",
            "agreement_score": "FairEval: ChatGPT Single-Agent Acc=53.8%, Kap=0.27; GPT-4 Single-Agent Acc=61.3%, Kap=0.36. Topical-Chat average: ChatGPT SA avg Spearman=0.544, Kendall=0.503; GPT-4 SA avg Spearman=0.658, Kendall=0.611",
            "high_agreement_conditions": "Use of stronger LLMs (GPT-4 outperforms ChatGPT); applying position calibration techniques (used in experiments) helps compared to naive prompting",
            "low_agreement_conditions": "No multi-agent discussion (single perspective), lack of role diversity, simpler prompting leads to lower agreement especially for ChatGPT",
            "artifact_complexity_effect": "Single-agent methods less able to capture nuanced distinctions in open-ended/subjective items compared to multi-agent debate; no explicit software-artifact complexity analysis",
            "criteria_clarity_effect": "Not directly quantified, but single-agent performance benefits from prompt structure and calibration; less robust to subjective, ambiguous criteria",
            "sample_size": "80 open-ended (FairEval); 60 dialogue contexts (Topical-Chat)",
            "inter_human_agreement": "See ChatEval entry for reported human kappas; single-agent agreement scores are lower than inter-human kappas reported (e.g., human kappas ~0.5-0.62 vs single-agent kappas 0.27-0.36)",
            "proxy_vs_human_comparison": "Single-agent LLM-human agreement lags behind inter-human agreement and behind multi-agent ChatEval (e.g., ChatGPT SA Acc 53.8% vs human avg accuracy ~71.7%; GPT-4 SA 61.3% vs human ~71.7%).",
            "calibration_or_training": "No model fine-tuning; used prompt template and position calibration; compared to external MEC/BPC techniques but single-agent baseline here was not fine-tuned",
            "key_findings": "Single-agent LLM-as-judge provides reasonable alignment but is outperformed by multi-agent debate (ChatEval); stronger models (GPT-4) give better alignment than weaker (ChatGPT), but still remain below average human agreement",
            "limitations_noted": "Single perspective can introduce bias and instability; lower accuracy and kappa versus human annotators and multi-agent setups; depends heavily on prompting and calibration heuristics.",
            "uuid": "e1838.1",
            "source_info": {
                "paper_title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "FairEval (MEC+BPC)",
            "name_full": "FairEval: calibration techniques (Multiple Evidence Calibration + Balanced Position Calibration)",
            "brief_description": "A recently proposed approach (FairEval) that applies calibration strategies to improve LLM evaluation (MEC and BPC) and is used as a comparison baseline in the paper; aims to mitigate fairness/position biases in LLM-as-judge evaluations.",
            "citation_title": "Large language models are not fair evaluators.",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge with calibration heuristics (MEC + BPC)",
            "llm_judge_model": "Applied to ChatGPT and GPT-4 in comparisons",
            "llm_judge_prompt_approach": "Calibration-based evaluation (Multiple Evidence Calibration, Balanced Position Calibration) — strategies to improve single-LM scoring",
            "artifact_type": "open-ended question answers (FairEval benchmark)",
            "artifact_domain": "general NLG (open-ended QA)",
            "evaluation_criteria": "same as FairEval benchmark criteria used by human annotators (helpfulness/relevance/accuracy as defined in dataset)",
            "human_evaluation_setup": "FairEval uses human annotation results as gold; this paper compares FairEval's best published results with ChatEval",
            "human_expert_count": "3 annotators per example in referenced human annotation (as used by FairEval data in experiments)",
            "human_expert_expertise": "benchmark human annotators (not specified as expert software developers)",
            "agreement_metric": "Accuracy, Cohen's Kappa (as reported in FairEval results)",
            "agreement_score": "FairEval reported (as cited in this paper): ChatGPT MEC+BPC Acc=58.7%, Kap=0.31; GPT-4 MEC+BPC Acc=62.5%, Kap=0.37",
            "high_agreement_conditions": "Calibration (MEC + BPC) improves single-agent alignment compared to naive prompting",
            "low_agreement_conditions": "Calibration helps but still below best multi-agent (ChatEval) results in this paper; limited by single-evaluator perspective",
            "artifact_complexity_effect": "Designed to handle open-ended QA but still shows lower alignment than human annotators in subjective tasks",
            "criteria_clarity_effect": "Calibration reduces some biases but does not substitute for multi-perspective debate when evaluation criteria are subjective or nuanced",
            "sample_size": "80 open-ended questions (FairEval benchmark used in this paper)",
            "inter_human_agreement": "See ChatEval entry; FairEval's human gold uses 3 annotators per item",
            "proxy_vs_human_comparison": "FairEval calibration improves single-agent agreement but in this paper ChatEval multi-agent surpasses FairEval's best published numbers (e.g., GPT-4 ChatEval Acc 63.8% &gt; FairEval GPT-4 62.5%; Kap 0.40 &gt; 0.37).",
            "calibration_or_training": "Yes — MEC (Multiple Evidence Calibration) and BPC (Balanced Position Calibration) are calibration strategies applied to LLM outputs (no model fine-tuning reported here)",
            "key_findings": "Calibration strategies (MEC+BPC) raise single-agent evaluator reliability but multi-agent debate (ChatEval) further improves alignment with human judgments.",
            "limitations_noted": "Calibration reduces some biases but still cannot fully match multi-agent performance or inter-human agreement; efficacy depends on model strength (better with GPT-4).",
            "uuid": "e1838.2",
            "source_info": {
                "paper_title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "G-EVAL",
            "name_full": "G-EVAL (probability-weighted LLM evaluation with chain-of-thought)",
            "brief_description": "An LLM-based evaluator that prompts models to produce chain-of-thought (CoT) or intermediate reasoning and computes probability-weighted aggregated scores to improve alignment with human judgments; used as a baseline on dialogue evaluation in this paper.",
            "citation_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment.",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge with chain-of-thought and probability-weighted score aggregation",
            "llm_judge_model": "G-EVAL reported for GPT-3.5 (G-EVAL-3.5) and GPT-4 (G-EVAL-4) in comparisons",
            "llm_judge_prompt_approach": "Chain-of-thought prompting (generate intermediate thoughts) and probability-weighted summation of output scores",
            "artifact_type": "dialogue response evaluation (Topical-Chat)",
            "artifact_domain": "dialogue NLG",
            "evaluation_criteria": "naturalness, coherence, engagingness, groundedness (Topical-Chat dimensions)",
            "human_evaluation_setup": "Evaluated against human judgments from Topical-Chat / Mehri & Eskenazi (2020)",
            "human_expert_count": "Not specified in this paper for Topical-Chat reference",
            "human_expert_expertise": "benchmark human annotators",
            "agreement_metric": "Turn-level Spearman's rho and Kendall's tau correlations against human judgments",
            "agreement_score": "G-EVAL-3.5 average Spearman=0.585, Kendall-tau=0.574; G-EVAL-4 average Spearman=0.588, Kendall-tau=0.575 (as cited in this paper's Table 2)",
            "high_agreement_conditions": "Using chain-of-thought style prompting and probability-weighted aggregation improved alignment over simple automated metrics",
            "low_agreement_conditions": "Still outperformed by ChatEval (GPT-4 multi-agent) in this paper; method depends on reliable intermediate reasoning generation",
            "artifact_complexity_effect": "Improves alignment for dialogue aspects relative to n-gram/embedding metrics, but multi-agent debate further improves results",
            "criteria_clarity_effect": "Using intermediate reasoning helps capture nuanced criteria, increasing correlation with human judgments",
            "sample_size": "60 dialogue contexts (Topical-Chat used in experiments)",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "G-EVAL achieves substantially higher correlations than ROUGE/BLEU/BERTScore but is slightly below GPT-4 multi-agent ChatEval results (e.g., ChatEval GPT-4 MA avg Spearman 0.684 &gt; G-EVAL-4 0.588).",
            "calibration_or_training": "No model fine-tuning reported; method relies on CoT prompting and probability weighting",
            "key_findings": "Chain-of-thought + probability-weighted scoring (G-EVAL) yields stronger alignment than traditional metrics, but multi-agent debate with GPT-4 can further raise correlations with human judgments.",
            "limitations_noted": "Relies on quality of generated intermediate thoughts; still lags behind multi-agent debate results in this paper.",
            "uuid": "e1838.3",
            "source_info": {
                "paper_title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Traditional Automated Metrics",
            "name_full": "n-gram and embedding-based automated metrics (ROUGE, BLEU, BERTScore)",
            "brief_description": "Classical automatic evaluation metrics: ROUGE and BLEU (n-gram overlap) and BERTScore (embedding similarity); shown here to have low correlation with human judgments on open-ended and dialogue tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "automated metrics (n-gram overlap and embedding similarity)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "generated text: summaries, answers, dialogue responses",
            "artifact_domain": "general NLG",
            "evaluation_criteria": "surface overlap and embedding similarity (proxy for semantic similarity)",
            "human_evaluation_setup": "Compared to human judgments on Topical-Chat (dialogue) dimensions",
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": "Spearman's rho and Kendall's tau (Topical-Chat correlations reported)",
            "agreement_score": "Topical-Chat average (as seen in Table 2): ROUGE-L avg rho=0.244, tau=0.244; BLEU-4 avg rho=0.259, tau=0.189; BERTScore avg rho=0.274, tau=0.262 — substantially lower than LLM-based evaluators",
            "high_agreement_conditions": "Perform better when surface overlap is a valid proxy (e.g., tasks with limited variation and high reference overlap)",
            "low_agreement_conditions": "Open-ended generation and dialogue where acceptable responses vary widely — these metrics show poor correlation with human preference",
            "artifact_complexity_effect": "Agreement is low for complex, open-ended, or subjective artifacts; metrics miss semantic and pragmatic adequacy",
            "criteria_clarity_effect": "They assume clear reference-based similarity; unclear or subjective criteria reduce their usefulness",
            "sample_size": "60 dialogue contexts (Topical-Chat results reported)",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Traditional metrics substantially underperform compared to LLM-based evaluators and human judgments (much lower Spearman/Kendall correlations).",
            "calibration_or_training": null,
            "key_findings": "n-gram and embedding metrics have weak correlation with human judgments on open-ended and dialogue tasks and are outperformed by LLM-based evaluators and multi-agent debate approaches.",
            "limitations_noted": "Cannot capture semantic adequacy, nuance, or diverse valid responses; poor for open-ended generation and domain-specific/expert tasks.",
            "uuid": "e1838.4",
            "source_info": {
                "paper_title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Human Annotators (benchmark gold)",
            "name_full": "Human annotators / human evaluation (benchmark reference gold labels)",
            "brief_description": "Human judgments used as the gold standard in the experiments: three annotators for FairEval open-ended questions (majority vote), and human judgments from Mehri & Eskenazi for Topical-Chat dialogue; reported inter-annotator kappas and average accuracy provide a reference for LLM alignment.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "human expert/crowd evaluation (majority vote aggregation used in FairEval), human rating scales for dialogue aspects",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "generated text artifacts: open-ended answers and dialogue responses",
            "artifact_domain": "general NLG",
            "evaluation_criteria": "dataset-specific human annotation guidelines (helpfulness/relevance/accuracy; naturalness/coherence/engagingness/groundedness)",
            "human_evaluation_setup": "FairEval: three annotators evaluated replies per item and majority vote produced ground truth (as used by Wu et al. 2023); Topical-Chat: human evaluators annotated 60 dialogue contexts per Mehri & Eskenazi (2020)",
            "human_expert_count": "3 (FairEval open-ended); unspecified for Topical-Chat within this paper",
            "human_expert_expertise": "Human annotators (crowd/benchmark annotators; not explicitly characterized as domain experts)",
            "agreement_metric": "Accuracy (for pairwise correctness on FairEval), Cohen's Kappa (inter-annotator reliability) reported for individual annotators",
            "agreement_score": "FairEval human annotators (per-table): Annotator1 Acc=68.8%, Kap=0.50; Annotator2 Acc=76.3%, Kap=0.62; Annotator3 Acc=70.0%, Kap=0.50; paper reports average human annotation accuracy ~71.7%",
            "high_agreement_conditions": "Multiple annotators and majority voting produce higher reliability; clear annotation rules in benchmark datasets",
            "low_agreement_conditions": "Subjective items and open-ended generation yield discrepancies across annotators (evidenced by varying accuracies and kappas), indicating inherent difficulty in tasks",
            "artifact_complexity_effect": "Human agreement decreases for highly subjective or ambiguous responses; reported annotator variance reflects this",
            "criteria_clarity_effect": "Clear annotation rules and multiple annotators (majority vote) increase inter-annotator agreement",
            "sample_size": "80 open-ended items (FairEval) ; 60 dialogue contexts (Topical-Chat)",
            "inter_human_agreement": "Annotator kappas: 0.50, 0.62, 0.50 on FairEval (per-table); average human accuracy ~71.7%",
            "proxy_vs_human_comparison": "Human agreement provides an upper-bound reference; multi-agent and single-agent LLM methods approach but do not reach human consistency in these experiments (ChatEval narrows the gap).",
            "calibration_or_training": null,
            "key_findings": "Human judgments exhibit non-negligible variance even among annotators (kappas ~0.5-0.62), underscoring that text evaluation is inherently challenging and that LLM-based evaluators should be compared to this imperfect human standard.",
            "limitations_noted": "Human evaluation is costly and time-consuming and still inconsistent for subjective tasks; benchmark human labels are not infallible and contain disagreement.",
            "uuid": "e1838.5",
            "source_info": {
                "paper_title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators.",
            "rating": 2
        },
        {
            "paper_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment.",
            "rating": 2
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 1
        }
    ],
    "cost": 0.019227499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatEval: Towards better LLM-based EVALUATORS THROUGH MULTI-AGENT DEBATE</h1>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Zhiyuan Liu*<br>Department of Computer Science and Technology<br>Tsinghua University<br>zorowin123@gmail.com</p>
<p>Jie Fu, Wei Xue
Hong Kong University of Science and Technology</p>
<p>Shanghai Zhang
Peking University</p>
<h4>Abstract</h4>
<p>Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agentbased approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. We derive insights and lessons from practical scenarios where humans instigate group discussions for brainstorming and propose different communication strategies within ChatEval. Our experiments on two benchmark tasks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompt can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a humanmimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.</p>
<h2>1 INTRODUCTION</h2>
<p>Evaluating the quality of text generated by language models or written by humans has long been a challenging endeavor, consistently garnering substantial attention (Celikyilmaz et al., 2020). Traditional methodologies predominantly rely on human annotation of texts (Callison-Burch, 2009), an approach considered overly demanding in terms of time and cost. Automatic evaluation metrics based on n-grams, such as Rouge (Lin, 2004), BLEU (Papineni et al., 2002), and METEOR (Banerjee \&amp; Lavie, 2005), have been proposed to tackle this issue (Kondrak, 2005). However, these methods have been shown to exhibit a relatively weak correlation with human judgments, particularly in the context of tasks involving open-ended generation or requiring domain-specific expertise (Novikova et al., 2017).</p>
<p>Recent advancements in the field of natural language processing have led to the emergence of billion-parameter scale LLMs, such as GPT-3 (Brown et al., 2020). These LLMs have demon-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>strated remarkable capabilities across diverse downstream tasks, presenting new opportunities for text quality evaluation using such models. Moreover, various training paradigms have been proposed to endow LLMs with the ability to accomplish tasks in a zero-shot manner and better adhere to human-provided instructions (Ouyang et al., 2022; Sanh et al., 2021; Wei et al., 2021). These advancements facilitate the prompting of LLMs to evaluate generated text, effectively simulating human evaluators in the assessment process.</p>
<p>In view of the impressive text understanding and instruction-following capabilities of recent LLMs, a body of literature (Liu et al., 2023b; Chiang \&amp; Lee, 2023; Gao et al., 2023; Shen et al., 2023) has adopted LLM as an evaluator to assess the quality of responses to open-ended questions or traditional NLG tasks, including dialogue response generation and summarization. This methodology is dubbed LLM-as-a-judge (Zheng et al., 2023). Findings from these researches indicate that LLM can mimic human behavior and provide evaluations that correspond with human judgments, revealing a potentially scalable and transparent alternative to costly and laborious human evaluations.</p>
<p>While a single powerful LLM can already tackle various missions, emerging studies suggest that multiple LLMs can further improve one another through debate and cooperation (Li et al., 2023a; Liang et al., 2023). By incorporating multiple LLMs into an integrated group and designing specific interaction mechanisms, different LLMs can engage in proposing and deliberating unique responses and thought processes across several rounds. This approach leads to enhanced factuality of generated responses (Du et al., 2023) and improvement in the completion of arduous tasks (Li et al., 2023a; Qian et al., 2023). Furthermore, the multi-agent group also addresses and mitigates the Degeneration-of-Thought (DOT) problem (Liang et al., 2023).</p>
<p>In the human evaluation processes, relying on a single perspective can introduce bias and instability in the results (Karpinska et al., 2021). Recognizing this, best practices often involve multiple human annotators collaborating in the evaluation (Van Der Lee et al., 2019). Drawing inspiration from this collaborative and iterative human evaluation approach, we propose ChatEval, a system that enables each agent to employ varied communication strategies in collaborative discussion, working towards formulating final judgments. Furthermore, to enrich the evaluation dynamics, every agent within ChatEval is endowed with a unique persona. This deliberate design ensures that each agent focuses on distinct perspectives or brings specific expertise to the table. By doing so, the collective evaluation benefits from a more comprehensive lens, capturing nuances and subtleties that a single perspective might overlook. We derive this idea primarily from the insight of 'There are a thousand Hamlets in a thousand people's eyes', meaning that every person has their unique interpretation or perspective, especially applicable to text evaluation. Indeed, these divergent perspectives shape the comprehensive and multifaceted assessment of Hamlet. Another underlying intuition of our work stems from renowned concepts in sociology and biology, including Collective Intelligence(Woolley et al., 2010) and Cognitive Synergy(Luppi et al., 2022), where multiple cognitive processes or systems interact and cooperate in a way that produces a combined effect greater than the sum of their separate effects.</p>
<p>To summarize, the main contribution of our work is as follows:</p>
<ol>
<li>We propose a multi-agent-based framework called ChatEval that aligns better with human preferences compared with single-agent-based approaches as depicted in Figure 1.</li>
<li>We propose various communication strategies and demonstrate the necessity of diverse role prompts in multi-agent debate scenarios.</li>
<li>We release our library. It's designed to be both composable and scalable, enabling researchers to implement their unique communication strategies easily. We hope this contributes to advancing research in the field of communicative agents and beyond.</li>
</ol>
<h1>2 Methodology</h1>
<p>In this section, we elaborate on the principal components in ChatEval including debater agents, diverse role specification, communication strategy, and provide a detailed overview of each component's role and functionality ${ }^{1}$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: When several referees participate in the evaluation process, they can discuss with each other and finally give a judgment that is better aligned with human annotators.</p>
<p>Debater Agents. Debater agents are one of the most significant components in our framework. We treat each individual LLM as an agent and ask them to generate their response from the given prompt ${ }^{2}$. Responses from other agents are served as chat history which will be replaced in the prompt template. After configuring the agents, we then start the group debate where each agent autonomously receives responses from the others and, in turn, delivers its own responses to them. It should be noted that the whole process does not require human intervention.</p>
<p>Diverse Role Specification. As presented in Section 1, diverse role specification is necessary for the framework as well. Although all the agents share a common prompt template, we substitute the role_description slot with diverse role prompts, specifying distinct personalities for different agents. We take inspiration from Wu et al. (2023) and formulate an analogous role description.</p>
<p>Communication Strategy. How to maintain the chat history is another significant issue in ChatEval. In our work, we use a more intuitive term to illustrate the maintenance of the chat history called communication strategy. In a nutshell, different communication strategies can be seen as different approaches to maintaining and manipulating their chat history. As is shown in Figure 2, We primarily design three different communication strategies and illustrate them as follows:</p>
<ol>
<li>One-By-One. During each round of the debate, the debater agents take turns in a set order to generate their response based on the current observation. When it's time for a debater agent to respond, we directly concatenate what previous other agents have said into its chat history slot.</li>
<li>Simultaneous-Talk. Unlike the one-by-one strategy, we carry out an alternative communication strategy called simultaneous-talk, where debater agents are prompted to asynchronously generate responses in each iteration of the discussion to nullify the impact of the speaking order.</li>
<li>Simultaneous-Talk-with-Summarizer. The main difference between this strategy and simultaneous-talk is that we additionally employ another LLM as a summarizer. At the end of each iteration of the debate, we prompt this extra LLM to summarize the messages conveyed so far and concatenate this summarization into all debater agents' chat history slots.
<sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The overall schematic diagram of our proposed three different kinds of communication strategy. The direction of the arrows represents the flow of information, meaning that what this person says will be appended to the chat history of the person pointed to by the arrow. Full algorithm description of the above communication strategies can be found in Appendix B.</p>
<p>Unlike previous work like Du et al. (2023), we do not explicitly ask the debater agents to reach a consensus at the end of the debate. In situations where the response format relies on direct comparison, we derive the final results from the majority vote among various annotators. Conversely, if the response format requires a direct score, we calculate the average score obtained from multiple annotators. This methodological approach ensures the impartiality and balance of our evaluation process.</p>
<h1>3 EXPERIMENTS</h1>
<p>We evaluate ChatEval on two benchmarks, FairEval and Topical-Chat which represent the categories of open-ended question answer and dialogue response generation, respectively.</p>
<h3>3.1 IMPLEMENTATION DETAILS</h3>
<p>We choose to utilize models from OpenAI's GPT family as our LLMs in ChatEval, including GPT-4 and ChatGPT (GPT-3.5-turbo) and set the temperature to 0 to ensure reproducibility. The rationale behind this selection is the exceptional performance these models offer, being among the most advanced and powerful in the world. Additionally, their accessibility and ease of use through APIs enable us to directly call and interact with the models during our research, significantly simplifying the process. In our current research, we focus on homogeneous groups of LLMs. That is, within a given multi-agent group, all LLMs belong to the same GPT family model, either all GPT-4 or all ChatGPT. We acknowledge the potential of heterogeneous groups for future research, which could provide fascinating insights into how strong models and weak models can cooperate in a multi-agent setting.</p>
<h3>3.2 BENCHMARKS</h3>
<p>The detailed introduction of different categories and benchmarks are listed as follows:
Open-ended Question Answer is a key component within the field of NLP and generative AI. It necessitates an AI system to provide comprehensive, detailed, and human-like responses to questions that don't have a predefined or fixed set of possible answers. The work of Chiang et al. (2023) encompasses a collection of 80 open-ended questions originating from a wide array of categories, including common-sense, counterfactual, coding, etc. We then take the human annotation results from Wu et al. (2023) to conduct the experiments in this paper. For each question, they direct three annotators to evaluate the replies given by Vicuna-13B and ChatGPT through the given rules and finally derive the results by the majority votes among the annotators.</p>
<p>Dialogue Response Generation is a task involves creating a coherent and contextually appropriate response to a given input dialogue. We draw upon the Topical-Chat (Gopalakrishnan et al., 2019) dataset for our study. We then take the human annotation results from Mehri \&amp; Eskenazi (2020) where they carry out the annotations on 60 dialogue contexts with each response generated by 6 different systems. Human evaluators analyzed these responses based on natural, coherence, engagingness, groundedness, and understandable, where we take the first four dimensions for experiments in our paper following Zhong et al. (2022).</p>
<h1>3.3 BASELINES</h1>
<p>We evaluate ChatEval against following methods. As the main portion of our comparison, we primarily focuses on the single-agent-based method. Single-Agent means that we directly query an LLM to generate the response towards the evaluation ${ }^{3}$. We use Multi-Agent to represent ChatEval where several agents discuss towards the evaluation. By default, we configure the communication strategy to one-by-one, agent numbers to 2 , and discussion turns to 2 in this section and employ position calibration techniques in both single-agent and multi-agent settings. We will discuss more debate configurations in Section 4 for completeness. For the open-ended question answer task, we also compare our method with FairEval (Wang et al., 2023b). They propose various strategies to improve the evaluation performance of a LLM including Multiple Evidence Calibration (MEC) and Balanced Position Calibration (BPC). For the dialogue response generation task, we also compare our method with G-EVAL (Liu et al., 2023b). They utilize CoT and probability-weighted summation for their method. Additionally, we include results from n-gram-based metrics, such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002) and embedding-based metrics such as BERTScore (Zhang et al., 2019).</p>
<h3>3.4 RESULTS FOR OPEN-ENDED QUESTION ANSWERS</h3>
<p>We adopt the same evaluation approach as Wang et al. (2023b) to assess the annotation results produced by different methods and annotators. Specifically, we calculate the Accuracy (Acc.), which measures the proportion of correctly classified instances out of the total instances, and the Kappa correlation coefficient (Kap.) (McHugh, 2012) which gauges the agreement between results from models and human annotators while taking into account the possibility of agreement occurring by chance. Both metrics provide insights into the reliability and consistency of the annotations. We take the human annotation results and FairEval's (Wang et al., 2023b) best results from their paper. As is shown in Table 1, different annotators can reach a relatively high agreement and perform better than any other LLM-based approach. Still, the average human annotations accuracy which is $71.7 \%$ shows there exists a certain degree of discrepancy among different unique individuals revealing that text evaluation is absolutely an arduous task. The second part and the third part of Table 1 show the results of FairEval's method and the results of our proposed method respectively. We find that (1) ChatEval can enhance the performance of the evaluation process, achieving higher alignment with human preference compared with single-agent evaluation. Specifically, the multi-agent-based method improves the accuracy by $6.2 \%$ for ChatGPT and $2.5 \%$ for GPT-4; (2) ChatEval surpasses FairEval's best results within both ChatGPT and GPT-4 settings showing the effectiveness of our proposed method.</p>
<h3>3.5 RESULTS FOR DIALOGUE RESPONSE GENERATION</h3>
<p>For the dialogue response generation benchmarks, we align the evaluation method with Zhong et al. (2022), calculating the turn-level Spearman and Kendall-Tau correlation in correspondence with human judgments on four aspects (naturalness, coherence, engagingness and groundedness). Results can be found in Table 2. In the first part of Table 2, we demonstrate that n-gram-based metrics and embedding-based metrics perform overall poorly on all the aspects evaluated illustrating that these methods can hardly reveal human preference. In the second part of Table 2, we show the results from the G-eval (Liu et al., 2023b) paper. They first ask the LLM to generate intermediate thought and finally calculate the weighted summation of the output scores based on the probability. The results show that their method outperforms previous traditional metrics depicting the fact that the LLM-based evaluator is effective and reliable for evaluating the dialogue response generation task. While their method delivers sound results, our proposed approach raises the bar in terms of performance for GPT-4. Specifically, ChatEval improves the average Spearman and KendallTau correlation by $0.096(16.3 \%)$ and $0.057(10.0 \%)$ respectively. Additionally, compared with the single-agent method, ChatEval amplifies the performance both for ChatGPT and GPT-4, showing the effectiveness of our method which is aligned with the results in Section 3.4.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Accuracy (Acc.) and Kappa correlation coefficient (Kap.) of different methods on FairEval benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evaluator</th>
<th style="text-align: left;">Methods</th>
<th style="text-align: left;">Acc. (\%)</th>
<th style="text-align: left;">Kap.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Annotator1</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">68.8</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Annotator2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">76.3</td>
<td style="text-align: left;">0.62</td>
</tr>
<tr>
<td style="text-align: left;">Annotator3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">FairEval</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">MEC+BPC</td>
<td style="text-align: left;">58.7</td>
<td style="text-align: left;">0.31</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">MEC+BPC</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">0.37</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Single-Agent</td>
<td style="text-align: left;">53.8</td>
<td style="text-align: left;">0.27</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Multi-Agent</td>
<td style="text-align: left;">$\mathbf{6 0 . 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 3 3}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">Single-Agent</td>
<td style="text-align: left;">61.3</td>
<td style="text-align: left;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">Multi-Agent</td>
<td style="text-align: left;">$\mathbf{6 3 . 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Turn-level Spearman ( $\rho$ ) and Kendall-Tau ( $\tau$ ) correlations of different methods on TopicalChat benchmark, SA means Single-Agent and MA means Multi-Agent. Our ChatGPT settings should be compared to G-EVAL-3.5, and GPT-4 settings should be compared to G-EVAL-4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Naturalness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Engagingness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Groundedness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.244</td>
</tr>
<tr>
<td style="text-align: left;">BLEU-4</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.189</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.262</td>
</tr>
<tr>
<td style="text-align: left;">G-EVAL-3.5</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.574</td>
</tr>
<tr>
<td style="text-align: left;">G-EVAL-4</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 4}$</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.627</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.575</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT(SA)</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.503</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT(MA)</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.510</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(SA)</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">$\mathbf{0 . 7 7 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 0}$</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.611</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(MA)</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 9}$</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">$\mathbf{0 . 7 6 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 5}$</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">$\mathbf{0 . 6 8 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 2}$</td>
</tr>
</tbody>
</table>
<h1>4 ANALYSIS</h1>
<p>In this section, we further explore the key components encompassed in ChatEval. We discuss the importance of diverse role prompts in Section 4.1, the effect of different communication strategies in Section 4.2, and the impact of role numbers and discussion turns in Section 4.3. If not specified otherwise, we choose the FairEval benchmark and ChatGPT as the backbone LLM for the analysis.</p>
<h3>4.1 THE IMPORTANCE OF DIVERSE ROLE PROMPTS</h3>
<p>Previously in Table 1 and 2, we demonstrate that ChatEval equipped with diverse role configurations can significantly improve the performance of evaluation. We further consider whether it is necessary to design diverse role prompts for the evaluation system. To answer so, we carry out the experiments by replacing all the role prompt with "You are now an Annotator, one of the referees in the text evaluation task." and keeping other prompt unchanged. We experiment with the one-by-one communication strategy and 2 agents with 2 discussion turns. The results in Table 3 illustrate that ChatEval with the same role prompt design underperforms that with diverse role prompt design and cannot effectively enhance the performance compared with single-agent setting, highlighting the cruciality of diverse role prompt design in the multi-agent debate framework.</p>
<h3>4.2 THE STUDY OF COMMUNICATION STRATEGIES</h3>
<p>As shown in Figure 2, we also design three different communication strategy termed as one-by-one, simultaneous-talk, simultaneous-talk-with-summarizer. The detailed descriptions and formal for-</p>
<p>mulations can be found in Appendix B. We experiment with 3 agents and 2 discussion turns with diverse role prompts in this section. As is shown in Table 4, we can find that the one-by-one communication strategy is more effective than other strategies for ChatGPT setting. Although the other two communication strategies did not perform as robustly as the one-by-one strategy, it is noteworthy that they still exceeded the performance of the naive single-agent method. Furthermore, the variations in performance among three different communication strategies underscore the influence of different strategies on the effectiveness of the evaluation process, revealing the potential for further exploration and optimization of ChatEval. Thus, future studies could be aimed at a more comprehensive understanding of different communication strategies, and how they could be effectively employed to enhance performance. This could serve as an avenue for substantial improvements and novel insights in the multi-agent debate framework.</p>
<h1>4.3 THE IMPACT OF ROLE NUMBERS AND DISCUSSION TURNS</h1>
<p>We then study the impact of different role numbers and discussion turns. From Figure 3a, a discernible trend is observed in the relationship between the role number and both Acc. and Kap. within the FairEval dataset. As the role number increases, there is a corresponding growth in Acc. and Kap. This pattern reaches an apex with an Acc. of $62.5 \%$ at role numbers 3 and 4 before declining at role number 5. Despite this decline in accuracy, Kap. continues to ascend between role numbers 3 and 4, underscoring the effectiveness of incorporating diverse roles within ChatEval.</p>
<p>Conversely, no significant upward trend is detected with respect to the increase in discussion turns, as is shown in Figure 3b. This observation aligns with the findings in Liang et al. (2023); Du et al. (2023), highlighting a consistent phenomenon where continual discussion often leads to stagnation or even degradation of performance. Such a trend may be attributed to issues associated with the ever-increasing context length, which consequently diminishes the performance. This insight prompts a more nuanced understanding of the balance needed between role differentiation and discussion dynamics to optimize the performance of ChatEval.</p>
<h3>4.4 QUALITATIVE ANALYSIS</h3>
<p>Table 5 showcases the debate process towards the evaluation of two assistants' responses to the open-ended question "What are the most effective ways to deal with stress?".
Assistant 1's response is :
Here are some effective ways to deal with stress:</p>
<ol>
<li>Exercise regularly: Physical activity can help reduce stress and improve mood.</li>
<li>Practice relaxation techniques: Deep breathing, meditation, and yoga are some effective ways to relax and reduce stress levels.</li>
<li>Get enough sleep: Lack of sleep can increase stress levels. Try to get at least 7-8 hours of sleep each night.</li>
<li>Eat a healthy diet: A well-balanced diet can help reduce stress levels and improve overall health.</li>
<li>Connect with others: Spending time with friends and family, joining a support group, or participating in social activities can help reduce stress levels and improve mood.</li>
<li>Time management: Prioritizing tasks and creating a schedule can help reduce stress and increase productivity.</li>
<li>Seek professional help: If stress is impacting your daily life, consider seeking professional help such as therapy or counseling.</li>
</ol>
<p>Assistant 2's response is :
Stress can be caused by a variety of factors, including work, relationships, health, and financial issues. Here are some effective ways to manage stress:</p>
<ol>
<li>Exercise: Regular exercise releases endorphins, which are natural stressrelievers. It can be as simple as going for a walk or jog, practicing yoga, or joining a fitness class.</li>
<li>Mindfulness: Mindfulness practices, such as meditation or deep breathing, can</li>
</ol>
<p>help to reduce stress and improve focus.
3. Social Support: Spending time with family and friends, or seeking support from a mental health professional, can provide a sense of connection and help to reduce feelings of isolation and loneliness.
4. Healthy Eating: A balanced diet that includes plenty of fruits, vegetables, and whole grains can help to reduce stress levels.
5. Good Sleep: Getting enough sleep is essential for good mental and physical health. Aim for 7-8 hours per night and establish a regular sleep routine.
6. Time Management: Prioritizing tasks and setting realistic goals can help to reduce stress levels by giving a sense of control and direction.
7. Relaxation Techniques: Techniques such as progressive muscle relaxation, aromatherapy, and massage can help to reduce stress levels and promote relaxation.
8. Seek professional help: If stress is interfering with daily life, it may be time to seek professional help from a therapist or counselor.</p>
<p>We can find that both of the responses produce similar strategies and equally compelling descriptions for dealing with stress, making it challenging to discern significant disparity in terms of quality. It is in this context of nuanced evaluation that the significance of the ChatEval process emerges. To understand this complexity better, We first outline the ChatEval process and subsequently delve into the agents' constructive behaviors during discussions.</p>
<p>As is shown in Table 5, Alice first points out that the response of Assistant 2 contains more detailed information and he prefers to choose Assistant 2 as a better response. Bob then says that she agrees with Alice's assessments, but in the meantime, she also points out that Assistant 1's response is also concise and carries out a thought-provoking question. Carol then gives the feedback that she believes both responses are equally valuable. In the subsequent discussion, Bob indicates that Assistant 1's response is straightforward while Assistant 2's is detailed, suggesting that the effectiveness of the response should depend on the context and individual's needs. At the end of the debate, we finally extract the evaluation results that both responses are of the same quality which is identical to human annotation results.</p>
<p>From this sequence, we can pinpoint several fascinating behaviors exhibited by the agents: (1) Opening Statement: Alice initiates the debate with a clear stance, establishing the foundational argument and guiding the trajectory of the subsequent discourse. (2) Alternative Proposal: Bob introduces an alternative viewpoint, emphasizing the need to consider diverse interpretations. This not only broadens the discussion but also stimulates critical thinking. In the context of a debate, the introduction of an alternative proposal prevents the stagnation of thought, challenges pre-existing bias, and uncovers considerations that might otherwise be overlooked, ensuring that the discussions are well-rounded. (3) Stance Maintenance: Alice's persistent adherence to her initial stance, even when faced with opposing views, exemplifies commitment and challenges other participants to refine their perspectives. By firmly holding his position, Alice encourages depth in the discourse, prompting others to dive deeper into their arguments and perhaps consider aspects they hadn't previously. It ensures the conversation remains robust, focused, and continually evolving, driving all participants to a higher level of engagement and critical thinking. (4) Seeking Consensus: The discussion's climax reveals a collective agreement amongst the participants, which is reached through mutual understanding and compromise, underlining the value of each presented viewpoint.</p>
<p>In light of the above, ChatEval stands out not just as a tool for comparison but as an embodiment of interactive natural language dialogue. By simulating human argumentative interactions, it differentiates itself from static, single-presented opinions. This dynamic interaction showcases the richness and complexity of language, capturing nuances often missed in singular viewpoints. As such, ChatEval offers a reliable evaluation process that not only mirrors human discourse but also highlights the transformative power of collaborative dialogue. This positions it uniquely, underscoring its significant potential to execute text evaluation tasks both reliably and effectively.</p>
<h1>5 Related Work</h1>
<p>Automatic NLG evaluation In the landscape of NLG, evaluating the quality of generated text represents a particularly arduous task. For a significant period, evaluation was primarily dependent on</p>
<p>Table 3: Effect of diverse role specification on FairEval benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evaluator</th>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Acc. (\%)</th>
<th style="text-align: center;">Kap.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Single-Agent</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Multi-Agent (Same Role Prompt)</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Multi-Agent (Diverse Role Prompt)</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.33</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparing of different communication strategies on FairEval benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evaluator</th>
<th style="text-align: left;">Communication Strategies</th>
<th style="text-align: center;">Acc. (\%)</th>
<th style="text-align: center;">Kap.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">One-by-One</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Simultaneous-Talk</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Simultaneous-Talk-with-Summarizer</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">0.27</td>
</tr>
</tbody>
</table>
<p>human annotations, a process that is labor-intensive and limited by scalability issues. Automatic NLG evaluation attempts to address these challenges by leveraging computational models to assess the quality of a generated text. Previous work lies on the following categories: (1) $n$-gram-based metrics: ROUGE (Lin, 2004) is a set of metrics that compute the amount of overlap between ngrams in the machine-generated summaries and the reference summaries. BLEU (Papineni et al., 2002) compare the generated text with reference translations, based on the co-occurrence of n-grams in both texts. In spite of being easily and widely used, the above method is incapable of capturing syntactic and semantic similarity (Stent et al., 2005). (2) embedding-based metrics: Word embeddings are vector representations of words that capture their semantic properties, such that words with similar meanings have similar embeddings. A bunch of work leverages word embeddings to evaluate the semantic similarity between two pieces of text. BERTScore (Zhang et al., 2019) use contextualized word embeddings from transformer models like BERT (Devlin et al., 2018), BLEURT (Sellam et al., 2020) utilize supervised training data to enhance the performance. MoverScore (Zhao et al., 2019) combine contextualized word embeddings with Earth Mover's Distance (Rubner et al., 2000). (3) LLM-based metrics: Amidst the flourishing advancement of LLM which embodies a wealth of information derived from extensive training data, using LLM as an evaluator has experienced notable progress. GPTScore (Fu et al., 2023) utilize conditional probability to assign the text a score representing its quality. Wang et al. (2023a) explore the potential of utilizing ChatGPT as an NLG evaluator by prompting it to score a text directly. Wang et al. (2023c) curate a reliable dataset containing pairwise comparison and evaluation explanation which can be used to train a foundation model making it a better evaluator. Bai et al. (2023) propose decentralized evaluation to provide fairer evaluation results. G-EVAL (Liu et al., 2023b) propose probability-weighted techniques to calibrate the score given by a single LLM.</p>
<p>Communicative Agents Most recently, significant attention has been dedicated to the development of communicative agents. These agents, often acted by LLMs like ChatGPT or GPT-4, are designed to interact and communicate effectively with other agents or human users using natural language. The primary goal is to facilitate more productive and efficient interaction and collaboration as different agents can autonomously communicate and negotiate to tackle a more complex task collectively. Several studies have explored various aspects of communicative agents. Li et al. (2023a) propose a cooperative agent framework dubbed as role-playing enabling agents to autonomously cooperate to solve complex tasks. Park et al. (2023) create a sandbox environment consisting of 25 individual virtual entities endowed with a character description and memory system. Every intelligent agent is capable of autonomously interacting with other agents and the environment simulating reliable human behavior. Qian et al. (2023) establish a chat-based software development framework that can complete a software design and produce executable software at a reduced cost compared to recruiting human programmers. Liu et al. (2023a) utilize a sandbox environment to curate reliable datasets in better alignment with human preference and train a socially-aligned LLM. Liang et al. (2023) and Du et al. (2023) also make use of the multi-agent debate framework in other scenarios such as translation and arithmetic problems resulting in better results. Wang et al. (2023d) propose an alternative method called self-collaboration to enable the communication of agents by utilizing a single LLM prompted by multi-persona descriptions. Mandi et al. (2023) propose a novel framework</p>
<p>Table 5: An illustrative example from ChatEval presenting a discussion process involving participants Alice ( ), Bob ( ) and Carol ( ). They initiate a group talk with the purpose of determining the superior response from two assistants. Due to spatial constraints within the layout, we use ... to omit some redundant text.</p>
<p>Both Assistant 1 and Assistant 2 provided comprehensive and detailed answers. They both suggested similar strategies for dealing with stress, such as exercise, relaxation techniques, getting enough sleep, eating a healthy diet, connecting with others, time management, and seeking professional help. However, Assistant 2 went a step further by mentioning mindfulness social support, and relaxation techniques like progressive muscle relaxation, aromatherapy, and massage. Therefore, I would lean towards Assistant 2 for providing a slightly more detailed response.</p>
<p>I agree with the Alice's assessments. However, I would like to point out that Assistant 1's answer was more concise and to the point that ... . On the other hand, Assistant 2's answer was more detailed and ... . In your opinion, does the additional detail provided by Assistant 2 make its response more helpful, or could the conciseness of Assistant 1's response be seen as more beneficial?</p>
<p>In terms of consistency with the question, both responses are equally consistent as they both provided effective ways to deal with stress. Assistant 1's response was indeed more concise, while Assistant 2 provided more detailed techniques. The effectiveness of these responses could depend on the individual's preference for brevity or detail. Therefore, I believe both responses are equally valuable.</p>
<p>I appreciate the points made by both of you. While Assistant 1's response was indeed more concise, I believe that the additional detail provided by Assistant 2 could be more beneficial for someone looking for a variety of strategies to manage stress. The inclusion of mindfulness practices and ... . Therefore, I still lean towards Assistant 2's response as being slightly more helpful.</p>
<p>However, I would like to emphasize that the effectiveness of a response can also depend on the context and the individual's needs. For someone who is overwhelmed and looking for quick, straightforward advice, the conciseness of Assistant 1's response might be more beneficial. On the other hand, for someone who is looking for a variety of strategies to try, the detailed response of Assistant 2 might be more helpful. Therefore, I believe both responses have their merits and it's difficult to definitively say one is better than the other. What do you think about this perspective?</p>
<p>I agree with the Bob's perspective that the effectiveness of a response can depend on the individual's needs and context. Therefore, I believe both responses are equally valuable and it's hard to definitively say one is better than the other.</p>
<p>After discussing with my co-workers, I believe both Assistants performed equally well in providing comprehensive and accurate responses.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) Performance with Increased Different Roles on FairEval Dataset. We adopt one-by-one communication strategy and 2 discussion turns. Additional role descriptions are shown in Appendix A. (b) Performance with Increased Discussion Turns on FairEval Dataset. We adopt one-by-one communication strategy and 2 different roles.
designed for the collaboration of multiple robots, utilizing multiple LLMs to enhance coordination and strategic planning among the robots. Concurrent with our work, Li et al. (2023b) propose Peer Rank and Discussion (PRD) which is similar to our approach. However, they probe the different dimensions of evaluation by using different models as agents and did not explore alternative communication strategies.</p>
<h1>6 CONCLUSION</h1>
<p>In this paper, we present evidence that ChatEval contributes to improving the evaluation performance concerning text quality, aligning more closely with human preferences. We emphasize the necessity of the diverse role specification and propose distinct communication strategies as integral components within ChatEval. Our qualitative analysis of the discussion process conveys insightful intuitions about how a text is evaluated by ChatEval and substantiates our approach's ability to support comprehensive evaluations akin to human judgment, thereby demonstrating the reliability and efficacy of our framework.</p>
<h2>REFERENCES</h2>
<p>Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-an-examiner. arXiv preprint arXiv:2306.04181, 2023.</p>
<p>Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 65-72, 2005.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Chris Callison-Burch. Fast, cheap, and creative: Evaluating translation quality using amazon's mechanical turk. In Proceedings of the 2009 conference on empirical methods in natural language processing, pp. 286-295, 2009.</p>
<p>Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation of text generation: A survey. CoRR, abs/2006.14799, 2020. URL https://arxiv.org/abs/2006.14799.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.</p>
<p>Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554, 2023.</p>
<p>Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anushree Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tür. Topical-chat: Towards knowledgegrounded open-domain conversations. 2019.</p>
<p>Marzena Karpinska, Nader Akoury, and Mohit Iyyer. The perils of using mechanical turk to evaluate open-ended text generation. arXiv preprint arXiv:2109.06835, 2021.</p>
<p>Grzegorz Kondrak. N-gram similarity and distance. In International symposium on string processing and information retrieval, pp. 115-126. Springer, 2005.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023a.</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762, 2023b.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multiagent debate. arXiv preprint arXiv:2305.19118, 2023.</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.</p>
<p>Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960, 2023a.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b.</p>
<p>Andrea I Luppi, Pedro AM Mediano, Fernando E Rosas, Negin Holland, Tim D Fryer, John T O'Brien, James B Rowe, David K Menon, Daniel Bor, and Emmanuel A Stamatakis. A synergistic core for human brain evolution and cognition. Nature Neuroscience, 25(6):771-782, 2022.</p>
<p>Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. arXiv preprint arXiv:2307.04738, 2023.</p>
<p>Mary L McHugh. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276-282, 2012.
Shikib Mehri and Maxine Eskenazi. Usr: An unsupervised and reference free evaluation metric for dialog generation. arXiv preprint arXiv:2005.00456, 2020.</p>
<p>Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, and Verena Rieser. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2241-2252, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1238. URL https:// aclanthology.org/D17-1238.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.</p>
<p>Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.</p>
<p>Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover's distance as a metric for image retrieval. International journal of computer vision, 40:99-121, 2000.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696, 2020.</p>
<p>Chenhui Shen, Liying Cheng, Yang You, and Lidong Bing. Are large language models good evaluators for abstractive summarization? arXiv preprint arXiv:2305.13091, 2023.</p>
<p>Amanda Stent, Matthew Marge, and Mohit Singhai. Evaluating evaluation methods for generation in the presence of variation. In International conference on intelligent text processing and computational linguistics, pp. 341-351. Springer, 2005.</p>
<p>Chris Van Der Lee, Albert Gatt, Emiel Van Miltenburg, Sander Wubben, and Emiel Krahmer. Best practices for the human evaluation of automatically generated text. In Proceedings of the 12th International Conference on Natural Language Generation, pp. 355-368, 2019.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048, 2023a.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023c.</p>
<p>Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. arXiv preprint arXiv:2307.05300, 2023d.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Anita Williams Woolley, Christopher F Chabris, Alex Pentland, Nada Hashmi, and Thomas W Malone. Evidence for a collective intelligence factor in the performance of human groups. science, 330(6004):686-688, 2010.</p>
<p>Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. Large language models are diverse role-players for summarization evaluation. arXiv preprint arXiv:2303.15078, 2023.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. arXiv preprint arXiv:1909.02622, 2019.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. Towards a unified multi-dimensional evaluator for text generation. arXiv preprint arXiv:2210.07197, 2022.</p>
<h1>A PROMPT TEMPLATE AND DIVERSE ROLE PROMPT</h1>
<p>The overall prompt template is shown in Table 6, we draw inspiration from Wu et al. (2023) and design several different role descriptions as follows.</p>
<p>General Public You are now General Public, one of the referees in this task. You are interested in the story and looking for updates on the investigation. Please think critically by yourself and note that it's your responsibility to choose one of which is the better first.</p>
<p>Critic You are now Critic, one of the referees in this task. You will check fluent writing, clear sentences, and good wording in summary writing. Your job is to question others judgment to make sure their judgment is well-considered and offer an alternative solution if two responses are at the same level.</p>
<p>News Author You are News Author, one of the referees in this task. You will focus on the consistency with the original article. Please help other people to determine which response is the better one.</p>
<p>Psychologist You are Psychologist, one of the referees in this task. You will study human behavior and mental processes in order to understand and explain human behavior. Please help other people to determine which response is the better one.</p>
<p>Scientist You are Scientist, one of the referees in this task. You are a professional engaged in systematic study who possesses a strong background in the scientific method, critical thinking, and problem-solving abilities. Please help other people to determine which response is the better one.</p>
<h2>B FORMAL DEPICTION OF DIFFERENT COMMUNICATION STRATEGY</h2>
<div class="codehilite"><pre><span></span><code><span class="k">[Question]</span>
<span class="na">{source_text}</span>
<span class="k">[The Start of Assistant 1&#39;s Answer]</span>
<span class="na">{compared_text_one}</span>
<span class="k">[The End of Assistant 1&#39;s Answer]</span>
<span class="k">[The Start of Assistant 2&#39;s Answer]</span>
<span class="na">{compared_text_two}</span>
<span class="k">[The End of Assistant 2&#39;s Answer]</span>
<span class="k">[System]</span>
<span class="na">We would like to request your feedback on the performance of two AI assistants in response</span>
<span class="na">to the user question displayed above.</span>
<span class="na">Please consider the helpfulness, relevance, accuracy, and level of detail of their responses.</span>
<span class="na">Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates</span>
<span class="na">better overall performance.</span>
<span class="na">There are a few other referees assigned the same task, it&#39;s your responsibility to discuss with</span>
<span class="na">them and think critically before you make your final judgment.</span>
<span class="na">Here is your discussion history</span><span class="o">:</span>
<span class="na">{chat_history}</span>
<span class="na">{role_description}</span>
<span class="na">Now it&#39;s your time to talk, please make your talk short and clear, {agent_name} !</span>
</code></pre></div>

<p>Table 6: The prompt template for FairEval Dataset. We replace the colored slot with real text before querying the LLMs. Note that we use the same template when conducting single-agent-based experiments and ignore the chat history and role description slot.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">One-by-One</span>
<span class="nt">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="nt">number</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">discuss</span><span class="w"> </span><span class="nt">turn</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">group</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">debate</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">D_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">D_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">chat</span><span class="w"> </span><span class="nt">history</span>
<span class="w">    </span><span class="nt">of</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">agent</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">H_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">H_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">answer_extracter</span><span class="w"> </span><span class="o">(</span><span class="nt">either</span><span class="w"> </span><span class="nt">majority</span><span class="w"> </span><span class="nt">vote</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">average</span><span class="w"> </span><span class="nt">score</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">E</span><span class="w"> </span><span class="nt">X</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="o">)</span>
<span class="nt">output</span><span class="o">:</span><span class="w"> </span><span class="nt">Final</span><span class="w"> </span><span class="nt">results</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">text</span><span class="w"> </span><span class="nt">evaluation</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="err">\</span><span class="o">)</span>
<span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">h_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">D_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">H_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nt">utilize</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">generate</span><span class="w"> </span><span class="nt">responses</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="o">&gt;</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="err">\</span><span class="o">(</span><span class="nt">H_</span><span class="p">{</span><span class="err">m</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">H_</span><span class="p">{</span><span class="err">m</span><span class="p">}</span><span class="o">+</span><span class="nt">h_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">                </span><span class="o">//</span><span class="w"> </span><span class="nt">concatenate</span><span class="w"> </span><span class="nt">current</span><span class="w"> </span><span class="nt">response</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">later</span><span class="w"> </span><span class="nt">agents</span><span class="err">&#39;</span><span class="w"> </span><span class="nt">chat</span>
<span class="w">                </span><span class="nt">history</span>
<span class="w">            </span><span class="nt">end</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">end</span>
<span class="nt">end</span>
<span class="nt">11</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">E</span><span class="w"> </span><span class="nt">X</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">H_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">H_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="nt">12</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">return</span><span class="w"> </span>\<span class="ss">(</span><span class="nv">A</span><span class="w"> </span><span class="nv">N</span><span class="w"> </span><span class="nv">S</span>\<span class="ss">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">Simultaneous-Talk</span>
<span class="nt">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="nt">number</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">discuss</span><span class="w"> </span><span class="nt">turn</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">group</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">debate</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">D_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">D_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">chat</span><span class="w"> </span><span class="nt">history</span>
<span class="w">        </span><span class="nt">of</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">agent</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">H_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">H_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">answer_extracter</span><span class="w"> </span><span class="o">(</span><span class="nt">either</span><span class="w"> </span><span class="nt">majority</span><span class="w"> </span><span class="nt">vote</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">average</span><span class="w"> </span><span class="nt">score</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">E</span><span class="w"> </span><span class="nt">X</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">buffer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">B</span><span class="w"> </span><span class="nt">U</span><span class="w"> </span><span class="nt">F</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">output</span><span class="o">:</span><span class="w"> </span><span class="nt">Final</span><span class="w"> </span><span class="nt">results</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">text</span><span class="w"> </span><span class="nt">evaluation</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">h_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">D_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">H_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">utilize</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">generate</span><span class="w"> </span><span class="nt">responses</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">b</span><span class="w"> </span><span class="nt">u</span><span class="w"> </span><span class="nt">f</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">u</span><span class="w"> </span><span class="nt">f</span><span class="o">+</span><span class="nt">h_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">add</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">responses</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">current</span><span class="w"> </span><span class="nt">turn</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">buffer</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">H_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">H_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">buf</span><span class="o">;</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">add</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">buffer</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">all</span><span class="w"> </span><span class="nt">agents</span><span class="err">&#39;</span><span class="w"> </span><span class="nt">chat</span><span class="w"> </span><span class="nt">history</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">end</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">E</span><span class="w"> </span><span class="nt">X</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">H_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">H_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="err">\</span><span class="o">)</span>
<span class="nt">Algorithm</span><span class="w"> </span><span class="nt">3</span><span class="o">:</span><span class="w"> </span><span class="nt">Simultaneous-Talk-with-Summarizer</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">agents</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="o">\(</span><span class="n">N</span><span class="o">\),</span><span class="w"> </span><span class="n">discuss</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="o">\(</span><span class="n">T</span><span class="o">\),</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">group</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">debate</span><span class="w"> </span><span class="n">agents</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">[</span><span class="n">D_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="o">\</span><span class="n">cdots</span><span class="o">,</span><span class="w"> </span><span class="n">D_</span><span class="o">{</span><span class="n">N</span><span class="o">}\</span><span class="n">right</span><span class="o">]\),</span><span class="w"> </span><span class="n">chat</span><span class="w"> </span><span class="n">history</span>
<span class="w">        </span><span class="n">of</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">[</span><span class="n">H_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="o">\</span><span class="n">cdots</span><span class="o">,</span><span class="w"> </span><span class="n">H_</span><span class="o">{</span><span class="n">N</span><span class="o">}\</span><span class="n">right</span><span class="o">]\),</span><span class="w"> </span><span class="n">answer_extracter</span><span class="w"> </span><span class="o">(</span><span class="n">either</span><span class="w"> </span><span class="n">majority</span><span class="w"> </span><span class="n">vote</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">average</span><span class="w"> </span><span class="n">score</span><span class="o">)</span>
<span class="w">        </span><span class="o">\(</span><span class="n">E</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="n">T</span><span class="o">\),</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="o">\(</span><span class="n">B</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">F</span><span class="o">\),</span><span class="w"> </span><span class="n">summarizer</span><span class="w"> </span><span class="o">\(</span><span class="n">S</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">M</span><span class="o">\)</span>
<span class="w">    </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Final</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">text</span><span class="w"> </span><span class="n">evaluation</span><span class="w"> </span><span class="o">\(</span><span class="n">A</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="n">S</span><span class="o">\)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="o">\(</span><span class="n">t</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="o">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">T</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="o">\(</span><span class="n">n</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="o">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">N</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">            </span><span class="o">\(</span><span class="n">h_</span><span class="o">{</span><span class="n">n</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">D_</span><span class="o">{</span><span class="n">n</span><span class="o">}\</span><span class="n">left</span><span class="o">(</span><span class="n">H_</span><span class="o">{</span><span class="n">n</span><span class="o">}\</span><span class="n">right</span><span class="o">)</span><span class="w"> </span><span class="o">;\)</span>
<span class="w">            </span><span class="c1">// utilize agents to generate responses</span>
<span class="w">            </span><span class="n">buf</span><span class="w"> </span><span class="o">\(\</span><span class="n">leftarrow</span><span class="o">\)</span><span class="w"> </span><span class="n">buf</span><span class="w"> </span><span class="o">\(+</span><span class="n">h_</span><span class="o">{</span><span class="n">n</span><span class="o">}\);</span>
<span class="w">            </span><span class="c1">// add the responses in current turn to the buffer</span>
<span class="w">        </span><span class="n">end</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="o">\(</span><span class="n">n</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="o">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">N</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">            </span><span class="o">\(</span><span class="n">H_</span><span class="o">{</span><span class="n">n</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">H_</span><span class="o">{</span><span class="n">n</span><span class="o">}+</span><span class="n">S</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">M</span><span class="o">(</span><span class="n">B</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">F</span><span class="o">)</span><span class="w"> </span><span class="o">;\)</span>
<span class="w">            </span><span class="c1">// add the summarized buffer to all agents&#39; chat history</span>
<span class="w">        </span><span class="n">end</span>
<span class="w">    </span><span class="n">end</span>
<span class="w">    </span><span class="o">\(</span><span class="n">A</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="n">T</span><span class="o">\</span><span class="n">left</span><span class="o">(\</span><span class="n">left</span><span class="o">[</span><span class="n">H_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="o">\</span><span class="n">cdots</span><span class="o">,</span><span class="w"> </span><span class="n">H_</span><span class="o">{</span><span class="n">N</span><span class="o">}\</span><span class="n">right</span><span class="o">]\</span><span class="n">right</span><span class="o">)\);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">\(</span><span class="n">A</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="n">S</span><span class="o">\);</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We use the same prompt template as our multi-agent debate settings in single-agent baseline except that we ignore some slot.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
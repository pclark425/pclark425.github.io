<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9444 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9444</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9444</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-62ad7ea9467bbcdbfe325b9ee561cab3908e4583</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/62ad7ea9467bbcdbfe325b9ee561cab3908e4583" target="_blank">MEGA: Multilingual Evaluation of Generative AI</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work presents the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages and presents a thorough analysis of the performance of models across languages and tasks.</p>
                <p><strong>Paper Abstract:</strong> Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9444.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9444.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Translate-Test vs Monolingual prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Translate-Test prompting compared to Monolingual prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of translate-test (translating test input into English and prompting with English few-shot examples) versus monolingual in-language prompting; evaluated across multiple tasks (classification, commonsense reasoning, QA, summarization) and models (text-davinci-003, gpt-3.5-turbo, gpt-4).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (DV003), gpt-3.5-turbo, gpt-4-32k</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XNLI, XCOPA, XStoryCloze, PAWS-X, QA tasks (XQuAD, TyDiQA, MLQA), XLSum</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cross-lingual natural language inference (XNLI), commonsense causal reasoning (XCOPA), story ending prediction (XStoryCloze), paraphrase identification (PAWS-X), extractive QA (XQuAD/TyDiQA/MLQA), multilingual abstractive summarization (XLSum). Metrics: accuracy for classification tasks, F1/EM for QA, ROUGE-L/BLEU-like for summarization (BOUGE-L in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Translate-Test: few-shot exemplars in English (k usually 8, k=4 for long-context tasks), test example translated into English using Bing Translator; prompt templates in English; answer verbalizers in English. Monolingual: few-shot exemplars in the same target language as test example; same English-written prompt template was used for all languages in this paper's evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Monolingual prompting (same-language few-shot), Zero-shot cross-lingual (English few-shot, test in target language without translation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples (average across languages from Table 1): DV003 XNLI monolingual accuracy 59.27; DV003 XNLI (Translate-Test) accuracy 67.0. GPT-3.5-turbo XNLI monolingual 62.1; GPT-3.5-turbo (TT) 64.3. DV003 XStoryCloze monolingual 74.7 vs (TT) 94.8. GPT-3.5-turbo XStoryCloze monolingual 87.7 vs (TT) 93.8. QCOPA: DV003 75.2 -> (TT) 83.8; GPT-3.5 79.1 -> (TT) 81.9. (See Table 1 for more task averages.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Translate-Test consistently improves performance over Monolingual for many low-resource and non-Latin-script languages and tasks (see above numeric deltas). For high-resource languages performance often similar between Monolingual and Translate-Test. GPT-4 often has Monolingual performance more on-par with Translate-Test and sometimes better, but in very low-resource cases Translate-Test still helps (example: GPT-4 XStoryCloze Burmese: Monolingual 77.6% vs Translate-Test 93.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Effect sizes vary by task and model: DV003 XNLI +7.73% accuracy (67.0 - 59.27); DV003 XStoryCloze +20.1% (94.8 - 74.7); GPT-3.5-turbo XStoryCloze +6.1% (93.8 - 87.7); XCOPA DV003 +8.6% (83.8 - 75.2); reported per-language relative improvements >30% for languages like Burmese, Tamil, Telugu on GPT-3.5-turbo (relative percentage improvement over Monolingual, Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute Translate-Test gains to better grounding of the model in English where models have more pretraining and instruction-tuning data; translate-test mitigates poor tokenizer behavior and sparse pretraining data for low-resource/non-Latin languages. They note that despite gains, a gap remains relative to English performance (example: Urdu XNLI GPT-3.5-Turbo translate-test 54% vs English 76.2%). They also caution that some gains may be inflated by possible data contamination (datasets present in training corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot examples: typically k=8 (k=4 for QA and summarization); prompt templates chosen from PromptSource by validation on English; Bing Translator used for Translate-Test; performed translate-test for most classification and toxicity tasks but not for QA and sequence labeling where label alignment is non-trivial; GPT-4 translate-test evaluated only for some datasets due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEGA: Multilingual Evaluation of Generative AI', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9444.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9444.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-Shot Cross-Lingual prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot cross-lingual few-shot prompting (English exemplars, target-language test)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using few-shot examples from a pivot language (English) but presenting the test example in the target language without translation to test cross-lingual in-context transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (DV003), gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XCOPA and other classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense reasoning and classification tasks; metric accuracy reported averaged across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-Shot Cross-Lingual: few-shot examples in English (k typically 8), prompt template in English, test example in the target language (no translation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Monolingual prompting (few-shot exemplars in the same target language) and Translate-Test.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative and aggregate observations: For DV003 zero-shot cross-lingual often performs on par with Monolingual. For gpt-3.5-turbo, zero-shot cross-lingual showed drops in performance on some tasks with very low-resource languages; specific average numeric drops are not tabulated per-format in main text for all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>DV003: zero-shot approx. similar to monolingual. GPT-3.5-turbo: lower performance in some cases (e.g., XCOPA languages like Quechua and Haitian Creole), sometimes producing explicit failure outputs ('I'm sorry, but the premise is not in a language that I understand.'). Providing few-shot examples in the target language (monolingual prompting) grounded the model and mitigated such failures.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that for models with weaker cross-lingual representation or poorer instruction-following on certain languages, few-shot examples in English can fail to ground the model in the target language; in some low-resource languages the model may not detect or handle the input language without in-language exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot k=8, prompt templates in English; observation particularly pronounced for extremely low-resource languages and on gpt-3.5-turbo; DV003 sometimes robust likely due to differences in instruction-following and training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEGA: Multilingual Evaluation of Generative AI', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9444.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9444.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot size (k) effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of few-shot examples (k) on prompting performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigation of how varying the number of in-context examples (k) influences model performance; authors vary k and measure stability/improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (primary reported), DV003 in some checks</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Subset of MEGA tasks (classification, commonsense reasoning; experiments shown in Figure 6)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various classification and commonsense reasoning tasks; metrics task-dependent (accuracy/F1).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context learning with varying k (number of exemplars); authors experiment with different k values and measure performance; default k used elsewhere is 8 (4 for longer-context tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report that increasing few-shot size generally improves performance, but gains saturate; performance often stable beyond k=8.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative: improvement up to k=8 then diminishing returns; exact numeric deltas vary across tasks and not provided as a single consolidated percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More in-context exemplars give stronger supervision signal for the task formatting and answer mapping, but returns diminish once the model has enough examples to infer the pattern; computational cost and context window limitations constrain increasing k.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments summarized in Figure 6; typical default use in paper: k=8 for most tasks, k=4 for QA and summarization; prompts selected from PromptSource; cost considerations limited larger k for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEGA: Multilingual Evaluation of Generative AI', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9444.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9444.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Use of explanations in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Including explanatory text (rationale) in few-shot exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Testing whether providing explanations or rationales in prompt exemplars improves multilingual model performance on reasoning/story tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo, text-davinci-003 (some checks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XStoryCloze, XCOPA (commonsense reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predicting causal/commonsense relation or plausible story endings; accuracy is the metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot exemplars augmented with explanations/rationales (i.e., demonstration examples that include why the chosen answer is correct) versus standard few-shot exemplars without explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard few-shot exemplars (no explanations) vs few-shot with explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report negligible impact of using explanations on XStoryCloze and XCOPA performance (Figure 6 summary). No large gains observed in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Reported as negligible; no substantial percentage improvements cited.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that explanations in the prompt, at least in the formats tested for these multilingual, low-resource reasoning datasets, do not meaningfully change performance — indicating that existing prompting strategies (including adding explanations) may be insufficient to close multilingual performance gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments described in Section 4.1 and summarized in Figure 6; datasets: XStoryCloze and XCOPA; few-shot sizes as per default (k up to 8).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEGA: Multilingual Evaluation of Generative AI', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9444.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9444.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt template selection (PromptSource tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt template selection and cross-language template reuse (PromptSource)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors select English prompt templates from PromptSource by validation performance on English, then reuse the same templates for all target languages; they study effect and note potential limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DV003, gpt-3.5-turbo, gpt-4 (prompt templates chosen for latter based on gpt-3.5-turbo tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All MEGA tasks where prompting applied</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various classification, QA, sequence-labeling, and generation tasks; measured by accuracy/F1/EM/ROUGE-L as appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt templates chosen from PromptSource by evaluating multiple English templates on the English validation set and selecting the best-performing template; that single English template is then applied across languages and prompting strategies (Monolingual, Translate-Test, Zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Potential alternatives would be language-specific prompt tuning (not performed at scale due to cost) or using prompt templates written in target languages (not exhaustively tested here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No single numeric performance value; authors report the procedure and note it affects measured performance and is a sensitivity in multilingual prompting. They performed separate prompt-tuning for DV003 and gpt-3.5-turbo and reused gpt-3.5-turbo-chosen prompts for GPT-4 to save cost.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors caution that using English-optimized templates for all languages may bias results and that prompt-template choice can substantially influence performance; full language-specific tuning might improve results but was computationally prohibitive.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>PromptSource used as repository; selection done on English validation split; templates kept in English across experiments; Appendix §A.7 contains note on impact of prompt-language choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEGA: Multilingual Evaluation of Generative AI', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9444.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9444.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieve-then-prompt for QA (DV003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval before prompting to fit QA context into small-context models (DV003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For DV003 (4096 token limit) the authors use a retrieval pipeline to include only the most relevant context chunk in prompt; they compare QA performance to models with larger context windows (gpt-3.5-turbo, gpt-4).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (DV003) vs gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>IndicQA and other extractive QA datasets (XQuAD, MLQA, TyDiQA-GoldP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Span-prediction QA: given context and question, return answer span; metrics: F1 and Exact Match.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>For DV003: few-shot examples provided with only the sentence line containing the answer; test context chunk selected via retrieval using embeddings (text-embedding-ada-002) and LangChain retrieval with max chunk size 100; for gpt-3.5-turbo and gpt-4 full context was provided (larger context windows).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>DV003 with retrieve-then-prompt vs gpt-3.5-turbo/gpt-4 with direct full-context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>DV003 QA average (XQuAD) F1/EM: 40.5 / 28.0 (monolingual, Table 1). gpt-3.5-turbo XQuAD: 60.4 / 38.2. The authors attribute part of the DV003 deficit to imperfect retrieval needed to fit prompts in its smaller context window.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Approximate effect: large drop for DV003 relative to gpt-3.5-turbo on QA tasks; eg. gpt-3.5-turbo XQuAD F1 60.4 vs DV003 40.5 (a ~20 percentage-point drop in F1), partly attributed to retrieval errors.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Approximate: -19.9 F1 points for DV003 vs gpt-3.5-turbo on XQuAD (40.5 vs 60.4); some of this gap caused by different prompting/retrieval pipeline rather than pure model capability.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Smaller context window forced a retrieve-then-prompt pipeline for DV003; retrieval can be imperfect especially for low-resource languages, harming downstream QA performance. Models with larger context windows can include full passages and thus avoid retrieval errors.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>DV003 context limit 4096 tokens required retrieval; retrieval used text-embedding-ada-002 embeddings and LangChain retrieval with chunk size 100; k for few-shot in QA set to 4; gpt-3.5-turbo and gpt-4 used full context (16k and 32k support respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEGA: Multilingual Evaluation of Generative AI', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9444.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9444.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tokenizer fertility / tokenization effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of tokenizer fertility (overtokenization) on multilingual performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigation of how tokenizer fertility (average number of sub-words per word) correlates with downstream multilingual performance; higher fertility indicates worse tokenizer quality for that language and model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI models (text-davinci-003, gpt-3.5-turbo, gpt-4), mBERT, BLOOM (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple MEGA tasks (Indic-XNLI / XNLI combined, others reported where correlation |ρ|>0.7)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification and other tasks across languages; performance metrics task-dependent (accuracy/F1).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not a prompt format per se, but a presentation-layer issue: input encoding/tokenization differs by language and model tokenizer, affecting prompt length, cost, and effective in-context signal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors observe statistically significant negative correlations between tokenizer fertility and dataset-specific performance for six tasks (Figure 5). They show extreme fertility values (~10) for Malayalam and Tamil with OpenAI tokenizers, where tokenizer behaves effectively like a byte-level tokenizer, increasing token counts and harming performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Higher tokenizer fertility reduces effective model capacity for a given context window (more tokens used to represent same content), increases API cost, and correlates with worse accuracy/F1 on downstream tasks. Tokenizer quality and amount of pretraining data together explain some cross-language performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tokenizer fertility measured as average number of sub-words per tokenized word; correlations reported where Pearson |ρ|>0.7 with p-value 0.05; specific fertility examples: Malayalam and Tamil ~10 for OpenAI tokenizers; pretraining-data-token counts used as proxy (GPT-3 distribution) to further explain discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEGA: Multilingual Evaluation of Generative AI', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Few-shot learning with multilingual generative language models <em>(Rating: 2)</em></li>
                <li>Language models are multilingual chain-of-thought reasoners <em>(Rating: 2)</em></li>
                <li>Do all languages cost the same? tokenization in the era of commercial language models <em>(Rating: 2)</em></li>
                <li>PromptSource: An integrated development environment and repository for natural language prompts <em>(Rating: 1)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9444",
    "paper_id": "paper-62ad7ea9467bbcdbfe325b9ee561cab3908e4583",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Translate-Test vs Monolingual prompting",
            "name_full": "Translate-Test prompting compared to Monolingual prompting",
            "brief_description": "Comparison of translate-test (translating test input into English and prompting with English few-shot examples) versus monolingual in-language prompting; evaluated across multiple tasks (classification, commonsense reasoning, QA, summarization) and models (text-davinci-003, gpt-3.5-turbo, gpt-4).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (DV003), gpt-3.5-turbo, gpt-4-32k",
            "model_size": null,
            "task_name": "XNLI, XCOPA, XStoryCloze, PAWS-X, QA tasks (XQuAD, TyDiQA, MLQA), XLSum",
            "task_description": "Cross-lingual natural language inference (XNLI), commonsense causal reasoning (XCOPA), story ending prediction (XStoryCloze), paraphrase identification (PAWS-X), extractive QA (XQuAD/TyDiQA/MLQA), multilingual abstractive summarization (XLSum). Metrics: accuracy for classification tasks, F1/EM for QA, ROUGE-L/BLEU-like for summarization (BOUGE-L in paper).",
            "presentation_format": "Translate-Test: few-shot exemplars in English (k usually 8, k=4 for long-context tasks), test example translated into English using Bing Translator; prompt templates in English; answer verbalizers in English. Monolingual: few-shot exemplars in the same target language as test example; same English-written prompt template was used for all languages in this paper's evaluation.",
            "comparison_format": "Monolingual prompting (same-language few-shot), Zero-shot cross-lingual (English few-shot, test in target language without translation).",
            "performance": "Examples (average across languages from Table 1): DV003 XNLI monolingual accuracy 59.27; DV003 XNLI (Translate-Test) accuracy 67.0. GPT-3.5-turbo XNLI monolingual 62.1; GPT-3.5-turbo (TT) 64.3. DV003 XStoryCloze monolingual 74.7 vs (TT) 94.8. GPT-3.5-turbo XStoryCloze monolingual 87.7 vs (TT) 93.8. QCOPA: DV003 75.2 -&gt; (TT) 83.8; GPT-3.5 79.1 -&gt; (TT) 81.9. (See Table 1 for more task averages.)",
            "performance_comparison": "Translate-Test consistently improves performance over Monolingual for many low-resource and non-Latin-script languages and tasks (see above numeric deltas). For high-resource languages performance often similar between Monolingual and Translate-Test. GPT-4 often has Monolingual performance more on-par with Translate-Test and sometimes better, but in very low-resource cases Translate-Test still helps (example: GPT-4 XStoryCloze Burmese: Monolingual 77.6% vs Translate-Test 93.2%).",
            "format_effect_size": "Effect sizes vary by task and model: DV003 XNLI +7.73% accuracy (67.0 - 59.27); DV003 XStoryCloze +20.1% (94.8 - 74.7); GPT-3.5-turbo XStoryCloze +6.1% (93.8 - 87.7); XCOPA DV003 +8.6% (83.8 - 75.2); reported per-language relative improvements &gt;30% for languages like Burmese, Tamil, Telugu on GPT-3.5-turbo (relative percentage improvement over Monolingual, Figure 3).",
            "explanation_or_hypothesis": "Authors attribute Translate-Test gains to better grounding of the model in English where models have more pretraining and instruction-tuning data; translate-test mitigates poor tokenizer behavior and sparse pretraining data for low-resource/non-Latin languages. They note that despite gains, a gap remains relative to English performance (example: Urdu XNLI GPT-3.5-Turbo translate-test 54% vs English 76.2%). They also caution that some gains may be inflated by possible data contamination (datasets present in training corpora).",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot examples: typically k=8 (k=4 for QA and summarization); prompt templates chosen from PromptSource by validation on English; Bing Translator used for Translate-Test; performed translate-test for most classification and toxicity tasks but not for QA and sequence labeling where label alignment is non-trivial; GPT-4 translate-test evaluated only for some datasets due to cost.",
            "uuid": "e9444.0",
            "source_info": {
                "paper_title": "MEGA: Multilingual Evaluation of Generative AI",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Zero-Shot Cross-Lingual prompting",
            "name_full": "Zero-shot cross-lingual few-shot prompting (English exemplars, target-language test)",
            "brief_description": "Using few-shot examples from a pivot language (English) but presenting the test example in the target language without translation to test cross-lingual in-context transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (DV003), gpt-3.5-turbo",
            "model_size": null,
            "task_name": "XCOPA and other classification tasks",
            "task_description": "Commonsense reasoning and classification tasks; metric accuracy reported averaged across languages.",
            "presentation_format": "Zero-Shot Cross-Lingual: few-shot examples in English (k typically 8), prompt template in English, test example in the target language (no translation).",
            "comparison_format": "Compared to Monolingual prompting (few-shot exemplars in the same target language) and Translate-Test.",
            "performance": "Qualitative and aggregate observations: For DV003 zero-shot cross-lingual often performs on par with Monolingual. For gpt-3.5-turbo, zero-shot cross-lingual showed drops in performance on some tasks with very low-resource languages; specific average numeric drops are not tabulated per-format in main text for all tasks.",
            "performance_comparison": "DV003: zero-shot approx. similar to monolingual. GPT-3.5-turbo: lower performance in some cases (e.g., XCOPA languages like Quechua and Haitian Creole), sometimes producing explicit failure outputs ('I'm sorry, but the premise is not in a language that I understand.'). Providing few-shot examples in the target language (monolingual prompting) grounded the model and mitigated such failures.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors hypothesize that for models with weaker cross-lingual representation or poorer instruction-following on certain languages, few-shot examples in English can fail to ground the model in the target language; in some low-resource languages the model may not detect or handle the input language without in-language exemplars.",
            "null_or_negative_result": true,
            "experimental_details": "Few-shot k=8, prompt templates in English; observation particularly pronounced for extremely low-resource languages and on gpt-3.5-turbo; DV003 sometimes robust likely due to differences in instruction-following and training.",
            "uuid": "e9444.1",
            "source_info": {
                "paper_title": "MEGA: Multilingual Evaluation of Generative AI",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Few-shot size (k) effect",
            "name_full": "Effect of number of few-shot examples (k) on prompting performance",
            "brief_description": "Investigation of how varying the number of in-context examples (k) influences model performance; authors vary k and measure stability/improvement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (primary reported), DV003 in some checks",
            "model_size": null,
            "task_name": "Subset of MEGA tasks (classification, commonsense reasoning; experiments shown in Figure 6)",
            "task_description": "Various classification and commonsense reasoning tasks; metrics task-dependent (accuracy/F1).",
            "presentation_format": "Few-shot in-context learning with varying k (number of exemplars); authors experiment with different k values and measure performance; default k used elsewhere is 8 (4 for longer-context tasks).",
            "comparison_format": null,
            "performance": "Authors report that increasing few-shot size generally improves performance, but gains saturate; performance often stable beyond k=8.",
            "performance_comparison": null,
            "format_effect_size": "Qualitative: improvement up to k=8 then diminishing returns; exact numeric deltas vary across tasks and not provided as a single consolidated percentage.",
            "explanation_or_hypothesis": "More in-context exemplars give stronger supervision signal for the task formatting and answer mapping, but returns diminish once the model has enough examples to infer the pattern; computational cost and context window limitations constrain increasing k.",
            "null_or_negative_result": false,
            "experimental_details": "Experiments summarized in Figure 6; typical default use in paper: k=8 for most tasks, k=4 for QA and summarization; prompts selected from PromptSource; cost considerations limited larger k for GPT-4.",
            "uuid": "e9444.2",
            "source_info": {
                "paper_title": "MEGA: Multilingual Evaluation of Generative AI",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Use of explanations in prompts",
            "name_full": "Including explanatory text (rationale) in few-shot exemplars",
            "brief_description": "Testing whether providing explanations or rationales in prompt exemplars improves multilingual model performance on reasoning/story tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo, text-davinci-003 (some checks)",
            "model_size": null,
            "task_name": "XStoryCloze, XCOPA (commonsense reasoning)",
            "task_description": "Predicting causal/commonsense relation or plausible story endings; accuracy is the metric reported.",
            "presentation_format": "Few-shot exemplars augmented with explanations/rationales (i.e., demonstration examples that include why the chosen answer is correct) versus standard few-shot exemplars without explanations.",
            "comparison_format": "Standard few-shot exemplars (no explanations) vs few-shot with explanations.",
            "performance": "Authors report negligible impact of using explanations on XStoryCloze and XCOPA performance (Figure 6 summary). No large gains observed in these experiments.",
            "performance_comparison": null,
            "format_effect_size": "Reported as negligible; no substantial percentage improvements cited.",
            "explanation_or_hypothesis": "Authors suggest that explanations in the prompt, at least in the formats tested for these multilingual, low-resource reasoning datasets, do not meaningfully change performance — indicating that existing prompting strategies (including adding explanations) may be insufficient to close multilingual performance gaps.",
            "null_or_negative_result": true,
            "experimental_details": "Experiments described in Section 4.1 and summarized in Figure 6; datasets: XStoryCloze and XCOPA; few-shot sizes as per default (k up to 8).",
            "uuid": "e9444.3",
            "source_info": {
                "paper_title": "MEGA: Multilingual Evaluation of Generative AI",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Prompt template selection (PromptSource tuning)",
            "name_full": "Prompt template selection and cross-language template reuse (PromptSource)",
            "brief_description": "Authors select English prompt templates from PromptSource by validation performance on English, then reuse the same templates for all target languages; they study effect and note potential limitations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DV003, gpt-3.5-turbo, gpt-4 (prompt templates chosen for latter based on gpt-3.5-turbo tuning)",
            "model_size": null,
            "task_name": "All MEGA tasks where prompting applied",
            "task_description": "Various classification, QA, sequence-labeling, and generation tasks; measured by accuracy/F1/EM/ROUGE-L as appropriate.",
            "presentation_format": "Prompt templates chosen from PromptSource by evaluating multiple English templates on the English validation set and selecting the best-performing template; that single English template is then applied across languages and prompting strategies (Monolingual, Translate-Test, Zero-shot).",
            "comparison_format": "Potential alternatives would be language-specific prompt tuning (not performed at scale due to cost) or using prompt templates written in target languages (not exhaustively tested here).",
            "performance": "No single numeric performance value; authors report the procedure and note it affects measured performance and is a sensitivity in multilingual prompting. They performed separate prompt-tuning for DV003 and gpt-3.5-turbo and reused gpt-3.5-turbo-chosen prompts for GPT-4 to save cost.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors caution that using English-optimized templates for all languages may bias results and that prompt-template choice can substantially influence performance; full language-specific tuning might improve results but was computationally prohibitive.",
            "null_or_negative_result": null,
            "experimental_details": "PromptSource used as repository; selection done on English validation split; templates kept in English across experiments; Appendix §A.7 contains note on impact of prompt-language choice.",
            "uuid": "e9444.4",
            "source_info": {
                "paper_title": "MEGA: Multilingual Evaluation of Generative AI",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Retrieve-then-prompt for QA (DV003)",
            "name_full": "Retrieval before prompting to fit QA context into small-context models (DV003)",
            "brief_description": "For DV003 (4096 token limit) the authors use a retrieval pipeline to include only the most relevant context chunk in prompt; they compare QA performance to models with larger context windows (gpt-3.5-turbo, gpt-4).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (DV003) vs gpt-3.5-turbo",
            "model_size": null,
            "task_name": "IndicQA and other extractive QA datasets (XQuAD, MLQA, TyDiQA-GoldP)",
            "task_description": "Span-prediction QA: given context and question, return answer span; metrics: F1 and Exact Match.",
            "presentation_format": "For DV003: few-shot examples provided with only the sentence line containing the answer; test context chunk selected via retrieval using embeddings (text-embedding-ada-002) and LangChain retrieval with max chunk size 100; for gpt-3.5-turbo and gpt-4 full context was provided (larger context windows).",
            "comparison_format": "DV003 with retrieve-then-prompt vs gpt-3.5-turbo/gpt-4 with direct full-context prompting.",
            "performance": "DV003 QA average (XQuAD) F1/EM: 40.5 / 28.0 (monolingual, Table 1). gpt-3.5-turbo XQuAD: 60.4 / 38.2. The authors attribute part of the DV003 deficit to imperfect retrieval needed to fit prompts in its smaller context window.",
            "performance_comparison": "Approximate effect: large drop for DV003 relative to gpt-3.5-turbo on QA tasks; eg. gpt-3.5-turbo XQuAD F1 60.4 vs DV003 40.5 (a ~20 percentage-point drop in F1), partly attributed to retrieval errors.",
            "format_effect_size": "Approximate: -19.9 F1 points for DV003 vs gpt-3.5-turbo on XQuAD (40.5 vs 60.4); some of this gap caused by different prompting/retrieval pipeline rather than pure model capability.",
            "explanation_or_hypothesis": "Smaller context window forced a retrieve-then-prompt pipeline for DV003; retrieval can be imperfect especially for low-resource languages, harming downstream QA performance. Models with larger context windows can include full passages and thus avoid retrieval errors.",
            "null_or_negative_result": true,
            "experimental_details": "DV003 context limit 4096 tokens required retrieval; retrieval used text-embedding-ada-002 embeddings and LangChain retrieval with chunk size 100; k for few-shot in QA set to 4; gpt-3.5-turbo and gpt-4 used full context (16k and 32k support respectively).",
            "uuid": "e9444.5",
            "source_info": {
                "paper_title": "MEGA: Multilingual Evaluation of Generative AI",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Tokenizer fertility / tokenization effects",
            "name_full": "Effect of tokenizer fertility (overtokenization) on multilingual performance",
            "brief_description": "Investigation of how tokenizer fertility (average number of sub-words per word) correlates with downstream multilingual performance; higher fertility indicates worse tokenizer quality for that language and model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI models (text-davinci-003, gpt-3.5-turbo, gpt-4), mBERT, BLOOM (comparison)",
            "model_size": null,
            "task_name": "Multiple MEGA tasks (Indic-XNLI / XNLI combined, others reported where correlation |ρ|&gt;0.7)",
            "task_description": "Classification and other tasks across languages; performance metrics task-dependent (accuracy/F1).",
            "presentation_format": "Not a prompt format per se, but a presentation-layer issue: input encoding/tokenization differs by language and model tokenizer, affecting prompt length, cost, and effective in-context signal.",
            "comparison_format": null,
            "performance": "Authors observe statistically significant negative correlations between tokenizer fertility and dataset-specific performance for six tasks (Figure 5). They show extreme fertility values (~10) for Malayalam and Tamil with OpenAI tokenizers, where tokenizer behaves effectively like a byte-level tokenizer, increasing token counts and harming performance.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Higher tokenizer fertility reduces effective model capacity for a given context window (more tokens used to represent same content), increases API cost, and correlates with worse accuracy/F1 on downstream tasks. Tokenizer quality and amount of pretraining data together explain some cross-language performance differences.",
            "null_or_negative_result": false,
            "experimental_details": "Tokenizer fertility measured as average number of sub-words per tokenized word; correlations reported where Pearson |ρ|&gt;0.7 with p-value 0.05; specific fertility examples: Malayalam and Tamil ~10 for OpenAI tokenizers; pretraining-data-token counts used as proxy (GPT-3 distribution) to further explain discrepancies.",
            "uuid": "e9444.6",
            "source_info": {
                "paper_title": "MEGA: Multilingual Evaluation of Generative AI",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Few-shot learning with multilingual generative language models",
            "rating": 2
        },
        {
            "paper_title": "Language models are multilingual chain-of-thought reasoners",
            "rating": 2
        },
        {
            "paper_title": "Do all languages cost the same? tokenization in the era of commercial language models",
            "rating": 2
        },
        {
            "paper_title": "PromptSource: An integrated development environment and repository for natural language prompts",
            "rating": 1
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 1
        }
    ],
    "cost": 0.0195425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MEGA: Multilingual Evaluation of Generative AI</h1>
<p>Kabir Ahuja ${ }^{\text {o<em> }} \quad$ Harshita Diddee $^{\circ </em>} \quad$ Rishav Hada ${ }^{\dagger}$ Millicent Ochieng ${ }^{\dagger}$ Krithika Ramesh ${ }^{\text {A* }}$ Prachi Jain ${ }^{\dagger}$ Akshay Nambi ${ }^{\dagger}$ Tanuja Ganu ${ }^{\dagger}$<br>Sameer Segal ${ }^{\dagger}$ Maxamed Axmed ${ }^{\dagger}$ Kalika Bali ${ }^{\dagger}$ Sunayana Sitaram ${ }^{\dagger}$<br>${ }^{\circ}$ University of Washington ${ }^{\circ}$ Carnegie Mellon University<br>${ }^{\dagger}$ Microsoft Corporation ${ }^{\text {J }}$ Johns Hopkins University<br>kahuja@cs.washington.edu, sunayana.sitaram@microsoft.com</p>
<h4>Abstract</h4>
<p>Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.</p>
<h2>1 Introduction</h2>
<p>Large Large Models (LLMs) such as ChatGPT and GPT-4 have created a lot of interest in the AI community and beyond, due to the step jump in their capabilities, such as maintaining context over conversations, fluency of generation, and reasoning. Many users have reported having tested these systems on languages other than English, with varying results, and recent demos of these models (Warren, 2023) have been shown in multiple (albeit highresource) languages. Recently, the GPT-4 model</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(OpenAI, 2023) was evaluated on the MMLU multiple choice questions benchmark by automatically translating it into 26 languages, and the results for some low-resource languages in the Latin script were found to be quite promising.</p>
<p>The multilingual capabilities of these models can be traced to their pre-training data, where even the predominantly English large-scale corpora contain hundreds of millions of non-English tokens (Blevins and Zettlemoyer, 2022). For GPT-3 unlabeled pre-training data has been documented to contain 119 languages (Brown et al., 2020), where roughly $93 \%$ of the tokens are in English ${ }^{1}$. Other LLMs like BLOOM (Scao et al., 2022) and PaLM (Chowdhery et al., 2022) have a better multilingual representation with $60 \%$ and $18 \%$ non-English data respectively for pre-training. While these models have been trained on multiple languages with varying distributions in the pre-training data, it is not clear how well they perform relative to each other across diverse tasks and languages due to a lack of comprehensive analysis across all models with the same experimental setup.</p>
<p>Recently, there has been a lot of interest in evaluating the different capabilities of LLMs, with comprehensive studies like HELM (Liang et al., 2022) that evaluate these models on a wide variety of capabilities. However, such studies are largely performed on English language data and there is a lack of such large-scale evaluation of LLMs for their multilingual capabilities. Given the current pace at which new language technologies are being developed that use LLMs, the importance of such an evaluation cannot be understated as the cases of inequalities in the performance of previousgeneration models across languages have been welldocumented (Blasi et al., 2022).</p>
<p>In our work, we present the first large-scale Multilingual Evaluation of Generative AI mod-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of our benchmarking exercise: Multilingual Evaluation of Generative AI (MEGA). Numbers in parentheses in Figure 1a contain the number of languages supported in the dataset.</p>
<p>els (MEGA), spanning 16 different datasets, 70 topologically diverse languages, and four LLMs i.e. GPT-3.5 models text-davinci-003 and gpt-3.5-turbo, GPT-4 (gpt-4-32k) and BLOOMZ (Muennighoff et al., 2022). We also compare these models with the models fine-tuned on these datasets like TULRv6 (Patra et al., 2022) and MuRIL (Khanuja et al., 2021), which are SoTA on different multilingual benchmarks.</p>
<p>Through our evaluation, we aim to answer three research questions. (1), how well do LLMs fare on multilingual benchmarks compared to fine-tuned SOTA models? (2), what languages do these models perform well in, and can we explain the trends in performance for these models across languages? (3), what prompting strategies should be used for using LLMs for non-English languages?</p>
<p>Our study highlights that there is a significant disparity between the performance of LLMs in English vs non-English languages, especially low-resource languages with non-Latin scripts for which fine-tuned models perform significantly better. While GPT-4 bridges this gap to some extent, the discrepancy still exists. Further, we find that for these languages it is often difficult to do better than simply machine translating the input in a target language to English and then sending it to the LLM for prediction (<em>translate-test</em>). We also discuss how different prompt-design choices like prompt-tuning, use of explanations, and number of few-shot examples impact multilingual performance. Finally, we perform some initial analysis to the test the possibility of test data contamination in LLMs that we evaluate and discuss its implications on our findings. Our work provides a blueprint for strategies that can be used for building systems using generative AI for multilingual users. We also release our code for the community to scale up the multilingual evaluation of generative models.</p>
<h1>2 MEGA</h1>
<p>In this section, we discuss different components of our benchmarking exercise to measure the multilingual capabilities of LLMs. We start by discussing different NLP tasks and datasets that we evaluate these models on, along with their linguistic diversity. We provide an overview of the models we evaluate, baselines for comparison, and describe our evaluation scheme and prompting strategies.</p>
<h3>2.1 Datasets and Languages</h3>
<p>We broadly consider five families of NLP tasks in our experiments covering 16 different datasets:</p>
<p><strong>Classification Tasks.</strong> Here, we further have four different sub-tasks, i) <em>Natural Language Inference</em> (classify if a hypothesis is entailed in the premise, contradicts it or neither), which includes XNLI (Conneau et al., 2018), Indic-XNLI (Aggarwal et al., 2022) (version of XNLI translated to 11 Indian languages), and GLUECos NLI (Khanuja et al., 2020b) for English-Hindi code-mixed data; ii) <em>Commonsense Reasoning</em> datasets including causal commonsense reasoning benchmark XCOPA (Ponti et al., 2020) and XStoryCloze (Lin et al., 2022a), where the correct ending of a story with four sentences is to be predicted; iii) <em>Paraphrase Identification</em> task PAWS-X (Yang et al., 2019a), where given two sentences, the</p>
<p><sup>2</sup>https://aka.ms/MEGA</p>
<p>model must predict if the two have the same meaning; iv) EN-ES-CS dataset for Sentiment Analysis on English-Spanish code-mixed tweets.
Question Answering (QA). For QA we consider Span-Prediction tasks, where the answer to a question is to be predicted within a piece of context provided. We evaluate on XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and IndicQA (Doddapaneni et al., 2022).
Sequence Labeling. This task involves classifying each token in a piece of text and we consider Named Entity Recognition dataset PAN-X (Pan et al., 2017) (also called WikiANN) and UDPOS (Nivre et al., 2018) for Part of Speech Tagging.
Natural Language Generation (NLG). For NLG we consider the multilingual Abstractive Summarization dataset XL-Sum.
Responsible AI (RAI). We consider the multilingual Toxicity Prediction dataset Jigsaw(Kivlichan et al., 2020), and Wino-MT to measure Gender Bias in MT systems.</p>
<p>All the datasets with the number of languages they include are listed in Figure 1a. These 16 datasets encompass a total of 70 languages covering 21 different language families, with IndoAryan and Afro-Asiatic languages in the majority (see Figure 1b). Note that for tasks with $&gt;30$ languages i.e. UDPOS, PAN-X, and XL-Sum, we run evaluations on the first 1000 examples of the test sets. For tasks where no public test sets are available (like XQUAD, TyDiQA-GoldP, and IndicQA), we evaluate on validation data. Refer to Appendix $\S$ A. 1 for a detailed description of all the datasets.</p>
<h3>2.2 Models</h3>
<p>OpenAI Models. We conduct all benchmarking experiments on the GPT-3.5 models text-davinci-003 (denoted as DV003 in the paper) and gpt-3.5-turbo (Ouyang et al., 2022) (GPT-3.5-Turbo) as well on the GPT4 model gpt-4-32k (OpenAI, 2023). The text-davinci-003 model has a maximum context size of 4096 tokens, while gpt-3.5-turbo and gpt-4-32k support context sizes of 16 k and 32 k respectively.
Baselines. We compare the performance of OpenAI models with two classes of baselines, i) Prompt-Based baselines, which like the OpenAI models are evaluated by prompting the model directly for solving a task, and ii) Fine-tuned Base-
lines, which are fine-tuned on task-specific training data. For the former we consider BLOOMZ (Muennighoff et al., 2022), a multi-task fine-tuned version of the BLOOM (Scao et al., 2022) model, which is a 176 billion parameter model trained on 46 natural languages and 13 programming languages. For fine-tuned baselines, we consider TULRv6 (Patra et al., 2022) (the current SoTA on XTREME benchmark), XLMR (Conneau et al., 2020), multilingual BERT (Devlin et al., 2019), and mT5 (Xue et al., 2021). For Indic-datasets we also compare with MuRIL(Khanuja et al., 2021), a multilingual BERT model trained on 16 Indic languages that obtains SOTA performance on many Indic benchmarks. All of these models (excluding mT5 for the XLSum and XCOPA), were fine-tuned with English data and then evaluated in a zero-cross-lingual fashion on other target languages.</p>
<h3>2.3 Evaluation Methodology</h3>
<p>LLMs exhibit two remarkable properties that make them effective at solving a variety of NLP tasks. The first is in-context learning (Brown et al., 2020), where the model learns to solve a task through the few input-output examples provided as part of the context without any weight updates. Secondly, the ability to follow instructions (Mishra et al., 2022; Wei et al., 2021; Ouyang et al., 2022) which is a property of instruction-tuned LLMs, where the models can be prompted to solve new-tasks based on the textual instructions provided in context.</p>
<p>We adopt these two techniques together to test the capabilities of LLMs to solve a variety of tasks in different languages. We define five main components to define the prompts: i) a test example $x_{\text {test }}$ for which the predictions are to be made; ii) $k$ few-shot exemplars $\left{\left(x_{i}, y_{i}\right)\right}<em _temp="{temp" _text="\text">{i=1}^{k}$, that are used to provide in-context supervision to the model; iii) a task instruction $\mathcal{I}$ which describes the instruction in text for the task to LLM; iv) a prompt template $f</em>(y)$ that maps the label $y$ to a textual representation. In our evaluation framework we often consider the instruction, template, and verbalizer as a single entity, and from now on will denote the template to encapsulate the three unless specified separately.}}(x)$ which turns a dataset input example into a text format that can be used for prompting; and v) an answer verbalizer $f_{\text {verb }</p>
<p>Given these components, the final prompt $f_{\text {prompt }}\left(x_{\text {test }} ;\left{\left(x_{i}, y_{i}\right)\right}<em _temp="{temp" _text="\text">{i=1}^{K}, \mathcal{I}, f</em>$ can}}, f_{\text {verb }}\right) \quad$ or $f_{\text {prompt }}\left(x_{\text {test }}\right)$ for short for a test input $x_{\text {test }</p>
<p>be defined as:</p>
<p>$$
\begin{aligned}
f_{\text {prompt }}\left(x_{\text {test }}\right)=\mathcal{I} |<em _temp="{temp" _text="\text">{i=1}^{K}\left{f</em>\right)\right} \
&amp; \left.| f_{\text {temp }}\left(x_{\text {test }}\right)\right.
\end{aligned}
$$}}\left(x_{i}\right)\right. &amp; \left.| f_{\text {verb }}\left(y_{i</p>
<p>where $|$ denotes the string concatenation operator. The prompt can then be provided as input to the LLM $P(.; \theta)$ to obtain the prediction $z_{\text {test }}=$ $\arg \max <em _prompt="{prompt" _text="\text">{z \in \mathcal{Z}} P\left(z \mid f</em>$ is the space of possible answers, which in all of our experiments is taken to be the entirety of the language as modeled by the LLM. We approximate the $\arg \max$ by sampling from the probability distribution predicted by the LLM.}}\left(x_{\text {test }}\right) ; \theta\right)$, where $\mathcal{Z</p>
<h3>2.3.1 Multilingual Prompting Strategies</h3>
<p>The choice of prompt significantly influences the performance of LLMs and these models have been shown to be brittle to simple prompting variations, such as the choice of prompt template and the training examples or even the ordering of examples (Zhao et al., 2021). For multilingual setups as highlighted in Lin et al. (2022a) and Shi et al. (2022), some additional variations to consider include, the choice of the language of the few-shot examples, the language of the prompt template, and the language of the test examples.</p>
<p>In this work, we evaluate models using three types of prompting strategies: Monolingual Prompting: In this setup, the $k$ randomly selected examples are of the same language as the test examples. Zero-Shot Cross-Lingual: Here, we evaluate generative models' zero-shot cross-lingual transfer ability during in-context learning. We use $k$-shot examples from a pivot language (always English in our experiments) which is different from the language of the test example. Translate-Test: In this setup also, the few-shot examples are sampled from English data. However, the test example itself is modified by translating it into English. We use Bing Translator to translate the test examples into English. We do not perform evaluations with Translate-Test prompting for QA and Sequence Labelling tasks where there is no trivial alignment between the labels in the translated text with native language text. To preserve costs, for GPT-4 we only run evaluations with the Monolingual prompting strategy except for a couple of datasets, which we explicitly discuss later in $\S 3$. Irrespective of the prompting strategy, we use the prompt templates written in English (see Appendix §A. 7 for the impact of this choice).</p>
<p>Prompt Tuning. We use PromptSource (Bach et al., 2022) for a database of existing prompts to use for our experiments. In order to select the best prompt for a dataset (to appropriately measure the capabilities of these models), we evaluate the performance of available English templates on PromptSource on the English validation set and select the prompt that gives the best performance. This prompt template is then used to evaluate models on the test sets for all languages and prompt strategies. While it would be ideal to tune the prompts separately for each language, the scale of our experiments and the computational costs of these models make it prohibitive. We investigate the impact this choice has on our results in $\S 4.1$. We perform separate prompt-tuning for DV003 and GPT-3.5-Turbo models, and to keep the costs in check, we use the prompts obtained for the latter for GPT-4 as well. Final prompts selected are included in Appendix §A.4.</p>
<p>Choice of Few-Shot Examples. In all our experiments, we choose few-shot examples randomly from the training or validation set (depending on what's available) of a dataset. For most datasets, we use 8 few-shot examples, excluding tasks with longer contexts like QA and summarization tasks where we use $k=4$.</p>
<h2>3 Results and Analysis</h2>
<p>In this section, we analyze the results of our benchmarking exercise across tasks and languages. Broadly, we cover the comparison between the effectivness of various prompting strategies $\S 3.1$ followed by the performance comparison of GPT-3.5 and GPT-4 models with appropriate baselines $\S 3.2$. We conclude with an examination of the factors that affects the performance of these models $\S 3.3$.</p>
<h3>3.1 Comparing different prompting strategies</h3>
<p>In Figure 2, we compare the performance of the three prompting strategies. We find that translatetest often improves the performance over the monolingual strategy, especially so in the case of DV003. We also find that for datasets, which include many low resource and non-latin script languages like IndicXNLI and XStoryCloze, the gains with translatetest are even more substantial for both the models. In Figure 3, we present the average (over different tasks) relative improvement by TranslateTest over Monolingual on GPT-3.5-Turbo for different languages and observe for languages like</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparing different prompting strategies discussed in §2.3.1 on DV003 and GPT-3.5-Turbo. The y-axis denotes the task-wise performance metric, e.g., Accuracy for XNLI and F1-Score for TyDiQA-GoldP. A list of metrics for all the tasks is provided in Table 1.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Relative percentage improvement over Monolingual prompting when using Translate-Test for GPT-3.5-Turbo. The bars are color-coded based on the class taxonomy provided in (Joshi et al., 2020)</p>
<p>Burmese, Tamil, and Telugu the relative improvement can be &gt; 30%! In general, we see that for low-resource languages, the translate-test results in substantial improvement in performance, while for high-resource languages the two perform similarly. While we do not evaluate GPT-4 Translate-Test exhaustively for all tasks, we do run the tests for XStoryCloze and XCOPA datasets. Based on these two, we observe that GPT-4's Monolingual prompting performance is often much more on-par with Translate-Test and many times even better. However, for low-resource languages we again see Translate-Test to perform much better, e.g., in XStoryCloze GPT-4's accuracy on Burmese is 77.6% vs 93.2% for Monolingual and Translate-Test respectively (Figures 10b and 10d in Appendix).</p>
<p>Note that while Translate-Test substantially improves performance on low-resource languages, compared to the performance of these models in English, the gap even after Translate-Test is significantly high. For example, using translate-test with GPT-3.5-Turbo for Urdu in XNLI results in 54% accuracy compared to 49.1% for monolingual. However, this contrasts with the 76.2% accuracy that the same model achieves in English.</p>
<p>Zero-Shot Cross-Lingual prompting for DV003 often performs on par with Monolingual but for GPT-3.5-Turbo, there is a drop in performance, especially so for tasks like XCOPA which have some extremely low resource languages: Quechua and Haitian Creole. For these languages, we observed that when provided few-shot examples in English, GPT-3.5-Turbo would often resort to predicting outputs like <em>"I'm sorry, but the premise is not in a language that I understand."</em> However, by providing examples in the language, we are able to ground the model to these languages and we almost never observe such predictions in that case.</p>
<h3>3.2 Comparing different models</h3>
<p>The aggregated results comparing different models and prompting strategies are provided in Table 1 and Table 7 (for Indic Datasets). Excluding the commonsense reasoning tasks XCOPA and XStoryCloze, the OpenAI models generally lag behind the fine-tuned baseline TULRv6 for most tasks often by a significant margin and often are only slightly better than some of the smaller fine-tuned multilingual models, i.e., mBERT and mT5-base. Between OpenAI models and BLOOMZ, the former models tend to outperform the latter (despite having a larger proportion of multilingual</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Classification</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Question Answering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sequence Labelling</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Summarization</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">XNLI</td>
<td style="text-align: center;">PAWS-X</td>
<td style="text-align: center;">XCOPA</td>
<td style="text-align: center;">XStoryCloze</td>
<td style="text-align: center;">XQuAD</td>
<td style="text-align: center;">TyDiQA-GoldP</td>
<td style="text-align: center;">MLQA</td>
<td style="text-align: center;">UDPOS</td>
<td style="text-align: center;">PAN-X</td>
<td style="text-align: center;">XLSum</td>
</tr>
<tr>
<td style="text-align: center;">Metrics</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">F1 / EM</td>
<td style="text-align: center;">F1 / EM</td>
<td style="text-align: center;">F1 / EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">BOUGE-L</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mBERT</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">64.5 / 49.4</td>
<td style="text-align: center;">59.7 / 43.9</td>
<td style="text-align: center;">61.4 / 44.2</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">mT5-Base</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">67.0 / 49.0</td>
<td style="text-align: center;">57.2 / 41.2</td>
<td style="text-align: center;">64.6 / 45.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">28.1 ${ }^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R Large</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">76.6 / 60.8</td>
<td style="text-align: center;">65.1 / 45.0</td>
<td style="text-align: center;">71.6 / 53.2</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">TuLRv6 - XXL</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">86 / 72.9</td>
<td style="text-align: center;">84.6 / 73.8</td>
<td style="text-align: center;">81 / 63.9</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Prompt-Based Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BLOOMZ</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">(82.2)</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">(70.7 / 58.8)</td>
<td style="text-align: center;">(75.2 / 63.2)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Open AI Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-803</td>
<td style="text-align: center;">59.27</td>
<td style="text-align: center;">67.08</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">40.5 / 28.0</td>
<td style="text-align: center;">49.7 / 38.3</td>
<td style="text-align: center;">44.0 / 28.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-803 (TT)</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">54.9 / 34.6</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">60.4 / 38.2</td>
<td style="text-align: center;">60.1 / 38.4</td>
<td style="text-align: center;">56.1 / 32.8</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">18.8</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo (TT)</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">46.3 / 27.0</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$16.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4-32k</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">68.3 / 46.6</td>
<td style="text-align: center;">71.5 / 50.9</td>
<td style="text-align: center;">67.2 / 43.3</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">19.7 ${ }^{\dagger}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Average performance across languages in each of the different datasets included in MEGA. TT suffix refers to the translate-test prompting strategy discussed in Section 2.3.1, without any suffix we refer to the monolingual strategy by default (except for XQuAD and IndicQA where it refers to cross-lingual setup). Numbers in bold with $\dagger$ symbol indicate best performing Fine-tuned model and the ones with $\ddagger$ refer to the best prompt-based generative model. The best overall numbers are underlined. For BLOOMZ the values in parenthesis indicate that the model was fine-tuned on the task during multi-task training. Missing values corresponding to the ' $x$ ' symbol denote experiments that were not applicable and the ones with '-' were the ones deprioritized due to limited compute. gpt-3.5-turbo (TT) on XL-Sum was only evaluated on 29 languages which are supported by Bing Translator.
pre-training data), except for datasets like PAWS-X, XQUAD, and TyDiQA-GoldP, where BLOOMZ performs better. However, it must be noted that all these three datasets were present in the multitask fine-tuning stage for BLOOMZ, especially for XQUAD and TyDiQA-GoldP for which the validation data that we use for evaluation is also likely to be included in the fine-tuning data ${ }^{3}$.</p>
<p>Between the OpenAI models, generally DV003 and GPT-3.5-Turbo perform on par, with TranslateTest performance of DV003 being generally better than GPT-3.5-Turbo, and the other way around for Monolingual performance. However, we do observe a notable exception to this, which is for the QA tasks where GPT-3.5-Turbo performs substantially better than DV003, especially so for IndicQA. We attribute this to the fact that in order to fit the prompt in the 4096 context size for DV003, we had to resort to retrive-then prompt strategy and imperfect retrieval for low-resource languages leads to worse performance. Please check §A. 5 of Appendix for more details on this. For GPT-4 on the other hand, we consistently observe substantial improvements, with it being Pareto Optimal (Choudhury and Deshpande, 2021) compared to the two GPT-3.5 models for all datasets with an exception of XL-Sum, where for some languages GPT-3.5-Turbo performs better. For the detailed</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>results spanning all models, tasks, and languages, please refer to Appendix §A.8.</p>
<h3>3.3 Factors Explaining Performance Trends</h3>
<p>In this section, we try to understand what factors influence our observed trends in multilingual LLM capabilities. We begin by investigating the Fertility of the tokenizers used by different models, which is defined as the average number of sub-words produced per tokenized word (higher means worse quality), as that has been shown to critically impact the downstream task performance of pre-trained multilingual models (Rust et al., 2021). In Figure 4, we plot the tokenizer fertility of different models. We observe that the tokenizers for the OpenAI models are substantially worse for low-resource, non-latin script languages: where the fertility for languages like Malayalam and Tamil is so high $(\sim 10)$ that the tokenizer essentially operates as a byte-level tokenizer for these languages. Note that this means that for low-resource languages, substantially larger number of tokens are needed to encode the inputs as well as for generation, which results in a significant additional API costs. Ahia et al. (2023) discusses how this phenomenon leads to large socio-economic disparities for speakers of underrepresented languages. We study if these discrepancies in the tokenizer's quality across languages have any effect on the performance. As can be seen in Figure 5, for six tasks we observe statis-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Tokenizer Fertility for OpenAI models, mBERT, and BLOOM for different languages</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Correlation between the performance of GPT-3.5-Turbo with the tokenizer fertility. We report the curves for the cases where the person coefficient |ρ| &gt; 0.7 with a p-value of 0.05. We have combined Indic-XNLI and XNLI for a better coverage of languages. Similar plots for GPT-4 can be found in Figure 7b of Appendix.</p>
<p>tically significant (negative) correlations between the tokenizer's fertility and dataset-specific performance i.e. the models obtain worse performance on languages for which the tokenizer is of poor quality, and vice-versa.</p>
<p>We also study the effect that the amount of data available for each language during pre-training (Wu and Dredze, 2020; Lauscher et al., 2020) has on the multilingual performance of these models. We measure the correlations between the language-wise number of tokens present in the pre-training data with language-wise performance on each dataset. While the exact language-wise pre-training data distribution for GPT-3.5 and GPT-4 models is not available, we use the GPT-3's language-wise pretraining distribution as a proxy. We observe that for four tasks (PAWS-X, XNLI, XCOPA, and XQuAD) statistically significant positive correlations between the pre-training data size and performance. Note that, the amount of pre-training data and tokenizer fertility are highly likely to be correlated with each other. However, we do see that using pre-training data we are able to explain some trends that are not explained by tokenizer fertility alone. For example, even though the OpenAI models have similar tokenizer fertilities for both French and Japanese, these models perform much better in French than they do for Japanese (72.1% accuracy vs 67% accuracy for GPT-3.5-Turbo) for PAWS-X. However, when we take into consideration the amount of pre-training data for these languages: roughly 3.5 B French tokens in the pre-training data versus 214M for Japanese, we can partially explain this discrepancy.</p>
<p>However, we must note that these two factors correlate well with only a subset of the tasks and what we are measuring is the correlation which might not imply causation. Investigating different factors that together more holistically explain multilingual capabilities is an important direction that we leave for future work. Please check Appendix §A.6 for detailed results from this section.</p>
<h2>4 Challenges in Multilingual Evaluation</h2>
<p>In this section, we examine some of the challenges and consequent limitations of a large-scale multilingual evaluation like ours.</p>
<h3>4.1 A Kaleidoscope of Choices.</h3>
<p>There are various moving parts when evaluating LLMs using prompting-based approaches, including the choice of prompt templates, instructions, and few-shot examples (Liu et al., 2022; Lu et al., 2022; Zhao et al., 2021), different prompting strategies (Wei et al., 2023; Nye et al., 2021; Ye and Durrett, 2022a), using external tools (Schick et al., 2023), the language of prompts (Shi et al., 2022; Lin et al., 2022a), as well as different decoding specific hyper-parameters (Shih et al., 2023), which can have varying degrees of impact on the performance, sometimes in unexpected ways. Holistic</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Analysing the effect on GPT-3.5-Turbo's performance given different evaluation factors. To obtain explanations we use Super-Natural Instructions (Wang et al., 2022).
cally exploring these choices for all the datasets and languages can quickly get out of hand, especially given the excessive computational cost of running these models. In order to understand the sensitivity of our observations to the choices we make in $\S 3$, we re-evaluate our setups on a subset of datasets and languages for a varying set of parameters. Our findings are summarized in Figure 6 , where we see that having a large few-shot size generally helps improve performance, however, the performance is often stable beyond $k=8$. Further, language-specific fine-tuning can help improve the performance like we see for Haitian Creole in XCOPA, but for Tamil we actually observe the accuracy to go down which might be attributed to the small size of the validation set ( 100 in the case of XCOPA). Finally, on XStoryCloze dataset (also for XCOPA), we see using explanations to prompt the models have negligible impact on the performance. Overall, these experiments indicate that the existing prompting approaches might not be sufficient to address the performance gap that exists for non-English languages (especially mid-tolow resource languages) and there is an imminent need to propose new methods as well as improve the representation of different languages in these model's pre-training (and instruction-tuning) data.</p>
<h3>4.2 Test data contamination</h3>
<p>Given the massive amount of online data that LLMs are trained with, it is critical to factor in the possibility of contamination of test datasets (Sainz et al.,
2023). Accordingly, we attempt to verify if the performances we observed are in fact, representative of the capabilities of these models or merely a result of memorization. Given the lack of transparency in the training distribution of recent models like GPT-4, we perform some preliminary investigations against this phenomenon. Specifically, we consider three factors: i) LLM's knowledge of the dataset, ii) availability of test datasets on the internet, and iii) dataset release date.</p>
<p>To measure the LLM's (we do this for GPT-4) memory of the dataset, we prompt it to fill the dataset cards for each of MEGA's datasets (denoted as Card Fill). This involves filling templatic information like the task's supported languages, inputoutput structure and description. If the model fills a dataset card correctly (Full), we note this as suspicion of contamination. If it fills the card partially correct (Partial) i.e. detecting either the correct task structure or correct set of languages, we mark it as partial evidence, and if it succeeds in neither, we mark it as no suspicion (None). For test dataset availability, we check if the test dataset can be accessed online directly without downloading either as part of the official release from the authors or via other sources such as Hugging Face dataset viewer (Data Acc. w/o Down.). For release date, we check if the dataset was made public after the cut-off date of September 2021.</p>
<p>The overall results from this analysis are provided in Table 2. We see that for a majority of datasets, GPT-4 can fill in the dataset card correctly; On the more recent datasets like XLSum and XStoryCloze it is only partially successful, while on Jigsaw and code-mixing datasets it fails to correctly fill the cards. Note that except XStoryCloze, Jigsaw and the Code-mixing datasets, evaluation sets for all other datasets are directly accessible online. Collectively, this connotes that for tasks like XStoryCloze and IndicQA there is a weak suspicion against contamination. While all other tasks are highly likely contaminated (except Jigsaw, and Code-Mixed datasets).</p>
<p>Implications. Our analysis implies a notable chance of the test data appearing in the training datasets of these LLMs. The contamination of test datasets is a serious problem for works centered around LLM evaluation (including ours), as they might lead to an overestimation of the capabilities of these models. However, we would like to highlight that despite the possibility of contamina-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Card Fill</th>
<th style="text-align: center;">Data Acc. w/o Down.</th>
<th style="text-align: left;">Release Date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">XNLI</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">September 2019</td>
</tr>
<tr>
<td style="text-align: left;">Indic-XNLI</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">April 2022</td>
</tr>
<tr>
<td style="text-align: left;">PAWS-X</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">August 2019</td>
</tr>
<tr>
<td style="text-align: left;">XCOPA</td>
<td style="text-align: center;">Partial</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">April 2020</td>
</tr>
<tr>
<td style="text-align: left;">XStoryCloze</td>
<td style="text-align: center;">Partial</td>
<td style="text-align: center;">No</td>
<td style="text-align: left;">May 2023</td>
</tr>
<tr>
<td style="text-align: left;">XQuAD</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">October 2019</td>
</tr>
<tr>
<td style="text-align: left;">MLQA</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">October 2019</td>
</tr>
<tr>
<td style="text-align: left;">TyDiQA-GoldP</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">February 2020</td>
</tr>
<tr>
<td style="text-align: left;">IndicQA</td>
<td style="text-align: center;">Partial</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">September 2022</td>
</tr>
<tr>
<td style="text-align: left;">PAN-X</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">July 2017</td>
</tr>
<tr>
<td style="text-align: left;">UDPOS</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">March 2020</td>
</tr>
<tr>
<td style="text-align: left;">XLSum</td>
<td style="text-align: center;">Partial</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: left;">June 2021</td>
</tr>
<tr>
<td style="text-align: left;">Jigsaw</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">No</td>
<td style="text-align: left;">February 2020</td>
</tr>
<tr>
<td style="text-align: left;">GLUECos NLI</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">No</td>
<td style="text-align: left;">June 2020</td>
</tr>
<tr>
<td style="text-align: left;">EN-ES-CS</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">No</td>
<td style="text-align: left;">May 2016</td>
</tr>
</tbody>
</table>
<p>Table 2: Contamination analysis for the datasets that we consider in MEGA. We use red color when there is a strong suspicion of contamination based on these three metrics, green for no suspicion, and yellow for partial.
tion, LLMs still vastly underperform on (especially low-resource) non-English languages . These observations about data contamination indicate that the disparity in performance between English and non-English languages might be even greater than what we observe in our work.</p>
<h2>5 Related Work</h2>
<p>Evaluation of LLMs. A growing interest in the evaluation of LLMs has harbingered several efforts towards the holistic evaluation of their capabilities. While work like BIG-bench Srivastava et al. (2023) cover a diverse range of tasks, the non-English tasks are mostly translation-oriented which limit the more general task based inferences that for such an evaluation. Similarly, Liang et al. (2022) propose a taxonomy of scenarios and metrics in Holistic Evaluation of Language Models (HELM) to define the space of LLM evaluation, and evaluate 30 language models on 42 scenarios and 7 metrics. However, all the scenarios are focused on datasets in standard English or its dialects.</p>
<h2>Multilingual Benchmarks and Evaluation.</h2>
<p>Benchmarks for multilingual evaluation, such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021) and XGLUE (Liang et al., 2020) have been proposed to measure cross-lingual transfer in pre-trained language models. Following their popularity, there has been the development of benchmarks covering specific language families, such as IndicXTREME (Doddapaneni et al., 2022) for Indian languages, Adelani et al. (2022) for African Languages, and Wilie et al. (2020) for Indonesian languages, as well. The evaluations on these bench-
marks have mainly focused on pre-train then finetune kinds of setups. Particularly for prompting style evaluation, Bang et al. (2023) evaluates the multilingual capabilities of ChatGPT and shows that it fails to generalize to low-resource languages with non-latin scripts. However, multilingual evaluation is performed only on a few tasks, and a subset of 50-100 examples are used for testing the model. Hendy et al. (2023) evaluate the translation abilities of GPT-3.5 models and find that these models, while perform well in translating high-resource languages, their capabilities for low-resource languages are limited. Concurrent work BUFFET (Asai et al., 2023) and Lai et al. (2023) also perform multilingual benchmarking of large language models, however, they evaluate the performance of ChatGPT and BLOOMZ in their work while our evaluation also spans GPT-4.</p>
<p>Multilingual Prompting: While most work on prompting or in-context learning in LLMs focuses on English data, recently, there has been some interest in prompting them with non-English data. Zhao and Schütze (2021), for instance, use discrete and soft prompting techniques to evaluate XLMRoBERTa and show that prompting can be more effective compared to fine-tuning when the amount of labeled data is limited. Lin et al. (2022a) show that English prompts perform better than prompts written in the target language (both hand-written and translated). Finally, (Shi et al., 2022) show chain-of-thought (CoT) prompting results leads to striking multilingual reasoning capabilities in LLMs, even in under-represented languages especially when prompted when English CoT.</p>
<h2>6 Conclusion</h2>
<p>In this work, we conduct an evaluation across different prompting strategies, models, tasks, and languages to investigate the multilingual capabilities of LLMs. We also investigate underlying properties like tokenizer quality and size of pretraining data to explain the trends in performance that we observe. Our investigation shows the consistent performance gap between high-resource, Latin script, and underresourced languages in addition to highlighting the efficacy, yet limited sufficiency of methods like translate-test prompting. Through our evaluation, we present evidence of the need to prioritize automatic benchmarking and human evaluation across as many languages as possible. We hope that this work spurs research in meeting this goal.</p>
<h2>Limitations</h2>
<p>Although we compare the evaluation results of GPT-3.5 and GPT-4 with BLOOMZ and SOTA models, we could not evaluate other closed models such as PaLM, which also contains training data in many languages. A limitation of our study is that we do not evaluate on all the multilingual datasets that are available, and we plan to scale up our evaluation in future versions of the study with the help of the research community. Even if we do evaluate all available multilingual datasets, they do not cover many typologically diverse and under-resourced languages, which is a fundamental limitation of trying to scale up multilingual evaluation today. For example, there is very little representation from African languages, Indigenous languages of the Americas etc. in any of the evaluation benchmarks available today. Finally, we restrict ourselves to the performance metrics and to some extent gender bias dimension of evaluation for this study - however, we plan to include evaluation of calibration, toxicity, bias, robustness, etc. in future work.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank Barun Patra and Vishrav Chaudhary for their help with TULR evaluation results. We also thank the anonymous reviewers for their helpful feedback, which helped us improve the quality of our paper.</p>
<h2>References</h2>
<p>David Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael Beukman, Chester PalenMichel, Constantine Lignos, Jesujoba Alabi, Shamsuddeen Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa Bukula, Rooweither Mabuya, Bonaventure F. P. Dossou, Blessing Sibanda, Happy Buzaaba, Jonathan Mukiibi, Godson Kalipe, Derguene Mbaye, Amelia Taylor, Fatoumata Kabore, Chris Chinenye Emezue, Anuoluwapo Aremu, Perez Ogayo, Catherine Gitau, Edwin MunkohBuabeng, Victoire Memdjokam Koagne, Allahsera Auguste Tapo, Tebogo Macucwa, Vukosi Marivate, Mboning Tchiaze Elvis, Tajuddeen Gwadabe, Tosin Adewumi, Orevaoghene Ahia, Joyce Nakatumba-Nabende, Neo Lerato Mokono, Ignatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa Oluwaseun Adeyemi, Gilles Quentin Hacheme, Idris Abdulmumin, Odunayo Ogundepo, Oreen Yousuf, Tatiana Moteu, and Dietrich Klakow. 2022. MasakhaNER 2.0: Africa-centric transfer learning for named entity recognition. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4488-4508, Abu</p>
<p>Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Divyanshu Aggarwal, Vivek Gupta, and Anoop Kunchukuttan. 2022. Indicxnli: Evaluating multilingual inference for indian languages. arXiv preprint arXiv:2204.08776.</p>
<p>Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, and Yulia Tsvetkov. 2023. Do all languages cost the same? tokenization in the era of commercial language models. ArXiv, abs/2305.13707.</p>
<p>Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623-4637.</p>
<p>Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. 2023. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. arXiv cs.CL 2305.14857.</p>
<p>Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Alshaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. 2022. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 93-104, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.</p>
<p>Damian Blasi, Antonios Anastasopoulos, and Graham Neubig. 2022. Systematic inequalities in language technology performance across the world's languages. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5486-5505, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Terra Blevins, Hila Gonen, and Luke Zettlemoyer. 2022. Prompting language models for linguistic structure. arXiv cs.CL 2211.07830.</p>
<p>Terra Blevins and Luke Zettlemoyer. 2022. Language contamination helps explains the cross-lingual capabilities of English pretrained models. In Proceedings</p>
<p>of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3563-3574, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165.</p>
<p>Monojit Choudhury and Amit Deshpande. 2021. How linguistically fair are multilingual pre-trained language models? In AAAI-21. AAAI, AAAI.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454-470.</p>
<p>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451 .</p>
<p>Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of EMNLP 2018, pages 2475-2485.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, pages 41714186.</p>
<p>Sumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M Khapra, Anoop Kunchukuttan, and Pratyush Kumar. 2022. Indicxtreme: A multi-task benchmark for evaluating indic languages. arXiv preprint arXiv:2212.05409.
A. Seza Doğruöz, Sunayana Sitaram, Barbara E. Bullock, and Almeida Jacqueline Toribio. 2021. A survey of code-switching: Linguistic and social perspectives for language technologies. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1654-1666, Online. Association for Computational Linguistics.</p>
<p>Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M Sohel Rahman, and Rifat Shahriyar. 2021a. Xlsum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693-4703.</p>
<p>Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021b. XLsum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693-4703, Online. Association for Computational Linguistics.</p>
<p>Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210.</p>
<p>Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International Conference on Machine Learning, pages 4411-4421. PMLR.</p>
<p>Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282-6293, Online. Association for Computational Linguistics.</p>
<p>Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal, Rajiv Teja Nagipogu, Shachi Dave, et al. 2021. Muril: Multilingual representations for indian languages. arXiv preprint arXiv:2103.10730.</p>
<p>Simran Khanuja, Sandipan Dandapat, Sunayana Sitaram, and Monojit Choudhury. 2020a. A new dataset for natural language inference from codemixed conversations. In Proceedings of the The 4th Workshop on Computational Approaches to Code Switching, pages 9-16.</p>
<p>Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, and Monojit Choudhury.</p>
<p>2020b. Gluecos: An evaluation benchmark for codeswitched nlp. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3575-3585.</p>
<p>Ian Kivlichan, Jeffrey Sorensen, Julia Elliott, Lucy Vasserman, Martin Görner, and Phil Culliton. 2020. Jigsaw multilingual toxic comment classification.</p>
<p>Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning.</p>
<p>Anne Lauscher, Vinit Ravishankar, Ivan Vulić, and Goran Glavaš. 2020. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4483-4499, Online. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020. Mlqa: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 73157330 .</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.</p>
<p>Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, et al. 2020. Xglue: A new benchmark dataset for cross-lingual pretraining, understanding and generation. arXiv preprint arXiv:2004.01401.</p>
<p>Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022a. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019-9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. 2022b. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019-9052.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086-8098, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849, San Diego, California. Association for Computational Linguistics.</p>
<p>Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. 2017. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages $46-51$.</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Allum Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generalization through multitask finetuning.</p>
<p>Akshay Nambi, Vaibhav Balloli, Mercy Ranjit, Tanuja Ganu, Kabir Ahuja, Sunayana Sitaram, and Kalika Bali. 2023. Breaking language barriers with a leap: Learning strategies for polyglot llms. arXiv cs.CL 2305.17740 .</p>
<p>Joakim Nivre, Mitchell Abrams, Željko Agić, Lars Ahrenberg, Lene Antonsen, Maria Jesus Aranzabe, Gashaw Arutie, Masayuki Asahara, Luma Ateyah, Mohammed Attia, et al. 2018. Universal dependencies 2.2 .</p>
<p>Maxwell I. Nye, Anders Johan Andreassen, Guy GurAri, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. CoRR, abs/2112.00114.</p>
<p>OpenAI. 2023. Gpt4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1946-1958.</p>
<p>Barun Patra, Saksham Singhal, Shaohan Huang, Zewen Chi, Li Dong, Furu Wei, Vishrav Chaudhary, and Xia Song. 2022. Beyond english-centric bitexis for better multilingual language representation learning. arXiv preprint arXiv:2210.14867.</p>
<p>Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić, and Anna Korhonen. 2020. Xcopa: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362-2376.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016a. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016b. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical formalizations of commonsense reasoning, pages 90-95.</p>
<p>Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, et al. 2021. Xtreme-r: Towards more challenging and nuanced multilingual evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10215-10245.</p>
<p>Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 8-14, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna Gurevych. 2021. How good is your tokenizer? on the monolingual performance of multilingual language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3118-3135, Online. Association for Computational Linguistics.</p>
<p>Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, and Eneko Agirre. 2023. Did chatgpt cheat on your test?</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools.</p>
<p>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2022. Language models are multilingual chain-of-thought reasoners. CoRR, abs/2210.03057.</p>
<p>Andy Shih, Dorsa Sadigh, and Stefano Ermon. 2023. Long horizon temperature scaling.</p>
<p>Sunayana Sitaram, Khyathi Raghavi Chandu, Sai Krishna Rallabandi, and Alan W Black. 2019. A survey of code-switched speech and language processing. arXiv preprint arXiv:1904.00784.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli,</p>
<p>Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo JaimovitchLópez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia ContrerasOchando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng</p>
<p>He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Sferish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xi-</p>
<p>aoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.</p>
<p>Gabriel Stanovsky, Noah A Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679-1684.</p>
<p>Mike Thelwall. 2017. The heart and soul of the web? sentiment strength detection in the social web with sentistrength. Cyberemotions: Collective emotions in cyberspace, pages 119-134.</p>
<p>David Vilares, Miguel A Alonso, and Carlos GómezRodríguez. 2016. En-es-cs: An english-spanish codeswitching twitter corpus for multilingual sentiment analysis. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 4149-4153.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. EMNLP 2018, page 353 .</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Tom Warren. 2023. Microsoft's chatgpt event live blog.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. CoRR, abs/2109.01652.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. arXiv 2201.11903 cs.CL.</p>
<p>Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, et al. 2020. Indonlu: Benchmark and resources for evaluating indonesian natural language understanding. arXiv preprint arXiv:2009.05387.</p>
<p>Shijie Wu and Mark Dredze. 2020. Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120-130, Online. Association for Computational Linguistics.</p>
<p>Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498.</p>
<p>Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019a. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of EMNLP 2019, pages 3685-3690.</p>
<p>Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019b. Paws-x: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3687-3692.</p>
<p>Xi Ye and Greg Durrett. 2022a. Can explanations be useful for calibrating black box models? In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6199-6212, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Xi Ye and Greg Durrett. 2022b. The unreliability of explanations in few-shot prompting for textual reasoning. In Advances in Neural Information Processing Systems, volume 35, pages 30378-30392. Curran Associates, Inc.</p>
<p>Daniel Zeman, Joakim Nivre, Mitchell Abrams, Elia Ackermann, Noëmi Aepli, Hamid Aghaei, and R Ziane. 2020. Universal dependencies 2.5. LINDAT/CLARIAHCZ digital library at the Institute of Formal and Applied Linguistics (UFAL), Faculty of Mathematics and Physics, Charles University. url: http://hdl. handle. net/11234/1-3226.</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. 2019. Paws: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298-1308.</p>
<p>Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018. Gender bias in coreference resolution: Evaluation and debiasing</p>
<p>methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15-20, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Mengjie Zhao and Hinrich Schütze. 2021. Discrete and soft prompting for multilingual models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8547-8555, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.</p>
<h2>A Appendix</h2>
<h2>A. 1 Tasks and Datasets</h2>
<p>In our experiments, we consider 16 tasks spanning the following task types - classification, sequence to sequence labeling and generation. Below we review the experimental setups and datasets used for benchmarking for these two tasks. A list of all the datasets with the languages covered by them can be found in Table 3.</p>
<h2>A.1.1 Classification</h2>
<p>These tasks involve classifying a single sentence or a group of sentences into a finite number of discrete labels. For each dataset, we measure the performance of different models in terms of classification accuracy. For prompt-based models in particular, since we add no constraint on the output space of the LLM we compute the exact match between the generated output and a verbalized label to determine if the example was classified correctly. We run experiments for all the prompting strategies that we discussed in the previous sections for each dataset. The details of each dataset that we use for benchmarking are given below:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">XNLI</td>
<td style="text-align: left;">Natural Language Inference</td>
<td style="text-align: left;">15</td>
</tr>
<tr>
<td style="text-align: left;">Indic-XNLI</td>
<td style="text-align: left;">Natural Language Inference</td>
<td style="text-align: left;">11</td>
</tr>
<tr>
<td style="text-align: left;">GLUECoS</td>
<td style="text-align: left;">Natural Language Inference</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">PAWS-X</td>
<td style="text-align: left;">Paraphrase Identification</td>
<td style="text-align: left;">7</td>
</tr>
<tr>
<td style="text-align: left;">XCOPA</td>
<td style="text-align: left;">Commonsense Reasoning</td>
<td style="text-align: left;">10</td>
</tr>
<tr>
<td style="text-align: left;">XStoryCloze</td>
<td style="text-align: left;">Commonsense Reasoning</td>
<td style="text-align: left;">11</td>
</tr>
<tr>
<td style="text-align: left;">TyDiQA-GoldP</td>
<td style="text-align: left;">Question Answering</td>
<td style="text-align: left;">9</td>
</tr>
<tr>
<td style="text-align: left;">MLQA</td>
<td style="text-align: left;">Question Answering</td>
<td style="text-align: left;">6</td>
</tr>
<tr>
<td style="text-align: left;">XQuAD</td>
<td style="text-align: left;">Question Answering</td>
<td style="text-align: left;">11</td>
</tr>
<tr>
<td style="text-align: left;">IndicQA</td>
<td style="text-align: left;">Question Answering</td>
<td style="text-align: left;">10</td>
</tr>
<tr>
<td style="text-align: left;">UDPOS</td>
<td style="text-align: left;">Part of Speech Tagging</td>
<td style="text-align: left;">38</td>
</tr>
<tr>
<td style="text-align: left;">PANX</td>
<td style="text-align: left;">NER</td>
<td style="text-align: left;">48</td>
</tr>
<tr>
<td style="text-align: left;">WinoMT</td>
<td style="text-align: left;">Gender Bias</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">GLUECoS</td>
<td style="text-align: left;">Sentiment Analysis</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Jigsaw</td>
<td style="text-align: left;">Toxicity Classification</td>
<td style="text-align: left;">6</td>
</tr>
<tr>
<td style="text-align: left;">XLSum</td>
<td style="text-align: left;">Summarization</td>
<td style="text-align: left;">44</td>
</tr>
</tbody>
</table>
<p>Table 3: Datasets and Language coverage of the datasets that MEGA presents evaluation for.</p>
<ol>
<li>Natural Language Inference: XNLI (Conneau et al., 2018) is a dataset for cross-lingual Natural Language Inference, which consists of professional translations of the MNLI (Wang et al., 2018) corpus into 14 languages. We also consider IndicXNLI (Aggarwal et al., 2022) that translates the XNLI dataset into 11 Indic languages by using Machine Translation, followed by validation by native speakers.</li>
<li>Paraphrase Identification: PAWS-X (Yang et al., 2019b) is a paraphrase identification dataset professionally translated from the PAWS (Zhang et al., 2019) dataset into six typologically diverse languages.</li>
<li>Commonsense Reasoning: XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset, which is a translation of the COPA (Roemmele et al., 2011) dataset into 11 typologically diverse languages, including very low-resource languages such as Eastern Apurímac Quechua and Haitian Creole.</li>
</ol>
<p>XStoryCloze (Lin et al., 2022b) is created by translating the English StoryCloze (Mostafazadeh et al., 2017) dataset using professional translators into 10 typologically diverse languages.</p>
<h2>A.1.2 Question Answering</h2>
<p>We focus on Span Prediction type of Question Answering (QA) tasks in our experiments, where given a context and a question the task is to predict the answer within the context. One major challenge that we come across for multilingual evaluation of QA tasks is that for many languages we often cannot fit the context and question pairs for the fewshot and text examples in the maximum context size of 4096 for the DV003 model. This is mainly attributed to the poor performance of GPT's tokenizer on many non-latin script languages which results in over-tokenizing the words in these languages.</p>
<p>To overcome this issue we follow two steps. First, for the few-shot examples we only provide the line within the paragraph containing the answer as the context. Second, for the test example, we index the chunks of the context using the embeddings from the text-embedding-ada-002 model. Given the question, the closest chunk in the full context is retrieved and used in the prompt for the test example. We use a maximum chunk size of 100 in our experiments and use the implementation for retrieval provided in the LangChain ${ }^{4}$ library. By doing this,we minimize the space taken by the context tokens in our prompt.</p>
<p>Note that, for newer GPT models i.e. GPT-3.5Turbo and GPT-4 which support longer context lengths, we do not use this retrieval strategy for QA tasks and prompt the models to obtain the answers directly. For each task, we calculate the Exact Match and F1 score as defined in Rajpurkar</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>et al. (2016a). For our experiments we consider the following four tasks:</p>
<ol>
<li>TyDiQA (Clark et al., 2020) is a QA dataset covering 11 typologically diverse languages. The task consists of two sub-tasks - passage selection and minimum answer span (Gold-P). For our experiments, we consider the Gold-P task and evaluate Monolingual and Zero-Shot Cross-Lingual prompting strategies. Since the labels do not directly transfer one-to-one across translation for QA tasks as they do for classification and require the use of alignment algorithms, we skip translate-test prompting for this task.</li>
<li>MLQA (Lewis et al., 2020) is an extractive QA dataset translated into 7 languages by professional translators. The task has two variants, the first where the question, context, and answer are all in the same language; and the second, where the question is in a different language than the context and answer. We consider the former variant of the task in our experiments. For MLQA, translatetest splits are also available, where each language's test data has been translated into English with answers aligned using the attention scores. There is no training data available for MLQA, and we use SQuAD'sRajpurkar et al. (2016a) training data for selecting few-shot examples in English and validation data for MLQA in other languages to get their few-shot examples. This way, we are able to evaluate for all three prompting setups.</li>
<li>XQuAD (Artetxe et al., 2020) consists of professional translations of a subset of the SQuaD dataset (Rajpurkar et al., 2016b) into 10 languages. XQuAD only has validation datasets available publicly, hence we evaluate the models on them. Like MLQA we use English SQuAD data for few-shot examples and since we cannot use validation data in other languages for few-shot, we only evaluate for zero-shot cross-lingual setup for this task.</li>
<li>IndicQA (Doddapaneni et al., 2022) is a manually curated cloze-style reading comprehension dataset that can be used for evaluating questionanswering models in 11 Indic languages. The context paragraphs are chosen from Wikipedia articles whose topics are closely related to Indic culture, history,etc. The publicly available test set has about 2000 sentences that we carry out our evaluation on.</li>
</ol>
<h2>A. 2 Sequences Labeling</h2>
<p>In the sequence labeling task, a sequence of tokens (such as words) to be labeled are provided to the
system.</p>
<h2>A.2.1 Part of Speech Tagging</h2>
<p>UDPOS (Zeman et al., 2020) is a dataset for Part of Speech Tagging taken from the Universal Dependencies 2.5 from the XTREME (Hu et al., 2020) benchmark. We benchmark a subset of the languages available in UDPOS.</p>
<h2>A.2.2 Named Entity Recognition</h2>
<p>PANX (Pan et al., 2017) or WikiANN is a Named Entity Recognition dataset consisting of Wikipedia sentences tagged with Person, Organization and Location.</p>
<p>For both tasks we use the linguistic structure prompting approach of Blevins et al. (2022) to define the prompts. The exact prompts used can be found in $\S$ A.4. Given the nature of both tasks, which would involve token alignment across the translation, we do not evaluate the translate-test prompting strategies for these setups. Also, since both tasks involve $&gt;30$ languages, to make the best use of the compute resources we only evaluate GPT-3.5-Turbo in a monolingual setup for these two tasks. Finally, we evaluate the first 1000 examples for each language for these datasets given the large number of languages. We have recomputed all baselines with this specification as well.</p>
<h2>A. 3 Generation</h2>
<h2>A.3.1 Summarization</h2>
<p>The XLSum (Hasan et al., 2021a) dataset contains article-summary pairs across 44 typologically diverse languages, ranging from high to very lowresource.</p>
<p>For a similar reason as the tagging datasets, we only evaluate on first 1000 examples of the test sets in different languages and recompute the baselines on the same testset using the weights of the XLSUM pretrained model, opensourced by the authors (Hasan et al., 2021b).</p>
<h2>A.3.2 Code-switching datasets</h2>
<p>All the datasets we consider so far are monolingual, however, a majority of the world's population speaks more than one language, leading to language contact phenomena such as code-switching (Doğruöz et al., 2021; Sitaram et al., 2019). We include two code-switching datasets in MEGA to benchmark the performance of generative models.</p>
<p>GLUECoS-NLI (Khanuja et al., 2020a) is a code-mixed NLI dataset in Hindi-English, consist-</p>
<p>ing of Bollywood (Hindi) movie conversations as premises, with manually created hypotheses.</p>
<p>The EN-ES-CS Sentiment Analysis dataset (Vilares et al., 2016), part of the GLUECoS benchmark (Khanuja et al., 2020b) is a code-mixed dataset consisting of English-Spanish Tweets annotated with SentiStrength (Thelwall, 2017) scores.</p>
<h2>A.3.3 RAI datasets</h2>
<p>We include two datasets that measure the Responsible AI (RAI) dimensions of fairness and toxicity - Jigsaw ${ }^{5}$ for toxic comment classification and WinoMT for gender bias.</p>
<p>The Jigsaw dataset contains online comments sourced from Wikipedia. The training data, which is in English, contains labels pertaining to the toxicity of the comment and any relevant identity mentions contained in the comment. We use the test dataset, which contains these comments for 6 languages as illustrated in Table 3 for evaluation. The test dataset contains a binary label indicating whether or not the comment is toxic. Our objective is to assess the performance of these models across multiple languages and observe the disparity in this performance that could arise due to a number of factors, a prominent one being the source data that these models are trained on. Using English prompts from PromptSource for the original monolingual Jigsaw task, we task the model with classifying a comment as toxic or non-toxic. We perform crosslingual few-shot prompting and translate-test experiments for the test sets of all 6 languages, and report the results excluding content violations in Table 21.</p>
<p>The WinoMT dataset (Stanovsky et al., 2019) is created by concatenating the WinoGender (Rudinger et al., 2018) and WinoBias (Zhao et al., 2018) datasets. WinoMT dataset consists of 3888 English sentences with equal distribution of Male and Female genders. It is also equally balanced between stereotypical and non-stereotypical gender role assignments. We follow the method as reported by (Stanovsky et al., 2019) in their paper. We perform zero-shot monolingual prompting of all sentences in the dataset to translate them in 8 target languages. Further using fast_align we map the English entity to its translation. Finally, we extract the target-side entity's using off the shelf tools for each target language. The extracted translated</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>gender can be finally compared against the gold annotations for English.</p>
<h2>A. 4 Prompts</h2>
<h2>A.4.1 XNLI, IndicXNLI, GLUECoS NLI</h2>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an NLP assistant whose purpose is to solve Natural Language Inference (NLI) problems. NLI is the task of determining the inference relation between two (short, ordered) texts: entailment, contradiction, or neutral. Answer as concisely as possible in the same format as the examples below:</p>
<p>Template $f_{\text {temp }}$ :
{premise}
Question: {hypothesis}
True, False, or Neither?
Verbalizer $f_{\text {verb }}$ :
Entailment : True,
Contradiction: False,
Neutral: Neither
Models : DV003
Template $f_{\text {temp }}$ :
{premise} Based on previous passage is it true that {hypothesis} ? Yes, No, or Maybe?</p>
<p>Verbalizer $f_{\text {verb }}$ :
Entailment : Yes,
Contradiction: No,
Neutral: Maybe</p>
<h2>A.4.2 PAWS-X</h2>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an NLP assistant whose purpose is to perform Paraphrase Identification. The goal of Paraphrase Identification is to determine whether a pair of sentences have the same meaning. Answer as concisely as possible in the same format as the examples below:</p>
<p>Template $f_{\text {temp }}$ :
${$ sentence1}
Question: {sentence2}
True or False?
Models : DV003
Template $f_{\text {temp }}$ :
Sentence 1: {sentence1} Sentence 2: {sentence2} Question: Does Sentence 1 paraphrase Sentence 2 ? Yes or No?</p>
<p>Verbalizer $f_{\text {verb }}$ :
Positive: Yes
Negative: No</p>
<h2>A.4.3 XCOPA</h2>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an AI assistant whose purpose is to perform open-domain commonsense causal reasoning. You will be provided a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. Answer as concisely as possible in the same format as the examples below:</p>
<p>Template $f_{\text {temp }}$ :
{ premise }
{\% if question == "cause" \%} This happened because...
{\% else \%} As a consequence... {\% endif \%} Help me pick the more plausible option: {choice1} - {choice2}</p>
<h2>Models : DV003</h2>
<p>Template $f_{\text {temp }}$ :
{ premise }
{\% if question == "cause" \%} This happened because...
{\% else \%} As a consequence... {\% endif \%} Help me pick the more plausible option: - choice1: {choice1}, choice2: {choice2}</p>
<p>Verbalizer $f_{\text {verb }}$ :
choice1: {choice1}
choice2: {choice2}</p>
<h2>A.4.4 XQUAD, TyDiQA, MLQA</h2>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.
Template $f_{\text {temp }}$ :
{context}
Q: {question}
Referring to the passage above, the correct answer to the given question is: {answer}</p>
<h2>Models : DV003</h2>
<p>Template $f_{\text {temp }}$ :
{context}
Q: {question}
Referring to the passage above, the correct answer to the given question is: {answer}</p>
<h2>A.4.5 IndicQA</h2>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.
Template $f_{\text {temp }}$ :
{context}
Q: {question}
Referring to the passage above, the correct answer to the given question is? If you can't find the answer, please respond "unanswerable". {answer}</p>
<h2>Models : DV003</h2>
<p>Template $f_{\text {temp }}$ :
{context}
Q: {question}
Referring to the passage above, the correct answer to the given question is: {answer}</p>
<h2>A.4.6 XStoryCloze</h2>
<p>Models : DV003, GPT-3.5-Turbo, GPT-4
Template $f_{\text {temp }}$ :
{input_sentence_1} {input_sentence_2}
{input_sentence_3} {input_sentence_4}
What is a possible continuation for the story given the following options?
Option1: {sentence_quiz1} Option2:
{sentence_quiz2}
Verbalizer $f_{\text {verb }}$ :
{sentence_quiz1}: Option1,
{sentence_quiz2}: Option2</p>
<h2>A.4.7 PANX</h2>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an NLP assistant whose purpose is to perform Named Entity Recognition (NER). NER involves identifying and classifying named entities in a text into predefined categories such as person names, organizations, locations, and others. You will need to use the tags defined below: O means the word doesn't correspond to any entity. B-PER/I-PER means the word corresponds to the</p>
<p>beginning of/is inside a person entity. B-ORG/IORG means the word corresponds to the beginning of/is inside an organization entity. B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity. Do not try to answer the question! Just tag each token in the sentence.
Template $f_{\text {temp }}:\left{\right.$ token_1 token_2 ... token_n}
Verbalizer $f_{\text {verb }}:$
{tag_1} {tag_2} ... {tag_n}: {token_1}<em>{tag_1} {token_2}</em>{tag_2}
... {token_n}_{tag_n}</p>
<h3>1.4.8 UDPOS</h3>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an NLP assistant whose purpose is to perform Part of Speech (PoS) Tagging. PoS tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. You will need to use the tags defined below:</p>
<ol>
<li>ADJ: adjective</li>
<li>ADP: adposition</li>
<li>ADV: adverb</li>
<li>AUX: auxiliary</li>
<li>CCONJ: coordinating-conjunction</li>
<li>DET: determiner</li>
<li>INTJ: interjection</li>
<li>NOUN: noun</li>
<li>NUM: numeral</li>
<li>PART: particle</li>
<li>PRON: pronoun</li>
<li>PROPN: proper-noun</li>
<li>PUNCT: punctuation</li>
<li>SCONJ: subordinating-conjunction</li>
<li>SYM: symbol</li>
<li>VERB: verb</li>
<li>X: other</li>
</ol>
<p>Template $f_{\text {temp }}:\left{\right.$ token_1 token_2 ... token_n}
Verbalizer $f_{\text {verb }}:$
{tag_1} {tag_2} ... {tag_n}: {token_1}<em>{tag_1} {token_2}</em>{tag_2} ... {token_n}_{tag_n}</p>
<h3>1.4.9 GLUECoS Sentiment Analysis</h3>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an NLP assistant whose purpose is to solve Sentiment Analysis problems. Sentiment Analysis is the task of determining whether the sentiment, opinion or emotion expressed in a textual data is: positive, negative, or neutral. Answer as concisely as possible in the same format as the examples below:
Template $f_{\text {temp }}$ : Does the following sentence have a positive, negative or neutral sentiment? {text}</p>
<h2>Models : DV003</h2>
<p>Template $f_{\text {temp }}$ : Does the following sentence have a positive, negative or neutral sentiment? {text}</p>
<h3>1.4.10 XLSum</h3>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an NLP assistant whose purpose is to summarize any given article. You should summarize all important information concisely in the same language in which you have been provided the document. Following the examples provided below:
Template $f_{\text {temp }}:$
{document}
$==$
Write a summary of the text above :</p>
<h3>1.4.11 Jigsaw</h3>
<p>Models : GPT-3.5-Turbo, GPT-4
Task Instruction $\mathcal{I}$ : You are an NLP assistant whose purpose is to solve the Jigsaw Toxicity Prompt problem. Toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. The objective is to identify toxic comments. Answer as concisely as possible in the same format as the examples below:
Template $f_{\text {temp }}:{$ text $}$ Should this online comment be removed for its toxicity? Yes or No?</p>
<h2>Models : DV003</h2>
<p>Template $f_{\text {temp }}:{$ text $}$ Someone posted this comment online. Do you find it toxic? Yes or No?</p>
<h3>1.4.12 WinoMT</h3>
<p>Models : GPT-3.5-Turbo, GPT-4
Template $f_{\text {temp }}$ : Translate the following English text to {target_language}: {sentence}</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://www.kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification/data&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ https://github.com/openai/gpt-3/blob/master/ dataset_statistics/languages_by_word_count.csv&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8041 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8041</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8041</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-48c83799530dc523ee01e6c1c40ad577d5c10a16</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/48c83799530dc523ee01e6c1c40ad577d5c10a16" target="_blank">DiscoveryBench: Towards Data-Driven Discovery with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The first comprehensive benchmark that formalizes the multi-step process of data-driven discovery is presented, which illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.</p>
                <p><strong>Paper Abstract:</strong> Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations across task complexity. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only 25%. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8041.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8041.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HMS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis Matching Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A composite, model-based metric (0–100) that measures alignment between a predicted and gold hypothesis by decomposing both into sub-hypotheses, matching contexts, and evaluating variable and relationship alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various evaluated LLMs (GPT-4o, GPT-4-0125-preview, Llama-3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (closed and open models)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (sociology, biology, humanities, economics, engineering, meta-science)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis (natural-language hypotheses produced by LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hypothesis Matching Score (HMS)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Decompose gold and predicted hypotheses into sub-hypotheses using a GPT-4-based evaluator; match sub-hypothesis contexts; compute context F1 (ctx_F1), variable F1 (var_F1) per matched sub-hypothesis, and relationship accuracy (rel_acc); combine these into a single HMS score via a weighted formula.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS (0–100), composed from ctx_F1, var_F1, rel_acc</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>HMS ∈ [0,100] computed as: HMS = ctx_F1(hp,hg) * (1/|M|) * Σ_i (var_F1(hp_i,hg_i) * rel_acc(hp_i,hg_i)), where ctx_F1 and var_F1 are F1 scores (0–1), rel_acc is scored as 100 (exact match), 50 (predicted relation is broader but encompasses gold), or 0 (different), and M is set of context-matched sub-hypothesis pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench (DB-REAL and DB-SYNTH)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None — evaluation is automated using a GPT-4-based evaluator (gpt-4-preview-0125); tasks and gold workflows were manually curated by authors but scoring is automated.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Overall best agent peaked at ~24.5% HMS (Reflexion (Oracle) with GPT-4o); best non-oracle agents in DB-REAL ranged ~12–16% HMS; DB-SYNTH peak ~23.2% (Reflexion with Llama-3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on a model-based evaluator (GPT-4) which may introduce evaluator bias; open-ended nature of hypotheses can make matching ambiguous; rel_acc heuristic (100/50/0) is coarse; outcome-based evaluation hides alternative valid discovery paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8041.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ctx_F1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context F1 score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An F1 score measuring alignment between contexts of sub-hypotheses in predicted vs. gold decompositions, used as the context-matching component of HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluator: gpt-4-preview-0125 (used to decide context equivalence)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-preview-0125</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis decomposition / alignment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Context F1 (ctx_F1)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Treat matched sub-hypothesis contexts as units and compute F1 between the set of predicted contexts and gold contexts (precision/recall of matched contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ctx_F1 (0–1, used multiplicatively in HMS)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard F1 (harmonic mean of precision and recall) over context matches determined by the GPT-4-based matching prompt; used as a weight for combined HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None (context equivalence judged by GPT-4-based evaluator prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper shows correlation between higher ctx_F1 and better variable/relation alignment; specific ctx_F1 distributions reported in aggregate plots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Context equivalence is judged by an LLM (subjective/fuzzy); errors in context matching propagate to HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8041.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>var_F1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variable F1 score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An F1 score measuring alignment between the sets of variables (columns) mentioned in matched sub-hypotheses of predicted vs. gold hypotheses; uses fuzzy matching to allow paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluator: gpt-4-preview-0125</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-preview-0125</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis variable alignment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Variable F1 (var_F1)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Extract variable sets for each sub-hypothesis using the evaluator; compute F1 between predicted and gold variable sets (with fuzzy matching for paraphrases) for each matched pair.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>var_F1 (0–1)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard token/set-based F1 between two variable sets per matched sub-hypothesis; averaged across matched pairs inside HMS formula.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None — variable extraction and fuzzy matching are performed by the GPT-4-based evaluator prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as a multiplicative component in HMS; aggregated var_F1 values plotted against ctx_F1 in paper analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Fuzzy variable matching may conflate distinct concepts; variable extraction depends on evaluator accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8041.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>rel_acc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relationship accuracy (rel_acc) scoring heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A coarse scoring heuristic (100/50/0) that judges whether the predicted relationship matches the gold relationship exactly, is a broader/generalized form that encompasses the gold relationship, or is different.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluator: gpt-4-preview-0125 (decides relation match category)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-preview-0125</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>relationship alignment within hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>rel_acc (relationship accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluator judges relation in predicted sub-hypothesis relative to gold: 100 if exact match, 50 if predicted is broader but encompasses gold, 0 otherwise; these values are multiplied by var_F1 in HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>rel_acc (discrete scores 100, 50, 0 used inside HMS)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Categorical scoring per matched sub-hypothesis: exact match=100, broader but encompassing=50, different=0.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None — relationship category decided by GPT-4-based prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as part of HMS; analyses show relationship accuracy declines for more complex workflows and domain-specific analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Coarse 3-level scoring loses nuance of relationship similarity; relies on evaluator judgments which can be subjective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8041.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-based hypothesis evaluator (gpt-4-preview-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-4 variant prompt-based evaluator used to decompose hypotheses, extract contexts/variables/relations, decide context equivalence, and compute components of HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-preview-0125</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>preview-0125</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (used as an evaluator across DiscoveryBench tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automated hypothesis evaluation/decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Model-based evaluator using GPT-4 (decomposition + matching prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A suite of prompts (decomposition, matching, variable extraction, relation comparison) executed on gpt-4-preview-0125 to produce structured representations of hypotheses and judgments used to compute HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Provides outputs used to compute ctx_F1, var_F1, rel_acc and thus HMS</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Produces structured JSON answers (sub-hypotheses, match booleans, variable counts, relation categories) which are converted into F1/accuracy components of HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None for scoring; authors manually curated gold hypotheses/workflows but scoring is automated via GPT-4-based prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to evaluate all agent outputs on DiscoveryBench; specific errors/limitations in evaluator noted in discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Evaluator introduces potential bias/instability; reliance on a single high-capability model for scoring can mask evaluator errors; prompts and evaluator model version are critical for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8041.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new benchmark (dataset + tasks + evaluation) formalizing multi-step data-driven discovery; contains real (DB-REAL) and synthetic (DB-SYNTH) tasks to evaluate LLM-based discovery agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (benchmark used to evaluate LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (sociology, biology, humanities, economics, engineering, meta-science)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark for hypothesis search & verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Model-based evaluation using HMS (GPT-4 evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Tasks consist of dataset(s), metadata, and a natural-language discovery goal; LLM agents produce hypotheses/workflows which are scored by the GPT-4-based evaluator yielding HMS and component metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS (0–100) with component metrics ctx_F1, var_F1, rel_acc</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS definition; benchmark reports agent mean/aggregate HMS across tasks and breakdowns by domain and workflow complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench (DB-REAL: 264 tasks; DB-SYNTH: 903 synthetic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Gold tasks and workflows were manually extracted/replicated from published papers and verified by the authors; no separate human judging of LLM outputs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Best agent performance peaks at ~25% HMS; detailed per-agent/per-model values reported (e.g., Reflexion (Oracle) GPT-4o 24.5% on DB-REAL).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Excludes forecasting/simulation-heavy tasks and very large multimodal datasets; evaluation depends on model-based evaluator; real-world reproducibility and false-discovery (p-hacking) risks acknowledged.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8041.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DB-REAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiscoveryBench - Real tasks (DB-REAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Component of DiscoveryBench containing 264 tasks collected from >20 published papers across six domains, with manually derived gold workflows and hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>sociology, biology, humanities, economics, engineering, meta-science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>real-world discovery tasks (hypotheses and workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Evaluated on LLM agents using HMS (GPT-4-based evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Agents are given dataset(s), metadata, and a goal; their NL hypotheses/workflows are scored against gold with HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS and components (ctx_F1, var_F1, rel_acc)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS definition</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DB-REAL (part of DiscoveryBench)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Gold workflows/hypotheses were implemented and verified manually by authors (replication efforts took many person-hours).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Agent HMS on DB-REAL: e.g., CodeGen (GPT-4o) 15.5, ReAct (GPT-4o) 15.4, DataVoyager 15.4, Reflexion (Oracle) 24.5 (GPT-4o); NoDataGuess baseline often low.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Subset of domains (excluded some simulation/forecasting-heavy natural/physical science tasks); replication was time-consuming and sometimes unsuccessful for original papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8041.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DB-SYNTH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiscoveryBench - Synthetic tasks (DB-SYNTH)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic component of DiscoveryBench with 903 tasks across 48 domains, generated via LLM-guided semantic-tree construction and data generation to enable controlled variation of difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (benchmark subset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>48 synthetic domains (diverse; used to mimic real workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>synthetic discovery tasks/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Evaluated using HMS (GPT-4-based evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Tasks synthesized by constructing semantic hypothesis trees, generating pandas expressions for derived variables, sampling leaf distributions, introducing noise/missingness, and forming goals by masking dimensions; agent outputs scored by HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS (0–100) and component metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS definition</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DB-SYNTH (DiscoveryBench synthetic split, 903 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Synthetic tasks generated with LLMs and validated by authors; no human scoring of outputs beyond automated evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Performance on DB-SYNTH similar to DB-REAL; Reflexion (Oracle) reaches up to ~23.2% HMS with Llama-3; non-oracle agents lower.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Synthetic tasks may not capture all complexities of real-world data; generation relies on LLMs and heuristics for noise/missingness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8041.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGen agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen discovery agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discovery agent that generates the full analysis code in one shot (demonstration provided), runs it, then produces an NL hypothesis and workflow summary based on results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (applies across DiscoveryBench tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automated hypothesis generation via code execution</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Agent framework evaluated with HMS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generates end-to-end code to perform data analysis; outputs are converted into NL hypothesis which is scored by HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS (and ctx_F1/var_F1/rel_acc components)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS definition</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench (DB-REAL and DB-SYNTH)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human scoring; authors provided demonstrations and agent code is run automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>DB-REAL CodeGen HMS: GPT-4o 15.5, GPT-4p 16.3, Llama-3 12.1; DB-SYNTH CodeGen lower in some model pairings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>One-shot code generation can fail on complex workflows requiring iterative analysis; depends on environment execution fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8041.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reason + Act) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multi-turn agent that interleaves reasoning (thought) steps with actions (code generation/execution), enabling stepwise discovery attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>iterative hypothesis generation and verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ReAct agent framework evaluated using HMS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generates thought + action traces across turns to design and run analyses; final NL hypothesis scored by HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS and components</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS definition</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human scoring; agent traces are executed automatically</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>DB-REAL ReAct HMS: GPT-4o 15.4, GPT-4p 15.6, Llama-3 13.5; DB-SYNTH performance generally lower than CodeGen in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Multi-turn reasoning did not outperform single-shot CodeGen in this benchmark; struggles with complex domain-specific analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8041.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DataVoyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DataVoyager agent (multi-component discovery agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-component system (planner, code generator, data analysis, critic) intended to orchestrate discovery; adapted here as an LLM-based discovery agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data-driven discovery with large generative models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>planned and critiqued hypothesis generation & analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>DataVoyager framework evaluated with HMS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Planner creates analysis plan, code generator implements it, data analysis executed, critic evaluates and iterates; final hypothesis scored by HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS and component metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS definition</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human scoring for agent outputs; some runs include domain-knowledge hints to the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>DB-REAL DataVoyager HMS (GPT-4o) ~15.4; adding domain knowledge improved performance in targeted archaeology tasks (jump from 9.9% to 17.5% in one reported case).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Planner/critic loop did not strongly outperform simpler agents; sensitive to domain-knowledge hints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8041.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion(Oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (Oracle feedback) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of CodeGen that receives oracle HMS feedback after each trial and generates reflections to improve in subsequent trials (up to 3), simulating learning from feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>iterative improvement of hypotheses via feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reflexion with oracle HMS feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>After a trial, the agent is provided the (oracle) HMS score and asked to reflect; it then reattempts the task up to a max of 3 trials to improve the final hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS (improvement across trials)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported final HMS after iterative improvement attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Oracle feedback is automated (HMS); no human judges in feedback loop.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reflexion (Oracle) substantially improved over base CodeGen (e.g., Reflexion GPT-4o 24.5% vs CodeGen GPT-4o 15.5% on DB-REAL).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Oracle HMS feedback is not available in real-world settings; improvement depends on fidelity of the HMS evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8041.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NoDataGuess</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NoDataGuess baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that attempts to guess the hypothesis using only dataset metadata and the goal (no access to the actual dataset), assessing memorization or prior-knowledge leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>baseline hypothesis generation (no data access)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>NoDataGuess baseline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Agent produces hypothesis using dataset description and goal only; output scored by HMS to measure whether models memorize or infer from metadata alone.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DiscoveryBench (DB-REAL)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>NoDataGuess HMS: GPT-4o 0.0 (refused to hallucinate), GPT-4p 4.7, Llama-3 11.5 on DB-REAL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Measures memorization/overfitting to known published results; some models may refuse to guess, producing zero scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8041.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>National Longitudinal Surveys (NLS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Public longitudinal socioeconomic datasets used as source data for several DB-REAL tasks and gold workflow replications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>sociology / economics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>source dataset for discovery tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Used as task datasets in DB-REAL; agent outputs scored by HMS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Original study workflows over NLS were replicated to form gold hypotheses/tasks; agents analyze NLS-derived datasets to generate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS (used to score agent outputs on NLS-derived tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Part of DB-REAL within DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Authors spent many person-hours replicating workflows and verifying gold hypotheses from NLS-based studies.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Replication from original papers was time-consuming and sometimes unsuccessful; domain-specific preprocessing required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8041.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GBIF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Global Biodiversity Information Facility (GBIF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open biodiversity occurrence data source used for some DB-REAL tasks (e.g., invasion of introduced plants) and workflow replications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>ecology / biology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>source dataset for discovery tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Used as task datasets in DB-REAL; agent outputs scored by HMS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Authors replicated published workflows using GBIF data to form DB-REAL tasks; agents analyze these datasets and are judged with HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Part of DB-REAL</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Replication sometimes required domain knowledge and specialized tools; took up to 90 person-hours for GBIF-derived tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Complex domain-specific preprocessing and spatial analyses (e.g., spatial autocorrelation) often required; many attempted replications failed due to missing code/data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8041.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WBOD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Bank Open Data (WBOD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Public economic and development indicators from the World Bank used as source datasets for some DB-REAL tasks (e.g., education vs GDP analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>economics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>source dataset for discovery tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Used in DB-REAL tasks and evaluated with HMS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Workflows from papers using WBOD were replicated to construct tasks whose hypotheses are scored by HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HMS</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See HMS</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Part of DB-REAL</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Authors verified workflows; some tasks required integrating multiple WBOD indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires careful preprocessing and selection of indicators; some domain knowledge needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8041.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e8041.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QRData</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QRData (related benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior dataset/benchmark targeting advanced quantitative/statistical reasoning (mostly textbook-style questions) with unique numeric gold answers; cited for contrast with DiscoveryBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistical / causal analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>statistical/causal question-answering benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>QRData benchmark (mentioned for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Targets well-defined statistical/causal problems with unique numeric gold answers, evaluated against numeric ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Numeric accuracy vs gold answers (unique numeric targets)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Primarily numeric error/accuracy relative to gold answers</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>QRData (reference/comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>N/A (benchmark with numeric gold answers)</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>QRData has well-defined numeric targets; DiscoveryBench differs by being open-ended and requiring pipeline design and semantic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Data-driven discovery with large generative models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data <em>(Rating: 2)</em></li>
                <li>Hypothesis: On the scope of scientific hypotheses <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8041",
    "paper_id": "paper-48c83799530dc523ee01e6c1c40ad577d5c10a16",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "HMS",
            "name_full": "Hypothesis Matching Score",
            "brief_description": "A composite, model-based metric (0–100) that measures alignment between a predicted and gold hypothesis by decomposing both into sub-hypotheses, matching contexts, and evaluating variable and relationship alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various evaluated LLMs (GPT-4o, GPT-4-0125-preview, Llama-3-70B)",
            "model_size": "various (closed and open models)",
            "scientific_domain": "multi-domain (sociology, biology, humanities, economics, engineering, meta-science)",
            "theory_type": "hypothesis (natural-language hypotheses produced by LLMs)",
            "evaluation_method_name": "Hypothesis Matching Score (HMS)",
            "evaluation_method_description": "Decompose gold and predicted hypotheses into sub-hypotheses using a GPT-4-based evaluator; match sub-hypothesis contexts; compute context F1 (ctx_F1), variable F1 (var_F1) per matched sub-hypothesis, and relationship accuracy (rel_acc); combine these into a single HMS score via a weighted formula.",
            "evaluation_metric": "HMS (0–100), composed from ctx_F1, var_F1, rel_acc",
            "metric_definition": "HMS ∈ [0,100] computed as: HMS = ctx_F1(hp,hg) * (1/|M|) * Σ_i (var_F1(hp_i,hg_i) * rel_acc(hp_i,hg_i)), where ctx_F1 and var_F1 are F1 scores (0–1), rel_acc is scored as 100 (exact match), 50 (predicted relation is broader but encompasses gold), or 0 (different), and M is set of context-matched sub-hypothesis pairs.",
            "dataset_or_benchmark": "DiscoveryBench (DB-REAL and DB-SYNTH)",
            "human_evaluation_details": "None — evaluation is automated using a GPT-4-based evaluator (gpt-4-preview-0125); tasks and gold workflows were manually curated by authors but scoring is automated.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Overall best agent peaked at ~24.5% HMS (Reflexion (Oracle) with GPT-4o); best non-oracle agents in DB-REAL ranged ~12–16% HMS; DB-SYNTH peak ~23.2% (Reflexion with Llama-3).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Relies on a model-based evaluator (GPT-4) which may introduce evaluator bias; open-ended nature of hypotheses can make matching ambiguous; rel_acc heuristic (100/50/0) is coarse; outcome-based evaluation hides alternative valid discovery paths.",
            "uuid": "e8041.0",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ctx_F1",
            "name_full": "Context F1 score",
            "brief_description": "An F1 score measuring alignment between contexts of sub-hypotheses in predicted vs. gold decompositions, used as the context-matching component of HMS.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluator: gpt-4-preview-0125 (used to decide context equivalence)",
            "model_size": "gpt-4-preview-0125",
            "scientific_domain": "multi-domain",
            "theory_type": "hypothesis decomposition / alignment",
            "evaluation_method_name": "Context F1 (ctx_F1)",
            "evaluation_method_description": "Treat matched sub-hypothesis contexts as units and compute F1 between the set of predicted contexts and gold contexts (precision/recall of matched contexts).",
            "evaluation_metric": "ctx_F1 (0–1, used multiplicatively in HMS)",
            "metric_definition": "Standard F1 (harmonic mean of precision and recall) over context matches determined by the GPT-4-based matching prompt; used as a weight for combined HMS.",
            "dataset_or_benchmark": "DiscoveryBench",
            "human_evaluation_details": "None (context equivalence judged by GPT-4-based evaluator prompts)",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Paper shows correlation between higher ctx_F1 and better variable/relation alignment; specific ctx_F1 distributions reported in aggregate plots.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Context equivalence is judged by an LLM (subjective/fuzzy); errors in context matching propagate to HMS.",
            "uuid": "e8041.1",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "var_F1",
            "name_full": "Variable F1 score",
            "brief_description": "An F1 score measuring alignment between the sets of variables (columns) mentioned in matched sub-hypotheses of predicted vs. gold hypotheses; uses fuzzy matching to allow paraphrases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluator: gpt-4-preview-0125",
            "model_size": "gpt-4-preview-0125",
            "scientific_domain": "multi-domain",
            "theory_type": "hypothesis variable alignment",
            "evaluation_method_name": "Variable F1 (var_F1)",
            "evaluation_method_description": "Extract variable sets for each sub-hypothesis using the evaluator; compute F1 between predicted and gold variable sets (with fuzzy matching for paraphrases) for each matched pair.",
            "evaluation_metric": "var_F1 (0–1)",
            "metric_definition": "Standard token/set-based F1 between two variable sets per matched sub-hypothesis; averaged across matched pairs inside HMS formula.",
            "dataset_or_benchmark": "DiscoveryBench",
            "human_evaluation_details": "None — variable extraction and fuzzy matching are performed by the GPT-4-based evaluator prompts.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used as a multiplicative component in HMS; aggregated var_F1 values plotted against ctx_F1 in paper analyses.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Fuzzy variable matching may conflate distinct concepts; variable extraction depends on evaluator accuracy.",
            "uuid": "e8041.2",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "rel_acc",
            "name_full": "Relationship accuracy (rel_acc) scoring heuristic",
            "brief_description": "A coarse scoring heuristic (100/50/0) that judges whether the predicted relationship matches the gold relationship exactly, is a broader/generalized form that encompasses the gold relationship, or is different.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluator: gpt-4-preview-0125 (decides relation match category)",
            "model_size": "gpt-4-preview-0125",
            "scientific_domain": "multi-domain",
            "theory_type": "relationship alignment within hypotheses",
            "evaluation_method_name": "rel_acc (relationship accuracy)",
            "evaluation_method_description": "Evaluator judges relation in predicted sub-hypothesis relative to gold: 100 if exact match, 50 if predicted is broader but encompasses gold, 0 otherwise; these values are multiplied by var_F1 in HMS.",
            "evaluation_metric": "rel_acc (discrete scores 100, 50, 0 used inside HMS)",
            "metric_definition": "Categorical scoring per matched sub-hypothesis: exact match=100, broader but encompassing=50, different=0.",
            "dataset_or_benchmark": "DiscoveryBench",
            "human_evaluation_details": "None — relationship category decided by GPT-4-based prompt.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used as part of HMS; analyses show relationship accuracy declines for more complex workflows and domain-specific analyses.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Coarse 3-level scoring loses nuance of relationship similarity; relies on evaluator judgments which can be subjective.",
            "uuid": "e8041.3",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4-eval",
            "name_full": "GPT-4-based hypothesis evaluator (gpt-4-preview-0125)",
            "brief_description": "A GPT-4 variant prompt-based evaluator used to decompose hypotheses, extract contexts/variables/relations, decide context equivalence, and compute components of HMS.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4-preview-0125",
            "model_size": "preview-0125",
            "scientific_domain": "multi-domain (used as an evaluator across DiscoveryBench tasks)",
            "theory_type": "automated hypothesis evaluation/decomposition",
            "evaluation_method_name": "Model-based evaluator using GPT-4 (decomposition + matching prompts)",
            "evaluation_method_description": "A suite of prompts (decomposition, matching, variable extraction, relation comparison) executed on gpt-4-preview-0125 to produce structured representations of hypotheses and judgments used to compute HMS.",
            "evaluation_metric": "Provides outputs used to compute ctx_F1, var_F1, rel_acc and thus HMS",
            "metric_definition": "Produces structured JSON answers (sub-hypotheses, match booleans, variable counts, relation categories) which are converted into F1/accuracy components of HMS.",
            "dataset_or_benchmark": "DiscoveryBench",
            "human_evaluation_details": "None for scoring; authors manually curated gold hypotheses/workflows but scoring is automated via GPT-4-based prompts.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Used to evaluate all agent outputs on DiscoveryBench; specific errors/limitations in evaluator noted in discussion.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Evaluator introduces potential bias/instability; reliance on a single high-capability model for scoring can mask evaluator errors; prompts and evaluator model version are critical for reproducibility.",
            "uuid": "e8041.4",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "DiscoveryBench",
            "name_full": "DiscoveryBench",
            "brief_description": "A new benchmark (dataset + tasks + evaluation) formalizing multi-step data-driven discovery; contains real (DB-REAL) and synthetic (DB-SYNTH) tasks to evaluate LLM-based discovery agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (benchmark used to evaluate LLMs)",
            "model_size": "",
            "scientific_domain": "multi-domain (sociology, biology, humanities, economics, engineering, meta-science)",
            "theory_type": "benchmark for hypothesis search & verification",
            "evaluation_method_name": "Model-based evaluation using HMS (GPT-4 evaluator)",
            "evaluation_method_description": "Tasks consist of dataset(s), metadata, and a natural-language discovery goal; LLM agents produce hypotheses/workflows which are scored by the GPT-4-based evaluator yielding HMS and component metrics.",
            "evaluation_metric": "HMS (0–100) with component metrics ctx_F1, var_F1, rel_acc",
            "metric_definition": "See HMS definition; benchmark reports agent mean/aggregate HMS across tasks and breakdowns by domain and workflow complexity.",
            "dataset_or_benchmark": "DiscoveryBench (DB-REAL: 264 tasks; DB-SYNTH: 903 synthetic tasks)",
            "human_evaluation_details": "Gold tasks and workflows were manually extracted/replicated from published papers and verified by the authors; no separate human judging of LLM outputs reported.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Best agent performance peaks at ~25% HMS; detailed per-agent/per-model values reported (e.g., Reflexion (Oracle) GPT-4o 24.5% on DB-REAL).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Excludes forecasting/simulation-heavy tasks and very large multimodal datasets; evaluation depends on model-based evaluator; real-world reproducibility and false-discovery (p-hacking) risks acknowledged.",
            "uuid": "e8041.5",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "DB-REAL",
            "name_full": "DiscoveryBench - Real tasks (DB-REAL)",
            "brief_description": "Component of DiscoveryBench containing 264 tasks collected from &gt;20 published papers across six domains, with manually derived gold workflows and hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A",
            "model_size": "",
            "scientific_domain": "sociology, biology, humanities, economics, engineering, meta-science",
            "theory_type": "real-world discovery tasks (hypotheses and workflows)",
            "evaluation_method_name": "Evaluated on LLM agents using HMS (GPT-4-based evaluator)",
            "evaluation_method_description": "Agents are given dataset(s), metadata, and a goal; their NL hypotheses/workflows are scored against gold with HMS.",
            "evaluation_metric": "HMS and components (ctx_F1, var_F1, rel_acc)",
            "metric_definition": "See HMS definition",
            "dataset_or_benchmark": "DB-REAL (part of DiscoveryBench)",
            "human_evaluation_details": "Gold workflows/hypotheses were implemented and verified manually by authors (replication efforts took many person-hours).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Agent HMS on DB-REAL: e.g., CodeGen (GPT-4o) 15.5, ReAct (GPT-4o) 15.4, DataVoyager 15.4, Reflexion (Oracle) 24.5 (GPT-4o); NoDataGuess baseline often low.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Subset of domains (excluded some simulation/forecasting-heavy natural/physical science tasks); replication was time-consuming and sometimes unsuccessful for original papers.",
            "uuid": "e8041.6",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "DB-SYNTH",
            "name_full": "DiscoveryBench - Synthetic tasks (DB-SYNTH)",
            "brief_description": "Synthetic component of DiscoveryBench with 903 tasks across 48 domains, generated via LLM-guided semantic-tree construction and data generation to enable controlled variation of difficulty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (benchmark subset)",
            "model_size": "",
            "scientific_domain": "48 synthetic domains (diverse; used to mimic real workflows)",
            "theory_type": "synthetic discovery tasks/hypotheses",
            "evaluation_method_name": "Evaluated using HMS (GPT-4-based evaluator)",
            "evaluation_method_description": "Tasks synthesized by constructing semantic hypothesis trees, generating pandas expressions for derived variables, sampling leaf distributions, introducing noise/missingness, and forming goals by masking dimensions; agent outputs scored by HMS.",
            "evaluation_metric": "HMS (0–100) and component metrics",
            "metric_definition": "See HMS definition",
            "dataset_or_benchmark": "DB-SYNTH (DiscoveryBench synthetic split, 903 tasks)",
            "human_evaluation_details": "Synthetic tasks generated with LLMs and validated by authors; no human scoring of outputs beyond automated evaluator.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Performance on DB-SYNTH similar to DB-REAL; Reflexion (Oracle) reaches up to ~23.2% HMS with Llama-3; non-oracle agents lower.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Synthetic tasks may not capture all complexities of real-world data; generation relies on LLMs and heuristics for noise/missingness.",
            "uuid": "e8041.7",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "CodeGen agent",
            "name_full": "CodeGen discovery agent",
            "brief_description": "A discovery agent that generates the full analysis code in one shot (demonstration provided), runs it, then produces an NL hypothesis and workflow summary based on results.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B",
            "model_size": "various",
            "scientific_domain": "multi-domain (applies across DiscoveryBench tasks)",
            "theory_type": "automated hypothesis generation via code execution",
            "evaluation_method_name": "Agent framework evaluated with HMS",
            "evaluation_method_description": "Generates end-to-end code to perform data analysis; outputs are converted into NL hypothesis which is scored by HMS.",
            "evaluation_metric": "HMS (and ctx_F1/var_F1/rel_acc components)",
            "metric_definition": "See HMS definition",
            "dataset_or_benchmark": "DiscoveryBench (DB-REAL and DB-SYNTH)",
            "human_evaluation_details": "No human scoring; authors provided demonstrations and agent code is run automatically.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "DB-REAL CodeGen HMS: GPT-4o 15.5, GPT-4p 16.3, Llama-3 12.1; DB-SYNTH CodeGen lower in some model pairings.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "One-shot code generation can fail on complex workflows requiring iterative analysis; depends on environment execution fidelity.",
            "uuid": "e8041.8",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reason + Act) agent",
            "brief_description": "Multi-turn agent that interleaves reasoning (thought) steps with actions (code generation/execution), enabling stepwise discovery attempts.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "model_name": "Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B",
            "model_size": "various",
            "scientific_domain": "multi-domain",
            "theory_type": "iterative hypothesis generation and verification",
            "evaluation_method_name": "ReAct agent framework evaluated using HMS",
            "evaluation_method_description": "Generates thought + action traces across turns to design and run analyses; final NL hypothesis scored by HMS.",
            "evaluation_metric": "HMS and components",
            "metric_definition": "See HMS definition",
            "dataset_or_benchmark": "DiscoveryBench",
            "human_evaluation_details": "No human scoring; agent traces are executed automatically",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "DB-REAL ReAct HMS: GPT-4o 15.4, GPT-4p 15.6, Llama-3 13.5; DB-SYNTH performance generally lower than CodeGen in some settings.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Multi-turn reasoning did not outperform single-shot CodeGen in this benchmark; struggles with complex domain-specific analyses.",
            "uuid": "e8041.9",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "DataVoyager",
            "name_full": "DataVoyager agent (multi-component discovery agent)",
            "brief_description": "A multi-component system (planner, code generator, data analysis, critic) intended to orchestrate discovery; adapted here as an LLM-based discovery agent.",
            "citation_title": "Data-driven discovery with large generative models",
            "mention_or_use": "use",
            "model_name": "Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B",
            "model_size": "various",
            "scientific_domain": "multi-domain",
            "theory_type": "planned and critiqued hypothesis generation & analysis",
            "evaluation_method_name": "DataVoyager framework evaluated with HMS",
            "evaluation_method_description": "Planner creates analysis plan, code generator implements it, data analysis executed, critic evaluates and iterates; final hypothesis scored by HMS.",
            "evaluation_metric": "HMS and component metrics",
            "metric_definition": "See HMS definition",
            "dataset_or_benchmark": "DiscoveryBench",
            "human_evaluation_details": "No human scoring for agent outputs; some runs include domain-knowledge hints to the agent.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "DB-REAL DataVoyager HMS (GPT-4o) ~15.4; adding domain knowledge improved performance in targeted archaeology tasks (jump from 9.9% to 17.5% in one reported case).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Planner/critic loop did not strongly outperform simpler agents; sensitive to domain-knowledge hints.",
            "uuid": "e8041.10",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Reflexion(Oracle)",
            "name_full": "Reflexion (Oracle feedback) agent",
            "brief_description": "An extension of CodeGen that receives oracle HMS feedback after each trial and generates reflections to improve in subsequent trials (up to 3), simulating learning from feedback.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "model_name": "Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B",
            "model_size": "various",
            "scientific_domain": "multi-domain",
            "theory_type": "iterative improvement of hypotheses via feedback",
            "evaluation_method_name": "Reflexion with oracle HMS feedback",
            "evaluation_method_description": "After a trial, the agent is provided the (oracle) HMS score and asked to reflect; it then reattempts the task up to a max of 3 trials to improve the final hypothesis.",
            "evaluation_metric": "HMS (improvement across trials)",
            "metric_definition": "Reported final HMS after iterative improvement attempts.",
            "dataset_or_benchmark": "DiscoveryBench",
            "human_evaluation_details": "Oracle feedback is automated (HMS); no human judges in feedback loop.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Reflexion (Oracle) substantially improved over base CodeGen (e.g., Reflexion GPT-4o 24.5% vs CodeGen GPT-4o 15.5% on DB-REAL).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Oracle HMS feedback is not available in real-world settings; improvement depends on fidelity of the HMS evaluator.",
            "uuid": "e8041.11",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "NoDataGuess",
            "name_full": "NoDataGuess baseline",
            "brief_description": "A baseline that attempts to guess the hypothesis using only dataset metadata and the goal (no access to the actual dataset), assessing memorization or prior-knowledge leakage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used with GPT-4o, GPT-4-0125-preview, Llama-3-70B",
            "model_size": "various",
            "scientific_domain": "multi-domain",
            "theory_type": "baseline hypothesis generation (no data access)",
            "evaluation_method_name": "NoDataGuess baseline",
            "evaluation_method_description": "Agent produces hypothesis using dataset description and goal only; output scored by HMS to measure whether models memorize or infer from metadata alone.",
            "evaluation_metric": "HMS",
            "metric_definition": "See HMS",
            "dataset_or_benchmark": "DiscoveryBench (DB-REAL)",
            "human_evaluation_details": "None",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "NoDataGuess HMS: GPT-4o 0.0 (refused to hallucinate), GPT-4p 4.7, Llama-3 11.5 on DB-REAL.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Measures memorization/overfitting to known published results; some models may refuse to guess, producing zero scores.",
            "uuid": "e8041.12",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "NLS",
            "name_full": "National Longitudinal Surveys (NLS)",
            "brief_description": "Public longitudinal socioeconomic datasets used as source data for several DB-REAL tasks and gold workflow replications.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "sociology / economics",
            "theory_type": "source dataset for discovery tasks",
            "evaluation_method_name": "Used as task datasets in DB-REAL; agent outputs scored by HMS",
            "evaluation_method_description": "Original study workflows over NLS were replicated to form gold hypotheses/tasks; agents analyze NLS-derived datasets to generate hypotheses.",
            "evaluation_metric": "HMS (used to score agent outputs on NLS-derived tasks)",
            "metric_definition": "See HMS",
            "dataset_or_benchmark": "Part of DB-REAL within DiscoveryBench",
            "human_evaluation_details": "Authors spent many person-hours replicating workflows and verifying gold hypotheses from NLS-based studies.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Replication from original papers was time-consuming and sometimes unsuccessful; domain-specific preprocessing required.",
            "uuid": "e8041.13",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GBIF",
            "name_full": "Global Biodiversity Information Facility (GBIF)",
            "brief_description": "Open biodiversity occurrence data source used for some DB-REAL tasks (e.g., invasion of introduced plants) and workflow replications.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "ecology / biology",
            "theory_type": "source dataset for discovery tasks",
            "evaluation_method_name": "Used as task datasets in DB-REAL; agent outputs scored by HMS",
            "evaluation_method_description": "Authors replicated published workflows using GBIF data to form DB-REAL tasks; agents analyze these datasets and are judged with HMS.",
            "evaluation_metric": "HMS",
            "metric_definition": "See HMS",
            "dataset_or_benchmark": "Part of DB-REAL",
            "human_evaluation_details": "Replication sometimes required domain knowledge and specialized tools; took up to 90 person-hours for GBIF-derived tasks.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Complex domain-specific preprocessing and spatial analyses (e.g., spatial autocorrelation) often required; many attempted replications failed due to missing code/data.",
            "uuid": "e8041.14",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "WBOD",
            "name_full": "World Bank Open Data (WBOD)",
            "brief_description": "Public economic and development indicators from the World Bank used as source datasets for some DB-REAL tasks (e.g., education vs GDP analyses).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "economics",
            "theory_type": "source dataset for discovery tasks",
            "evaluation_method_name": "Used in DB-REAL tasks and evaluated with HMS",
            "evaluation_method_description": "Workflows from papers using WBOD were replicated to construct tasks whose hypotheses are scored by HMS.",
            "evaluation_metric": "HMS",
            "metric_definition": "See HMS",
            "dataset_or_benchmark": "Part of DB-REAL",
            "human_evaluation_details": "Authors verified workflows; some tasks required integrating multiple WBOD indicators.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Requires careful preprocessing and selection of indicators; some domain knowledge needed.",
            "uuid": "e8041.15",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "QRData",
            "name_full": "QRData (related benchmark)",
            "brief_description": "A prior dataset/benchmark targeting advanced quantitative/statistical reasoning (mostly textbook-style questions) with unique numeric gold answers; cited for contrast with DiscoveryBench.",
            "citation_title": "Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "statistical / causal analysis",
            "theory_type": "statistical/causal question-answering benchmark",
            "evaluation_method_name": "QRData benchmark (mentioned for comparison)",
            "evaluation_method_description": "Targets well-defined statistical/causal problems with unique numeric gold answers, evaluated against numeric ground truth.",
            "evaluation_metric": "Numeric accuracy vs gold answers (unique numeric targets)",
            "metric_definition": "Primarily numeric error/accuracy relative to gold answers",
            "dataset_or_benchmark": "QRData (reference/comparison)",
            "human_evaluation_details": "N/A (benchmark with numeric gold answers)",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "QRData has well-defined numeric targets; DiscoveryBench differs by being open-ended and requiring pipeline design and semantic reasoning.",
            "uuid": "e8041.16",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Data-driven discovery with large generative models",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data",
            "rating": 2
        },
        {
            "paper_title": "Hypothesis: On the scope of scientific hypotheses",
            "rating": 1
        }
    ],
    "cost": 0.0240585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DiscoveryBench: Towards Data-Driven Discovery with Large Language Models</h1>
<p>Bodhisattwa Prasad Majumder ${ }^{<em> 1}$ Harshit Surana ${ }^{</em> 12}$ Dhruv Agarwal ${ }^{<em> 3}$<br>Bhavana Dalvi Mishra ${ }^{</em> 1}$ Abhijeetsingh Meena ${ }^{2}$ Aryan Prakhar ${ }^{2}$ Tirth Vora ${ }^{2}$<br>Tushar Khot ${ }^{1}$ Ashish Sabharwal ${ }^{1}$ Peter Clark ${ }^{1}$<br>${ }^{1}$ Allen Institute for AI ${ }^{2}$ OpenLocus ${ }^{3}$ University of Massachusetts Amherst<br>Website: https://github.com/allenai/discoverybench<br>https://huggingface.co/datasets/allenai/discoverybench<br>*equal contributions</p>
<h4>Abstract</h4>
<p>Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations across task complexity. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only $25 \%$. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.</p>
<h2>1 Introduction</h2>
<p>Knowledge discovery via the scientific process has been a catalyst for human progress for centuries but has, thus far, been a predominantly manual pursuit [16]. Recent breakthroughs in capabilities of large language models (LLMs) to reason and interface with the world using code [9, 40], external tools [41], and interactive agents [51, 32], however, now suggest the possibility of realizing a discovery system that is fully autonomous. Indeed, recent works [33] provide initial evidence for this paradigm within the setting of data-driven discovery, where both search and verification of hypotheses may be carried out using a dataset alone (i.e., after physical experiments and data collection ${ }^{1}$ ), but the extent of this ability remains unclear. We, therefore, aim to systematically evaluate the following question:</p>
<p>How good are current state-of-the-art LLMs at automated data-driven discovery?</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Each DiscoveryBench task consists of a goal and dataset(s) (left). Solving the task requires both statistical analysis and scientific semantic reasoning, e.g., deciding which analysis is appropriate for the domain, and mapping goal terms to column names (center). A faceted evaluation allows open-ended final answers to be rigorously evaluated (right).</p>
<p>Answering this question is hard, as data-driven discovery in the wild (real-world) is diverse across domains and subject areas, which in turn makes it difficult to build a robust evaluation framework to measure progress. We address this using a pragmatic formalization of data-driven discovery, namely the search for a relationship that may hold between variables in a context, where (importantly) the description of those facets may not be in the language of the dataset. A data-driven discovery task then has one of these components missing, e.g., "How did urban land use affect the invasion of introduced plants in Catalonia?". Importantly, this formalization allows for systematic, reproducible evaluation over a wide variety of real-world problems, by leveraging these facets (Fig 1, right).
Unlike prior datasets for statistical analysis [28] or AutoML [55, 15], DiscoveryBench tasks also require scientific semantic reasoning, for instance, deciding which of the many possible analysis techniques are appropriate for the domain (e.g., spatial autocorrelation for plant invasion, Fig 1 center), how to clean and/or normalize the data, and how to map goal terms to dataset terms (e.g., "land use" to "habitat type"). Task solutions typically requires a multistep workflow (Fig 1, center). In this way, DiscoveryBench is the first large-scale dataset to address the broader data-driven discovery pipeline, not just the statistical analysis component, and explore LLMs' capacity for this.
Given this framework, we created DiscoveryBench by manually extracting 264 discovery tasks, i.e., goal + dataset(s), from over 20 published papers, as well as creating real-world discovery workflows that solve each task. We additionally provide 903 synthetic tasks across 48 domains generated using LLMs to mimic the real-world discovery process. The synthetic benchmark allows us to conduct controlled model evaluations by varying task difficulty. Our contributions are thus:</p>
<ul>
<li>DiscoveryBench, the first comprehensive benchmark to formalize the multi-step process of data-driven hypothesis search and verification, covering many real-world discovery tasks plus additional synthetic tasks.</li>
<li>A pragmatic formalism for data-driven discovery, flexible enough to characterize many real-world tasks while constrained enough to allow for rigorous, reproducible evaluation.</li>
<li>A comprehensive evaluation across state-of-the-art LLM-based reasoning methods ("discovery agents"). We find performance peaks at $25 \%$, demonstrating the challenging nature of our task.</li>
</ul>
<p>These suggest that DiscoveryBench may be a valuable resource for helping make progress on autonomous, data-driven discovery.</p>
<h1>2 Related Work</h1>
<p>Automated data-driven discovery has been a long-standing dream of AI [33, 21]. Although there have been a range of data-driven discovery systems, from early ones that fit equations to idealized data, e.g., Bacon [23], to more modern ones handling complex real-world problems, e.g., AlphaFold [19], their associated datasets are task-specific and customized to a pre-built pipeline. In contrast, DiscoveryBench aims to be a general test over multiple tasks, including testing whether systems can design appropriate pipelines themselves.
A number of datasets and tools are available for AutoML, a related technology aimed at automating workflows for building optimal machine learning models [18, 55, 24]. AutoML tools include packages like Scikit [13], and embedded in cloud platforms such as Google Cloud Platform, Microsoft Azure,</p>
<p>and Amazon Web Services. However, associated datasets for AutoML are primarily used for training models, rather than for open-ended discovery tasks.</p>
<p>Similarly, there are several datasets that test statistical analysis in various fields, e.g., [42, 25, 50]. Software packages like Tableaux, SAS, and R also support users in that task. However, these datasets and tools are designed specifically for data analysis, while DISCOVERYBENCH aims to automate the broader pipeline including ideation, semantic reasoning, and pipeline design, where statistical analysis is just one component.</p>
<p>One recent dataset similar in spirit to ours is QRData [28]. QRData also explores LLM capabilities but targets statistical/causal analysis for well-defined (mainly) textbook questions that have unique, (mainly) numeric gold answers. In contrast, DISCOVERYBENCH has no prescribed boundaries on statistical techniques to apply, uses open-ended questions and answers, and complex tasks drawn from state-of-the-art published work.</p>
<h1>3 Formalization</h1>
<p>We begin by formalizing what we mean by a data-driven hypothesis and how the structure of a complex hypothesis may be viewed as a hypothesis semantic tree.</p>
<p>A data-driven hypothesis $h$ in $\mathcal{H}$ (the space of such hypotheses) is a declarative sentence about the state of the world whose truth value may be inferred from a given dataset $D$ using a verification procedure $\mathcal{V}_{D}: \mathcal{H} \rightarrow$ {supported, unsupported $}$, for instance, via statistical modeling.</p>
<p>Each hypothesis may further be expressed using a propositional formula $\phi$ over a set of subhypotheses $h_{i} \in \mathcal{H}$ using logical connectives, e.g., disjunctions and conjunctions, such that $h:=\phi\left(h_{1}, \ldots, h_{n}\right)$ and $\mathcal{V}<em D="D">{D}(h)=\phi\left(\mathcal{V}</em>}\left(h_{1}\right), \ldots, \mathcal{V<em n="n">{D}\left(h</em>$.}\right)\right)$. For instance, suppose $h$ is the hypothesis "for men younger than 20, popularity of product $A$ varies proportional to their age $\left(h_{1}\right)$, while there exists an inverse relationship for those older than $40\left(h_{2}\right)$ ", then $h$ can be expressed as the conjunction $h_{1} \wedge h_{2</p>
<p>Inspired by recent work of Thompson and Skau [47], we additionally introduce a structured formalism that breaks a hypothesis down into three hypothesis dimensions:</p>
<ul>
<li>Contexts $(c)$ : Boundary conditions that limit the scope of a hypothesis. E.g., "for men over the age of 30" or "in Asia and Europe" or unbounded/full dataset when not specified.</li>
<li>Variables $(v)$ : Known set of concepts that interact in a meaningful way under a given context to produce the hypothesis. E.g., gender, age, or income. Note that each hypothesis is associated with a target variable and a set of independent variables.</li>
<li>Relationships $(r)$ : Interactions between a given set of variables under a given context that produces the hypothesis. E.g., "quadratic relationship", "inversely proportional", or piecewise conditionals.</li>
</ul>
<p>With slight abuse of notation, we can now equivalently define hypothesis $h:=\psi(c, v, r)$, where $\psi(\cdot, \cdot, \cdot)$ returns the declarative sentence "under context $c$, variables $v$ have relationship $r$." For instance, for sub-hypothesis $h_{1}$ in our example above, $c_{1}:=$ "men younger than 20", $v_{1}:={$ gender, consumer_age, product_popularity}, and $r_{1}:=$ "popularity is proportional to age".</p>
<p>Hypothesis Semantic Tree. Observe that each independent variable in a hypothesis may itself be a target variable for a prior hypothesis. To emphasize this hierarchical nature, we introduce the concept of a hypothesis semantic tree whose nodes are variables (independent or derived) and whose sub-trees represent hypotheses, as follows. Consider a hypothesis $h$. A semantic hypothesis tree $\mathcal{T}<em h_prime="h^{\prime">{h}$ with $h$ as the primary hypothesis is a Markov tree whose root node is the target variable of $h$, each of whose leaf nodes is an independent variable that is not derived further, and each of whose internal nodes is the target variable of an intermediate hypothesis. In other words, each sub-tree $\mathcal{T}</em>$ with $v$ as the target variable. In particular, a sub-tree rooted
}}$ rooted at an internal node $v$ of $\mathcal{T}_{h}$ is itself a hypothesis semantic tree for a hypothesis $h^{\prime<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Hypothesis Semantic Tree</p>
<p>at $v$ and all its immediate children $C_{v}$ implicitly encodes $h^{\prime}$ as $\psi(c,{v} \cup C_{v}, r)$ where $r$ is the relationship between $v$ and $C_{v}$ under context $c$ as specified in $h^{\prime}$. More generally, $\mathcal{T}_{h}$ can encode many different hypotheses by choosing one node as the target variable and considering nodes at arbitrary descendant levels as independent variables.</p>
<p>For instance, in Fig 2, we show a semantic tree $\mathcal{T}<em 0="0">{h}$ with the following primary hypothesis ( $h$ ): "The visibility of a galaxy reduces when the blue spectrum dominates and the distance of the galaxy from Earth increases", where galaxy_visibility $\left(v</em>}\right)$ is the target variable with independent variables distance $\left(v_{1}\right)$ and galaxy_color $\left(v_{2}\right)$. Consider now the sub-tree rooted at $v_{2}$, which encodes the following intermediate hypothesis: "Visibility of blue light from galaxies increases with an increase in galaxy size and decrease in star density", where galaxy_color $\left(v_{2}\right)$ is the target variable with independent variables galaxy_size $\left(v_{3}\right)$ and galaxy_density $\left(v_{4}\right)$. Note further that due to there existing ancestor edges from $v_{3}$ and $v_{4}$ to $v_{0}$, $\mathcal{T<em 0="0">{h}$ also encodes the hypothesis: "The visibility of a galaxy reduces with distance from Earth combined with an increase in galaxy size and decrease in star density", where the target variable is $v</em>$.
(Task) Dataset. We formally define a dataset $D$ on which hypothesis search and verification is performed as a collection of tuples $\left{\mathbf{x}}$ and the independent variables are $v_{3}$ and $v_{4<em i="1">{i}\right}</em>}^{m}$ that supports multiple hypothesis semantic trees resulting in a semantic forest $\mathcal{F}:=\cup_{i} \mathcal{T<em i="i">{h^{(i)}}$, where each $\mathbf{x}</em>}$ is a row in the dataset and $x \in \mathbf{x<em i="i">{i}$ is an observation for a particular column. Further, $\mathbf{x}</em>$ with different degrees of observability of internal nodes, altering the difficulty of the discovery task.}$ may span only a subset of nodes in $\mathcal{F}$, i.e., not all nodes in $\mathcal{F}$ may be observed. Specifically, while roots and leaves are always observed, internal nodes (target variables for intermediate hypotheses) may be latent. Therefore, multiple versions of $D$ may be collected for $\mathcal{F</p>
<h1>4 DiscoveryBench</h1>
<p>We now introduce a novel benchmark, DiscoveryBench, for discovering data-driven hypotheses. In this benchmark, a data-driven discovery task is defined as follows: Given one or more task dataset(s) $D$ and a discovery goal $G$, derive a hypothesis $h=\psi(c, v, r)$ addressing $G$ with the highest specificity for the context $c$, variables $v$, and relationship $r$ supported by $D$. Optionally, a workflow of deriving such a hypothesis can be outputted to augment information already present in the hypothesis. DiscoveryBench has two components: DB-Real encompassing data-driven hypotheses and workflows derived from published scientific papers and DB-Synth capturing systemic variations in data-driven hypotheses and workflows obtained from synthetically generated datasets. We release our dataset under the ODC-BY license: https://github.com/allenai/discoverybench.</p>
<h3>4.1 DB-REAL: Collecting data-driven hypotheses in the wild</h3>
<p>Our goal is to replicate the scientific process undertaken by researchers to search for and validate a hypothesis from one or more datasets. We focus on six scientific domains where data-driven research is the cornerstone of scientific progress: sociology, biology, humanities, economics, engineering, and meta-science. Our data collection follows either a data-first or code-first approach.</p>
<p>For the data-first approach: 1) we filter papers based on open public datasets ( $D$ ) such as National Longitudinal Surveys (NLS), Global Biodiversity Information Facility (GBIF), and World Bank Open Data (WBOD) that have workflow details; 2) we then try to replicate these workflows in Python. For this data-first approach, replication took up to 90 person-hours per dataset, often ( $30 \%$ ) not resulting in success. This highlights building data-driven discovery benchmarks from real studies is not only challenging and time-consuming, but automating discovery can also be key for scientific progress and reproducibility.</p>
<p>The data-first approach by design is limited to well-known aforementioned public datasets. To improve diversity in domains, datasets ( $D$ ), and workflows, we also adopted a code-first approach to look beyond popular public datasets. In this approach, we 1) search for code repositories based on scientific papers with available datasets and 2) attempt to replicate them in Python with existing code or from scratch with interpretation of the associated paper. We looked at 785 data points in Zenodo, EU's Open Research Repository, with a filter for computational notebooks. Over 85\% of the repositories either had missing code, code that could not be easily translated to Python, or a</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Workflow categories in DB-REAL with representative examples.
proprietary/non-open dataset. We finalized a candidate list of 14 repositories, but in the end, 3 of them passed all our checks for their hypotheses to be included in the benchmark ${ }^{2}$.
Upon replication of the result or implementation of the full procedure as described in the paper, we include the (dataset $D$, hypothesis $h$, implementation workflow) tuple to the benchmark.
During the process, the implementation workflow may lead to other hypotheses that are not directly reported in the paper but can be supported by the data. We included them in DISCOVERYBENCH, which leads to a good mix of already reported science-worthy hypotheses as well as novel hypotheses grounded in datasets. This is particularly useful as our goal is to evaluate LLMs' ability to solve a discovery task that is realistic but never reported before.
Finally, the task datasets are supplemented with a dataset description, natural language descriptions of the columns, and additional background knowledge related to the domain or the datasets. Some of our tasks, for instance, archaeology, require domain knowledge to derive a particular hypothesis.
Inferring task difficulty. Using the hypothesis semantic tree defined in Section 3, we say that the difficulty of a discovery task is proportional to the path length from an observed node to the target hypothesis node in the tree. However, knowing the tree structure from a task dataset alone is impractical due to incomplete a priori information about unobserved intermediate nodes and edges between observed nodes. To infer task difficulty, we, therefore, approximate the path length between the target and leaf nodes using the length of the implementation workflow required to derive a target hypothesis. Specifically, for each step in the workflow, we add 1 to the discovery path length. In some cases, we derive two tasks: easy and hard from the same hypothesis, where for easy, we provide the derived variables as observed variables in the dataset (e.g., BMI), and for hard, it would require deriving intermediate variables (BMI from height and weight) to reach the target. Additionally, given the view of a task dataset as encoding the union of multiple semantic trees rooted at different hypotheses, i.e., a semantic forest $\mathcal{F}$, we further posit that task difficulty increases as the number of trees in the forest $(|\mathcal{F}|)$ increases. Intuitively, discovery becomes harder as the hypothesis search space increases. In practice, this setting is observed when a task requires access to multiple datasets.
Forming discovery goals. By definition, each hypothesis can be fully specified by the declarative sentence as $h:=\psi(c, v, r)$. To systematically construct the discovery goals for the task, we first mask one of each dimension, context $c$, variable $v$, relationship $r$, and generate a discovery goal to</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>identify the masked information given the rest of the hypothesis and the task dataset(s). For instance, for a target hypothesis, "The effect of socioeconomic status on college degree completion is higher for females (0.4995) than males (0.4467)", we form a discovery goal as "How does socioeconomic status impact on college degree completion for females compared to males?" seeking the relationship $r$ to be discovered from the dataset(s) given the relevant variables $v$ and context $c$. Additionally, we ensure each discovery goal leads to only one answer, i.e., the target hypothesis.</p>
<h1>4.1.1 Features of DB-REAL benchmark</h1>
<p>DiscoveryBench incorporates a broad landscape of data-driven discovery. With over 500 instances of data preparation activities such as cleaning, deduplication, and integration, captures the complexity of real-world data preprocessing for discovery. Tasks also demand a spectrum of statistical methods, from statistical tests to mixture models, and include domain-specific approaches in econometric and ecological modeling, as reflected in the Fig 33.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: right;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># tasks</td>
<td style="text-align: center;">25</td>
<td style="text-align: right;">239</td>
</tr>
<tr>
<td style="text-align: left;"># unique hypotheses</td>
<td style="text-align: center;">14</td>
<td style="text-align: right;">144</td>
</tr>
<tr>
<td style="text-align: left;"># tasks need $&gt;1$ dataset</td>
<td style="text-align: center;">4</td>
<td style="text-align: right;">110</td>
</tr>
<tr>
<td style="text-align: left;"># domains</td>
<td style="text-align: center;">3</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics for DB-REAL</p>
<p>Table 1 shows the diversity of tasks both in train and test split for DB-REAL. Most importantly, the benchmark incorporates $114(4+110)$ tasks that require more than one related datasets to be analyzed, with a maximum of 6 datasets for a task. Each workflow within the dataset can be viewed as a composition of unit actions-such as code generation for statistical tests-that LLMs excel at, showing how our tasks require the chaining of such atomic actions to address complex scenarios for data-driven discovery. We measure the complexity of these workflows by quantifying the number of unit actions involved, referring to this as the workflow length, whose distribution can be seen in Fig 5.</p>
<h3>4.2 DB-Synth: Generating data-driven hypotheses using LLMs</h3>
<p>To scale data collection, we next introduce a supplementary benchmark, which is synthetically constructed to enable controlled model evaluations. Our goal is to reverse-engineer the process of hypothesis discovery to synthesize datasets and discovery tasks of varying difficulty that require analysis workflows similar to those in the real-world benchmark. Our approach leverages the broad pre-trained knowledge of LLMs in four stages:
Domain sampling: First, we prompt the model to generate a list of diverse topics or domains along with their natural language descriptions. E.g., "Ancient architecture" $\rightarrow$ "Related to historic buildings, architectural marvels, and ancient construction techniques".
Semantic tree construction: For each domain, we then build a semantic tree $\mathcal{T}<em h="h">{h}$, recursively deriving nodes starting from a primary hypothesis $h$. Specifically, we prompt the model with the domain and a sampled real-world workflow (e.g., "within-cluster analysis") to generate a hypothesis and its target variable. Setting the target variable as root, we then derive child nodes by generating the independent variables required to verify $h$ using $V(\cdot)$. We operationalize this by generating a column name and description for each child node (along with a data type and range) and a pandas expression ${ }^{4}$ [49] over only independent variables in $\mathcal{T}</em>}$ such that its execution results in the target variable. We repeat this with each leaf in $\mathcal{T<em i="i">{h}$ as the root of a new semantic sub-tree, generating intermediate hypotheses and a new set of variables until the desired height of $\mathcal{T}$ is reached. ${ }^{5}$ We also generate a set of distractor columns disjoint from nodes in $\mathcal{T}$, thus resulting in a synthetic semantic forest $\mathcal{F}$.
Data generation: We then construct a task dataset $D:=\left{\mathbf{x}</em>\right}<em i="i">{i=1}^{m}$ by generating synthetic data in a bottom-up manner (i.e., from leaves to root) for each node in $\mathcal{F}$. Starting with various sampling strategies for leaf nodes (see more in Sec D), for each subsequent level in $\mathcal{F}$, we create new columns for nodes by simply executing their pandas expressions. Finally, to mimic real-world challenges in data collection, we probabilistically perturb each instance $x \in \mathbf{x}</em>$.}$ by adding noise or dropping values to create missing data ${ }^{6}$. Note that at this stage, $D$ contains a column for each node in $\mathcal{F</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Task generation: For each internal node $h$ in $\mathcal{F}$, we now create multiple task datasets $D_{h}^{(l)}$ from $D$, varying the difficulty of the discovery task based on the path length $l$ between $h$ and the observed independent variables in $\mathcal{F}$. Finally, we follow the same strategy for goal formulation as DB-REAL. We generate 903 tasks over 48 diverse domains and assign them to train, dev, and test sets using a 60/20/20 split, where each task is additionally tagged with a difficulty level from 1-4. While we evaluate our agents on the test, the training set can serve as supervised data for improving models.</p>
<h1>4.3 Evaluation</h1>
<p>We evaluate task performance by measuring the alignment of the predicted and gold hypotheses in natural language. ${ }^{7}$ We take inspiration from recent works in LLM benchmarking [43, 54, 52, 14, 26, 27] and design a model-based evaluation strategy using gpt-4-preview-0125 as the evaluator, conditioned on our structured formalism of data-driven hypotheses.
Recall the propositional form $h:=\phi\left(h_{1}, \ldots, h_{n}\right)$ of a hypothesis $h$ that decomposes it into subhypotheses. We first use our GPT-4 based evaluator to independently decompose the gold $\left(h^{g}\right)$ and predicted $\left(h^{p}\right)$ hypotheses into their respective sub-hypotheses $\left{h_{i}^{g}\right}<em j="j">{i=1}^{n}$ and $\left{h</em>\right}}^{p<em k="k">{j=1}^{m}$, asking it to also identify, for each sub-hypothesis $h</em>$, its context, variables, and relationship dimensions (prompt in Listing 1). Given this structured representation of the gold and predicted hypotheses, we then compute a hypothesis match score (HMS), which measures the degree to which two hypotheses align on each dimension, as follows.
To compute HMS, we match each predicted sub-hypothesis $h_{j}^{p}$ with a gold sub-hypothesis $h_{i}^{g}$ when their contexts are judged as equivalent by our GPT-4 based evaluator (prompt in Listing 2). ${ }^{8}$ Let $M$ denote this set of context-matched pairs of predicted and gold sub-hypotheses. At this point, treating each sub-hypothesis context as a single unit, we can compute an F1 score, $\operatorname{ctx}<em _mathrm_F="\mathrm{F">{\mathrm{F} 1}$, capturing how aligned the $n$ contexts of sub-hypothesis of $h^{g}$ with the $m$ contexts of sub-hypotheses of $h^{p}$. Then, for each matched pair of sub-hypotheses, we measure how well the variables and relations align, using an F1 score for the variables $\left(\operatorname{var}</em>} 1}\right)$ and an accuracy score for the relation $\left(\mathrm{rel<em _mathrm_F="\mathrm{F">{\mathrm{acc}}\right)$. Specifically, for each sub-hypothesis pair in $M$, we extract the set of interacting variables in the gold and predicted sub-hypotheses using the GPT-4 based evaluator (prompt in Listing 3). We compute the alignment between these two sets of variables as an F1 score, $\operatorname{var}</em>} 1}$, similar to how $\operatorname{ctx<em _mathrm_acc="\mathrm{acc">{\mathrm{F} 1}$ was computed. For relationships, we compute relationship accuracy with reference to the relationship between the gold variables $\left(\mathrm{rel}</em>\right)$ based on evaluator judgments using the following scoring heuristic: 100 if there is an exact match of the relation, 50 when the predicted relationship is broader than the gold relationship but encompasses it, and 0 otherwise (prompt in Listing 4). Finally, we compute HMS $\in[0,100]$ as the average alignment of the variable and relationship dimensions over context-matched sub-hypotheses, weighted by the overall context alignment:}</p>
<p>$$
\operatorname{HMS}\left(h^{p}, h^{g}\right)=\operatorname{ctx}<em i="1">{\mathrm{F} 1}\left(h^{p}, h^{g}\right) \times \frac{1}{|M|} \sum</em>}^{|M|}\left(\operatorname{var<em i="i">{\mathrm{F} 1}\left(h</em>}^{p}, h_{i}^{g}\right) \times \operatorname{rel<em i="i">{\mathrm{acc}}\left(h</em>\right)\right)
$$}^{p}, h_{i}^{g</p>
<h2>5 Experiments</h2>
<h3>5.1 Discovery Agents</h3>
<p>We benchmark state-of-the-art LLM-based few-shot reasoning methods as discovery agents with two closed models, GPT-4o and GPT-4-0125-preview (GPT-4p), and one open, Llama-3-70B, model powering the reasoning methods. A discovery agent takes the task description, paths to the task dataset(s) $D$, metadata about the datasets (description, column descriptions), and the goal, $G$, to produce a natural language (NL) hypothesis specified by context, variables, and relationship.</p>
<ul>
<li>CodeGen generates the entire code at one go to solve the task, where we provide a demonstration of a solution code in the context. After code execution and based on the result, it generates the NL hypothesis and summarizes the workflow.</li>
<li>ReAct [51] solves the task by generating thought and subsequent codes in a multi-turn fashion.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPT-4o</th>
<th style="text-align: center;">GPT-4p</th>
<th style="text-align: center;">Llama-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DB-REAL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">NoDataGuess</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: left;">CodeGen</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">12.1</td>
</tr>
<tr>
<td style="text-align: left;">React</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">13.5</td>
</tr>
<tr>
<td style="text-align: left;">DataVoyager</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion (Oracle)</td>
<td style="text-align: center;">$\mathbf{2 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{1 9 . 5}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">DB-SYNTH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">CodeGen</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">10.9</td>
</tr>
<tr>
<td style="text-align: left;">React</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">12.0</td>
</tr>
<tr>
<td style="text-align: left;">DataVoyager</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">11.7</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion (Oracle)</td>
<td style="text-align: center;">$\mathbf{1 5 . 7}$</td>
<td style="text-align: center;">$\mathbf{1 2 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 3 . 2}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (Left) Hypothesis Matching Scores (HMS) across agent-LLM pairs in DB-REAL and DB-SYnTH. (Right) Scatter plot for $\mathrm{ctx}<em _mathrm_F="\mathrm{F">{\mathrm{F} 1}$ and average $\operatorname{var}</em>$, showing accurate contexts increases the probability of predicting variables and relations accurately. Scores are for the best model on DB-REAL and only include data points ( $44.2 \%$ ) where both scores are non-zero.} 1} \times \operatorname{rel}_{\mathrm{acc}</p>
<ul>
<li>DataVoyager is a multi-component data-driven discovery agent from [33]. It has four components, planner, code generator, data analysis, and critic, that orchestrate the discovery process.</li>
<li>Reflexion (Oracle) [44] is an extension of CodeGen agent, where at the end of one trial, we provide the "oracle" HMS score as an evaluation signal, and it generates a reflection to improve (when $\mathrm{HMS}&lt;1$ ) in the next trial till it solves the task, or maximum trials (3) are reached.</li>
<li>NoDataGuess guesses the hypothesis (in DB-REAL) just from the dataset description and the goal without accessing the datasets where we measure LLM's memorization of already published works.</li>
</ul>
<h1>5.2 Main Results</h1>
<p>Fig 4(left) shows that overall performance for all framework-LLM pairs is low for both DB-REAL and DB-SYnTH, highlighting the challenging nature of the task and the benchmark. Most importantly, effective reasoning prompts such as React and planning with a self-critic (DataVoyager) do not help improve the simple CodeGen agent. But with oracle feedback, Reflexion (Oracle) significantly improves over CodeGen (base) performance. Analysis reveals that almost all non-reflexion agents solve the easiest (in terms of workflow category and length) instances from the benchmark. GPT-4o refuses to hallucinate in the NoDataGuess baseline, whereas surprisingly Llama-3 performs similarly in both data and no-data modes. We additionally observe that the models' performance in DB-REAL and DB-SYNTH are similar, indicating our synthetic benchmark captures complexities of the real workflow but provides a systematic way to analyze the models' performance.</p>
<h3>5.3 Analysis</h3>
<p>Context is important. Fig 4(right) shows the trends of the $\mathrm{ctx}<em _mathrm_F="\mathrm{F">{\mathrm{F} 1}$ and combined $\operatorname{var}</em>$. A positive trend signifies that to predict variables and relationships accurately, precise and accurate context prediction is necessary. However, correct identification of context is an important first step, although it does not guarantee success.} 1} \times \operatorname{rel}_{\mathrm{acc}</p>
<p>Workflow complexity barrier. Almost all agents struggle more with tasks involving complex statistical techniques, complex data preparation methods, or domain-specific models. The top three workflow categories where the best non-oracle model was highly performant are correlation analysis (55\%), data selection (18\%), and summary statistics (18\%), whereas the lowest three workflow categories are spatial analysis ( $0 \%$ ), pollen dating ( $0 \%$ ), and ecological modeling ( $0 \%$ ).</p>
<p>Domain knowledge dependency. To check if additional domain helps agents perform better, we collect targeted domain knowledge for the archaeology-related tasks that needed significant domain knowledge during data collection. When added as additional hints, we find that DataVoyager's (GPT-4p) performance jumps from $9.9 \%$ (w/ domain knowledge) to $17.5 \%$ (w/o domain knowledge).</p>
<p>Performance across domains and goal types. Fig 5(a) depicts that biology ( $0 \%$ ) and engineering (7\%) perform the worst due to their higher dependence on advanced statistical methods, while economics (25\%) and sociology (23\%) perform better. Additionally, Fig 5(b) shows goals related to discovering a relationship given context and variables are more easily solved than the other two types</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Best non-oracle agent's performance (HMS) (a) across domains, (b) for goal types (dimension to be discovered), and (c) for different workflow lengths. In (c) workflow length categories for DB-REAL are s: $&lt;10$, m: $&gt;10$, $&lt;20$, l: $&gt;20$. For DB-SYNTH, it is the semantic tree height.</p>
<p>of goals, finding context and variables. This is explained by the complexity of the hypothesis search, which is broader for finding the right context or a set of variables given a fixed relationship, whereas finding the relationship given context and variables is easier.</p>
<p><strong>Impact of workflow length.</strong> Inherently, the difficulty of the tasks is measured by the gold workflow length (DB-REAL) or the height of the semantic tree (DB-SYNTH). Figure 5(c) shows a decreasing trend in performance as workflow length (hence, complexity) increases. The performance drops significantly even for medium-length workflows, highlighting current agents' limitations.</p>
<h2>6 Conclusion</h2>
<p>We present DISCOVERYBENCH, the first data-driven discovery benchmark consisting of 264 discovery tasks that capture real scientific workflows extracted from published works. We supplement this with 903 structurally generated synthetic tasks, tailored to evaluate discovery agents at various levels of difficulty. We benchmark state-of-the-art reasoning frameworks with the most advanced LLMs, but the best agent's performance only peaks at 25% underscoring the challenging nature of the task and the benchmark. We hope our timely contribution can increase interest and efforts in making progress on reliable and reproducible autonomous scientific discovery using large generative models.</p>
<h2>References</h2>
<ul>
<li>[1] K. L. Alexander, C. Riordan, J. Fennessey, and A. M. Pallas. Social background, academic resources, and college graduation: Recent evidence from the national longitudinal survey. <em>American Journal of Education</em>, 90(4):315–333, 1982.</li>
<li>[2] A. P. S. Alves, M. Kalinowski, G. Giray, D. Mendez, N. Lavesson, K. Azevedo, H. Villamizar, T. Escovedo, H. Lopes, S. Biffl, et al. Status quo and problems of requirements engineering for machine learning: Results from an international survey. In <em>International Conference on Product-Focused Software Process Improvement</em>, pages 159–174. Springer, 2023.</li>
<li>[3] R. Apel and G. Sweeten. The impact of incarceration on employment during the transition to adulthood. <em>Social problems</em>, 57(3):448–479, 2010.</li>
<li>[4] E. N. Appiah. The effect of education expenditure on per capita gdp in developing countries. <em>International Journal of Economics and Finance</em>, 9(10):136–144, 2017.</li>
<li>[5] J. Brinkmann. Copper output, demand for wood and energy expenditure–evaluating economic aspects of bronze age metallurgy. <em>How's life</em>, pages 11–34, 2019.</li>
<li>[6] J. P. Brozio, J. Müller, M. Furholt, W. Kirleis, S. Dreibrodt, I. Feeser, W. Dörfler, M. Weinelt, H. Raese, and A. Bock. Monuments and economies: What drove their variability in the middle-holocene neolithic? <em>The Holocene</em>, 29(10):1558–1571, 2019.</li>
<li>[7] J. P. Brozio, J. Kneisel, S. Schaefer-Di Maida, J. Laabs, I. Feeser, A. Ribeiro, and S. Schultrich. Patterns of socio-economic cultural transformations in neolithic and bronze age societies in the central northern european plain: Human-environmental interaction concerning bourdieu's forms</li>
</ul>
<p>of capital. In Perspectives on Socio-environmental Transformations in Ancient Europe, pages 105-142. Springer, 2024.
[8] F. O. Cerezer, C. S. Dambros, M. T. Coelho, F. A. Cassemiro, E. Barreto, J. S. Albert, R. O. Wüest, and C. H. Graham. Accelerated body size evolution in upland environments is correlated with recent speciation in south american freshwater fishes. Nature Communications, 14(1):6070, 2023.
[9] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[10] M. Dal Corso, W. Kirleis, J. Kneisel, N. Taylor, M. Wieckowska-Lüth, and M. Zanon. How's Life? Living Conditions in the 2nd and 1st Millennia BCE. Sidestone Press, 2019.
[11] C. Dougherty. Numeracy, literacy and earnings: Evidence from the national longitudinal survey of youth. Economics of education review, 22(5):511-521, 2003.
[12] I. Feeser, W. Dörfler, J. Kneisel, M. Hinz, and S. Dreibrodt. Human impact and population dynamics in the neolithic and bronze age: Multi-proxy evidence from north-western central europe. The Holocene, 29(10):1596-1606, 2019.
[13] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter. Efficient and robust automated machine learning. In NeurIPS, 2015.
[14] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. ArXiv, abs/2302.04166, 2023. URL https://api.semanticscholar.org/CorpusID:256662188.
[15] P. Gijsbers, M. L. P. Bueno, S. Coors, E. LeDell, S. Poirier, J. Thomas, B. Bischl, and J. Vanschoren. Amlb: an automl benchmark. ArXiv, abs/2207.12560, 2022. URL https: //api.semanticscholar.org/CorpusID:251066648.
[16] D. J. Glass and N. Hall. A brief history of the hypothesis. Cell, 134(3):378-381, 2008.
[17] R. Heyard and L. Held. Meta-regression to explain shrinkage and heterogeneity in large-scale replication projects. Technical report, Center for Open Science, 2024.
[18] H. Jin, F. Chollet, Q. Song, and X. Hu. Autokeras: An automl library for deep learning. J. Mach. Learn. Res., 24:6:1-6:6, 2023.
[19] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
[20] E. Kaiser and W. Schier. Time and Materiality: Periodization and Regional Chronologies at the Transition from Bronze to Iron Age in Eurasia (1200-600 BCE). Edited by Elke Kaiser and Wolfram Schier. Verlag Marie Leidorf GmbH, 2021.
[21] H. Kitano. Artificial intelligence to win the nobel prize and beyond: Creating the engine for scientific discovery. AI magazine, 37(1):39-49, 2016.
[22] J. Kneisel. Chronology and transformation. the transition from bronze to iron age in northern europe. Time and materiality: Periodization and regional chronologies at the transition from Bronze to Iron Age in Eurasia (1200-600 BCE), pages 237-263, 2021.
[23] P. Langley. Data-driven discovery of physical laws. Cogn. Sci., 5:31-54, 1981. URL https : //api.semanticscholar.org/CorpusID:39694251.
[24] E. LeDell and S. Poirier. H2O AutoML: Scalable automatic machine learning. In Proceedings of the AutoML Workshop at ICML, 2020.
[25] M. Y. Li, E. B. Fox, and N. D. Goodman. Automated statistical model discovery with language models. ArXiv, abs/2402.17879, 2024. URL https://api.semanticscholar.org/ CorpusID:268041863.
[26] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/ tatsu-lab/alpaca_eval, 2023.
[27] B. Y. Lin, K. Chandu, F. Brahman, Y. Deng, A. Ravichander, V. Pyatkin, R. L. Bras, and Y. Choi. Wildbench: Benchmarking language models with challenging tasks from real users in the wild, 2024. URL https://huggingface.co/spaces/allenai/WildBench.</p>
<p>[28] X. Liu, Z. Wu, X. Wu, P. Lu, K.-W. Chang, and Y. Feng. Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. arXiv preprint arXiv:2402.17644, 2024.
[29] L. Lorenz. Kommunikationsstrukturen mittelneolithischer Gesellschaften im nordmitteleuropäischen Tiefland. Verlag Dr. Rudolf Habelt GmbH, in Kommission, 2018.
[30] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=S37hDerQLB.
[31] S.-D. Maida et al. Unter hügeln: Bronzezeitliche transformationsprozesse in schleswig-holstein am beispiel des fundplatzes von mang de bargen (bornhöved, kr. segeberg) band 1, 2023.
[32] B. P. Majumder, B. D. Mishra, P. Jansen, O. Tafjord, N. Tandon, L. Zhang, C. Callison-Burch, and P. Clark. CLIN: A continually learning language agent for rapid task adaptation and generalization. arXiv preprint arXiv:2310.10134, 2023.
[33] B. P. Majumder, H. Surana, D. Agarwal, S. Hazra, A. Sabharwal, and P. Clark. Data-driven discovery with large generative models. ICML, 2024.
[34] G. I. P. Ottaviano, G. Peri, and G. C. Wright. Immigration, offshoring, and american jobs. American Economic Review, 103(5):1925-1959, 2013.
[35] L. C. Pal. Impact of education on economic development. Khazanah Pendidikan Islam, 5(1): $10-19,2023$.
[36] A. Palmisano, A. Bevan, A. Kabelindde, N. Roberts, and S. Shennan. Long-term demographic trends in prehistoric italy: Climate impacts and regionalised socio-ecological trajectories. Journal of world prehistory, 34(3):381-432, 2021.
[37] E. W. Parkinson, T. R. McLaughlin, C. Esposito, S. Stoddart, and C. Malone. Radiocarbon dated trends and central mediterranean prehistory. Journal of world prehistory, 34:317-379, 2021.
[38] N. Rambeli, D. A. A. Marikan, J. M. Podivinsky, R. Amiruddin, and I. Ismail. The dynamic impact of government expenditure in education on economic growth. International Journal of Business and Society, 22(3):1487-1507, 2021.
[39] M. Riera, J. Pino, L. Sáez, P. Aymerich, and Y. Melero. Effect of introduction pathways on the invasion success of non-native plants along environmental gradients. Biological Invasions, pages $1-20,2024$.
[40] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.
[41] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.
[42] Z. Shao, F. Wang, Y. Xu, W. Wei, C. Yu, Z. Zhang, D. Yao, G. Jin, X. Cao, G. Cong, C. S. Jensen, and X. Cheng. Exploring progress in multivariate time series forecasting: Comprehensive benchmarking and heterogeneity analysis. ArXiv, abs/2310.06119, 2023.
[43] S. Shashidhar, A. Chinta, V. Sahai, Z. Wang, and H. Ji. Democratizing llms: An exploration of cost-performance trade-offs in self-refined open-source models. ArXiv, abs/2310.07611, 2023. URL https://api.semanticscholar.org/CorpusID:263834891.
[44] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning. In NeurIPS, 2023. URL https: //api.semanticscholar.org/CorpusID:258833055.
[45] P. K. Smith, B. Bogin, and D. Bishai. Are time preference and body mass index associated?: Evidence from the national longitudinal survey of youth. Economics \&amp; Human Biology, 3(2): $259-270,2005$.
[46] C. Sommerfeld. Gerätegeld Sichel: studien zur monetären Struktur bronzezeitlicher Horte im nördlichen Mitteleuropa, volume 19. Walter de Gruyter, 2013.</p>
<p>[47] W. H. Thompson and S. Skau. On the scope of scientific hypotheses. Royal Society Open Science, 10(8):230607, 2023.
[48] H. Weatherly, K. Lopez, and C. Tierra. The impact of education on gdp per capita. 2022.
[49] Wes McKinney. Data Structures for Statistical Computing in Python. In Stéfan van der Walt and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference, pages 56 61, 2010. doi: 10.25080/Majora-92bf1922-00a.
[50] Z. Yang, X. Liu, T. Li, D. Wu, J. Wang, Y. Zhao, and H. Han. A systematic literature review of methods and datasets for anomaly-based network intrusion detection. Comput. Secur., 116: 102675, 2022.
[51] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In ICLR, 2023. URL https://openreview.net/ forum?id=WE_vluYUL-X.
[52] Z. Yuan, J. Liu, Q. Zi, M. Liu, X. Peng, and Y. Lou. Evaluating instruction-tuned large language models on code comprehension and generation. ArXiv, abs/2308.01240, 2023. URL https://api.semanticscholar.org/CorpusID:260379087.
[53] K. Zaw, D. Hamilton, and W. Darity. Race, wealth and incarceration: Results from the national longitudinal survey of youth. Race and Social Problems, 8:103-115, 2016.
[54] Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen. Evaluating large language models at evaluating instruction following. ArXiv, abs/2310.07641, 2023. URL https: //api.semanticscholar.org/CorpusID:263834884.
[55] S. Zhang, C. Gong, L. Wu, X. Liu, and M. Zhou. AutoML-GPT: Automatic machine learning with GPT. ArXiv, abs/2305.02499, 2023.</p>
<h1>A FAQs</h1>
<ol>
<li>Dataset or Benchmark: Is this a dataset or a benchmark? A benchmark</li>
<li>Benchmark: For benchmarks, the supplementary materials must ensure that all results are easily reproducible (i.e., all necessary datasets, code, and evaluation procedures must be accessible and documented)
Datasets: DISCOVERYBENCH is released at https://github.com/allenai/ discoverybench/tree/main/discoverybench.
Code (Baseline Models): Code for Discovery Agents are provided in the repository, at: https://github.com/allenai/discoverybench/tree/main/agents. A CLI is available to run the discovery agents on the benchmark.
Evaluation Procedures: Please follow our main paper for the details of our evaluation process. The code to run eval on a single instance of our benchmark is provided at: https:// github.com/allenai/discoverybench/tree/main/eval. A CLI and some example scripts have been provided as well.</li>
<li>Accessibility: The following are accessibility items on the submission checklist:</li>
</ol>
<p>Links to access the benchmark: The link to access the benchmark is provided in the main submission (https://github.com/allenai/discoverybench/tree/main/ discoverybench).
Any data should use open and widely used formats. Simulation environments should explain how they can be used: Our data are stored in widely accessible standard formats (e.g., JSON, CSV), with the structure described in Appendix F.</p>
<p>Long-term preservation. Code and data are provided on GitHub. All aspects will be publicly available for a long term.
Explicit Licence: Our benchmark is licensed using ODC-BY and the associated code is licensed with APACHE 2.0, as included in the GitHub repository.
Structured Metadata for a dataset: Our dataset is also available as the HuggingFace dataset: https://huggingface.co/datasets/allenai/discoverybench. Structured Metadata will be available once we finalize our work after addressing the reviewers' comments, if any.
A persistent dereferenceable identifier (e.g., a code repository such as GitHub): The repository for our benchmark is: https://github.com/allenai/discoverybench.</p>
<h1>B Limitations</h1>
<p>We currently filtered domains and tasks that required forecasting, simulation, or very specific modeling (species distribution, infection spread, astrophysics equations for exoplanets) in the benchmark as they were very time-consuming to replicate as well as discover hypotheses. As a result, we discarded more papers focused on natural and physical sciences compared to social sciences, which we plan to include in future benchmarks.
We currently do not tackle the challenge of understanding and processing massive datasets, such as the 8.92 petabytes data from the Cancer Genome Atlas (https://portal.gdc.cancer.gov) or the extensive brain data from the Allen Institute (https://alleninstitute.org/division/ brain-science). While the potential to discover new insights from such vast data volumes is significant, ensuring these findings are robust and not subject to $p$-hacking remains unaddressed by our current methods.
We currently do not handle multi-modal data and complex pipelines, such as those needed for analyzing satellite and other geospatial data relevant to climate science and astronomy data. This would involve multiple stages of data processing, the use of various tools, and managing workflow complexities, for example, analyzing thousands of species patterns combined with satellite data to study habitats. So we do not incorporate workflows like those of EarthRanger (https://www. earthranger.com).
Ethical Considerations There could be many potential societal consequences of systems tuned on our proposed benchmark since it involves using LLMs, such as policy misuse, legal ramifications, and false discovery. On the positive side, our proposed benchmark can advance the rate of discovery, leading to an improved standard of living and social well-being.</p>
<h2>C Data collection for DB-REAL</h2>
<p>For data-first approach, replication took 15 to 40 person-hours for each NLS-related paper and up to 90 person-hours for the GBIF dataset, where specialized domain knowledge and tools led to higher complexity. All papers replicated in the NLS dataset were included, while less than half of the papers in specialized datasets like GBIF and WBOD were added to DISCOVERYBENCH.
Citation/Repositories for DB-REAL: List of scientific works from where we have replicated our gold workflows and hypotheses:</p>
<ol>
<li>Sociology: $[53,3,1,45,11]$</li>
<li>Biology: $[8,39]$</li>
<li>Economics: $[35,4,48,38,34]$</li>
<li>Engineering: [2]</li>
<li>Meta-science: [17]</li>
<li>Humanities: $[7,6,29,31,46,12,37,22,20,5,10,36,37]$</li>
</ol>
<p>All assets come under CC license or open licenses.</p>
<h2>D Data Generation for DB-SYnTH</h2>
<p>For leaves, we use different sampling strategies based on the data type. Specifically, for categorical nodes, we sample instances with replacement from the range of allowed values, whereas for numeric, we first select a distribution (e.g., normal) and its parameters based on the specified range and then perform sampling. For each subsequent level in $\mathcal{F}$, we create new columns for nodes by simply executing their pandas expressions ${ }^{9}$. To recover from any execution errors, we additionally use a self-refine [30] approach to generate new pandas expressions guided by the execution error logs. Finally, to mimic real-world challenges in data collection, we probabilistically perturb each instance</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$x \in \mathbf{x}_{i}$ by adding noise or dropping values to create missing data ${ }^{10}$. After generation, $D$ contains a column for each node in $\mathcal{F}$.</p>
<h1>E Datasheets</h1>
<h2>E. 1 Motivation</h2>
<ul>
<li>For what purpose was the dataset created? DISCOVERYBENCH is created to help assess large language models' (LLMs) ability to automate the search and verification of hypotheses purely from a set of provided datasets.</li>
<li>Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? Authors belong to the Allen Institute for AI, OpenLocus, and the University of Massachusetts Amherst. The data collection is part of research efforts conducted by the Allen Institute for AI.</li>
<li>Who funded the creation of the dataset? Allen Institute for AI.</li>
</ul>
<h2>E. 2 Collection Process</h2>
<ul>
<li>How was the data associated with each instance acquired? Our goal is to replicate the scientific process undertaken by researchers to search for and validate a hypothesis from one or more datasets. We focus on six scientific domains where data-driven research is the cornerstone of scientific progress: sociology, biology, humanities, economics, engineering, and meta-science. Our data collection follows either a data-first or code-first approach. Each instance has been manually implemented and verified by the authors for solvability.</li>
</ul>
<h2>E. 3 Uses</h2>
<ul>
<li>Has the dataset been used for any tasks already? We use this benchmark to evaluate LLM's ability to search and verify hypotheses purely from a set of datasets.</li>
<li>Are there tasks for which the dataset should not be used? We do not expect the community members to use this data to train models that can aggravate $p$-hacking.</li>
</ul>
<h2>E. 4 Distribution and Maintainance</h2>
<ul>
<li>How will the dataset will be distributed? We distribute this benchmark via our GitHub repository: https://github.com/allenai/discoverybench and https: //huggingface.co/datasets/allenai/discoverybench.</li>
<li>How can the owner/curator/manager of the dataset be contacted? For any benchmark-related queries, please contact: bodhisattwam@allenai.org. For any coderelated discussions, please raise an issue in GitHub: https://github.com/allenai/ discoverybench.</li>
</ul>
<h2>F Composition of DISCOVERYBENCH</h2>
<h2>F. 1 Metadata structure</h2>
<ul>
<li>id: An identifier for the metadata.</li>
<li>domain: The broad field of study or area of research.</li>
<li>workflow_tags: A set of keywords summarizing the main processes or techniques used in the replication implementation. They provide an overview of the methodological approach and facilitating the identification of relevant analytical techniques.</li>
</ul>
<h2>- domain_knowledge:</h2>
<ul>
<li>Contextual information or insights related to the dataset, explaining how certain behaviors or variables can be interpreted within the field of study.</li>
</ul>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>It helps open avenues to think in directions that LLM might not have considered otherwise, broadening the understanding of the field.</li>
<li>datasets: Contains detailed information about the datasets used, including:</li>
<li>name: The name or filename of the dataset.</li>
<li>description: A summary of the dataset's contents and the type of data it includes.</li>
<li>max_depth: The maximum hierarchical level of nested data structures within the dataset, indicating the complexity of the data.</li>
<li>columns: Detailed descriptions of each column in the dataset, including:</li>
<li>name: The column's name or header.</li>
<li>description: Explanation of the data contained in the column and its significance.</li>
<li>depth: The hierarchical level of the column within the dataset, indicating its structural position.</li>
<li>hypotheses: Statements or predictions being tested, divided into:</li>
<li>main: Primary hypotheses that are central to the discovery task.</li>
<li>workflow: A step-by-step description of the replication process followed to validate the hypotheses, outlining the methods and procedures used from data preparation to final analysis. Some of the workflows and sub-workflows are high-level and thus the same for different queries as they follow the same implementation leading to a range of hypotheses.</li>
<li>queries: Goals related to each hypothesis, each including:</li>
<li>qid: A unique identifier for the goal for a given true/gold hypothesis.</li>
<li>difficulty: Categorization of the difficulty. Structurally defined for DB-SYNTH using the semantic tree definition.</li>
<li>true_hypothesis: The hypothesis being tested through the goal. This defines the primary statement or prediction under investigation.</li>
<li>relevant_cols: Columns from the dataset that are relevant to answering the query, indicating the specific data points that can be used in the analysis. Only appears for DB-SYNTH.</li>
<li>target_col: The column being predicted or the dependent variable in the analysis. Only appears for DB-SYNTH.</li>
<li>question_type: The type of question being asked categorizing the nature of the inquiry.</li>
<li>question: The discovery goal.</li>
</ul>
<h1>F. 2 Directory structure for DB-REAL</h1>
<p>There may be more than one query per metadata. The train split contains 14 metadata files and 25 queries. The test split contains 144 metadata files and 239 queries. Metadata folders with the same prefixes use the same underlying dataset with either a subset or a preprocessed version. When dealing with a full dataset (i.e., nls_raw), the task becomes substantially harder due to the data preparation required.</p>
<div class="codehilite"><pre><span></span><code><span class="o">|-</span><span class="n">test</span>
<span class="o">|---</span><span class="n">archaeology</span>
<span class="o">|---</span><span class="n">introduction_pathways_non</span><span class="o">-</span><span class="n">native_plants</span>
<span class="o">|---</span><span class="n">meta_regression</span>
<span class="o">|---</span><span class="n">meta_regression_raw</span>
<span class="o">|---</span><span class="n">nls_incarceration</span>
<span class="o">|---</span><span class="n">nls_raw</span>
<span class="o">|---</span><span class="n">nls_ses</span>
<span class="o">|---</span><span class="n">requirements_engineering_for_ML_enabled_systems</span>
<span class="o">|---</span><span class="n">worldbank_education_gdp</span>
<span class="o">|---</span><span class="n">worldbank_education_gdp_indicators</span>
<span class="o">|-</span><span class="n">train</span>
<span class="o">|---</span><span class="n">evolution_freshwater_fish</span>
<span class="o">|---</span><span class="n">immigration_offshoring_effect_on_employment</span>
<span class="o">|---</span><span class="n">nls_bmi</span>
<span class="o">|---</span><span class="n">nls_bmi_raw</span>
</code></pre></div>

<h1>F. 3 Directory structure for DB-SYnTH</h1>
<p>There is one query per metadata. The train split contains 551 metadata files (queries), the dev split contains 153 metadata files (queries), and the test split contains 200 metadata files (queries).</p>
<div class="codehilite"><pre><span></span><code>    |-test
    |---ancient-languages_*_*
    |---artificial-ecosystems_*_*
    |---astronomy_*_*
    |---board-games_*_*
    |---coding-competitions_*_*
    |---digital-artistry_*_*
    |---futuristic-technology_*_*
    |---impressionist-art_*_*
    |---machine-learning_*_*
    |---molecular-gastronomy_*_*
    |---neuroscience_*_*
    |---philosophical-debates_*_*
    |---robotics_*_*
    |-train
    |---adventure-travel_*_*
    |---ancient-architecture_*_*
    |---ancient-astronomy_*_*
    |---aviation_*_*
    |---biodiversity-conservation_*_*
    |---cryptic-puzzles_*_*
    |---cryptocurrency_*_*
    |---culinary-arts_*_*
    |---cybersecurity_*_*
    |---environmental-activism_*_*
    |---fashion-design_*_*
    |---fine-arts_*_*
    |---literary-classics_*_*
    |---marine-biology_*_*
    |---marine-conservation_*_*
    |---medieval-literature_*_*
    |---musical-therapy_*_*
    |---photography_*_*
    |---robotic-explorers_*_*
    |---solar-power_*_*
    |---space-tourism_*_*
    |---steampunk-culture_*_*
    |---theater-productions_*_*
    |---underwater-archaeology_*_*
    |---urban-gardening_*_*
    |---vintage-automobiles_*_*
    |---virtual-reality_*_*
</code></pre></div>

<h2>G Discovery Agent</h2>
<p>The command discovery_agent.py is used with various options to customize its behavior for discovery tasks. Below are the options explained:</p>
<ul>
<li>Usage: discovery_agent.py [OPTIONS] QUERY - Executes the discovery agent with specified options.</li>
</ul>
<h2>- Options:</h2>
<ul>
<li>
<p>-agent_type [coder|react]: Specifies the type of agent to use for discovery. The default type is coder. Options include coder for code-related tasks and react for reactive tasks.</p>
</li>
<li>
<p>-model_name TEXT: Sets the model to be used. The default is gpt-4o. Available models include gpt-4-turbo, llama-3-70b-chat, claude-3-opus, and gemini-pro. An exhaustive list is available in config/model_config.json.</p>
</li>
<li>-api_config TEXT: Path to the API configuration file. The default path is config/api_config.json.</li>
<li>-log_file TEXT: Specifies the path to the log file where operations details are stored.</li>
<li>-metadata_path TEXT: Path to the metadata file. This option is required.</li>
<li>-metadata_type [real|synth]: Specifies the type of metadata, where real stands for actual metadata and synth for synthetic. This option is required.</li>
<li>-add_domain_knowledge: Includes domain-specific knowledge in the query processing.</li>
<li>-add_workflow_tags: Includes workflow tags in the query to enhance context.</li>
<li>-help: Displays the help message and exits, showing all available command options.</li>
</ul>
<h1>H Evaluation</h1>
<p>Explain about evaluation in a line and then explain the CLI usage here.
The command discovery_eval.py is used to evaluate the outputs generated by the discovery agent. Below are the detailed descriptions of the command options:</p>
<ul>
<li>Usage: discovery_eval.py [OPTIONS] QUERY - Executes the evaluation agent with specified options and a query.</li>
</ul>
<h2>- Options:</h2>
<ul>
<li>-gold_hypo TEXT: Specifies the gold standard hypothesis for comparison. This field is required.</li>
<li>-gold_workflow TEXT: Specifies the gold standard workflow to be used as a reference during evaluation.</li>
<li>-pred_hypo TEXT: Specifies the predicted hypothesis generated by the discovery agent. This field is required.</li>
<li>-pred_workflow TEXT: Specifies the predicted workflow generated by the discovery agent.</li>
<li>-metadata_path TEXT: Specifies the path to the metadata file that is utilized during evaluation. This field is required.</li>
<li>-metadata_type [real|synth]: Determines the type of metadata used in the evaluation, where real indicates actual metadata and synth indicates synthetic metadata. This field is required.</li>
<li>-eval_output_path TEXT: Specifies where the evaluation results should be saved.</li>
<li>-help: Displays the help message and exits, detailing all available command options.</li>
</ul>
<h2>I Experiments</h2>
<p>For GPT-based models, we use OpenAI API (https://platform.openai.com/docs/ models), and for Llama3, we used Together API (https://docs.together.ai/docs/ inference-models)</p>
<h2>J Evaluator Prompts</h2>
<p>We provide below the exact prompts used for our GPT-4 based evaluation of the generated hypothesis against the gold hypothesis.</p>
<p>Listing 1 Decomposition Prompt to obtain sub-hypotheses from a hypothesis.</p>
<div class="codehilite"><pre><span></span><code><span class="n">decomposition_prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">    Given a set of dataset columns, a ground-truth hypothesis, and the</span>
<span class="s2">    analysis workflow used, your task is to extract the set of sub-hypotheses</span>
<span class="s2">    that are present in the hypothesis such that each sub-hypothesis covers a</span>
<span class="s2">    separate context, is self-sufficient, and operates on a coherent set of 3</span>
<span class="s2">    dimensions: Context, Variables, and Relations.</span>
<span class="s2">    Here are the definitions for these dimensions:</span>
<span class="s2">    - Contexts: Boundary conditions that limit the scope of a sub-hypothesis.</span>
<span class="s2">    E.g., &quot;for men over the age of 30&quot;, &quot;in Asia and Europe&quot;, or &quot;None&quot; if</span>
<span class="s2">    there is no boundary condition specified.</span>
<span class="s2">    - Variables: Known concepts that interact in a meaningful way under a</span>
<span class="s2">    given context to produce the sub-hypothesis. E.g., gender, age, income, or</span>
<span class="s2">    &quot;None&quot; if there is no interacting variable.</span>
<span class="s2">    - Relations: Interactions between a given set of variables under a given</span>
<span class="s2">    context to produce the sub-hypothesis. E.g., &quot;quadratic relationship&quot;,</span>
<span class="s2">    &quot;inversely proportional&quot;, piecewise conditionals, or &quot;None&quot; if there is no</span>
<span class="s2">    interacting relationship.</span>
<span class="s2">    Make sure to only use the information present in the hypothesis and the</span>
<span class="s2">    workflow. Do not add any new information.</span>
<span class="s2">    If no sub-hypotheses can be extracted, return an empty list.</span>
<span class="s2">    Here is the metadata for the task:</span>
<span class="s2">    ```json</span>
<span class="s2">    {{</span>
<span class="s2">        &quot;datasets&quot;: {dataset_metadata},</span>
<span class="s2">        &quot;hypothesis&quot;: &quot;{hypothesis}&quot;,</span>
<span class="s2">        &quot;workflow&quot;: &quot;{workflow}&quot;</span>
<span class="s2">    }}</span>
<span class="s2">    ```</span>

<span class="s2">    Return your answer as a JSON object in the following format:</span>
<span class="s2">    ```json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;sub_hypo&quot;: [</span>
<span class="s2">        {{</span>
<span class="s2">            &quot;text&quot;: the sub-hypothesis in natural language,</span>
<span class="s2">            &quot;context&quot;: a short text description of the context of the</span>
<span class="s2">            sub-hypothesis,</span>
<span class="s2">            &quot;variables&quot;: a list of columns involved in the sub-hypothesis,</span>
<span class="s2">            &quot;relations&quot;: a short text description of the relationship between</span>
<span class="s2">            the variables of the sub-hypothesis,</span>
<span class="s2">            &quot;explanation&quot;: a short text explanation for the breakdown of the</span>
<span class="s2">            sub-hypothesis</span>
<span class="s2">        }},</span>
<span class="s2">    ...</span>
<span class="s2">    ]</span>
<span class="s2">    }}</span>
</code></pre></div>

<p>Listing 2 Matching prompt to match contexts of two sub-hypotheses.</p>
<div class="codehilite"><pre><span></span><code>matching_prompt = f&quot;&quot;&quot;
    Given a gold hypothesis, a gold context, a predicted hypothesis, and a
    predicted context, your task is
    to determine if the predicted context semantically matches the
    ground-truth context.
    Here is the definition for Context: Boundary conditions that limit the
    scope of a sub-hypothesis. E.g., &quot;for men over the age of 30&quot;, &quot;in Asia
    and Europe&quot;, or &quot;None&quot; if there is no boundary condition specified.
    If the predicted context matches the gold context, return true, otherwise
    return false.
    Here is the metadata for the task:
    ```json
    {{
        &quot;gold_hypothesis&quot;: &quot;{gold_hypotheis}&quot;,
        &quot;gold_context&quot;: &quot;{gold_context}&quot;,
        &quot;predicted_hypothesis&quot;: &quot;{pred_hypothesis}&quot;,
        &quot;predicted_context&quot;: &quot;{pred_context}&quot;
    }}
    ```
    Return your answer as a JSON object in the following format:
    ```json
    {{
        &quot;match&quot;: true or false
    }}
</code></pre></div>

<p>Listing 3 Prompt for variable alignment between two sub-hypotheses.</p>
<div class="codehilite"><pre><span></span><code><span class="n">main_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are going to compare two natural-language hypotheses HypoA and HypoB</span>
<span class="s2">    accompanied with optional workflows: WorkflowA for HypoA and WorkflowB for</span>
<span class="s2">    HypoB.</span>
<span class="s2">    Both the hypotheses answer the natural language query &quot;QUERY&quot; over the</span>
<span class="s2">    dataset(s) described by dataset description(s) and column description(s)</span>
<span class="s2">    below.</span>
<span class="s2">    Compare HypoA and HypoB in terms of three aspects: Contexts, Variables,</span>
<span class="s2">    and Relations.</span>
<span class="s2">    E.g., for the hypothesis &quot;From 1995 to 2009, the number of sandhill cranes</span>
<span class="s2">    around the tundra (Indigilka River) surged by an astounding ~10X&quot;:</span>
<span class="s2">    * Contexts refer to the stratification of the data under which the given</span>
<span class="s2">    hypothesis is True. E.g., &quot;For all women&quot;, &quot;From 1995 to 2009&quot;.</span>
<span class="s2">    * Variables refer to the set of variables (either dependent or independent)</span>
<span class="s2">    that are mentioned in the hypothesis. E.g., number of sandhill cranes,</span>
<span class="s2">    location.</span>
<span class="s2">    * Relations refer to the form of relation between the variables. E.g.,</span>
<span class="s2">    &quot;surged by ~10x&quot;.</span>
<span class="s2">    Answer the following questions for a given pair of hypotheses, HypoA and</span>
<span class="s2">    HypoB, along with an explanation grounded on the QUERY and the DATASET(S).</span>
<span class="s2">    Here is the metadata for the task:</span>
<span class="s2">    ~~~json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;datasets&quot;: {datasets_json},</span>
<span class="s2">    &quot;query&quot;: {query},</span>
<span class="s2">    &quot;HypoA&quot;: {gold_hypo},</span>
<span class="s2">    &quot;WorkflowA&quot;: {gold_workflow},</span>
<span class="s2">    &quot;HypoB&quot;: {gen_hypo},</span>
<span class="s2">    &quot;WorkflowB&quot;: {gen_workflow}</span>
<span class="s2">    }}</span>
<span class="s2">    ~~~</span>
<span class="s2">    {variable_question}&quot;&quot;&quot;</span>
<span class="n">variable_question</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">    Question: For both HypoA and HypoB, what are the different variables found</span>
<span class="s2">    in the hypotheses? </span><span class="se">\</span>
<span class="s2">    Return your answer as a JSON object in the following format:</span>
<span class="s2">    ~~~json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;sizeA&quot;: num of variables used in HypoA</span>
<span class="s2">    &quot;sizeB&quot;: num of variables used in HypoB</span>
<span class="s2">    &quot;intersection&quot;: num of variables common in HypoA and HypoB. Use *fuzzy</span>
<span class="s2">    matching* to determine intersection, accounting for paraphrases or</span>
<span class="s2">    slightly different surface forms</span>
<span class="s2">    &quot;explanation&quot;: a short text explanation about the variables</span>
<span class="s2">    }}</span>
<span class="s2">    Answer:&quot;&quot;&quot;</span>
</code></pre></div>

<p>Listing 4 Prompt for relationship alignment between two sub-hypotheses.</p>
<div class="codehilite"><pre><span></span><code><span class="n">main_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are going to compare two natural-language hypotheses HypoA and HypoB</span>
<span class="s2">    accompanied with optional workflows: WorkflowA for HypoA and WorkflowB for</span>
<span class="s2">    HypoB.</span>
<span class="s2">    Both the hypotheses answer the natural language query &quot;QUERY&quot; over the</span>
<span class="s2">    dataset(s) described by dataset description(s) and column description(s)</span>
<span class="s2">    below.</span>
<span class="s2">    Compare HypoA and HypoB in terms of three aspects: Contexts, Variables,</span>
<span class="s2">    and Relations.</span>
<span class="s2">    E.g., for the hypothesis &quot;From 1995 to 2009, the number of sandhill cranes</span>
<span class="s2">    around the tundra (Indigilka River) surged by an astounding ~10X&quot;:</span>
<span class="s2">    * Contexts refer to the stratification of the data under which the given</span>
<span class="s2">    hypothesis is True. E.g., &quot;For all women&quot;, &quot;From 1995 to 2009&quot;.</span>
<span class="s2">    * Variables refer to the set of variables (either dependent or independent)</span>
<span class="s2">    that are mentioned in the hypothesis. E.g., number of sandhill cranes,</span>
<span class="s2">    location.</span>
<span class="s2">    * Relations refer to the form of relation between the variables. E.g.,</span>
<span class="s2">    &quot;surged by ~10x&quot;.</span>
<span class="s2">    Answer the following questions for a given pair of hypotheses, HypoA and</span>
<span class="s2">    HypoB, along with an explanation grounded on the QUERY and the DATASET(S).</span>
<span class="s2">    Here is the metadata for the task:</span>
<span class="s2">    ~~json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;datasets&quot;: {datasets_json},</span>
<span class="s2">    &quot;query&quot;: {query},</span>
<span class="s2">    &quot;HypoA&quot;: {gold_hypo},</span>
<span class="s2">    &quot;WorkflowA&quot;: {gold_workflow},</span>
<span class="s2">    &quot;HypoB&quot;: {gen_hypo},</span>
<span class="s2">    &quot;WorkflowB&quot;: {gen_workflow}</span>
<span class="s2">    }}</span>
<span class="s2">    ~</span>
<span class="s2">    {variable_question}&quot;&quot;&quot;</span>
<span class="n">dimension_question</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Question: Does HypoB exhibit the same relation as HypoA?</span>
<span class="s2">    Compare using the following example hierarchy of relationships (based on</span>
<span class="s2">    specificity): </span><span class="se">\</span>
<span class="s2">    &quot;there exists a relationship&quot; &gt; &quot;positive relationship&quot; &gt; &quot;positive AND</span>
<span class="s2">    (linear OR quadratic)&quot; &gt; &quot;positive AND linear.&quot;</span>
<span class="s2">    Options: A) very similar B) similar but general than HypoA C) different</span>
<span class="s2">    Return your answer as a JSON object in the following format:</span>
<span class="s2">    ~~json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;answer&quot;: one of the options from A) very similar B) similar but general</span>
<span class="s2">    than HypoA C) different</span>
<span class="s2">    &quot;explanation&quot;: a short text explanation about the relationship comparison</span>
<span class="s2">    }}</span>
<span class="s2">    Answer:&quot;&quot;&quot;</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ Each value is noised independently; therefore, each row has sufficient true data useful for discovery.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
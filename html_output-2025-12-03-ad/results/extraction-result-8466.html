<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8466 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8466</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8466</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-b123a0d46ad917b79c43c5ae981e03ed2458ed11</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11" target="_blank">Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.</p>
                <p><strong>Paper Abstract:</strong> Solving algebraic word problems requires executing a series of arithmetic operations—a program—to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8466.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8466.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program-Induced Rationale Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program Induction by Rationale Generation (this paper's model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence model that generates a latent program (sequence of instructions invoking a pre-defined set of operations and an external memory) which, when executed, produces a human-readable rationale interleaved with algebraic operations and the final answer; training uses heuristic search over latent programs constrained by the rationale tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Program-Induced Rationale Model (Ling et al., this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-layer LSTM encoder-decoder with attention (hidden size 200, embeddings 200) that emits a latent sequence of instructions z (operations, argument pointers or softmax tokens, and destination: output or memory). Uses a pre-defined set of 22 operations (arithmetic, trig, conversions, Choose, Str_to_Float, Float_to_Str, etc.), external memory buffer for intermediate numeric values, pointer networks for copying from input/output, staged backpropagation to handle long instruction sequences, heuristic search over candidate instructions (graph expansion depth D=5) and beam decoding (beam=200).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Rationale generation (chain-of-thought-like textual decomposition)', 'Latent program induction (generating and executing discrete operations)', 'Symbolic arithmetic and numeric operations (Add, Subtract, Multiply, Divide, Power, Log, Sqrt, Sine, Cosine, Tangent, Factorial, Choose)', 'External memory for intermediate values', 'Copy mechanisms (copy-input and copy-output / pointer networks)', 'Heuristic/search-guided decoding constrained by rationale tokens']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model explicitly generates a latent program z where each instruction is an operation (from a fixed inventory of 22 ops) with arguments chosen via softmax or pointer copies; some instruction results are written to an external memory and later referenced (enabling multi-step symbolic computation). The rationale is produced as output tokens interleaved with execution-driven numeric results; this textual rationale functions like a chain-of-thought and constrains the search for executable programs. Copy mechanisms let the model reuse tokens from input or previously produced output; staged backprop prevents memory explosion when training on long instruction sequences. Decoding applies heuristic filtering to only keep instructions that can generate the next rationale token (allowing at most one level of indirection during the constrained search).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared the program-induced rationale model against standard attention seq2seq baselines and incremental augmentations: (1) Seq2Seq (attention-only), (2) Seq2Seq + copy-input pointer, (3) Seq2Seq + copy-output pointer, and (4) the program-induced rationale model. Evaluation metrics were rationale perplexity, BLEU-4 for rationale generation, and multiple-choice answer accuracy. The paper also discusses that rationale supervision constrains program search (heuristic expansion depth D=5) but does not present additional ablations explicitly varying the number/types of operations beyond these model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Custom 100,949-training-sample algebraic word-problem dataset with natural-language rationales and multiple-choice options (held-out dev/test 250 each); task requires producing the rationale and selecting the correct option.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On the held-out test set: Perplexity (lower better): Seq2Seq 524.7, +Copy Input 46.8, +Copy Output 45.9, Our Model 28.5. BLEU-4 (higher better): Seq2Seq 8.57, +Copy Input 21.3, +Copy Output 20.6, Our Model 27.2. Accuracy (multiple-choice): Seq2Seq 20.8%, +Copy Input 20.4%, +Copy Output 20.2%, Our Model 36.4%. These numeric results show stepwise gains from adding copy mechanisms and a large gain from the program-induced rationale approach.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Baseline seq2seq produces very high perplexity and often degenerate/short rationales (e.g., "Answer should be D"); adding input copy reduces perplexity and produces more plausible rationales because variables from the problem can be reused; output-copy gives small additional improvements because repeated tokens can be copied after first generation. The program-induced model can generate intermediate computed values (e.g., '1326', '1/221') via arithmetic ops, leading to markedly better perplexity, BLEU and roughly doubling accuracy vs. baselines. The paper also notes that rationale supervision is crucial: without modeling rationales the search space for latent programs is too unconstrained and learning fails. However, generating long, complex multi-step rationales remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Generating answer rationales as a latent variable that constrains program induction is an effective indirect supervision strategy for learning arithmetic programs; the ability to perform algebraic manipulations (via discrete operations and memory) during generation is essential for solving algebraic word problems and yields substantial improvements in rationale quality and answer accuracy compared to standard seq2seq models with and without copying. The paper claims indirect supervision via rationales makes program induction tractable where direct induction from question-answer pairs would be intractable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8466.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8466.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-based Sequence-to-Sequence with Copy Mechanisms (baseline variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard attention encoder-decoder models (seq2seq) and augmented versions with input-copy and output-copy pointer mechanisms used as baselines to generate rationales and answers; these are purely neural token-generation approaches without explicit discrete operation execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation by jointly learning to align and translate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Seq2Seq; Seq2Seq + Copy-Input; Seq2Seq + Copy-Output</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Attention-based encoder-decoder (Bahdanau-style) trained to output rationales and final option token. Augmented baselines include a pointer network to copy tokens from the input (copy-input) and a pointer mechanism to copy from previously generated output tokens (copy-output). Implementations follow conventional softmax generation except for pointer additions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct token-level generation (single-step, monolithic neural inference)', 'Copying from input text (pointer networks)', 'Copying from output (pointer / output-attention)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>These baselines reason by learning to map input tokens to output tokens directly through distributed neural representations; they do not execute discrete symbolic operations. Copy-input allows reuse of values present in the problem text to alleviate sparsity of numeric tokens; copy-output allows reuse of previously generated outputs to repeat computed tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablation-style incremental comparison: Seq2Seq (attention) alone, then add copy-input, then add copy-output; metrics measured on same dataset (perplexity, BLEU-4, accuracy) to quantify gains from adding copying mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same custom algebraic word-problem dataset with rationales and multiple-choice answers (this paper's dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>See paper table: Seq2Seq Perplexity 524.7, BLEU 8.57, Accuracy 20.8%; +Copy Input Perplexity 46.8, BLEU 21.3, Accuracy 20.4%; +Copy Output Perplexity 45.9, BLEU 20.6, Accuracy 20.2%. Copy mechanisms drastically reduce perplexity and improve BLEU but do not improve answer accuracy beyond chance levels.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Plain seq2seq struggles with numeric sparsity and generates short/degenerate rationales; copy-input fixes many numeric token generation issues and yields much lower perplexity and higher BLEU, but models still fail to reliably compute answers (accuracy around chance), indicating that copying alone cannot replace explicit arithmetic computation. Output-copy provides marginal gains over input-copy for perplexity/BLEU (because repeated tokens can be re-used) but does not materially change answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Neural seq2seq models without explicit arithmetic operations or program induction are insufficient to solve these algebraic problems: adding copying helps generate plausible rationales (lower perplexity, higher BLEU) but does not enable correct problem solving; explicit program induction and arithmetic operations are necessary to produce correct answers at a substantially higher rate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sequence to sequence learning with neural networks <em>(Rating: 2)</em></li>
                <li>Neural machine translation by jointly learning to align and translate <em>(Rating: 2)</em></li>
                <li>Learning to automatically solve algebra word problems <em>(Rating: 2)</em></li>
                <li>Solving general arithmetic word problems <em>(Rating: 2)</em></li>
                <li>Latent predictor networks for code generation <em>(Rating: 2)</em></li>
                <li>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8466",
    "paper_id": "paper-b123a0d46ad917b79c43c5ae981e03ed2458ed11",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Program-Induced Rationale Model",
            "name_full": "Program Induction by Rationale Generation (this paper's model)",
            "brief_description": "A sequence-to-sequence model that generates a latent program (sequence of instructions invoking a pre-defined set of operations and an external memory) which, when executed, produces a human-readable rationale interleaved with algebraic operations and the final answer; training uses heuristic search over latent programs constrained by the rationale tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Program-Induced Rationale Model (Ling et al., this paper)",
            "model_description": "Two-layer LSTM encoder-decoder with attention (hidden size 200, embeddings 200) that emits a latent sequence of instructions z (operations, argument pointers or softmax tokens, and destination: output or memory). Uses a pre-defined set of 22 operations (arithmetic, trig, conversions, Choose, Str_to_Float, Float_to_Str, etc.), external memory buffer for intermediate numeric values, pointer networks for copying from input/output, staged backpropagation to handle long instruction sequences, heuristic search over candidate instructions (graph expansion depth D=5) and beam decoding (beam=200).",
            "reasoning_methods": [
                "Rationale generation (chain-of-thought-like textual decomposition)",
                "Latent program induction (generating and executing discrete operations)",
                "Symbolic arithmetic and numeric operations (Add, Subtract, Multiply, Divide, Power, Log, Sqrt, Sine, Cosine, Tangent, Factorial, Choose)",
                "External memory for intermediate values",
                "Copy mechanisms (copy-input and copy-output / pointer networks)",
                "Heuristic/search-guided decoding constrained by rationale tokens"
            ],
            "reasoning_methods_description": "The model explicitly generates a latent program z where each instruction is an operation (from a fixed inventory of 22 ops) with arguments chosen via softmax or pointer copies; some instruction results are written to an external memory and later referenced (enabling multi-step symbolic computation). The rationale is produced as output tokens interleaved with execution-driven numeric results; this textual rationale functions like a chain-of-thought and constrains the search for executable programs. Copy mechanisms let the model reuse tokens from input or previously produced output; staged backprop prevents memory explosion when training on long instruction sequences. Decoding applies heuristic filtering to only keep instructions that can generate the next rationale token (allowing at most one level of indirection during the constrained search).",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared the program-induced rationale model against standard attention seq2seq baselines and incremental augmentations: (1) Seq2Seq (attention-only), (2) Seq2Seq + copy-input pointer, (3) Seq2Seq + copy-output pointer, and (4) the program-induced rationale model. Evaluation metrics were rationale perplexity, BLEU-4 for rationale generation, and multiple-choice answer accuracy. The paper also discusses that rationale supervision constrains program search (heuristic expansion depth D=5) but does not present additional ablations explicitly varying the number/types of operations beyond these model comparisons.",
            "task_or_benchmark": "Custom 100,949-training-sample algebraic word-problem dataset with natural-language rationales and multiple-choice options (held-out dev/test 250 each); task requires producing the rationale and selecting the correct option.",
            "performance_results": "On the held-out test set: Perplexity (lower better): Seq2Seq 524.7, +Copy Input 46.8, +Copy Output 45.9, Our Model 28.5. BLEU-4 (higher better): Seq2Seq 8.57, +Copy Input 21.3, +Copy Output 20.6, Our Model 27.2. Accuracy (multiple-choice): Seq2Seq 20.8%, +Copy Input 20.4%, +Copy Output 20.2%, Our Model 36.4%. These numeric results show stepwise gains from adding copy mechanisms and a large gain from the program-induced rationale approach.",
            "qualitative_findings": "Baseline seq2seq produces very high perplexity and often degenerate/short rationales (e.g., \"Answer should be D\"); adding input copy reduces perplexity and produces more plausible rationales because variables from the problem can be reused; output-copy gives small additional improvements because repeated tokens can be copied after first generation. The program-induced model can generate intermediate computed values (e.g., '1326', '1/221') via arithmetic ops, leading to markedly better perplexity, BLEU and roughly doubling accuracy vs. baselines. The paper also notes that rationale supervision is crucial: without modeling rationales the search space for latent programs is too unconstrained and learning fails. However, generating long, complex multi-step rationales remains challenging.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Generating answer rationales as a latent variable that constrains program induction is an effective indirect supervision strategy for learning arithmetic programs; the ability to perform algebraic manipulations (via discrete operations and memory) during generation is essential for solving algebraic word problems and yields substantial improvements in rationale quality and answer accuracy compared to standard seq2seq models with and without copying. The paper claims indirect supervision via rationales makes program induction tractable where direct induction from question-answer pairs would be intractable.",
            "uuid": "e8466.0",
            "source_info": {
                "paper_title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Seq2Seq Baselines",
            "name_full": "Attention-based Sequence-to-Sequence with Copy Mechanisms (baseline variants)",
            "brief_description": "Standard attention encoder-decoder models (seq2seq) and augmented versions with input-copy and output-copy pointer mechanisms used as baselines to generate rationales and answers; these are purely neural token-generation approaches without explicit discrete operation execution.",
            "citation_title": "Neural machine translation by jointly learning to align and translate",
            "mention_or_use": "use",
            "model_name": "Seq2Seq; Seq2Seq + Copy-Input; Seq2Seq + Copy-Output",
            "model_description": "Attention-based encoder-decoder (Bahdanau-style) trained to output rationales and final option token. Augmented baselines include a pointer network to copy tokens from the input (copy-input) and a pointer mechanism to copy from previously generated output tokens (copy-output). Implementations follow conventional softmax generation except for pointer additions.",
            "reasoning_methods": [
                "Direct token-level generation (single-step, monolithic neural inference)",
                "Copying from input text (pointer networks)",
                "Copying from output (pointer / output-attention)"
            ],
            "reasoning_methods_description": "These baselines reason by learning to map input tokens to output tokens directly through distributed neural representations; they do not execute discrete symbolic operations. Copy-input allows reuse of values present in the problem text to alleviate sparsity of numeric tokens; copy-output allows reuse of previously generated outputs to repeat computed tokens.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Ablation-style incremental comparison: Seq2Seq (attention) alone, then add copy-input, then add copy-output; metrics measured on same dataset (perplexity, BLEU-4, accuracy) to quantify gains from adding copying mechanisms.",
            "task_or_benchmark": "Same custom algebraic word-problem dataset with rationales and multiple-choice answers (this paper's dataset).",
            "performance_results": "See paper table: Seq2Seq Perplexity 524.7, BLEU 8.57, Accuracy 20.8%; +Copy Input Perplexity 46.8, BLEU 21.3, Accuracy 20.4%; +Copy Output Perplexity 45.9, BLEU 20.6, Accuracy 20.2%. Copy mechanisms drastically reduce perplexity and improve BLEU but do not improve answer accuracy beyond chance levels.",
            "qualitative_findings": "Plain seq2seq struggles with numeric sparsity and generates short/degenerate rationales; copy-input fixes many numeric token generation issues and yields much lower perplexity and higher BLEU, but models still fail to reliably compute answers (accuracy around chance), indicating that copying alone cannot replace explicit arithmetic computation. Output-copy provides marginal gains over input-copy for perplexity/BLEU (because repeated tokens can be re-used) but does not materially change answer correctness.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Neural seq2seq models without explicit arithmetic operations or program induction are insufficient to solve these algebraic problems: adding copying helps generate plausible rationales (lower perplexity, higher BLEU) but does not enable correct problem solving; explicit program induction and arithmetic operations are necessary to produce correct answers at a substantially higher rate.",
            "uuid": "e8466.1",
            "source_info": {
                "paper_title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
                "publication_date_yy_mm": "2017-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sequence to sequence learning with neural networks",
            "rating": 2
        },
        {
            "paper_title": "Neural machine translation by jointly learning to align and translate",
            "rating": 2
        },
        {
            "paper_title": "Learning to automatically solve algebra word problems",
            "rating": 2
        },
        {
            "paper_title": "Solving general arithmetic word problems",
            "rating": 2
        },
        {
            "paper_title": "Latent predictor networks for code generation",
            "rating": 2
        },
        {
            "paper_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
            "rating": 2
        }
    ],
    "cost": 0.010791249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems</h1>
<p>Wang Ling ${ }^{\text {D }}$ Dani Yogatama ${ }^{\text { }}$ Chris Dyer ${ }^{\text { }}$ Phil Blunsom ${ }^{\text {® }}$<br>$\triangle$ DeepMind $\diamond$ University of Oxford<br>{lingwang, dyogatama,cdyer,pblunsom}@google.com</p>
<h4>Abstract</h4>
<p>Solving algebraic word problems requires executing a series of arithmetic operations-a program-to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000 -sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.</p>
<h2>1 Introduction</h2>
<p>Behaving intelligently often requires mathematical reasoning. Shopkeepers calculate change, tax, and sale prices; agriculturists calculate the proper amounts of fertilizers, pesticides, and water for their crops; and managers analyze productivity. Even determining whether you have enough money to pay for a list of items requires applying addition, multiplication, and comparison. Solving these tasks is challenging as it involves recognizing how goals, entities, and quantities in the real-world map onto a mathematical formalization, computing the solution, and mapping the solution back onto the world. As a proxy for the richness of the real world, a series of papers have
used natural language specifications of algebraic word problems, and solved these by either learning to fill in templates that can be solved with equation solvers (Hosseini et al., 2014; Kushman et al., 2014) or inferring and modeling operation sequences (programs) that lead to the final answer (Roy and Roth, 2015).</p>
<p>In this paper, we learn to solve algebraic word problems by inducing and modeling programs that generate not only the answer, but an answer rationale, a natural language explanation interspersed with algebraic expressions justifying the overall solution. Such rationales are what examiners require from students in order to demonstrate understanding of the problem solution; they play the very same role in our task. Not only do natural language rationales enhance model interpretability, but they provide a coarse guide to the structure of the arithmetic programs that must be executed. In fact the learner we propose (which relies on a heuristic search; $\S 4$ ) fails to solve this task without modeling the rationales-the search space is too unconstrained.</p>
<p>This work is thus related to models that can explain or rationalize their decisions (Hendricks et al., 2016; Harrison et al., 2017). However, the use of rationales in this work is quite different from the role they play in most prior work, where interpretation models are trained to generate plausible sounding (but not necessarily accurate) posthoc descriptions of the decision making process they used. In this work, the rationale is generated as a latent variable that gives rise to the answer-it is thus a more faithful representation of the steps used in computing the answer.</p>
<p>This paper makes three contributions. First, we have created a new dataset with more than 100,000 algebraic word problems that includes both answers and natural language answer rationales (§2). Figure 1 illustrates three representative instances</p>
<p>Problem 1:
Question: Two trains running in opposite directions cross a man standing on the platform in 27 seconds and 17 seconds respectively and they cross each other in 23 seconds. The ratio of their speeds is:
Options: A) $3 / 7$ B) $3 / 2$ C) $3 / 88$ D) $3 / 8$ E) $2 / 2$
Rationale: Let the speeds of the two trains be $\mathrm{x} \mathrm{m} / \mathrm{sec}$ and y m/sec respectively. Then, length of the first train $=27 \mathrm{x}$ meters, and length of the second train $=17 \mathrm{y}$ meters. $(27 \mathrm{x}+17 \mathrm{y}) /(\mathrm{x}+$ $\mathrm{y})=23 \rightarrow 27 \mathrm{x}+17 \mathrm{y}=23 \mathrm{x}+23 \mathrm{y} \rightarrow 4 \mathrm{x}=6 \mathrm{y} \rightarrow \mathrm{x} / \mathrm{y}=3 / 2$.
Correct Option: B
Problem 2:
Question: From a pack of 52 cards, two cards are drawn together at random. What is the probability of both the cards being kings?
Options: A) $2 / 1223$ B) $1 / 122$ C) $1 / 221$ D) $3 / 1253$ E) $2 / 153$
Rationale: Let $s$ be the sample space.
Then $m \mathrm{~s})=52 \mathrm{C} 2=1326$
$\mathrm{E}=$ event of getting 2 kings out of 4
$\mathrm{n}(\mathrm{E})=4 \mathrm{C} 2=6$
$\mathrm{P}(\mathrm{E})=6 / 1326=1 / 221$
Answer is C
Correct Option: C
Problem 3:
Question: For which of the following does $p(a)-p(b)=p(a b)$ for all values of $a$ and $b$ ?
Options:A) $p(x)=x^{2}$, B) $p(x)=x / 2$, C) $p(x)=x+5$, D) $p(x)=2 \mathrm{ct}$, E) $p(x)=|x|$
Rationale: To solve this easiest way is just put the value and see that if it equals or not,
with option A. $p(a)=a^{2}$ and $p(b)=b^{2}$
so L.H.S $=a^{2}-b^{2}$
and R.H.S $=(a-b)^{2} \rightarrow a^{2}+b^{2}-2 a b$.
so L.H.S not equal to R.H.S
with option B. $p(a)=a / 2$ and $p(b)=b / 2$
L.H.S $=a / 2-b / 2 \rightarrow 1 / 2(a-b)$
R.H.S $=(a-b) / 2$
so L.H.S $=$ R.H.S which is the correct answer.
answer:B
Correct Option: B
Figure 1: Examples of solved math problems.
from the dataset. Second, we propose a sequence to sequence model that generates a sequence of instructions that, when executed, generates the rationale; only after this is the answer chosen (§3). Since the target program is not given in the training data (most obviously, its specific form will depend on the operations that are supported by the program interpreter); the third contribution is thus a technique for inferring programs that generate a rationale and, ultimately, the answer. Even constrained by a text rationale, the search space of possible programs is quite large, and we employ a heuristic search to find plausible next steps to guide the search for programs (§4). Empirically, we are able to show that state-of-the-art sequence to sequence models are unable to perform above chance on this task, but that our model doubles the accuracy of the baseline (§6).</p>
<h2>2 Dataset</h2>
<p>We built a dataset with 100,000 problems with the annotations shown in Figure 1. Each question is decomposed in four parts, two inputs and two outputs: the description of the problem, which we will denote as the question, and the possible (multiple choice) answer options, denoted as options. Our goal is to generate the description of the rationale used to reach the correct answer, denoted as rationale and the correct option label. Problem 1 illustrates an example of an algebra problem, which must be translated into an expression (i.e., $(27 x+17 y) /(x+y)=23)$ and then the desired quantity $(x / y)$ solved for. Problem 2 is an example that could be solved by multi-step arithmetic operations proposed in (Roy and Roth, 2015). Finally, Problem 3 describes a problem that is solved by testing each of the options, which has not been addressed in the past.</p>
<h3>2.1 Construction</h3>
<p>We first collect a set of 34,202 seed problems that consist of multiple option math questions covering a broad range of topics and difficulty levels. Examples of exams with such problems include the GMAT (Graduate Management Admission Test) and GRE (General Test). Many websites contain example math questions in such exams, where the answer is supported by a rationale.</p>
<p>Next, we turned to crowdsourcing to generate new questions. We create a task where users are presented with a set of 5 questions from our seed dataset. Then, we ask the Turker to choose one of the questions and write a similar question. We also force the answers and rationale to differ from the original question in order to avoid paraphrases of the original question. Once again, we manually check a subset of the jobs for each Turker for quality control. The type of questions generated using this method vary. Some turkers propose small changes in the values of the questions (e.g., changing the equality $p(a) p(b)=p(a b)$ in Problem 3 to a different equality is a valid question, as long as the rationale and options are rewritten to reflect the change). We designate these as replica problems as the natural language used in the question and rationales tend to be only minimally unaltered. Others propose new problems in the same topic where the generated questions tend to differ more radically from existing ones. Some Turkers also copy math problems available on the web, and we</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Training Examples</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100,949</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dev Examples</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">250</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Test Examples</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">250</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Numeric</td>
<td style="text-align: center;">Average Length</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vocab Size</td>
<td style="text-align: center;">21,009</td>
<td style="text-align: center;">14,745</td>
</tr>
<tr>
<td style="text-align: center;">Non-Numeric</td>
<td style="text-align: center;">Average Length</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">89.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vocab Size</td>
<td style="text-align: center;">17,849</td>
<td style="text-align: center;">25,034</td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Average Length</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">105.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vocab Size</td>
<td style="text-align: center;">38,858</td>
<td style="text-align: center;">39,779</td>
</tr>
</tbody>
</table>
<p>Table 1: Descriptive statistics of our dataset.
define in the instructions that this is not allowed, as it will generate multiple copies of the same problem in the dataset if two or more Turkers copy from the same resource. These Turkers can be detected by checking the nearest neighbours within the collected datasets as problems obtained from online resources are frequently submitted by more than one Turker. Using this method, we obtained 70,318 additional questions.</p>
<h3>2.2 Statistics</h3>
<p>Descriptive statistics of the dataset is shown in Figure 1. In total, we collected 104,519 problems (34,202 seed problems and 70,318 crowdsourced problems). We removed 500 problems as heldout set ( 250 for development and 250 for testing). As replicas of the heldout problems may be present in the training set, these were removed manually by listing for each heldout instance the closest problems in the training set in terms of character-based Levenstein distance. After filtering, 100,949 problems remained in the training set.</p>
<p>We also show the average number of tokens (total number of tokens in the question, options and rationale) and the vocabulary size of the questions and rationales. Finally, we provide the same statistics exclusively for tokens that are numeric values and tokens that are not.</p>
<p>Figure 2 shows the distribution of examples based on the total number of tokens. We can see that most examples consist of 30 to 500 tokens, but there are also extremely long examples with more than 1000 tokens in our dataset.</p>
<h2>3 Model</h2>
<p>Generating rationales for math problems is challenging as it requires models that learn to perform math operations at a finer granularity as each step within the solution must be explained. For instance, in Problem 1, the equation $(27 x+$
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Distribution of examples per length.
$17 y) /(x+y)=23$ must be solved to obtain the answer. In previous work (Kushman et al., 2014), this could be done by feeding the equation into an expression solver to obtain $x / y=3 / 2$. However, this would skip the intermediate steps $27 x+17 y=23 x+23 y$ and $4 x=6 y$, which must also be generated in our problem. We propose a model that jointly learns to generate the text in the rationale, and to perform the math operations required to solve the problem. This is done by generating a program, containing both instructions that generate output and instructions that simply generate intermediate values used by following instructions.</p>
<h3>3.1 Problem Definition</h3>
<p>In traditional sequence to sequence models (Sutskever et al., 2014; Bahdanau et al., 2014), the goal is to predict the output sequence $\boldsymbol{y}=y_{1}, \ldots, y_{|\boldsymbol{y}|}$ from the input sequence $\boldsymbol{x}=x_{1}, \ldots, x_{|\boldsymbol{x}|}$, with lengths $|\boldsymbol{y}|$ and $|\boldsymbol{x}|$.</p>
<p>In our particular problem, we are given the problem and the set of options, and wish to predict the rationale and the correct option. We set $\boldsymbol{x}$ as the sequence of words in the problem, concatenated with words in each of the options separated by a special tag. Note that knowledge about the possible options is required as some problems are solved by the process of elimination or by testing each of the options (e.g. Problem 3). We wish to generate $\boldsymbol{y}$, which is the sequence of words in the rationale. We also append the correct option as the last word in $\boldsymbol{y}$, which is interpreted as the chosen option. For example, $\boldsymbol{y}$ in Problem 1 is "Let the $\ldots=3 / 2 .\langle$ EOR $\rangle$ B $\langle$ EOS $\rangle$ ", whereas in Problem 2 it is "Let s be ... Answer is C $\langle$ EOR $\rangle$ C $\langle$ EOS $\rangle$ ", where " $\langle$ EOS $\rangle$ " is the end of sentence symbol and " $\langle$ EOR $\rangle$ " is the end of rationale symbol.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$i$</th>
<th style="text-align: center;">$\boldsymbol{x}$</th>
<th style="text-align: center;">$\boldsymbol{z}$</th>
<th style="text-align: center;">$\boldsymbol{v}$</th>
<th style="text-align: center;">$\boldsymbol{r}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">From</td>
<td style="text-align: center;">Id("Let")</td>
<td style="text-align: center;">Let</td>
<td style="text-align: center;">$y_{1}$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">a</td>
<td style="text-align: center;">Id("s")</td>
<td style="text-align: center;">$s$</td>
<td style="text-align: center;">$y_{2}$</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">pack</td>
<td style="text-align: center;">Id("be")</td>
<td style="text-align: center;">be</td>
<td style="text-align: center;">$y_{3}$</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">Id("the")</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">$y_{4}$</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">Id("sample")</td>
<td style="text-align: center;">sample</td>
<td style="text-align: center;">$y_{5}$</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">cards</td>
<td style="text-align: center;">Id("space")</td>
<td style="text-align: center;">space</td>
<td style="text-align: center;">$y_{6}$</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">,</td>
<td style="text-align: center;">Id(".")</td>
<td style="text-align: center;">.</td>
<td style="text-align: center;">$y_{7}$</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">two</td>
<td style="text-align: center;">Id("'s")</td>
<td style="text-align: center;">$\backslash n$</td>
<td style="text-align: center;">$y_{8}$</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">cards</td>
<td style="text-align: center;">Id("Then")</td>
<td style="text-align: center;">Then</td>
<td style="text-align: center;">$y_{9}$</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">are</td>
<td style="text-align: center;">Id("n")</td>
<td style="text-align: center;">$n$</td>
<td style="text-align: center;">$y_{10}$</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">drawn</td>
<td style="text-align: center;">Id("(")</td>
<td style="text-align: center;">$i$</td>
<td style="text-align: center;">$y_{11}$</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">together</td>
<td style="text-align: center;">Id("s")</td>
<td style="text-align: center;">$s$</td>
<td style="text-align: center;">$y_{12}$</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">at</td>
<td style="text-align: center;">Id(" $i$ ")</td>
<td style="text-align: center;">)</td>
<td style="text-align: center;">$y_{13}$</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">Id("=")</td>
<td style="text-align: center;">=</td>
<td style="text-align: center;">$y_{14}$</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">.</td>
<td style="text-align: center;">Str_to_Float $\left(x_{5}\right)$</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">$\underline{m_{1}}$</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">What</td>
<td style="text-align: center;">Float_to_Str $\left(m_{1}\right)$</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">$y_{15}$</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">is</td>
<td style="text-align: center;">Id("C")</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">$y_{16}$</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">Id("2")</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$y_{17}$</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">probability</td>
<td style="text-align: center;">Id("=")</td>
<td style="text-align: center;">$=$</td>
<td style="text-align: center;">$y_{18}$</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">Str_to_Float $\left(y_{17}\right)$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$m_{2}$</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">both</td>
<td style="text-align: center;">Choose $\left(m_{1}, m_{2}\right)$</td>
<td style="text-align: center;">1326</td>
<td style="text-align: center;">$\underline{m_{3}}$</td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">cards</td>
<td style="text-align: center;">Float_to_Str $\left(m_{3}\right)$</td>
<td style="text-align: center;">1326</td>
<td style="text-align: center;">$y_{19}$</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">being</td>
<td style="text-align: center;">Id("E")</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">$y_{20}$</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">kings</td>
<td style="text-align: center;">Id("=")</td>
<td style="text-align: center;">$=$</td>
<td style="text-align: center;">$y_{21}$</td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">Id("event")</td>
<td style="text-align: center;">event</td>
<td style="text-align: center;">$y_{22}$</td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">$&lt;$ O $&gt;$</td>
<td style="text-align: center;">Id("of")</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">$y_{23}$</td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">A)</td>
<td style="text-align: center;">Id("getting")</td>
<td style="text-align: center;">getting</td>
<td style="text-align: center;">$y_{24}$</td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2/1223</td>
<td style="text-align: center;">Id("2")</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$y_{25}$</td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">$&lt;$ O $&gt;$</td>
<td style="text-align: center;">Id("kings")</td>
<td style="text-align: center;">kings</td>
<td style="text-align: center;">$y_{26}$</td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">B)</td>
<td style="text-align: center;">Id("out")</td>
<td style="text-align: center;">out</td>
<td style="text-align: center;">$y_{27}$</td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">1/122</td>
<td style="text-align: center;">Id("of")</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">$y_{28}$</td>
</tr>
<tr>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
</tr>
<tr>
<td style="text-align: center;">$</td>
<td style="text-align: center;">\boldsymbol{z}</td>
<td style="text-align: center;">$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Id("〈EOS〉")</td>
</tr>
</tbody>
</table>
<p>Table 2: Example of a program $\boldsymbol{z}$ that would generate the output $\boldsymbol{y}$. In $\boldsymbol{v}$, italics indicates string types; bold indicates float types. Refer to $\S 3.3$ for description of variable names.</p>
<h3>3.2 Generating Programs to Generate Rationales</h3>
<p>We wish to generate a latent sequence of program instructions, $\boldsymbol{z}=z_{1}, \ldots, z_{|\boldsymbol{z}|}$, with length $|\boldsymbol{z}|$, that will generate $\boldsymbol{y}$ when executed.</p>
<p>We express $\boldsymbol{z}$ as a program that can access $\boldsymbol{x}, \boldsymbol{y}$, and the memory buffer $\boldsymbol{m}$. Upon finishing execution we expect that the sequence of output tokens to be placed in the output vector $\boldsymbol{y}$.</p>
<p>Table 2 illustrates an example of a sequence of instructions that would generate an excerpt from Problem 2, where columns $\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{v}$, and $\boldsymbol{r}$ denote the input sequence, the instruction sequence (program), the values of executing the instruction, and where each value $v_{i}$ is written (i.e., either to the output or to the memory). In this example, instructions from indexes 1 to 14 simply fill each position with the observed output $y_{1}, \ldots, y_{14}$ with a string, where the Id operation simply returns its parame-
ter without applying any operation. As such, running this operation is analogous to generating a word by sampling from a softmax over a vocabulary. However, instruction $z_{15}$ reads the input word $x_{5}, 52$, and applies the operation Str_to_Float, which converts the word 52 into a floating point number, and the same is done for instruction $z_{20}$, which reads a previously generated output word $y_{17}$. Unlike, instructions $z_{1}, \ldots, z_{14}$, these operations write to the external memory $\boldsymbol{m}$, which stores intermediate values. A more sophisticated instruction-which shows some of the power of our model-is $z_{21}=\operatorname{Choose}\left(m_{1}, m_{2}\right) \rightarrow m_{3}$ which evaluates $\binom{m_{1}}{m_{2}}$ and stores the result in $m_{3}$. This process repeats until the model generates the end-of-sentence symbol. The last token of the program as said previously must generate the correct option value, from "A" to "E".</p>
<p>By training a model to generate instructions that can manipulate existing tokens, the model benefits from the additional expressiveness needed to solve math problems within the generation process. In total we define 22 different operations, 13 of which are frequently used operations when solving math problems. These are: Id, Add, Subtract, Multiply, Divide, Power, Log, Sqrt, Sine, Cosine, Tangent, Factorial, and Choose (number of combinations). We also provide 2 operations to convert between Radians and Degrees, as these are needed for the sine, cosine and tangent operations. There are 6 operations that convert floating point numbers into strings and vice-versa. These include the Str_to_Float and Float_to_Str operations described previously, as well as operations which convert between floating point numbers and fractions, since in many math problems the answers are in the form " $3 / 4$ ". For the same reason, an operation to convert between a floating point number and number grouped in thousands is also used (e.g. 1000000 to " $1,000,000$ " or " 1.000 .000 "). Finally, we define an operation (Check) that given the input string, searches through the list of options and returns a string with the option index in {"A", "B", "C", "D", "E"}. If the input value does not match any of the options, or more than one option contains that value, it cannot be applied. For instance, in Problem 2, once the correct probability " $1 / 221$ " is generated, by applying the check operation to this number we can obtain correct option "C".</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Illustration of the generation process of a single instruction tuple at timestamp $i$.</p>
<h3>3.3 Generating and Executing Instructions</h3>
<p>In our model, programs consist of sequences of instructions, $\boldsymbol{z}$. We turn now to how we model each $z_{i}$, conditional on the text program specification, and the program's history. The instruction $z_{i}$ is a tuple consisting of an operation $\left(o_{i}\right)$, an ordered sequence of its arguments $\left(\boldsymbol{a}<em i="i">{i}\right)$, and a decision about where its results will be placed $\left(r</em>}\right)$ (is it appended in the output $\boldsymbol{y}$ or in a memory buffer $\boldsymbol{m}$ ?), and the result of applying the operation to its arguments $\left(v_{i}\right)$. That is, $z_{i}=\left(o_{i}, r_{i}, \boldsymbol{a<em i="i">{i}, v</em>\right)$.</p>
<p>Formally, $o_{i}$ is an element of the pre-specified set of operations $\mathcal{O}$, which contains, for example add, div, Str_to_Float, etc. The number of arguments required by $o_{i}$ is given by $\operatorname{argc}\left(o_{i}\right)$, e.g., $\operatorname{argc}(\mathrm{add})=2$ and $\operatorname{argc}(\log )=1$. The arguments are $\boldsymbol{a}<em 1="1" i_="i,">{i}=a</em>$. We define the instruction probability as:}, \ldots, a_{i, \operatorname{argc}\left(o_{i}\right)}$. An instruction will generate a return value $v_{i}$ upon execution, which will either be placed in the output $\boldsymbol{y}$ or hidden. This decision is controlled by $r_{i</p>
<p>$$
\begin{aligned}
&amp; p\left(o_{i}, \boldsymbol{a}<em i="i">{i}, r</em>}, v_{i} \mid \boldsymbol{z<em i="i">{&lt;i}, \boldsymbol{x}, \boldsymbol{y}, \boldsymbol{m}\right)= \
&amp; p\left(o</em>} \mid \boldsymbol{z<em i="i">{&lt;i}, \boldsymbol{x}\right) \times p\left(r</em>} \mid \boldsymbol{z<em i="i">{&lt;i}, \boldsymbol{x}, o</em>\right) \times \
&amp; \prod_{j=1}^{\operatorname{argc}\left(o_{i}\right)} p\left(a_{i, j} \mid \boldsymbol{z}<em i="i">{&lt;i}, \boldsymbol{x}, o</em>\right) \times \
&amp; {\left[v_{i}=\operatorname{apply}\left(o_{i}, \boldsymbol{a}\right)\right] }
\end{aligned}
$$}, \boldsymbol{m}, \boldsymbol{y</p>
<p>where $[p]$ evaluates to 1 if $p$ is true and 0 otherwise, and $\operatorname{apply}(f, \boldsymbol{x})$ evaluates the operation $f$ with arguments $\boldsymbol{x}$. Note that the apply function is not learned, but pre-defined.</p>
<p>The network used to generate an instruction at a given timestamp $i$ is illustrated in Figure 3. We first use the recurrent state $\mathbf{h}<em i="i">{i}$ to generate $p\left(o</em>}\right.$ $\left.\boldsymbol{z<em o__i="o_{i">{&lt;i}, \boldsymbol{x}\right)=\operatorname{softmax}</em>} \in \mathcal{O}}\left(\mathbf{h<em i="i">{i}\right)$, using a softmax over the
set of available operations $\mathcal{O}$.
In order to predict $r</em>}$, we generate a new hidden state $\mathbf{r<em i="i">{i}$, which is a function of the current program context $\mathbf{h}</em>}$, and an embedding of the current predicted operation, $o_{i}$. As the output can either be placed in the memory $\boldsymbol{m}$ or the output $\boldsymbol{y}$, we compute the probability $p\left(r_{i}=\right.$ OUTPUT $\mid$ $\left.\boldsymbol{z<em i="i">{&lt;i}, \boldsymbol{x}, o</em>}\right)=\sigma\left(\mathbf{r<em r="r">{i} \cdot \mathbf{w}</em>$.}+b_{r}\right)$, where $\sigma$ is the logistic sigmoid function. If $r_{i}=$ OUTPUT, $v_{i}$ is appended to the output $\boldsymbol{y}$; otherwise it is appended to the memory $\boldsymbol{m</p>
<p>Once we generate $r_{i}$, we must predict $\boldsymbol{a}<em i="i">{i}$, the $\operatorname{argc}\left(o</em>}\right)$-length sequence of arguments that operation $o_{i}$ requires. The $j$ th argument $a_{i, j}$ can be either generated from a softmax over the vocabulary, copied from the input vector $\boldsymbol{x}$, or copied from previously generated values in the output $\boldsymbol{y}$ or memory $\boldsymbol{m}$. This decision is modeled using a latent predictor network (Ling et al., 2016), where the control over which method used to generate $a_{i, j}$ is governed by a latent variable $q_{i, j} \in$ {SOFTMAX, COPY-INPUT, COPY-OUTPUT}. Similar to when predicting $r_{i}$, in order to make this choice, we also generate a new hidden state for each argument slot $j$, denoted by $\mathbf{q<em i="i">{i, j}$ with an LSTM, feeding the previous argument in at each time step, and initializing it with $\mathbf{r}</em>$.}$ and by reading the predicted value of the output $r_{i</p>
<ul>
<li>If $q_{i, j}=$ SOFTMAX, $a_{i, j}$ is generated by sampling from a softmax over the vocabulary $\mathcal{Y}$,</li>
</ul>
<p>$$
p\left(a_{i, j} \mid q_{i, j}=\operatorname{SOFTMAX}\right)=\underset{a_{i, j} \in \mathcal{Y}}{\operatorname{softmax}}\left(\mathbf{q}_{i, j}\right)
$$</p>
<p>This corresponds to a case where a string is used as argument (e.g. $y_{1}=$ "Let").</p>
<ul>
<li>If $q_{i, j}=$ COPY-INPUT, $a_{i, j}$ is obtained by copying an element from the input vector with a pointer network (Vinyals et al., 2015) over input words $x_{1}, \ldots, x_{|\boldsymbol{x}|}$, represented by their encoder LSTM state $\mathbf{u}<em _boldsymbol_x="|\boldsymbol{x">{1}, \ldots, \mathbf{u}</em>$. As such, we compute the probability distribution over input words as:}|</li>
</ul>
<p>$$
\begin{aligned}
p\left(a_{i, j} \mid q_{i, j}=\right. &amp; \text { COPY-INPUT }) \
&amp; \left.\underset{a_{i, j} \in x_{1}, \ldots, x_{|\boldsymbol{x}|}}{\operatorname{softmax}}\left(f\left(\mathbf{u}<em i_="i," j="j">{a</em>\right)\right)\right.
\end{aligned}
$$}}, \mathbf{q}_{i, j</p>
<p>Function $f$ computes the affinity of each token $x_{a_{i, j}}$ and the current output context $\mathbf{q}<em a__i_="a_{i," j="j">{i, j}$. A common implementation of $f$, which we follow, is to apply a linear projection from $\left[\mathbf{u}</em>]$ is vector concatenation), followed by a tanh and a linear projection into a single value.}} ; \mathbf{q}_{i, j}\right]$ into a fixed size vector (where $[\mathbf{u} ; \mathbf{v</p>
<ul>
<li>If $q_{i, j}=$ COPY-OUTPUT, the model copies from either the output $\boldsymbol{y}$ or the memory $\boldsymbol{m}$. This is equivalent to finding the instruction $z_{i}$, where the value was generated. Once again, we define a pointer network that points to the output instructions and define the distribution over previously generated instructions as:</li>
</ul>
<p>$$
\begin{aligned}
p\left(a_{i, j} \mid q_{i, j}=\right. &amp; \text { COPY-OUTPUT }) \
&amp; \left.\operatorname{softmax}<em i_="i," j="j">{a</em>} \in z_{1}, \ldots, z_{i-1}}\left(f\left(\mathbf{h<em i_="i," j="j">{a</em>\right)\right)\right.
\end{aligned}
$$}}, \mathbf{q}_{i, j</p>
<p>Here, the affinity is computed using the decoder state $\mathbf{h}<em i_="i," j="j">{a</em>$.}}$ and the current state $\mathbf{q}_{i, j</p>
<p>Finally, we embed the argument $a_{i, j}{ }^{1}$ and the state $\mathbf{q}<em i_="i," j_1="j+1">{i, j}$ to generate the next state $\mathbf{q}</em>}$. Once all arguments for $o_{i}$ are generated, the operation is executed to obtain $v_{i}$. Then, the embedding of $v_{i}$, the final state of the instruction $\mathbf{q<em i="i">{i,\left|\boldsymbol{a}</em>}\right|}$ and the previous state $\mathbf{h<em i_1="i+1">{i}$ are used to generate the state at the next timestamp $\mathbf{h}</em>$.</p>
<h2>4 Inducing Programs while Learning</h2>
<p>The set of instructions $\boldsymbol{z}$ that will generate $\boldsymbol{y}$ is unobserved. Thus, given $\boldsymbol{x}$ we optimize the marginal probability function:
$p(\boldsymbol{y} \mid \boldsymbol{x})=\sum_{\boldsymbol{z} \in \mathcal{Z}} p(\boldsymbol{y} \mid \boldsymbol{z}) p(\boldsymbol{z} \mid \boldsymbol{x})=\sum_{\boldsymbol{z} \in \mathcal{Z}(y)} p(\boldsymbol{z} \mid \boldsymbol{x})$
where $p(\boldsymbol{y} \mid \boldsymbol{z})$ is the Kronecker delta function $\delta_{e(\boldsymbol{z}), \boldsymbol{y}}$, which is 1 if the execution of $\boldsymbol{z}$, denoted as $e(\boldsymbol{z})$, generates $y$ and 0 otherwise. Thus, we can redefine $p(\boldsymbol{y} \mid \boldsymbol{x})$, the marginal over all programs $\mathcal{Z}$, as a marginal over programs that would generate $\boldsymbol{y}$, defined as $\mathcal{Z}(\boldsymbol{y})$. As marginalizing over $\boldsymbol{z} \in$ $\mathcal{Z}(\boldsymbol{y})$ is intractable, we approximate the marginal by generating samples from our model. Denote the set of samples that are generated by $\hat{\mathcal{Z}}(\boldsymbol{y})$. We maximize $\sum \boldsymbol{z} \in \hat{\mathcal{Z}}(\boldsymbol{y}) p(\boldsymbol{z} \mid \boldsymbol{x})$.</p>
<p>However, generating programs that generate $\boldsymbol{y}$ is not trivial, as randomly sampling from the RNN distribution over instructions at each timestamp is unlikely to generate a sequence $\boldsymbol{z} \in \mathcal{Z}(\boldsymbol{y})$.</p>
<p>This is analogous to the question answering work in Liang et al. (2016), where the query that</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>generates the correct answer must be found during inference, and training proved to be difficult without supervision. In Roy and Roth (2015) this problem is also addressed by adding prior knowledge to constrain the exponential space.</p>
<p>In our work, we leverage the fact that we are generating rationales, where there is a sense of progression within the rationale. That is, we assume that the rationale solves the problem step by step. For instance, in Problem 2, the rationale first describes the number of combinations of two cards in a deck of 52 cards, then describes the number of combinations of two kings, and finally computes the probability of drawing two kings. Thus, while generating the final answer without the rationale requires a long sequence of latent instructions, generating each of the tokens of the rationale requires far less operations.</p>
<p>More formally, given the sequence $z_{1}, \ldots, z_{i-1}$ generated so far, and the possible values for $z_{i}$ given by the network, denoted $\mathcal{Z}<em i="i">{i}$, we wish to filter $\mathcal{Z}</em>}$ to $\mathcal{Z<em k="k">{i}\left(y</em>}\right)$, which denotes a set of possible options that contain at least one path capable of generating the next token at index $k$. Finding the set $\mathcal{Z<em k="k">{i}\left(y</em>$. This means that the model can only generate one intermediate value in memory (not including the operations that convert strings into floating point values and vice-versa).}\right)$ is achieved by testing all combinations of instructions that are possible with at most one level of indirection, and keeping those that can generate $y_{k</p>
<p>Decoding. During decoding we find the most likely sequence of instructions $\boldsymbol{z}$ given $\boldsymbol{x}$, which can be performed with a stack-based decoder. However, it is important to refer that each generated instruction $z_{i}=\left(o_{i}, r_{i}, a_{i, 1}, \ldots, a_{i,\left|a_{i}\right|}, v_{i}\right)$ must be executed to obtain $v_{i}$. To avoid generating unexecutable code-e.g., $\log (0)$-each hypothesis instruction is executed and removed if an error occurs. Finally, once the " $E O R\rangle$ " tag is generated, we only allow instructions that would generate one of the option "A" to "E" to be generated, which guarantees that one of the options is chosen.</p>
<h2>5 Staged Back-propagation</h2>
<p>As it is shown in Figure 2, math rationales with more than 200 tokens are not uncommon, and with additional intermediate instructions, the size $\boldsymbol{z}$ can easily exceed 400. This poses a practical challenge for training the model.</p>
<p>For both the attention and copy mechanisms,</p>
<p>for each instruction $z_{i}$, the model needs to compute the probability distribution between all the attendable units $\boldsymbol{c}$ conditioned on the previous state $\mathbf{h}<em _mathbf_0="\mathbf{0">{i-1}$. For the attention model and input copy mechanisms, $\boldsymbol{c}=\boldsymbol{x}</em>}, \boldsymbol{i}=\mathbf{1}}$ and for the output copy mechanism $\boldsymbol{c}=\boldsymbol{z}$. These operations generally involve an exponential number of matrix multiplications as the size of $\boldsymbol{c}$ and $\boldsymbol{z}$ grows. For instance, during the computation of the probabilities for the input copy mechanism in Equation 1, the affinity function $f$ between the current context $\boldsymbol{q}$ and a given input $\boldsymbol{u<em i-1="i-1">{k}$ is generally implemented by projecting $\boldsymbol{u}$ and $\boldsymbol{q}$ into a single vector followed by a non-linearity, which is projected into a single affinity value. Thus, for each possible input $\boldsymbol{u}, 3$ matrix multiplications must be performed. Furthermore, for RNN unrolling, parameters and intermediate outputs for these operations must be replicated for each timestamp. Thus, as $\boldsymbol{z}$ becomes larger the attention and copy mechanisms quickly become a memory bottleneck as the computation graph becomes too large to fit on the GPU. In contrast, the sequence-to-sequence model proposed in (Sutskever et al., 2014), does not suffer from these issues as each timestamp is dependent only on the previous state $\mathbf{h}</em>$.</p>
<p>To deal with this, we use a training method we call staged back-propagation which saves memory by considering slices of $K$ tokens in $\boldsymbol{z}$, rather than the full sequence. That is, to train on a minibatch where $|z|=300$ with $K=100$, we would actually train on 3 mini-batches, where the first batch would optimize for the first $\boldsymbol{z}<em 101:="101:" 200="200">{1: 100}$, the second for $\boldsymbol{z}</em>$. The advantage of this method is that memory intensive operations, such as attention and the copy mechanism, only need to be unrolled for $K$ steps, and $K$ can be adjusted so that the computation graph fits in memory.}$ and the third for $\boldsymbol{z}_{201: 300</p>
<p>However, unlike truncated back-propagation for language modeling, where context outside the scope of $K$ is ignored, sequence-to-sequence models require global context. Thus, the sequence of states $\boldsymbol{h}$ is still built for the whole sequence $\boldsymbol{z}$. Afterwards, we obtain a slice $\boldsymbol{h}_{j: j+K}$, and compute the attention vector. ${ }^{2}$ Finally, the prediction of the instruction is conditioned on the LSTM state and the attention vector.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>6 Experiments</h2>
<p>We apply our model to the task of generating rationales for solutions to math problems, evaluating it on both the quality of the rationale and the ability of the model to obtain correct answers.</p>
<h3>6.1 Baselines</h3>
<p>As the baseline we use the attention-based sequence to sequence model proposed by Bahdanau et al. (2014), and proposed augmentations, allowing it to copy from the input (Ling et al., 2016) and from the output (Merity et al., 2016).</p>
<h3>6.2 Hyperparameters</h3>
<p>We used a two-layer LSTM with a hidden size of $H=200$, and word embeddings with size 200. The number of levels that the graph $\mathcal{G}$ is expanded during sampling $D$ is set to 5 . Decoding is performed with a beam of 200. As for the vocabulary of the softmax and embeddings, we keep the most frequent 20,000 word types, and replace the rest of the words with an unknown token. During training, the model only learns to predict a word as an unknown token, when there is no other alternative to generate the word.</p>
<h3>6.3 Evaluation Metrics</h3>
<p>The evaluation of the rationales is performed with average sentence level perplexity and BLEU4 (Papineni et al., 2002). When a model cannot generate a token for perplexity computation, we predict unknown token. This benefits the baselines as they are less expressive. As the perplexity of our model is dependent on the latent program that is generated, we force decode our model to generate the rationale, while maximizing the probability of the program. This is analogous to the method used to obtain sample programs described in Section 4, but we choose the most likely instructions at each timestamp instead of sampling. Finally, the correctness of the answer is evaluated by computing the percentage of the questions, where the chosen option matches the correct one.</p>
<h3>6.4 Results</h3>
<p>The test set results, evaluated on perplexity, BLEU, and accuracy, are presented in Table 3.</p>
<p>Perplexity. In terms of perplexity, we observe that the regular sequence to sequence model fares poorly on this dataset, as the model requires the generation of many values that tend to be</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Perplexity</th>
<th style="text-align: right;">BLEU</th>
<th style="text-align: right;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seq2Seq</td>
<td style="text-align: right;">524.7</td>
<td style="text-align: right;">8.57</td>
<td style="text-align: right;">20.8</td>
</tr>
<tr>
<td style="text-align: left;">+Copy Input</td>
<td style="text-align: right;">46.8</td>
<td style="text-align: right;">21.3</td>
<td style="text-align: right;">20.4</td>
</tr>
<tr>
<td style="text-align: left;">+Copy Output</td>
<td style="text-align: right;">45.9</td>
<td style="text-align: right;">20.6</td>
<td style="text-align: right;">20.2</td>
</tr>
<tr>
<td style="text-align: left;">Our Model</td>
<td style="text-align: right;">$\mathbf{2 8 . 5}$</td>
<td style="text-align: right;">$\mathbf{2 7 . 2}$</td>
<td style="text-align: right;">$\mathbf{3 6 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results over the test set measured in Perplexity, BLEU and Accuracy.
sparse. Adding an input copy mechanism greatly improves the perplexity as it allows the generation process to use values that were mentioned in the question. The output copying mechanism improves perplexity slightly over the input copy mechanism, as many values are repeated after their first occurrence. For instance, in Problem 2, the value " 1326 " is used twice, so even though the model cannot generate it easily in the first occurrence, the second one can simply be generated by copying the first one. We can observe that our model yields significant improvements over the baselines, demonstrating that the ability to generate new values by algebraic manipulation is essential in this task. An example of a program that is inferred is shown in Figure 4. The graph was generated by finding the most likely program $\boldsymbol{z}$ that generates $\boldsymbol{y}$. Each node isolates a value in $\boldsymbol{x}, \boldsymbol{r} \boldsymbol{n}$, or $\boldsymbol{y}$, where arrows indicate an operation executed with the outgoing nodes as arguments and incoming node as the return of the operation. For simplicity, operations that copy or convert values (e.g. from string to float) were not included, but nodes that were copied/converted share the same color. Examples of tokens where our model can obtain the perplexity reduction are the values " 0.025 ", " 0.023 ", " 0.002 " and finally the answer " E ", as these cannot be copied from the input or output.</p>
<p>BLEU. We observe that the regular sequence to sequence model achieves a low BLEU score. In fact, due to the high perplexities the model generates very short rationales, which frequently consist of segments similar to "Answer should be D", as most rationales end with similar statements. By applying the copy mechanism the BLEU score improves substantially, as the model can define the variables that are used in the rationale. Interestingly, the output copy mechanism adds no further improvement in the perplexity evaluation. This is because during decoding all values that can be copied from the output are values that could
have been generated by the model either from the softmax or the input copy mechanism. As such, adding an output copying mechanism adds little to the expressiveness of the model during decoding.</p>
<p>Finally, our model can achieve the highest BLEU score as it has the mechanism to generate the intermediate and final values in the rationale.</p>
<p>Accuracy. In terms of accuracy, we see that all baseline models obtain values close to chance ( $20 \%$ ), indicating that they are completely unable to solve the problem. In contrast, we see that our model can solve problems at a rate that is significantly higher than chance, demonstrating the value of our program-driven approach, and its ability to learn to generate programs.</p>
<p>In general, the problems we solve correctly correspond to simple problems that can be solved in one or two operations. Examples include questions such as "Billy cut up each cake into 10 slices, and ended up with 120 slices altogether. How many cakes did she cut up? A) 9 B) 7 C) 12 D) 14 E) 16 ", which can be solved in a single step. In this case, our model predicts " $120 / 10=12$ cakes. Answer is C" as the rationale, which is reasonable.</p>
<h3>6.5 Discussion.</h3>
<p>While we show that our model can outperform the models built up to date, generating complex rationales as those shown in Figure 1 correctly is still an unsolved problem, as each additional step adds complexity to the problem both during inference and decoding. Yet, this is the first result showing that it is possible to solve math problems in such a manner, and we believe this modeling approach and dataset will drive work on this problem.</p>
<h2>7 Related Work</h2>
<p>Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math problem. Other work has focused on learning to map math expressions into formal languages (Roy et al., 2016). We aim to generate natural language rationales, where the bindings between variables and the problem solving approach are mixed into a single generative model that attempts to solve the problem while explaining the approach taken.</p>
<p>Our approach is strongly tied with the work on sequence to sequence transduction using the encoder-decoder paradigm (Sutskever et al., 2014;</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair.</p>
<p>Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence.</p>
<p>Providing textual explanations for classification decisions has begun to receive attention, as part of increased interest in creating models whose decisions can be interpreted. Lei et al. (2016), jointly modeled both a classification decision, and the selection of the most relevant subsection of a document for making the classification decision. Hendricks et al. (2016) generate textual explanations for visual classification problems, but in contrast to our model, they first generate an answer, and then, conditional on the answer, generate an explanation. This effectively creates a post-hoc justification for a classification decision rather than a program for deducing an answer. These papers,
like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction.</p>
<h2>8 Conclusion</h2>
<p>In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000 question and rationale pairs, and propose a model that can generate natural language and perform arithmetic operations in the same decoding process. Experiments show that our method outperforms existing neural models, in both the fluency of the rationales that are generated and the ability to solve the problem.</p>
<h2>References</h2>
<p>Jacob Andreas, Andreas Vlachos, and Stephen Clark. 2013. Semantic parsing as machine translation. In Proc. of ACL.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-</p>
<p>gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv 1409.0473.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proc. of EMNLP.</p>
<p>Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwiska, Sergio Gmez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adri Puigdomnech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. 2016. Hybrid computing using a neural network with dynamic external memory. Nature 538(7626):471476.</p>
<p>Brent Harrison, Upol Ehsan, and Mark O. Riedl. 2017. Rationalization: A neural machine translation approach to generating natural language explanations. CoRR abs/1702.07826.</p>
<p>Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell. 2016. Generating visual explanations. In Proc. ECCV.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proc. of EMNLP.</p>
<p>Bevan Keeley Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with bayesian tree transducers. In Proc. of ACL.</p>
<p>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proc. of EMNLP.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proc. of ACL.</p>
<p>Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predictions. In Proc. of EMNLP.</p>
<p>Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, and Ni Lao. 2016. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. arXiv 1611.00020.</p>
<p>Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomás Kociský, Andrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent predictor networks for code generation. In Proc. of ACL.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv 1609.07843.</p>
<p>Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. 2016. Neural programmer: Inducing latent programs with gradient descent. In Proc. ICLR.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proc. of ACL.</p>
<p>Chris Quirk, Raymond Mooney, and Michel Galley. 2015. Language to code: Learning semantic parsers for if-this-then-that recipes. In Proc. of ACL.</p>
<p>Scott E. Reed and Nando de Freitas. 2016. Neural programmer-interpreters. In Proc. of ICLR.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proc. of EMNLP.</p>
<p>Subhro Roy, Shyam Upadhyay, and Dan Roth. 2016. Equation parsing: Mapping sentences to grounded equations. In Proc. of EMNLP.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. arXiv 1409.3215.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Proc. of NIPS.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ This modeling strategy is sometimes known as late fusion, as the attention vector is not used for state propagation, it is incorporated "later".&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
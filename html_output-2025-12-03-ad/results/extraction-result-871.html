<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-871 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-871</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-871</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-061d113a7b3f32deab6bc50fea676fa0b1e0f658</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/061d113a7b3f32deab6bc50fea676fa0b1e0f658" target="_blank">Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work takes a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks and applies an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations.</p>
                <p><strong>Paper Abstract:</strong> Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. Our source code is available at: this https URL.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e871.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e871.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Deep Q-Network (KG-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning agent that constructs a structured knowledge-graph belief of the text-game world from past textual observations using information extraction, and uses that graph as the state representation for value-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs a knowledge graph (objects as nodes, relations/positions as edges) from textual observations using an external OpenIE information-extraction tool plus human-authored rules; the graph is used as the agent's state (belief) representation and fed into a DQN-style value estimator. Action representation follows DRRN-style embedding/matching (project each candidate action into a vector and score against the state embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic text games (proposed for synthetic/textworld-style games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based games where the agent receives textual observations per step; inherently partially observable because each observation only describes the current local view and not the full hidden world state.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>OpenIE (pre-trained general-purpose Open Information Extraction tool) and human-written rule scripts for relation extraction and conversion to graph triples.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured relation triples (subject, predicate, object) and extracted relation mentions (i.e., structured edges/nodes for a knowledge graph).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Knowledge graph: objects are nodes, relations/positions are edges; graph built from extracted triples over observations (structured symbolic representation serving as belief state).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>New observations are processed with OpenIE and human-written rules to extract relation triples which are added to (and/or update) the knowledge graph; the graph therefore accumulates relations from historical observations to form the agent's belief. The paper references this mechanism but does not detail exact graph maintenance/merge/pruning policies.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Value-based RL (DQN) over the structured knowledge-graph state representation; action scoring follows DRRN-like embedding/matching.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Graph-based state representation (knowledge graph of objects/locations) is used for state estimation; no explicit shortest-path or path-finding algorithm (e.g., A*) is reported in this paper's discussion of KG-DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an external IE tool (OpenIE) to extract relations and build a knowledge graph provides a structured belief state that can capture object relations and spatial/interaction information across observations; this structured belief is then usable by RL (DQN) to improve decision making in partially observable text environments. The current paper notes KG-DQN but highlights limitations of prior methods that summarize history into single vectors, which can inject noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e871.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e871.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Constrained A2C (KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An actor-critic agent that extends knowledge-graph state representations to human-written IF games by adding information-extraction heuristics and using the graph to constrain/augment policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extends knowledge-graph approaches to human-authored IF games by applying information-extraction heuristics (built on OpenIE-like extraction and additional rules) to create an object-graph (nodes and relations) from historical observations; uses a GRU-based recurrent action generator and an A2C (actor-critic) RL algorithm that conditions on the graph-derived state representation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interactive Fiction (human-authored IF) games (e.g., Jericho benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Human-written IF games with rich, varied natural-language observations; partially observable because each step's text does not fully reveal the underlying world state and relevant past context may be necessary to choose actions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Pre-trained OpenIE-style information extraction and human-authored extraction heuristics/rules (to extract relations and populate the object graph).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured relation triples and heuristic-extracted relations (graph edges) used to build/augment the object/knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Object-centric knowledge/graph representation built from past observations via IE and heuristics; objects and relations become nodes/edges encoding the agent's belief about the world.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Incoming observations are processed with IE heuristics to extract relations; the extracted relations are incorporated into the object graph, which is therefore updated over time to reflect the agent's accumulated knowledge; exact merge/pruning mechanics are defined in the referenced KG-A2C work rather than in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Actor-Critic RL (A2C) where the policy and value estimators condition on the graph-based state; uses a GRU-based recurrent action generator for producing templates/arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Uses the object/knowledge graph for state representation and action-conditioning; no explicit path-finding/search algorithm (e.g., A*, BFS) is described in this paper's summary of KG-A2C.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Reported as baseline KG-A2C scores in Table 2 of this paper (across Jericho games); aggregated winning percentage reported as 27% (9/33) for KG-A2C in the table comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-based belief representations created via external IE and heuristics can be ported to human-authored IF games, enabling actor-critic policies to condition on structured object relations; however, the current paper emphasizes that some prior graph approaches summarize history into single vectors and may introduce noise, motivating object-centric retrieval and RC-based scoring instead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e871.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e871.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPRC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Paragraph Reading Comprehension DQN (MPRC-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The agent introduced in this paper that models action selection as an extractive multi-paragraph reading-comprehension task and mitigates partial observability via object-centric retrieval of historical observation paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MPRC-DQN (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Value-based RL (DQN) where the Q-function Q(o,a;θ) is parameterized by a reading-comprehension (RC) model: the current (and retrieved) observation text(s) are treated as context and a template verb phrase as the question; BiDAF-style and self-attention layers produce verb-aware contextualized observation representations, object-specific argument embeddings are pooled, and a linear layer predicts Q-values for template+object instantiations. Partial observability is handled by an object-centric retrieval: spaCy detects objects in the current observation, and the most recent K past observations that mention any shared object are retrieved, concatenated (with separators), and provided to the RC model (K=2 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho interactive fiction benchmark (human-authored IF games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A suite of 33 human-authored IF games with diverse natural-language observations; partially observable because single-step observations omit historical context and world state, and the action space is large and combinatorial (templates with object placeholders).</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>spaCy (for tokenization and object/entity detection) used in preprocessing to detect objects that drive history retrieval; GloVe embeddings used but are not a planning tool.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual tokenization and object/entity labels (i.e., detected object mentions/indices in observations used to select historical paragraphs).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Textual multi-paragraph belief: the agent does not build an explicit symbolic graph; instead it retrieves and concatenates the most recent K historical observation texts that mention objects shared with the current observation, producing an extended textual context (multi-paragraph) that serves as the belief/state input to the RC-based Q-function.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each step, spaCy-detected objects in the current observation are used to retrieve the most recent K past observations that contain any shared object; these K observations (sorted by time) are concatenated with the current observation (separated by special tokens) forming the extended observation input. There is no explicit symbolic merging; the belief is the concatenated text fed to the RC encoder. This retrieval is time-sensitive: more recent observations are preferred and K controls history depth.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned value-based RL (DQN) where action evaluation is implemented via an RC model that scores template+object instantiations by extracting supporting object mentions from the multi-paragraph context (extractive QA formulation) and projecting pooled argument representations to Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>No explicit graph search or shortest-path planning is used; navigation decisions are made by scoring candidate movement templates via the RC-based Q-function conditioned on concatenated observations (i.e., implicit, learned navigation via policy/Q-values rather than explicit path search).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Reported final performance across Jericho: MPRC-DQN achieves state-of-the-art on many games (e.g., overall winning percentage reported as 64%/21 when comparing to some baselines and 76%/25 when including RC-DQN variant) — spaCy is used in the pipeline but no ablation isolating spaCy/tool effect is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Object-centric retrieval driven by NLP tools (spaCy) to assemble a multi-paragraph textual belief, combined with an RC-style action scorer, significantly improves sample efficiency and final performance on partially observable IF games versus methods that (a) compress history into single vectors or (b) use graph extraction pipelines; the approach integrates tool outputs (object detections) by guiding retrieval rather than by building an explicit symbolic graph, and planning is performed by a learned Q-function over text contexts rather than by explicit path search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e871.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e871.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL: A general interactive fiction agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art rule-based (no-learning) IF agent that uses a suite of heuristics to explore, interact, and construct an internal representation of the game world.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NAIL: A general interactive fiction agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A heuristic, modular agent using reliable handcrafted heuristics for systematic exploration, object interaction, and internal state-building; constructs an internal representation of the game world via rules and heuristics to guide action selection rather than learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interactive Fiction games (Jericho benchmark referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Human-authored IF games; partially observable with complex textual descriptions and a large action space.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Not specified in this paper; NAIL is described as heuristic/rule-based and does not appear here to rely on external IE tools in the presented summary.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Internal representation constructed from heuristics and observations (rule-based world model); specifics are described in the NAIL paper rather than here.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>NAIL updates its internal representation using heuristics applied to observations (details not provided in this paper's related-work description).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Heuristic rule-based planning (no learning); sequences of heuristics for exploration, interaction and world-model maintenance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NAIL demonstrates that carefully designed heuristics and internal representations can be effective in IF, but learning-based approaches that better exploit textual structure (e.g., RC-based models) can achieve higher sample efficiency and generalization in many games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>NAIL: A general interactive fiction agent <em>(Rating: 1)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on textbased games <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games: A colossal adventure <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-871",
    "paper_id": "paper-061d113a7b3f32deab6bc50fea676fa0b1e0f658",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "KG-DQN",
            "name_full": "Knowledge Graph Deep Q-Network (KG-DQN)",
            "brief_description": "A reinforcement-learning agent that constructs a structured knowledge-graph belief of the text-game world from past textual observations using information extraction, and uses that graph as the state representation for value-based RL.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "KG-DQN",
            "agent_description": "Constructs a knowledge graph (objects as nodes, relations/positions as edges) from textual observations using an external OpenIE information-extraction tool plus human-authored rules; the graph is used as the agent's state (belief) representation and fed into a DQN-style value estimator. Action representation follows DRRN-style embedding/matching (project each candidate action into a vector and score against the state embedding).",
            "environment_name": "Synthetic text games (proposed for synthetic/textworld-style games)",
            "environment_description": "Text-based games where the agent receives textual observations per step; inherently partially observable because each observation only describes the current local view and not the full hidden world state.",
            "is_partially_observable": true,
            "external_tools_used": "OpenIE (pre-trained general-purpose Open Information Extraction tool) and human-written rule scripts for relation extraction and conversion to graph triples.",
            "tool_output_types": "Structured relation triples (subject, predicate, object) and extracted relation mentions (i.e., structured edges/nodes for a knowledge graph).",
            "belief_state_mechanism": "Knowledge graph: objects are nodes, relations/positions are edges; graph built from extracted triples over observations (structured symbolic representation serving as belief state).",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "New observations are processed with OpenIE and human-written rules to extract relation triples which are added to (and/or update) the knowledge graph; the graph therefore accumulates relations from historical observations to form the agent's belief. The paper references this mechanism but does not detail exact graph maintenance/merge/pruning policies.",
            "planning_approach": "Value-based RL (DQN) over the structured knowledge-graph state representation; action scoring follows DRRN-like embedding/matching.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Graph-based state representation (knowledge graph of objects/locations) is used for state estimation; no explicit shortest-path or path-finding algorithm (e.g., A*) is reported in this paper's discussion of KG-DQN.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Using an external IE tool (OpenIE) to extract relations and build a knowledge graph provides a structured belief state that can capture object relations and spatial/interaction information across observations; this structured belief is then usable by RL (DQN) to improve decision making in partially observable text environments. The current paper notes KG-DQN but highlights limitations of prior methods that summarize history into single vectors, which can inject noise.",
            "uuid": "e871.0",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "Graph-Constrained A2C (KG-A2C)",
            "brief_description": "An actor-critic agent that extends knowledge-graph state representations to human-written IF games by adding information-extraction heuristics and using the graph to constrain/augment policy learning.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "mention",
            "agent_name": "KG-A2C",
            "agent_description": "Extends knowledge-graph approaches to human-authored IF games by applying information-extraction heuristics (built on OpenIE-like extraction and additional rules) to create an object-graph (nodes and relations) from historical observations; uses a GRU-based recurrent action generator and an A2C (actor-critic) RL algorithm that conditions on the graph-derived state representation.",
            "environment_name": "Interactive Fiction (human-authored IF) games (e.g., Jericho benchmark)",
            "environment_description": "Human-written IF games with rich, varied natural-language observations; partially observable because each step's text does not fully reveal the underlying world state and relevant past context may be necessary to choose actions.",
            "is_partially_observable": true,
            "external_tools_used": "Pre-trained OpenIE-style information extraction and human-authored extraction heuristics/rules (to extract relations and populate the object graph).",
            "tool_output_types": "Structured relation triples and heuristic-extracted relations (graph edges) used to build/augment the object/knowledge graph.",
            "belief_state_mechanism": "Object-centric knowledge/graph representation built from past observations via IE and heuristics; objects and relations become nodes/edges encoding the agent's belief about the world.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Incoming observations are processed with IE heuristics to extract relations; the extracted relations are incorporated into the object graph, which is therefore updated over time to reflect the agent's accumulated knowledge; exact merge/pruning mechanics are defined in the referenced KG-A2C work rather than in this paper.",
            "planning_approach": "Actor-Critic RL (A2C) where the policy and value estimators condition on the graph-based state; uses a GRU-based recurrent action generator for producing templates/arguments.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Uses the object/knowledge graph for state representation and action-conditioning; no explicit path-finding/search algorithm (e.g., A*, BFS) is described in this paper's summary of KG-A2C.",
            "performance_with_tools": "Reported as baseline KG-A2C scores in Table 2 of this paper (across Jericho games); aggregated winning percentage reported as 27% (9/33) for KG-A2C in the table comparisons.",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Graph-based belief representations created via external IE and heuristics can be ported to human-authored IF games, enabling actor-critic policies to condition on structured object relations; however, the current paper emphasizes that some prior graph approaches summarize history into single vectors and may introduce noise, motivating object-centric retrieval and RC-based scoring instead.",
            "uuid": "e871.1",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "MPRC-DQN",
            "name_full": "Multi-Paragraph Reading Comprehension DQN (MPRC-DQN)",
            "brief_description": "The agent introduced in this paper that models action selection as an extractive multi-paragraph reading-comprehension task and mitigates partial observability via object-centric retrieval of historical observation paragraphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MPRC-DQN (this paper)",
            "agent_description": "Value-based RL (DQN) where the Q-function Q(o,a;θ) is parameterized by a reading-comprehension (RC) model: the current (and retrieved) observation text(s) are treated as context and a template verb phrase as the question; BiDAF-style and self-attention layers produce verb-aware contextualized observation representations, object-specific argument embeddings are pooled, and a linear layer predicts Q-values for template+object instantiations. Partial observability is handled by an object-centric retrieval: spaCy detects objects in the current observation, and the most recent K past observations that mention any shared object are retrieved, concatenated (with separators), and provided to the RC model (K=2 in experiments).",
            "environment_name": "Jericho interactive fiction benchmark (human-authored IF games)",
            "environment_description": "A suite of 33 human-authored IF games with diverse natural-language observations; partially observable because single-step observations omit historical context and world state, and the action space is large and combinatorial (templates with object placeholders).",
            "is_partially_observable": true,
            "external_tools_used": "spaCy (for tokenization and object/entity detection) used in preprocessing to detect objects that drive history retrieval; GloVe embeddings used but are not a planning tool.",
            "tool_output_types": "Textual tokenization and object/entity labels (i.e., detected object mentions/indices in observations used to select historical paragraphs).",
            "belief_state_mechanism": "Textual multi-paragraph belief: the agent does not build an explicit symbolic graph; instead it retrieves and concatenates the most recent K historical observation texts that mention objects shared with the current observation, producing an extended textual context (multi-paragraph) that serves as the belief/state input to the RC-based Q-function.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each step, spaCy-detected objects in the current observation are used to retrieve the most recent K past observations that contain any shared object; these K observations (sorted by time) are concatenated with the current observation (separated by special tokens) forming the extended observation input. There is no explicit symbolic merging; the belief is the concatenated text fed to the RC encoder. This retrieval is time-sensitive: more recent observations are preferred and K controls history depth.",
            "planning_approach": "Learned value-based RL (DQN) where action evaluation is implemented via an RC model that scores template+object instantiations by extracting supporting object mentions from the multi-paragraph context (extractive QA formulation) and projecting pooled argument representations to Q-values.",
            "uses_shortest_path_planning": false,
            "navigation_method": "No explicit graph search or shortest-path planning is used; navigation decisions are made by scoring candidate movement templates via the RC-based Q-function conditioned on concatenated observations (i.e., implicit, learned navigation via policy/Q-values rather than explicit path search).",
            "performance_with_tools": "Reported final performance across Jericho: MPRC-DQN achieves state-of-the-art on many games (e.g., overall winning percentage reported as 64%/21 when comparing to some baselines and 76%/25 when including RC-DQN variant) — spaCy is used in the pipeline but no ablation isolating spaCy/tool effect is reported.",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Object-centric retrieval driven by NLP tools (spaCy) to assemble a multi-paragraph textual belief, combined with an RC-style action scorer, significantly improves sample efficiency and final performance on partially observable IF games versus methods that (a) compress history into single vectors or (b) use graph extraction pipelines; the approach integrates tool outputs (object detections) by guiding retrieval rather than by building an explicit symbolic graph, and planning is performed by a learned Q-function over text contexts rather than by explicit path search.",
            "uuid": "e871.2",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "NAIL",
            "name_full": "NAIL: A general interactive fiction agent",
            "brief_description": "A state-of-the-art rule-based (no-learning) IF agent that uses a suite of heuristics to explore, interact, and construct an internal representation of the game world.",
            "citation_title": "NAIL: A general interactive fiction agent",
            "mention_or_use": "mention",
            "agent_name": "NAIL",
            "agent_description": "A heuristic, modular agent using reliable handcrafted heuristics for systematic exploration, object interaction, and internal state-building; constructs an internal representation of the game world via rules and heuristics to guide action selection rather than learned policies.",
            "environment_name": "Interactive Fiction games (Jericho benchmark referenced)",
            "environment_description": "Human-authored IF games; partially observable with complex textual descriptions and a large action space.",
            "is_partially_observable": true,
            "external_tools_used": "Not specified in this paper; NAIL is described as heuristic/rule-based and does not appear here to rely on external IE tools in the presented summary.",
            "tool_output_types": null,
            "belief_state_mechanism": "Internal representation constructed from heuristics and observations (rule-based world model); specifics are described in the NAIL paper rather than here.",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": "NAIL updates its internal representation using heuristics applied to observations (details not provided in this paper's related-work description).",
            "planning_approach": "Heuristic rule-based planning (no learning); sequences of heuristics for exploration, interaction and world-model maintenance.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "NAIL demonstrates that carefully designed heuristics and internal representations can be effective in IF, but learning-based approaches that better exploit textual structure (e.g., RC-based models) can achieve higher sample efficiency and generalization in many games.",
            "uuid": "e871.3",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "NAIL: A general interactive fiction agent",
            "rating": 1
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on textbased games",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games: A colossal adventure",
            "rating": 1
        }
    ],
    "cost": 0.0161095,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</h1>
<p>Xiaoxiao Guo<em><br>IBM Research<br>xiaoxiao.guo@ibm.com<br>$\frac{\text { Mo Yu</em> }}{\text { IBM Research }}$<br>yum@us.ibm.com<br>Yupeng Gao<br>IBM Research<br>yupeng.gao@ibm.com<br>Chuang Gan<br>MIT-IBM Watson AI Lab<br>chuangg@ibm.com<br>Murray Campbell<br>IBM Research<br>mcam@us.ibm.com<br>Shiyu Chang<br>MIT-IBM Watson AI Lab<br>shiyu.chang@ibm.com</p>
<h4>Abstract</h4>
<p>Interactive Fiction (IF) games with real humanwritten natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the humanwritten textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Interactive systems capable of understanding natural language and responding in the form of natural language text have high potentials in various applications. In pursuit of building and evaluating such systems, we study learning agents for Interactive Fiction (IF) games. IF games are world-simulating software in which players use text commands to control the protagonist and influence the world, as illustrated in Figure 1. IF gameplay agents need to simultaneously understand the game's information from a text display (observation) and generate</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Sample gameplay for the classic dungeon game Zork1. The objective is to solve various puzzles and collect the 19 treasures to install the trophy case. The player receives textual observations describing the current game state and additional reward scalars encoding the game designers' objective of game progress. The player sends textual action commands to control the protagonist.
natural language command (action) via a text input interface. Without providing an explicit game strategy, the agents need to identify behaviors that maximize objective-encoded cumulative rewards.</p>
<p>IF games composed of human-written texts (distinct from previous text games with synthetic texts) create superb new opportunities for studying and evaluating natural language understanding (NLU) techniques due to their unique characteristics. (1) Game designers elaborately craft on the literariness of the narrative texts to attract players when creating IF games. The resulted texts in IF games are more linguistically diverse and sophisticated than the template-generated ones in synthetic text games. (2) The language contexts of IF games</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our approach to solving the IF games as Multi-Paragraph Reading Comprehension (MPRC) tasks.
are more versatile because various designers contribute to enormous domains and genres, such as adventure, fantasy, horror, and sci-fi. (3) The text commands to control characters are less restricted, having sizes over six orders of magnitude larger than previous text games. The recently introduced Jericho benchmark provides a collection of such IF games (Hausknecht et al., 2019a).</p>
<p>The complexity of IF games demands more sophisticated NLU techniques than those used in synthetic text games. Moreover, the task of designing IF game-play agents, intersecting NLU and reinforcement learning (RL), poses several unique challenges on the NLU techniques. The first challenge is the difficulty of exploration in the huge natural language action space. To make RL agents learn efficiently without prohibitive exhaustive trials, the action estimation must generalize learned knowledge from tried actions to others. To this end, previous approaches, starting with a single embedding vector of the observation, either predict the elements of actions independently (Narasimhan et al., 2015; Hausknecht et al., 2019a); or embed each valid action as another vector and predict action value based on the vector-space similarities (He et al., 2016). These methods do not consider the compositionality or role-differences of the action elements, or the interactions among them and the observation. Therefore, their modeling of the action values is less accurate and less data-efficient.</p>
<p>The second challenge is partial observability. At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world. But the latest observation is often not a sufficient summary of the interaction history and may not provide enough
information to determine the long-term effects of actions. Previous approaches address this problem by building a representation over past observations (e.g., building a graph of objects, positions, and spatial relations) (Ammanabrolu and Riedl, 2019; Ammanabrolu and Hausknecht, 2020). These methods treat the historical observations equally and summarize the information into a single vector without focusing on important contexts related to the action prediction for the current observation. Therefore, their usages of history also bring noise, and the improvement is not always significant.</p>
<p>We propose a novel formulation of IF game playing as Multi-Passage Reading Comprehension (MPRC) and harness MPRC techniques to solve the huge action space and partial observability challenges. The graphical illustration is shown in Figure 2. First, the action value prediction (i.e., predicting the long-term rewards of selecting an action) is essentially generating and scoring a compositional action structure by finding supporting evidence from the observation. We base on the fact that each action is an instantiation of a template, i.e., a verb phrase with a few placeholders of object arguments it takes (Figure 2b). Then the action generation process can be viewed as extracting objects for a template's placeholders from the textual observation, based on the interaction between the template verb phrase and the relevant context of the objects in the observation. Our approach addresses the structured prediction and interaction problems with the idea of context-question attention mechanism in RC models. Specifically, we treat the observation as a passage and each template verb phrase as a question. The filling of object placeholders in the template thus becomes an</p>
<p>extractive QA problem that selects objects from the observation given the template. Simultaneously each action (i.e., a template with all placeholder replaced) gets its evaluation value predicted by the RC model. Our formulation and approach better capture the fine-grained interactions between observation texts and structural actions, in contrast to previous approaches that represent the observation as a single vector and ignore the fine-grained dependency among action elements.</p>
<p>Second, alleviating partial observability is essentially enhancing the current observation with potentially relevant history and predicting actions over the enhanced observation. Our approach retrieves potentially relevant historical observations with an object-centric approach (Figure 2a), so that the retrieved ones are more likely to be connected to the current observation as they describe at least one shared interactable object. Our attention mechanisms are then applied across the retrieved multiple observation texts to focus on informative contexts for action value prediction.</p>
<p>We evaluated our approach on the suite of Jericho IF games, compared to all previous approaches. Our approaches achieved or outperformed the state-of-the-art performance on 25 out of 33 games, trained with less than one-tenth of game interaction data used by prior art. We also provided ablation studies on our models and retrieval strategies.</p>
<h2>2 Related Work</h2>
<p>IF Game Agents. Previous work mainly studies the text understanding and generation in parserbased or rule-based text game tasks, such as TextWorld platform (Côté et al., 2018) or custom domains (Narasimhan et al., 2015; He et al., 2016; Adhikari et al., 2020). The recent platform Jericho (Hausknecht et al., 2019a) supports over thirty human-written IF games. Earlier successes in real IF games mainly rely on heuristics without learning. NAIL (Hausknecht et al., 2019b) is the state-of-theart among these "no-learning" agents, employing a series of reliable heuristics for exploring the game, interacting with objects, and building an internal representation of the game world. With the development of learning environments like Jericho, the RL-based agents have started to achieve dominating performance.</p>
<p>A critical challenge for learning-based agents is how to handle the combinatorial action space in IF games. LSTM-DQN (Narasimhan et al., 2015)
was proposed to generate verb-object action with pre-defined sets of possible verbs and objects, but treat the selection and learning of verbs and objects independently. Template-DQN (Hausknecht et al., 2019a) extended LSTM-DQN for template-based action generation, introducing one additional but still independent prediction output for the second object in the template. Deep Reinforcement Relevance Network (DRRN) (He et al., 2016) was introduced for choice-based games. Given a set of valid actions at every game state, DRRN projects each action into a hidden space that matches the current state representation vector for action selection. Action-Elimination Deep Q-Network (AEDQN) (Zahavy et al., 2018) learns to predict invalid actions in the adventure game Zork. It eliminates invalid action for efficient policy learning via utilizing expert demonstration data.</p>
<p>Other techniques focus on addressing the partial observability in text games. Knowledge Graph DQN (KG-DQN) (Ammanabrolu and Riedl, 2019) was proposed to deal with synthetic games. The method constructs and represents the game states as knowledge graphs with objects as nodes and uses pre-trained general purposed OpenIE tool and human-written rules to extract relations between objects. KG-DQN handles the action representation following DRRN. KG-A2C (Ammanabrolu and Hausknecht, 2020) later extends the work for IF games, by adding information extraction heuristics to fit the complexity of the object relations in IF games and utilizing a GRU-based action generator to handle the action space.</p>
<h2>Reading Comprehension Models for Question</h2>
<p>Answering. Given a question, reading comprehension (RC) aims to find the answer to the question based on a paragraph that may contain supporting evidence. One of the standard RC settings is extractive QA (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019), which extracts a span from the paragraph as an answer. Our formulation of IF game playing resembles this setting.</p>
<p>Many neural reader models have been designed for RC. Specifically, for the extractive QA task, the reader models usually build question-aware passage representations via attention mechanisms (Seo et al., 2016; Yu et al., 2018), and employ a pointer network to predict the start and end positions of the answer span (Wang and Jiang, 2016). Powerful pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019) have been</p>
<p>recently applied to enhance the encoding and attention mechanisms of the aforementioned reader models. They give performance boost but are more resource-demanding and do not suit the IF game playing task very well.</p>
<p>Reading Comprehension over Multiple Paragraphs. Multi-paragraph reading comprehension (MPRC) deals with the more general task of answering a question from multiple related paragraphs, where each paragraph may not necessarily support the correct answer. Our formulation becomes an MPRC setting when we enhance the state representation with historical observations and predict actions from multiple observation paragraphs.</p>
<p>A fundamental research problem in MPRC, which is also critical to our formulation, is to select relevant paragraphs from all the input paragraphs for the reader to focus on. Previous approaches mainly apply traditional IR approaches like BM25 (Chen et al., 2017; Joshi et al., 2017), or neural ranking models trained with distant supervision (Wang et al., 2018; Min et al., 2019a), for paragraph selection. Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018), which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches (Ding et al., 2019; Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019), where paragraphs are connected if they share the same-named entities. The paragraph retrieval then becomes a traversal over such graphs via entity links. These entity-centric paragraph retrieval approaches share a similar high-level idea to our object-based history retrieval approach. The techniques above have been applied to deal with evidence from Wikipedia, news collections, and, recently, books (Mou et al., 2020). We are the first to extend these ideas to IF games.</p>
<h2>3 Multi-Paragraph RC for IF Games</h2>
<h3>3.1 Problem Formulation</h3>
<p>Each IF game can be defined as a Partially Observable Markov Decision Process (POMDP), namely a 7-tuple of $\langle S, A, T, O, \Omega, R, \gamma\rangle$, representing the hidden game state set, the action set, the state transition function, the set of textual observations composed from vocabulary words, the textual observation function, the reward function, and the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Our RC-based action prediction model architecture. The template text is a verb phrase with placeholders for objects, such as [pick up OBJ] and [break OBJ with OBJ].
discount factor respectively. The game playing agent interacts with the game engine in multiple turns until the game is over or the maximum number of steps is reached. At the $t$-th turn, the agent receives a textual observation describing the current game state $o_{t} \in O$ and sends a textual action command $a_{t} \in A$ back. The agent receives additional reward scalar $r_{t}$ which encodes the game designers' objective of game progress. Thus the task of the game playing can be formulated to generate a textual action command per step as to maximize the expected cumulative discounted rewards $\mathbf{E}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right]$. Value-based RL approaches learn to approximate an observation-action value function $Q\left(o_{t}, a_{t} ; \boldsymbol{\theta}\right)$ which measures the expected cumulative rewards of taking action $a_{t}$ when observing $o_{t}$. The agent selects action based on the action value prediction of $Q(o, a ; \boldsymbol{\theta})$.</p>
<p>Template Action Space. Template action space considers actions satisfying decomposition in the form of $\left\langle\right.$ verb, arg $\left.<em 1="1">{0}, \operatorname{arg}</em>}\right\rangle$. verb is an interchangeable verb phrase template with placeholders for objects and $\operatorname{arg<em 1="1">{0}$ and $\operatorname{arg}</em>$ are optional objects. For example, the action command [east], [pick up eggs] and [break window with stone] can be represented as template actions $\langle$ east, none, none $\rangle$, $\langle$ pick up OBJ, eggs, none and $\langle$ break OBJ with OBJ, window, stone $\rangle$. We reuse the template library and object list from Jericho. The verb phrases usually consist of several vocabulary words and each object is usually a single word.</p>
<h3>3.2 RC Model for Template Actions</h3>
<p>We parameterize the observation-action value function $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg}<em 1="1">{0}, \operatorname{arg}</em>}\right\rangle ; \boldsymbol{\theta}\right)$ by utilizing the decomposition of the template actions and contextquery contextualized representation in RC. Our model treats the observation $o$ as a context in RC and the $\operatorname{verb}=\left(v_{1}, v_{2}, \ldots, v_{k}\right)$ component of the template actions as a query. Then a verb-aware observation representation is derived via a RC reader model with Bidirectional Attention Flow (BiDAF) (Seo et al., 2016) and self-attention. The observation representation responding to the $\operatorname{arg<em 1="1">{0}$ and $\operatorname{arg}</em>}$ words are pooled and projected to a scalar value estimate for $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg<em 1="1">{0}, \operatorname{arg}</em>\right)$. A high-level model architecture of our model is illustrated in Figure 3.}\right\rangle ; \boldsymbol{\theta</p>
<p>Observation and verb Representation. We tokenize the observation and the verb phrase into words, then embed these words using pre-trained GloVe embeddings (Pennington et al., 2014). A shared encoder block that consists of LayerNorm (Ba et al., 2016) and Bidirectional GRU (Cho et al., 2014) processes the observation and verb word embeddings to obtain the separate observation and verb representation.</p>
<p>Observation-verb Interaction Layers. Given the separate observation and verb representation, we apply two attention mechanisms to compute a verb-contextualized observation representation. We first apply BiDAF with observation as the context input and verb as the query input. Specifically, we denote the processed embeddings for observation word $i$ and template word $j$ as $\boldsymbol{o}<em j="j">{i}$ and $\boldsymbol{t}</em>}$. The attention between the two words is then $a_{i j}=\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{1}} \cdot \boldsymbol{o}</em>}}+\boldsymbol{w<em _boldsymbol_j="\boldsymbol{j">{\mathbf{2}} \cdot \boldsymbol{t}</em>}}+\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{3}} \cdot\left(\boldsymbol{o}</em>}} \otimes \boldsymbol{t<em _mathbf_1="\mathbf{1">{\boldsymbol{j}}\right)$, where $\boldsymbol{w}</em>}}, \boldsymbol{w<em _mathbf_3="\mathbf{3">{\mathbf{2}}$, $\boldsymbol{w}</em>}}$ are learnable vectors and $\otimes$ is element-wise product. We then compute the "verb2observation" attention vector for the $i$-th observation word as $\boldsymbol{c<em j="j">{\boldsymbol{i}}=\sum</em>} p_{i j} \boldsymbol{t<em i="i" j="j">{\boldsymbol{j}}$ with $p</em>}=\exp \left(a_{i j}\right) / \sum_{j} \exp \left(a_{i j}\right)$. Similarly, we compute the "observation2verb" attention vector as $\boldsymbol{q}=\sum_{i} p_{i} \boldsymbol{o<em i="i">{\boldsymbol{i}}$ with $p</em>=$ $\exp \left(\max <em i="i" j="j">{j} a</em> \exp \left(\max }\right) / \sum_{i<em i="i" j="j">{j} a</em>}\right)$. We concatenate and project the output vectors as $\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{4}} \cdot\left[\boldsymbol{o}</em>}}\right.$, $\boldsymbol{c<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}, \boldsymbol{o}</em>}} \otimes \boldsymbol{c<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}, \boldsymbol{q} \otimes \boldsymbol{c}</em> \mid$, followed by a linear layer with leaky ReLU activation units (Maas et al., 2013). The output vectors are processed by an encoder block. We then apply a residual self-attention on the outputs of the encoder block. The self-attention is the same as BiDAF, but only between the observation and itself.}</p>
<p>Observation-Action Value Prediction. We generate an action by replacing the placeholders $\left(\operatorname{arg}<em 1="1">{0}\right.$ and $\left.\operatorname{arg}</em>}\right)$ in a template with objects appearing in the observation. The observation-action value $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg<em m="m">{0}=\operatorname{obj}</em>}, \operatorname{arg<em n="n">{1}=\operatorname{obj}</em>}\right\rangle ; \theta\right)$ is achieved by processing each object's corresponding verb-contextualized observation representation. Specifically, we get the indices of an $o b j$ in the observation texts $I(o b j, o)$. When the object is a noun phrase, we take the index of its headword. ${ }^{2}$ Because the same object has different meanings when it replaces different placeholders, we apply two GRU-based embedding functions for the two placeholders, to get the object's verb-placeholder dependent embeddings. We derive a single vector representation $\boldsymbol{h<em 0="0">{\text {org }</em>}=\text { obj <em 0="0">{m}}$ for the case that the placeholder $\operatorname{arg}</em>}$ is replaced by $o b j_{m}$ by meanpooling over the verb-placeholder dependent embeddings indexed by $I\left(o b j_{m}, o\right)$ for the corresponding placeholder $\operatorname{arg<em 5="5">{0}$. We apply a linear transformation on the concatenated embeddings of the two placeholders to obtain the observation action value $Q(o, a)=\boldsymbol{w}</em>} \cdot\left[\boldsymbol{h<em 1="1">{\text {org }</em>}=\text { obj <em _org="{org" _text="\text">{m}}, \boldsymbol{h}</em><em m="m">{1}=\text { obj }</em>}}\right]$ for $a=\langle$ verb, $\left.\operatorname{arg<em m="m">{0}=\operatorname{obj}</em>}, \operatorname{arg<em n="n">{1}=\operatorname{obj}</em>\right\rangle$. Our formulation avoids the repeated computation overhead among different actions with a shared template verb phrase.</p>
<h3>3.3 Multi-Paragraph Retrieval Method for Partial Observability</h3>
<p>The observation at the current step sometimes does not have full-textual evidence to support action selection and value estimation, due to the inherent partial observability of IF games. For example, when repeatedly attacking a troll with a sword, the player needs to know the effect or feedback of the last attack to determine if an extra attack is necessary. It is thus important for an agent to efficiently utilize historical observations to better support action value prediction. In our RC-based action prediction model, the historical observation utilization can be formulated as selecting evidential observation paragraphs in history, and predicting the action values from multiple selected observations, namely a Multiple-Paragraph Reading Comprehension (MPRC) problem. We propose to retrieve past observations with an object-centric approach.</p>
<p>Past Observation Retrieval. Multiple past observations may share objects with the current obser-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Agents</th>
<th>Action strategy</th>
<th>State strategy</th>
<th>Interaction data</th>
</tr>
</thead>
<tbody>
<tr>
<td>TDQN</td>
<td>Independent selection of template and the <br> two objects</td>
<td>None</td>
<td>1 M</td>
</tr>
<tr>
<td>DRRN</td>
<td>Action as a word sequence without distin- <br> guishing the roles of verbs and objects</td>
<td>None</td>
<td>1 M</td>
</tr>
<tr>
<td>KG-A2C</td>
<td>Recurrent neural decoder that selects the <br> template and objects in a fixed order</td>
<td>Object graph from historical observations <br> based on OpenIE and human-written rules</td>
<td>1.6 M</td>
</tr>
<tr>
<td>Ours</td>
<td>Observation-template representation for <br> object-centric value prediction</td>
<td>Object-based history observation retrieval</td>
<td>0.1 M</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary of the main technical differences between our agent and the baselines. All agents use DQN to update the model parameters except KG-A2C uses A2C. All agents use the same handicaps.
vation, and it is computationally expensive and unnecessary to retrieve all of such observations. The utility of past observations associated with each object is often time-sensitive in that new observations may entirely or partially invalidate old observations. We thus propose a time-sensitive strategy for retrieving past observations. Specifically, given the detected objects from the current observation, we retrieve the most recent $K$ observations with at least one shared object. The $K$ retrieved observations are sorted by time steps and concatenated to the current observation. The observations from different time steps are separated by a special token. Our RC-based action prediction model treats the concatenated observations as the observation inputs, and no other parts are changed. We use the notation $o_{t}$ to represent the current observation and the extended current observation interchangeably.</p>
<h3>3.4 Training Loss</h3>
<p>We apply the Deep Q-Network (DQN) (Mnih et al., 2015) to update the parameters $\boldsymbol{\theta}$ of our RC-based action prediction model. The loss function is:</p>
<p>$$
\begin{aligned}
\mathcal{L}(\theta)=\mathbf{E}<em t="t">{\left(o</em> ; \theta\right)\right.\right. \
&amp; \left.\left.-\left(r_{t}+\gamma \max }, a_{t}, r_{t}, o_{t+1}\right) \sim \rho(\mathcal{D})} &amp; \left[\left|Q\left(o_{t}, a_{t<em t_1="t+1">{b} Q\left(o</em>\right)\right)\right|\right]
\end{aligned}
$$}, b ; \theta^{-</p>
<p>where $\mathcal{D}$ is the experience replay consisting of recent gameplay transition records and $\rho$ is a distribution over the transitions defined by a sampling strategy.</p>
<p>Prioritized Trajectories. The distribution $\rho$ has a decent impact on DQN performance. Previous work samples transition tuples with immediate positive rewards more frequently to speed up learning (Narasimhan et al., 2015; Hausknecht et al., 2019a). We observe that this heuristic is often insufficient. Some transitions with zero immediate
rewards or even negative rewards are also indispensable in recovering well-performed trajectories. We thus extend the strategy from transition level to trajectory level. We prioritize transitions from trajectories that outperform the exponential moving average score of recent trajectories.</p>
<h2>4 Experiments</h2>
<p>We evaluate our proposed methods on the suite of Jericho supported games. We compared to all previous baselines that include recent methods addressing the huge action space and partial observability challenges.</p>
<h3>4.1 Setup</h3>
<p>Jericho Handicaps and Configuration. The handicaps used by our methods are the same as other baselines. First, we use the Jericho API to check if an action is valid with game-specific templates. Second, we augmented the observation with the textual feedback returned by the command [inventory] and [look]. Previous work also included the last action or game score as additional inputs. Our model discarded these two types of inputs as we did not observe a significant difference by our model. The maximum game step number is set to 100 following baselines.</p>
<p>Implementation Details. We apply spaCy ${ }^{3}$ to tokenize the observations and detect the objects in the observations. We use the 100-dimensional GloVe embeddings as fixed word embeddings. The out-of-vocabulary words are mapped to a randomly initialized embedding. The dimension of Bi-GRU hidden states is 128 . We set the observation representation dimension to be 128 throughout the model. The history retrieval window $K$ is 2 . For DQN configuration, we use the $\epsilon$-greedy strategy</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Game</th>
<th style="text-align: center;">Human <br> Max</th>
<th style="text-align: center;">Walkthrough-100</th>
<th style="text-align: center;">TDQN</th>
<th style="text-align: center;">Baselines <br> DRRN</th>
<th style="text-align: center;">KG-A2C</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MPRC-DQN</td>
<td style="text-align: center;">RC-DQN</td>
</tr>
<tr>
<td style="text-align: center;">905</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">acorncourt</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">advent</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">36</td>
</tr>
<tr>
<td style="text-align: center;">adventureland</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: center;">afflicted</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">anchor</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">awaken</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">balances</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">deephome</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">detective</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">169</td>
<td style="text-align: center;">197.8</td>
<td style="text-align: center;">207.9</td>
<td style="text-align: center;">317.7</td>
<td style="text-align: center;">291.3</td>
</tr>
<tr>
<td style="text-align: center;">dragon</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$-5.3$</td>
<td style="text-align: center;">$-3.5$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">4.84</td>
</tr>
<tr>
<td style="text-align: center;">enchanter</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">inhumane</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">jewel</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">karn</td>
<td style="text-align: center;">170</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">library</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">18.1</td>
</tr>
<tr>
<td style="text-align: center;">ludicorp</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: center;">moonlit</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">omniquest</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">pentari</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">43.8</td>
</tr>
<tr>
<td style="text-align: center;">reverb</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">snacktime</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">sorcerer</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;">spellbrkr</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">spirit</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">5.2</td>
</tr>
<tr>
<td style="text-align: center;">temple</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">tryst205</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">yomomma</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">zenon</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">zork1</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">38.8</td>
</tr>
<tr>
<td style="text-align: center;">zork3</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$3^{a}$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">2.83</td>
</tr>
<tr>
<td style="text-align: center;">ztuu</td>
<td style="text-align: center;">$100^{b}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: center;">Winning percentage / counts</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24\%/8</td>
<td style="text-align: center;">30\%/10</td>
<td style="text-align: center;">27\%/9</td>
<td style="text-align: center;">64\%/21</td>
<td style="text-align: center;">52\%/17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">76\%/25</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Average game scores on Jericho benchmark games. The best performing agent score per game is in bold.
The Winning percentage / counts row computes the percentage / counts of games that the corresponding agent is best. The scores of baselines are from their papers. The missing scores are represented as "-", for which games KG-A2C skipped. We also added the 100 -step results from a human-written game-playing walkthrough, as a reference of human-level scores. We denote the difficulty levels of the games defined in the original Jericho paper with colors in their names - possible (i.e., easy or normal) games in green color, difficult games in tan and extreme games in red. Best seen in color.
a Zork3 walkthrough does not maximize the score in the first 100 steps but explores more. ${ }^{b}$ Our agent discovers some unbounded reward loops in the game Ztuu.
for exploration, annealing $\epsilon$ from 1.0 to $0.05 . \gamma$ is 0.98 . We use Adam to update the weights with $10^{-4}$ learning rate. Other parameters are set to their default values. More details of the Reproducibility Checklist is in Appendix A.</p>
<p>Baselines. We compare with all the public results on the Jericho suite, namely TDQN (Hausknecht et al., 2019a), DRRN (He et al., 2016), and KGA2C (Ammanabrolu and Hausknecht, 2020). As discussed, our approaches differ from them mainly in the strategies of handling the large action space and partial observability of IF games. We summarize these main technical differences in Table 1. In summary, all previous agents predict actions con-
ditioned on a single vector representation of the whole observation texts. Thus they do not exploit the fine-grained interplay among the template components and the observations. Our approach addresses this problem by formulating action prediction as an RC task, better utilizing the rich textual observations with deeper language understanding.</p>
<p>Training Sample Efficiency. We update our models for 100, 000 times. Our agents interact with the environment one step per update, resulting in a total of 0.1 M environment interaction data. Compared to the other agents, such as KG-A2C (1.6M), TDQN (1M), and DRRN (1M), our environment interaction data is significantly smaller.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">Template Action <br> Space $\left(\times 10^{6}\right)$</th>
<th style="text-align: center;">Avg. Steps <br> Per Reward</th>
<th style="text-align: center;">Dialog <br> Actions</th>
<th style="text-align: center;">Darkness <br> Limit</th>
<th style="text-align: center;">Nonstandard</th>
<th style="text-align: center;">Inventory</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">advent</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">karn</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ludicorp</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">pentari</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">spirit</td>
<td style="text-align: center;">195</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 3: Difficulty levels and characteristics of games on which our approach achieves the most considerable improvement. Dialog indicates that it is necessary to speak with another character. Darkness indicates that accessing some dark areas requires a light source. Nonstandard Actions refers to actions with words not in an English dictionary. Inventory Limit restricts the number of items carried by the player. Please refer to (Hausknecht et al., 2019a) for more comprehensive definitions.</p>
<h3>4.2 Overall Performance</h3>
<p>We summarize the performance of our MultiParagraph Reading Comprehension DQN (MPRCDQN) agent and baselines in Table 2. Of the 33 IF games, our MPRC-DQN achieved or improved the state of the art performance on 21 games (i.e., a winning rate of $64 \%$ ). The best performing baseline (DRRN) achieved the state-of-the-art performance on only ten games, corresponding to the winning rate of $30 \%$, lower than half of ours. Note that all the methods achieved the same initial scores on five games, namely 905, anchor, awaken, deephome, and moonlit. Apart from these five games, our MPRC-DQN achieved more than three times wins. Our MPRC-DQN achieved significant improvement on some games, such as adventureland, afflicted, detective, etc. Appendix C shows some game playing trajectories.</p>
<p>We include the performance of an RC-DQN agent, which implements our RC-based action prediction model but only takes the current observations as inputs. It also outperformed the baselines by a large margin. After we consider the RC-DQN agent, our MPRC-DQN still has the highest winning percentage, indicating that our RC-based action prediction model has a significant impact on the performance improvement of our MPRC-DQN and the improvement from the multi-passage retrieval is also unneglectable. Moreover, compared to RC-DQN, our MPRC-DQN has another advantage of faster convergence. The learning curves of our MPRC-DQN and RC-DQN agents on various games are in Appendix B.</p>
<p>Finally, our approaches, overall, achieve the new state-of-the-art on 25 games (i.e., a winning rate of $76 \%$ ), giving a significant advance in the field of IF game playing.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Competitors</th>
<th style="text-align: center;">Win</th>
<th style="text-align: center;">Draw</th>
<th style="text-align: center;">Lose</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. TDQN</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. DRRN</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. KG-A2C</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 4: Pairwise comparison between our MPRC-DQN versus each baseline.</p>
<p>Pairwise Competition. To better understand the performance difference between our approach and each of the baselines, we adopt a direct one-to-one comparison metric based on the results from Table 2. Our approach has a high winning rate when competing with any of the baselines, summarized in Table 4. All the baselines have a rare chance to beat us on games. DRRN gives a higher chance of draw-games when competing with ours.</p>
<p>Human-Machine Gap. We additionally compare IF gameplay agents to human players to better understand the improvement significance and the potential improvement upper-bound. We measure each agent's game progress as the macro-average of the normalized agent-to-human game score ratios, capped at $100 \%$. The progress of our MPRCDQN is $28.5 \%$, while the best performing baseline DRRN is $17.8 \%$, showing that our agent's improvement is significant even in the realm of human players. Nevertheless, there is a vast gap between the learning agents and human players. The gap indicates IF games can be a good benchmark for the development of natural language understanding techniques.</p>
<p>Difficulty Levels of Games. Jericho categorizes the supported games into three difficulty levels, namely possible games, difficult games, and extreme games, based on the characteristics of the game dynamics, such as the action space size, the length of the game, and the average number of</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning curves for ablative studies. (left) Model ablative studies on the game Detective. (middle) Model ablative studies on Zork1. (right) Retrieval strategy study on Zork1. Best seen in color.</p>
<p>Steps to receive a non-zero reward. Our approach improves over prior art on seven of the sixteen possible games, seven of the eleven difficult games, and three of the six extreme games in Table 2. It shows that the strategies of our method are generally beneficial for any difficulty levels of game dynamics. Table 3 summarizes the characteristics of the seven games in which our method improves the most, i.e., larger than 15% of the game progress in the first 100 steps.4 First, these mostly improved games have medium action space sizes, and it is an advantageous setting for our methods where modeling the template-object-observation interactions is effective. Second, our approach improves most on games with a reasonably high degree of reward sparsity, such as <em>karn</em>, <em>spirit</em>, and <em>zork3</em>, indicating that our RC-based value function formulation helps in optimization and mitigates the reward sparsity. Finally, we remark that these game difficulty levels are not directly categorized based on natural language-related characteristics, such as text comprehension and puzzle-solving difficulties. Future studies on additional game categories based on those natural language-related characteristics would shed light on related improvements.</p>
<h3>4.3 Ablative Studies</h3>
<p><strong>RC-model Design.</strong> The overall results show that our RC-model plays a critical role in performance improvement. We compare our RC-model to some alternative models as ablative studies. We consider three alternatives, namely (1) our RC-model without the self-attention component (w/o self-att), (2) without the argumentspecific embedding (w/o arg-emb) and (3) our RC-model with Transformer-based block encoder (RC-Trans) following QANet (Yu et al., 2018). Detailed architecture is in Appendix A.</p>
<p>The learning curves for different RC-models are in Figure 4 (left/middle). The RC-models without either self-attention or argument-specific embedding degenerate, and the argument-specific embedding has a greater impact. The Transformer-based encoder block sometimes learns faster than Bi-GRU at the early learning stage. It achieved a comparable final performance, even with much greater computational resource requirements.</p>
<p><strong>Retrieval Strategy.</strong> We compare with history retrieval strategies with different history sizes (K) and pure recency-based strategies (i.e., taking the latest K observations as history, denoted as w/o rec). The learning curves of different strategies are in Figure 4 (right). In general, the impact of history window size is highly game-dependent, but the pure recency based ones do not differ significantly from RC-DQN at the beginning of learning. The issues of pure recency based strategy are: (1) limited additional information about objects provided by successive observations; and (2) higher variance of retrieved observations due to policy changes.</p>
<h3>5 Conclusion</h3>
<p>We formulate the general IF game playing as MPRC tasks, enabling an MPRC-style solution to efficiently address the key IF game challenges on the huge combinatorial action space and the partial observability in a unified framework. Our approaches achieved significant improvement over the previous state-of-the-art on both game scores and training data efficiency. Our formulation also bridges broader NLU/RC techniques to address other critical challenges in IF games for future work, e.g., common-sense reasoning, novelty-driven exploration, and multi-hop inference.</p>
<h3>Acknowledgments</h3>
<p>We would like to thank Matthew Hausknecht for helpful discussions on the Jericho environments.</p>
<p><sup>4</sup>We ignore <em>ztuu</em> due to the infinite reward loops.</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L Hamilton. 2020. Learning dynamic knowledge graphs to generalize on textbased games. arXiv preprint arXiv:2002.09127.</p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. arXiv, pages arXiv2001.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565.</p>
<p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2019. Learning to retrieve reasoning paths over wikipedia graph for question answering. arXiv preprint arXiv:1911.10470.</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18701879 .</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. 2018. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pages 41-75. Springer.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. 2019. Cognitive graph for multi-hop reading comprehension at scale. In Proceedings of ACL 2019.</p>
<p>Ameya Godbole, Dilip Kavarthapu, Rajarshi Das, Zhiyu Gong, Abhishek Singhal, Hamed Zamani, Mo Yu, Tian Gao, Xiaoxiao Guo, Manzil Zaheer, et al. 2019. Multi-step entity-centric information retrieval for multi-hop question answering. arXiv preprint arXiv:1909.07598.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Côté, and Xingdi Yuan. 2019a. Interactive fiction games: A colossal adventure. arXiv preprint arXiv:1909.05398.</p>
<p>Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019b. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.</p>
<p>Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17361745 .</p>
<p>Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, page 3 .</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard em approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 28442857.</p>
<p>Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. Knowledge guided text retrieval and reading for open domain question answering. arXiv preprint arXiv:1911.03868.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533.</p>
<p>Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui Su. 2020. Frustratingly hard evidence retrieval for qa over books. In Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events, pages 108-113.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227-2237.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension.</p>
<p>Shuohang Wang and Jing Jiang. 2016. Machine comprehension using match-lstm and answer pointer. arXiv preprint arXiv:1608.07905.</p>
<p>Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018. R 3: Reinforced ranker-reader for open-domain question answering. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2017. Evidence aggregation for answer re-ranking in open-domain question answering. arXiv preprint arXiv:1711.05116.</p>
<p>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541.</p>
<p>Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 3562-3573.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://spacy.io&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
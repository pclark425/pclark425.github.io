<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4768 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4768</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4768</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-265158018</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.08154v3.pdf" target="_blank">Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios</a></p>
                <p><strong>Paper Abstract:</strong> Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input questions and the answer format of reasoning paths may be known or unknown. Self-agreement firstly samples from language model's decoder to generate a \textit{diverse} set of reasoning paths, and subsequently prompts the language model \textit{one more time} to determine the optimal answer by selecting the most \textit{agreed} answer among the sampled reasoning paths. Self-agreement simultaneously achieves remarkable performance on six public reasoning benchmarks and superior generalization capabilities.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4768.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4768.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Agreement (GPT-3.5, first scenario)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Agreement (ask k times + ask one more time) applied to GPT-3.5-turbo in mixed-task (first) scenario</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-optimization method that samples multiple diverse chain-of-thought reasoning paths from the same LLM (ask k times) and then prompts the LLM once more to extract final answers and pick the majority (ask one more time); designed to be generalizable when question type and answer-format are unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo chat model accessed via public API (transformer-based conversational model; size not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Agreement (ask k times + ask one more time)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate k reasoning paths by sampling from the model's decoder (temperature/p-nucleus or temperature sampling). Then provide the k paths as input to the same LLM with a prompt that (1) extracts the final answers from each path and (2) selects the most agreed (majority) answer via greedy decoding; diversity is produced by stochastic sampling (temperature/top-k/p settings).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mixed multi-task (first scenario) covering GSM8K, MultiArith, SVAMP, CommonsenseQA, ARC-c, Last Letters</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A mixed collection of arithmetic, commonsense and symbolic reasoning benchmarks used to simulate the real-world case where question type and answer format are unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K: Self-Agreement accuracy 58.5% (reported) vs Zero-Shot CoT baseline 48.5% on GSM8K; paper reports average absolute accuracy improvement across tasks of +5.3% for GPT-3.5-turbo (using 20 sampled outputs/run, averaged over 5 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Zero-Shot CoT (single chain / greedy): GSM8K 48.5% (baseline reported in paper); Self-Consistency not applicable in this scenario (no triggers).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling multiple diverse reasoning paths and using the LLM itself to extract and majority-vote (Self-Agreement) substantially improves accuracy over single-chain zero-shot CoT on mixed/unknown tasks (improvement reported across all tested benchmarks; average +5.3% for GPT-3.5-turbo). Self-Agreement is generalizable to unknown task types and unknown answer formats because it relies on the LLM to parse and compare answers rather than rule-based extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>None reported for this particular model+scenario; self-consistency was declared not applicable because many sampled paths lacked explicit answer triggers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4768.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4768.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Agreement (Llama-2-13B, first scenario)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Agreement (ask k times + ask one more time) applied to Llama-2-13B-Chat in mixed-task (first) scenario</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same Self-Agreement ensemble pipeline applied to an open-source 13B chat model (Llama-2-13B-Chat); uses sampling to create diverse chains and LLM-based majority selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama-2 chat model with ~13 billion parameters, optimized for dialogue via alignment; transformer-based and open-source (as used in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Agreement (ask k times + ask one more time)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Same as above: stochastic sampling of k reasoning paths (temperature + top-k truncation for Llama-2), then a single greedy pass where the model extracts and counts final answers to pick the majority.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mixed multi-task (first scenario) covering GSM8K, MultiArith, SVAMP, CommonsenseQA, ARC-c, Last Letters</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mixed collection of arithmetic, commonsense and symbolic reasoning benchmarks used to simulate unknown real-world inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K: Zero-Shot CoT baseline reported 26.0% for Llama-2-13B-Chat; Self-Agreement reported 31.7% on GSM8K (delta +5.7). The paper reports an average absolute accuracy improvement of +5.8% for Llama-2-13B-Chat across datasets in the first scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Zero-Shot CoT (single chain / greedy): GSM8K 26.0% (reported baseline in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-Agreement consistently improves accuracy across model scales including Llama-2-13B-Chat in mixed unknown scenarios; gains are similar across model scales though smaller models show reduced absolute gains (some abilities are scale-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Smaller Llama variant (7B) shows relatively smaller gains; model-scale dependent emergence noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4768.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4768.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Agreement vs Mixed-Few-Shot (GPT-3.5, second scenario)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Agreement compared to mixed-few-shot CoT and self-consistency in the second scenario (unknown question type, known answer format) on GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison showing Self-Agreement (diverse sampling + LLM-based aggregation) outperforms mixed-few-shot CoT and that self-consistency can degrade when task type is unknown despite known answer-format triggers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo chat model accessed via public API.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Agreement (diverse) vs Mixed-Few-Shot CoT (similar)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse (Self-Agreement) vs similar (Mixed-Few-Shot CoT single-shot few-shot chains)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Mixed-few-shot CoT: provide a mixed set of few-shot chain-of-thought demonstrations (one demo per task) and run single-generation per question; Self-Agreement: sample many diverse paths and use LLM to majority-vote. Self-Consistency (comparator baseline) is also a diverse-sampling + rule-based majority-vote that requires known answer-format triggers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mixed multi-task (second scenario) where question type is unknown but answer format contains a trigger (e.g., 'The answer is')</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks mixed across reasoning categories but with answers present in a predictable trigger format so rule-based extraction is possible in principle.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K: Mixed-Few-Shot CoT reported 69.0% (baseline); Self-Agreement reported 81.3% on GSM8K (+12.3). Paper reports average absolute accuracy improvement for Self-Agreement of +5.5% for GPT-3.5-turbo (second scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Mixed-Few-Shot CoT: GSM8K 69.0% (reported); Self-Consistency performed worse than mixed-few-shot CoT in this scenario (average -1.2% for GPT-3.5-turbo) because rule-based extraction failed when task type could not be reliably determined.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-Agreement strongly outperforms a naive mixed-few-shot CoT baseline on mixed-task inputs with known answer-format triggers (large gains on harder datasets like GSM8K). Self-Consistency, which relies on rule-based extraction, can underperform when question type is unknown even if triggers exist, because string-based extraction/comparison fails across heterogeneous tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Self-Consistency performs worse than mixed-few-shot CoT in the second scenario (reported average -1.2% for GPT-3.5), illustrating that a diverse sampling method coupled with brittle answer-extraction can hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4768.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4768.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Agreement vs Self-Consistency / USC (third scenario)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Agreement compared to Self-Consistency and USC in the third scenario (known task type and known answer format) on GPT-3.5-turbo and larger models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When tasks and answer formats are known (classical fixed benchmarks), Self-Agreement matches or slightly exceeds Self-Consistency and outperforms USC in many cases; with sufficiently many sampled paths both ensemble methods converge in performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, also evaluated on LaMDA, PaLM, Minerva in paper for third scenario</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-turbo (OpenAI chat model); paper also reports results on larger proprietary models (LaMDA, PaLM, Minerva) and Llama variants for this scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Agreement (diverse) compared to Self-Consistency (diverse, rule-based) and USC (LLM-based selector)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse (both Self-Agreement and Self-Consistency sample multiple paths; few-shot CoT is similar/single)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Self-Consistency: sample multiple CoT paths then apply rule-based answer extraction (requires known triggers) and majority vote. USC: LLM-based selecting most consistent answer (related idea). Self-Agreement: LLM-based answer extraction + majority vote; differs by prompt design and generalizable extraction/comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Standard single-dataset benchmarks where task and answer-format known (third scenario), e.g., GSM8K, SVAMP, MultiArith, CSQA, ARC-c, Last Letters</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Canonical reasoning benchmarks with known answer formats used in prior CoT/self-consistency studies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On GSM8K with 20 sampled reasoning paths: Self-Consistency reported 80.3% (paper table), Self-Agreement reported 82.4% (delta +2.1). USC reported lower numbers (e.g., USC 76.9% in the table for GSM8K with 20 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Self-Consistency: GSM8K 80.3% (20 samples); USC: GSM8K 76.9% (20 samples) as reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-Agreement attains performance comparable to or slightly better than Self-Consistency on known-format benchmarks, and substantially outperforms USC in the reported runs. When the number of sampled reasoning paths is large enough, Self-Agreement and Self-Consistency converge to similar performance; Self-Agreement's advantage is its generalizability to unknown answer formats.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Paper reports that when number of sampled paths is increased to 40, Self-Agreement's average advantage over Self-Consistency slightly decreases (an observed average decrease of 0.3 in one analysis), indicating that at large sample counts both methods can reach similar ceilings and that small fluctuations can make Self-Agreement slightly worse in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4768.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4768.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Method-level findings on diverse vs similar reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate empirical findings in the paper on the effectiveness of diverse (multiple sampled paths + aggregation) versus similar (single-chain CoT) reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper-level synthesis of experiments comparing diverse-sampling ensemble methods (Self-Agreement, Self-Consistency, Multi-Agents) against single-chain CoT baselines (zero-shot / few-shot); reports robustness, sample-efficiency, and failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-3.5-turbo, Llama-2-13B-Chat, Llama-2-7B/70B, PaLM, LaMDA, Minerva reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various transformer-based LLMs across scales evaluated: GPT-3.5-turbo (public), Llama-2 chat family (7B/13B/70B), and larger closed models (LaMDA, PaLM, Minerva) for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Diverse-sampling ensembling (Self-Agreement / Self-Consistency / Multi-Agents) versus single-chain CoT (zero-shot, few-shot, mixed-few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse vs similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Diverse: create multiple reasoning trajectories by sampling (temperature/p, top-k), then aggregate via majority vote; Similar: single deterministic/greedy chain-of-thought prompted once (zero-shot or few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple reasoning benchmarks (arithmetic, commonsense, symbolic) as above</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See dataset descriptions: GSM8K, SVAMP, MultiArith (arithmetic); CommonsenseQA, ARC-c (commonsense); Last Letters (symbolic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>General empirical patterns: diverse-sampling + aggregation (Self-Agreement) consistently improves over single-chain CoT baselines across tasks and models (paper reports average gains e.g., +5.3% GPT-3.5 first scenario, +5.8% Llama-2-13B first scenario; large task-specific gains like +12.4 on GSM8K vs few-shot CoT in one table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Single-chain CoT (zero-shot / few-shot) baselines: lower accuracy across tasks (examples in paper: GSM8K single-chain baselines: zero-shot 48.5% for GPT-3.5; mixed-few-shot 69.0% for GPT-3.5 in second scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Diverse reasoning (sampling multiple CoT paths) + aggregation reliably improves reasoning accuracy vs single-chain CoT across benchmarks and models. 2) Self-Agreement (LLM-based aggregation) generalizes to unknown task/answer-format settings where rule-based aggregators fail. 3) Performance improves with number of sampled paths but shows diminishing returns (performance nearly converges at 5–10 samples); sampling 20 used in main experiments as a tradeoff. 4) Robust across sampling diversity (temperature/p settings). 5) With sufficiently many samples, diverse aggregation methods converge to similar ceilings (Self-Agreement ≈ Self-Consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>1) Self-Consistency can perform worse than mixed-few-shot CoT in the second scenario due to brittle answer-extraction when task types are unknown (reported average -1.2% for GPT-3.5). 2) When increasing sampled reasoning paths from 20 to 40, Self-Agreement's average edge over Self-Consistency decreased and in one reported aggregate the Self-Agreement average dropped by 0.3, indicating that at large sampling budgets gains can fluctuate and are not strictly monotonic. 3) Smaller models (e.g., Llama-2-7B) show smaller gains, reflecting scale-dependent emergence of debating/refinement abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Universal self-consistency for large language model generation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4768",
    "paper_id": "paper-265158018",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Self-Agreement (GPT-3.5, first scenario)",
            "name_full": "Self-Agreement (ask k times + ask one more time) applied to GPT-3.5-turbo in mixed-task (first) scenario",
            "brief_description": "An ensemble-optimization method that samples multiple diverse chain-of-thought reasoning paths from the same LLM (ask k times) and then prompts the LLM once more to extract final answers and pick the majority (ask one more time); designed to be generalizable when question type and answer-format are unknown.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI gpt-3.5-turbo chat model accessed via public API (transformer-based conversational model; size not specified in paper).",
            "reasoning_method_name": "Self-Agreement (ask k times + ask one more time)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate k reasoning paths by sampling from the model's decoder (temperature/p-nucleus or temperature sampling). Then provide the k paths as input to the same LLM with a prompt that (1) extracts the final answers from each path and (2) selects the most agreed (majority) answer via greedy decoding; diversity is produced by stochastic sampling (temperature/top-k/p settings).",
            "task_name": "Mixed multi-task (first scenario) covering GSM8K, MultiArith, SVAMP, CommonsenseQA, ARC-c, Last Letters",
            "task_description": "A mixed collection of arithmetic, commonsense and symbolic reasoning benchmarks used to simulate the real-world case where question type and answer format are unknown.",
            "performance": "GSM8K: Self-Agreement accuracy 58.5% (reported) vs Zero-Shot CoT baseline 48.5% on GSM8K; paper reports average absolute accuracy improvement across tasks of +5.3% for GPT-3.5-turbo (using 20 sampled outputs/run, averaged over 5 runs).",
            "comparison_with_other_method": true,
            "performance_other_method": "Zero-Shot CoT (single chain / greedy): GSM8K 48.5% (baseline reported in paper); Self-Consistency not applicable in this scenario (no triggers).",
            "key_findings": "Sampling multiple diverse reasoning paths and using the LLM itself to extract and majority-vote (Self-Agreement) substantially improves accuracy over single-chain zero-shot CoT on mixed/unknown tasks (improvement reported across all tested benchmarks; average +5.3% for GPT-3.5-turbo). Self-Agreement is generalizable to unknown task types and unknown answer formats because it relies on the LLM to parse and compare answers rather than rule-based extraction.",
            "counter_examples_or_negative_results": "None reported for this particular model+scenario; self-consistency was declared not applicable because many sampled paths lacked explicit answer triggers.",
            "uuid": "e4768.0",
            "source_info": {
                "paper_title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Self-Agreement (Llama-2-13B, first scenario)",
            "name_full": "Self-Agreement (ask k times + ask one more time) applied to Llama-2-13B-Chat in mixed-task (first) scenario",
            "brief_description": "Same Self-Agreement ensemble pipeline applied to an open-source 13B chat model (Llama-2-13B-Chat); uses sampling to create diverse chains and LLM-based majority selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-13B-Chat",
            "model_description": "Meta's Llama-2 chat model with ~13 billion parameters, optimized for dialogue via alignment; transformer-based and open-source (as used in paper).",
            "reasoning_method_name": "Self-Agreement (ask k times + ask one more time)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Same as above: stochastic sampling of k reasoning paths (temperature + top-k truncation for Llama-2), then a single greedy pass where the model extracts and counts final answers to pick the majority.",
            "task_name": "Mixed multi-task (first scenario) covering GSM8K, MultiArith, SVAMP, CommonsenseQA, ARC-c, Last Letters",
            "task_description": "Mixed collection of arithmetic, commonsense and symbolic reasoning benchmarks used to simulate unknown real-world inputs.",
            "performance": "GSM8K: Zero-Shot CoT baseline reported 26.0% for Llama-2-13B-Chat; Self-Agreement reported 31.7% on GSM8K (delta +5.7). The paper reports an average absolute accuracy improvement of +5.8% for Llama-2-13B-Chat across datasets in the first scenario.",
            "comparison_with_other_method": true,
            "performance_other_method": "Zero-Shot CoT (single chain / greedy): GSM8K 26.0% (reported baseline in paper).",
            "key_findings": "Self-Agreement consistently improves accuracy across model scales including Llama-2-13B-Chat in mixed unknown scenarios; gains are similar across model scales though smaller models show reduced absolute gains (some abilities are scale-dependent).",
            "counter_examples_or_negative_results": "Smaller Llama variant (7B) shows relatively smaller gains; model-scale dependent emergence noted.",
            "uuid": "e4768.1",
            "source_info": {
                "paper_title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Self-Agreement vs Mixed-Few-Shot (GPT-3.5, second scenario)",
            "name_full": "Self-Agreement compared to mixed-few-shot CoT and self-consistency in the second scenario (unknown question type, known answer format) on GPT-3.5-turbo",
            "brief_description": "Comparison showing Self-Agreement (diverse sampling + LLM-based aggregation) outperforms mixed-few-shot CoT and that self-consistency can degrade when task type is unknown despite known answer-format triggers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI gpt-3.5-turbo chat model accessed via public API.",
            "reasoning_method_name": "Self-Agreement (diverse) vs Mixed-Few-Shot CoT (similar)",
            "reasoning_method_type": "diverse (Self-Agreement) vs similar (Mixed-Few-Shot CoT single-shot few-shot chains)",
            "reasoning_method_description": "Mixed-few-shot CoT: provide a mixed set of few-shot chain-of-thought demonstrations (one demo per task) and run single-generation per question; Self-Agreement: sample many diverse paths and use LLM to majority-vote. Self-Consistency (comparator baseline) is also a diverse-sampling + rule-based majority-vote that requires known answer-format triggers.",
            "task_name": "Mixed multi-task (second scenario) where question type is unknown but answer format contains a trigger (e.g., 'The answer is')",
            "task_description": "Tasks mixed across reasoning categories but with answers present in a predictable trigger format so rule-based extraction is possible in principle.",
            "performance": "GSM8K: Mixed-Few-Shot CoT reported 69.0% (baseline); Self-Agreement reported 81.3% on GSM8K (+12.3). Paper reports average absolute accuracy improvement for Self-Agreement of +5.5% for GPT-3.5-turbo (second scenario).",
            "comparison_with_other_method": true,
            "performance_other_method": "Mixed-Few-Shot CoT: GSM8K 69.0% (reported); Self-Consistency performed worse than mixed-few-shot CoT in this scenario (average -1.2% for GPT-3.5-turbo) because rule-based extraction failed when task type could not be reliably determined.",
            "key_findings": "Self-Agreement strongly outperforms a naive mixed-few-shot CoT baseline on mixed-task inputs with known answer-format triggers (large gains on harder datasets like GSM8K). Self-Consistency, which relies on rule-based extraction, can underperform when question type is unknown even if triggers exist, because string-based extraction/comparison fails across heterogeneous tasks.",
            "counter_examples_or_negative_results": "Self-Consistency performs worse than mixed-few-shot CoT in the second scenario (reported average -1.2% for GPT-3.5), illustrating that a diverse sampling method coupled with brittle answer-extraction can hurt performance.",
            "uuid": "e4768.2",
            "source_info": {
                "paper_title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Self-Agreement vs Self-Consistency / USC (third scenario)",
            "name_full": "Self-Agreement compared to Self-Consistency and USC in the third scenario (known task type and known answer format) on GPT-3.5-turbo and larger models",
            "brief_description": "When tasks and answer formats are known (classical fixed benchmarks), Self-Agreement matches or slightly exceeds Self-Consistency and outperforms USC in many cases; with sufficiently many sampled paths both ensemble methods converge in performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, also evaluated on LaMDA, PaLM, Minerva in paper for third scenario",
            "model_description": "GPT-3.5-turbo (OpenAI chat model); paper also reports results on larger proprietary models (LaMDA, PaLM, Minerva) and Llama variants for this scenario.",
            "reasoning_method_name": "Self-Agreement (diverse) compared to Self-Consistency (diverse, rule-based) and USC (LLM-based selector)",
            "reasoning_method_type": "diverse (both Self-Agreement and Self-Consistency sample multiple paths; few-shot CoT is similar/single)",
            "reasoning_method_description": "Self-Consistency: sample multiple CoT paths then apply rule-based answer extraction (requires known triggers) and majority vote. USC: LLM-based selecting most consistent answer (related idea). Self-Agreement: LLM-based answer extraction + majority vote; differs by prompt design and generalizable extraction/comparison.",
            "task_name": "Standard single-dataset benchmarks where task and answer-format known (third scenario), e.g., GSM8K, SVAMP, MultiArith, CSQA, ARC-c, Last Letters",
            "task_description": "Canonical reasoning benchmarks with known answer formats used in prior CoT/self-consistency studies.",
            "performance": "On GSM8K with 20 sampled reasoning paths: Self-Consistency reported 80.3% (paper table), Self-Agreement reported 82.4% (delta +2.1). USC reported lower numbers (e.g., USC 76.9% in the table for GSM8K with 20 samples).",
            "comparison_with_other_method": true,
            "performance_other_method": "Self-Consistency: GSM8K 80.3% (20 samples); USC: GSM8K 76.9% (20 samples) as reported in paper tables.",
            "key_findings": "Self-Agreement attains performance comparable to or slightly better than Self-Consistency on known-format benchmarks, and substantially outperforms USC in the reported runs. When the number of sampled reasoning paths is large enough, Self-Agreement and Self-Consistency converge to similar performance; Self-Agreement's advantage is its generalizability to unknown answer formats.",
            "counter_examples_or_negative_results": "Paper reports that when number of sampled paths is increased to 40, Self-Agreement's average advantage over Self-Consistency slightly decreases (an observed average decrease of 0.3 in one analysis), indicating that at large sample counts both methods can reach similar ceilings and that small fluctuations can make Self-Agreement slightly worse in some settings.",
            "uuid": "e4768.3",
            "source_info": {
                "paper_title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Method-level findings on diverse vs similar reasoning",
            "name_full": "Aggregate empirical findings in the paper on the effectiveness of diverse (multiple sampled paths + aggregation) versus similar (single-chain CoT) reasoning",
            "brief_description": "Paper-level synthesis of experiments comparing diverse-sampling ensemble methods (Self-Agreement, Self-Consistency, Multi-Agents) against single-chain CoT baselines (zero-shot / few-shot); reports robustness, sample-efficiency, and failure modes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-3.5-turbo, Llama-2-13B-Chat, Llama-2-7B/70B, PaLM, LaMDA, Minerva reported in paper)",
            "model_description": "Various transformer-based LLMs across scales evaluated: GPT-3.5-turbo (public), Llama-2 chat family (7B/13B/70B), and larger closed models (LaMDA, PaLM, Minerva) for some tasks.",
            "reasoning_method_name": "Diverse-sampling ensembling (Self-Agreement / Self-Consistency / Multi-Agents) versus single-chain CoT (zero-shot, few-shot, mixed-few-shot)",
            "reasoning_method_type": "diverse vs similar",
            "reasoning_method_description": "Diverse: create multiple reasoning trajectories by sampling (temperature/p, top-k), then aggregate via majority vote; Similar: single deterministic/greedy chain-of-thought prompted once (zero-shot or few-shot).",
            "task_name": "Multiple reasoning benchmarks (arithmetic, commonsense, symbolic) as above",
            "task_description": "See dataset descriptions: GSM8K, SVAMP, MultiArith (arithmetic); CommonsenseQA, ARC-c (commonsense); Last Letters (symbolic).",
            "performance": "General empirical patterns: diverse-sampling + aggregation (Self-Agreement) consistently improves over single-chain CoT baselines across tasks and models (paper reports average gains e.g., +5.3% GPT-3.5 first scenario, +5.8% Llama-2-13B first scenario; large task-specific gains like +12.4 on GSM8K vs few-shot CoT in one table).",
            "comparison_with_other_method": true,
            "performance_other_method": "Single-chain CoT (zero-shot / few-shot) baselines: lower accuracy across tasks (examples in paper: GSM8K single-chain baselines: zero-shot 48.5% for GPT-3.5; mixed-few-shot 69.0% for GPT-3.5 in second scenario).",
            "key_findings": "1) Diverse reasoning (sampling multiple CoT paths) + aggregation reliably improves reasoning accuracy vs single-chain CoT across benchmarks and models. 2) Self-Agreement (LLM-based aggregation) generalizes to unknown task/answer-format settings where rule-based aggregators fail. 3) Performance improves with number of sampled paths but shows diminishing returns (performance nearly converges at 5–10 samples); sampling 20 used in main experiments as a tradeoff. 4) Robust across sampling diversity (temperature/p settings). 5) With sufficiently many samples, diverse aggregation methods converge to similar ceilings (Self-Agreement ≈ Self-Consistency).",
            "counter_examples_or_negative_results": "1) Self-Consistency can perform worse than mixed-few-shot CoT in the second scenario due to brittle answer-extraction when task types are unknown (reported average -1.2% for GPT-3.5). 2) When increasing sampled reasoning paths from 20 to 40, Self-Agreement's average edge over Self-Consistency decreased and in one reported aggregate the Self-Agreement average dropped by 0.3, indicating that at large sampling budgets gains can fluctuate and are not strictly monotonic. 3) Smaller models (e.g., Llama-2-7B) show smaller gains, reflecting scale-dependent emergence of debating/refinement abilities.",
            "uuid": "e4768.4",
            "source_info": {
                "paper_title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Universal self-consistency for large language model generation",
            "rating": 2,
            "sanitized_title": "universal_selfconsistency_for_large_language_model_generation"
        }
    ],
    "cost": 0.020833749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios
24 May 2024</p>
<p>Lei Lin linlei@kuaishou.com 
Kuaishou Technology
BeijingChina</p>
<p>Jiayi Fu fujiayi@kuaishou.com 
Kuaishou Technology
BeijingChina</p>
<p>Pengli Liu 
Kuaishou Technology
BeijingChina</p>
<p>Qingyang Li 
Yan Gong 
Kuaishou Technology
BeijingChina</p>
<p>School of Computer Science and Engineering
Northeastern University
ShenyangChina</p>
<p>Junchen Wan wanjunchen@kuaishou.com 
Kuaishou Technology
BeijingChina</p>
<p>Fuzheng Zhang zhangfuzheng@kuaishou.com 
Kuaishou Technology
BeijingChina</p>
<p>Zhongyuan Wang 
Kuaishou Technology
BeijingChina</p>
<p>Di Zhang 
Kun Gai 
Kuaishou Technology
BeijingChina</p>
<p>..... ZihaoEric Zhao 
Kuaishou Technology
BeijingChina</p>
<p>Shi Wallace 
Dan Feng 
Singh 2021 Klein 
Yongchao Zhou 
Andrei Ioan Muresanu 
Ziwen Han 
Keiran Paster 
Silviu Pitis 
Harris Chan 
Jimmy 2022 Ba 
Large 
Anni Zou 
Zhuosheng Zhang 
Hai Zhao 
Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios
24 May 20247FF59AD29A3AC7819A37513579B627CAarXiv:2311.08154v3[cs.CL]
Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality.To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly.However, current ensemble-optimization methods either simply employ rule-based postprocessing such as self-consistency, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown.To avoid their limitations, we propose Self-Agreement, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input questions and the answer format of reasoning paths may be known or unknown.Self-agreement firstly samples from language model's decoder to generate a diverse set of reasoning paths, and subsequently prompts the language model one more time to determine the optimal answer by selecting the most agreed answer among the sampled reasoning paths.Selfagreement simultaneously achieves remarkable performance on six public reasoning benchmarks and superior generalization capabilities.</p>
<p>Introduction</p>
<p>Although large language models (LLMs) have revolutionized the natural language processing (NLP) landscape, their ability to solve challenging tasks (e.g., arithmetic, commonsense and symbolic reasoning) is often seen as a limitation, which is difficult to be overcome solely by scaling up the size of LLMs (Rae et al., 2021;Srivastava et al., 2022).To address this issue, Wei et al. (2022b) have proposed chain-of-thought (CoT) prompting, which * Equal Contribution.</p>
<p>provides a few examples consisting of reasoning steps to prompt LLMs to generate intermediate reasoning steps towards final answers.It has been demonstrated that CoT prompting can elicit strong reasoning capabilities from LLMs, and achieve superior performance in solving complex tasks (Wei et al., 2022b).However, the naive greedy decoding strategy used in CoT prompting usually causes the repetitiveness and local optimality.</p>
<p>This work studies ensemble-optimization (Qiao et al., 2022) in multi-step reasoning situations.Ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly.It avoids the repetitiveness and local optimality that plague greedy decoding, while mitigating the stochasticity caused by a single sampled generation (Wang et al., 2022).</p>
<p>Current ensemble-optimization methods predominantly fall into two categories, i.e., verifier or re-ranker based methods and post-processing based methods.Verifier or re-ranker based methods either train an additional verifier (Cobbe et al., 2021;Li et al., 2023b) or train a re-ranker based on task-related human annotations (Thoppilan et al., 2022) to select the best generation among multiple generations.Post-processing based methods take a majority vote among all generated reasoning paths (Wang et al., 2022;Du et al., 2023;Liang et al., 2023;Chen et al., 2023a) or top K complex reasoning paths (Fu et al., 2022), and then choose the optimal answer that receives the most votes.</p>
<p>While the success of ensemble-optimization work, we show that there are two major limitations.Figure 1 illustrates the main limitations of existing ensemble-optimization methods.On one hand, although verifier or re-ranker based methods have favorable generalization ability to some extent as they impose no restrictions on answer formats of reasoning paths, but they can only be applied in scenario where the type of questions is already known, yet fail to generalize to scenarios where the</p>
<p>Self-consistency</p>
<p>Not Applicable</p>
<ol>
<li>First, we can calculate how many miles James runs in a week: \n\n12 miles/day x 5, {...}, time = 60 miles / 10 miles/hour = 5 hours.</li>
</ol>
<p>Step 1: Calculate the total distance James runs in a week.\n12 miles per day * 5 days per week = 60 miles, {...}, James runs for 6 hours a week.</p>
<p>First, we can calculate how many miles James runs in a week by multiplying the number of miles, {...}, James runs for a total of 6 hours.</p>
<p>Sample diverse reasoning paths</p>
<p>Agent-based medthods type of questions is unknown.Besides, it is costly to train such an additional model.On the other hand, post-processing based methods can only be applied in scenario where both the type of questions and answer formats of reasoning paths are already known.</p>
<p>Nevertheless, in practical applications, language models often encounter situations of unknown type of questions or different answer formats, where it cannot be clearly identified which task the questions belong to and what answer format of the reasoning paths appears to be. Figure 2 shows a simple example to illustrate three major cases covering almost all possible situations.Briefly, we categorize the types of questions into known and unknown, i.e., whether we can identify in advance that the question belongs to a particular task, and answer formats into with (w/) and without (wo/) trigger (e.g., The answer is). 1 So, the first case (a) is that both the type of question and answer format are unknown, the second one (b) is that the type of question is unknown while the answer format is known, and the third one (c) is that the type of question and answer format are already known.When encountering situations of unknown type of questions and answer formats (i.e., the first or second case), it is neither reasonable to train an additional model given several task-related human annotations nor possible to manually identify which task it refers to, not to mention that the question encountered in real-world scenarios is not even from a predefined set of tasks.Besides, the answer formats of reasoning paths in real-world are ever-changing.However, existing ensemble-optimization studies 1 It is natural to simulate if the type of questions is known by whether or not to mix different reasoning tasks.Besides, we simulate whether the reasoning path has a trigger via using few-shot CoT or zero-shot CoT.We employ zero-shot CoT without 2nd answer extraction phase to simulate the reasoning paths with no trigger.commonly assume that both the type of questions and answer format of reasoning paths are already known, which is contradictory to real-world scenarios and has limited application values.Therefore, one natural question can be raised: how to design a simple ensemble-optimization method to solve reasoning tasks in almost all situations?</p>
<p>To this end, we propose self-agreement, a new ensemble-optimization scheme that further improves reasoning performance of language models in almost all possible scenarios by a large margin.We get insights from the majority vote design used in Wang et al. (2022), and start from the intuition that assessing whether or not a person really knows how to solve a question should depend on multiple reasoning paths generated by themself, since a person might acquire the wrong answer due to carelessness or other factors.</p>
<p>Figure 3 illustrates the self-agreement method with an example.Self-agreement comprises two phases: Firstly, we prompt the language model with few-shot CoT (Wei et al., 2022b) (w/ answer trigger) or zero-shot CoT (Kojima et al., 2022) (wo/ answer trigger).Then, we sample from language model's decoder to generate a diverse set of reasoning paths.We name this phase ask k times, where k is the size of the diverse set; Secondly, we prompt the language model one more time to determine the optimal answer by selecting the most agreed answer among the sampled reasoning paths.We name this phase ask one more time.Such an approach is analogous to the fact that the most agreed answer selected from multiple reasoning paths is likely to be the correct answer, since most reasoning paths generated by themself have a high probability to arrive at the same correct answer if a person actually knows how to solve it.Therefore, we refer to our approach as Self-Agreement, which stands for the fact.Self-agreement is simple yet effective,  b) and (c) represent three major different cases covering almost all possible situations.The first scenario (a) is that both the type of question and answer format are unknown, and the second one (b) is that the type of question is unknown while the answer format is known.The third one (c) is that the type of question and answer format are already known (i.e., the type of questions belongs to GSM8K (Cobbe et al., 2021), and the answer has the trigger (e.g., The answer is)).</p>
<p>completely unsupervised, requires no additional human annotations or auxiliary models, and avoids any additional training or fine-tuning.To the best of our knowledge, we are the first to propose the realworld scenarios in ensemble-optimization studies, and design a simple ensemble-optimization method applying in almost all scenarios. 2 To validate the efficacy of self-agreement, we conduct experiments on six public reasoning benchmarks covering arithmetic, commonsense and symbolic reasoning.Experimental results show that self-agreement improves reasoning performance of language models by a striking margin across all tasks in all three scenarios.Moreover, our method also exhibits versatility across various language models and model sizes, task settings, reasoning paths with different diversity, and prompting methods, highlighting its broad applicability.</p>
<p>Related Work</p>
<p>In-context Learning Language models have revolutionized a wide range of NLP tasks, where scaling up the model size is one of the key ingredients (Vaswani et al., 2017;Kenton and Toutanova, 2019;Raffel et al., 2020;Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022;Thoppilan et al., 2022).The success of LLMs is often attributed to emergent abilities when the model reaches a sufficient scale (Wei et al., 2022a).That is, the model can follow the format of given prompts (typically a few task-specific examples) thus solving the corresponding tasks (also referred as in-context learning).The method of conditioning LLMs is called "prompting" (Liu et al., 2023), which can be categorized into two main directions: few-shot prompting (a few examples as the prompt)</p>
<p>2 After completion of this work, we find that Chen et al. (2023b) share almost the same idea with us, which is archived after our work.We provide discussion with it in Appendix C. and zero-shot prompting (instructions describing the task as the prompt).Prompting allows a single model to carry out various tasks universally.Due to its superior benefits, there are studies (Liu et al., 2021;Lu et al., 2021Lu et al., , 2022) ) further investigate how to improve the performance of in-context learning.Specifically, different wording or order of given examples may lead to performance fluctuations (Zhao et al., 2021;Webson and Pavlick, 2021).This work takes an important step forward in multi-step reasoning by showing the critical role of language model itself in the process of selecting the most agreed answer based on multiple reasoning paths.</p>
<p>Reasoning with Language Models Reasoning, the process of making inference based on existing information or knowledge, is the core of human intelligence and essential for solving complex questions (Yu et al., 2023a).In contrast to the excellent performance of LLMs in simple and single-step tasks, language models (even 100B or more parameters) are demonstrated to struggle at solving challenging tasks required multi-step reasoning (Rae et al., 2021;Srivastava et al., 2022).To address this issue, Nye et al. (2021) have proposed to decompose multi-step reasoning problems into intermediate steps before obtaining final answers.Furthermore, Wei et al. (2022b) have proposed chainof-thought prompting, which elicits this reasoning process from language models.Since then, designing prompts manually (Wei et al., 2022b;Fu et al., 2022;Diao et al., 2023) or automatically (Kojima et al., 2022;Zhang et al., 2022;Shum et al., 2023), example selection in CoT prompting (Fu et al., 2022;Zhang et al., 2022;Shum et al., 2023;Diao et al., 2023), and data augmentation (Fu et al., 2023;Yue et al., 2023;Yu et al., 2023b) have become a hot topic in NLP.Our work sits in the context of zero-shot CoT and few-shot CoT, and proposes a new ensemble-optimization method that substan-1.First, we can calculate how many miles James runs in a week: \n\n12 miles/day x 5, {...}, time = 60 miles / 10 miles/hour = 5 hours.</p>
<p>Step 1: Calculate the total distance James runs in a week.\n12 miles per day * 5 days per week = 60 miles, {...}, James runs for 6 hours a week.</p>
<p>First, we can calculate how many miles James runs in a week by multiplying the number of miles, {...}, James runs for a total of 6 hours.</p>
<p>Sample diverse reasoning paths</p>
<p>When making a majority vote on multiple corresponding solutions, {...}, follow these detailed steps: 1.Understand the Question: {.The majority voted answer is: James runs for 6 hours a week.</p>
<p>Figure 3: The self-agreement method contains two stages: (1) ask k times: sample from the language model itself k times to generate k diverse reasoning paths using few-shot CoT (Wei et al., 2022b) or zero-shot CoT (Kojima et al., 2022); and (2) ask one more time: select the most agreed answer based on k reasoning paths by language model itself.Yellow denotes the carefully designed prompt proposed in this work (see Table 11 in the Appendix), and red denotes the most agreed answer generated by the language model itself.</p>
<p>tially outperforms the original zero-shot CoT and few-shot CoT, respectively.Ensemble-Optimization in Reasoning with Language Models Ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly (Qiao et al., 2022).We view most ensemble-optimization work as different implementations of the majority voted idea. 3For example, self-consistency (Wang et al., 2022) takes the majority vote over sampled reasoning paths, while multi-agents (debate) (Du et al., 2023) obtains the majority voted answer over multiple agents' outputs.It is clear that the essence of them is based on the majority voted design, and our work is in lines with it, but we explore how to allow language model itself to achieve the overall procedure.So, the main difference between them lies in two major steps consisting of the process of the majority vote, i.e., answer extraction and answer comparison.To be specific, self-consistency and multiagents (debate) are rule-based and not generalizable method to extract and compare answers.However, self-agreement is an entirely generalizable method, whether extracting answers or comparing answers.</p>
<p>Methodology</p>
<p>Existing ensemble-optimization studies (Wang et al., 2022;Fu et al., 2022) commonly assume that both the type of questions fed to the model and answer format of reasoning paths outputted by the model are already known, and conduct evaluations on the questions from the same dataset.However, a more realistic setting is that the type of input questions or answer format of reasoning paths is unknown and they come in an arbitrary manner.</p>
<p>To address such scenarios, one natural idea is that Previous work has demonstrated that language models are good answer extractors given specific prompts (Kojima et al., 2022;Zhou et al., 2022;Yang et al., 2023a), and also suitable for comparing the consistency of final answers extracted from multiple reasoning paths given specific prompts (Yang et al., 2023b,c), as the extracted final answers in reasoning tasks tend to be short and semantically straightforward sentences or phrases.For instance, as shown in Figure 3, for language models, James runs for 6 hours a week and James runs for a total of 6 hours extracted from solution 2, 3 respectively represent the same final answer.Such behaviors are often attributed to emergent abilities (Wei et al., 2022a).That is, the model can understand and follow the format of given prompts thus solving the corresponding tasks.Motivated by the above findings and the majority vote design used in Wang et al. (2022), we propose the following self-agreement method that allows language model itself to extract final answers of multiple reasoning paths, count the votes of different answers, and then select the majority voted answer.Specifically, firstly, we prompt the language model with few-shot CoT (Wei et al., 2022b) (w/ answer trigger) or zero-shot CoT (Kojima et al., 2022) (wo/ answer trigger).Then, we sample from language model's decoder to generate a diverse set of reasoning paths.We refer to this stage as ask k times, where k is is the size of the diverse set.Table 1: Self-agreement, when applied on GPT-3.5-turbo and Llama-2-13B-Chat.Our performance gain (+blue) is computed over the mixed-few-shot CoT or zero-shot CoT (Kojima et al., 2022), which is our primary baseline.Our method substantially increases the performance over mixed-few-shot CoT and zero-shot CoT.The best performance across each model for each task is shown in bold.</p>
<p>Self-agreement is compatible with reasoning paths to the questions with different diversity. 4Secondly, we prompt the language model one more time to determine the optimal answer by selecting the most agreed answer among the sampled reasoning paths.We refer to this stage as ask one more time.Figure 3 shows the overall procedure of our approach.</p>
<p>Ask k Times</p>
<p>We simulate whether the type of questions is known or not by whether or not we mix tasks, 5 and whether the answer format of reasoning paths is known or not by utilizing few-shot CoT (Wei et al., 2022b) or zero-shot CoT (Kojima et al., 2022).In this stage, we first modify the input question x into a prompt x ′ .The prompt x ′ would be "Q:
[X]. A: " with a few hand-crafted examples or "Q: [X]. A:
Let's think step by step."if we employ few-shot CoT or zero-shot CoT, where [X] is an input slot for x.The same prompted text x ′ is then fed into language models and generate multiple sentences z i via sampling strategies, where i = 1, . . ., m indexes the m candidate outputs.</p>
<p>Ask One More Time</p>
<p>After sampling multiple reasoning paths z i from the model's decoder, self-agreement first extracts the final answers a i of each reasoning path, and then selects the most "agreed" answer among the final answer set by taking a majority vote over a i .The overall process can be achieved by prompting language model itself one more time.The carefully designed prompt is given in Table 11 in the Appendix respectively.Specifically, we first modify the input question x and its multiple reasoning paths z i into a prompt s.Prompted text s is then fed into language models and generate subsequent sentence v.We can use any decoding strategy, but we use greedy decoding in this stage for the simplicity.</p>
<p>Experiments</p>
<p>We carry out a series of experiments to confirm the efficacy of our method on three scenarios.Our findings indicate that across a wide range of tasks, scenarios, models, and prompting methods, selfagreement generally enhances the reasoning performance of language models.We introduce experimental setup in §4.1, main results in §4.2, and analysis in §4.3.See Appendix A and B for more experimental details and additional experiments.</p>
<p>Experimental Setup</p>
<p>Tasks and Datasets We evaluate self-agreement on six public reasoning benchmarks for a fair com- Table 2: Self-agreement, when applied on GPT-3.5-turbo.† models are not publicly accessible, and the numbers are obtained from their papers.Our performance gain (+blue) is computed over few-shot CoT (Wei et al., 2022b), which is our primary baseline.Our methods substantially increase the performance over Wei et al. (2022b), with an average +4.0 gain on GPT-3.5-turbo.The previous finetuing SOTA baselines are obtained from: a: GPT-3 175B finetuned plus an additional 175B verifier (Cobbe et al., 2021), b: Relevance and LCA operation classifier (Roy and Roth, 2015), c: (Pi et al., 2022), d: DeBERTaV3-large + KEAR (Xu et al., 2021), e: UnifiedQA-FT (Khashabi et al., 2020).The best performance for each task is shown in bold.</p>
<p>parison with existing methods. 6These benchmarks can be divided into three categories of reasoning tasks: (i) Arithmetic Reasoning For these tasks, we use GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), and MultiArith (Roy and Roth, 2016); (ii) Commonsense Reasoning We evaluate two commonsense reasoning tasks: CommonseQA (CSQA) (Talmor et al., 2018) and the AI2 Reasoning Challenge (ARC) (Clark et al., 2018).The ARC dataset is divided into two sets: a challenge set (denoted as ARC-c), and an easy set (denoted as ARCe).We evaluate the effectiveness of our method on ARC-c; (iii) Symbolic Reasoning We choose last letter concatenation (e.g., the input is "Elon Musk" and the output should be "nk") from Wei et al. (2022b).To simulate the first and second scenarios, we select 200 examples from each reasoning task randomly and then mix them, since the cost of ask k times stage is heavily expensive.Language Models We evaluate self-agreement over two transformer-based language models on the first and second scenarios, and four transformerbased language models on the third scenario.For the first and second scenarios, we consider the following language models: (i) GPT-3.5-turbo.We use the public gpt-3.5-turboversion of GPT-3.5 from OpenAI API;7 (ii) Llama-2-13B-Chat (Tou-vron et al., 2023) with 13-billion parameters, optimized for dialogue use cases via alignment techniques.Llama-2-Chat models are completely opensourced8 and have similar performance compared with GPT-3 (Brown et al., 2020).For the third scenario, we consider the following language models: (i) LaMDA-137B (Thoppilan et al., 2022) with 137-billion parameters, pre-trained on a mixture of web documents, dialog data and Wikipedia; (ii) PaLM-540B (Chowdhery et al., 2022) with 540-billion parameters, pre-trained on a high quality corpus of 780 billion tokens; (iii) Minerva-540B (Lewkowycz et al., 2022) with 540-billion parameters, pretrained on general natural language data and further trained on technical content; (iv) GPT-3.5-turbo.Baselines As shown in Figure 2, there are three major scenarios covering almost all possible situations.In the first scenario, we compare self-agreement with 2 baselines: (i) Zero-Shot CoT (Kojima et al., 2022); (ii) Self-Consistency (Wang et al., 2022).In the second scenario, we compare self-agreement with 3 baselines: (i) Mixed-Few-Shot CoT.To adapt few-shot CoT to such scenario, we randomly collects one demonstration from each reasoning task used in Wei et al. (2022b) and then leverage the mixed demonstrations for all input questions.(ii) Self-Consistency (Wang et al., 2022); (iii) Multi- Last Letters T=0.5, no top k T=0.7, no top k T=0.9, no top k p=0.9 p=0.95 Greedy Decode</p>
<p>Figure 4: Self-agreement significantly improves accuracy over zero-shot CoT with greedy decoding (grey) on the first scenario across arithmetic, commonsense and symbolic reasoning tasks, over GPT-3.5-turbo.Sampling a higher number of diverse reasoning paths consistently improves reasoning accuracy.In addition, self-agreement is robust to reasoning paths with various diversity across arithmetic, commonsense and symbolic reasoning tasks.</p>
<p>Agents (Debate) (Du et al., 2023).In the third scenario, we compare self-agreement with 3 baselines: (i) Few-Shot CoT (Wei et al., 2022b); (ii) Self-Consistency (Wang et al., 2022); (iii) USC (Chen et al., 2023b).</p>
<p>Sampling Schemes To sample diverse reasoning paths in the ask k times stage, we follow the same settings of Wang et al. (2022).In particular, we apply temperature sampling with T = 0.5 and truncated at the top-k (k = 40) tokens with the highest probability for Llama-2-Chat models with various sizes.For GPT-3.5-turbo, we use T = 0.7 without top-k truncation.</p>
<p>Evaluation Metrics For all datasets, we use accuracy to evaluate the model's reasoning performance.</p>
<p>Main Results</p>
<p>We report the results of self-agreement averaged over 5 runs, where we sample 20 outputs from the model in each run.</p>
<p>The results on the first scenario with six datasets from three categories are shown in Table 1.Selfagreement improves the reasoning performance in all reasoning tasks across all two language models over zero-shot CoT without requiring the answer formats to be similar, while self-consistency is not applicable in this scenario as the reasoning paths have no trigger (i.e., The answer is).More surprisingly, the gains brought by our method are almost the same across different language models' scales, further demonstrating the general effectiveness of our method.For example, we see +5.8% average absolute accuracy improvement over Llama-2-13B-Chat and +5.3% for GPT-3.5-turbowhen employing self-agreement.</p>
<p>We can also discern a generally consistent performance trend in the second scenario, mirroring that of the first scenario.Self-agreement improves the reasoning performance (average absolute accuracy +5.5% for GPT-3.5-turbo and +6.2% for Llama-2-13B-Chat) across all two language models over mixed-few-shot CoT, while self-consistency performs even worse (average absolute accuracy -1.2% for GPT-3.5-turbo and -0.5% for Llama-2-13B-Chat) than mixed-few-shot CoT.Because determining whether two strings represent the same final answer is not applicable for it when the type of input questions is unknown.We provide further analysis in Appendix B.3.It can also be seen that multi-agents (debate) attains more competitive performance compared with zero-shot CoT when using GPT-3.5-turboas agents.We qualitatively find that it is more difficult for models of small scale to arrive at the correct answer as the debate progresses when facing cases where all the agents initially make incorrect predictions (see Table 15 in the Appendix).The underlying reason is that the process of debating is an emergent ability of model scale (Wei et al., 2022a).That is, stronger models can better understand the debating rules and refine other agents' incorrect outputs for further improvement.</p>
<p>Table 2 summarizes accuracy of our method, few-shot CoT, self-consistency and USC for each dataset.Similarly, self-agreement yields large gains over few-shot CoT for all reasoning tasks.Meanwhile, self-agreement achieves almost the same superior performance as self-consistency across almost all reasoning tasks, which it does not need answer parsing to perform the voting.On the contrary, the performance of USC is far infe- rior to self-consistency and self-agreement, indicating that self-agreement has better expandability and can really approach the performance ceiling of self-consistency while USC cannot.This is also demonstrated in Chen et al. (2023b).We consider there is an inescapable gap between performance and generalization.We provide detailed discussion with USC in Appendix C. We also find that selfagreement has larger performance gains for morecomplicated problems.For instance, in Arithmetic Reasoning, for GSM8K (the dataset with the lowest baseline performance), the performance gain (+12.4) is more than tripled for other tasks.This is analogous to the intuition self-agreement leveraged that complex reasoning tasks typically admit multiple reasoning paths that reach a correct answer.</p>
<p>The more that deliberate thinking and analysis is required for a problem, the greater the diversity of reasoning paths that can recover the answer.</p>
<p>Analysis</p>
<p>We conduct a number of additional experiments to analyze different aspects of our approach.For all experiments, we use GPT-3.5-turbo, and conduct experiments on the first scenario as it is the most relevant to the real-world scenario, unless otherwise specified.We only mix GSM8K, Com-monsenseQA and Last Letters selected from each category to simulate the first scenario.</p>
<p>Effects of the Number of Sampled Reasoning</p>
<p>Paths We argue that the most agreed answer selected from multiple reasoning paths is likely to be the correct answer.So, we are curios about the effect of the number of sampled reasoning paths.As shown in Figure 4, we observe that the performance first increases sharply and then slows down as the number of sampled reasoning paths increases.This is why we sample 20 outputs for all scenarios and tasks.In addition, sampling a higher number of reasoning paths leads to a consistently better performance, but performance nearly converges at the number of 5 or 10.This is consistent with our state-ments that assessing whether or not a person really knows how to solve a question should depend on a small number of reasoning paths generated by themself, since a person might acquire the wrong answer due to carelessness or other factors.</p>
<p>Self-Agreement is Robust to Reasoning Paths with Different Diversity and Scaling Sampling strategies with different hyperparameters denote generated reasoning paths with various diversity.So, we are curios about whether self-agreement is robust to reasoning paths with various diversity.</p>
<p>To show the effect of reasoning paths with various diversity, we conduct the experiment by varying T in temperature sampling (Ficler and Goldberg, 2017) and p in nucleus sampling (Holtzman et al., 2019), over GPT-3.5-turbo.As shown in Figure 4, we can see that self-agreement gradually improves reasoning performance regardless of the diversity of reasoning paths as the number of reasoning paths increases.It also suggests that self-agreement is robust to reasoning paths with various diversity.</p>
<p>Figure 5 shows that self-agreement robustly improves performance in GSM8K, CommonsenseQA and Last Letters across all scales for the Llama-2-Chat model series (i.e., 7B, 13B and 70B).It is worth noting that the gain when used with Llama-2-7B-Chat is relatively low due to the fact that certain abilities only emerge when the model reaches a sufficient scale (Brown et al., 2020).</p>
<p>Effects of Different Self-Agreement Prompts We further carry out experiments to examine the influence of the prompt used in ask one more time stage of self-agreement.Specifically, we modify the original prompt by first translating it into another language and then back again to construct the modified prompt.The original and modified prompts are given in Table 11 and 13 in the Appendix.As shown in Table 3, we can observe self-agreement (w/ modified prompt) achieves almost the same performance, indicating that self-agreement is robust to the prompts with the same meaning.</p>
<p>In this work, we explore a more realistic setting with significant application values in ensembleoptimization studies, as shown in Figure 2. To this end, we propose self-agreement, a simple yet generalizable ensemble-optimization method applying in almost all scenarios.Self-agreement simultaneously achieves remarkable performances on six public reasoning benchmarks and superior generalization capabilities.Our findings encourage the research community to focus on a deeper understanding of the role of language model itself in ensemble-optimization studies, which has favorable generalization abilities and superior performance.We hope this work will open new research possibilities in prompting, language models, ensemble-optimization and multi-step reasoning.</p>
<p>Limitations</p>
<p>There are two limitations of our approach.First, the ask k times stage of self-agreement incurs more computational cost.We suggest people can try a small number of paths (e.g., 5 or 10) to achieve most of the performance gains while not bringing too much cost, since we have shown that performance nearly converges at the number of 5 or 10.Besides, there are some work investigating how to enable the LLM to run inference in batches (Cheng et al., 2023;Lin et al., 2023).As part of future work, one could mix different questions as a batch, and run inference in a batch one time.After that, self-agreement requires only two inferences, thus significantly decreasing computational cost.Second, in the ask one more time stage, we modify the input question x and its multiple reasoning paths z i into a prompt s.This may cause the length of a prompt s exceeds the maximum context length of language models.For example, the maximum context length of Llama-2-Chat models and GPT-3.5-turbo is 4,096 and 8,192 respectively.In practice, as shown in Figure 4, performance nearly converges at the number of 5 or 10.So, the maximum context length of language models is likely to be enough for self-agreement in most situations.</p>
<p>Meanwhile, there are also some work focusing on how to increase the context length of language models (Li et al., 2023a;Xiong et al., 2023).Therefore, in the future, the limitations of the context length will become smaller and smaller.</p>
<p>A Experimental Details</p>
<p>A.1 Tasks and Datasets</p>
<p>We evaluate self-agreement on six public reasoning benchmarks for a fair comparison with existing methods, covering arithmetic, commonsense and symbolic reasoning tasks.The detailed statistics of the datasets are shown in Table 4.</p>
<p>A.2 Baseline Methods</p>
<p>We introduce the baseline methods in detail.</p>
<p>Few-Shot CoT Few-Shot CoT (Wei et al., 2022b) employs several additional templated demonstrations as: "Q: q.A: (r, a)." before the input question, where q, r and a are manually crafted questions, rationales and final answers.</p>
<p>Zero-Shot CoT Zero-Shot CoT (Kojima et al., 2022) simply inserts the prompt "Let's think step by step" after the input question.</p>
<p>Self-Consistency Self-Consistency (Wang et al., 2022) first samples multiple reasoning paths by using few-shot CoT, and then selects the most consistent answer by marginalizing out the sampled reasoning paths.</p>
<p>Multi-Agents (Debate) Multi-Agents (Debate) (Du et al., 2023) takes language models as multi-agents to propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer.</p>
<p>USC USC (Chen et al., 2023b) leverages LLMs themselves to select the most consistent answer among multiple candidates.</p>
<p>A.3 Implementation Details</p>
<p>For all language models we evaluated, we perform prompting-based inference only.For GPT-3.5-turbo, we use the public version gpt-3.5-turbo of GPT-3.5 from OpenAI API.For Llama-2-Chat models, we use the open-sourced library, and run experiments on NVIDIA Tesla A800 (8x8 configuration, 80G).</p>
<p>Model</p>
<p>Arithmetic Commonsense Symbolic</p>
<p>GSM8K CSQA Letter</p>
<p>Both the type of questions and the answer format are unknown (the first scenario)</p>
<p>GPT-3.5-turbo Table 5: Self-agreement, when applied on GPT-3.5turbo.Our performance gain (+blue) is computed over the zero-shot CoT (Kojima et al., 2022), which is our primary baseline.Our method substantially increases the performance over zero-shot CoT.</p>
<p>For GPT-3.5-turbo, we follow the same experimental settings of Kojima et al. (2022) and set 128, 256 max tokens for ask k times and ask one more time phases respectively, without frequency penalty or presence penalty.For Llama-2-Chat models, we set 128 max tokens for ask k times without frequency penalty or presence penalty.In the ask one more time stage, we continue to generate tokens until the stop token (e.g., "</s>") is generated.We use greedy decoding across all the models in ask one more time phase.Our implementation is in PyTorch (Paszke et al., 2019).</p>
<p>Here, we describe the implementation details of baseline methods in different scenarios.In the first scenario, to adapt zero-shot CoT to such scenario, we use the general prompt (i.e., The answer is) for answer extraction.For all ensemble-optimization methods, we employ zero-shot CoT (Kojima et al., 2022) without 2nd answer extraction phase to simulate the reasoning paths with no trigger.Selfconsistency is not applicable in this scenario as the reasoning paths have no trigger.In the second scenario, to adapt few-shot CoT to such scenario, we randomly select one demonstration from each reasoning task used in Wei et al. (2022b) and then leverage the mixed demonstrations for all input questions (called mixed-few-shot CoT).The demonstrations are given in Table 14, and other experimental settings are the same as Wei et al. (2022b).To adapt self-consistency to such scenario, we extract the strings between the trigger (i.e., "The answer is") and full stop (i.e., ".") of the reasoning paths.For example, as shown in Figure 2, we obtain 6 hours extracted from the output 2. To adapt multi-agents (debate) (Du et al., 2023) to such scenario, we modify the prompt used in Du et al. (2023) for GSM8K to make it applying in almost all scenarios, since Du et al. (2023) have designed a specific prompt for each task, which is not a generalizable method.The modified prompt is given in Table 12.Other experimental settings are the same as Du et al. (2023).In the third scenario, we use the same prompts and experiment settings used in Wei et al. (2022b) to employ fewshot CoT, the same experimental settings of Wang et al. (2022) to employ self-consistency, and the same experimental settings of Chen et al. (2023b) to employ USC.</p>
<p>B Additional Experiments B.1 Are 200 Randomly Selected Examples</p>
<p>Enough?</p>
<p>As mentioned in Section 4.1, to be economical, we only select 200 examples from each public reasoning benchmark randomly and then mix them to simulate situations of the unknown type of input questions.To further validate the soundness of experimental results in the first scenario provided in Table 1, we conduct experiments on GSM8K, Com-monsenseQA and Last Letters by varying the number of randomly selected examples (i.e., 200, 400 and 600).9As shown in Table 5, the performance fluctuates slightly with different number of randomly selected examples from each reasoning task over zero-shot CoT and self-agreement.Overall speaking, self-agreement improves the reasoning performance in all reasoning tasks over zero-shot CoT.</p>
<p>B.2 Further Analysis on Self-Agreement vs.</p>
<p>Self-Consistency</p>
<p>To have an entirely fair comparison with selfconsistency, we conduct experiments on 20 and 40 sampled reasoning paths respectively.As shown in Table 6, it is interesting to note that when the number of sampled reasoning paths is 20, selfagreement achieves an average performance increase of 0.2 over all reasoning tasks compared to self-consistency, but it instead decreases by 0.3 when the number of sampled reasoning paths is increased to 40.To investigate this influence, we conduct experiments on GSM8K and Common-seQA with self-consistency and self-agreement by varying the number of sampled reasoning paths.Figure 6: Self-agreement utilizes sampled reasoning paths more efficiently than self-consistency when the number of sampled reasoning paths is less than 20.Sampling a enough high number of reasoning paths achieves essentially the same performance for self-consistency and self-agreement.</p>
<p>As shown in Figure 6, self-agreement utilizes sampled reasoning paths more efficiently than selfconsistency when the number of sampled reasoning paths is less than 20.Sampling a enough high number of reasoning paths achieves essentially the same performance for self-consistency and self-agreement.The underlying reason is that self-agreement is a generalizable answer extractor while self-consistency is a rule-based answer extractor, i.e., only extract the reasoning paths with trigger (e.g., The answer is).This causes it to drop reasoning paths without trigger.</p>
<p>B.3 Why Self-Consistency Perform Worse in the Second Scenario?</p>
<p>To better figure out this question, we need to review the theory and implementations of selfconsistency (Wang et al., 2022).In detail, given a question, self-consistency first sample from the language model's decoder to generate diverse multiple reasoning paths, extract the final answers of sampled reasoning paths to form a answer set a i , and then choose the answer with the most occurrences.The overall process is outlined in Alogorithm 1.It is clear that the most important step for the self-consistency method is how to extract the answers of reasoning paths exactly, while the answer_cleaning function is to perform different extraction rules according to different types of input questions.For instance, we extract arabic numerals for GSM8K, but (A -E) for CommonseQA.</p>
<p>To adapt self-consistency to this scenario, we extract the strings between the trigger (e.g., "The answer is") and full stop (e.g., ".") as the final answers.This is why self-consistency even perform worse than the baseline mixed-few-shot CoT in this scenario.Because determining whether two strings represent the same final answer is not applicable for it when the type of input questions is unknown.99.0 (-0.2) 86.2 (-0.3) 79.4 (+0.0) 86.9 (-0.2) 81.5 (-0.9)</p>
<p>Table 6: Self-agreement, when applied on GPT-3.5-turbo.Our performance gain (+blue) is computed over selfconsistency (Wang et al., 2022).The best performance for each task is shown in bold.</p>
<p>Algorithm 1 Pseudocode of Self-Consistency in a Python-like style.</p>
<h1>answer_cleaning: extract the final answer from a reasoning path to a question that belongs to a known task.We adopt the code in https://github.com/kojima-takeshi188/zero_shot_cot/blob/main/utils.py# questions: questions # groundtruth_answers: groundtruth answers to each question # diverse_answers: multiple sampled reasoning paths to each question # maj_ans: return the string with the most occurrences # judge_ans: return True if two answers are the same or False correct_counts = 0 # the number of correct answers for i in range(len(questions)): pred_lists = [] # collect the final answer extracted from each reasoning path groundtruth_answer = groundtruth_answers[i] for j in range(len(diverse_answers[i])): pred = answer_cleaning(diverse_answers[i][j]) # extract the answer of j reasoning path to i question if pred: pred_lists.append(pred)optimal_answer = maj_ans(pred_lists) # return the string with the most occurrences in pred_list if judge_ans(groundtruth_answer, optimal_answer):</h1>
<p>correct_counts += 1 print(correct_counts / len(questions))</p>
<p>For instance, 6 hours and 6 extracted from outputs 2, 3 of Figure 2 respectively represent two different answers when employing self-consistency in this scenario.</p>
<p>B.4 Further Exploration of Self-Agreement on Open-ended Reasoning Task</p>
<p>As shown in Figure 2, we only categorize the types of questions into known and unknown from a specific angle, i.e., whether we can identify in advance that the question belongs to a particular task.However, we wonder if this is really the case in realworld scenarios.In practice, there exists another angle of its division.In detail, a reasoning path to a question has multiple different sub-answers consisting of the final answer.For instance, a question "Axel has 50 silver pesos and 80 gold pesos ..., How many pesos does Anna have?What's the total number of pesos they have together?"and its groundtruth answer "Anna has 140 pesos and they have a total of 270 pesos together.its groundtruth answer "The answer is A and B.". Sadly, as far as we know, there is no such publicly available reasoning dataset to test the effectiveness of our proposed self-agreement.We guess that there are two reasons.First, constructing such a dataset is more labor intensive.Second, it is difficult to assess the accuracy of the final answers when solving such questions and most likely requires human evaluation.Additionally, we consider that our method still can achieve substantially better performance over baselines when encountering this type of input questions.To answer this, we explain the nature of most ensemble-optimization work and selfagreement in terms of definitions and experiments.</p>
<p>Model</p>
<p>Arithmetic Commonsense</p>
<p>GSM8K CSQA</p>
<p>Both the type of questions and the answer format are known (the third scenario)   (Wei et al., 2022b), which is our primary baseline.Our methods substantially increase the performance over Wei et al. (2022b).</p>
<p>From the perspective of definitions, we view most ensemble-optimization work as different implementations of the majority voted idea. 10For example, self-consistency (Wang et al., 2022) takes the majority vote over sampled reasoning paths, while multi-agents (debate) (Du et al., 2023) obtains the common final answer over multiple agents' outputs.It is clear that the essence of them is based on the majority voted design, and our work is in lines with it, but we explore how to allow language model itself to achieve the overall procedure.So, the main difference between them lies in two major steps consisting of the process of the majority vote, i.e., answer extraction and answer comparison.To be specific, self-consistency and multiagents (debate) are rule-based and not generalizable method to extract and compare answers.However, self-agreement is an entirely generalizable method, whether extracting answers or comparing answers.</p>
<p>From the perspective of experiments, in order to promote the development of this field and test the general effectiveness of our method, we build an open-ended arithmetic dataset, GSM8K-Multi, based on GSM8K (Cobbe et al., 2021), where a reasoning path to a question has two different subanswers consisting of the final answer.The reason for building the dataset in the arithmetic category is that GSM8K is a more complicated dataset than others (the dataset with the lowest baseline performance) to better measure the effectiveness of different methods.To build this dataset, we only select 500 examples randomly from GSM8K to save labor costs.We then hire crowd-sourced workers who are Chinese-English bilingual speakers with enough mathematical knowledge to manually determine whether the math question can be rewrit-10 Details refer to corresponding papers.</p>
<p>Model Arithmetic</p>
<p>GSM8K SVAMP</p>
<p>Both the type of questions and the answer format are known (the third scenario)   (Wei et al., 2022b) or Complex CoT (Fu et al., 2022), which is our primary baseline.Our methods substantially increase the performance over Wei et al. (2022b) and (Fu et al., 2022).</p>
<p>ten into the type we need.11If possible, directly rewrite the math question and ensure coherence between two different problems.If not, discard it.</p>
<p>Table 10 provides examples of original questions and rephrased questions of GSM8K.The corresponding solutions are also rewritten by them to guarantee the accuracy of answers.We finally obtain 362 examples as the test set of GSM8K-Multi.</p>
<p>The entire dataset will be released soon.Thus, to validate the statement that self-agreement still can achieve substantially better performance over baselines, we follow the same experimental settings used in the first scenario and conduct experiments on the GSM8K-Multi dataset.As shown in Table 7, overall speaking, self-agreement achieves better performance over all strong baselines on GSM8K-Multi, demonstrating that it is entirely generalizable method, whether extracting answers or comparing answers. 12n the future, we plan to design an automated evaluation method to save labor costs.As far as we know, there are some work to investigate how to use ChatGPT or GPT-4 as an automated evaluation metric for various NLP tasks (Wang et al., 2023).</p>
<p>B.5 Experiments with More Language Models</p>
<p>To further demonstrate the general effectiveness of self-agreement with more language models, we conduct experiments on GSM8K and CSQA with Llama-2-70B-Chat and GPT-4 in the third scenario.The reason we choose GSM8K and CSQA is that they are more complicated than others (the dataset with the lowest baseline performance).As shown in Table 8, self-agreement consistently improves over the few-shot CoT, and the performance is generally comparable to the standard self-consistency, which self-agreement does not need answer parsing to perform the voting.</p>
<p>B.6 Experiments with More CoT Prompting Methods</p>
<p>To  (Cobbe et al., 2021), where a reasoning path to a question has two different subanswers consisting of the final answer, resulting in a more free-form and complicated answer formats.However, they only test the effectiveness of their method on two public fix-ended reasoning tasks.Fourthly, the prompt used in this work is different from that used in USC.We design a more domain-specific prompt to select the optimal response, which can further boost USC.This is also demonstrated in Chen et al. (2023b).Fifthly, selfagreement achieves better performance than USC, indicating that self-agreement has better expandability and can really approach the performance ceiling of self-consistency while USC cannot.To be specific, for self-agreement, sampling a higher number of reasoning paths leads to a consistently better performance, while the opposite is true for USC.So, we strongly question the statements in this paper, i.e., the performance is generally comparable to the standard self-consistency, which USC does not need answer parsing to perform the voting.</p>
<p>It is unfair to compare with self-consistency when the number of sampled reasoning paths is 8 rather than 40 (experimental settings in self-consistency paper).As shown in Table 6, it can be seen that selfagreement matches the standard self-consistency performance without requiring the answer formats to be similar, however, the performance of USC is far inferior to self-consistency and self-agreement.Also, sampling a higher number of reasoning paths leads to a consistently better performance, while the opposite is true for USC.Therefore, we consider there is an inescapable gap between performance and generalization.We focus on how to solve open-ended reasoning tasks, not open-ended generation tasks.Because the ability of LLMs to challenging tasks (e.g., arithmetic, commonsense and symbolic reasoning) is often seen as a limitation, which is difficult to be overcome solely by scaling up the size of LLMs.</p>
<p>D Case Study</p>
<p>We also provide examples generated by selfagreement for different reasoning tasks in the first scenario, when applied on 17,18,19,20,21 represent corresponding reasoning tasks.We show self-agreement can extract answers from reasoning paths to different type of input questions, compare different answers and then generate the majority voted answer.</p>
<p>E Broader Impacts</p>
<p>This work investigates the critical role of language model itself in the ensemble-optimization studies.</p>
<p>Self-agreement allows language model itself to extract final answers of multiple reasoning paths, count the votes of different answers, and then select the majority voted answer.The overall procedure can be achieved by prompting language model itself one more time.</p>
<p>Self-agreement first explores an interesting direction, i.e., open-ended reasoning tasks.It takes an important step forward in ensemble-optimization studies where most of them are focused on fixended reasoning tasks.However, in principle the idea of this method can be extended to any openended generation tasks, not just open-ended reasoning tasks if a good metric can be well defined to compare multiple generations, where the good metric or overall procedure can be achieved by prompting language model itself one more time.</p>
<p>Original Question: Adrien's total salary was 30 percent higher than Lylah's.Four years later, his salary had increased, and he was earning 40% more than what he was making four years ago.If Adrien's and Lylah's salary increased simultaneously, and Adrien earned $40000 four years ago, calculate the total salary the two were receiving four years later?Rephrased Question: Adrien's total salary was 30 percent higher than Lylah's.Four years later, his salary had increased, and he was earning 40% more than what he was making four years ago.If Adrien's and Lylah's salary increased simultaneously, and Adrien earned $40000 four years ago, please calculate the total salary and salary difference for both of them after four years?</p>
<p>Original Question: John buys 2 pairs of shoes for each of his 3 children.They cost $60 each.How much did he pay?Rephrased Question: John buys 2 pairs of shoes for each of his 3 children.They cost $60 each.How much does each child's shoes cost?How much did John pay in total?</p>
<p>Table 10: Examples of original questions and rephrased questions of GSM8K (Cobbe et al., 2021).</p>
<p>ORIGINAL PROMPT FOR REASONING TASKS</p>
<p>When making a majority vote on multiple corresponding solutions while solely relying on the final answer of each solution, follow these detailed steps:</p>
<p>1.Understand the Question: Begin by thoroughly understanding the question, including its requirements, given conditions, and objectives.This is essential for evaluating each solution.</p>
<p>2.Extract the final answer for each solution: Extract the final answer for each solution, and then organize them in a clear list for ease of comparison.</p>
<p>3.Count the Votes: Tally the number of times each final answer appears to determine which one received the majority of votes.</p>
<p>4.Select the Final Answer: The final answer that received the most votes is chosen as the majority choice.In the case of a tie, you can either choose one of the tied answers.</p>
<p>Below is a question and several candidate solutions or one candidate solution.By considering all these steps above, you can make a majority vote on several corresponding solutions, and then write "The majority voted answer is" in the last line.The majority voted answer should be one of the final answer sets extracted from several candidate solutions, not a certain solution.</p>
<p>Table 11: Original prompt used in the ask one more time stage of self-agreement method.</p>
<p>Task</p>
<p>Type Prompt</p>
<p>General Starting</p>
<p>Can you solve the following problem?<Problem> Explain your reasoning.Your final answer should be in the form \boxed{{answer}}, at the end of your response.</p>
<p>Debate</p>
<p>These are the solutions to the problem from other agents: <other agent responses> Using the solutions from other agents as additional information, can you provide your answer to the problem?The original problem is <Problem>.Your final answer should be in the form \boxed{{answer}}, at the end of your response.</p>
<p>runs 12 miles a day for 5 days a week.If he runs 10 miles an hour how many hours does he run a week?GSM8K Q: What island country is ferret popular?Answer Choices: (A) own home (B) north carolina (C) great britain (D) hutch (E) outdoors CSQA</p>
<p>Figure 1 :
1
Figure 1: A simple example to illustrate the main limitations of existing ensemble-optimization methods.Both verifier or re-ranker based methods and post-processing based methods can only solve the question that belongs to a known task, yet fail to generalize to situations of the unknown type of questions.</p>
<p>First</p>
<p>Figure 2: (a), (b) and (c) represent three major different cases covering almost all possible situations.The first scenario (a) is that both the type of question and answer format are unknown, and the second one (b) is that the type of question is unknown while the answer format is known.The third one (c) is that the type of question and answer format are already known (i.e., the type of questions belongs to GSM8K(Cobbe et al., 2021), and the answer has the trigger (e.g., The answer is)).</p>
<p>Figure 5 :
5
Figure 5: Self-agreement improves performance across different language model scales.</p>
<p>Both the type of questions and the answer format are unknown (the first scenario)The type of questions is unknown and the answer format is known (the second scenario)
ModelArithmeticCommonsenseSymbolicAvgGSM8KMultiArithSVAMPCSQAARC-cLetterZero-Shot CoT48.593.073.571.081.581.074.8GPT-3.5-turboSelf-ConsistencyN/AN/AN/AN/AN/AN/AN/ASelf-Agreement58.5 (+10.0)93.4 (+0.4)79.6 (+6.1) 77.8 (+6.8) 87.7 (+6.2)83.8 (+2.8)80.1 (+5.3)Zero-Shot CoT26.077.550.557.566.531.051.5Llama-2-13B-ChatSelf-ConsistencyN/AN/AN/AN/AN/AN/AN/ASelf-Agreement31.7 (+5.7)77.8 (+0.3)56.3 (+5.8) 65.0 (+7.5) 68.5 (+2.0) 44.5 (+13.5) 57.3 (+5.8)Mixed-Few-Shot CoT69.097.576.571.086.083.080.5GPT-3.5-turboSelf-Consistency Multi-Agents (Debate)68.0 80.298.1 96.076.6 76.271.2 69.083.1 78.479.1 61.379.3 76.9Self-Agreement81.3 (+12.3)99.1 (+2.6)83.7 (+7.2) 75.4 (+4.4) 87.8 (+1.8)88.9 (+6.9)86.0 (+5.5)Mixed-Few-Shot CoT31.584.061.066.568.019.055.0Llama-2-13B-ChatSelf-Consistency Multi-Agents (Debate)32.8 44.083.9 79.056.7 54.068.0 52.068.1 56.516.0 27.054.5 52.1Self-Agreement41.5 (+10.0) 94.7 (+10.7) 66.8 (+5.8) 70.2 (+3.7) 70.6 (+2.6)23.1 (+4.1)61.2 (+6.2)</p>
<p>Both the type of questions and the answer format are known (the third scenario)
ModelArithmeticCommonsenseSymbolicGSM8KMultiArithSVAMPCSQAARC-cLetterPrevious finetuning SOTA55.0 a60.5 b57.4 c91.2 d75.0 eN/ALaMDA  †Few-Shot CoT Self-Consistency17.1 27.751.8 75.738.9 53.357.9 63.155.1 59.88.2 8.2PaLM  †Few-Shot CoT Self-Consistency56.5 74.494.7 99.379.0 86.679.0 80.785.2 88.765.8 70.8Minerva  †Few-Shot CoT Self-Consistency58.8 78.5----------Few-Shot CoT70.098.282.074.585.480.6GPT-3.5-turboSelf-Consistency USC80.3 76.899.2 98.285.9 83.579.0 48.987.0 73.981.9 79.4Self-Agreement 82.4 (+12.4) 99.0 (+0.8) 86.0 (+4.0) 79.4 (+4.9) 86.8 (+1.4) 81.0 (+0.4)</p>
<p>Table 3 :
3
Self-agreement works with different prompts.
ModelGSM8K CSQA LetterZero-Shot CoT48.571.081.0GPT-3.5-turboSelf-Consistency Self-Agreement (w/ original prompt)N/A 58.5N/A 77.8N/A 83.8Self-Agreement (w/ modified prompt)56.777.583.4</p>
<p>Table 4 :
4
Detailed dataset description.</p>
<p>Both the type of questions and the answer format are known (the third scenario)
ModelArithmeticCommonsenseSymbolicGSM8KMultiArithSVAMPCSQAARC-cLetter20 sampled reasoning pathsSelf-Consistency80.399.285.979.087.081.9USC76.898.283.548.973.979.4Self-Agreement 82.4 (+2.1)99.0 (-0.2)86.0 (+0.1) 79.4 (+0.4) 86.8 (-0.2) 81.0 (-0.9)GPT-3.5-turbo40 sampled reasoning pathsSelf-Consistency81.999.286.579.487.182.4USC76.098.782.463.779.173.8Self-Agreement 82.7 (+0.8)</p>
<p>Table 8 :
8
Our performance gain (+blue) is computed over few-shot CoT</p>
<p>Table 9 :
9
Our performance gain (+blue) is computed over few-shot CoT</p>
<p>Table 12 :
12
General prompts are used for each task.</p>
<p>For example, in temperature sampling strategies(Ficler and Goldberg, 2017), higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
Nearing completion of this work, we find that Zou et al. (2023) have applied CoT prompting to mixed-task scenarios to simulate the real-world applications, which share the same idea with us while we propose it in ensemble-optimization studies.
We use the test split for all tasks if the labels are available for evaluation. For CommonsenseQA, we use the dev split.
https://openai.com/blog/openai-api
Model checkpoints and inference code are available at https://github.com/facebookresearch/llama.
Note that the test set of Last Letters dataset only contains 500 examples. Thus, we use all examples when randomly selected 600 examples from each reasoning task.
We follow two principles to rephrase the question. First, keep the difficulty of the original question as much as possible. Second, add a new problem under the original conditions and it does not conflict with the original problem.
We hire the same crowd-sourced workers to manually evaluate the accuracy of the answers. We follow two principles for the fair and strict evaluation. First, only the accuracy of the final answer instead of the reasoning path is evaluated to save labor costs. Second, the solution is correct only if both sub-answers consisting of the final answer are correct.
Calibrate before use: Improving few-shot performance of language models.In International Conference on Machine Learning, pages 12697-12706.PMLR.MODIFIED PROMPT FOR REASONING TASKSWhen majority voting on multiple corresponding solutions and relying only on the final answer for each solution, follow the steps detailed below:1.Understanding the question: The first step is to thoroughly understand the question, including its requirements, given conditions and objectives.This is essential for evaluating each solution.2.Extract the final answer for each solution: Extract the final answer for each solution and then organize them in a clear list for easy comparison.3.Counting of votes: The number of times each final answer appears is counted to determine which answer receives the majority of votes.4.Choose the final answer: The final answer that receives the most votes is chosen as the majority choice.In case of a tie, you can choose one of the tied answers.Below is a question and several candidate solutions or one candidate solution.By considering all the steps above, you can take a majority vote on several corresponding solutions and then write "The majority voted answer is" in the last line.The majority voted answer should be one of the final answer sets extracted from several candidate solutions, not a certain solution.Table13: Modified prompt used in the ask one more time stage of self-agreement method.Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: There are 15 trees originally.Then there were 21 trees after some more were planted.So there must have been 21 -15 = 6.The answer is 6.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: There are originally 3 cars.2 more cars arrive.3 + 2 = 5.The answer is 5.Q: Leah had 32 chocolates and her sister had 42.If they ate 35, how many pieces do they have left in total?A: Originally, Leah had 32 chocolates.Her sister had 42.So in total they had 32 + 42 = 74.After eating 35, they had 74 -35 = 39.The answer is 39.Q: What do people use to absorb extra ink from a fountain pen?Answer Choices: (A) shirt pocket (B) calligrapher's hand (C) inkwell (D) desk drawer (E) blotterA: The answer must be an item that can absorb ink.Of the above choices, only blotters are used to absorb ink.The answer is E. A: Dry surfaces will more likely cause more friction via rubbing than other smoother surfaces, hence dry palms will produce the most heat.The answer is A.Q: Take the last letters of the words in "Elon Musk" and concatenate them.A:The last letter of "Elon" is "n".The last letter of "Musk" is "k".Concatenating them is "nk".The answer is nk.Q: Jame will turn 27 in 5 years.In 8 years his cousin will be 5 years younger than twice his age.How many years separate the age of the two now?Round 1Agent 1: Sure, I'd be happy to help!Let's start by analyzing the information given: * Jame will turn 27 in 5 years.<em> In 8 years, his cousin will be 5 years younger than twice his age.We can start by finding out how old Jame will be in 8 years: Jame's age in 8 years = 27 + 8 = 35 Now, let's find out how old his cousin will be in 8 years: Cousin's age in 8 years = Jame's age in 8 years -5 = 35 -5 = 30 Since his cousin will be 5 years younger than twice Jame's age, we can find out Jame's current age by subtracting 5 from twice his age: Twice Jame's age = 2 x 27 = 54 Current age = 54 -5 = 49 So, Jame is currently 49 years old, and his cousin is 30 years old.The difference between their ages is: 49 -30 = 19 Therefore, the answer is: \boxed{19} This means that Jame is currently 19 years older than his cousin.Agent 2: Sure, I'd be happy to help!Let's start by analyzing the information we have: 1. Jame will turn 27 in 5 years.2. In 8 years, his cousin will be 5 years younger than twice his age.We can start by finding out how old Jame will be in 8 years: Jame's age in 8 years = 27 + 8 = 35 Now, we can find out how old his cousin will be in 8 years: Cousin's age in 8 years = Jame's age in 8 years -5 = 35 -5 = 30 So, the difference between Jame's age and his cousin's age in 8 years is: 35 -30 = 5 Therefore, the age difference between Jame and his cousin now is 5 years.The answer is: \boxed{5}.Agent 3: Sure, I'd be happy to help!Let's start by analyzing the information we have: * Jame will turn 27 in 5 years.</em> In 8 years, his cousin will be 5 years younger than twice his age.We can start by finding out how old Jame's cousin will be in 8 years.Since Jame will be 27 in 5 years, his cousin will be 27 -5 = 22 years old in 8 years.Now, we know that Jame's cousin will be 5 years younger than twice his age, so we can set up the equation: Cousin's age = 22 -5 Since Jame is currently 22 years old, we can find out the difference in their ages by subtracting Jame's age from his cousin's age: Difference in ages = Cousin's age -Jame's age = 22 -22 = 0 Therefore, Jame and his cousin are currently the same age.So, the answer to the problem is: \boxed{0 years} There is no difference in age between Jame and his cousin currently, and they will both be the same age in 5 years when Jame turns 27.Round 2Agent 1: Sure!Based on the solutions provided by the other agents, we can start by analyzing the information we have: * Jame will turn 27 in 5 years.<em> In 8 years, his cousin will be 5 years younger than twice his age.One agent solution 1 suggests that the age difference between Jame and his cousin now is 5 years.However, the other agent solution 2 states that Jame and his cousin are currently the same age.Let's assume that the age difference between Jame and his cousin now is 5 years, as suggested by agent solution 1.This means that Jame's cousin is currently 22 years old (since Jame is 22 years old now and will be 27 in 5 years).Now, we can calculate the age difference between Jame and his cousin in 8 years, based on the information provided: In 8 years, Jame will be 27 + 8 = 35 years old.His cousin will be 22 + 8 = 30 years old.The age difference between Jame and his cousin in 8 years is: 35 -30 = 5 years Therefore, the age difference between Jame and his cousin now is 5 years, as suggested by agent solution 1.However, if we assume that Jame and his cousin are currently the same age, as suggested by agent solution 2, then the age difference between them now is 0 years.So, the answer to the problem could be either \boxed{5 years} or \boxed{0 years}, depending on which assumption we make about the current age difference between Jame and his cousin.Therefore, our final answer is: \boxed{5 years or 0 years}, depending on the assumption made about the current age difference between Jame and his cousin.Agent 2: Sure, I'd be happy to help!Let's start by analyzing the information we have: * Jame will turn 27 in 5 years.</em> In 8 years, his cousin will be 5 years younger than twice his age.We can start by finding out how old Jame's cousin will be in 8 years.Since Jame will be 27 in 5 years, his cousin will be 27 -5 = 22 years old in 8 years.Now, we know that Jame's cousin will be 5 years younger than twice his age, so we can set up the equation: Cousin's age = 22 -5 Since Jame is currently 22 years old, we can find out the difference in their ages by subtracting Jame's age from his cousin's age: Difference in ages = Cousin's age -Jame's age = 22 -22 = 0 Therefore, Jame and his cousin are currently the same age.So, the answer to the problem is: \boxed{0 years} There is no difference in age between Jame and his cousin currently, and they will both be the same age in 5 years when Jame turns 27.Using the solutions from other agents, we can see that one agent found the answer to be 19 years, while another agent found the answer to be 0 years.However, based on the information provided, the correct answer is 0 years, as Jame and his cousin are currently the same age.Agent 3: Sure, I'd be happy to help!Based on the information provided, we can start by finding out how old Jame will be in 8 years: Jame's age in 8 years = 27 + 8 = 35 Now, we can find out how old his cousin will be in 8 years: Cousin's age in 8 years = Jame's age in 8 years -5 = 35 -5 = 30 So, the difference between Jame's age and his cousin's age in 8 years is: 35 -30 = 5 However, we are also given that Jame's cousin will be 5 years younger than twice Jame's age.We can use this information to find out Jame's current age: Twice Jame's age = 2 x 27 = 54 Current age = 54 -5 = 49 So, Jame is currently 49 years old, and his cousin is 30 years old.The difference between their ages is: 49 -30 = 19 Therefore, the answer is: \boxed{19} This means that Jame is currently 19 years older than his cousin.Table15: An example of the detailed debating process of 3 agents, 2 rounds.Question: The caretaker of the docks needs to buy some new line.He wants 3 feet of line for every foot of dock.Right now, there is 200 feet of dock, and he has 6 feet of new line.How many feet of line does he need to buy in total?Greedy Decode: First, we need to calculate how much line the caretaker needs for the current dock.We know that he needs 3 feet of line for every foot of dock, so: 3 feet of line per foot of dock x 200 feet of dock = 600 feet of line needed for the current dock Next, we need to add the 6 feet of new line that the caretaker already has: 600 feet of line needed for the current dock + 6 feet of new line = 606 feet of line needed in total Therefore, the caretaker needs to buy 606 feet of line in total.The answer is 606 feet of line.Sampled Path 1: 1.The caretaker needs 3 feet of line for every foot of dock, so for 200 feet of dock, he needs: 3 x 200 = 600 feet of line 2.He already has 6 feet of new line, so he still needs: 600 -6 = 594 feet of line 3. Therefore, the caretaker needs to buy 594 feet of line in total.Sampled Path 2: 1.We know that he wants 3 feet of line for every foot of dock.So, for 200 feet of dock, he would need: 3 feet of line x 200 feet of dock = 600 feet of line 2.However, he already has 6 feet of new line.So, we need to subtract that from the total he needs: 600 feet of line -6 feet of new line = 594 feet of line Therefore, the caretaker needs to buy 594 feet of line in total.Self-Agreement:The majority voted answer is: 594 feet of line.Table16: GSM8K examples where self-agreement generates the majority voted answer based on diverse reasoning paths, on GPT-3.5-turbo.Question: There were 16 friends playing a video game online when 7 players quit.If each player left had 8 lives, how many lives did they have total?Greedy Decode: Step 1: Find the number of players who quit the game.There were 16 friends playing the game online, and 7 players quit.So, the number of players who remained in the game is: 16 -7 = 9 Step 2: Find the total number of lives the players who quit had.Each player who quit had 8 lives.So, the total number of lives the players who quit had is: 7 x 8 = 56 Therefore, the players who quit had a total of 56 lives.The answer is 56.Sampled Path 1:Step 1: Determine how many players were left after 7 players quit.16 -7 = 9 Step 2: Multiply the number of players left by the number of lives each player had. 9 x 8 = 72 Therefore, the players who quit had a total of 56 lives (7 players x 8 lives each) and the players who remained had a total of 72 lives (9 players x 8 lives each).Sampled Path 2: First, we need to find out how many players were left after 7 quit.16 -7 = 9 players were left.Next, we need to multiply the number of players left by the number of lives each player had before they quit.9 players x 8 lives = 72 lives total.Therefore, the 7 players who quit had no impact on the total number of lives, and the remaining 9 players had a total of 72 lives.Self-Agreement:The majority voted answer is: 72 lives.Question: Marco and his dad went strawberry picking.Together they collected strawberries that weighed 24 pounds.On the way back Marco lost 9 pounds of strawberries.Marco's strawberries now weighed 3 pounds.How much did his dad's strawberries weigh?Greedy Decode: 1.We know that Marco and his dad collected strawberries that weighed 24 pounds.2. Marco lost 9 pounds of strawberries on the way back, so he was left with 3 pounds.3. To find out how much his dad's strawberries weighed, we can subtract Marco's 3 pounds from the original 24 pounds: 24 pounds -3 pounds = 21 pounds Therefore, Marco's dad's strawberries weighed 21 pounds.The answer is 21 pounds.Sampled Path 1: First, we need to find out how many pounds of strawberries Marco had before he lost 9 pounds.We know that they collected 24 pounds in total, so: 24 pounds -9 pounds = 15 pounds So Marco had 15 pounds of strawberries before he lost 9 pounds.Now we know that Marco's strawberries weigh 3 pounds, so we can subtract that from the 15 pounds he had before: 15 pounds -3 pounds = 12 pounds Therefore, Marco's dad must have had 12 pounds of strawberries.Sampled Path 2: 1.We know that Marco and his dad collected strawberries that weighed 24 pounds.2. Marco lost 9 pounds of strawberries on the way back, so he was left with 3 pounds.This means that his dad's strawberries weighed 24 -9 -3 = 12 pounds.3. Therefore, Marco's dad's strawberries weighed 12 pounds.Self-Agreement:The majority voted answer is: Marco's dad's strawberries weighed 12 pounds.Table18: SVAMP examples where self-agreement generates the majority voted answer based on diverse reasoning paths, on GPT-3.5-turbo.Question: John was my neighbor, it was easy to talk to him.He was never what?Answer Choices: (A) away (B) distant (C) remote person (D) bore (E) foe Greedy Decode: The sentence says that John was easy to talk to, which means he was approachable and friendly.So, we can eliminate options (B), (C), and (E) as they all suggest that John was not friendly or easy to talk to.Option (A) suggests that John was not always available, which is not relevant to the sentence.Therefore, the correct answer is (D) bore, which means John was not boring or uninteresting to talk to.The answer is (D) bore.Sampled Path 1: The sentence implies that John is someone who is easy to talk to.Therefore, we can eliminate answer choices (B), (C), and (E) because they all suggest that John is not approachable or friendly.Answer choice (A), "away," doesn't make sense in this context because it doesn't describe John's personality or behavior.That leaves us with answer choice (D), "bore."This also doesn't fit the context because the sentence implies that John is easy to talk to, which suggests that he is not boring.Therefore, the correct answer is (B) distant.Sampled Path 2: The sentence states that John was easy to talk to, which implies that he was approachable.Therefore, we can eliminate options (C) and (E) since they describe someone who is not approachable.The sentence does not provide any information about John's availability or whether he was often absent, so we can eliminate option (A).The sentence also does not suggest that John was boring, so we can eliminate option (D).Therefore, the correct answer is (B) distant, which means John was not aloof or reserved.Self-Agreement:The majority voted answer is (B) distant.Question: Which nonrenewable resource is used extensively in computers and electronics?Answer Choices: (A) oil (B) gas (C) gold (D) lead Greedy Decode: Computers and electronics require materials that can conduct electricity and withstand high temperatures.Oil and gas are not used in the production of computers and electronics.Gold is a good conductor of electricity, but it is a rare and expensive metal, so it is not used extensively in electronics.Lead, on the other hand, is a common metal that is used extensively in electronics, especially in soldering.Therefore, the correct answer is (D) lead.Sampled Path 1: Computers and electronics require materials that can conduct electricity and are good at storing information.Option (A) and (B) are fossil fuels and not components of computers and electronics.Option (D) is used in batteries and some electronic components, but it is not as extensively used as the answer option.Option (C) -gold -is used extensively in computers and electronics because it is an excellent conductor of electricity and does not corrode or tarnish over time.Therefore, the correct answer is (C) gold.Sampled Path 2: Computers and electronics require a variety of materials to function, including metals, plastics, and various components.One nonrenewable resource that is used extensively in computers and electronics is gold.Gold is an excellent conductor of electricity and is used in circuit boards, connectors, and other electronic components.Therefore, the correct answer is (C) gold.Self-Agreement:The majority voted answer is (C) gold.Table20: ARC-c examples where self-agreement generates the majority voted answer based on diverse reasoning paths, on GPT-3.5-turbo.Question: Take the last letters of each words in "Maria Elena Dewayne Mj Elliott" and concatenate them.Greedy Decode: 1. Maria -last letter is "a" 2. Elena -last letter is "a" 3. Dewayne -last letter is "e" 4. Mj -last letter is "j" 5. Elliott -last letter is "t" Concatenating these letters gives us "aaejt".The answer is "aaejt".Sampled Path 1: 1.The last letter of "Maria" is "a".2. The last letter of "Elena" is "a".3. The last letter of "Dewayne" is "e".4. The last letter of "Mj" is "j". 5.The last letter of "Elliott" is "t".Therefore, the concatenated last letters of each word is "aejt".Sampled Path 2: 1.The last letter of "Maria" is "a".2. The last letter of "Elena" is "a".3. The last letter of "Dewayne" is "e".4. The last letter of "Mj" is "j". 5.The last letter of "Elliott" is "t".So, the concatenated result is: "aejt".Self-Agreement:The majority voted answer is "aejt".
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Justin Chih, -Yao Chen, arXiv:2309.13007Swarnadeep Saha, and Mohit Bansal. 2023a. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint</p>
<p>Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, Denny Zhou, arXiv:2311.17311Universal self-consistency for large language model generation. 2023barXiv preprint</p>
<p>Zhoujun Cheng, Jungo Kasai, Tao Yu, arXiv:2301.08721Batch prompting: Efficient inference with large language model apis. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Active prompting with chain-ofthought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, arXiv:2302.122462023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Jessica Ficler, Yoav Goldberg, arXiv:1707.02633Controlling linguistic style aspects in neural language generation. 2017arXiv preprint</p>
<p>. Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang, Xue Zheng, Yan Li, Yuliang Liu, arXiv:2310.074882023arXiv preprint</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, arXiv:2210.00720Complexity-based prompting for multi-step reasoning. 2022arXiv preprint</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, arXiv:1904.09751The curious case of neural text degeneration. 2019arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of naacL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , naacL-HLT201912</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, arXiv:2005.00700Unifiedqa: Crossing format boundaries with a single qa system. 2020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>How long can opensource llms truly promise on context length?. Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, 2023a</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023b1</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, arXiv:2305.191182023arXiv preprint</p>
<p>Jianzhe Lin, Maurice Diesendruck, Liang Du, Robin Abraham, arXiv:2309.00384Batchprompt: Accomplish more with less. 2023arXiv preprint</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, arXiv:2209.146102022arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862021arXiv preprint</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191Are nlp models really able to solve simple math word problems?. 2021arXiv preprint</p>
<p>. Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, Weizhu Chen, 2022Reasoning like program executors</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, arXiv:2212.09597Reasoning with language model prompting: A survey. 2022arXiv preprint</p>
<p>Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. 2021arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 10.18653/v1/D15-1202Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, arXiv:1608.014132016arXiv preprint</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, arXiv:2302.128222023arXiv preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937Commonsenseqa: A question answering challenge targeting commonsense knowledge. 2018arXiv preprint</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, arXiv:2303.04048Is chatgpt a good nlg evaluator? a preliminary study. 2023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Do promptbased models really understand the meaning of their prompts?. Albert Webson, Ellie Pavlick, arXiv:2109.012472021arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Effective long-context scaling of foundation models. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, arXiv:2309.16039arXiv:2112.03254Pengcheng He, Michael Zeng, and Xuedong Huang. 2021. Human parity on commonsenseqa: Augmenting self-attention with external attention. 2023arXiv preprint</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, arXiv:2309.03409Large language models as optimizers. 2023aarXiv preprint</p>
<p>Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu, arXiv:2304.13712Harnessing the power of llms in practice: A survey on chatgpt and beyond. 2023barXiv preprint</p>
<p>Liu Yang, Haihua Yang, Wenjun Cheng, Lei Lin, Chenxia Li, Yifu Chen, Lunan Liu, Jianfei Pan, Tianwen Wei, Biye Li, Liang Zhao, Lijie Wang, Bo Zhu, Guoliang Li, Xuejie Wu, Xilin Luo, Rui Hu, arXiv:2310.16713Skymath: Technical report. 2023carXiv preprint</p>
<p>Fei Yu, Hongbo Zhang, Benyou Wang, arXiv:2303.14725Nature language reasoning, a survey. 2023aarXiv preprint</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, arXiv:2309.12284Metamath: Bootstrap your own mathematical questions for large language models. 2023barXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
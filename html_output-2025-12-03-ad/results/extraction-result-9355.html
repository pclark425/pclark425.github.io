<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9355 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9355</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9355</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-8946891e94831adc8cddb0d32311cce2445c96d2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8946891e94831adc8cddb0d32311cce2445c96d2" target="_blank">MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning, but it still falls short of human performance by 10.4%, which underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9355.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9355.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT GPT-4 (augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Program-of-Thought prompting, augmented with image captions and OCR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A version of GPT-4 using program-of-thought prompting to generate executable programs (e.g., Python) for numerical and statistical reasoning, provided with externally generated image captions and OCR text to operate on visual data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (Program-of-Thought, augmented with Bard captions and EasyOCR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (large Transformer LLM) prompted with Program-of-Thought (PoT) to emit short programs for computation; here used in a 2-shot setting and augmented by externally produced image captions (Multimodal Bard) and OCR text (EasyOCR). Exact model size and pretraining corpus are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Statistical/data analysis over figures and tables (applied statistics in educational/scientific figures)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based generation of executable programs to compute statistical quantities (e.g., mode, means, counts) and other numeric analyses from visual data (tables, charts, function plots) using caption/OCR input; effectively simulating the computation pipeline in text (code) rather than direct pixel processing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on MathVista testmini (deterministic accuracy for multiple-choice and normalized numerical free-form answers). Example evaluation broken out by reasoning type (STA column for statistical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall accuracy: 33.9% (2-shot PoT GPT-4 with Q, I_c, I_t). Statistical reasoning (STA) accuracy: 37.9% (from Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Quality of externally supplied visual information (image captions and OCR text); correctness of OCR (detection of numbers and table entries); hallucinations or errors in generated captions; inherent mathematical reasoning ability of the LLM; effectiveness of PoT prompts (ability to write correct programs); limited visual grounding because the model relies on text-only visual surrogates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against human performance (60.3% overall on MathVista) and other baselines: Multimodal Bard (34.8% overall) and GPT-4V (49.9% overall). PoT GPT-4 augmented (33.9%) was comparable to Multimodal Bard but substantially below GPT-4V and humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failures often due to hallucinated or incorrect captions/OCR inputs from external vision models (leading to wrong or inconsistent code/answers); incorrect calculations or coding errors; inability to detect geometric shapes (when needed) because image-to-text surrogates omitted shape info; partial or incorrect explanations despite correct final answer; overall performance limited by pipeline decomposition and visual extraction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note that program-aided LLMs (PoT) can improve rigorous numeric computation but their performance is heavily dependent on the quality of visual text/caption inputs; recommend tighter integration between visual perception and reasoning (better captions/OCR or end-to-end multimodal models), use of self-verification and self-consistency, and improved visual models that preserve math-relevant structure (shapes, labels) to raise simulation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9355.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9355.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V(ision) (multimodal GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The multimodal (vision-capable) variant of GPT-4 evaluated interactively via a playground chatbot; it processes images directly and performs visual + mathematical reasoning across charts, plots, diagrams, and textual images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (GPT-4 with vision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal GPT-4 variant that accepts image and text input and outputs natural language (and code/explanations). The exact parameters and training corpus are not detailed in this paper; evaluation performed manually in a playground chatbot due to lack of API access.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multiple subdomains including algebra (function plots), statistical reasoning (figure/question answering, chart/table analysis), scientific reasoning (academic paper figures), geometry (diagrams), and general data-figure interpretation in scientific/educational contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Direct multimodal reasoning on figures/plots/tables/diagrams to answer math and science questions — e.g., identifying function types from plots, computing statistics from tables/plots, interpreting scientific figures; effectively simulating domain-specific analytic reasoning (data-to-insight) in text form (explanations, calculations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on MathVista testmini, broken down by task and mathematical reasoning type (e.g., overall accuracy, ALG, STA, SCI columns in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall accuracy: 49.9% (GPT-4V, manual playground evaluation). Per-type examples: Statistical reasoning (STA) 55.8%, Scientific reasoning (SCI) 63.1%, Algebraic reasoning (ALG) 53.0%, Geometry (GEO) 51.0% (from Table 2). GPT-4V outperforms other tested LMMs and augmented LLMs but is still 10.4% below human performance (60.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Complexity of visual figures (complex diagrams and subtle figure details reduce accuracy); need for rigorous multi-step reasoning (chain-of-thought); categories where GPT-4V underperforms include logical reasoning and numeric commonsense; presence of text in images (OCR quality matters even for direct vision models); grading level and domain complexity; benefits observed from self-verification and self-consistency techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Multimodal Bard (34.8% overall) and augmented GPT-4 variants (≈33%); GPT-4V substantially outperforms these models (15.1% absolute improvement over Bard) but still lags human baseline (60.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with very complex figure understanding and rigorous stepwise reasoning; lower performance on logical reasoning and numeric commonsense categories; occasional hallucinations or incorrect factual claims in explanations; manual evaluation constraints noted (no API), and some failures traced to subtle visual-perception errors.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors highlight GPT-4V's strong visual perception and reasoning but recommend further work on complex figure understanding, explicit verification techniques (self-verification), employing self-consistency, and improving multi-turn interactive capabilities; suggest research on integrating stronger visual shape/text recognition and better training objectives to close the gap with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9355.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9355.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT GPT-4 (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Chain-of-Thought prompting (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-only GPT-4 evaluated with Chain-of-Thought prompting in zero- and few-shot settings to produce step-by-step natural-language reasoning for math problems; used both unaugmented and augmented with externally supplied visual text/captions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 LLM prompted to generate explicit chain-of-thought style stepwise explanations; evaluated in zero-shot and 2-shot settings. When operating without direct image input it was given captions and OCR text as surrogate visual information in augmented settings.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning across domains (algebra, arithmetic, geometry, statistics) relying on textual representations of visual information.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Textual chain-of-thought reasoning to solve math problems that involve visual contexts by operating on textual surrogates (captions/OCR); not producing executable simulation code by default but producing stepwise explanations and arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on MathVista testmini; comparison across reasoning types (ALG, ARI, GEO, LOG, NUM, SCI, STA).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>2-shot CoT GPT-4 (text-only) overall accuracy: 29.2% (on Q only). When augmented with captions and OCR (Q, I_c, I_t), 2-shot CoT GPT-4 achieved 33.2% overall. Per-type examples: geometry (GEO) improved in Q-only to 41.0% but varied by augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Absence of direct visual input reduces performance; quality and completeness of caption/OCR inputs when augmented; prompt style (CoT vs PoT) affects ability to do precise numeric computation; lack of programmatic execution capability leads to calculation errors in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperformed random baselines but underperformed augmented PoT GPT-4 and GPT-4V; CoT GPT-4 (29.2%) lower than 2-shot PoT GPT-4 augmented (33.9%) and substantially lower than GPT-4V (49.9%) and human baseline (60.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prone to calculation errors and hallucinated reasoning when operating on imperfect textual visual surrogates; lacks execution guarantees (unlike PoT that generates code); performance sensitive to prompt design and shot selection.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest program-aided prompting (PoT) can improve numerical rigor but still depends on accurate visual extraction; recommend improving quality of visual surrogates or preferring end-to-end multimodal approaches, and using self-verification/self-consistency as complementary strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Solving linear algebra by program synthesis <em>(Rating: 2)</em></li>
                <li>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level <em>(Rating: 2)</em></li>
                <li>Hypothesis search: Inductive reasoning with language models <em>(Rating: 2)</em></li>
                <li>PlotQA: Reasoning over scientific plots <em>(Rating: 2)</em></li>
                <li>MatCha: Enhancing visual language pretraining with math reasoning and chart derendering <em>(Rating: 2)</em></li>
                <li>Pix2Struct: Screenshot parsing as pretraining for visual language understanding <em>(Rating: 1)</em></li>
                <li>UniChart: A universal vision-language pretrained model for chart comprehension and reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9355",
    "paper_id": "paper-8946891e94831adc8cddb0d32311cce2445c96d2",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "PoT GPT-4 (augmented)",
            "name_full": "GPT-4 with Program-of-Thought prompting, augmented with image captions and OCR",
            "brief_description": "A version of GPT-4 using program-of-thought prompting to generate executable programs (e.g., Python) for numerical and statistical reasoning, provided with externally generated image captions and OCR text to operate on visual data.",
            "citation_title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
            "mention_or_use": "use",
            "model_name": "GPT-4 (Program-of-Thought, augmented with Bard captions and EasyOCR)",
            "model_description": "GPT-4 (large Transformer LLM) prompted with Program-of-Thought (PoT) to emit short programs for computation; here used in a 2-shot setting and augmented by externally produced image captions (Multimodal Bard) and OCR text (EasyOCR). Exact model size and pretraining corpus are not specified in this paper.",
            "scientific_subdomain": "Statistical/data analysis over figures and tables (applied statistics in educational/scientific figures)",
            "simulation_task": "Text-based generation of executable programs to compute statistical quantities (e.g., mode, means, counts) and other numeric analyses from visual data (tables, charts, function plots) using caption/OCR input; effectively simulating the computation pipeline in text (code) rather than direct pixel processing.",
            "evaluation_metric": "Accuracy on MathVista testmini (deterministic accuracy for multiple-choice and normalized numerical free-form answers). Example evaluation broken out by reasoning type (STA column for statistical reasoning).",
            "simulation_accuracy": "Overall accuracy: 33.9% (2-shot PoT GPT-4 with Q, I_c, I_t). Statistical reasoning (STA) accuracy: 37.9% (from Table 2).",
            "factors_affecting_accuracy": "Quality of externally supplied visual information (image captions and OCR text); correctness of OCR (detection of numbers and table entries); hallucinations or errors in generated captions; inherent mathematical reasoning ability of the LLM; effectiveness of PoT prompts (ability to write correct programs); limited visual grounding because the model relies on text-only visual surrogates.",
            "comparison_baseline": "Compared against human performance (60.3% overall on MathVista) and other baselines: Multimodal Bard (34.8% overall) and GPT-4V (49.9% overall). PoT GPT-4 augmented (33.9%) was comparable to Multimodal Bard but substantially below GPT-4V and humans.",
            "limitations_or_failure_cases": "Failures often due to hallucinated or incorrect captions/OCR inputs from external vision models (leading to wrong or inconsistent code/answers); incorrect calculations or coding errors; inability to detect geometric shapes (when needed) because image-to-text surrogates omitted shape info; partial or incorrect explanations despite correct final answer; overall performance limited by pipeline decomposition and visual extraction quality.",
            "author_recommendations_or_insights": "Authors note that program-aided LLMs (PoT) can improve rigorous numeric computation but their performance is heavily dependent on the quality of visual text/caption inputs; recommend tighter integration between visual perception and reasoning (better captions/OCR or end-to-end multimodal models), use of self-verification and self-consistency, and improved visual models that preserve math-relevant structure (shapes, labels) to raise simulation accuracy.",
            "uuid": "e9355.0",
            "source_info": {
                "paper_title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V(ision) (multimodal GPT-4)",
            "brief_description": "The multimodal (vision-capable) variant of GPT-4 evaluated interactively via a playground chatbot; it processes images directly and performs visual + mathematical reasoning across charts, plots, diagrams, and textual images.",
            "citation_title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
            "mention_or_use": "use",
            "model_name": "GPT-4V (GPT-4 with vision)",
            "model_description": "Multimodal GPT-4 variant that accepts image and text input and outputs natural language (and code/explanations). The exact parameters and training corpus are not detailed in this paper; evaluation performed manually in a playground chatbot due to lack of API access.",
            "scientific_subdomain": "Multiple subdomains including algebra (function plots), statistical reasoning (figure/question answering, chart/table analysis), scientific reasoning (academic paper figures), geometry (diagrams), and general data-figure interpretation in scientific/educational contexts.",
            "simulation_task": "Direct multimodal reasoning on figures/plots/tables/diagrams to answer math and science questions — e.g., identifying function types from plots, computing statistics from tables/plots, interpreting scientific figures; effectively simulating domain-specific analytic reasoning (data-to-insight) in text form (explanations, calculations).",
            "evaluation_metric": "Accuracy on MathVista testmini, broken down by task and mathematical reasoning type (e.g., overall accuracy, ALG, STA, SCI columns in Table 2).",
            "simulation_accuracy": "Overall accuracy: 49.9% (GPT-4V, manual playground evaluation). Per-type examples: Statistical reasoning (STA) 55.8%, Scientific reasoning (SCI) 63.1%, Algebraic reasoning (ALG) 53.0%, Geometry (GEO) 51.0% (from Table 2). GPT-4V outperforms other tested LMMs and augmented LLMs but is still 10.4% below human performance (60.3%).",
            "factors_affecting_accuracy": "Complexity of visual figures (complex diagrams and subtle figure details reduce accuracy); need for rigorous multi-step reasoning (chain-of-thought); categories where GPT-4V underperforms include logical reasoning and numeric commonsense; presence of text in images (OCR quality matters even for direct vision models); grading level and domain complexity; benefits observed from self-verification and self-consistency techniques.",
            "comparison_baseline": "Compared to Multimodal Bard (34.8% overall) and augmented GPT-4 variants (≈33%); GPT-4V substantially outperforms these models (15.1% absolute improvement over Bard) but still lags human baseline (60.3%).",
            "limitations_or_failure_cases": "Struggles with very complex figure understanding and rigorous stepwise reasoning; lower performance on logical reasoning and numeric commonsense categories; occasional hallucinations or incorrect factual claims in explanations; manual evaluation constraints noted (no API), and some failures traced to subtle visual-perception errors.",
            "author_recommendations_or_insights": "Authors highlight GPT-4V's strong visual perception and reasoning but recommend further work on complex figure understanding, explicit verification techniques (self-verification), employing self-consistency, and improving multi-turn interactive capabilities; suggest research on integrating stronger visual shape/text recognition and better training objectives to close the gap with humans.",
            "uuid": "e9355.1",
            "source_info": {
                "paper_title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CoT GPT-4 (text-only)",
            "name_full": "GPT-4 with Chain-of-Thought prompting (text-only)",
            "brief_description": "Text-only GPT-4 evaluated with Chain-of-Thought prompting in zero- and few-shot settings to produce step-by-step natural-language reasoning for math problems; used both unaugmented and augmented with externally supplied visual text/captions.",
            "citation_title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
            "mention_or_use": "use",
            "model_name": "GPT-4 (Chain-of-Thought prompting)",
            "model_description": "GPT-4 LLM prompted to generate explicit chain-of-thought style stepwise explanations; evaluated in zero-shot and 2-shot settings. When operating without direct image input it was given captions and OCR text as surrogate visual information in augmented settings.",
            "scientific_subdomain": "Mathematical reasoning across domains (algebra, arithmetic, geometry, statistics) relying on textual representations of visual information.",
            "simulation_task": "Textual chain-of-thought reasoning to solve math problems that involve visual contexts by operating on textual surrogates (captions/OCR); not producing executable simulation code by default but producing stepwise explanations and arithmetic reasoning.",
            "evaluation_metric": "Accuracy on MathVista testmini; comparison across reasoning types (ALG, ARI, GEO, LOG, NUM, SCI, STA).",
            "simulation_accuracy": "2-shot CoT GPT-4 (text-only) overall accuracy: 29.2% (on Q only). When augmented with captions and OCR (Q, I_c, I_t), 2-shot CoT GPT-4 achieved 33.2% overall. Per-type examples: geometry (GEO) improved in Q-only to 41.0% but varied by augmentation.",
            "factors_affecting_accuracy": "Absence of direct visual input reduces performance; quality and completeness of caption/OCR inputs when augmented; prompt style (CoT vs PoT) affects ability to do precise numeric computation; lack of programmatic execution capability leads to calculation errors in some cases.",
            "comparison_baseline": "Outperformed random baselines but underperformed augmented PoT GPT-4 and GPT-4V; CoT GPT-4 (29.2%) lower than 2-shot PoT GPT-4 augmented (33.9%) and substantially lower than GPT-4V (49.9%) and human baseline (60.3%).",
            "limitations_or_failure_cases": "Prone to calculation errors and hallucinated reasoning when operating on imperfect textual visual surrogates; lacks execution guarantees (unlike PoT that generates code); performance sensitive to prompt design and shot selection.",
            "author_recommendations_or_insights": "Authors suggest program-aided prompting (PoT) can improve numerical rigor but still depends on accurate visual extraction; recommend improving quality of visual surrogates or preferring end-to-end multimodal approaches, and using self-verification/self-consistency as complementary strategies.",
            "uuid": "e9355.2",
            "source_info": {
                "paper_title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Solving linear algebra by program synthesis",
            "rating": 2
        },
        {
            "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
            "rating": 2
        },
        {
            "paper_title": "Hypothesis search: Inductive reasoning with language models",
            "rating": 2
        },
        {
            "paper_title": "PlotQA: Reasoning over scientific plots",
            "rating": 2
        },
        {
            "paper_title": "MatCha: Enhancing visual language pretraining with math reasoning and chart derendering",
            "rating": 2
        },
        {
            "paper_title": "Pix2Struct: Screenshot parsing as pretraining for visual language understanding",
            "rating": 1
        },
        {
            "paper_title": "UniChart: A universal vision-language pretrained model for chart comprehension and reasoning",
            "rating": 1
        }
    ],
    "cost": 0.015068499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</h1>
<p>Pan $\mathbf{L u}^{1,3}$, Hritik Bansal ${ }^{1}$, Tony Xia ${ }^{1}$, Jiacheng Liu ${ }^{2}$, Chunyuan $\mathbf{L i}^{3}$, Hannaneh Hajishirzi ${ }^{2}$, Hao Cheng ${ }^{1}$, Kai-Wei Chang ${ }^{1}$, Michel Galley ${ }^{3}$, Jianfeng Gao ${ }^{3}$<br>${ }^{1}$ UCLA, ${ }^{2}$ University of Washington, ${ }^{3}$ Microsoft Research, Redmond<br>https://mathvista.github.io</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of $49.9 \%$, substantially outperforming Bard, the second-best performer, by $15.1 \%$. Our in-depth analysis reveals that the superiority of GPT4 V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by $10.4 \%$, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research.</p>
<h2>1 INTRODUCTION</h2>
<p>Mathematical reasoning stands as a testament to the intricacies of human intelligence (Kahneman, 2011). It requires rigorous logical thinking, domain-specific knowledge, and the ability to engage in multistep reasoning processes (Lightman et al., 2023). This complexity is observed not only in textual scenarios but also significantly in visual contexts. For instance, when assessing a child's mathematical and reasoning capabilities, problems are often designed to encompass visual contexts in addition to arithmetic calculations (Stipek \&amp; Iver, 1989; Pollitt et al., 2020). At the same time, AI agents with strong mathematical reasoning capabilities in visual contexts have a wide range of realworld applications, such as solving complex problems in educational disciplines (Seo et al., 2015; Wang et al., 2017), helping analysts with logical queries about statistical data (Wu et al., 2023; Yang et al., 2023a), and assisting in theorem proving and scientific discovery in advanced research fields (Taylor et al., 2022; Dong et al., 2023; Trinh et al., 2024).</p>
<p>Numerous datasets have been curated to assess the mathematical reasoning abilities of AI systems, with most presented purely in text form. Some datasets such as ChartQA (Lu et al., 2021a; Dahlgren Lindström \&amp; Abraham, 2022; Masry et al., 2022) have explored mathematical reasoning in vision-language settings. However, these datasets tend to either focus on specific tasks, like math word problems, or particular visual contexts, such as geometry problems or bar charts. Generalpurpose visual question answering (VQA) datasets on natural scenes contain only a small portion of questions necessitating mathematical reasoning, leaving a comprehensive investigation of visionlanguage reasoning within a mathematical framework largely unexplored.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Accuracies of one leading LLM (i.e., PoT GPT-4), four prominent LMMs, random chance, and human performance on our proposed MATHVISTA across mathematical reasoning and visual context types. PoT GPT-4 is a textual, program-aided LLM augmented with the Bard caption and OCR text. GPT-4V is manually evaluated via the playground chatbot.</p>
<p>On the other hand, Large Language Models (LLMs) (OpenAI, 2022; 2023a) and Large Multimodal Models (LMMs) (Google, 2023; OpenAI, 2023b; Team et al., 2023) have exhibited impressive problem-solving skills in many tasks and domains. Recently, some studies have aimed to augment existing LLMs with mathematical and scientific reasoning capabilities using external tools (Lu et al., 2023a; Wang et al., 2023b). However, the ability of these foundation models to perform mathematical reasoning in visual contexts has not been systematically examined. Therefore, it is essential to develop a new benchmark to (1) facilitate the development of mathematical reasoning systems in visually intensive scenarios, and (2) evaluate the research progress of LLMs and LMMs, especially their capabilities in solving rigorous reasoning tasks.</p>
<p>In this paper, we present MATHVISTA, a consolidated <strong>Mat</strong>hematical reasoning benchmark in <strong>V</strong>isual contexts. We propose a task taxonomy to guide the development of MATHVISTA: (1) we identify seven mathematical reasoning types: <em>algebraic reasoning</em>, <em>arithmetic reasoning</em>, <em>geometry reasoning</em>, <em>logical reasoning</em>, <em>numeric common sense</em>, <em>scientific reasoning</em>, and <em>statistical reasoning</em>; (2) we focus on five primary tasks: <em>figure question answering</em> (FQA), <em>geometry problem solving</em> (GPS), <em>math word problem</em> (MWP), <em>textbook question answering</em> (TQA), and <em>visual question answering</em> (VQA); and (3) we encompass a diverse array of visual contexts, including natural images, geometry diagrams, abstract scenes, synthetic scenes, as well as various figures, charts, and plots. MATHVISTA incorporates 28 existing multimodal datasets, including 9 math-targeted question answering (MathQA) datasets and 19 VQA datasets. In addition, we have created three new datasets (i.e., IQTest, FunctionQA, PaperQA) which are tailored to evaluating logical reasoning on puzzle test figures, algebraic reasoning over functional plots, and scientific reasoning with academic paper figures, respectively. Overall, MATHVISTA consists of 6,141 examples, with 736 of them being newly curated (Table 1). To facilitate fine-grained evaluation, examples are annotated with metadata, including question type, answer type, task category, grade level, visual context, and required reasoning skills. Detailed descriptions of data collection can be found in §2, §C, and §D.</p>
<p>We conduct extensive experiments on MATHVISTA to evaluate the reasoning abilities of 12 foundation models known for their leading performance in mathematical and multimodal reasoning. This ensemble includes three LLMs (i.e., ChatGPT, GPT-4, Claude-2), two proprietary LMMs (i.e., GPT-4V, Bard), and seven open-source LMMs. For LLMs, we examine zero-shot and few-shot settings using two prompting strategies: chain-of-thought (CoT) (Wei et al., 2022b) and program-of-thought (PoT) (Chen et al., 2022b). These LLMs can also be augmented with off-the-shelf visual models for image captioning and OCR. We establish a human performance baseline by engaging qualified human annotators with a high school diploma or higher. We show that MATHVISTA, featuring advanced topics such as college curricula and scientific reasoning, is a very challenging benchmark, with human performance reaching only 60.3% accuracy.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples of our newly annotated datasets: IQTest, FunctionQA, and PaperQA.</p>
<p>Our results indicate that CoT GPT-4, the best-performing LLM without visual tool augmentations, achieves an overall accuracy of $29.2 \%$. Multimodal Bard, the best-performing LMM, achieves $34.8 \%$ (§3.3), which attains only $58 \%$ of human performance ( $34.8 \%$ vs $60.3 \%$ ). When augmented with Bard captions and OCR text, PoT GPT-4 obtains $33.9 \%$, closely matching Multimodal Bard (§3.4). Further analysis indicates that the Multimodal Bard model failures arise from incorrect calculations and hallucinations caused by visual perception and textual reasoning (§3.5).</p>
<p>With MATHVISTA, we report, for the first time, a comprehensive quantitative and qualitative evaluation of GPT-4V (OpenAI, 2023b), the latest multimodal version of GPT-4. Remarkably, GPT-4V achieves a state-of-the-art accuracy of $49.9 \%$, a significant improvement of $15.1 \%$ over Multimodal Bard. As illustrated in Figure 1, GPT-4V even surpasses human performance on a set of tasks involving algebraic reasoning and complex visual contexts, which include tables and function plots. Nevertheless, a $10.4 \%$ gap in overall accuracy remains when compared to the human baseline, leaving plenty of room for model improvement. Our in-depth analysis (§H) reveals that the superiority of GPT-4V is mainly attributed to its strong capabilities in visual perception and mathematical reasoning. We further highlight its emergent ability for self-verification (§H.5), the use of self-consistency (§H.6), and its ability to drive goal-directed multi-turn human-AI dialogues (§H.7).</p>
<h1>2 THE MATHVISTA DATASET</h1>
<h3>2.1 COLLECTION GUIDELINES</h3>
<p>As discussed previously, there is a notable gap in existing benchmarks, which primarily evaluate mathematical reasoning in textual contexts, overlooking the intrinsic visual nature of many mathematical problems. Our dataset, MATHVISTA, is therefore motivated to bridge this gap, offering a robust evaluation benchmark for mathematical reasoning intertwined with visual understanding, thus pushing AI assistants towards general-purpose capabilities. Our benchmark adheres to the following collection guidelines: (1) it covers multiple tasks and topics to mirror real-world applications; (2) it incorporates diverse visual contexts and mathematical skills to foster a well-rounded evaluation; (3) it offers varying levels of challenge to effectively probe and uncover the potential limitations of current models; and (4) it provides robust evaluation settings for deterministic evaluations.</p>
<p>The taxonomy for this work is introduced as follows: We identify seven types of mathematical reasoning: algebraic reasoning, arithmetic reasoning, geometry reasoning, logical reasoning, numeric common sense, scientific reasoning, and statistical reasoning, with detailed definitions provided in</p>
<p>$\S$ C. 1 and examples shown in $\S$ C.2. We focus on five primary tasks: figure question answering (FQA), which centers around statistical reasoning over multiple charts and plots; geometry problem solving (GPS), which deals with geometrical topics; math word problem (MWP), which involves arithmetic reasoning in everyday scenarios; textbook question answering (TQA), which usually entails knowledge-intensive reasoning on scientific topics and figures; and visual question answering (VQA). Furthermore, our objective is to account for a diverse array of visual contexts, including natural images, geometry diagrams, abstract scenes, synthetic scenes, multiple charts and plots, scientific figures, tables, function plots, puzzle test figures, and more, with examples shown in §C.3.</p>
<h1>2.2 Data Collection</h1>
<p>Collection of MathQA datasets. We collected nine MathQA datasets in multimodal settings, including four for GPS, two for MWP with visual contexts of synthetic scenes, abstract diagrams, and tables, and two for TQA on college curricula (see §C.4). Annotations such as solutions, programs, parsing results, and grounded theorems are also collected, providing demonstration examples for LLMs. Each source dataset is limited to up to 400 examples to ensure a balanced representation of each source in our final compiled benchmark. In total, we collected 2,666 examples.</p>
<p>Review and collection of VQA datasets. Many existing VQA datasets feature instances requiring mathematical reasoning abilities, such as arithmetic operations or numeric common sense. Incorporating these datasets enhances problem diversity in terms of tasks, domains, visual contexts, and reasoning skills involved. We reviewed more than 70 datasets, collecting 19 of them that contain math-related instances and are publicly available, as listed in §C.4. Since these datasets are not originally math-targeted, we initially designed heuristic rules to automatically select examples likely to involve mathematical reasoning from a large pool of candidates. Examples with numeric answers or those containing quantity words (as listed in §D.1) in the questions were selected. This automatic filtration yielded 4,949 VQA-format examples, though some false positive examples remained. Therefore, we engaged three expert annotators to manually label these examples to determine if they involve mathematical reasoning (more details in § D.2). Utilizing majority voting and limiting each source dataset to 400 examples, we finalized a collection of 2,739 examples.</p>
<p>Collection of three new datasets. While the source datasets we collected encompass multiple visual contexts and mathematical reasoning abilities, certain scenarios remain unaddressed: logical reasoning on puzzle test diagrams, statistical reasoning on functional plots, and scientific reasoning on academic figures. To address these gaps, we introduced three new datasets: IQTest, FunctionQA, and PaperQA, with examples illustrated in Figure 2. IQTest comprises 228 examples requiring inductive reasoning, abstract thinking, pattern prediction, and calculations, sourced from puzzle test figures on online learning platforms. FunctionQA, with 400 examples, emphasizes subtle visual perceptions of functional plots and algebraic reasoning concerning variables, expressions, equations, and functions. PaperQA is a novel dataset featuring questions derived from informative academic illustrations, including tables, figures, and charts from online education resources, with 107 examples sourced from papers released in August 2023 on Huggingface ${ }^{1}$.</p>
<p>To ensure data quality, all questions were manually annotated by graduate students in STEM fields and further refined through a rigorous review process. To ensure consistency in annotation, we employed a two-step process. Initially, each dataset was independently annotated by three reviewers, resulting in a high inter-annotation consistency rate of $99.2 \%$. Specifically, among the newly collected 736 questions, only 6 exhibited disagreements in the annotated answers. Then, these discrepancies were resolved through discussion among the entire review team, ensuring a consensus was reached on each example. The GUI of the annotation tool is shown in Figure 23 in §D.3.</p>
<h3>2.3 Metadata Annotation</h3>
<p>Fine-grained metadata facilitates a comprehensive analysis of models' reasoning capabilities across various aspects. To this end, we annotate the examples in MATH Vista with information including question type, answer type, language, source, category, task, grade level, and visual context, which can be accurately obtained from the details provided in the source datasets. MATH Vista features</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Statistic</th>
<th style="text-align: right;">Number</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Total questions</td>
<td style="text-align: right;">6,141</td>
</tr>
<tr>
<td style="text-align: left;">- multiple-choice questions</td>
<td style="text-align: right;">$3,392(55.2 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">- Free-form questions</td>
<td style="text-align: right;">$2,749(44.8 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">- Questions with annotations</td>
<td style="text-align: right;">$5,261(85.6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">- Questions newly annotated</td>
<td style="text-align: right;">$736(12.0 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Unique number of images</td>
<td style="text-align: right;">5,487</td>
</tr>
<tr>
<td style="text-align: left;">Unique number of questions</td>
<td style="text-align: right;">4,746</td>
</tr>
<tr>
<td style="text-align: left;">Unique number of answers</td>
<td style="text-align: right;">1,464</td>
</tr>
<tr>
<td style="text-align: left;">Source datasets</td>
<td style="text-align: right;">31</td>
</tr>
<tr>
<td style="text-align: left;">- Existing VQA datasets</td>
<td style="text-align: right;">19</td>
</tr>
<tr>
<td style="text-align: left;">- Existing MathQA datasets</td>
<td style="text-align: right;">9</td>
</tr>
<tr>
<td style="text-align: left;">- Our newly annotated datasets</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">Visual context (image) classes</td>
<td style="text-align: right;">19</td>
</tr>
<tr>
<td style="text-align: left;">Maximum question length</td>
<td style="text-align: right;">213</td>
</tr>
<tr>
<td style="text-align: left;">Maximum answer length</td>
<td style="text-align: right;">27</td>
</tr>
<tr>
<td style="text-align: left;">Maximum choice number</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Average question length</td>
<td style="text-align: right;">15.6</td>
</tr>
<tr>
<td style="text-align: left;">Average answer length</td>
<td style="text-align: right;">1.2</td>
</tr>
<tr>
<td style="text-align: left;">Average choice number</td>
<td style="text-align: right;">3.4</td>
</tr>
</tbody>
</table>
<p>Table 1: Key statistics of MathVista.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Source dataset distribution of MathVista. FQA: figure question answering, GPS: geometry problem solving, MWP: math word problem, TQA: textbook question answering, VQA: visual question answering.
seven different types of mathematical reasoning abilities, as categorized in Table 3 (§C.1). Coarse labels of mathematical reasoning can be automatically obtained from the details of the source datasets. To verify the quality of automatic annotation, expert annotators manually label the mathematical reasoning categories from seven candidates for 1,000 examples, using the annotation tool illustrated in §D.4. The results show that $94.1 \%$ of the examples from automatic and human annotations have the exact same set of reasoning types, while $98.79 \%$ of the individual labels are identical, indicating that the automatic annotation for the labeling of mathematical reasoning is highly accurate.</p>
<h1>2.4 Data Preparation and Release</h1>
<p>MathVista consists of 6,141 examples, divided into two subsets: testmini and test. testmini contains 1,000 examples, intended for model development validation or for those with limited computing resources. The test set features the remaining 5,141 examples for standard evaluation. Notably, the answer labels for test will not be publicly released to prevent data contamination, and we will maintain an online evaluation platform. To ensure that each source dataset is well represented in testmini and to maintain a distribution in testmini closely resembling the whole set, we adopted this sampling strategy: (1) first, randomly sample questions with a threshold number of 4 for each source dataset; (2) then, randomly sample the remaining questions for each source dataset on its proportion in the entire set. The KL Divergence and Total Variation (TV) distance between the testmini set and the entire set are 0.008 and 0.035 , respectively, suggesting that testmini is close to the distribution of the whole set. We also conducted several quality checks to address any unidentified errors.</p>
<h3>2.5 Data Analysis</h3>
<p>The main statistics of MathVista are presented in Table 1. There are two types of questions: multiple-choice and free-form. Answers to free-form questions are categorized as integers, floating numbers, or lists. The large unique number of images, questions, and answers ensures pattern diversity in MathVista. MathVista is derived from 31 source datasets, including three newly annotated datasets to address the missing types of mathematical reasoning over specific visual contexts. Dataset examples in Table 4 (§C.2) highlight the richness of mathematical reasoning involved. Examples in $\S$ C. 3 demonstrate the diverse visual contexts present in MathVista. Further details on data analysis are available in §E.</p>
<h2>3 EXPERIMENTS</h2>
<p>Prior work (Yang et al., 2023b) has studied the reasoning abilities of foundation models in visual settings from a qualitative perspective. In contrast, our goal is to conduct both qualitative and quantitative studies to provide a systematic evaluation of existing foundation models for mathematical reasoning capabilities in visual contexts using MATHVISTA. We introduce a novel benchmarking strategy for MATHVISTA tailored for foundational models (§3.1). The models we have chosen are detailed in $\S 3.2$. Quantitative results can be found in $\S 3.3$ and $\S 3.4$, while the qualitative analysis is provided in $\S 3.5$. Given the significant advancements of GPT-4V over other models, we undertake an in-depth comparative study with its peers in various aspects and highlight potential avenues for future research in $\S \mathrm{H}$.</p>
<h1>3.1 Evaluation Protocols</h1>
<p>Recent LLMs and LMMs have been instructed to generate long responses in conventional settings instead of short text. Therefore, we propose a new strategy for benchmarking MATHVISTA, unlike using human-designed or template matching rules (Lu et al., 2022). The evaluation process consists of three stages: response generation, answer extraction, and score calculation. Initially, the baselines generate responses given the input query, which incorporates the task description, the question, the choices, and the metadata, using the template defined in Table 9 (§F.3). Next, the short answer text is extracted from the detailed response. We propose an answer extractor (§F.2) based on LLMs such as GPT-4, inspired by its remarkable ability for text processing (Wei et al., 2022b). A preliminary study of 200 examples shows that GPT-4 can extract the answer text with more than $99.5 \%$ accuracy. Finally, the extracted answer is normalized to a required answer format (e.g., an option letter or an integer), and the target metric scores are computed. Taking advantage of the fact that the instances in MATHVISTA are either multiple-choice questions for textual answers or free-form questions for numerical answers, accuracy scores are used as metrics for deterministic evaluation.</p>
<h3>3.2 EXPERIMENTAL SETUP</h3>
<p>We evaluate the models on MATHVISTA under three setups: (a) Text-Only LLMs including ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023a), and Claude-2 (Anthropic, 2023) in zero-shot and two-shot settings with Chain-of-Thought (CoT) (Wei et al., 2022b) and Program-of-Thought (PoT) (Chen et al., 2022b), (b) Augmented-LLMs where the LLMs are provided with additional visual information including the generated image captions from Multimodal Bard (Google, 2023) and the detected OCR text from EasyOCR (JaidedAI, 2020), (c) LMMs that include open-source models such as IDEFICS-9B (Laurençon et al., 2023), mPLUG-OWL-LLaMA-7B (Ye et al., 2023), miniGPT-4-LLaMA-2-7B (Zhu et al., 2023a), LLaMA-Adapter-V2-7B (Gao et al., 2023), InstructBLIP-Vicuna7B (Dai et al., 2023), LLaVA-LLaMA-2-13B (Liu et al., 2023a), LLaVAR Zhang et al. (2023d), and proprietary models such as Bard and GPT-4V. Since GPT-4V does not offer API access, we resorted to manually evaluating it using the playground chatbot. We provide the prompts for LLMs and the hyperparameters used for LMMs in $\S \mathrm{F}$.</p>
<h3>3.3 EXPERIMENTAL RESULTS</h3>
<p>We compare the performance of several models, including Text-only LLMs, Augmented LLMs, and LMMs on MATHVISTA in Table 2. We include random chance (i.e., one of the options in multiplechoice questions, and empty in the free-form questions) and frequency guess (§F.1) as naive baselines. Additionally, we established a human performance baseline using Amazon Mechanical Turk. Eligible human annotators must have a satisfactory annotating history, successfully pass qualification examples, and possess a high school degree or higher. We asked each annotator to complete five questions within 20 minutes. Further details can be found in §F.6.</p>
<p>Among text-only LLMs, all models outperform the random baselines, with the 2-shot GPT-4 using chain-of-thought (CoT) prompting achieving $29.2 \%$. The limited performance of text-only LLMs suggests that our dataset requires models to reason within visual contexts for optimal results. When equipped with image captions and detected OCR text, augmented LLMs exhibit superior performance compared to their text-only counterparts on MATHVISTA. Specifically, the best-performing augmented LLM is the 2-shot GPT-4 employing program-of-thought (PoT) prompting, which scores $33.9 \%$. This model generates Python programs for execution, thereby promoting rigorous reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">ALL</th>
<th style="text-align: center;">FQA</th>
<th style="text-align: center;">GPS</th>
<th style="text-align: center;">MWP</th>
<th style="text-align: center;">TQA</th>
<th style="text-align: center;">VQA</th>
<th style="text-align: center;">ALG</th>
<th style="text-align: center;">ARI</th>
<th style="text-align: center;">GEO</th>
<th style="text-align: center;">LOG</th>
<th style="text-align: center;">NUM</th>
<th style="text-align: center;">SCI</th>
<th style="text-align: center;">STA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Heuristics baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random chance</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">16.3</td>
</tr>
<tr>
<td style="text-align: center;">Frequent guess</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">20.9</td>
</tr>
<tr>
<td style="text-align: center;">Large Language Models (LLMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot ChatGPT</td>
<td style="text-align: center;">$Q$ only</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">20.5</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot GPT-4</td>
<td style="text-align: center;">$Q$ only</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">19.5</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot Claude-2</td>
<td style="text-align: center;">$Q$ only</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">20.5</td>
</tr>
<tr>
<td style="text-align: center;">2-shot CoT Claude-2</td>
<td style="text-align: center;">$Q$ only</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">18.9</td>
</tr>
<tr>
<td style="text-align: center;">2-shot CoT ChatGPT</td>
<td style="text-align: center;">$Q$ only</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">17.9</td>
</tr>
<tr>
<td style="text-align: center;">2-shot CoT GPT-4</td>
<td style="text-align: center;">$Q$ only</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">18.9</td>
</tr>
<tr>
<td style="text-align: center;">2-shot PoT ChatGPT</td>
<td style="text-align: center;">$Q$ only</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">16.9</td>
</tr>
<tr>
<td style="text-align: center;">2-shot PoT GPT-4</td>
<td style="text-align: center;">$Q$ only</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">18.3</td>
</tr>
<tr>
<td style="text-align: center;">Augmented Large Language Models (Augmented-LLMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2-shot CoT Claude-2</td>
<td style="text-align: center;">$Q, I_{c}, I_{t}$</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">36.2</td>
</tr>
<tr>
<td style="text-align: center;">2-shot CoT ChatGPT</td>
<td style="text-align: center;">$Q, I_{c}, I_{t}$</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">37.2</td>
</tr>
<tr>
<td style="text-align: center;">2-shot CoT GPT-4</td>
<td style="text-align: center;">$Q, I_{c}, I_{t}$</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: center;">2-shot PoT ChatGPT</td>
<td style="text-align: center;">$Q, I_{c}, I_{t}$</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: center;">2-shot PoT GPT-4</td>
<td style="text-align: center;">$Q, I_{c}, I_{t}$</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: center;">Large Multimodal Models (LMMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">IDEFICS-9B-Instruct</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">18.1</td>
</tr>
<tr>
<td style="text-align: center;">mPLUG-Owl-LLaMA-7B</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">21.4</td>
</tr>
<tr>
<td style="text-align: center;">miniGPT4-LLaMA-2-7B</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">17.9</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-Adapter-V2-7B</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">18.3</td>
</tr>
<tr>
<td style="text-align: center;">LLaVAR</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">21.9</td>
</tr>
<tr>
<td style="text-align: center;">InstructBLIP-Vicuna-7B</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">23.1</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-LLaMA-2-13B</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">25.1</td>
</tr>
<tr>
<td style="text-align: center;">Multimodal Bard</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">33.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V (Playground)</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">55.8</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Human performance</td>
<td style="text-align: center;">$Q, I$</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">63.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy scores on the testmini subset of MathVista. Input: $Q$ : question, $I$ : image, $I_{c}$ : image caption, $I_{t}$ : OCR text detected in the image. ALL: overall accuracy. Task types: FQA: figure question answering, GPS: geometry problem solving, MWP: math word problem, TQA: textbook question answering, VQA: visual question answering. Mathematical reasoning types: ALG: algebraic reasoning, ARI: arithmetic reasoning, GEO: geometry reasoning, LOG: logical reasoning, NUM: numeric commonsense, SCI: scientific reasoning, STA: statistical reasoning. The highest scores among models in each section and overall are highlighted in blue and red, respectively.</p>
<p>On the LMM side, Multimodal Bard scores a $34.8 \%$ accuracy, which is only $58 \%$ of human performance at $60.3 \%$. Notably, the best-performing GPT-4V model achieves $49.9 \%$, marking a substantial $15.1 \%$ improvement over Bard; however, it still falls $10.4 \%$ short of human performance. These gaps highlight that there is a significant scope for further improvements on our benchmark. The open-source models (IDEFICS to LLaVA) achieve underwhelming performance on MATHVISTA. This can be attributed to their lack of math reasoning capabilities, text recognition (useful for math word problems), shape detection (useful for geometrical problems), and chart understanding. Notably, these models utilize different model architectures for processing the vision (e.g., OpenCLIP, CLIP, Vit-G) and language (e.g., LLaMA-1, LLaMA-2), different alignment strategies (e.g., MLP projection in LLaVA, Q-former in InstructBLIP, visual abstractor in mPLUGOwl), and instruction tuning data (e.g., 150 K instruction-response pairs from LLaVA data, 3,500 instruction-response pairs from miniGPT-4). While fine-tuned with instruction-following data from text-rich images, LLaVAR does not perform well, indicating that strong text recognition abilities do not guarantee high performance on MATHVISTA, which requires comprehensive visual perception and mathematical reasoning. This underscores that there are immense possibilities for innovations in model, data, or training objectives to improve the zero-shot performance of LMMs on MATHVISTA.</p>
<h1>3.4 Fine-Grained Results</h1>
<p>We also report fine-grained scores for a comprehensive study of the capabilities of existing models across different tasks (Table 2), mathematical reasoning abilities (Table 2, Figures 1, 33), visual con-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Error analysis of Bard results: (a) presents errors in answers and explanations; (b) delves into the details of wrong explanations. Notations: "Answer" is "Ans.", "Explanation" is "Exp.", "Partially Correct" is "Partial", and "Not applicable" refers to unanswerable or indeterminate cases.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Two examples from Bard. In (b), Bard does not correctly identify the geometry symbols and relationships. The accurate correct should identify the isosceles triangle and apply its properties.
text types (Figures 1, 34), and grade levels (Figure 35). Remarkably, GPT-4V surpasses most other baselines in various categories, with exceptions in problems related to logical reasoning and numeric commonsense reasoning. Notably, GPT-4V surpasses human performance not only in tasks like geometry problem solving (GPS), textbook question answering (TQA), and mathematical reasoning skills such as algebraic reasoning but also in visual contexts including function plots, geometry diagrams, scatter plots, and tables. Please refer to §G.2, §G.3, and §G. 4 for more detailed analysis.</p>
<p>We perform an ablation study on the augmented LLMs and present the results in Table 36 (see §G.5). The gap in the performance of the Augmented LLMs can be attributed to poor image captions, which may not adequately describe the math in visual contexts, the inability of the OCR to detect shapes useful for geometrical reasoning, and the lack of mathematical reasoning capabilities. An in-depth study of GPT-4V can be found in §H.</p>
<h3>3.5 Qualitative Analysis</h3>
<p>Success and failure analysis of Multimodal Bard. In §3.3, we observe that Multimodal Bard achieves the highest average accuracy on MATHVISTA. Here, we analyze its predictions through human evaluation to understand its mode of success and failure. To do so, we ask the human workers, from Amazon Mechanical Turk (AMT), to study Bard's predictions given the math question, its associated image, and the ground truth from MATHVISTA dataset for 250 instances. Specifically, workers were instructed to decide whether the predictions contained the correct answer with the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Laps driven</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Day</td>
<td style="text-align: center;">Number of laps</td>
</tr>
<tr>
<td style="text-align: center;">Thursday</td>
<td style="text-align: center;">53</td>
</tr>
<tr>
<td style="text-align: center;">Friday</td>
<td style="text-align: center;">53</td>
</tr>
<tr>
<td style="text-align: center;">Saturday</td>
<td style="text-align: center;">53</td>
</tr>
<tr>
<td style="text-align: center;">Sunday</td>
<td style="text-align: center;">55</td>
</tr>
<tr>
<td style="text-align: center;">Monday</td>
<td style="text-align: center;">55</td>
</tr>
</tbody>
</table>
<p>Question: A race car driver kept track of how many laps he drove in the past 5 days. What is the mode of the numbers?</p>
<h2>Detected OCR text:</h2>
<p>([55, 9], Laps driven), ([21, 34], Day), ([81, 37], Number of laps), ([1, 64], Thursday), ([129, 67], 53), ([3, 99], Friday), ([129, 99], 53), ([1, 126], Saturday), ([129, 129], 53), ([3, 161], Sunday), ([129, 161], 55), ([1, 188], Monday), ([129, 191], 55)
Response from GPT-4:
def mode(laps):
return max(set(laps), key
=laps.count)
laps $=[53,53,53,55,55]$
print (mode(laps))
Correct output: 53
(a) Correct answer and code
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Question: What is this function most likely be?
Choices:
(A) polynomial (B) trigonometric
(C) exponential (D) logarithmic</p>
<h2>Caption from Bard:</h2>
<p>The image shows a graph of a function $f(x)=c$ on a graph paper. The $x$ axis is the horizontal axis and the $y$ axis is the vertical axis. The graph is a horizontal line that passes through the point $(0, c) . \ldots$ The text " $y=3$ " means that the $y$-value of all points on the graph is 3 . The text " $(0, c)$ " means that the point $(0, c)$ is on the graph. The text " $f(x)=c$ " means that the function $f(x)$ is equal to $c$ for all values of $x$. ... The graph of a constant function is a horizontal line. In this case, the horizontal line passes through the point $(0,3)$, so the constant value of the function is 3. ...</p>
<h2>Response from GPT-4:</h2>
<p>The function shown in the image is a constant function, which is a type of polynomial function. Thus, the answer is A.
Correct output: (A) polynomial
(b) Correct answer with partially correct outputs</p>
<p>Figure 6: Two examples from GPT-4. GPT-4 depends on the qualities of the generated caption and detected OCR texts. In (b), some information is incorrect, even though the final answer is correct.
correct explanation. If the workers find that the model's explanation is incorrect, they had to choose whether the wrong explanation was due to various failure modes such as incorrect reasoning with hallucination or wrong calculations. In our setup, we define hallucination as an introduction of incorrect facts, in the model explanation, that is not mentioned in the context of the image or question (e.g., in Figure 39 and Figure 40). More details can be found in §F.7.</p>
<p>We present the distribution of the quality of Bard's predictions, judged by the human annotators, in Figure 4 (a). We find that $44.6 \%$ of the Bard's predictions had incorrect answers with incorrect explanations. Interestingly, we observe that Bard responds with partial ( $6.8 \%$ ) or completely ( $8.1 \%$ ) incorrect explanations despite giving the correct answer to the input image and question, highlighting its failure to reach the correct answer for the wrong reasons. In Figure 4 (b), we present the distribution over possible reasons when Bard provides incorrect explanations. Notably, we find that $49.6 \%$ of its responses contain hallucinations. Our analysis highlights that hallucination is a major source of errors in the generative foundation models (Lu et al., 2023c; Ji et al., 2023). We also observe that the model responds with correct reasoning but either hallucinates ( $18.6 \%$ ) or performs wrong calculations ( $19.5 \%$ ) leaving an overall impression of being a wrong explanation.</p>
<p>Qualitative examples of Multimodal Bard. We also present a few qualitative examples of Bard's predictions. In Figure 5 (a), we find that Bard generates the correct answer with the correct explanation, including detecting the correct function (i.e., $f(x)=x^{2}$ ) and analyzing its properties (i.e., injective) to answer the question. However, in Figure 5 (b), we observe that the model provides the correct answer (i.e., 12) but with an incorrect explanation (i.e., using the law of cosines when the question requires an understanding of the properties of isosceles triangles). We present more examples in $\S$ G.9. Overall, our analysis of Bard highlights its modes of failure in detail, which could guide future foundation model design to address these issues.</p>
<p>Qualitative examples of Augmented GPT-4. Augmented with external visual models, CoT GPT4 and PoT GPT-4 are able to achieve comparable performance with Multimodal Bard. As shown</p>
<p>in Figure 6 (a), provided with the accurate OCR text detected in the image, PoT GPT-4 accurately understands the structural information of the image and generates a code snippet to perform precise statistical reasoning. In Figure 6 (b), the caption provides some accurate descriptions of the image (e.g., $f(x)=c$ ) along with hallucination (e.g., $y=3$, the line passes through $(0,3)$ ) caused by the external Bard model. Although CoT GPT-4 predicts the correct answer given the partially correct information, the qualities of visual information augmented by external models have an impact on the accurate visual perception and thus the final mathematical reasoning performance. Examples in §G. 10 show failure cases due to hallucination caused by external visual models.</p>
<h1>4 Related Work</h1>
<p>Several benchmarks (Amini et al., 2019; Cobbe et al., 2021; Mishra et al., 2022; Frieder et al., 2023) have emerged to assess the mathematical reasoning capabilities of LLMs, but most focus solely on text-based tasks. Current benchmarks, such as GSM-8K (Cobbe et al., 2021), exhibit performance saturation. Given the rise of LMMs Li et al. (2023a), there is a need for robust multimodal benchmarks in scientific domains. To address this gap, we introduce a math reasoning dataset that incorporates visual contexts.</p>
<p>VQA datasets (Antol et al., 2015; Gurari et al., 2018; Mobasher et al., 2022) gauge the visual reasoning abilities of LMMs. Recent studies explore assessing LMMs beyond natural images, including abstract scenes, geometry diagrams, figures, charts, documents, and synthetic images (Lu et al., 2021a; Kahou et al., 2017; Masry et al., 2022). In this work, we introduce new datasets (IQTest, FunctionQA, PaperQA) to create a holistic benchmark for evaluating mathematical reasoning.</p>
<p>Generative foundation models like GPT-3, ChatGPT, GPT-4, Claude, and LLaMA have enabled diverse task solutions without fine-tuning. Specialized pretraining methods like PixStruct (Lee et al., 2023), MatCha (Liu et al., 2022), and UniChart (Masry et al., 2023) enhance chart reasoning in visual contexts. Models like LLaVA, miniGPT4, InstructBLIP, and Bard leverage large-scale imagetext data, while specialized versions, such as LLaVAR (Zhang et al., 2023d; Ye et al., 2023), emphasize document understanding and math comprehension. Recent works (Bitton et al., 2023; Yu et al., 2023) evaluate instruction-following and reasoning capabilities, underscoring the growing importance of generative foundation models in practical applications. We introduce MATHVISTA as a benchmark to evaluate their math reasoning capabilities in varied visual contexts.</p>
<h2>5 CONCLUSION</h2>
<p>In this work, we introduce MATHVISTA, a benchmark designed to systematically analyze the mathematical reasoning capabilities of state-of-the-art models in visually complex scenarios. Our evaluation of 12 prominent foundation models highlights that significant advancements have been made, especially with the GPT-4V model. However, a substantial gap of $10.4 \%$ still exists between GPT4 V , the best-performing model, and human performance. This disparity sets a clear direction for future research, emphasizing the need for models that can seamlessly integrate mathematical reasoning with visual comprehension. Moreover, our exploration of GPT-4V's self-verification, selfconsistency, and chatbot interactions offers valuable insights for future investigations.</p>
<h2>REFERENCES</h2>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 20</p>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pp. 2357-2367, 2019. 10, 20</p>
<p>Anthropic. Claude 2, 2023. URL https://www.anthropic.com/index/claude-2. 6, 20</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 2425-2433, 2015. 10, 20, 27</p>
<p>Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 20</p>
<p>Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt. VisIT-Bench: A benchmark for vision-language instruction following inspired by real-world use. arXiv preprint arXiv:2308.06595, 2023. 10, 20</p>
<p>Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: WHOOPS! A vision-and-language benchmark of synthetic and compositional images. arXiv preprint arXiv:2303.07274, 2023. 20</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. 20</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 20</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. 20</p>
<p>Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th International Conference on Computational Linguistics, pp. 1511-1520, 2022. 20, 27</p>
<p>Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. MapQA: A dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545, 2022. 20, 27</p>
<p>Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. UniGeo: Unifying geometry logical reasoning via reformulating mathematical expression. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3313-3323, 2022a. 20, 27</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 20</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022b. 2, 6, 21</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang, and Pan Lu. TheoremQA: A theorem-driven question answering dataset. arXiv preprint arXiv:2305.12524, 2023. 21, 27</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 10, 20</p>
<p>Adam Dahlgren Lindström and Savitha Sam Abraham. CLEVR-Math: A dataset for compositional language, visual and mathematical reasoning. In 16th International Workshop on NeuralSymbolic Learning and Reasoning, NeSy 2022, Windsor, UK, september 28-30, 2022., volume 3212. CEUR-WS, 2022. 1, 20, 27</p>
<p>Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose visionlanguage models with instruction tuning, 2023. 6, 20, 39</p>
<p>Qingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao, Zhifang Sui, and Furu Wei. Large language model for science: A study on P vs. NP. arXiv preprint arXiv:2309.05689, 2023. 1</p>
<p>Iddo Drori and Nakul Verma. Solving linear algebra by program synthesis. arXiv preprint arXiv:2111.08171, 2021. 21</p>
<p>Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32):e2123433119, 2022. 21</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of chatgpt. In 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks, 2023. 10, 20</p>
<p>Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, et al. CodeApex: A bilingual programming evaluation benchmark for large language models. arXiv preprint arXiv:2309.01940, 2023. 20</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. LLaMA-Adapter V2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 6, 20</p>
<p>Google. Bard, 2023. URL https://bard.google.com/. 2, 6, 20
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904-6913, 2017. 20, 27</p>
<p>Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. VizWiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608-3617, 2018. 10, 20, 27</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022. 20</p>
<p>Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023. 20</p>
<p>JaidedAI. EasyOCR: Ready-to-use OCR, 2020. URL https://github.com/JaidedAI/ EasyOCR. 6</p>
<p>Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D Hawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. arXiv preprint arXiv:2211.16492, 2022. 20</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023. 9</p>
<p>Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. DVQA: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5648-5656, 2018. 20, 27</p>
<p>Daniel Kahneman. Thinking, fast and slow. macmillan, 2011. 1
Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. FigureQA: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. 10, 20, 27</p>
<p>Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14, pp. 235-251. Springer, 2016. 20, 27</p>
<p>Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? Textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pp. 4999-5007, 2017. 20, 27</p>
<p>Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):1-10, 2018. 20, 27</p>
<p>Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. OBELICS: An open web-scale filtered dataset of interleaved image-text documents, 2023. 6, 39</p>
<p>Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2Struct: Screenshot parsing as pretraining for visual language understanding. In International Conference on Machine Learning, pp. 18893-18912. PMLR, 2023. 10, 20</p>
<p>Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023a. 10</p>
<p>Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023b. 39</p>
<p>Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang Lyu, and Min Zhang. A comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering. arXiv preprint arXiv:2311.07536, 2023c. 39</p>
<p>Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan L Yuille. Super-CLEVR: A virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14963-14973, 2023d. 20, 27</p>
<p>Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are we learning yet? A meta review of evaluation failures across machine learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 20</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023. 1</p>
<p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740-755. Springer, 2014. 20</p>
<p>Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian Martin Eisenschlos. MatCha: Enhancing visual language pretraining with math reasoning and chart derendering. arXiv preprint arXiv:2212.09662, 2022. 10, 20</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023a. 6, 20</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. AgentBench: Evaluating LLMs as agents. arXiv preprint arXiv:2308.03688, 2023b. 20</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMBench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. 20</p>
<p>Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of OCR in large multimodal models. arXiv preprint arXiv:2305.07895, 2023d. 20</p>
<p>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. In The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021a. 1, $10,20,21,27$</p>
<p>Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. IconQA: A new benchmark for abstract diagram understanding and visual language reasoning. In The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021b. 20, 27</p>
<p>Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 6, 20, 27</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In The 37th Conference on Neural Information Processing Systems (NeurIPS), 2023a. 2, 37</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In International Conference on Learning Representations (ICLR), 2023b. 21, 27</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. In The 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023c. 9, 20</p>
<p>Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 2263-2279, 2022. 1, 10, 20, 27</p>
<p>Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. UniChart: A universal vision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023. 10, 20</p>
<p>Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. InfographicsVQA. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1697-1706, 2022. 20, 27</p>
<p>Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, and Pratyush Kumar. PlotQA: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1527-1536, 2020. 20, 27</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. LILA: A unified benchmark for mathematical reasoning. In The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. 10, 20</p>
<p>Shaghayegh Mobasher, Ghazal Zamaninejad, Maryam Hashemi, Melika Nobakhtian, and Sauleh Eetemadi. ParsVQA-Caps: A benchmark for visual question answering and image captioning in persian. people, 101:404, 2022. 10, 20</p>
<p>Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of GPT-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023. 20</p>
<p>OpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt. 2, 6, 20
OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023a. 2, 6, 20
OpenAI. GPT-4V(ision) system card, 2023b. URL https://openai.com/research/ gpt-4v-system-card. 2, 3</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023. 97</p>
<p>Rachel Pollitt, Caroline Cohrssen, and Wee Tiong Seah. Assessing spatial reasoning during play: Educator observations, assessment and curriculum planning. Mathematics Education Research Journal, 32(2):331-363, 2020. 1</p>
<p>Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278-25294, 2022. 20</p>
<p>Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pp. 146-162. Springer, 2022. 20, 27</p>
<p>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pp. 1466-1476, 2015. 1, 20, 27</p>
<p>Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. KVQA: Knowledgeaware visual question answering. In Proceedings of the AAAI conference on artificial intelligence, pp. 8876-8884, 2019. 20, 27</p>
<p>Wenqi Shao, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu, Siyuan Huang, Hongsheng Li, Yu Qiao, et al. Tiny LVLM-eHub: Early multimodal experiments with bard. arXiv preprint arXiv:2308.03729, 2023. 20</p>
<p>Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556-2565, 2018. 20</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 37</p>
<p>Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8317-8326, 2019. 20, 27</p>
<p>Deborah Stipek and Douglas Mac Iver. Developmental change in children's assessment of intellectual competence. Child development, pp. 521-538, 1989. 1</p>
<p>Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. SciEval: A multi-level large language model evaluation benchmark for scientific research. arXiv preprint arXiv:2308.13149, 2023. 20</p>
<p>Sanaz Talaifar and William B Swann. Self-verification theory. Encyclopedia of personality and individual differences, pp. 4813-4821, 2020. 97</p>
<p>John Chong Min Tan and Mehul Motani. Large language model (llm) as a system of multiple expert agents: An approach to solve the abstraction and reasoning corpus (arc) challenge. arXiv preprint arXiv:2310.05146, 2023. 21</p>
<p>Leonard Tang, Elizabeth Ke, Nikhil Singh, Bo Feng, Derek Austin, Nakul Verma, and Iddo Drori. Solving probability and statistics problems by probabilistic program synthesis at human level and predicting solvability. In International Conference on Artificial Intelligence in Education, pp. 612-615. Springer, 2022. 21</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. 1</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 20</p>
<p>Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476-482, 2024. 1</p>
<p>Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman. Hypothesis search: Inductive reasoning with language models. arXiv preprint arXiv:2309.05660, 2023a. 21</p>
<p>Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. SciBench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023b. 2, 20, 27</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 103</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 845-854, 2017. 1</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a. 20</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b. 2, 6, 21, 103</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. BloombergGPT: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023. 1</p>
<p>Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. LVLM-eHub: A comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023. 20</p>
<p>Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. FinGPT: Open-source financial large language models. arXiv preprint arXiv:2306.06031, 2023a. 1</p>
<p>Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The Dawn of LMMs: Preliminary explorations with gpt-4v(ision). arXiv preprint arXiv:2309.17421, 2023b. 6, 97</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPlug-Owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 6, 10, 20</p>
<p>Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860, 2021. 20</p>
<p>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-Vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 10, 20</p>
<p>Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6720-6731, 2019. 20</p>
<p>Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Qiao Yu. LLaMA-Adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023a. 20</p>
<p>Xiang Zhang, Senyu Li, Zijun Wu, and Ning Shi. Lost in translation: When gpt-4v (ision) can't see eye to eye with text. a vision-language-consistency analysis of vllms and beyond. arXiv preprint arXiv:2310.12520, 2023b. 21</p>
<p>Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. PMC-VQA: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023c. 20, 27</p>
<p>Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. LLaVAR: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023d. 6, 10, 20</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023a. 6, 20</p>
<p>Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open, billionscale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023b. 20</p>
<h1>CONTENTS</h1>
<p>A Detailed Related Work ..... 20
B Limitations of the Benchmark ..... 21
C Data Collection Guidelines ..... 22
C. 1 Mathematical Reasoning Definition ..... 22
C. 2 Mathematical Reasoning Examples ..... 23
C. 3 Visual Context Types ..... 24
C. 4 Source Dataset Summary ..... 27
D Data Collection Details ..... 28
D. 1 Automatic Selection of Mathematical Problems ..... 28
D. 2 Human Labeling of Mathematical Problems ..... 28
D. 3 Annotating Three New Datasets ..... 29
D. 4 Human Labeling of Mathematical Reasoning ..... 29
E More Dataset Analysis ..... 30
F More Details on the Setup ..... 33
F. 1 Frequent Guess ..... 33
F. 2 Prompt for Answer Extraction ..... 33
F. 3 Prompts for Response Generation ..... 34
F. 4 Prompt for Caption Generation ..... 34
F. 5 Model Hyperparameters ..... 34
F. 6 Human Performance ..... 34
F. 7 Multimodal Bard Assessment Task ..... 35
G More Experimental Results ..... 36
G. 1 Results on the Test Set ..... 36
G. 2 Scores for Math Reasoning Types ..... 36
G. 3 Scores for Various Visual Contexts ..... 37
G. 4 Scores Across Different Grade Levels ..... 37
G. 5 Ablation Study for LLMs ..... 38
G. 6 LLMs with Different Shots ..... 39
G. 7 LMMs with Different Shots ..... 39
G. 8 Hallucinations in Model Explanations ..... 40
G. 9 More Examples for Multimodal Bard ..... 41
G. 10 Comparisons of Different Models ..... 47</p>
<p>H A Comparative Study of GPT-4V, Bard, and Other Models ..... 54
H. 1 GPT-4V Playground for Manual Evaluation ..... 54
H. 2 Leaderboard Scores ..... 55
H. 3 Abilities in Mathematical Reasoning ..... 56
H.3.1 Algebraic Reasoning ..... 56
H.3.2 Arithmetic Reasoning ..... 59
H.3.3 Geometry Reasoning ..... 61
H.3.4 Logical Reasoning ..... 63
H.3.5 Numeric Commonsense Reasoning ..... 66
H.3.6 Scientific Reasoning ..... 69
H.3.7 Statistical Reasoning ..... 72
H. 4 Abilities Across Visual Contexts ..... 74
H.4.1 Abstract Scene ..... 74
H.4.2 Bar Chart ..... 76
H.4.3 Function Plot ..... 77
H.4.4 Geometry Diagram ..... 79
H.4.5 Line Plot ..... 81
H.4.6 Natural Image ..... 83
H.4.7 Puzzle Test ..... 85
H.4.8 Scatter Plot ..... 87
H.4.9 Scientific Scene ..... 89
H.4.10 Synthetic Scene ..... 92
H.4.11 Table ..... 94
H.4.12 Other Visual Contexts ..... 96
H. 5 Self-Verification in GPT-4V ..... 97
H. 6 Self-Consistency for GPT-4V ..... 103
H. 7 GPT-4V for Multi-Turn Human-AI Interaction ..... 109</p>
<h1>A Detailed Related Work</h1>
<p>Mathematical reasoning benchmarks. Recently, numerous benchmarks (Amini et al., 2019; Cobbe et al., 2021; Mishra et al., 2022; Frieder et al., 2023) have been proposed to evaluate the mathematical reasoning capabilities of Large Language Models (LLMs). However, most of these are textual only (Lu et al., 2023c), despite a substantial amount of mathematical information and reasoning being encapsulated in visual modalities. Meanwhile, some datasets exhibit performance saturation; for instance, GPT-4 achieves $92.0 \%$ accuracy on GSM-8K (Cobbe et al., 2021), a dataset of gradeschool mathematics questions. On the other hand, the recent rapid advancement of Large Multimodal Models (LMMs) necessitates the establishment of robust multimodal benchmarks. However, current multimodal reasoning benchmarks provide limited coverage of rigorous and scientific domains (Antol et al., 2015; Kembhavi et al., 2016; Kahou et al., 2017; Mathew et al., 2022), which are key components for creating general-purpose AI assistants. To bridge this gap, it is crucial to develop a robust math reasoning dataset that integrates visual contexts.</p>
<p>Vision-language reasoning benchmarks. High-quality evaluation datasets and benchmarks are a cornerstone for assessing the progress of machine learning models to solve real-world tasks Liao et al. (2021). Prior studies such as VQA (Antol et al., 2015; Goyal et al., 2017), VizWiz (Gurari et al., 2018), and ParsVQA-Caps (Mobasher et al., 2022) assess the general-purpose visual question answering abilities of the LMMs, with or without task-specific training, on open-ended questions about images. In addition, there are several works that focus on evaluating specific skills of the LMMs beyond natural scenes, such as abstract scenes and shapes) (Antol et al., 2015; Lu et al., 2021b; Ji et al., 2022), geometry diagrams (Seo et al., 2015; Lu et al., 2021a; Chen et al., 2022a; Cao \&amp; Xiao, 2022), figures and charts (Methani et al., 2020; Masry et al., 2022; Kahou et al., 2017; Chang et al., 2022; Kafle et al., 2018), documents (text in images) (Singh et al., 2019; Mathew et al., 2022; Liu et al., 2023d), or synthetic images (Dahlgren Lindström \&amp; Abraham, 2022; Li et al., 2023d; Bitton-Guetta et al., 2023). Besides, there has been significant progress on developing datasets to judge LMMs on skills that require external knowledge (Schwenk et al., 2022; Shah et al., 2019), common sense reasoning (Zellers et al., 2019; Yin et al., 2021), scientific-knowledge (Lu et al., 2022; Kembhavi et al., 2017; 2016), medical understanding (Zhang et al., 2023c; Lau et al., 2018). In this work, we create new datasets (IQTest, FunctionQA, PaperQA) and subsequently design a benchmark for holistic evaluation of the math reasoning capabilities of the LMMs.</p>
<p>Generative foundation models and their evaluation. Recently, there has been a surge of generative foundation models (Bommasani et al., 2021) that are trained on web-scale data, such as GPT-3, ChatGPT, GPT-4, Claude, LLaMA, LLaMA-Adapter (Brown et al., 2020; OpenAI, 2022; 2023a; Anthropic, 2023; Touvron et al., 2023; Zhang et al., 2023a), with the ability to solve a wide range of downstream tasks (Wei et al., 2022a) without any task-specific finetuning. Prior work has focused on evaluating their abilities to respond to the queries from various disciplines, grounded in text, such as QA, math, medicine, coding and science (Bubeck et al., 2023; Nori et al., 2023; Chen et al., 2021; Fu et al., 2023; Sun et al., 2023; Wang et al., 2023b; Huang et al., 2023; 2022; Liu et al., 2023b; Zhang et al., 2023a). Prior work, such as PixStruct (Lee et al., 2023), MatCha (Liu et al., 2022), and UniChart (Masry et al., 2023), has focused on developing specialized pretraining recipe for improved math and chart reasoning in visual contexts.</p>
<p>On the vision-language side, there are several generative foundation models such as LLaVA, miniGPT4, InstructBLIP, Flamingo, LLaMA-Adapter V2, Multimodal Bard (Liu et al., 2023a; Zhu et al., 2023a; Dai et al., 2023; Alayrac et al., 2022; Awadalla et al., 2023; Gao et al., 2023; Google, 2023) that are trained on vast amount of paired (Schuhmann et al., 2022; Sharma et al., 2018; Lin et al., 2014) and interleaved image-text data (Zhu et al., 2023b). In addition, there has been recent development on specialized versions of these LMMs for document understanding where visual contexts require text recognition, math understanding being one of them (Zhang et al., 2023d; Ye et al., 2023). In recent times, there have been several works, such as Visit-Bench, LVLM-eHub, MMBench (Bitton et al., 2023; Yu et al., 2023; Liu et al., 2023c; Xu et al., 2023; Shao et al., 2023), that assess their instruction-following and reasoning capabilities. As the generative foundation models become more relevant to real-world applications, unlike prior work, we propose MATHVISTA to benchmark their capabilities of math reasoning (logical, arithmetic, statistical) on a diverse set of visual contexts (word problems in images, natural scenes, geometrical shapes, and plots).</p>
<p>Recent work of LLM prompting and GPT-4V. We have witnessed the remarkable abilities of large language models (LLMs), and their reasoning capabilities are further enhanced by promoting approaches such as chain-of-thought (CoT) (Wei et al., 2022b), program-of-thought (PoT) (Chen et al., 2022b), and inductive reasoning (Wang et al., 2023a; Tan \&amp; Motani, 2023). For example, the feasibility of using LLMs to solve the Abstraction and Reasoning Corpus (ARC) challenge has been verified using zero-shot, few-shot, and context-grounded prompting (Tan \&amp; Motani, 2023). In this paper, we evaluate LLMs using zero-shot, few-shot, CoT prompting, PoT prompting, as well as tool-augmented prompting, to explore their potential in solving mathematical reasoning in visual contexts on MATH Vista. Program-aided methods are widely used for mathematical reasoning due to their advancements in precise logical reasoning and arithmetic calculations (Drori \&amp; Verma, 2021; Tang et al., 2022; Drori et al., 2022). In this work, we have developed the LLM baselines with PoT.</p>
<p>Recently, OpenAI released GPT-4V, the multimodal version of GPT-4, which shows promising performance in vision-language reasoning. However, the fine-grained study of its strengths and limitations still remains underexplored. The recent work (Zhang et al., 2023b) contributes pioneering efforts in this field, studying whether large multimodal models (LMMs), like GPT-4V, execute vision and language tasks consistently or independently. As concurrent work, our paper provides, for the first time, a comprehensive quantitative and qualitative study of GPT-4V and other LLMs in mathematical reasoning within visual contexts.</p>
<h1>B LIMITATIONS OF THE BENCHMARK</h1>
<p>Our benchmark, MATH Vista, makes significant contributions by combining mathematical and visual tasks, a domain where existing models like GPT-4V have shown promise but also face challenges, especially in complex figure understanding and rigorous reasoning. While we have made strides in evaluating model performance, we acknowledge several limitations.</p>
<p>One limitation is the dataset coverage. While MATH Vista encompasses a broad spectrum of tasks and visual contexts, there may be gaps in the representation of certain types of mathematical problems and visuals. Furthermore, the dataset's focus on mathematical reasoning within visual contexts, spanning specific domains like science and college-level math, necessitates a more labor-intensive process for collecting high-quality data compared to textual-only or general-purpose datasets. Thus, the scalability and generalizability of our benchmark to other domains remain a concern. Annotations were sourced from original data providers, resulting in only $85.6 \%$ of examples (Table 1) having annotations. Due to the heterogeneity of these sources, annotations lack a unified format and structure. For example, the annotations could be logic forms of the problem parsing from Geometry3K (Lu et al., 2021a), natural language solutions from TabMWP (Lu et al., 2023b), and theorems from TheoremQA (Chen et al., 2023). Given the rapid development in foundation models, our study focused exclusively on the most recent and prominent models.</p>
<p>In future iterations, our benchmark will be beneficial to encompass a broader array of problems and visual contexts, while also providing unified and comprehensive annotations. Our benchmark is part of an ongoing research process, and we are committed to maintaining the datasets, such as refining the potential data noise, in response to the community feedback. Also, we are committed to evolving the leaderboard in response to new models.</p>
<p>In conclusion, while there are limitations to our current approach, MATH Vista represents a significant step forward in the field. We are dedicated to continuously improving our benchmark to better understand and enhance the capabilities of AI in mathematical and visual reasoning.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/papers&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
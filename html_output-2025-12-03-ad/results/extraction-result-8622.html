<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8622 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8622</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8622</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-277501965</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.01389v1.pdf" target="_blank">De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning</a></p>
                <p><strong>Paper Abstract:</strong> De novo molecular design has extensive applications in drug discovery and materials science. The vast chemical space renders direct molecular searches computationally prohibitive, while traditional experimental screening is both time- and labor-intensive. Efficient molecular generation and screening methods are therefore essential for accelerating drug discovery and reducing costs. Although reinforcement learning (RL) has been applied to optimize molecular properties via reward mechanisms, its practical utility is limited by issues in training efficiency, convergence, and stability. To address these challenges, we adopt Direct Preference Optimization (DPO) from NLP, which uses molecular score-based sample pairs to maximize the likelihood difference between high- and low-quality molecules, effectively guiding the model toward better compounds. Moreover, integrating curriculum learning further boosts training efficiency and accelerates convergence. A systematic evaluation of the proposed method on the GuacaMol Benchmark yielded excellent scores. For instance, the method achieved a score of 0.883 on the Perindopril MPO task, representing a 6\% improvement over competing models. And subsequent target protein binding experiments confirmed its practical efficacy. These results demonstrate the strong potential of DPO for molecular design tasks and highlight its effectiveness as a robust and efficient solution for data-driven drug discovery.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8622.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8622.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolRL-DPO+CL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>De Novo Molecular Design via Direct Preference Optimization and Curriculum Learning (multi-agent GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's primary system: a multi-agent autoregressive GPT-based SMILES generator (8 layers, 8 attention heads) pretrained on large molecular datasets and fine-tuned with Direct Preference Optimization (DPO) combined with curriculum learning to produce task-specific molecules for drug discovery and target-binding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multi-agent GPT (MolRL-DPO+CL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer (GPT-style), multi-agent</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Two priors: (1) GuacaMol training subset of ChEMBL for benchmark tasks; (2) ZINC (~100M molecules) for target-binding tasks; trained on SMILES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / de novo molecular design (GuacaMol benchmark, target protein binding tasks JNK3, GSK3B, DRD2 and combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive SMILES generation from pretrained GPT prior; multi-agent sampling to produce candidate molecules; task-specific scoring (computational chemistry library / oracle) to build preference pairs; Direct Preference Optimization (DPO) contrastive fine-tuning with curriculum learning (progressively narrowing score gaps and multi-stage memory-based reinitialization).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Paper reports high validity (97% GuacaMol prior, 99.6% ZINC prior) and improved benchmark/task scores but does not provide explicit novelty statistics (e.g., % not in training set or Tanimoto similarities). Novelty is implied via improved task scores and diversity metrics in GuacaMol tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Task-specificity enforced by evaluating generated molecules with task-specific scoring functions (GuacaMol task objectives, TDC docking/oracle for target binding), selecting top-k scoring molecules as 'preferred' in pair construction so optimization aligns generation with target properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>GuacaMol task scores (20 tasks, MPOs, similarity/rediscovery tasks), validity rate, top-k/top-100 aggregated scores reported (e.g., Top-10 and Top-100 distributions), docking/oracle scores from TDC for protein binding tasks, and wall-clock training time comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Outperformed baselines on 16/20 GuacaMol tasks (total score improvements reported), reached 0.883 on Perindopril MPO (reported ~6% improvement), achieved high docking/task scores (e.g., GSK3B+DRD2 0.993 top-1), and much faster training (60 hours vs MolRL-MGPT 400 hours on a single A100). Generated molecules showed strong task-aligned scores (several top-1 scores of 1.0 for single-target tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against SMILES LSTM, Graph GA, REINVENT, GEGL, MolRL-MGPT; reported superior or more stable performance across most GuacaMol tasks and faster convergence (claimed ~6x speedup over MolRL-MGPT). Authors report better stability than GEGL and improved top-task performance over MolRL-MGPT on several MPOs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>No explicit large-scale novelty/similarity metrics reported; relies on computed scoring/oracle functions rather than human annotations (authors note this as simplification); sensitivity to hyperparameters (DPO β, learning rate), number of agents, and sampling-to-training ratio; potential early-exploration bias mitigated by memory-based multi-stage reinitialization; model size and full compute scaling details not reported; general RL-type limitations (choice of scoring functions and exploration/exploitation trade-offs) acknowledged.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8622.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8622.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolRL-MGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolRL-MGPT (GPT-based generative model with reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published GPT-based molecular generative approach that integrates GPT-style generation with reinforcement learning to improve molecular diversity and optimize target-directed properties; used in this paper as a primary baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>De novo drug design using reinforcement learning with multiple gpt agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolRL-MGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-style autoregressive transformer integrated with reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular design for drug discovery, benchmark and target-directed optimization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>GPT-based SMILES generation coupled with reinforcement learning optimization (multi-agent reinforcement setup in the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in this paper; reported by authors of that prior work to improve diversity/targeted properties, but this paper only reports comparative benchmark outcomes and compute times.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Task-specific optimization via reinforcement learning reward functions in the original MolRL-MGPT framework; in this paper used as a baseline on GuacaMol tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>GuacaMol task scores and training time; MolRL-MGPT reported as achieving strong performance but required substantially more compute (400 hours on a single A100 under comparison conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used as a strong prior baseline (previous state-of-the-art for some tasks); in this paper MolRL-MGPT's training time is much longer (400h) and the DPO+CL method claims superior scores on multiple tasks beyond MolRL-MGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared in tables and text: MolRL-MGPT required longer training and was outperformed by the DPO+CL approach on multiple GuacaMol tasks (DPO+CL improved some MPOs beyond MolRL-MGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Reported by this paper to have slower training and longer convergence (400h vs 60h reported for DPO+CL under same hardware), and in some tasks appears to be near a performance plateau (small incremental gains reported on certain MPO tasks). Detailed architecture/training specifics are not reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8622.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8622.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptations of ChatGPT for molecular tasks (conversational LLM applied to chemical representation and editing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work explored adapting conversational large language models (ChatGPT) to molecular tasks, showing the model can capture chemical patterns and generate valid molecular representations via language modeling and retrieval/feedback pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt-powered conversational drug editing using retrieval and domain feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>large conversational transformer (GPT-family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug design / molecular editing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM adaptation and prompt/retrieval-based pipelines to generate and edit SMILES or molecular representations; some works combine retrieval and domain feedback to guide edits.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in this paper; prior works are reported to produce valid molecular representations and demonstrate potential for chemical editing, but specific novelty metrics are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Addressed via retrieval of domain knowledge and domain feedback loops in referenced work; the paper notes ChatGPT adaptations capture chemical patterns but does not present task-specific results in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Mentioned qualitatively (ability to capture chemical patterns and generate valid representations); referenced work likely uses validity and task-specific metrics but this paper does not report them directly.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as evidence that cross-domain adaptation of LLMs to molecular tasks is promising; no direct experimental results in this manuscript beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned alongside GPT-based generative RL approaches as part of the broader trend of LLM cross-domain adaptation for molecular design; not directly compared in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Only qualitatively discussed: adaptation is possible but specifics (e.g., precise control over properties, synthesizability, hallucination risk) are not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>De novo drug design using reinforcement learning with multiple gpt agents. <em>(Rating: 2)</em></li>
                <li>Chatgpt-powered conversational drug editing using retrieval and domain feedback. <em>(Rating: 2)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model. <em>(Rating: 2)</em></li>
                <li>Aligning protein generative models with experimental fitness via direct preference optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8622",
    "paper_id": "paper-277501965",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "MolRL-DPO+CL",
            "name_full": "De Novo Molecular Design via Direct Preference Optimization and Curriculum Learning (multi-agent GPT)",
            "brief_description": "This paper's primary system: a multi-agent autoregressive GPT-based SMILES generator (8 layers, 8 attention heads) pretrained on large molecular datasets and fine-tuned with Direct Preference Optimization (DPO) combined with curriculum learning to produce task-specific molecules for drug discovery and target-binding tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multi-agent GPT (MolRL-DPO+CL)",
            "model_type": "autoregressive transformer (GPT-style), multi-agent",
            "model_size": null,
            "training_data": "Two priors: (1) GuacaMol training subset of ChEMBL for benchmark tasks; (2) ZINC (~100M molecules) for target-binding tasks; trained on SMILES strings.",
            "application_domain": "drug discovery / de novo molecular design (GuacaMol benchmark, target protein binding tasks JNK3, GSK3B, DRD2 and combinations).",
            "generation_method": "Autoregressive SMILES generation from pretrained GPT prior; multi-agent sampling to produce candidate molecules; task-specific scoring (computational chemistry library / oracle) to build preference pairs; Direct Preference Optimization (DPO) contrastive fine-tuning with curriculum learning (progressively narrowing score gaps and multi-stage memory-based reinitialization).",
            "novelty_of_chemicals": "Paper reports high validity (97% GuacaMol prior, 99.6% ZINC prior) and improved benchmark/task scores but does not provide explicit novelty statistics (e.g., % not in training set or Tanimoto similarities). Novelty is implied via improved task scores and diversity metrics in GuacaMol tasks.",
            "application_specificity": "Task-specificity enforced by evaluating generated molecules with task-specific scoring functions (GuacaMol task objectives, TDC docking/oracle for target binding), selecting top-k scoring molecules as 'preferred' in pair construction so optimization aligns generation with target properties.",
            "evaluation_metrics": "GuacaMol task scores (20 tasks, MPOs, similarity/rediscovery tasks), validity rate, top-k/top-100 aggregated scores reported (e.g., Top-10 and Top-100 distributions), docking/oracle scores from TDC for protein binding tasks, and wall-clock training time comparisons.",
            "results_summary": "Outperformed baselines on 16/20 GuacaMol tasks (total score improvements reported), reached 0.883 on Perindopril MPO (reported ~6% improvement), achieved high docking/task scores (e.g., GSK3B+DRD2 0.993 top-1), and much faster training (60 hours vs MolRL-MGPT 400 hours on a single A100). Generated molecules showed strong task-aligned scores (several top-1 scores of 1.0 for single-target tasks).",
            "comparison_to_other_methods": "Compared against SMILES LSTM, Graph GA, REINVENT, GEGL, MolRL-MGPT; reported superior or more stable performance across most GuacaMol tasks and faster convergence (claimed ~6x speedup over MolRL-MGPT). Authors report better stability than GEGL and improved top-task performance over MolRL-MGPT on several MPOs.",
            "limitations_and_challenges": "No explicit large-scale novelty/similarity metrics reported; relies on computed scoring/oracle functions rather than human annotations (authors note this as simplification); sensitivity to hyperparameters (DPO β, learning rate), number of agents, and sampling-to-training ratio; potential early-exploration bias mitigated by memory-based multi-stage reinitialization; model size and full compute scaling details not reported; general RL-type limitations (choice of scoring functions and exploration/exploitation trade-offs) acknowledged.",
            "uuid": "e8622.0",
            "source_info": {
                "paper_title": "De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MolRL-MGPT",
            "name_full": "MolRL-MGPT (GPT-based generative model with reinforcement learning)",
            "brief_description": "A previously published GPT-based molecular generative approach that integrates GPT-style generation with reinforcement learning to improve molecular diversity and optimize target-directed properties; used in this paper as a primary baseline.",
            "citation_title": "De novo drug design using reinforcement learning with multiple gpt agents.",
            "mention_or_use": "mention",
            "model_name": "MolRL-MGPT",
            "model_type": "GPT-style autoregressive transformer integrated with reinforcement learning",
            "model_size": null,
            "training_data": null,
            "application_domain": "de novo molecular design for drug discovery, benchmark and target-directed optimization",
            "generation_method": "GPT-based SMILES generation coupled with reinforcement learning optimization (multi-agent reinforcement setup in the referenced work).",
            "novelty_of_chemicals": "Not quantified in this paper; reported by authors of that prior work to improve diversity/targeted properties, but this paper only reports comparative benchmark outcomes and compute times.",
            "application_specificity": "Task-specific optimization via reinforcement learning reward functions in the original MolRL-MGPT framework; in this paper used as a baseline on GuacaMol tasks.",
            "evaluation_metrics": "GuacaMol task scores and training time; MolRL-MGPT reported as achieving strong performance but required substantially more compute (400 hours on a single A100 under comparison conditions).",
            "results_summary": "Used as a strong prior baseline (previous state-of-the-art for some tasks); in this paper MolRL-MGPT's training time is much longer (400h) and the DPO+CL method claims superior scores on multiple tasks beyond MolRL-MGPT.",
            "comparison_to_other_methods": "Compared in tables and text: MolRL-MGPT required longer training and was outperformed by the DPO+CL approach on multiple GuacaMol tasks (DPO+CL improved some MPOs beyond MolRL-MGPT).",
            "limitations_and_challenges": "Reported by this paper to have slower training and longer convergence (400h vs 60h reported for DPO+CL under same hardware), and in some tasks appears to be near a performance plateau (small incremental gains reported on certain MPO tasks). Detailed architecture/training specifics are not reproduced in this paper.",
            "uuid": "e8622.1",
            "source_info": {
                "paper_title": "De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ChatGPT (adaptation)",
            "name_full": "Adaptations of ChatGPT for molecular tasks (conversational LLM applied to chemical representation and editing)",
            "brief_description": "Prior work explored adapting conversational large language models (ChatGPT) to molecular tasks, showing the model can capture chemical patterns and generate valid molecular representations via language modeling and retrieval/feedback pipelines.",
            "citation_title": "Chatgpt-powered conversational drug editing using retrieval and domain feedback.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (adapted)",
            "model_type": "large conversational transformer (GPT-family)",
            "model_size": null,
            "training_data": null,
            "application_domain": "drug design / molecular editing and generation",
            "generation_method": "LLM adaptation and prompt/retrieval-based pipelines to generate and edit SMILES or molecular representations; some works combine retrieval and domain feedback to guide edits.",
            "novelty_of_chemicals": "Not quantified in this paper; prior works are reported to produce valid molecular representations and demonstrate potential for chemical editing, but specific novelty metrics are not provided here.",
            "application_specificity": "Addressed via retrieval of domain knowledge and domain feedback loops in referenced work; the paper notes ChatGPT adaptations capture chemical patterns but does not present task-specific results in this manuscript.",
            "evaluation_metrics": "Mentioned qualitatively (ability to capture chemical patterns and generate valid representations); referenced work likely uses validity and task-specific metrics but this paper does not report them directly.",
            "results_summary": "Cited as evidence that cross-domain adaptation of LLMs to molecular tasks is promising; no direct experimental results in this manuscript beyond citation.",
            "comparison_to_other_methods": "Mentioned alongside GPT-based generative RL approaches as part of the broader trend of LLM cross-domain adaptation for molecular design; not directly compared in experiments here.",
            "limitations_and_challenges": "Only qualitatively discussed: adaptation is possible but specifics (e.g., precise control over properties, synthesizability, hallucination risk) are not evaluated in this paper.",
            "uuid": "e8622.2",
            "source_info": {
                "paper_title": "De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "De novo drug design using reinforcement learning with multiple gpt agents.",
            "rating": 2,
            "sanitized_title": "de_novo_drug_design_using_reinforcement_learning_with_multiple_gpt_agents"
        },
        {
            "paper_title": "Chatgpt-powered conversational drug editing using retrieval and domain feedback.",
            "rating": 2,
            "sanitized_title": "chatgptpowered_conversational_drug_editing_using_retrieval_and_domain_feedback"
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model.",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Aligning protein generative models with experimental fitness via direct preference optimization",
            "rating": 1,
            "sanitized_title": "aligning_protein_generative_models_with_experimental_fitness_via_direct_preference_optimization"
        }
    ],
    "cost": 0.0103245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning
2 Apr 2025</p>
<p>Junyu Hou 221220151@nju.edu.cn 
Nanjing University</p>
<p>De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning
2 Apr 20251F0A02F9A7900DE38753AE0AF2C0CCB2arXiv:2504.01389v1[cs.LG]De Novo Molecular DesignDPOCurriculum Learning
De novo molecular design has extensive applications in drug discovery and materials science.The vast chemical space renders direct molecular searches computationally prohibitive, while traditional experimental screening is both time-and labor-intensive.Efficient molecular generation and screening methods are therefore essential for accelerating drug discovery and reducing costs.Although reinforcement learning (RL) has been applied to optimize molecular properties via reward mechanisms, its practical utility is limited by issues in training efficiency, convergence, and stability.To address these challenges, we adopt Direct Preference Optimization (DPO) from NLP, which uses molecular scorebased sample pairs to maximize the likelihood difference between highand low-quality molecules, effectively guiding the model toward better compounds.Moreover, integrating curriculum learning further boosts training efficiency and accelerates convergence.A systematic evaluation of the proposed method on the GuacaMol Benchmark yielded excellent scores.For instance, the method achieved a score of 0.883 on the Perindopril MPO task, representing a 6% improvement over competing models.And subsequent target protein binding experiments confirmed its practical efficacy.These results demonstrate the strong potential of DPO for molecular design tasks and highlight its effectiveness as a robust and efficient solution for data-driven drug discovery.</p>
<p>Introduction</p>
<p>De novo molecular design is one of the core tasks in fields such as catalyst design, energy materials design, and pharmaceutical research, aiming to generate novel molecules from scratch that satisfy specified physicochemical properties and biological activity requirements [21].This process plays a pivotal role in new drug discovery, materials science, and synthetic chemistry.Traditional methods for candidate molecule screening and optimization typically rely on extensive experimental synthesis and biological assays, which are both time-and labor-intensive and require substantial financial investment [7].Moreover, the chemical space is astronomically vast-estimated to contain more than 10 60 potential molecules [4] -making exhaustive exploration by manual means virtually impossible.Consequently, computer-aided drug design (CADD) has emerged as a prominent research focus, leveraging mathematical models, statistical techniques, and advanced computational technologies to efficiently search and optimize within this enormous chemical space [28,8,31,33].</p>
<p>In recent years, molecular conditional generation has played a critical role in drug development and materials design [22], and reinforcement learning (RL) has gradually been introduced into the field of molecular design [24,25,16].By employing molecular scoring functions as reward signals, RL enables generative models to continuously adjust and improve their generation strategies toward predefined objectives-such as enhancing biological activity, optimizing physicochemical properties, and improving synthetic accessibility-thereby demonstrating enormous potential [30,34].</p>
<p>However, RL-based methods still face several challenges: (1) Convergence Challenges and Training Instability: The high-dimensional and non-convex nature of molecular generation makes RL models prone to slow convergence and local optima.For instance, REINVENT [3] exhibits volatile policy updates and noisy rewards, requiring extensive training before reliably generating molecules that satisfy multiple objectives.</p>
<p>(2) Exploration Inefficiency and Limited Coverage: The vast chemical space limits the effectiveness of traditional RL approaches, which often get trapped in narrow structural regions.DrugEx [18], for example, generates molecules meeting specific activity criteria but lacks sufficient scaffold diversity.</p>
<p>(3) Multi-Objective Optimization and Reward Design Challenges: Designing effective reward functions for molecular optimization is complex and often requires empirical tuning.For example, to simultaneously maximize logP, TPSA, and structural similarity, some studies have employed multi-layered, nonlinear composite reward models, increasing both implementation complexity and limiting generalizability across different tasks [15,29,27].</p>
<p>To address these challenges, we draw inspiration from two established methodologies in machine learning.Direct Preference Optimization (DPO), originally developed in NLP, has shown strong optimization capabilities in reinforcement learning tasks by leveraging paired samples to optimize likelihood differences, eliminating the need for explicit reward modeling [26,6].Meanwhile, Curriculum Learning, which gradually increases task complexity, has been adopted to enhance molecular generation [2,11].By starting with simpler tasks and progressively optimizing bioactivity, physicochemical properties, and synthetic feasibility, this approach improves model learning [23,10].We contend that integrating DPO with curriculum learning can both accelerate model convergence and substantially improve the overall performance of the generated molecules.</p>
<p>In this study, we first employ traditional autoregressive training to train a prior model, which is then assigned to four agent models.These agent models are responsible for sampling and constructing paired samples, which are subsequently trained using DPO and curriculum learning to optimize the molecular generation process, ultimately yielding molecules that satisfy the desired proper- The model is initially pre-trained, followed by optimization using Direct Preference Optimization.As curriculum learning progresses, the molecular scores of the collected compounds steadily increase while the distinction between superior and inferior molecules gradually narrows.Ultimately, the process yields molecules that meet the predefined quality criteria.ties.We evaluate our method on the Guacamol benchmark [5] and target protein binding experiments, where experimental results demonstrate that our approach achieves superior performance across multiple evaluation metrics, validating its effectiveness in molecular conditional generation tasks.</p>
<p>The primary objective of this study is to investigate the application of DPO combined with curriculum learning in molecular conditional generation, aiming to improve the molecular generation process through a more efficient and stable approach.By integrating these two methods, we aim to improve molecular discovery and optimization efficiency while providing strong technological support for drug development.Our main contributions are threefold:</p>
<p>-We propose a novel de novo molecular design framework that combines Direct Preference Optimization (DPO) with curriculum learning.</p>
<p>-Our method has achieved high scores on the GuacaMol benchmark and demonstrated outstanding performance in target protein docking experiments.</p>
<p>-The proposed framework exhibits strong potential for scalability in terms of multi-objective optimization, training stability, and computational efficiency.</p>
<p>Related Work</p>
<p>In recent years, computationally driven de novo molecular design has witnessed rapid advancements, primarily evolving along three key directions: (1) continuous optimization of reinforcement learning frameworks, (2) crossdomain adaptation of large language models (LLMs), and (3) efficiency improvements in preference learning paradigms.These innovations have enhanced chemical space exploration and multi-objective optimization, laying a stronger foundation for drug discovery.</p>
<p>Reinforcement Learning-Based Molecular Generation</p>
<p>Reinforcement learning (RL)-based generative models have established a systematic paradigm for molecular design [19,34].Among them, REINVENT [20] integrates recurrent neural networks (RNNs) with policy gradient algorithms to enable targeted chemical space exploration.Another notable approach [24] leverages prior knowledge to constrain the reward function, optimizing molecular properties while maintaining synthetic feasibility.However, traditional RL approaches face challenges when handling high-dimensional chemical spaces, including inefficient policy updates and susceptibility to local optima.</p>
<p>Curriculum Learning Strategies</p>
<p>To enhance training efficiency in complex tasks, researchers have introduced curriculum learning frameworks into de novo molecular design.Guo et al. [10] proposed a strategy that gradually increases task difficulty: the model initially focuses on generating simpler chemical structures, thereby establishing a solid foundation, and then progressively tackles more challenging optimization tasks.This staged approach not only accelerates convergence but also improves the diversity and quality of the generated molecules, demonstrating the effectiveness of curriculum learning in refining generative models for molecular design.</p>
<p>Large Language Models for Molecular Generation</p>
<p>Large Language Models (LLMs) have recently been applied to molecular design, offering novel strategies for molecule generation.Liu et al. [17] explored the adaptation of ChatGPT for molecular tasks, demonstrating its ability to capture chemical patterns and generate valid molecular representations through language modeling.Similarly, Hu et al. [12] introduced MolRL-MGPT, which integrates a GPT-based generative strategy with reinforcement learning to enhance molecular diversity and optimize target-directed properties.These studies highlight the promising potential of LLMs to provide scalable and effective approaches for molecular design.</p>
<p>Preference Optimization in Molecular Design</p>
<p>Preference learning techniques provide an efficient pathway for strategy optimization in molecular generation.Rafailov et al. proposed direct preference optimization (DPO) [26], which employs implicit reward modeling to bypass the complexity of explicit reward function design in traditional RL.This paradigm has recently been successfully adapted to molecular design [9,6]: Widatalla et al. utilized experimental data to construct preference pairs, enabling DPO to directly optimize protein stability [32].Experimental results show that ProteinDPO performs exceptionally well in protein stability prediction and demonstrates strong generalization capabilities for large proteins and multi-chain complexes.This suggests that it has effectively learned transferable insights from its biophysical alignment data.</p>
<p>Methodology</p>
<p>Our molecular generation framework is built upon three core technical components integrated through a structured training pipeline: (1) Pretraining establishes chemical validity by learning SMILES syntax from large-scale datasets;</p>
<p>(2) Direct Preference Optimization (DPO) replaces reward modeling with contrastive learning to align generation with target objectives; (3) Curriculum Learning introduces progressive difficulty levels for gradual chemical space exploration.</p>
<p>To synergistically combine these components, we design a two-stage training procedure: pretraining initializes molecular priors, followed by DPO fine-tuning guided by curriculum-constructed preference pairs.The following subsections detail each component.</p>
<p>Pretrain on large molecular dataset</p>
<p>In this study, we adopt the same model architecture as MolRL-MGPT by building a multi-agent GPT model with 8 layers and 8 attention heads [12].Two distinct prior models were pre-trained on different datasets: one on the GuacaMol dataset (a subset of ChEMBL) for benchmark evaluation and another on the ZINC dataset (containing approximately 100 M molecules) for general-purpose molecular generation tasks [13].The primary objective during pretraining is to enable the model to deeply learn the syntax rules of SMILES representations, thereby allowing it to generate valid SMILES structures one character at a time and effectively capture the distribution of the chemical space.This pretraining strategy not only demonstrates the model's capability in efficiently generating chemically valid molecules but also lays a solid foundation for subsequent taskspecific optimization and performance enhancement.</p>
<p>To rigorously train our prior models on SMILES representations, we employ an autoregressive framework that decomposes each sequence into a series of incremental prediction tasks.Let S = (c 1 , c 2 , . . ., c L ) denote a SMILES sequence of length L, where each c i represents a character from the SMILES vocabulary V.In our autoregressive training approach, we generate training pairs (x i , y i ) for i = 1, 2, . . ., L − 1, where
x i = (c 1 , c 2 , . . . , c i ) and y i = (c 1 , c 2 , . . . , c i , c i+1 ).(1)
The model parameterized by θ learns a conditional probability distribution P θ (c i+1 | c 1 , . . ., c i ) such that the joint probability of the sequence can be expressed as:
P θ (S) = L i=1 P θ (c i | c 1 , . . . , c i−1 ),(2)
with the convention that
P θ (c 1 | •) = P θ (c 1 ).
The training objective is to minimize the cross-entropy loss over the entire training set, which for a single sequence is given by:
L(θ) = − L−1 i=1 log P θ (c i+1 | c 1 , . . . , c i ).(3)
This objective encourages the model to assign high probabilities to the correct next character at each step.Parameter updates are performed using gradient descent:
θ ← θ − η∇ θ L(θ),(4)
where η is the learning rate.This autoregressive framework enables the model to learn the syntax rules of SMILES representations, thereby generating valid molecular structures character by character.</p>
<p>The model trained on the Guacamol dataset for approximately 3 hours, achieving an 97% validity rate for the generated molecular structures, demonstrating that it had effectively learned the SMILES generation rules.The model trained on the ZINC dataset for approximately 70 hours, achieving a 99.6% validity rate for the generated molecules, further validating its generalization capability.All pretraining experiments were conducted on a single A100 GPU.</p>
<p>DPO for Molecular Optimization</p>
<p>Building upon the pretrained molecular generation capability, we introduce Direct Preference Optimization (DPO) to align molecular generation with chemical preferences.DPO is a contrastive learning approach that optimizes the generation policy without explicitly modeling a reward function.Instead of using reinforcement learning with human feedback (RLHF) methods that first train a reward model and then optimize the policy using algorithms like PPO, DPO directly optimizes the policy by enforcing preference constraints.</p>
<p>The training data consists of triplets (x, y w , y l ), where: x represents the input.y w is the preferred (or "winning") response.y l is the less preferred (or "losing") response.</p>
<p>DPO is built upon the idea that an optimal policy π * should satisfy the following preference ratio constraint:
π * (y w |x) π * (y l |x) = exp(r(y w |x) − r(y l |x))
where r(y | x) is an implicit reward function that ranks different responses.Instead of explicitly learning this reward function, DPO directly optimizes the policy ratio by defining the following log preference probability:
log σ(β • (log π θ (y w |x) − log π θ (y l |x)))
where σ(z) = 1 1+e −z is the sigmoid function, and β is a temperature hyperparameter controlling sensitivity to preference differences.</p>
<p>The final DPO objective function is:
L(θ) = E (x,yw,y l )∼D [log σ(β • (log π θ (y w |x) − log π θ (y l |x)))]
In our molecular generation task, there is no explicit input x.Furthermore, whereas other DPO tasks often require human annotation of preference samples, which is costly, our task does not rely on human annotations to determine response quality.Instead, we leverage a chemical computation library to evaluate the quality of generated molecules, thereby streamlining the preference learning process.</p>
<p>Curriculum Learning for Structured Molecular Optimization</p>
<p>To address the challenge of learning complex chemical spaces, we integrate DPO with curriculum learning.Curriculum learning is a machine learning strategy inspired by the human learning process, where the model begins with simple tasks and gradually progresses to more complex ones.By organizing training samples in order of increasing difficulty, the model builds a solid foundation with easier examples, leading to more efficient learning, better generalization, and ultimately enhanced performance on challenging tasks.</p>
<p>Aligned with this progressive approach, our pair construction process incrementally increases the difficulty of the learning task.Initially, the score gap between the superior and inferior samples is large, making it straightforward for the model to distinguish high-quality molecules.As training advances, this gap is gradually reduced, requiring the model to discern more subtle differences.This strategy reinforces the curriculum learning paradigm and refines the model's finegrained discrimination of molecular quality, ultimately enhancing the validity of the generated compounds.</p>
<p>Furthermore, we adopt a multi-stage learning mechanism to enhance model performance.Specifically, during training, high-quality molecules collected by the model are stored in memory.Subsequently, all agents are reinitialized to the pre-trained model and continue training by constructing new sample pairs from the high-scoring molecules in memory.The primary objective of this strategy is to mitigate potential biases introduced during early exploration, preventing the model from converging to suboptimal solutions.By leveraging previously identified high-quality molecules, the model can effectively restart its learning process in a more optimized direction, ultimately improving the quality of generated molecules.</p>
<p>As illustrated in the figure 2, our training process is divided into three stages.In the first stage, the model learns the fundamental requirements of the task and rapidly identifies high-scoring molecules from the vast chemical space.In the second stage, the model fine-tunes molecular scaffolds to further refine its understanding.Finally, in the third stage, the model modifies functional groups based on the optimal molecular scaffolds stored in memory, further optimizing molecular structures.This multi-stage approach significantly enhances molecular design efficiency, leading to a remarkable score of 0.993 on the GSK3B+DRD2 task.</p>
<p>Fig. 2: In the GSK3B+DRD2 docking experiment, the model achieved good performance through curriculum learning.In Course 1, the model learns the fundamental requirements of the task.In Course 2, it fine-tunes the molecular scaffold.In Course 3, it adjusts functional groups to optimize molecular structures.</p>
<p>Training Procedure</p>
<p>Integrating DPO with curriculum learning, we design a two-stage training protocol, as illustrated in Figure 1.The process begins with pre-training to obtain the prior model, followed by reinforcement fine-tuning using DPO and curriculum learning.</p>
<p>In the pre-training phase, the model learns to generate valid SMILES strings and capture the chemical space distribution, forming a prior model.</p>
<p>During reinforcement fine-tuning, the agents-initialized from the prior-generates molecules, which are evaluated by a task-specific scoring function.Preference pairs are then constructed by selecting the top-k highest-scoring molecules (k varies across agents) as "preferred samples" and randomly sampling lower-quality ones as "dispreferred samples".These pairs are used to optimize the policy via DPO, progressively aligning the model's generation strategy with the target objectives.</p>
<p>As training progresses, the scores of the generated molecules steadily improve while the gap between preferred and dispreferred samples narrows, indicating an increase in training difficulty.Initially, the model makes broad scaffold-level adjustments to identify promising frameworks; as scaffolds stabilize, it shifts to fine-tuning functional groups to further optimize molecular properties.</p>
<p>The DPO training process code is as follows:</p>
<p>Experiments</p>
<p>To validate the effectiveness of our model, we designed and conducted a series of experiments, including the Guacamol benchmark evaluation, target protein binding experiments, and impact analysis.The experimental results demonstrate that our model is not only capable of handling classical molecular design tasks but also performs exceptionally well in tasks that are more closely aligned with real-world drug discovery.Furthermore, the impact analysis, which examines model performance under different parameter settings, helping us identify the optimal parameter settings.</p>
<p>GuacaMol benchmark</p>
<p>Guacamol Introduction Guacamol Benchmark, proposed by BenevolentAI in 2019 [5], is a standardized framework for evaluating molecular generation models in terms of diversity, synthetic feasibility, and goal-directed optimization.It comprises 20 tasks covering key challenges in molecular design.These tasks can be broadly categorized into rediscovery and similarity-based optimization, isomer generation, and molecular property balancing.Additionally, multi-parameter optimization (MPO) tasks focus on improving physicochemical properties of known drugs, while SMARTS-constrained tasks enforce structural constraints.Lastly, scaffold hopping and decorator hopping tasks assess the model's ability to modify core structures and substituents.</p>
<p>Baselines To comprehensively evaluate our approach, we compare it against several representative baselines: SMILES LSTM [29]: An LSTM-based model trained via maximum likelihood estimation to generate SMILES strings.Graph GA [14]: A graph-based genetic algorithm that optimizes molecular structures through crossover and mutation.Reinvent Initialize agents {πi} N i=1 with θi ∼ N (0, 0.02) 4:</p>
<p>for t ← 1 to T do 5: AgentLoop 6:</p>
<p>for each agent πi do 7: Sampling 8:</p>
<p>Di ← SampleSMILES(πi, m batch ) 9:</p>
<p>s ← Fscore(Di) 10:</p>
<p>M ← UpdateMemory(Di, s) 11:</p>
<p>EndSampling 12:</p>
<p>Positive Selection:</p>
<p>x w ∼ pM(x) ∝ exp(s/τ )// top-weighted historical samples</p>
<p>13:</p>
<p>Negative Selection:</p>
<p>x l ∼ Uniform(Dt)// current batch negatives</p>
<p>14:</p>
<p>Compute log-ratios for each sample:
log r θ (x) = log π θ (x) − log π ref (x)</p>
<p>15:</p>
<p>Optimize loss:
LDPO = −E log σ β(r θ (x w ) − r θ (x l )) 16: Gradient step: θ ← θ − η∇ θ LDPO 17:
end for 18:</p>
<p>EndAgentLoop 19:</p>
<p>Log max M.s, top k -mean(M.s)20:</p>
<p>end for 21:</p>
<p>return Top k (M.x, M.s) 22: end procedure neural networks with reinforcement learning, using reward functions to enhance both bioactivity and physicochemical properties.GEGL [1]: An approach that integrates graph neural networks with reinforcement learning to directly optimize molecular graphs.MolRL-MGPT [12]: A hybrid model that fuses GPT-based generative strategies with reinforcement learning to boost molecular diversity and target-specific performance.</p>
<p>Experimental Details First of all, we pre-trained the model using the training set provided by Guacamol.The pre-training was conducted for 15 epochs on a single A100 GPU over a duration of 3 hours.After pre-training, the model achieved a molecular validity of 97%, demonstrating its high accuracy in molecular structure generation.</p>
<p>Subsequently, we further trained the model on 20 tasks from the Guacamol benchmark to evaluate its performance across different objectives.The hyperparameter settings used in this stage were as follows: Batch size = 50, n_steps = 1000, num_agents = 4, Learning rate = 1e-4, Memory size = 1000.</p>
<p>The complete training of the 20 benchmark tasks required 60 hours, whereas MolRL-MGPT took 400 hours under the same conditions (both on a single A100 GPU).Our model demonstrated a training speed nearly 6 times faster, highlighting the advantages of DPO's stable training and faster convergence, which significantly reduces training costs.As shown in the table 1, our model achieved the best performance on multiple tasks, with its overall score surpassing other baselines.Specifically, our method outperformed existing approaches on 16 out of 20 benchmark tasks, demonstrating a clear advantage in molecular generation.Compared to GEGL, our model exhibited higher stability across diverse tasks, achieving consistently superior performance rather than excelling in only a subset of cases.</p>
<p>Our model's effectiveness is particularly evident in challenging tasks.For instance, in Perindopril MPO, it significantly outperformed existing methods by a margin of 0.05, highlighting its robustness in complex molecular design.Another compelling example is the Ranolazine MPO task, where MolRL-MGPT, the previous state-of-the-art model, improved upon its closest competitor by only 0.006, suggesting that performance on this task had reached a plateau.However, our approach further improved upon MolRL-MGPT by an additional 0.011, demonstrating that our model can break through existing performance bottlenecks and further optimize molecular generation outcomes.</p>
<p>These results indicate that our model possesses a strong learning capability, effectively handling molecular generation tasks and producing high-quality molecules that meet target requirements.Furthermore, these findings validate the effectiveness of the DPO method in optimizing molecular generation, providing a solid foundation for future research.</p>
<p>Molecular Generation for High Binding Affinity to Target Proteins</p>
<p>In this experiment, we utilized a prior model pretrained on the ZINC dataset.</p>
<p>The evaluation was conducted on six tasks: JNK3, GSK3B, DRD2, and their pairwise combinations (JNK3+GSK3B, JNK3+DRD2, GSK3B+DRD2).The model performance was evaluated using the oracle function provided by TDC.For multi-objective optimization tasks, we used the arithmetic mean of the individual target scores as the final score to assess the overall performance of the generated molecules across multiple targets.As shown in the table 2, our model successfully generated molecules with strong binding potential to JNK3, GSK3B, and DRD2, demonstrating its effectiveness in molecular generation tasks.These results indicate that our model not only performs well on the Guacamol benchmark but also excels in real-world drug discovery tasks, providing a solid foundation for future research in molecular design and generation.</p>
<p>Impact Analysis</p>
<p>This study suggests that multiple factors, including the learning rate, the number of agents, the sampling-to-training ratio, and the DPO parameter β, may influence model performance.Through preliminary analysis, we identified that the number of agents and the sampling-to-training ratio have a particularly significant impact.To validate this hypothesis, we conducted a systematic impact analysis experiment focusing on these two key parameters.The experimental results demonstrate that appropriately adjusting the number of agents and the sampling-to-training ratio can significantly enhance model performance, providing valuable theoretical insights and practical guidance for further model optimization.Agents Num Experimental results in figure 3 indicate that the model achieves optimal performance when employing 2 to 4 agents.When the number of agents is too small, the model's expressive capacity is limited, making it difficult to effectively learn complex patterns.Conversely, an excessive number of agents may introduce redundant information and increase optimization complexity, thereby lowering the model's performance upper bound and slowing down convergence.Moreover, the number of agents also affects the score distribution of molecules stored in memory: fewer agents lead to a more concentrated distribution, whereas a larger number of agents result in a more dispersed distribution.Due to differences in the learning configurations of individual agents, their capabilities diverge as training progresses, leading to an increasingly diverse molecular distribution.This broader distribution facilitates the optimization of DPO training and enhances the model's generalization capability.4, increasing the samplingto-training ratio within a certain range can improve the model's performance upper bound and accelerate convergence.If the ratio is too small, the weights assigned to a few high-quality molecules obtained by chance become excessively large, causing the model to shift towards them-even when they do not represent the optimal direction-ultimately leading to a suboptimal solution.Conversely, if the ratio is too large, the model remains in the sampling phase for an extended period, collecting a large number of redundant molecules while lacking sufficient training.This results in inadequate gradient updates and significantly prolongs the model's convergence time.</p>
<p>Conclusion</p>
<p>This study proposes a molecule generation method based on DPO and curriculum learning, and achieves favorable experimental results on the Guacamol benchmark and several target tasks.The experiments demonstrate that the proposed method has significant advantages in tasks that generate molecules with specified properties.</p>
<p>Fig. 1 :
1
Fig.1: Structure of the DPO+Curriculum Learning model.The model is initially pre-trained, followed by optimization using Direct Preference Optimization.As curriculum learning progresses, the molecular scores of the collected compounds steadily increase while the distinction between superior and inferior molecules gradually narrows.Ultimately, the process yields molecules that meet the predefined quality criteria.</p>
<p>2 :
2
[3]: A model combining recurrent Algorithm 1 Direct Preference Optimization (DPO) Training 1: procedure DPO_Train(Fscore, k) Initialize: Load pretrained prior p ref ← GPT(θprior) 3:</p>
<p>Fig. 3 :
3
Fig. 3: Model performance on Ranolazine MPO and Amlodipine MPO tasks under different numbers of agents.(The curve represents the Top-10 score, while the shaded region indicates the score distribution of the top 100 molecules.)</p>
<p>Fig. 4 :
4
Fig. 4: Model performance on Perindopril MPO and Amlodipine MPO tasks under different Sampling-to-Training Ratios.(The curve represents the Top-10 score, while the shaded region indicates the score distribution of the top 100 molecules.)</p>
<p>Disclosure of Interests.The authors have no competing interests to declare that are relevant to the content of this article.Maksim D Kuznetsov, Arip Asadulaev, et al.Deep learning enables rapid identification of potent ddr1 kinase inhibitors.Nature biotechnology, 37(9):1038-1040, 2019.34.Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley.Optimization of molecules via deep reinforcement learning.Scientific reports, 9(1):10752, 2019.</p>
<p>Table 1 :
1
Scores of DPO and baselines on the GuacaMol benchmark.(All task scores are rounded to three decimal places.)
SMILES-GraphGA Reinvent GEGLMolRL-DPO&amp;CLLSTMMGPTCelecoxib rediscovery1.0001.0001.0001.0001.0001.000Troglitazone rediscovery 1.0001.0001.0000.5521.0001.000Thiothixene rediscovery 1.0001.0001.0001.0001.0001.000Aripiprazole similarity 1.0001.0001.0001.0001.0001.000Albuterol similarity1.0001.0001.0001.0001.0001.000Mestranol similarity1.0001.0001.0001.0001.0001.000C11H240.9930.9710.9991.0001.0001.000C9H10N2O2PF2Cl0.8790.9820.8771.0000.9391.000Median molecules 10.4380.4060.4340.4550.4490.455Median molecules 20.4220.4320.3950.4370.4220.422Osimertinib MPO0.9070.9530.8891.0000.9770.990Fexofenadine MPO0.9590.9981.0001.0001.0001.000Ranolazine MPO0.8550.9200.8950.9330.9390.950Perindopril MPO0.8080.7920.7640.8330.8100.883Amlodipine MPO0.8940.8940.8880.9050.9060.906Sitagliptin MPO0.5450.8910.5390.7490.8230.838Zaleplon MPO0.6690.7540.5900.7630.7900.797Valsartan SMARTS0.9780.9900.0951.0000.9970.994deco hop0.9961.0000.9941.0001.0001.000scaffold hop0.9981.0000.9901.0001.0001.000Total17.34017.98316.35017.62718.05218.235</p>
<p>Table 2 :
2
The scores of the generated molecules on JNK3, GSK3β, DRD2, and pairwise combination tasks.
top 1 top 10 mean top 100 meanJNK31.0001.0001.000GSk3B1.0001.0001.000DRD21.0001.0001.000JNK3+GSK3B 0.9440.9430.938JNK3+DRD2 0.9250.9250.920GSK3B+DRD2 0.9930.9920.989</p>
<p>Guiding deep molecular optimization with genetic exploration. Junsu Sungsoo Ahn, Hankook Kim, Jinwoo Lee, Shin, Advances in neural information processing systems. 202033</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Reinvent 2.0: an ai tool for de novo drug design. Thomas Blaschke, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, Atanas Patronov, Journal of chemical information and modeling. 60122020</p>
<p>The art and practice of structure-based drug design: a molecular modeling perspective. Regine S Bohacek, Colin Mcmartin, Wayne C Guida, Medicinal research reviews. 1611996</p>
<p>Guacamol: benchmarking models for de novo molecular design. Nathan Brown, Marco Fiscato, Marwin Hs Segler, Alain C Vaucher, Journal of chemical information and modeling. 5932019</p>
<p>Decomposed direct preference optimization for structure-based drug design. Xiwei Cheng, Xiangxin Zhou, Yuwei Yang, Yu Bao, Quanquan Gu, arXiv:2407.139812024arXiv preprint</p>
<p>Innovation in the pharmaceutical industry: new estimates of r&amp;d costs. Henry G Joseph A Dimasi, Ronald W Grabowski, Hansen, Journal of health economics. 472016</p>
<p>Automatic chemical design using a data-driven continuous representation of molecules. Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, Alán Aspuru-Guzik, ACS central science. 422018</p>
<p>Aligning target-aware molecule diffusion models with exact energy optimization. Siyi Gu, Minkai Xu, Alexander Powers, Weili Nie, Tomas Geffner, Karsten Kreis, Jure Leskovec, Arash Vahdat, Stefano Ermon, Advances in Neural Information Processing Systems. 202537</p>
<p>Improving de novo molecular design with curriculum learning. Jeff Guo, Vendy Fialková, Juan Diego Arango, Christian Margreitter, Jon Paul Janet, Kostas Papadopoulos, Ola Engkvist, Atanas Patronov, Nature Machine Intelligence. 462022</p>
<p>On the power of curriculum learning in training deep networks. Guy Hacohen, Daphna Weinshall, International conference on machine learning. PMLR2019</p>
<p>De novo drug design using reinforcement learning with multiple gpt agents. Xiuyuan Hu, Guoqing Liu, Yang Zhao, Hao Zhang, Advances in Neural Information Processing Systems. 202336</p>
<p>Zinc-a free database of commercially available compounds for virtual screening. J John, Brian K Irwin, Shoichet, Journal of chemical information and modeling. 4512005</p>
<p>A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Jan H Jensen, Chemical science. 10122019</p>
<p>Junction tree variational autoencoder for molecular graph generation. Wengong Jin, Regina Barzilay, Tommi Jaakkola, International conference on machine learning. PMLR2018</p>
<p>Multi-objective molecule generation using interpretable substructures. Wengong Jin, Regina Barzilay, Tommi Jaakkola, International conference on machine learning. PMLR2020</p>
<p>Chatgpt-powered conversational drug editing using retrieval and domain feedback. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao, arXiv:2305.180902023arXiv preprint</p>
<p>Drugex v2: de novo design of drug molecules by pareto-based multi-objective reinforcement learning in polypharmacology. Xuhan Liu, Kai Ye, Herman Wt Van Vlijmen, Gerard Jp Michael Tm Emmerich, Adriaan P Ijzerman, Van Westen, Journal of cheminformatics. 131852021</p>
<p>Drugex v3: scaffold-constrained drug design with graph transformerbased reinforcement learning. Xuhan Liu, Kai Ye, Herman Wt Van Vlijmen, Adriaan P Ijzerman, Gerard Jp Van Westen, Journal of Cheminformatics. 151242023</p>
<p>Reinvent 4: Modern ai-driven generative molecule design. Jiazhen Hannes H Loeffler, Alessandro He, Jon Paul Tibo, Alexey Janet, Voronov, Ola Lewis H Mervin, Engkvist, Journal of Cheminformatics. 161202024</p>
<p>Rational drug design. Soma Mandal, Sanat K Mandal, European journal of pharmacology. 6251-32009</p>
<p>De novo molecular design and generative models. Joshua Meyers, Benedek Fabian, Nathan Brown, Drug discovery today. 26112021</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, Journal of Machine Learning Research. 211812020</p>
<p>Molecular de-novo design through deep reinforcement learning. Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, Hongming Chen, Journal of cheminformatics. 92017</p>
<p>Deep reinforcement learning for de novo drug design. Mariya Popova, Olexandr Isayev, Alexander Tropsha, Science advances. 4778852018</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362024</p>
<p>Inverse molecular design using machine learning: Generative models for matter engineering. Benjamin Sanchez, -Lengeling , Alán Aspuru-Guzik, Science. 36164002018</p>
<p>Computer-based de novo design of drug-like molecules. Gisbert Schneider, Uli Fechner, Nature reviews Drug discovery. 482005</p>
<p>Generating focused molecule libraries for drug discovery with recurrent neural networks. Marwin Hs Segler, Thierry Kogej, Christian Tyrchan, Mark P Waller, ACS central science. 412018</p>
<p>Reinforcement learning for molecular design guided by quantum mechanics. Gregor Simm, Robert Pinsler, José Miguel Hernández-Lobato, International Conference on Machine Learning. PMLR2020</p>
<p>A deep learning approach to antibiotic discovery. Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Shawn Craig R Macnair, Lindsey A French, Zohar Carfrae, Bloom-Ackermann, Cell. 18042020</p>
<p>Aligning protein generative models with experimental fitness via direct preference optimization. Talal Widatalla, Rafael Rafailov, Brian Hie, bioRxiv. 2024</p>
<p>. Alex Zhavoronkov, Yan A Ivanenkov, Alex Aliper, Mark S Veselov, Vladimir A Aladinskiy, Anastasiya V Aladinskaya, Victor A Terentiev, A Daniil, Polykovskiy, </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7588 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7588</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7588</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-a4a26d7c6bc022f604a246c6ce6cc1cfbe618441</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a4a26d7c6bc022f604a246c6ce6cc1cfbe618441" target="_blank">APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel Attention Prompt tuning method, namely AP ROMPT, which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning.</p>
                <p><strong>Paper Abstract:</strong> With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large pre-trained language models. However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement. In this work, we propose a novel Attention Prompt tuning method, namely AP ROMPT , for efficient adaptation of pre-trained language models. We first demonstrate that existing prompt tuning can be considered as a special case of attention prompt tuning. We then formally introduce AP ROMPT , which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning. Experimental results on the SuperGLUE benchmark consistently demonstrate that our proposed approach outperforms state-of-the-art baselines and full fine-tuning method with pre-trained models at different scales. In addition, a comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7588.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7588.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input Prompt (soft prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input Soft Prompt Prepending</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learnable continuous embeddings prepended to the model input sequence (virtual tokens) that are tuned while the backbone is frozen; used to condition PLMs for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-to-text Transformer (T5) pretrained and used as frozen backbone; prompt tokens prepended to input and only prompt embeddings tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE subset (WSC, CB, BoolQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Natural language understanding tasks from SuperGLUE (coreference/WSC, NLI/CB, QA/BoolQ).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Soft prompts prepended to input tokens (virtual tokens added at sequence start)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Input-layer soft prompt P of length m prepended to the input embeddings X forming [P, X]; prompt length searched among {5,10,20,50,100} for baselines; initialization by sampling vocabulary embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Prompt Tuning: BoolQ 83.65% / CB 88.41% / WSC 78.31% accuracy (T5-Large, Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>T5-Large backbone; input prompt lengths searched; prompts trained for 100 epochs with learning rate 0.3, batch size 16; max input length 512 tokens</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7588.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7588.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fixed Key-Value Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fixed Key-Value Prompts Derived from Input Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Key and value prompts computed as H^k P and H^v P from the learned input prompts and kept fixed (no tuning) to test equivalence between input prompts and constrained key/value prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-Large encoder architecture; keys and values in self-attention extended with fixed matrices derived from input prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE subset (WSC, CB, BoolQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLU tasks from SuperGLUE to measure effect of representing input prompts as fixed key/value prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Key/value matrices augmented by fixed prompts (no extra tuned parameters beyond the input prompt-derived K/V)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / internal representation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Compute P_k = H^k P and P_v = H^v P from input prompts P and append to key/value matrices; no tuning of these K/V prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Fixed Key-value Prompts: BoolQ 83.65% / CB 88.41% / WSC 78.31% accuracy (T5-Large, Table 1) — identical to Prompt Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt Tuning: BoolQ 83.65% / CB 88.41% / WSC 78.31% accuracy (T5-Large, Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>0.00% absolute (identical performance vs Prompt Tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>T5-Large; fixed K/V computed from optimal input prompts without any tuning</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7588.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7588.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Learned Key-Value Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unconstrained (Learned) Key-Value Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Key and value prompts that are free parameters (not constrained by input prompts) and are learned during fine-tuning; appended to K and V matrices in attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-Large with key and value prompt matrices learned (additional small matrices appended to K and V).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE subset (WSC, CB, BoolQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLU tasks to evaluate benefit of allowing K/V prompts to be learned rather than fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Key/value matrices augmented by learned prompt columns (trained during fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / internal representation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Key-value prompts are learned during fine-tuning (search over prompt lengths); compared against fixed key-value prompts and input prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Learned Key-value Prompts (T5-Large): BoolQ 84.06% / CB 88.93% / WSC 78.88% accuracy (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Fixed Key-value Prompts / Prompt Tuning: BoolQ 83.65% / CB 88.41% / WSC 78.31%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.41% to +0.75% absolute improvements across tasks (learned K/V vs fixed; e.g., BoolQ +0.41% absolute)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>T5-Large; key/value prompt lengths searched; prompts trained for 100 epochs; Adafactor optimizer</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7588.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7588.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Tuning + Key-Value Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint Input Prompting and Learned Key-Value Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combination of input soft prompts and separately learned key/value prompts trained together; reported as giving highest performance among tested prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-Large where both input-layer soft prompts and attention-layer key/value prompts are learned jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE subset (WSC, CB, BoolQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLU tasks to test if combining input prompts and unconstrained attention prompts yields added value.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input prompts prepended to sequence plus learned attention key/value prompt matrices appended to K and V</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Both input prompts P and separate K/V prompts are trained; prompt lengths tuned on dev set; comparisons in Table 1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Prompt Tuning + Key-value Prompts (T5-Large): BoolQ 84.25% / CB 89.17% / WSC 79.12% accuracy (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt Tuning alone: BoolQ 83.65% / CB 88.41% / WSC 78.31%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.6% to +0.81% absolute improvement over Prompt Tuning (task-dependent; e.g., WSC +0.81% absolute)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>T5-Large; input prompts and K/V prompts jointly tuned; same training hyperparameters as other prompt methods</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7588.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7588.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention Prompts (APrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>APrompt: Query, Key and Value Attention Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Novel method that inserts trainable query, key and value prompt matrices into each Transformer's self-attention block (in addition to input prompts) to steer attention computation; designed for efficient adaptation with <0.5% tunable params.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (Base, Large, XL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 encoder-decoder family used as frozen backbones (T5-Base 220M, T5-Large 770M, T5-XL 3B); only input prompts, attention prompts (P_q, P_k, P_v), and small task head are trained.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>220M / 770M / 3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE (8 tasks aggregated and per-task reporting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of NLU tasks (BoolQ, CB, COPA, MRC, ReC, RTE, WiC, WSC); reported both task-wise and average scores.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Attention-layer prompt augmentation — query/key/value prompt matrices appended to attention Q, K, V (plus optional input prompts prepended to embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / architectural injection</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>APrompt inserts P_q, P_k, P_v in each encoder (and decoder) layer; input prompts fixed at length 10; search over query-key and key-value prompt lengths in {1,5,10,20,50}; also ablations over prompt positions (first/last/all layers); trained for 100 epochs with LR 0.3</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average SuperGLUE score / per-task Accuracy / F1 / EM depending on task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T5-Large APrompt average score 86.55% (Table 2); per-task examples: BoolQ 90.35% Acc, CB 95.83% F1/Acc, WSC 90.13% Acc (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Fine-Tuning (T5-Large) average 84.47%; P-Tuning v2 average 84.59% (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>APrompt +2.08% absolute vs full fine-tuning average (T5-Large) and +1.96% absolute vs P-Tuning v2 (T5-Large); APrompt outperforms all prompt-baselines across model sizes (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>T5-Base/Large/XL backbones; prompts trained for 100 epochs, LR 0.3, batch size 16; prompt length search for attention prompts; scores averaged over 5 runs</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Main SuperGLUE results reported as statistically significant vs baselines on each PLM (all p-value < 0.005)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7588.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7588.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Positioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Layer-wise Prompt Insertion Positioning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Placement of prompts (input/attention) at different layers — first layer, last layer, first 12, last 12, alternating 12, or all layers — strongly affects performance; inserting prompts to all layers yields best results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-XL (3B) backbone with APrompt prompts applied at the specified positions (both encoder and decoder) while keeping backbone frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE subset (BoolQ, CB, RTE, WSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation of positional effects of prompt insertion on several representative SuperGLUE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt insertion positions (first-layer, last-layer, first-12 layers, last-12 layers, alternating-12 layers, all layers)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt placement</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Five position variants trained and compared; prompts inserted into both encoder and decoder at the chosen layers; input prompts also applied in analogous positions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T5-XL APrompt (All layers) — BoolQ 90.72% / CB 95.48% / RTE 93.36% / WSC 96.17% (Table 4: 'APrompt (All)')</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>First-layer: BoolQ 78.56% / CB 85.48% / RTE 90.28% / WSC 90.88%; Last-layer: BoolQ 75.82% / CB 84.37% / RTE 90.75% / WSC 91.15% (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Applying prompts to all layers yields very large absolute gains vs only first/last layer (e.g., BoolQ +11.9 to +14.9 pp absolute; WSC +4.8 to +5.3 pp absolute compared to first/last layer placements)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>T5-XL; prompt lengths tuned; same optimization/training hyperparameters as main experiments</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7588.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7588.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Lengths (query-key & key-value)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention Prompt Length Hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The lengths (number of prompt rows/columns) of query-key prompts and key-value prompts are tuned and affect performance differently per task; no single prompt length is optimal across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-XL (3B) with APrompt, where both query-key prompt length and key-value prompt length are hyperparameters searched over {1,5,10,20,50}.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RTE and WSC (examples), general SuperGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Investigate sensitivity of APrompt to attention-prompt length choices across tasks of varying difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Two-dimensional prompt-length grid search (query-key length x key-value length)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / hyperparameter</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Grid search across {1,5,10,20,50} for both prompt types; reported that RTE best with (query-key=5, key-value=10) while WSC best with (query-key=10, key-value=20); performance relatively stable within ranges</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T5-XL APrompt peak task results reported: RTE up to 93.36% and WSC up to 96.17% (with tuned prompt lengths; Table 2 & Fig.6 commentary)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Task-dependent absolute differences due to prompt-length choice (examples: RTE best config vs other configs yielded the highest RTE score; WSC best config yielded highest WSC score — numerical deltas are task- and config-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>T5-XL; lengths searched from {1,5,10,20,50}; other training settings as main experiments</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7588.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7588.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot Presentation (32-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Resource Few-Shot Prompting (32 examples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation in a low-resource scenario where only 32 training examples per task are used to fine-tune prompts, showing that prompt-based methods (especially APrompt) can outperform full fine-tuning under severe data constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-XL (3B) with various prompt-tuning methods and full fine-tuning compared in 32-sample training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot SuperGLUE subset (BoolQ, CB, RTE, WSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Low-resource evaluation using 32 randomly selected training examples per task; measure robustness and generalization of prompt methods.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt-tuning with only 32 labeled examples (few-shot fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>experimental setting / data availability</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>32 examples per task; same prompt architectures as in full-data experiments; best checkpoints selected via dev set; single fixed seed sampling per task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T5-XL 32-shot APrompt: BoolQ 75.66% / CB 78.51% / RTE 75.83% / WSC 72.30% (Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Best baseline (P-Tuning v2) 32-shot: BoolQ 73.36% / CB 77.94% / RTE 73.48% / WSC 70.50% (Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>APrompt improvements vs best baseline: BoolQ +2.30 pp, CB +0.57 pp, RTE +2.35 pp, WSC +1.80 pp absolute (T5-XL few-shot Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>32 randomly sampled examples per task; fixed seed; training and selection as described in Section 6</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7588.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7588.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention Prompt Balancing (ratio)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ratio Trade-off between Key-Value and Query-Key Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Allocation of a fixed budget of trainable parameters between key-value prompts and query-key prompts affects task performance; neither extreme (all K/V or all Q/K) is optimal in most cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-XL with APrompt where total number of trainable attention-prompt parameters is fixed and the ratio allocated to key-value prompts is varied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE subset (BoolQ, CB, RTE, WSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Study effect of splitting fixed prompt-parameter budget across key-value vs query-key prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Fixed total prompt-parameter budget; vary key-value prompt fraction among {0,0.25,0.5,0.75,1}</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / resource allocation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Run with key-value prompt ratios 0, 0.25, 0.5, 0.75, 1; evaluate per-task performance; report that best ratio differs by task (e.g., WSC best at 0.75, CB best at 0.5), and extremes 0 or 1 underperform</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example highlights: APrompt (overall T5-XL full setup) achieves high task scores (CB 95.48%, WSC 96.17%); best per-task ratio observed: WSC best at 0.75 key-value ratio, CB best at 0.5 (Fig.7 commentary)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Extremes (0 or 1) generally underperform mixed allocations; mixed allocations produce small but meaningful absolute gains task-dependently</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fixed total number of trainable parameters; ratio swept over {0,0.25,0.5,0.75,1}; T5-XL backbone</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7588.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7588.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input-Prompt Removal (APrompt-)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>APrompt Variant without Input Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of APrompt which removes input-layer soft prompts and retains only attention-layer prompts; shows only minor degradation relative to full APrompt, indicating attention prompts carry most of the benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-XL APrompt variant with input prompts removed; attention prompts (P_q, P_k, P_v) still trained.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE subset (BoolQ, CB, RTE, WSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Ablation to measure contribution of input prompts relative to attention prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>APrompt architecture with no input prompts (only attention prompts active)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>ablation / prompt composition</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Compare APrompt- against APrompt and pruned APrompt+ variants (Table 5); same training settings as main experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>APrompt- (T5-XL): BoolQ 90.41% / CB 95.28% / RTE 93.12% / WSC 95.82% (Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Full APrompt (T5-XL): BoolQ 90.72% / CB 95.48% / RTE 93.36% / WSC 96.17% (Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Small absolute drops when removing input prompts: BoolQ -0.31 pp, CB -0.20 pp, RTE -0.24 pp, WSC -0.35 pp (APrompt- vs APrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>T5-XL; APrompt- trains only attention prompts; same training hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Power of Scale for Parameter-Efficient Prompt Tuning <em>(Rating: 2)</em></li>
                <li>P-tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks <em>(Rating: 2)</em></li>
                <li>XPrompt: Exploring the Extreme of Prompt Tuning <em>(Rating: 2)</em></li>
                <li>Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization <em>(Rating: 1)</em></li>
                <li>Prefix-tuning: Optimizing Continuous Prompts for Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7588",
    "paper_id": "paper-a4a26d7c6bc022f604a246c6ce6cc1cfbe618441",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Input Prompt (soft prompt)",
            "name_full": "Input Soft Prompt Prepending",
            "brief_description": "Learnable continuous embeddings prepended to the model input sequence (virtual tokens) that are tuned while the backbone is frozen; used to condition PLMs for downstream tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-Large",
            "model_description": "Text-to-text Transformer (T5) pretrained and used as frozen backbone; prompt tokens prepended to input and only prompt embeddings tuned.",
            "model_size": "770M",
            "task_name": "SuperGLUE subset (WSC, CB, BoolQ)",
            "task_description": "Natural language understanding tasks from SuperGLUE (coreference/WSC, NLI/CB, QA/BoolQ).",
            "problem_format": "Soft prompts prepended to input tokens (virtual tokens added at sequence start)",
            "format_category": "prompt style",
            "format_details": "Input-layer soft prompt P of length m prepended to the input embeddings X forming [P, X]; prompt length searched among {5,10,20,50,100} for baselines; initialization by sampling vocabulary embeddings.",
            "performance_metric": "Accuracy",
            "performance_value": "Prompt Tuning: BoolQ 83.65% / CB 88.41% / WSC 78.31% accuracy (T5-Large, Table 1)",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "T5-Large backbone; input prompt lengths searched; prompts trained for 100 epochs with learning rate 0.3, batch size 16; max input length 512 tokens",
            "statistical_significance": null,
            "uuid": "e7588.0"
        },
        {
            "name_short": "Fixed Key-Value Prompts",
            "name_full": "Fixed Key-Value Prompts Derived from Input Prompts",
            "brief_description": "Key and value prompts computed as H^k P and H^v P from the learned input prompts and kept fixed (no tuning) to test equivalence between input prompts and constrained key/value prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-Large",
            "model_description": "T5-Large encoder architecture; keys and values in self-attention extended with fixed matrices derived from input prompts.",
            "model_size": "770M",
            "task_name": "SuperGLUE subset (WSC, CB, BoolQ)",
            "task_description": "NLU tasks from SuperGLUE to measure effect of representing input prompts as fixed key/value prompts.",
            "problem_format": "Key/value matrices augmented by fixed prompts (no extra tuned parameters beyond the input prompt-derived K/V)",
            "format_category": "prompt style / internal representation",
            "format_details": "Compute P_k = H^k P and P_v = H^v P from input prompts P and append to key/value matrices; no tuning of these K/V prompts.",
            "performance_metric": "Accuracy",
            "performance_value": "Fixed Key-value Prompts: BoolQ 83.65% / CB 88.41% / WSC 78.31% accuracy (T5-Large, Table 1) — identical to Prompt Tuning",
            "baseline_performance": "Prompt Tuning: BoolQ 83.65% / CB 88.41% / WSC 78.31% accuracy (T5-Large, Table 1)",
            "performance_change": "0.00% absolute (identical performance vs Prompt Tuning)",
            "experimental_setting": "T5-Large; fixed K/V computed from optimal input prompts without any tuning",
            "statistical_significance": null,
            "uuid": "e7588.1"
        },
        {
            "name_short": "Learned Key-Value Prompts",
            "name_full": "Unconstrained (Learned) Key-Value Prompts",
            "brief_description": "Key and value prompts that are free parameters (not constrained by input prompts) and are learned during fine-tuning; appended to K and V matrices in attention.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-Large",
            "model_description": "T5-Large with key and value prompt matrices learned (additional small matrices appended to K and V).",
            "model_size": "770M",
            "task_name": "SuperGLUE subset (WSC, CB, BoolQ)",
            "task_description": "NLU tasks to evaluate benefit of allowing K/V prompts to be learned rather than fixed.",
            "problem_format": "Key/value matrices augmented by learned prompt columns (trained during fine-tuning)",
            "format_category": "prompt style / internal representation",
            "format_details": "Key-value prompts are learned during fine-tuning (search over prompt lengths); compared against fixed key-value prompts and input prompts",
            "performance_metric": "Accuracy",
            "performance_value": "Learned Key-value Prompts (T5-Large): BoolQ 84.06% / CB 88.93% / WSC 78.88% accuracy (Table 1)",
            "baseline_performance": "Fixed Key-value Prompts / Prompt Tuning: BoolQ 83.65% / CB 88.41% / WSC 78.31%",
            "performance_change": "+0.41% to +0.75% absolute improvements across tasks (learned K/V vs fixed; e.g., BoolQ +0.41% absolute)",
            "experimental_setting": "T5-Large; key/value prompt lengths searched; prompts trained for 100 epochs; Adafactor optimizer",
            "statistical_significance": null,
            "uuid": "e7588.2"
        },
        {
            "name_short": "Prompt Tuning + Key-Value Prompts",
            "name_full": "Joint Input Prompting and Learned Key-Value Prompts",
            "brief_description": "Combination of input soft prompts and separately learned key/value prompts trained together; reported as giving highest performance among tested prompt variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-Large",
            "model_description": "T5-Large where both input-layer soft prompts and attention-layer key/value prompts are learned jointly.",
            "model_size": "770M",
            "task_name": "SuperGLUE subset (WSC, CB, BoolQ)",
            "task_description": "NLU tasks to test if combining input prompts and unconstrained attention prompts yields added value.",
            "problem_format": "Input prompts prepended to sequence plus learned attention key/value prompt matrices appended to K and V",
            "format_category": "prompt style / hybrid",
            "format_details": "Both input prompts P and separate K/V prompts are trained; prompt lengths tuned on dev set; comparisons in Table 1",
            "performance_metric": "Accuracy",
            "performance_value": "Prompt Tuning + Key-value Prompts (T5-Large): BoolQ 84.25% / CB 89.17% / WSC 79.12% accuracy (Table 1)",
            "baseline_performance": "Prompt Tuning alone: BoolQ 83.65% / CB 88.41% / WSC 78.31%",
            "performance_change": "+0.6% to +0.81% absolute improvement over Prompt Tuning (task-dependent; e.g., WSC +0.81% absolute)",
            "experimental_setting": "T5-Large; input prompts and K/V prompts jointly tuned; same training hyperparameters as other prompt methods",
            "statistical_significance": null,
            "uuid": "e7588.3"
        },
        {
            "name_short": "Attention Prompts (APrompt)",
            "name_full": "APrompt: Query, Key and Value Attention Prompts",
            "brief_description": "Novel method that inserts trainable query, key and value prompt matrices into each Transformer's self-attention block (in addition to input prompts) to steer attention computation; designed for efficient adaptation with &lt;0.5% tunable params.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (Base, Large, XL)",
            "model_description": "T5 encoder-decoder family used as frozen backbones (T5-Base 220M, T5-Large 770M, T5-XL 3B); only input prompts, attention prompts (P_q, P_k, P_v), and small task head are trained.",
            "model_size": "220M / 770M / 3B",
            "task_name": "SuperGLUE (8 tasks aggregated and per-task reporting)",
            "task_description": "A suite of NLU tasks (BoolQ, CB, COPA, MRC, ReC, RTE, WiC, WSC); reported both task-wise and average scores.",
            "problem_format": "Attention-layer prompt augmentation — query/key/value prompt matrices appended to attention Q, K, V (plus optional input prompts prepended to embeddings)",
            "format_category": "prompt style / architectural injection",
            "format_details": "APrompt inserts P_q, P_k, P_v in each encoder (and decoder) layer; input prompts fixed at length 10; search over query-key and key-value prompt lengths in {1,5,10,20,50}; also ablations over prompt positions (first/last/all layers); trained for 100 epochs with LR 0.3",
            "performance_metric": "Average SuperGLUE score / per-task Accuracy / F1 / EM depending on task",
            "performance_value": "T5-Large APrompt average score 86.55% (Table 2); per-task examples: BoolQ 90.35% Acc, CB 95.83% F1/Acc, WSC 90.13% Acc (Table 2)",
            "baseline_performance": "Fine-Tuning (T5-Large) average 84.47%; P-Tuning v2 average 84.59% (Table 2)",
            "performance_change": "APrompt +2.08% absolute vs full fine-tuning average (T5-Large) and +1.96% absolute vs P-Tuning v2 (T5-Large); APrompt outperforms all prompt-baselines across model sizes (Table 2)",
            "experimental_setting": "T5-Base/Large/XL backbones; prompts trained for 100 epochs, LR 0.3, batch size 16; prompt length search for attention prompts; scores averaged over 5 runs",
            "statistical_significance": "Main SuperGLUE results reported as statistically significant vs baselines on each PLM (all p-value &lt; 0.005)",
            "uuid": "e7588.4"
        },
        {
            "name_short": "Prompt Positioning",
            "name_full": "Layer-wise Prompt Insertion Positioning",
            "brief_description": "Placement of prompts (input/attention) at different layers — first layer, last layer, first 12, last 12, alternating 12, or all layers — strongly affects performance; inserting prompts to all layers yields best results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-XL",
            "model_description": "T5-XL (3B) backbone with APrompt prompts applied at the specified positions (both encoder and decoder) while keeping backbone frozen.",
            "model_size": "3B",
            "task_name": "SuperGLUE subset (BoolQ, CB, RTE, WSC)",
            "task_description": "Evaluation of positional effects of prompt insertion on several representative SuperGLUE tasks.",
            "problem_format": "Prompt insertion positions (first-layer, last-layer, first-12 layers, last-12 layers, alternating-12 layers, all layers)",
            "format_category": "prompt style / prompt placement",
            "format_details": "Five position variants trained and compared; prompts inserted into both encoder and decoder at the chosen layers; input prompts also applied in analogous positions",
            "performance_metric": "Accuracy",
            "performance_value": "T5-XL APrompt (All layers) — BoolQ 90.72% / CB 95.48% / RTE 93.36% / WSC 96.17% (Table 4: 'APrompt (All)')",
            "baseline_performance": "First-layer: BoolQ 78.56% / CB 85.48% / RTE 90.28% / WSC 90.88%; Last-layer: BoolQ 75.82% / CB 84.37% / RTE 90.75% / WSC 91.15% (Table 4)",
            "performance_change": "Applying prompts to all layers yields very large absolute gains vs only first/last layer (e.g., BoolQ +11.9 to +14.9 pp absolute; WSC +4.8 to +5.3 pp absolute compared to first/last layer placements)",
            "experimental_setting": "T5-XL; prompt lengths tuned; same optimization/training hyperparameters as main experiments",
            "statistical_significance": null,
            "uuid": "e7588.5"
        },
        {
            "name_short": "Prompt Lengths (query-key & key-value)",
            "name_full": "Attention Prompt Length Hyperparameters",
            "brief_description": "The lengths (number of prompt rows/columns) of query-key prompts and key-value prompts are tuned and affect performance differently per task; no single prompt length is optimal across tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-XL",
            "model_description": "T5-XL (3B) with APrompt, where both query-key prompt length and key-value prompt length are hyperparameters searched over {1,5,10,20,50}.",
            "model_size": "3B",
            "task_name": "RTE and WSC (examples), general SuperGLUE",
            "task_description": "Investigate sensitivity of APrompt to attention-prompt length choices across tasks of varying difficulty.",
            "problem_format": "Two-dimensional prompt-length grid search (query-key length x key-value length)",
            "format_category": "prompt style / hyperparameter",
            "format_details": "Grid search across {1,5,10,20,50} for both prompt types; reported that RTE best with (query-key=5, key-value=10) while WSC best with (query-key=10, key-value=20); performance relatively stable within ranges",
            "performance_metric": "Accuracy",
            "performance_value": "T5-XL APrompt peak task results reported: RTE up to 93.36% and WSC up to 96.17% (with tuned prompt lengths; Table 2 & Fig.6 commentary)",
            "baseline_performance": null,
            "performance_change": "Task-dependent absolute differences due to prompt-length choice (examples: RTE best config vs other configs yielded the highest RTE score; WSC best config yielded highest WSC score — numerical deltas are task- and config-specific)",
            "experimental_setting": "T5-XL; lengths searched from {1,5,10,20,50}; other training settings as main experiments",
            "statistical_significance": null,
            "uuid": "e7588.6"
        },
        {
            "name_short": "Few-shot Presentation (32-shot)",
            "name_full": "Low-Resource Few-Shot Prompting (32 examples)",
            "brief_description": "Evaluation in a low-resource scenario where only 32 training examples per task are used to fine-tune prompts, showing that prompt-based methods (especially APrompt) can outperform full fine-tuning under severe data constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-XL",
            "model_description": "T5-XL (3B) with various prompt-tuning methods and full fine-tuning compared in 32-sample training regime.",
            "model_size": "3B",
            "task_name": "Few-shot SuperGLUE subset (BoolQ, CB, RTE, WSC)",
            "task_description": "Low-resource evaluation using 32 randomly selected training examples per task; measure robustness and generalization of prompt methods.",
            "problem_format": "Prompt-tuning with only 32 labeled examples (few-shot fine-tuning)",
            "format_category": "experimental setting / data availability",
            "format_details": "32 examples per task; same prompt architectures as in full-data experiments; best checkpoints selected via dev set; single fixed seed sampling per task",
            "performance_metric": "Accuracy / F1",
            "performance_value": "T5-XL 32-shot APrompt: BoolQ 75.66% / CB 78.51% / RTE 75.83% / WSC 72.30% (Table 3)",
            "baseline_performance": "Best baseline (P-Tuning v2) 32-shot: BoolQ 73.36% / CB 77.94% / RTE 73.48% / WSC 70.50% (Table 3)",
            "performance_change": "APrompt improvements vs best baseline: BoolQ +2.30 pp, CB +0.57 pp, RTE +2.35 pp, WSC +1.80 pp absolute (T5-XL few-shot Table 3)",
            "experimental_setting": "32 randomly sampled examples per task; fixed seed; training and selection as described in Section 6",
            "statistical_significance": null,
            "uuid": "e7588.7"
        },
        {
            "name_short": "Attention Prompt Balancing (ratio)",
            "name_full": "Ratio Trade-off between Key-Value and Query-Key Prompts",
            "brief_description": "Allocation of a fixed budget of trainable parameters between key-value prompts and query-key prompts affects task performance; neither extreme (all K/V or all Q/K) is optimal in most cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-XL",
            "model_description": "T5-XL with APrompt where total number of trainable attention-prompt parameters is fixed and the ratio allocated to key-value prompts is varied.",
            "model_size": "3B",
            "task_name": "SuperGLUE subset (BoolQ, CB, RTE, WSC)",
            "task_description": "Study effect of splitting fixed prompt-parameter budget across key-value vs query-key prompts.",
            "problem_format": "Fixed total prompt-parameter budget; vary key-value prompt fraction among {0,0.25,0.5,0.75,1}",
            "format_category": "prompt style / resource allocation",
            "format_details": "Run with key-value prompt ratios 0, 0.25, 0.5, 0.75, 1; evaluate per-task performance; report that best ratio differs by task (e.g., WSC best at 0.75, CB best at 0.5), and extremes 0 or 1 underperform",
            "performance_metric": "Accuracy",
            "performance_value": "Example highlights: APrompt (overall T5-XL full setup) achieves high task scores (CB 95.48%, WSC 96.17%); best per-task ratio observed: WSC best at 0.75 key-value ratio, CB best at 0.5 (Fig.7 commentary)",
            "baseline_performance": null,
            "performance_change": "Extremes (0 or 1) generally underperform mixed allocations; mixed allocations produce small but meaningful absolute gains task-dependently",
            "experimental_setting": "Fixed total number of trainable parameters; ratio swept over {0,0.25,0.5,0.75,1}; T5-XL backbone",
            "statistical_significance": null,
            "uuid": "e7588.8"
        },
        {
            "name_short": "Input-Prompt Removal (APrompt-)",
            "name_full": "APrompt Variant without Input Prompts",
            "brief_description": "A variant of APrompt which removes input-layer soft prompts and retains only attention-layer prompts; shows only minor degradation relative to full APrompt, indicating attention prompts carry most of the benefit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-XL",
            "model_description": "T5-XL APrompt variant with input prompts removed; attention prompts (P_q, P_k, P_v) still trained.",
            "model_size": "3B",
            "task_name": "SuperGLUE subset (BoolQ, CB, RTE, WSC)",
            "task_description": "Ablation to measure contribution of input prompts relative to attention prompts.",
            "problem_format": "APrompt architecture with no input prompts (only attention prompts active)",
            "format_category": "ablation / prompt composition",
            "format_details": "Compare APrompt- against APrompt and pruned APrompt+ variants (Table 5); same training settings as main experiments",
            "performance_metric": "Accuracy / F1",
            "performance_value": "APrompt- (T5-XL): BoolQ 90.41% / CB 95.28% / RTE 93.12% / WSC 95.82% (Table 5)",
            "baseline_performance": "Full APrompt (T5-XL): BoolQ 90.72% / CB 95.48% / RTE 93.36% / WSC 96.17% (Table 5)",
            "performance_change": "Small absolute drops when removing input prompts: BoolQ -0.31 pp, CB -0.20 pp, RTE -0.24 pp, WSC -0.35 pp (APrompt- vs APrompt)",
            "experimental_setting": "T5-XL; APrompt- trains only attention prompts; same training hyperparameters",
            "statistical_significance": null,
            "uuid": "e7588.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
            "rating": 2
        },
        {
            "paper_title": "P-tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
            "rating": 2
        },
        {
            "paper_title": "XPrompt: Exploring the Extreme of Prompt Tuning",
            "rating": 2
        },
        {
            "paper_title": "Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization",
            "rating": 1
        },
        {
            "paper_title": "Prefix-tuning: Optimizing Continuous Prompts for Generation",
            "rating": 1
        }
    ],
    "cost": 0.018508499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models</h1>
<p>Qifan Wang1, Yuning Mao1, Jingang Wang2, Hanchao Yu1, Shaoliang Nie1, Sinong Wang1, Fuli Feng3, Lifu Huang4, Xiaojun Quan5, Zenglin Xu6 and Dongfang Liu7*
1Meta AI 2Meituan Lab 3University of Science and Technology of China
4Virginia Tech 5Sun Yat-sen University 6Peng Chen Lab 7Rochester Institute of Technology
wqfcr@fb.com, yuningm@fb.com</p>
<h6>Abstract</h6>
<p>With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large pre-trained language models. However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement. In this work, we propose a novel Attention Prompt tuning method, namely APrompt, for efficient adaptation of pre-trained language models. We first demonstrate that existing prompt tuning can be considered as a special case of attention prompt tuning. We then formally introduce APrompt, which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning. Experimental results on the SuperGLUE benchmark consistently demonstrate that our proposed approach outperforms state-of-the-art baselines and full fine-tuning method with pretrained models at different scales. In addition, a comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach.</p>
<h2>1 Introduction</h2>
<p>Pre-trained Language Models (PLMs) have gained significant popularity in various natural language understanding tasks (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), exhibiting remarkable success under the <em>pretrain-then-finetune</em> paradigm. It has been consistently demonstrated in recent studies (Aribandi et al., 2022; Zhang et al., 2022) that scaling up the size of these models leads to improved performance. Consequently, large language models such as LLaMA 65B (Touvron et al., 2023), GPT-3 175B (Brown et al., 2020),</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of APrompt (ours) and previous works, including Fine-Tuning (Aribandi et al., 2022), Prompt Tuning (Lester et al., 2021), and P-Tuning V2 (Liu et al., 2022) methods. Our method consistently improves over prompt tuning methods and also outperforms fine-tuning method across tasks and model scales.</p>
<p>and PaLM 540B (Chowdhery et al., 2022) are becoming increasingly prevalent. Despite their compelling performance, fine-tuning large-scale PLMs is highly parameter-inefficient due to storing gradients and updating for all model parameters. This inefficiency also arises from the requirement to store and deploy a complete copy of the fine-tuned model for each individual task, resulting in computational expenses that hinder fast model adaptation.</p>
<p>To tackle the challenges associated with full fine-tuning, researchers have proposed parameter-efficient tuning approaches (Guo et al., 2021; He et al., 2022a; Hu et al., 2022) involving techniques such as <em>partial tuning</em> and <em>extra module</em>. <em>Partial tuning</em> methods (Yosinski et al., 2014) focus on fine-tuning only a portion of the backbone, such as the classifier head or the last few layers, while keeping the remaining layers frozen. On the other hand, <em>extra module</em> methods introduce learnable bias terms (Cai et al., 2020) or additional adapters (Houlsby et al., 2019) to the network for adaptation. These strategies operate within the <em>pretrain-</em></p>
<p>then-finetune paradigm and effectively reduce the number of learnable parameters. However, in general, these approaches tend to underperform the full fine-tuning models with large performance gaps.</p>
<p>Recently, prompt tuning approaches (Lester et al., 2021; Li and Liang, 2021; He et al., 2022b; Yang et al., 2023) have been proposed, which utilize a set of learnable soft prompts prepended to the input. These soft prompts consist of continuous embeddings that are updated during the tuning process while keeping the backbone frozen. Prompt tuning offers a conceptually simpler and more flexible method compared to other parameter-efficient tuning approaches. It has been demonstrated to perform closer to full model tuning, especially with large-scale PLMs (Ma et al., 2022; Razdaibiedina et al., 2023a). Prompt tuning provides a promising parameter-efficient alternative to fine-tuning, as the soft prompts used in this approach are typically orders of magnitude smaller, constituting less than $0.5 \%$ of the total model parameters. However, most existing prompt tuning approaches have mainly focused on modifying the input layers and have not thoroughly explored the core architecture of the Transformer's self-attention mechanism. Therefore, they often underperform to full fine-tuning and leave substantial room for improvement.</p>
<p>In this paper, we present a novel Attention Prompt tuning approach, APrompt, for efficient and effective large language model adaptation. We begin by reexamining the prompt tuning approach and establish, both theoretically and empirically, that its input prompts can be considered as specialized key-value prompts. We then formally introduce our APrompt. Unlike previous prompt tuning methods, APrompt incorporates three sets of learnable prompts: query, key, and value prompts. These prompts are prepended to the respective matrices in the self-attention block within the Transformer layer. During model tuning, these attention prompts are learned alongside the original input prompts, resulting in more effective guidance of attention computation for new tasks. Evaluation on the SuperGLUE benchmark showcases the superior performance of APrompt compared to state-of-the-art methods. The ablation study results provide strong evidence for the effectiveness and efficiency of the proposed attention prompts. We summarize the main contributions as follows:</p>
<ul>
<li>We establish a connection between existing prompt tuning methods and our approach,
demonstrating that input prompts can be viewed as a specialized form of attention prompts. This insight serves as valuable knowledge, enhancing our understanding of both the existing prompt tuning techniques and the novelty of our proposed approach.</li>
<li>We design novel attention prompt tuning by incorporating query, key, and value prompts into the self-attention computation along with the input prompts. By doing so, these attention prompts play a crucial role in effectively guiding the model's fine-tuning process, enabling faster and more accurate adjustments during the adaptation process.</li>
<li>We conduct comprehensive experiments on various tasks in the SuperGLUE benchmark, demonstrating the effectiveness of the proposed approach over several state-of-the-art prompt tuning and full fine-tuning methods.</li>
</ul>
<h2>2 Related Work</h2>
<p>Pre-trained Language Models Pre-trained Language Models (PLMs) (Yang et al., 2019; Ainslie et al., 2020; Zaheer et al., 2020; Zhao et al., 2023) have demonstrated huge success across various natural language processing tasks. Pioneering works such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) learn contextual representations with masked language model (MLM) and next sentence prediction tasks. Recently, a range of large-scale PLMs, including GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), and PaLM (Chowdhery et al., 2022), have emerged with diverse pretraining designs. However, the exponential increase in the number of parameters poses challenges in fine-tuning these models. It becomes computationally expensive to store and maintain all fine-tuned parameters for each tasks.</p>
<p>Parameter-Efficient Tuning As the size of PLMs becomes larger, it is increasingly unaffordable to update and save full model copies for each downstream application. Parameter-efficient tuning methods (Pfeiffer et al., 2020, 2021) arise in the era of LLM. Depending on whether new parameters are introduced, we divide parameter-efficient tuning methods into categories of partial tuning and extra module. Partial tuning methods simply update parts of the model such as the bias term (Zaken et al., 2022) or the last layers (Lee et al., 2019). Extra</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Input prompts $P$ are specialized key and value prompts with $P_{k}=H^{k} P$ and $P_{v}=H^{v} P$.
module methods introduce task-specific parameters to various locations of the model including SideTuning (Zhang et al., 2020) and Adaptors (Hu et al., 2022; Houlsby et al., 2019). There has also been effort unifying different parameter-efficient tuning methods for increased robustness and performance (Mao et al., 2022; He et al., 2022a).</p>
<p>Prompt Tuning Prompt tuning (Han et al., 2023; Yan et al., 2023) inserts learnable parameters to the model as virtual tokens where the insertion can happen at the model input (Lester et al., 2021) or each layer (Li and Liang, 2021). Later variants improve prompt tuning methods for NLU (Liu et al., 2022) and NLG (An et al., 2022) tasks respectively through a series of optimization and adaptations. More recent studies add residual connections to improve the performance and stability of prompt tuning (Razdaibiedina et al., 2023b) and extend prompt tuning to the continual learning setting (Razdaibiedina et al., 2023a). However, most of these methods only simply add prompts to input layers, which greatly limited their performances. Most recently, mixture prompt tuning has been proposed in MixPrompt (Yang et al., 2023) and $\mathrm{E}^{2} \mathrm{VPT}$ (Han et al., 2023), which combines the input prompts with key-value prompts. These methods can be treated as special cases of our attention prompt tuning approach.</p>
<h2>3 Prompt Tuning Revisit</h2>
<h3>3.1 Preliminary</h3>
<p>Prompt tuning methods (Lester et al., 2021; Liu et al., 2022) are proposed as a group of parameter efficient models for fast adaptation of largescale PLMs to downstream tasks. They introduce a set of task-specific prompts or prompt tokens $P \in R^{d \times m}$, and prepend them to the input sequence $X \in R^{d \times n}$ to form a new input $X_{\text {new }}=[P, X] \in R^{d \times(m+n)}$, as shown in left Figure 2. Here $m$ is the length of prompt tokens, $n$ is the input sequence length, and $d$ is the dimension of the embedding vector. These prompts are learned on the downstream task during fine-tuning with the backbone frozen. Prompt tuning achieves promis-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">T5-Large</th>
<th style="text-align: center;">WSC</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">Boolq</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prompt Tuning</td>
<td style="text-align: center;">78.31</td>
<td style="text-align: center;">88.41</td>
<td style="text-align: center;">83.65</td>
</tr>
<tr>
<td style="text-align: center;">Fixed Key-value Prompts</td>
<td style="text-align: center;">78.31</td>
<td style="text-align: center;">88.41</td>
<td style="text-align: center;">83.65</td>
</tr>
<tr>
<td style="text-align: center;">Key-value Prompts</td>
<td style="text-align: center;">78.88</td>
<td style="text-align: center;">88.93</td>
<td style="text-align: center;">84.06</td>
</tr>
<tr>
<td style="text-align: center;">Prompt Tuning + Key-value Prompts</td>
<td style="text-align: center;">$\mathbf{7 9 . 1 2}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 1 7}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 2 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance (Accuracy) of Prompt Tuning and different key-value prompts variants on WSC, CB and Boolq tasks from SuperGLUE with T5-Large model.
ing results compared to other parameter-efficient tuning methods.</p>
<h3>3.2 Connection with Key-Value Prompts</h3>
<p>In this section, we investigate deeper on how the prompt tuning works and show that traditional input prompts are equivalent to constrained keyvalue prompts. Recall that in prompt tuning, the prompt tokens are first prepended to the input tokens. The new sequence $X_{\text {new }}=[P, X]$ is then fed into the Transformer encoder layer to compute the contextual embeddings of the text tokens for the next layer. The self-attention is defined as:</p>
<p>$$
\begin{gathered}
\operatorname{Attn}([P, X])=\operatorname{softmax}\left(\frac{Q^{T} K_{n e w}}{\sqrt{d}}\right) V_{n e w} \
Q=H^{q} X, K_{n e w}=H^{k} X_{n e w}, V_{n e w}=H^{v} X_{n e w}
\end{gathered}
$$</p>
<p>where $Q, K_{\text {new }}$ and $V_{\text {new }}$ are the new query, key and value embedding matrices, with $H^{q}, H^{k}$ and $H^{v}$ as the pre-trained model parameters that are frozen. It is worth noting that for the query $Q$, there is no need to compute a new one since only the original text tokens $X$ are updated and used in the next layer. Then we have:</p>
<p>$$
K_{\text {new }}=H^{k} X_{\text {new }}=\left[H^{k} P, H^{k} X\right]=\left[P_{k}, K\right]
$$</p>
<p>Similarly, we have $V_{\text {new }}=\left[P_{v}, V\right]$. Therefore, we can conclude that adding the prompt tokens $P$ during prompt tuning is equivalent of prepending key prompts $P_{k}$ and value prompts $P_{v}$ to the original key and value matrices respectively, as shown in Figure 2. Note that these key-value prompts are constrained or coupled by the input prompts $P$.</p>
<h3>3.3 Empirical Study</h3>
<p>To further validate the findings, we conduct an experiment by comparing four methods, Prompt Tuning (Lester et al., 2021), Fixed Key-value Prompt, Key-value Prompt, and Prompt Tuning + Key-value Prompt, on three tasks from SuperGLUE with T5Large backone. Fixed Key-value Prompt directly adds fixed key and value prompts computed from</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overview of our APrompt model. We introduce three sets of trainable attention prompts, namely query, key, and value prompts, during the fine-tuning of the model, in addition to the input prompts. These prompts are incorporated into the query, key, and value matrices within the multi-head attention computation.</p>
<p>The optimal prompts <em>P</em><em> learned in Prompt Tuning, i.e., </em>P<sub>k</sub><em> = </em>H<sup>k</sup>P<strong> and <em>P<sub>v</sub></em> = *H<sup>v</sup>P</strong> , without any tuning. Key-value Prompt learns the optimal key and value prompts during fine-tuning. Prompt Tuning + Key-value Prompt learns both the input prompts and key-value prompts during fine-tuning.</p>
<p>The comparative results of these methods are presented in Table 1. Firstly, it is clear that the Fixed Key-value Prompts method achieves identical results to Prompt Tuning, which aligns with our expectations and <em>validates the equivalence between input prompts and constrained key-value prompts</em>. Secondly, when allowing the key-value prompts to be learned during fine-tuning, we observe improved performance compared to fixed key-value prompts. The reason is that the fixed key-value prompts can be seen as a special case within the search space of unconstrained key-value prompts. Lastly, combining both input prompts and key-value prompts during fine-tuning leads to the highest performance. Our hypothesis is that while key-value prompts theoretically have the potential to encompass the information contained in input prompts, they exist in different embedding spaces. In practice, input prompts can provide additional value during the fine-tuning process. Further analysis and discussion are provided in the experimental section.</p>
<h1>4 Attention Prompt Tuning</h1>
<p>Drawing inspiration from the insights presented in Section 3, we propose an attention prompt tuning approach by introducing the attention prompts in the Transformer layers to facilitate attention computation. The overall model architecture of APrompt is depicted in Figure 3. Fundamentally, our model enables the training of only three components while keeping all other parameters frozen. These components are as follows: (1) Input prompts, denoted as <em>P<sub>i</sub></em>, which are inserted at the beginning of the input sequence for each Transformer encoder layer. (2) Attention prompts, represented by <em>P<sub>q</sub></em>, <em>P<sub>k</sub></em>, and <em>P<sub>v</sub></em>, are incorporated into the query, key, and value matrices within the self-attention module, respectively. These prompts allow the model to learn new attention patterns from the fine-tuning data. (3) A task-specific head, which is a lightweight module dedicated to the specific task and can be trained efficiently.</p>
<h3>4.1 Input Prompts</h3>
<p>In a similar vein to traditional prompt tuning methods (Lester et al., 2021; Li and Liang, 2021), input prompts consist of a set of <em>d</em>-dimensional embedding vectors, where the dimensionality matches that of the text tokens. These prompts are inserted at the beginning of the input sequence in each Transformer encoder layer and interact with all the text tokens. They facilitate the learning of task-specific embeddings, effectively guiding the model's performance on new tasks.</p>
<p>Formally, these input prompts are defined as <em>P<sub>i</sub></em> = {<em>P<sub>i</sub><sup>1</sup></em>, <em>P<sub>i</sub><sup>2</sup></em>, ..., <em>P<sub>i</sub><sup>N</sup></em>}, where <em>P<sub>i</sub><sup>l</sup></em> denotes the learnable input prompts in the <em>j<sub>th</sub></em> Transformer encoder layer, and <em>N</em> is the total number of layers. Then the encoder layers are represented as:</p>
<p>$$Z^1 = L_1(P_i^1, E)$$</p>
<p>$$
Z^{j}=L_{j}\left(P_{i}^{j}, Z^{j-1}\right) \quad j=2,3, \ldots, N
$$</p>
<p>where $Z^{j}$ represents the contextual embeddings of the text tokens computed by the $j_{t h}$ encoder layer. The different colors indicate trainable and frozen parameters, respectively. $E$ is the text token embeddings initialized from the backbone.</p>
<h3>4.2 Attention Prompts</h3>
<p>While input prompts are effective in acquiring knowledge about the new task, they do not possess the capability to guide the interaction of information within each encoder layer. During fine-tuning on a new task with new data, the word distribution may differ significantly from the examples seen during pre-training. Consequently, it becomes imperative to enhance the model's capacity for capturing new information from the fine-tuning data. This entails enabling better attention among the input tokens to effectively learn new patterns that emerge in the task-specific context.</p>
<p>$$
Q_{m}=\square K_{m}=\square K_{m} \quad V_{m}=\square V
$$</p>
<p>Figure 4: The new query, key and value matricies.
In order to address this, we introduce a novel set of attention prompts that are integrated into the attention block within each encoder layer. These attention prompts can be categorized into two groups: query-key prompts and key-value prompts. The query-key prompts, denoted as $P_{q}^{Q K}$ and $P_{k}^{Q K}$, consist of small matrices (comprising a few rows) that are appended to the original query and key matrices within the attention module. By incorporating these query-key prompts, we enhance the computation of attention maps among the tokens, thereby improving the attention mechanism. The key-value prompts, represented by $P_{k}^{K V}$ and $P_{v}^{K V}$, which are two supplementary matrices (a few columns) inserted to the key and value matrices, respectively. These key-value prompts provide additional information for the input tokens to attend to, thereby enhancing the representation of the learned embeddings. By incorporating both query-key prompts and key-value prompts, we aim to enable more effective information interaction and capture new patterns during the fine-tuning process. The new query, key and value matrices are augmented with these new attention prompts as shown in Figure 4. It is worth noting that the new key matrix is appended by both the key prompts from query-key and key-value prompts. Then the
new attention computations are:</p>
<p>$$
\begin{aligned}
L(\cdot) &amp; =\operatorname{MLP}(\operatorname{LN}(\operatorname{MSA}(\cdot))) \
\operatorname{MSA}(\cdot) &amp; =\operatorname{softmax}\left(\frac{Q_{m}^{2}-K_{m w}}{\sqrt{d}}\right) V_{m e w}
\end{aligned}
$$</p>
<p>where MLP and LN are the frozen multi-layer perceptron and layer norm, and MSA is the multi-head self-attention inside the Transformer encoder layer. In this way, the attention prompts can effectively guide the model adaptation to the new task.</p>
<h3>4.3 Task-specific Head</h3>
<p>For each downstream task, we also fine-tune a taskspecific head, which is a very small module dedicated to the specific task to generate the predictions.</p>
<p>$$
y=\operatorname{Head}\left(Z^{N}\right)
$$</p>
<p>where $Z^{N}$ is the output contextual embedding from the top layer of the encoder.</p>
<h2>5 Experiments</h2>
<h3>5.1 Datasets</h3>
<p>Following previous works on prompt tuning (Lester et al., 2021; Liu et al., 2022), we use NLU tasks from the SuperGLUE benchmark to evaluate the performance of the language model (Raffel et al., 2020; Aribandi et al., 2022). Specifically, we use the following 8 datasets: BoolQ (Clark et al., 2019), CB (Jiang and de Marneffe, 2019), COPA (Roemmele et al., 2011), MRC (Khashabi et al., 2018), ReC (Zhang et al., 2018), RTE (Giampiccolo et al., 2007), WiC (Pilehvar and CamachoCollados, 2019) and WSC (Levesque et al., 2012). More details are provided in the Appendix.</p>
<h3>5.2 Baselines</h3>
<p>Our model is compared with five state-of-the-art prompt tuning and fine-tuning methods.
Fine-Tuning (Aribandi et al., 2022) is the standard full fine-tuning approach of T5, where all the pre-trained parameters are fine-tuned.
Prompt-Tuning (Lester et al., 2021) is the vanilla prompt tuning approach which adds the input prompts in the first input layer.
P-Tuning v2 (Liu et al., 2022) builds on top of Prompt-Tuning by inserting a set of individual prompts to each Transformer layers.
XPrompt (Ma et al., 2022) designs more efficient prompts by pruning the least important token-level and piece-level prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Para</th>
<th style="text-align: center;">Boolq <br> Acc</th>
<th style="text-align: center;">CB <br> F1/Acc</th>
<th style="text-align: center;">COPA <br> Acc</th>
<th style="text-align: center;">MRC <br> F1/EM</th>
<th style="text-align: center;">ReC <br> F1/EM</th>
<th style="text-align: center;">RTE <br> Acc</th>
<th style="text-align: center;">WiC <br> Acc</th>
<th style="text-align: center;">WSC <br> Acc</th>
<th style="text-align: center;">Average <br> Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T5-Base (220M)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Fine-Tuning* (Aribandi et al., 2022)</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">82.30</td>
<td style="text-align: center;">91.30</td>
<td style="text-align: center;">60.00</td>
<td style="text-align: center;">58.25</td>
<td style="text-align: center;">80.55</td>
<td style="text-align: center;">84.50</td>
<td style="text-align: center;">69.30</td>
<td style="text-align: center;">81.70</td>
<td style="text-align: center;">76.10</td>
</tr>
<tr>
<td style="text-align: center;">Prompt-Tuning (Lester et al., 2021)</td>
<td style="text-align: center;">$0.06 \%$</td>
<td style="text-align: center;">78.12</td>
<td style="text-align: center;">84.42</td>
<td style="text-align: center;">54.37</td>
<td style="text-align: center;">51.14</td>
<td style="text-align: center;">71.35</td>
<td style="text-align: center;">75.27</td>
<td style="text-align: center;">62.29</td>
<td style="text-align: center;">67.36</td>
<td style="text-align: center;">68.04</td>
</tr>
<tr>
<td style="text-align: center;">P-Tuning v2 (Liu et al., 2022)</td>
<td style="text-align: center;">$0.53 \%$</td>
<td style="text-align: center;">80.81</td>
<td style="text-align: center;">90.23</td>
<td style="text-align: center;">61.28</td>
<td style="text-align: center;">55.64</td>
<td style="text-align: center;">78.13</td>
<td style="text-align: center;">81.98</td>
<td style="text-align: center;">67.56</td>
<td style="text-align: center;">78.32</td>
<td style="text-align: center;">74.35</td>
</tr>
<tr>
<td style="text-align: center;">XPrompt (Ma et al., 2022)</td>
<td style="text-align: center;">$0.04 \%$</td>
<td style="text-align: center;">79.67</td>
<td style="text-align: center;">86.72</td>
<td style="text-align: center;">56.95</td>
<td style="text-align: center;">53.08</td>
<td style="text-align: center;">74.36</td>
<td style="text-align: center;">78.29</td>
<td style="text-align: center;">64.31</td>
<td style="text-align: center;">73.68</td>
<td style="text-align: center;">70.88</td>
</tr>
<tr>
<td style="text-align: center;">ResPrompt (Razdaibiedina et al., 2023b)</td>
<td style="text-align: center;">$0.21 \%$</td>
<td style="text-align: center;">79.25</td>
<td style="text-align: center;">85.33</td>
<td style="text-align: center;">58.64</td>
<td style="text-align: center;">52.91</td>
<td style="text-align: center;">73.19</td>
<td style="text-align: center;">77.14</td>
<td style="text-align: center;">62.36</td>
<td style="text-align: center;">70.82</td>
<td style="text-align: center;">69.95</td>
</tr>
<tr>
<td style="text-align: center;">A Prompt (Ours)</td>
<td style="text-align: center;">$0.45 \%$</td>
<td style="text-align: center;">81.83</td>
<td style="text-align: center;">91.86</td>
<td style="text-align: center;">61.54</td>
<td style="text-align: center;">59.07</td>
<td style="text-align: center;">81.18</td>
<td style="text-align: center;">85.76</td>
<td style="text-align: center;">69.50</td>
<td style="text-align: center;">81.49</td>
<td style="text-align: center;">76.84</td>
</tr>
<tr>
<td style="text-align: center;">T5-Large (770M)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Fine-Tuning* (Aribandi et al., 2022)</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">88.30</td>
<td style="text-align: center;">95.35</td>
<td style="text-align: center;">87.00</td>
<td style="text-align: center;">67.25</td>
<td style="text-align: center;">87.85</td>
<td style="text-align: center;">90.60</td>
<td style="text-align: center;">73.50</td>
<td style="text-align: center;">88.50</td>
<td style="text-align: center;">84.47</td>
</tr>
<tr>
<td style="text-align: center;">Prompt-Tuning (Lester et al., 2021)</td>
<td style="text-align: center;">$0.03 \%$</td>
<td style="text-align: center;">83.65</td>
<td style="text-align: center;">88.41</td>
<td style="text-align: center;">82.67</td>
<td style="text-align: center;">63.28</td>
<td style="text-align: center;">82.46</td>
<td style="text-align: center;">85.19</td>
<td style="text-align: center;">71.05</td>
<td style="text-align: center;">78.31</td>
<td style="text-align: center;">79.25</td>
</tr>
<tr>
<td style="text-align: center;">P-Tuning v2 (Liu et al., 2022)</td>
<td style="text-align: center;">$0.48 \%$</td>
<td style="text-align: center;">87.92</td>
<td style="text-align: center;">95.56</td>
<td style="text-align: center;">86.20</td>
<td style="text-align: center;">70.47</td>
<td style="text-align: center;">89.03</td>
<td style="text-align: center;">89.14</td>
<td style="text-align: center;">71.81</td>
<td style="text-align: center;">86.59</td>
<td style="text-align: center;">84.59</td>
</tr>
<tr>
<td style="text-align: center;">XPrompt (Ma et al., 2022)</td>
<td style="text-align: center;">$0.02 \%$</td>
<td style="text-align: center;">85.54</td>
<td style="text-align: center;">91.39</td>
<td style="text-align: center;">85.05</td>
<td style="text-align: center;">67.32</td>
<td style="text-align: center;">85.47</td>
<td style="text-align: center;">87.30</td>
<td style="text-align: center;">73.22</td>
<td style="text-align: center;">80.28</td>
<td style="text-align: center;">81.95</td>
</tr>
<tr>
<td style="text-align: center;">ResPrompt (Razdaibiedina et al., 2023b)</td>
<td style="text-align: center;">$0.15 \%$</td>
<td style="text-align: center;">83.51</td>
<td style="text-align: center;">90.64</td>
<td style="text-align: center;">82.79</td>
<td style="text-align: center;">65.16</td>
<td style="text-align: center;">84.72</td>
<td style="text-align: center;">86.97</td>
<td style="text-align: center;">71.13</td>
<td style="text-align: center;">80.36</td>
<td style="text-align: center;">80.66</td>
</tr>
<tr>
<td style="text-align: center;">A Prompt (Ours)</td>
<td style="text-align: center;">$0.37 \%$</td>
<td style="text-align: center;">90.35</td>
<td style="text-align: center;">95.83</td>
<td style="text-align: center;">88.32</td>
<td style="text-align: center;">71.98</td>
<td style="text-align: center;">90.64</td>
<td style="text-align: center;">90.47</td>
<td style="text-align: center;">74.67</td>
<td style="text-align: center;">90.13</td>
<td style="text-align: center;">86.55</td>
</tr>
<tr>
<td style="text-align: center;">T5-XL (3B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Fine-Tuning* (Aribandi et al., 2022)</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">89.60</td>
<td style="text-align: center;">94.20</td>
<td style="text-align: center;">96.00</td>
<td style="text-align: center;">76.15</td>
<td style="text-align: center;">92.05</td>
<td style="text-align: center;">91.70</td>
<td style="text-align: center;">74.30</td>
<td style="text-align: center;">95.20</td>
<td style="text-align: center;">88.65</td>
</tr>
<tr>
<td style="text-align: center;">Prompt-Tuning (Lester et al., 2021)</td>
<td style="text-align: center;">$0.01 \%$</td>
<td style="text-align: center;">87.58</td>
<td style="text-align: center;">91.25</td>
<td style="text-align: center;">91.56</td>
<td style="text-align: center;">73.49</td>
<td style="text-align: center;">90.14</td>
<td style="text-align: center;">89.35</td>
<td style="text-align: center;">74.21</td>
<td style="text-align: center;">87.16</td>
<td style="text-align: center;">85.59</td>
</tr>
<tr>
<td style="text-align: center;">P-Tuning v2 (Liu et al., 2022)</td>
<td style="text-align: center;">$0.45 \%$</td>
<td style="text-align: center;">90.11</td>
<td style="text-align: center;">94.08</td>
<td style="text-align: center;">95.33</td>
<td style="text-align: center;">75.21</td>
<td style="text-align: center;">92.39</td>
<td style="text-align: center;">92.13</td>
<td style="text-align: center;">75.46</td>
<td style="text-align: center;">94.25</td>
<td style="text-align: center;">88.62</td>
</tr>
<tr>
<td style="text-align: center;">XPrompt (Ma et al., 2022)</td>
<td style="text-align: center;">$0.01 \%$</td>
<td style="text-align: center;">89.14</td>
<td style="text-align: center;">92.73</td>
<td style="text-align: center;">95.18</td>
<td style="text-align: center;">75.01</td>
<td style="text-align: center;">91.18</td>
<td style="text-align: center;">92.16</td>
<td style="text-align: center;">74.85</td>
<td style="text-align: center;">89.43</td>
<td style="text-align: center;">87.46</td>
</tr>
<tr>
<td style="text-align: center;">ResPrompt (Razdaibiedina et al., 2023b)</td>
<td style="text-align: center;">$0.04 \%$</td>
<td style="text-align: center;">88.46</td>
<td style="text-align: center;">92.54</td>
<td style="text-align: center;">93.12</td>
<td style="text-align: center;">75.17</td>
<td style="text-align: center;">91.20</td>
<td style="text-align: center;">91.64</td>
<td style="text-align: center;">75.32</td>
<td style="text-align: center;">89.15</td>
<td style="text-align: center;">87.08</td>
</tr>
<tr>
<td style="text-align: center;">A Prompt (Ours)</td>
<td style="text-align: center;">$0.32 \%$</td>
<td style="text-align: center;">90.72</td>
<td style="text-align: center;">95.48</td>
<td style="text-align: center;">95.83</td>
<td style="text-align: center;">78.68</td>
<td style="text-align: center;">93.75</td>
<td style="text-align: center;">93.36</td>
<td style="text-align: center;">76.43</td>
<td style="text-align: center;">96.17</td>
<td style="text-align: center;">90.05</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance comparison result (\%) on SuperGLUE development set. ${ }^{<em> * </em>}$ indicates the results reported in (Aribandi et al., 2022). 'Para' is the number of trainable parameters. The best results are in bold with underline representing the second best ones. For tasks with two metrics, the average score is reported. All scores are averaged over 5 runs. Results are statistically significant with respect to all baselines on each PLM (all p-value $&lt;0.005$ ).</p>
<p>ResPrompt (Razdaibiedina et al., 2023b) adds residual connections to improve the performance and stability of prompt tuning.</p>
<h3>5.3 Implementation Details</h3>
<p>APROMPT is implemented with the OpenPrompt library (Ding et al., 2022), which is a unified and extensible toolkit for prompt tuning research. Our model is trained on 16 NVIDIA Tesla V100 GPUs. We translate each SuperGLUE dataset into a text-to-text format following (Raffel et al., 2020). Three scales pre-trained models are used: T5-Base, T5Large and T5-XL with 200M, 770M and 3B parameters, respectively. Following previous studies (Lester et al., 2021; Ma et al., 2022), we train our prompts for 100 epochs with a constant learning rate of 0.3 and a batch size of 16 . There are three hyperparameters in our model: the lengthes of input, query-key and key-value prompts. For our method, we set the number of input prompts to 10 (as we found our model is less sensitive to it), and linearly search the best prompt length for both query-key and key-value prompts from ${1,5,10$, $20,50}$. For all prompt tuning baselines, we search the best input prompt length from ${5,10,20,50$, $100}$. The best checkpoints are selected via early stopping on the development set. The models are trained using the Adafactor (Shazeer and Stern,</p>
<p>2018) optimizer with weight decay $1 e^{-5}$.</p>
<h3>5.4 Main Results</h3>
<p>The main performance comparison results are presented in Table 2. There are several key observations from these results. First, AProMPT consistently outperforms all prompt tuning baselines across different backbone models, showcasing the effective design of its attention prompts. For instance, when evaluating on Boolq with T5-Large, the Acc score of APROMPT demonstrates a significant improvement of $5.62 \%$ and $2.76 \%$ compared to two strong methods, XPrompt and P-Tuning v2, respectively. This highlights the limitation of existing prompt tuning approaches that primarily focus on designing input prompt tokens, failing to capture the intricate token interactions within new data. In contrast, the attention prompts employed by our approach successfully bridge this gap, resulting in enhanced performance. Second, AProMPT outperforms the full fine-tuning method in most cases, while other prompt tuning baselines still exhibit certain gaps, particularly when using smaller backbone models like T5-Base. This observation highlights the effectiveness of our approach across a range of natural language understanding tasks. Moreover, our model achieves these results while training only around $0.4 \%$ of the parame-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Boolq</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">WSC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fine-Tuning</td>
<td style="text-align: center;">71.31</td>
<td style="text-align: center;">75.72</td>
<td style="text-align: center;">70.35</td>
<td style="text-align: center;">67.52</td>
</tr>
<tr>
<td style="text-align: center;">Prompt-Tuning</td>
<td style="text-align: center;">69.48</td>
<td style="text-align: center;">75.16</td>
<td style="text-align: center;">71.73</td>
<td style="text-align: center;">67.47</td>
</tr>
<tr>
<td style="text-align: center;">P-Tuning v2</td>
<td style="text-align: center;">73.36</td>
<td style="text-align: center;">77.94</td>
<td style="text-align: center;">73.48</td>
<td style="text-align: center;">70.50</td>
</tr>
<tr>
<td style="text-align: center;">XPrompt</td>
<td style="text-align: center;">71.53</td>
<td style="text-align: center;">76.57</td>
<td style="text-align: center;">72.68</td>
<td style="text-align: center;">69.75</td>
</tr>
<tr>
<td style="text-align: center;">ResPrompt</td>
<td style="text-align: center;">70.38</td>
<td style="text-align: center;">76.20</td>
<td style="text-align: center;">70.85</td>
<td style="text-align: center;">68.35</td>
</tr>
<tr>
<td style="text-align: center;">APrompt (Ours)</td>
<td style="text-align: center;">$\mathbf{7 5 . 6 6}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 5 1}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 8 3}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 3 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance comparision results with fewshot (32 samples) setting on Boolq, CB, RTE and WSC tasks for the T5-XL model. APrompt consistently outperforms all baselines in low resource scenarios.
ters in the backbone, making it significantly more parameter-efficient than the full fine-tuned model. It is worth noting that although APrompt introduces additional attention prompts, the length of the input prompts are largely reduced (fixed to 10) and thus resulting in even less total trainable parameters compared with P-Tuning v2. Third, it is worth noting that the gap between fine-tuning and other prompt tuning methods diminishes as the size of the backbone models increases. This finding aligns with previous studies (Lester et al., 2021; Ma et al., 2022) and underscores the trend of convergence between fine-tuning and alternative prompt tuning approaches.</p>
<h2>6 Analysis and Discussion</h2>
<p>Results on Low-resource Scenario We conducted further evaluations to assess the performance of APrompt and other baseline models in a low-resource setting. Following (Schick and Schütze, 2021), we randomly selected 32 examples for each task as the new training set, using a fixed random seed. We fine-tuned the prompt model on this limited training set and reported the results on the full dev set using the best checkpoint in Table 3. It is evident that all methods experience a significant drop in performance due to the limited data available for training. Nevertheless, APrompt consistently outperforms the baseline models on tasks such as Boolq, CB, RTE, and WSC. Additionally, we observe that most prompt tuning approaches achieve better results compared to finetuning, indicating that despite the challenges of overfitting when training with limited data, prompt tuning methods exhibit superior generalization capabilities compared to full fine-tuning.</p>
<p>Impact of Different Prompts To investigate the impact of different prompts in our model, we conducted an ablation study by exploring various prompt combinations in APrompt. Specif-
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Ablation study on the impact of different prompt combinations on four tasks of SuperGLUE.
ically, we experimented with four additional models: one without input prompts, one without querykey prompts, one without key-value prompts, and one without both query-key and key-value prompts. The Acc scores obtained on four SuperGLUE tasks with different backbone models are presented in Figure 5. The results reveal that the model's performance drops when any of the trainable prompts is removed, which aligns with our expectations. Furthermore, we observed that the performance drop of APrompt without input prompts is relatively small compared to the models without attention prompts. This suggests the significance of both query-key and key-value prompts in comparison to input prompts, thereby validating the analysis presented in section 3. Once again, it is worth noting that combining all prompts in APrompt leads to the best performance.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Ablation study of query-key and key-value prompt lengths. We vary the number of prompts for different combinations, and evaluate (Acc) on RTE and WSC tasks with T5-XL as the backbone model.</p>
<p>Impact of Prompt Length In APrompt, the lengths of query-key prompts and key-value</p>
<table>
<thead>
<tr>
<th>Position</th>
<th>Boolq</th>
<th>CB</th>
<th>RTE</th>
<th>WSC</th>
</tr>
</thead>
<tbody>
<tr>
<td>First-layer</td>
<td>78.56</td>
<td>85.48</td>
<td>90.28</td>
<td>90.88</td>
</tr>
<tr>
<td>First 12-layers</td>
<td>88.35</td>
<td>88.64</td>
<td>92.42</td>
<td>94.32</td>
</tr>
<tr>
<td>Last-layer</td>
<td>75.82</td>
<td>84.37</td>
<td>90.75</td>
<td>91.15</td>
</tr>
<tr>
<td>Last 12-layers</td>
<td>82.55</td>
<td>88.49</td>
<td>92.67</td>
<td>95.36</td>
</tr>
<tr>
<td>Alternative 12-layers</td>
<td>90.31</td>
<td>90.14</td>
<td>92.74</td>
<td>95.21</td>
</tr>
<tr>
<td>APrompt (All)</td>
<td>$\mathbf{9 0 . 7 2}$</td>
<td>$\mathbf{9 5 . 4 8}$</td>
<td>$\mathbf{9 3 . 3 6}$</td>
<td>$\mathbf{9 6 . 1 7}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance comparison with different prompt positions on Boolq, CB, RTE and WSC for T5-XL.
prompts are the only hyperparameters that require tuning. To further analyze the impact of different prompt lengths on model performance, we conducted an ablation study by modifying both prompt lengths across ${1,5,10,20,50}$. We experiment over all possible length combinations, and a detailed discussion on how to balance these two prompts will be provided in later experiments. The model performance results of all prompt length combinations on RTE and WSC are shown in Figure 6. It can be seen that there is no universal optimal prompt length that consistently achieves the best performance across both tasks. For instance, on RTE, the highest score is obtained with 10 keyvalue prompts and 5 query-key prompts, while on WSC, the best performance is achieved with 20 key-value prompts and 10 query-key prompts. We hypothesize that different tasks and datasets exhibit distinct data distributions, with 'hard' tasks potentially requiring longer prompts to effectively capture the underlying patterns and knowledge within the data. However, this comes with the trade-off of an increased number of trainable parameters. Nonetheless, we observed that our model's performance remains relatively stable within a certain range of prompt lengths.</p>
<p>Impact of Prompt Positions This study evaluates the impact of prompt positions to the model performance. Concretely, we train five additional models with different prompt locations (applied to both encoder and decoder), including only first layer, last layer, first 12 layers, last 12 layers and alternative 12 layers. The performance comparison results on MAVE are reported in Table 4. It is not surprising to see that inserting prompts to all encoder and decoder layers achieves the best performance. We can also observe that only putting the prompts to the first (input) or last (output) layer results in large performance drops, which is consistent with the observations in other prompt tuning works (Liu et al., 2022; Jia et al., 2022).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Performance comparison of different ratios of the key-value prompts when fixing the total number of trainable parameters using T5-XL.</p>
<p>Effect of Attention Prompts Balancing The query-key and key-value prompts in APrompt contribute differently to the model performance. To further investigate their correlation and effectiveness, we conduct an experiment by fixing the total number of trainable parameters, and adjusting the ratio of key-value prompts from ${0,0.25,0.5,0.75,1}$. The model performances at different ratios on four SuperGLUE tasks are illustrated in Figure 7. We observe slightly different patterns on different tasks. For example, on WSC, key-value prompts with 0.75 ratio achieves the best score, while key-value prompts with 0.5 gives the best performance on CB. Nevertheless, APrompt with ratio 0 (no key-value prompts) or 1 (no querykey prompts) underperforms other prompt combinations in most cases, indicating the effectiveness of both attention prompts.</p>
<p>Variants of APrompt We compare APrompt with its two variants to analyze the performancescale trade-off. Firstly, we remove the input prompts, retaining only the attention prompts, resulting in a variant named APrompt-. Additionally, we apply the pruning technique in XPrompt (Ma et al., 2022) to eliminate the least important prompts, creating a variant called APrompt +. From the results in Table 5, we observe that the contribution of input prompts is not particularly significant, aligning with the findings in section 3. When applying prompt trimming, we note that while the number of trainable parameters decreases, the performance behavior varies across different tasks, leaving a room for further exploration in terms of finding the optimal balance between the number of prompts and model performance.</p>
<h2>7 Conclusions</h2>
<p>This work first connects existing prompt tuning with attention prompt tuning and show that input</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Para</th>
<th style="text-align: center;">Boolq</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">WSC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">APROMPT-</td>
<td style="text-align: center;">$0.25 \%$</td>
<td style="text-align: center;">90.41</td>
<td style="text-align: center;">95.28</td>
<td style="text-align: center;">93.12</td>
<td style="text-align: center;">95.82</td>
</tr>
<tr>
<td style="text-align: left;">APROMPT</td>
<td style="text-align: center;">$0.32 \%$</td>
<td style="text-align: center;">90.72</td>
<td style="text-align: center;">95.48</td>
<td style="text-align: center;">93.36</td>
<td style="text-align: center;">96.17</td>
</tr>
<tr>
<td style="text-align: left;">APROMPT +</td>
<td style="text-align: center;">$0.21 \%$</td>
<td style="text-align: center;">91.05</td>
<td style="text-align: center;">95.34</td>
<td style="text-align: center;">93.51</td>
<td style="text-align: center;">96.02</td>
</tr>
</tbody>
</table>
<p>Table 5: Different variants of our model with T5-XL.
prompts can be considered as a special case of keyvalue prompts in the attention layer. Inspired by the observation, we introduce three sets of new prompts, namely query, key, and value prompts, and incorporate them into the attention layer to guide the attention computation during fine-tuning. Experimental results on SuperGLUE demonstrate the superior performance of our model over several state-of-the-art baselines.</p>
<h2>Limitations</h2>
<p>There are two limitations of our APROMPT model. First, while APROMPT outperforms other prompt tuning and fine-tuning approaches, identifying the optimal combination of attention prompts automatically poses a challenge that remains unanswered. In our experiments, we conduct grid search to empirically determine the optimal prompt length. However, in the future, we intend to explore a systematic solution that can identify either the optimal combination or a suboptimal one. Second, our current model learns task-specific prompts for each individual task. To address this, we plan to investigate a parametric network that can guide the learning of task-agnostic prompts, thereby enhancing the model's flexibility and adaptability across multiple tasks.</p>
<h2>References</h2>
<p>Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 268-284. Association for Computational Linguistics.</p>
<p>Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and JianGuang Lou. 2022. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR, abs/2203.03131.</p>
<p>Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,</p>
<p>Jai Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. 2022. Ext5: Towards extreme multitask scaling for transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. 2020. Tinytl: Reduce memory, not parameters for efficient on-device learning. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2924-2936. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics.</p>
<p>Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022. Openprompt: An open-source framework for promptlearning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022 - System Demonstrations, Dublin, Ireland, May 22-27, 2022, pages 105-113. Association for Computational Linguistics.</p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL@ACL 2007 Workshop on Textual Entailment and Paraphrasing, Prague, Czech Republic, June 28-29, 2007, pages 1-9. Association for Computational Linguistics.</p>
<p>Demi Guo, Alexander M. Rush, and Yoon Kim. 2021. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 4884-4896. Association for Computational Linguistics.</p>
<p>Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, and Dongfang Liu. 2023. E'2vpt: An effective and efficient approach for visual prompt tuning. CoRR, abs/2307.13770.</p>
<p>Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2022a. Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 2529, 2022. OpenReview.net.</p>
<p>Yun He, Huaixiu Steven Zheng, Yi Tay, Jai Prakash Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng, and Ed H. Chi. 2022b. Hyperprompt: Prompt-based task-conditioning of transformers. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 8678-8690. PMLR.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long</p>
<p>Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. 2022. Visual prompt tuning. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXIII, volume 13693 of Lecture Notes in Computer Science, pages 709-727. Springer.</p>
<p>Nanjiang Jiang and Marie-Catherine de Marneffe. 2019. Evaluating BERT for natural language inference: A case study on the commitmentbank. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 6085-6090. Association for Computational Linguistics.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 252-262. Association for Computational Linguistics.</p>
<p>Jaejun Lee, Raphael Tang, and Jimmy Lin. 2019. What would elsa do? freezing layers during transformer fine-tuning. CoRR, abs/1911.03090.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 30453059. Association for Computational Linguistics.</p>
<p>Hector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International Conference, KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,</p>
<p>ACL 2020, Online, July 5-10, 2020, pages 7871-7880. Association for Computational Linguistics.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 45824597. Association for Computational Linguistics.</p>
<p>Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 61-68. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang, Wei Wu, Xiaojun Quan, and Dawei Song. 2022. Xprompt: Exploring the extreme of prompt tuning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, page 11033-11047. Association for Computational Linguistics.</p>
<p>Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. 2022. UniPELT: A unified framework for parameter-efficient language model tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6253-6264, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2021. Adapterfusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 487-503. Association for Computational Linguistics.</p>
<p>Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020. Adapterhub: A framework for adapting transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pages 46-54. Association for Computational Linguistics.</p>
<p>Mohammad Taher Pilehvar and José Camacho-Collados. 2019. Wic: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 12671273. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi. 2023a. Progressive prompts: Continual learning for language models. In The Tenth International Conference on Learning Representations, ICLR 2023, May 1-5, 2023. OpenReview.net.</p>
<p>Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, Jimmy Ba, and Amjad Almahairi. 2023b. Residual prompt tuning: Improving prompt tuning with residual reparameterization. In Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011. AAAI.</p>
<p>Timo Schick and Hinrich Schütze. 2021. It's not just size that matters: Small language models are also fewshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2339-2352. Association for Computational Linguistics.</p>
<p>Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 4603-4611. PMLR.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,</p>
<p>and Samuel R. Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3261-3275.</p>
<p>Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, and Qifan Wang. 2023. Prompt learns prompt: Exploring knowledge-aware generative prompt collaboration for video captioning. In Proceedings of the ThirtySecond International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China, pages 1622-1630. ijcai.org.</p>
<p>Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Yu Chen, Madian Khabsa, Sinong Wang, Zenglin Xu, and Dongfang Liu. 2023. Mixpave: Mixprompt tuning for few-shot product attribute value extraction. In Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5754-5764.</p>
<p>Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 33203328 .</p>
<p>Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 2227, 2022, pages 1-9. Association for Computational Linguistics.</p>
<p>Jeffrey O. Zhang, Alexander Sax, Amir Zamir, Leonidas J. Guibas, and Jitendra Malik. 2020. Sidetuning: A baseline for network adaptation via additive side networks. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August</p>
<p>23-28, 2020, Proceedings, Part III, volume 12348 of Lecture Notes in Computer Science, pages 698-714. Springer.</p>
<p>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension. CoRR, abs/1810.12885.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. CoRR, abs/2303.18223.</p>
<h2>A Dataset Statistics</h2>
<p>Table 6 shows details of the eight datasets from SuperGLUE benchmark (Wang et al., 2019) that we used for our experiments, along with their training sizes and evaluation metrics. Following (Raffel et al., 2020) and (Lester et al., 2021), for tasks that have two evaluation metrics we use the average of both scores as the final performance metric.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Examples</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Boolq</td>
<td style="text-align: center;">9,427</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">various</td>
<td style="text-align: center;">F1/Acc</td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">blogs, encyclop</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">MRC</td>
<td style="text-align: center;">5,100</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">various</td>
<td style="text-align: center;">F1/EM</td>
</tr>
<tr>
<td style="text-align: center;">ReC</td>
<td style="text-align: center;">101,000</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">various</td>
<td style="text-align: center;">F1/EM</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">2,500</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">news, Wiki</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">WiC</td>
<td style="text-align: center;">6,000</td>
<td style="text-align: center;">WSD</td>
<td style="text-align: center;">lexical databases</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">WSC</td>
<td style="text-align: center;">259</td>
<td style="text-align: center;">coref.</td>
<td style="text-align: center;">fiction books</td>
<td style="text-align: center;">Acc</td>
</tr>
</tbody>
</table>
<p>Table 6: The details of 8 SuperGLUE tasks used in our experiments. NLI denotes natural language inference, QA denotes questions and answers task, WSD denotes word sense disambiguation, EM denotes exact match scoring, Acc denotes accuracy.</p>
<h2>B Training Details</h2>
<h2>B. 1 Settings</h2>
<p>Our model is implemented with the OpenPrompt library (Ding et al., 2022), which is a unified and extensible toolkit for prompt tuning research. Our model is trained on 16 NVIDIA Tesla V100 GPUs. We translate each SuperGLUE dataset into a text-to-text format following (Raffel et al., 2020). We train our prompts for 100 epochs with a constant learning rate of 0.3 and a batch size of 16 . There are three hyperparameters in our model: the lengthes of input, query-key and key-value prompts. For our method, we set the number of input prompts to 10 across all tasks, and linearly search the best prompt length for both query-key and key-value prompts from ${1,5,10,20,50}$. For all prompt tuning baselines, we search the best input prompt length from ${5,10,20,50,100}$. The best checkpoints are selected via early stopping on the development set. Adafactor (Shazeer and Stern, 2018) optimizer is used in model training with weight decay $1 e^{-5}$.</p>
<h2>B. 2 Tokenization and Preprocessing</h2>
<p>Following common practice (Lester et al., 2021), for all our experiments, we set the maximum input length (including the input prompt) to 512 tokens. We use padding to maximum length and mask out the padded tokens. In case of input exceeding</p>
<p>512 tokens, we truncate the input. We do not perform any specific text preprocessing (e.g. removing punctuation) but instead directly tokenize the raw text from SuperGLUE datasets using the corresponding model tokenizer. For all experiments, we follow T5 (Raffel et al., 2020) formatting. We feed input examples along with their descriptors (e.g. 'sentence1' and 'sentence2'), and cast all classification tasks into text-to-text format (e.g. 0 and 1 classes in Boolq task are cast into 'True' and 'False') replicating guidelines from T5.</p>
<h2>B. 3 Prompt initialization</h2>
<p>In our experiments, we initialize input prompts using randomly sampled vocabulary embeddings similar to (Lester et al., 2021). We sample uniformly across the whole vocabulary, without limiting to top-k most common tokens. The attention prompts are randomly initialized.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Boolq</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">WSC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fine-Tuning (Aribandi et al., 2022)</td>
<td style="text-align: center;">89.60</td>
<td style="text-align: center;">94.20</td>
<td style="text-align: center;">91.70</td>
<td style="text-align: center;">95.20</td>
</tr>
<tr>
<td style="text-align: center;">Partial-1 (Yosinski et al., 2014)</td>
<td style="text-align: center;">83.28</td>
<td style="text-align: center;">84.66</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">87.51</td>
</tr>
<tr>
<td style="text-align: center;">Adapter (Pfeiffer et al., 2020)</td>
<td style="text-align: center;">85.37</td>
<td style="text-align: center;">86.54</td>
<td style="text-align: center;">85.75</td>
<td style="text-align: center;">89.17</td>
</tr>
<tr>
<td style="text-align: center;">BitFit (Zaken et al., 2022)</td>
<td style="text-align: center;">85.91</td>
<td style="text-align: center;">90.53</td>
<td style="text-align: center;">86.34</td>
<td style="text-align: center;">90.48</td>
</tr>
<tr>
<td style="text-align: center;">LoRA (Hu et al., 2022)</td>
<td style="text-align: center;">89.48</td>
<td style="text-align: center;">94.66</td>
<td style="text-align: center;">91.91</td>
<td style="text-align: center;">95.82</td>
</tr>
<tr>
<td style="text-align: center;">AProMPT (Ours)</td>
<td style="text-align: center;">$\mathbf{9 0 . 7 2}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 4 8}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{9 6 . 1 7}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance comparison with other nonprompt tuning based parameter efficient methods on Boolq, CB, RTE and WSC for T5-XL.</p>
<h2>C Comparison with Other Parameter Efficient Methods</h2>
<p>To conduct a full performance evaluation, we further conduct comparision of our approach with other non-prompt tuning based parameter efficient methods, including Parial tuning (Yosinski et al., 2014) (Partial-1 means only fine-tuning the first layer), Adapter (Pfeiffer et al., 2020), BitFit (Zaken et al., 2022) and LoRA (Hu et al., 2022). The performance comparision results are reported in Table 7. It can be seen that AProMPT outperforms all the parameter efficient methods with large margins. In fact, existing prompt tuning approaches also achieve better performances compared to these baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Boolq</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">COPA</th>
<th style="text-align: center;">MRC</th>
<th style="text-align: center;">ReC</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">WiC</th>
<th style="text-align: center;">WSC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prompt-Tuning</td>
<td style="text-align: center;">2h38m</td>
<td style="text-align: center;">8 m</td>
<td style="text-align: center;">11 m</td>
<td style="text-align: center;">46 m</td>
<td style="text-align: center;">18h25m</td>
<td style="text-align: center;">57 m</td>
<td style="text-align: center;">51 m</td>
<td style="text-align: center;">8 m</td>
</tr>
<tr>
<td style="text-align: center;">P-Tuning v2</td>
<td style="text-align: center;">3h36m</td>
<td style="text-align: center;">10 m</td>
<td style="text-align: center;">14 m</td>
<td style="text-align: center;">1h13m</td>
<td style="text-align: center;">20h12m</td>
<td style="text-align: center;">1h28m</td>
<td style="text-align: center;">1h14m</td>
<td style="text-align: center;">12 m</td>
</tr>
<tr>
<td style="text-align: center;">XPrompt</td>
<td style="text-align: center;">4h47m</td>
<td style="text-align: center;">14 m</td>
<td style="text-align: center;">29 m</td>
<td style="text-align: center;">1h53m</td>
<td style="text-align: center;">28h41m</td>
<td style="text-align: center;">2h26m</td>
<td style="text-align: center;">2h19m</td>
<td style="text-align: center;">19 m</td>
</tr>
<tr>
<td style="text-align: center;">ResPrompt</td>
<td style="text-align: center;">3h47m</td>
<td style="text-align: center;">10 m</td>
<td style="text-align: center;">23 m</td>
<td style="text-align: center;">1h21m</td>
<td style="text-align: center;">22h32m</td>
<td style="text-align: center;">1h31m</td>
<td style="text-align: center;">1h34m</td>
<td style="text-align: center;">12 m</td>
</tr>
<tr>
<td style="text-align: center;">AProMPT</td>
<td style="text-align: center;">3h21m</td>
<td style="text-align: center;">9 m</td>
<td style="text-align: center;">15 m</td>
<td style="text-align: center;">1h7m</td>
<td style="text-align: center;">21h</td>
<td style="text-align: center;">1h24m</td>
<td style="text-align: center;">1h16m</td>
<td style="text-align: center;">10 m</td>
</tr>
</tbody>
</table>
<p>Table 8: Training time of AProMPT on SuperGLUE.</p>
<h1>D Training Time</h1>
<p>We further discuss and report the training time of different methods on all the tasks in SuperGLUE in Table 8. For Prompt-Tuning, the trainable parameters consist of the prompts designed for the input layer. In the case of P-Tuning V2, the trainable parameters encompass the prompts associated with all layers. XPrompt focuses on trainable parameters related to the pruned prompts, specifically for the input layer, following the pruning process. As for ResPrompt, the trainable parameters include both the input prompts and the residual network components. The total count of trainable parameters for each approach is detailed in Table 2.</p>            </div>
        </div>

    </div>
</body>
</html>
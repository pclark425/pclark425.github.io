<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9176 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9176</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9176</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-278534727</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.08137v1.pdf" target="_blank">Large Language Models for Computer-Aided Design: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have seen rapid advancements in recent years, with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities across diverse domains. While substantial research has been conducted on LLMs in various fields, a comprehensive review focusing on their integration with Computer-Aided Design (CAD) remains notably absent. CAD is the industry standard for 3D modeling and plays a vital role in the design and development of products across different industries. As the complexity of modern designs increases, the potential for LLMs to enhance and streamline CAD workflows presents an exciting frontier. This article presents the first systematic survey exploring the intersection of LLMs and CAD. We begin by outlining the industrial significance of CAD, highlighting the need for AI-driven innovation. Next, we provide a detailed overview of the foundation of LLMs. We also examine both closed-source LLMs as well as publicly available models. The core of this review focuses on the various applications of LLMs in CAD, providing a taxonomy of six key areas where these models are making considerable impact. Finally, we propose several promising future directions for further advancements, which offer vast opportunities for innovation and are poised to shape the future of CAD technology. Github: https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9176.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9176.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4CAD (GPT-4 / GPT-4V)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal LLMs for 3D CAD program generation (reported as GPT-4 and GPT-4V in LLM4CAD studies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surveyed works used GPT-4 and GPT-4V as text(+-image)-based generators of executable CAD programs; generated code is executed and evaluated by parsing rate and 3D similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal models from OpenAI; GPT-4 is a high-capacity LLM with improved reasoning and multimodal capability (text+image in GPT-4V); exact parameter counts not specified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computer‑Aided Design (CAD) / 3D modeling</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate executable CAD programs from text and other optional inputs (sketches/images) that are parsed into 3D shapes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Parsing rate (whether generated code executes to produce a model); intersection-over-union (IoU) between generated and ground-truth 3D shapes; qualitative execution success and debugging iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported qualitatively: GPT-4V outperformed GPT-4 in parsing rate and IoU; text-only input with GPT-4V achieved the best parsing/IoU compared to multimodal input combinations. No numerical scores were provided in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Multimodality of input (text vs text+image+sketch), model variant (GPT-4V > GPT-4), presence of an iterative debugging/refinement loop, prompt design, and the richness/format of training or fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared primarily against the sibling model GPT-4 (and against other input combinations); GPT-4V reported as superior in parsing rate and IoU. No direct human‑expert baseline numeric comparison reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Generated code can be non-executable or semantically incorrect requiring iterative debugging; multimodal inputs did not always improve parsing/IoU over text-only in reported experiments; spatial reasoning and exact numeric parameter generation remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use iterative execute-and-refine (debugger) loops; prefer multimodal models with strong vision understanding (GPT-4V) and design prompts carefully; visual feedback and execution-aware refinement improve final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Computer-Aided Design: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9176.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9176.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned LLMs (sampling strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning LLMs for CAD code generation with varied sampling strategies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study fine-tuned LLMs for CAD code generation using four different sampling strategies for creating fine-tuning data and reported differences in parsing rate and IoU across strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLMs fine-tuned (baseline GPT-4 for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned LLM variants (not all named) compared against baseline GPT-4 without fine-tuning; fine-tuning done on synthetic/generated CAD code datasets with different sampling strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>CAD code generation / program synthesis for CAD</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate executable CAD code from prompts; evaluate parsing success and geometric similarity (IoU).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Parsing rate (execution success) and intersection-over-union (IoU) for geometry quality.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Qualitative outcomes: All four fine-tuned LLMs outperformed baseline GPT-4 in most cases; the second sampling strategy (ensured diversity in code length) achieved highest average parsing rate, while the third (focused on shortest lengths) achieved best IoU. No numeric values reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Fine-tuning data sampling strategy (diversity vs length-focused), data template selection, code length distributions, and representativeness of fine-tuning examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to baseline GPT-4 (no fine-tuning); fine-tuned models typically outperformed GPT-4 on parsing rate and IoU.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance depends strongly on how fine-tuning data are sampled; gains are dataset- and distribution-dependent; no absolute numeric benchmarks presented in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Design fine-tuning curricula carefully (diversity in code templates and lengths helps parsing success); tailor sampling to target evaluation metric (e.g., IoU vs parsing rate).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Computer-Aided Design: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9176.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9176.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CADCodeVerify benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CADCodeVerify: iterative verifier-refiner framework and benchmark for CAD code generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses a VLM to evaluate generated CAD objects via Q&A and produces automated feedback to iteratively refine LLM-generated CAD code; includes a 200-prompt benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating CAD Code with Vision-Language Models for 3D Designs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs evaluated (GPT-4, Gemini, Code LLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmarked LLMs include closed-source GPT-4, Google Gemini, and open/source Code LLaMA variants; each used to generate CAD scripts which are assessed by a VLM-based verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>CAD code generation / automated verification in design</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate design generation by producing CAD scripting code and then automatically verify/refine generated geometry using a VLM-driven QA-based verifier loop.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Execution success (executable code), QA-based discrepancy detection by the VLM, and benchmark-specific correctness against expert-annotated scripts in the 200-prompt dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Survey reports that the framework can iteratively improve generated objects via automated feedback loops; exact numeric model comparisons on the 200-prompt benchmark are not provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Quality of the VLM verifier, fidelity of question-answer pairs, initial LLM code quality, and the ability of the LLM to incorporate verifier feedback (i.e., iterative refinement loop effectiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared LLMs GPT-4, Gemini, and Code LLaMA within the CADCodeVerify framework; relative numeric performance not listed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Verifier depends on VLM visual understanding and question set design; some errors may not be detectable via the chosen QA prompts; complete automation without human oversight remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use VLM-based automated verification as a scalable feedback mechanism; design diverse QA probe sets; use iterative loops to correct syntactic and semantic code errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Computer-Aided Design: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9176.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9176.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Query2CAD (GPT-3.5 / GPT-4 Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Query2CAD: natural language to executable CAD macro generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Framework that feeds user queries into robust LLMs (e.g., GPT-3.5 Turbo, GPT-4 Turbo) to generate Python macros for CAD; uses error messages and LLM-driven corrections to reach executable code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Query2CAD: Generating CAD models using natural language queries.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo, GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat-family models optimized for interactive generation; used to produce Python macros that invoke CAD APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>CAD automation / program synthesis for CAD software (macros)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate an expert CAD programmer by generating executable Python macros from natural language task descriptions, with self-refinement using error messages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Execution success (whether generated macro runs in target CAD environment) and improvement via self-refinement loops; also used BLIP2 for image captioning to improve code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Survey states that the iterative approach corrected errors via LLM feedback and produced executable macros in practice; no numeric success rates reported in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Quality of the error messages, prompt context and included API descriptions, assistance from vision captioning (BLIP2) for image-guided prompts, and iterative refinement strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparison framed against non-iterative single-generation runs (implicit); no formal numeric baseline provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dependence on error-message clarity and completeness; generated macros may require many iteration cycles; potential for hallucinated API usage if prompt lacks full API spec.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Provide rich execution context and error traces to the LLM; integrate vision-captioning when visual examples are available; use iterative generate-execute-debug loops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Computer-Aided Design: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9176.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9176.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BlenderLLM / BlendNet pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BlenderLLM with BlendNet dataset: script generation and validation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combined dataset synthesis and iterative SFT approach where GPT-4o was used to generate/validate scripts and Qwen2.5-Coder-7B-Instruct was fine-tuned iteratively (BlenderLLM) to produce CAD scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (validator) and Qwen2.5-Coder-7B-Instruct (generation model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4o used as a high-quality multimodal validator; Qwen2.5-Coder-7B-Instruct is a fine-tuned code-capable open model used as the generator and iteratively improved.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>CAD code/script generation and dataset curation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate human instruction-following and script execution by generating CAD scripts from instruction templates and validating alignment between instruction and rendered image.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Dataset-level verification: human-verified vs model-validated samples; final dataset sizes reported (e.g., 2,000 human-verified and 6,000 model-validated samples in a related BlendNet pipeline). Script quality assessed by validator scores and human checks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported process produced large quantities of model-validated script-image pairs and allowed iterative improvement of the generation model; no single numeric accuracy for simulation reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Quality of seed instructions, effectiveness of self-instruct data distillation, validator model capability (GPT-4o), and filtering thresholds used to accept generated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicit comparison to single-pass generation without iterative dataset-filtering; no explicit numeric baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Risk of validator/model co-adaptation (model-validations may reinforce model biases); final quality depends on human verification budget and validator reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use strong multimodal validators and human-in-the-loop verification to bootstrap high-quality datasets; iterate generation+filtering cycles to improve generator performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Computer-Aided Design: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9176.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9176.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parametric CAD generation (ChatGPT / LLaMA-family)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs for parametric CAD sequence generation and autocompletion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Works used instruction-tuned LLMs (e.g., ChatGPT, LLaMA variants) to generate parametric CAD sequences (JSON or key-value param sequences) from text and multimodal inputs for controllable CAD generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT, LLaMA-family (Vicuna, LLaMA-3), LLaVA variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat models (ChatGPT) and open LLaMA-based chat models (Vicuna, LLaMA-3) often fine-tuned with LoRA; applied in multimodal pipelines combined with frozen image encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Parametric CAD / design automation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate parametric construction sequences (JSON) for CAD models from textual descriptions and/or images, including autocompletion and autoconstraint tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Parsing/serialization correctness, visual fidelity after rendering from generated parametric sequences, and qualitative user-controlled generation fidelity; some works used VLM scoring for preference data.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported qualitatively: fine-tuned ChatGPT outperformed baselines on CAD autocompletion subsets; LLaMA-3-based CAD-Fusion used visual feedback to improve visually preferred outputs. No numerical accuracy values provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model fine-tuning, use of LoRA, multimodal conditioning (rendered views or point clouds), tokenization mapping of 3D to 1D, hierarchy-aware masking during training, and the inclusion of visual feedback/rewarding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to other baselines (unspecified in survey summary) and to non-fine-tuned models; ChatGPT and LLaMA-3 variants reported improvements after fine-tuning and with visual feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Difficulty producing fully accurate continuous numeric parameters; generated sequences may be syntactically correct but semantically mis-specified; limited spatial reasoning affects complex 3D relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Incorporate visual feedback and hierarchy-aware training objectives; represent 3D structures in tokenizable 1D sequences to aid the LLM; fine-tune with in-domain parametric examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Computer-Aided Design: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9176.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9176.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs as evaluators / simulators of human judgment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of LLMs/VLMs (e.g., GPT-4o, GPT-4) to score and evaluate CAD outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surveyed works leveraged LLMs or VLMs to score image-rendered CAD outputs for fidelity, attribute correctness, or dimensions, effectively using LLMs as automated evaluators/simulators of human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-4, LLaVA-OV-Qwen2-7B, Clip-FlanT5-XL (for VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capability multimodal LLMs (GPT-4 family) and specialized VLMs used to compute scores for shape quality, attributes, and script-object alignment; some works used Clip-FlanT5-XL for visual question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Model evaluation / automated QA for CAD outputs</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate human evaluators by answering VQA prompts about rendered models or scoring along defined criteria (legibility, completeness, tolerance, shape quality/quantity/distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>VLM/LLM-generated scores (ordinal or scalar) for evaluation criteria, automated VQA agreement with ground-truth or human labels, and use of these scores as reward signals for training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Survey reports successful use of VLMs/LLMs to build preference datasets and automatic scoring pipelines (e.g., CADBench used GPT-4o); specific numeric agreement rates with humans are not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt specification of evaluation criteria, strength of the VLM's visual understanding, multimodal alignment quality, and quality/diversity of the rendered views provided as input.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Often used to replace or augment costly human annotation; compared implicitly to manual human scoring but no formal numeric comparison is supplied in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>VLMs have limited spatial reasoning for complex 3D judgments; scoring can be sensitive to prompt phrasing; risk of over-reliance on model judgments without human calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use carefully crafted evaluation prompts and consider combining human verification with LLM-based scoring; use visual-understanding-capable models for reward/feedback generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Computer-Aided Design: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM4CAD: Multimodal Large Language Models for Three-Dimensional Computer-Aided Design Generation <em>(Rating: 2)</em></li>
                <li>Query2CAD: Generating CAD models using natural language queries. <em>(Rating: 2)</em></li>
                <li>Generating CAD Code with Vision-Language Models for 3D Designs <em>(Rating: 2)</em></li>
                <li>CAD-Recode: Reverse Engineering CAD Code from Point Clouds <em>(Rating: 2)</em></li>
                <li>BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement <em>(Rating: 2)</em></li>
                <li>CADCodeVerify: (framework described in survey) — benchmark and iterative verification <em>(Rating: 1)</em></li>
                <li>CAD-Fusion: Text-to-CAD Generation Through Infusing Visual Feedback in Large Language Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9176",
    "paper_id": "paper-278534727",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "LLM4CAD (GPT-4 / GPT-4V)",
            "name_full": "Multimodal LLMs for 3D CAD program generation (reported as GPT-4 and GPT-4V in LLM4CAD studies)",
            "brief_description": "Surveyed works used GPT-4 and GPT-4V as text(+-image)-based generators of executable CAD programs; generated code is executed and evaluated by parsing rate and 3D similarity metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 and GPT-4V",
            "model_description": "Closed-source multimodal models from OpenAI; GPT-4 is a high-capacity LLM with improved reasoning and multimodal capability (text+image in GPT-4V); exact parameter counts not specified in the survey.",
            "scientific_subdomain": "Computer‑Aided Design (CAD) / 3D modeling",
            "simulation_task": "Generate executable CAD programs from text and other optional inputs (sketches/images) that are parsed into 3D shapes.",
            "evaluation_metric": "Parsing rate (whether generated code executes to produce a model); intersection-over-union (IoU) between generated and ground-truth 3D shapes; qualitative execution success and debugging iterations.",
            "simulation_accuracy": "Reported qualitatively: GPT-4V outperformed GPT-4 in parsing rate and IoU; text-only input with GPT-4V achieved the best parsing/IoU compared to multimodal input combinations. No numerical scores were provided in the survey summary.",
            "factors_affecting_accuracy": "Multimodality of input (text vs text+image+sketch), model variant (GPT-4V &gt; GPT-4), presence of an iterative debugging/refinement loop, prompt design, and the richness/format of training or fine-tuning data.",
            "comparison_baseline": "Compared primarily against the sibling model GPT-4 (and against other input combinations); GPT-4V reported as superior in parsing rate and IoU. No direct human‑expert baseline numeric comparison reported in the survey.",
            "limitations_or_failure_cases": "Generated code can be non-executable or semantically incorrect requiring iterative debugging; multimodal inputs did not always improve parsing/IoU over text-only in reported experiments; spatial reasoning and exact numeric parameter generation remain challenging.",
            "author_recommendations_or_insights": "Use iterative execute-and-refine (debugger) loops; prefer multimodal models with strong vision understanding (GPT-4V) and design prompts carefully; visual feedback and execution-aware refinement improve final outputs.",
            "uuid": "e9176.0",
            "source_info": {
                "paper_title": "Large Language Models for Computer-Aided Design: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Fine-tuned LLMs (sampling strategies)",
            "name_full": "Fine-tuning LLMs for CAD code generation with varied sampling strategies",
            "brief_description": "A study fine-tuned LLMs for CAD code generation using four different sampling strategies for creating fine-tuning data and reported differences in parsing rate and IoU across strategies.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Unspecified LLMs fine-tuned (baseline GPT-4 for comparison)",
            "model_description": "Fine-tuned LLM variants (not all named) compared against baseline GPT-4 without fine-tuning; fine-tuning done on synthetic/generated CAD code datasets with different sampling strategies.",
            "scientific_subdomain": "CAD code generation / program synthesis for CAD",
            "simulation_task": "Generate executable CAD code from prompts; evaluate parsing success and geometric similarity (IoU).",
            "evaluation_metric": "Parsing rate (execution success) and intersection-over-union (IoU) for geometry quality.",
            "simulation_accuracy": "Qualitative outcomes: All four fine-tuned LLMs outperformed baseline GPT-4 in most cases; the second sampling strategy (ensured diversity in code length) achieved highest average parsing rate, while the third (focused on shortest lengths) achieved best IoU. No numeric values reported in survey.",
            "factors_affecting_accuracy": "Fine-tuning data sampling strategy (diversity vs length-focused), data template selection, code length distributions, and representativeness of fine-tuning examples.",
            "comparison_baseline": "Compared to baseline GPT-4 (no fine-tuning); fine-tuned models typically outperformed GPT-4 on parsing rate and IoU.",
            "limitations_or_failure_cases": "Performance depends strongly on how fine-tuning data are sampled; gains are dataset- and distribution-dependent; no absolute numeric benchmarks presented in the survey summary.",
            "author_recommendations_or_insights": "Design fine-tuning curricula carefully (diversity in code templates and lengths helps parsing success); tailor sampling to target evaluation metric (e.g., IoU vs parsing rate).",
            "uuid": "e9176.1",
            "source_info": {
                "paper_title": "Large Language Models for Computer-Aided Design: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CADCodeVerify benchmark",
            "name_full": "CADCodeVerify: iterative verifier-refiner framework and benchmark for CAD code generation",
            "brief_description": "A framework that uses a VLM to evaluate generated CAD objects via Q&A and produces automated feedback to iteratively refine LLM-generated CAD code; includes a 200-prompt benchmark.",
            "citation_title": "Generating CAD Code with Vision-Language Models for 3D Designs",
            "mention_or_use": "use",
            "model_name": "Various LLMs evaluated (GPT-4, Gemini, Code LLaMA)",
            "model_description": "Benchmarked LLMs include closed-source GPT-4, Google Gemini, and open/source Code LLaMA variants; each used to generate CAD scripts which are assessed by a VLM-based verifier.",
            "scientific_subdomain": "CAD code generation / automated verification in design",
            "simulation_task": "Simulate design generation by producing CAD scripting code and then automatically verify/refine generated geometry using a VLM-driven QA-based verifier loop.",
            "evaluation_metric": "Execution success (executable code), QA-based discrepancy detection by the VLM, and benchmark-specific correctness against expert-annotated scripts in the 200-prompt dataset.",
            "simulation_accuracy": "Survey reports that the framework can iteratively improve generated objects via automated feedback loops; exact numeric model comparisons on the 200-prompt benchmark are not provided in the survey text.",
            "factors_affecting_accuracy": "Quality of the VLM verifier, fidelity of question-answer pairs, initial LLM code quality, and the ability of the LLM to incorporate verifier feedback (i.e., iterative refinement loop effectiveness).",
            "comparison_baseline": "Compared LLMs GPT-4, Gemini, and Code LLaMA within the CADCodeVerify framework; relative numeric performance not listed in the survey.",
            "limitations_or_failure_cases": "Verifier depends on VLM visual understanding and question set design; some errors may not be detectable via the chosen QA prompts; complete automation without human oversight remains challenging.",
            "author_recommendations_or_insights": "Use VLM-based automated verification as a scalable feedback mechanism; design diverse QA probe sets; use iterative loops to correct syntactic and semantic code errors.",
            "uuid": "e9176.2",
            "source_info": {
                "paper_title": "Large Language Models for Computer-Aided Design: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Query2CAD (GPT-3.5 / GPT-4 Turbo)",
            "name_full": "Query2CAD: natural language to executable CAD macro generation",
            "brief_description": "Framework that feeds user queries into robust LLMs (e.g., GPT-3.5 Turbo, GPT-4 Turbo) to generate Python macros for CAD; uses error messages and LLM-driven corrections to reach executable code.",
            "citation_title": "Query2CAD: Generating CAD models using natural language queries.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo, GPT-4 Turbo",
            "model_description": "OpenAI chat-family models optimized for interactive generation; used to produce Python macros that invoke CAD APIs.",
            "scientific_subdomain": "CAD automation / program synthesis for CAD software (macros)",
            "simulation_task": "Simulate an expert CAD programmer by generating executable Python macros from natural language task descriptions, with self-refinement using error messages.",
            "evaluation_metric": "Execution success (whether generated macro runs in target CAD environment) and improvement via self-refinement loops; also used BLIP2 for image captioning to improve code generation.",
            "simulation_accuracy": "Survey states that the iterative approach corrected errors via LLM feedback and produced executable macros in practice; no numeric success rates reported in the survey summary.",
            "factors_affecting_accuracy": "Quality of the error messages, prompt context and included API descriptions, assistance from vision captioning (BLIP2) for image-guided prompts, and iterative refinement strategy.",
            "comparison_baseline": "Comparison framed against non-iterative single-generation runs (implicit); no formal numeric baseline provided in the survey.",
            "limitations_or_failure_cases": "Dependence on error-message clarity and completeness; generated macros may require many iteration cycles; potential for hallucinated API usage if prompt lacks full API spec.",
            "author_recommendations_or_insights": "Provide rich execution context and error traces to the LLM; integrate vision-captioning when visual examples are available; use iterative generate-execute-debug loops.",
            "uuid": "e9176.3",
            "source_info": {
                "paper_title": "Large Language Models for Computer-Aided Design: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "BlenderLLM / BlendNet pipeline",
            "name_full": "BlenderLLM with BlendNet dataset: script generation and validation pipeline",
            "brief_description": "Combined dataset synthesis and iterative SFT approach where GPT-4o was used to generate/validate scripts and Qwen2.5-Coder-7B-Instruct was fine-tuned iteratively (BlenderLLM) to produce CAD scripts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (validator) and Qwen2.5-Coder-7B-Instruct (generation model)",
            "model_description": "GPT-4o used as a high-quality multimodal validator; Qwen2.5-Coder-7B-Instruct is a fine-tuned code-capable open model used as the generator and iteratively improved.",
            "scientific_subdomain": "CAD code/script generation and dataset curation",
            "simulation_task": "Simulate human instruction-following and script execution by generating CAD scripts from instruction templates and validating alignment between instruction and rendered image.",
            "evaluation_metric": "Dataset-level verification: human-verified vs model-validated samples; final dataset sizes reported (e.g., 2,000 human-verified and 6,000 model-validated samples in a related BlendNet pipeline). Script quality assessed by validator scores and human checks.",
            "simulation_accuracy": "Reported process produced large quantities of model-validated script-image pairs and allowed iterative improvement of the generation model; no single numeric accuracy for simulation reported in the survey.",
            "factors_affecting_accuracy": "Quality of seed instructions, effectiveness of self-instruct data distillation, validator model capability (GPT-4o), and filtering thresholds used to accept generated samples.",
            "comparison_baseline": "Implicit comparison to single-pass generation without iterative dataset-filtering; no explicit numeric baseline reported.",
            "limitations_or_failure_cases": "Risk of validator/model co-adaptation (model-validations may reinforce model biases); final quality depends on human verification budget and validator reliability.",
            "author_recommendations_or_insights": "Use strong multimodal validators and human-in-the-loop verification to bootstrap high-quality datasets; iterate generation+filtering cycles to improve generator performance.",
            "uuid": "e9176.4",
            "source_info": {
                "paper_title": "Large Language Models for Computer-Aided Design: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Parametric CAD generation (ChatGPT / LLaMA-family)",
            "name_full": "LLMs for parametric CAD sequence generation and autocompletion",
            "brief_description": "Works used instruction-tuned LLMs (e.g., ChatGPT, LLaMA variants) to generate parametric CAD sequences (JSON or key-value param sequences) from text and multimodal inputs for controllable CAD generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT, LLaMA-family (Vicuna, LLaMA-3), LLaVA variants",
            "model_description": "Instruction-tuned chat models (ChatGPT) and open LLaMA-based chat models (Vicuna, LLaMA-3) often fine-tuned with LoRA; applied in multimodal pipelines combined with frozen image encoders.",
            "scientific_subdomain": "Parametric CAD / design automation",
            "simulation_task": "Generate parametric construction sequences (JSON) for CAD models from textual descriptions and/or images, including autocompletion and autoconstraint tasks.",
            "evaluation_metric": "Parsing/serialization correctness, visual fidelity after rendering from generated parametric sequences, and qualitative user-controlled generation fidelity; some works used VLM scoring for preference data.",
            "simulation_accuracy": "Reported qualitatively: fine-tuned ChatGPT outperformed baselines on CAD autocompletion subsets; LLaMA-3-based CAD-Fusion used visual feedback to improve visually preferred outputs. No numerical accuracy values provided in the survey.",
            "factors_affecting_accuracy": "Model fine-tuning, use of LoRA, multimodal conditioning (rendered views or point clouds), tokenization mapping of 3D to 1D, hierarchy-aware masking during training, and the inclusion of visual feedback/rewarding.",
            "comparison_baseline": "Compared to other baselines (unspecified in survey summary) and to non-fine-tuned models; ChatGPT and LLaMA-3 variants reported improvements after fine-tuning and with visual feedback.",
            "limitations_or_failure_cases": "Difficulty producing fully accurate continuous numeric parameters; generated sequences may be syntactically correct but semantically mis-specified; limited spatial reasoning affects complex 3D relationships.",
            "author_recommendations_or_insights": "Incorporate visual feedback and hierarchy-aware training objectives; represent 3D structures in tokenizable 1D sequences to aid the LLM; fine-tune with in-domain parametric examples.",
            "uuid": "e9176.5",
            "source_info": {
                "paper_title": "Large Language Models for Computer-Aided Design: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLMs as evaluators / simulators of human judgment",
            "name_full": "Use of LLMs/VLMs (e.g., GPT-4o, GPT-4) to score and evaluate CAD outputs",
            "brief_description": "Surveyed works leveraged LLMs or VLMs to score image-rendered CAD outputs for fidelity, attribute correctness, or dimensions, effectively using LLMs as automated evaluators/simulators of human judgments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o, GPT-4, LLaVA-OV-Qwen2-7B, Clip-FlanT5-XL (for VQA)",
            "model_description": "High-capability multimodal LLMs (GPT-4 family) and specialized VLMs used to compute scores for shape quality, attributes, and script-object alignment; some works used Clip-FlanT5-XL for visual question answering.",
            "scientific_subdomain": "Model evaluation / automated QA for CAD outputs",
            "simulation_task": "Simulate human evaluators by answering VQA prompts about rendered models or scoring along defined criteria (legibility, completeness, tolerance, shape quality/quantity/distribution).",
            "evaluation_metric": "VLM/LLM-generated scores (ordinal or scalar) for evaluation criteria, automated VQA agreement with ground-truth or human labels, and use of these scores as reward signals for training.",
            "simulation_accuracy": "Survey reports successful use of VLMs/LLMs to build preference datasets and automatic scoring pipelines (e.g., CADBench used GPT-4o); specific numeric agreement rates with humans are not provided in the survey.",
            "factors_affecting_accuracy": "Prompt specification of evaluation criteria, strength of the VLM's visual understanding, multimodal alignment quality, and quality/diversity of the rendered views provided as input.",
            "comparison_baseline": "Often used to replace or augment costly human annotation; compared implicitly to manual human scoring but no formal numeric comparison is supplied in the survey summary.",
            "limitations_or_failure_cases": "VLMs have limited spatial reasoning for complex 3D judgments; scoring can be sensitive to prompt phrasing; risk of over-reliance on model judgments without human calibration.",
            "author_recommendations_or_insights": "Use carefully crafted evaluation prompts and consider combining human verification with LLM-based scoring; use visual-understanding-capable models for reward/feedback generation.",
            "uuid": "e9176.6",
            "source_info": {
                "paper_title": "Large Language Models for Computer-Aided Design: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM4CAD: Multimodal Large Language Models for Three-Dimensional Computer-Aided Design Generation",
            "rating": 2,
            "sanitized_title": "llm4cad_multimodal_large_language_models_for_threedimensional_computeraided_design_generation"
        },
        {
            "paper_title": "Query2CAD: Generating CAD models using natural language queries.",
            "rating": 2,
            "sanitized_title": "query2cad_generating_cad_models_using_natural_language_queries"
        },
        {
            "paper_title": "Generating CAD Code with Vision-Language Models for 3D Designs",
            "rating": 2,
            "sanitized_title": "generating_cad_code_with_visionlanguage_models_for_3d_designs"
        },
        {
            "paper_title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
            "rating": 2,
            "sanitized_title": "cadrecode_reverse_engineering_cad_code_from_point_clouds"
        },
        {
            "paper_title": "BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement",
            "rating": 2,
            "sanitized_title": "blenderllm_training_large_language_models_for_computeraided_design_with_selfimprovement"
        },
        {
            "paper_title": "CADCodeVerify: (framework described in survey) — benchmark and iterative verification",
            "rating": 1,
            "sanitized_title": "cadcodeverify_framework_described_in_survey_benchmark_and_iterative_verification"
        },
        {
            "paper_title": "CAD-Fusion: Text-to-CAD Generation Through Infusing Visual Feedback in Large Language Models",
            "rating": 2,
            "sanitized_title": "cadfusion_texttocad_generation_through_infusing_visual_feedback_in_large_language_models"
        }
    ],
    "cost": 0.0207,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Computer-Aided Design: A Survey
13 May 2025</p>
<p>Licheng Zhang licheng.zhang@student.unimelb.edu.au 
Bach Le bach.le@unimelb.edu.au 
Naveed Akhtar naveed.akhtar1@unimelb.edu.au 
Tuan Ngo </p>
<p>The University of Melbourne
Australia</p>
<p>The University of Melbourne
Australia</p>
<p>The University of Melbourne
Australia</p>
<p>SIEW-KEI LAM
Nanyang Technological University
Singapore</p>
<p>The University of Melbourne
Australia</p>
<p>The University of Melbourne
ParkvilleVictoriaAustralia</p>
<p>The University of Melbourne
Australia</p>
<p>The University of Melbourne
Naveed AkhtarAustralia</p>
<p>Siew-Kei Lam
Nanyang Technological University
Singapore</p>
<p>The University of Melbourne
Australia</p>
<p>Large Language Models for Computer-Aided Design: A Survey
13 May 2025BEDB4B745D9BCBCF60AB6DF879A5E8DEarXiv:2505.08137v1[cs.LG]Manuscript submitted to ACM Manuscript submitted to ACMComputer-Aided DesignLarge Language ModelsVision-Language ModelsCAD Code GenerationParametric CAD Generation
Large Language Models (LLMs) have seen rapid advancements in recent years, with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities across diverse domains.While substantial research has been conducted on LLMs in various fields, a comprehensive review focusing on their integration with Computer-Aided Design (CAD) remains notably absent.CAD is the industry standard for 3D modeling and plays a vital role in the design and development of products across different industries.As the complexity of modern designs increases, the potential for LLMs to enhance and streamline CAD workflows presents an exciting frontier.This article presents the first systematic survey exploring the intersection of LLMs and CAD.We begin by outlining the industrial significance of CAD, highlighting the need for AI-driven innovation.Next, we provide a detailed overview of the foundation of LLMs.We also examine both closed-source LLMs as well as publicly available models.The core of this review focuses on the various applications of LLMs in CAD, providing a taxonomy of six key areas where these models are making considerable impact.Finally, we propose several promising future directions for further advancements, which offer vast opportunities for innovation and are poised to shape the future of CAD technology.Github: https://github.com/lichengzhanguom/LLMs-CAD-Survey-TaxonomyCCS Concepts: • Applied computing → Computer-aided design; • Information systems → Language models; • Computing methodologies → Natural language processing; Computer vision tasks.</p>
<p>Introduction</p>
<p>Language is a fundamental aspect of human intelligence, as essential as vision.For decades, researchers have strived to endow machines with the ability to reason and communicate in natural language [212].In recent years, this vision has come closer to reality with the advent of Large Language Models (LLMs), which have demonstrated remarkable Manuscript submitted to ACM progress across a wide range of tasks [154].A key insight driving LLMs is that knowledge about the world can be effectively captured and represented through large-scale language modeling.This enables the construction of generalpurpose models that can tackle a variety of problems via pre-training and task-specific fine-tuning, alignment, or prompting-bypassing the need for extensive domain-specific training from scratch [193].</p>
<p>LLMs are typically built upon the Transformer architecture [174], with a vastly larger number of parameters compared to earlier Pre-trained Language Models (PLMs).This increase in scale has resulted in significantly improved capabilities in language understanding and generation.The release of ChatGPT [147] marked a major milestone, showcasing unprecedented performance.Since then, many powerful LLMs have been introduced, including GPT-4 [2] and GPT-4V [125] by OpenAI; LLaMA [169], LLaMA-2 [170], and LLaMA-3 [49] by Meta; and Gemini [163], PaLM [31], PaLM-2 [7], and GLaM [42] by Google, among others.Parallel to these advancements in LLMs, Computer-Aided Design (CAD) has remained a cornerstone technology in engineering and industrial design.CAD refers to the use of computers to assist in the creation, modification, analysis, or optimization of a design [50,113,136,146].It is widely used to produce 2D drawings and 3D models of physical products, supporting applications in fields such as architecture, automotive design, manufacturing, and 3D printing.CAD enhances design precision, facilitates rapid iteration, and reduces both development time and cost.Moreover, CAD systems preserve essential design information-such as geometry, dimensions, and structural details-in standardized file formats, promoting reusability and collaboration.</p>
<p>Despite the growing popularity of LLMs and the critical importance of CAD in modern industry, no survey article to date has systematically explored the intersection of these two domains.While reviewing efforts have been made for various LLM applications [8,53,54,57,82,115,120,154,202,212], a comprehensive survey of exploiting LLMs for CAD still remains missing.</p>
<p>LLMs have already demonstrated transformative potential in various generative and analytical tasks, including image synthesis [19,80], text generation [87,192], and code completion [29,121].This makes them a promising candidate for advancing CAD through automation, intelligent design assistance, and semantic understanding of design specifications.</p>
<p>Hence, a dedicated review of this emerging research area is both timely and essential.It has the potential to offer invaluable insights for researchers and practitioners seeking to integrate LLMs into CAD workflows.This article presents the first comprehensive survey of research at the intersection of LLMs and CAD.Our objective is to consolidate recent developments, identify trends, and highlight opportunities for future research in this direction.</p>
<p>In addition to surveying LLM applications in CAD, we also extend the scope of prior LLM surveys by incorporating the latest generation of models.Specifically, our survey makes the following key contributions.</p>
<ol>
<li>Overview of CAD and its industrial significance: We provide a context on the role of CAD across various industries to onboard researchers and practitioners in the LLM domain to CAD's industrial significance.</li>
</ol>
<p>Foundations of LLMs:</p>
<p>We review the related foundational concepts of LLMs in an accessible manner to bring CAD experts up to speed with the fundamental knowledge necessary to appreciate the contemporary developments related to LLMs.</p>
<p>Review of state-of-the-art LLMs:</p>
<p>We provide a summary of the key recent developments in LLMs that are relevant to propel CAD developments with LLM augmentations.</p>
<p>Applications of LLMs in CAD:</p>
<p>We systematically categorize and analyze the existing literature on LLMs in CAD-related tasks. 5. Discussion and future directions: We provide a detailed discussion on the limitations, challenges, and potential future pathways for integrating LLMs with CAD.</p>
<p>Manuscript submitted to ACM</p>
<p>CAD in Industry</p>
<p>Computer-Aided Design involves the use of computers or workstations to assist in the creation and modification of designs, replacing traditional pencil drawings with precise digital sketches.CAD enhances designers' productivity, improves design quality, and streamlines communication through clear documentation.Notably, CAD tools can reduce design and prototyping time by 30% to 50%, particularly in iterative design processes [22].</p>
<p>CAD applications span a wide range of sectors, including automotive, aerospace and defense, architecture, manufacturing, electronics, healthcare, consumer goods, energy and utilities, fashion and textiles, entertainment and media, marine and shipbuilding, agriculture and heavy equipment, education and research, jewelry and art 1 .</p>
<p>As noted in [3], CAD is especially important in engineering and manufacturing, buildings and construction, medicine and biotechnology, industrial design and consumer products, and media and entertainment.Similarly, [63] identifies its applications in architecture, electronics, shipbuilding, aerospace, textile industry, and education.From these sources, we can observe that CAD is extensively exploited across a variety of practical design related domains.</p>
<p>Taking architectural design as a case in point, CAD is used to model buildings and infrastructure with high precision.</p>
<p>Architects and designers can generate detailed 3D models, explore alternative design options, develop construction plans, and collaborate effectively with structural engineers and other project stakeholders [3].CAD also enables virtual beautification of building designs, helping to reflect realistic expectations visually [63].</p>
<p>The global 3D CAD software market reflects this widespread adoption.This market is projected to grow from USD 13.40 billion in 2025 to USD 24.23 billion by 2034 2 .This growth is largely driven by technological advancements.In 2023, North America led the global CAD software market, while the Asia-Pacific region was forecasted to experience significant growth through 2034.It is noteworthy that the on-premises deployment model accounted for the largest market share in 2023.The large enterprise segment is anticipated to expand significantly between 2025 and 2034.</p>
<p>Moreover, the AEC (Architecture, Engineering, and Construction) segment dominated the CAD software market in 2023.These facts present a clear picture of industrial significance of CAD based design.</p>
<p>Foundations of LLMs</p>
<p>In this section, we discuss foundational concepts related to LLMs in an accessible manner.Our aim is to introduce the relevant terms and concepts to non-LLM experts to bridge the gap between LLM and CAD research communities.This section is not aimed at discussing the details of these concepts.Readers interested in technical details are referred to [120,212].The context window, also known as context length, refers to the maximum number of tokens an LLM can see or process at once when generating or understanding text.</p>
<p>Background for LLMs</p>
<p>Decoder-only Transformer.</p>
<p>The decoder-only Transformer architecture [174] is one of the most widely used structures for building LLMs.Its core structure consists of a stack of Transformer blocks [174], where each block includes two sub-layers: one for self-attention modeling and another for feed-forward network modeling.A softmax layer is placed on top of the final Transformer block to generate a probability distribution over the vocabulary.Self-attention, feed-forward networks (FFNs) and softmax layer are all standard components of contemporary neural networks.</p>
<p>Although implementation details may vary, many LLMs share this general architecture.These models are referred to as "large" due to their significant width and depth [193].</p>
<p>Training LLMs.</p>
<p>Training LLMs involves the standard neural network optimization process, typically using gradient descent algorithms [81].Studies have shown that model performance improves as they are trained on larger datasets, and when they are scaled up in terms of architecture and computational capacity [70].This insight has led to continued efforts to increase both the size of training data and model complexity, resulting in increasingly powerful contemporary LLMs.</p>
<p>3.1.4Fine-tuning LLMs.LLMs are first pre-trained on a large corpus of textual data.This pre-training achieves generalpurpose language understanding and generation capabilities.Commonly, this is followed by fine-tuning these models to solve specific natural language processing (NLP) tasks.This fine-tuning involves further training of the model on a limited data, which is available for the downstream task.However, fine-tuning only changes the model parameters slightly, thereby making the process of adaptation computationally less expensive.Since the fine-tuning process may also use instruction-following data, such fine-tuning is also known as instruction fine-tuning.</p>
<p>To enable instruction-following, datasets containing various instructions and corresponding responses are required.</p>
<p>Scaling the number of such tasks for fine-tuning generally improves model performance [32].Unlike pre-training, which may require billions or trillions of samples, fine-tuning can be performed with tens or hundreds of thousands of high-quality samples [25].Fine-tuning plays a central role in enabling and enhancing this versatility, and ongoing research continues to improve fine-tuning techniques to make LLMs more efficient and effective.</p>
<p>3.1.5Aligning LLMs.Alignment refers to the process of guiding LLMs to behave in accordance with human intentions and ethical standards.This often involves incorporating human feedback, labeled data, or explicitly defined preferences into the training process.Alignment is essential to ensure responsible and safe artificial intelligence (AI) behavior.</p>
<p>Typically, alignment follows two main steps after initial pre-training:</p>
<ol>
<li>Supervised Fine-tuning (SFT): This step usually uses instruction-based data to further refine the model.</li>
</ol>
<p>Reinforcement</p>
<p>Learning from Human Feedback (RLHF) [128]: In this phase, alignment is treated as a reinforcement learning (RL) problem [151].A reward model-representing the environment-evaluates outputs based on human feedback, while the LLM acts as the agent being optimized to maximize these rewards.</p>
<p>These techniques are critical to adapting LLMs for real-world applications, especially where safety, ethical behavior, and user alignment are essential.</p>
<p>Prompting LLMs.</p>
<p>Prompting plays a key role in effective utilization of LLMs, as it requires no additional training or fine-tuning.LLMs are highly versatile once they are pre-trained on large-scale datasets, and careful prompting can strongly exploit this versatility.Instead of building task-specific systems, users can simply provide well-crafted prompts to perform a wide range of tasks.Consequently, prompt engineering [132] has become an active area of research within the NLP community.Owing to the benefits of prompting, LLMs are also known for their zero-shot learning-the ability to perform tasks they were not explicitly trained on.</p>
<p>Manuscript submitted to ACM Another relevant concept in this regard is in-context learning (ICL).In ICL, a well-trained LLM is provided an example of the input-output mapping in the prompt itself during the model inference stage.The model tries to understand the semantics behind this mapping and generalize it to the query in the prompt.The benefit of ICL is that it does not require further training or fine-tuning of the model on the new input-output mapping.</p>
<p>As well-known, LLMs have still been reported to face challenges in tasks that require arithmetic reasoning [201] and commonsense reasoning [64].This shortcoming can be helped with ICL.Nevertheless, the 'reasoning' requirements of such tasks call for more sophisticated solutions.Hence, to incorporate reasoning capabilities, researchers use Chainof-Thought (CoT) prompting, which encourages models to solve problems by breaking them down into a series of intermediate reasoning steps.This mimics human-like cognitive processes and has shown notable improvements, particularly in complex mathematical reasoning tasks [176].</p>
<p>There are three common forms of CoT prompting used in ICL:</p>
<p>• Zero-shot CoT prompting, where the model is prompted without examples but encouraged to generate intermediate reasoning steps.</p>
<p>• One-shot CoT prompting, where one example is provided in the prompt.</p>
<p>• Few-shot CoT prompting, where a small number of examples are included.</p>
<p>These prompting strategies enable LLMs to generalize better and solve more complex problems without modifying the model's internal parameters.</p>
<p>Training at Scale</p>
<p>3.2.1 Data Source.Data is a foundational element in training LLMs, and its importance in learning effective models cannot be overstated.While increasing the quantity of training data is essential, more data does not necessarily equate to better model performance.Several challenges must be addressed when sourcing the data:</p>
<p>• Data Quality: Raw data collected from diverse sources is often noisy or irrelevant.Therefore, filtering and cleaning processes are critical during data preparation.As demonstrated in [131], up to 90% of web-scraped data may need to be removed before training the model due to its adverse effect on the model.• Data diversity: A robust dataset must encompass a wide variety of domains and languages to ensure generalization and reduce bias.Data sourced from a single or similar corpus can compromise the versatility of the model.</p>
<p>• Bias: Bias can arise from class imbalance or insufficient representation across languages and topics in the dataset.</p>
<p>To mitigate bias, datasets are often needed to be explicitly balanced and diversified.</p>
<p>• Privacy concerns: When utilizing large-scale datasets, protecting sensitive information is a key concern.LLMs can be fine-tuned to detect and refuse prompts that might lead to data leakage [191].</p>
<p>Pre-training corpora to train LLMs at scale are typically divided into two categories: general data and specialized data [54,212].We summarize the common sources of these categories below.</p>
<p>• General Data:</p>
<ol>
<li>
<p>Webpages: The Internet offers a massive corpus of both high-and low-quality text.However, filtering is vital to remove spam, misinformation, or irrelevant content while preserving valuable sources.</p>
</li>
<li>
<p>Conversational Text: Public dialogue datasets [14,142] and social media platforms provide conversational data useful for training models in dialogue and response generation.</p>
</li>
<li>
<p>Books: Long-form text from books supports the learning of linguistic richness, narrative flow, and long-term dependencies.Datasets like Books3 and BookCorpus2 (from the Pile [44]) are commonly used.</p>
</li>
</ol>
<p>Manuscript submitted to ACM</p>
<p>• Specialized Data:</p>
<ol>
<li>
<p>Multilingual Text: They enhance the multilingual capabilities of LLMs.</p>
</li>
<li>
<p>Scientific Texts: They strengthen domain knowledge in technical or academic contexts, often sourced from arXiv, scientific textbooks, or math websites.</p>
</li>
<li>
<p>Code: Programming data improves the model's ability to generate structured code and solve logic-based tasks.</p>
</li>
</ol>
<p>Common sources include Stack Exchange [196] and GitHub.Interestingly, formulating reasoning problems as code can also improve the accuracy of generated responses [105].</p>
<p>Model Considerations</p>
<p>. Developing LLMs at scale often involves careful considerations related to the underlying neural architecture of the models.Key considerations in this regard include:</p>
<p>• Layer normalization and residual connections [194]: Most LLMs need to apply layer normalization inside residual blocks to enhance stability and trainability of their deep Transformer-based architectures, referred as pre-layer normalization (Pre-LN).Post-layer normalization (Post-LN) can achieve better performance in shallower models, but it may lead to training instability in deeper architectures due to issues like vanishing gradients [194].Recent research has explored hybrid strategies to combine the benefits of both Pre-LN and Post-LN [75].</p>
<p>• Activation functions: Activation functions are responsible for incorporating non-linearity in the modeling process of neural networks.The choice of activation function is crucial, particularly for FFNs.While ReLU [48] is the prevailing standard, other options have also proven effective in the context of LLMs, e.g., Gaussian Error Linear Unit (GeLU) [55], used in GPT-3 [20] and BLOOM [83], Gated Linear Unit (GLU) [35], used in Gemma [165], and SwiGLU [153], adopted in models like PaLM [31] and LLaMA [169].</p>
<p>• Bias removal: Inclusion/removal of bias terms is another relevant design choice for LLM architectures.Some models, such as LLaMA [169] and Gemma [165], eliminate bias terms in components like layer normalization, FFNs and query-key-value transformations to improve training dynamics.</p>
<p>Distributed Training.</p>
<p>Training LLMs at scale demands significant computational resources, typically provided by distributed systems.To that end, various forms of parallelism are employed, including; data parallelism, model parallelism, tensor parallelism and pipeline parallelism.For instance, LLaMA-3 (405 billion (B) parameters) [49] was trained on up to 16,000 H100 GPUs, with each server containing 8 GPUs.Communication across servers was managed using distributed parallelism.</p>
<p>Scaling Laws.</p>
<p>Scaling laws [70] describe the empirical relationship between LLM performance and key training variables such as model size, dataset size, and computational budget.Research shows that performance consistently improves with increased data-even having the scale of trillions of tokens, and larger models.These scaling laws serve as a blueprint for LLM development, helping researchers make informed decisions about how to allocate resources effectively.</p>
<p>In summary, LLMs are advanced AI systems that can simulate human-like intelligence through large-scale learning [38].By leveraging deep learning architectures and massive corpora of data, they learn complex patterns in text, enabling them to generate coherent, contextually appropriate responses and content [56,88].</p>
<p>Manuscript submitted to ACM</p>
<p>Mainstream LLMs</p>
<p>LLMs are a type of Transformer-based PLMs with tens to hundreds of billions of parameters.In this section, we categorize LLMs into closed-source and publicly available models.Within each category, we further group models by their families and present them according to their development timeline.</p>
<p>Closed Source LLMs</p>
<p>4.1.1The GPT Family (OpenAI).The Generative Pre-trained Transformer (GPT) family consists of decoder-only Transformer models developed by OpenAI.This family of LMMs started with GPT-1 [138] and GPT-2 [139], and moved on to more advanced models, discussed below.</p>
<p>GPT-3 [20]: Often regarded as the first true LLM, GPT-3 is an autoregressive model with 175 billion parameters.It shares the same architecture as GPT-2 but introduces sparse attention and adopts gradient noise scaling [111] for better training.GPT-3 is significantly larger than earlier PLMs and exhibits emergent capabilities not seen in smaller models.</p>
<p>CODEX [26]: A descendant of GPT-3, Codex is fine-tuned on code corpora from GitHub and is optimized for generating code from natural language prompts.</p>
<p>WebGPT [119]: Another GPT-3 variant, WebGPT is trained to answer open-ended questions by browsing web pages, enhancing its ability to perform web-based information retrieval.</p>
<p>InstructGPT [128]: This version aligns the model with human intent by fine-tuning it using datasets containing human-written demonstrations and preferences.It shows improved truthfulness and reduced toxicity.GPT-3.5:Built on GPT-3 and Codex, GPT-3.5 benefits from fine-tuning on code and improved instruction-following as well as RLHF.GPT-3.5 Turbo [15] is a faster, more cost-effective version optimized for chat.</p>
<p>ChatGPT [147]: A major milestone in LLMs, ChatGPT was fine-tuned with human preference data sourced from a wide range of texts (e.g., Wikipedia, books, websites, scientific papers, articles and news media [184]).It excels at delivering engaging, natural conversations [175] and completing diverse tasks such as translation, summarization, and Q&amp;A [149].</p>
<dl>
<dt>GPT-4 [2]</dt>
<dd>
<p>GPT-4 is a multimodal model capable of processing both text and images.It delivers significantly improved performance on complex tasks and achieves human-level results on professional benchmarks.GPT-4 uses "predictable scaling" to optimize training efficiency, which improves performance in a measurable manner with respect to the required training resources.GPT-4 Turbo [150] is an enhanced version of GPT-4 with better capabilities, larger context length, and integrated tools such as vision, DALL•E 3, and text-to-speech.[125]: GPT-4V focuses on safely deploying the visual capabilities of GPT-4, offering strong performance in a variety of vision tasks.</p>
</dd>
</dl>
<p>GPT-4V</p>
<p>GPT-4o [62]: GPT-4o is an "omni" model that handles any combination of text, audio, image, and video as input and output.It brings powerful multimodal capabilities with an emphasis on audio.</p>
<p>OpenAI o1 and o3-mini [65, 127]:</p>
<p>• o1 uses a novel optimization technique and a tailored dataset, supporting visual reasoning.</p>
<p>• o3-mini is a lightweight model optimized for reasoning and cost-efficiency, though it does not support vision.GPT-4.5 [126]: Building on the advancements of GPT-4o, GPT-4.5 offers a more natural interaction experience.It features a broader knowledge base, a stronger alignment with user intent, and improved emotional intelligence, allowing it to better understand and respond to emotional cues.Additionally, GPT-4.5 reduces the occurrence of hallucinations, ensuring more accurate and reliable responses.</p>
<p>Manuscript submitted to ACM</p>
<p>The Gemini Series (Google).</p>
<p>Gemini is a new family of multimodal LLMs, succeeding LaMDA [168] and PaLM-2.</p>
<p>Gemini [163]: Gemini demonstrates strong capabilities across text, image, video, and audio understanding, with support for a 32K context length.However, Gemma [165] and Gemma 2 [166] are lightweight, open-source versions of Gemini, providing powerful multimodal capabilities for free.</p>
<p>We summarize state-of-the-art closed-source LLMs in Table 1.</p>
<p>Since ChatGPT, released in November 2022, was the first LLM to demonstrate exceptional performance across a variety of tasks, we focus on models released from 2023 onward.Before ChatGPT, LLMs were generally less powerful, but following its release, models have grown powerful enough to compete with ChatGPT, driven by exponential growth in both model size and data.It is worth noticing that we only list representative LLMs in the table.For example, we list Gemini but omit Gemini 1.5 due to limited technical innovations.However, since PaLM and PaLM-2 represent distinct models, both are included.</p>
<p>Publicly Available LLMs</p>
<p>4.2.1</p>
<p>The LLaMA Family (Meta).The LLaMA family consists of a series of foundational language models developed by Meta.Manuscript submitted to ACM LLaMA [169]: The first model in the LLaMA family, LLaMA, ranges from 7B to 65B parameters and is pre-trained on trillions of tokens.It is built upon the Transformer architecture, similar to GPT-3 [18], with several architectural modifications, including:</p>
<p>• Adoption of the SwiGLU activation function instead of ReLU.</p>
<p>• Replacement of absolute positional embeddings with rotary positional embeddings.</p>
<p>• Use of root mean squared layer normalization rather than standard layer normalization.</p>
<p>LLaMA-2 [170]: LLaMA-2 includes both foundational models and chat-optimized versions, such as LLaMA-2 Chat.</p>
<p>While the architecture remains largely the same as LLaMA, LLaMA-2 was trained on 40% more data.LLaMA-2 Chat further improves safety by fine-tuning on safe response samples and incorporating an additional RLHF step.</p>
<p>LLaMA-3 / 3.1 [49]: LLaMA-3 / 3.1 models are trained on a dataset seven times larger than that of LLaMA-2.LLaMA-3 comes in two sizes: 8B and 70B.With the largest model featuring 405B parameters, LLaMA-3.1 benefits from 15T tokens of training data and a significantly larger context window (128K compared to LLaMA-2's 8K).These improvements lead to its competitive performance, achieving results comparable to GPT-4 and GPT-4o.</p>
<p>LLaMA-4 [114]: LLaMA-4 introduces significant advancements by supporting text, images, audio, and video, enabling seamless multimodal reasoning and generation.This is a major shift from its predecessors, LLaMA-2 and LLaMA-3, which were primarily text-based.Leveraging a Mixture of Experts (MoE) architecture, LLaMA-4 efficiently scales to trillions of parameters, activating only a subset of experts during inference for optimal performance.Additionally, it</p>
<p>Manuscript submitted to ACM supports an impressive context length of up to 10 million tokens, making it ideal for long-form tasks like multi-document summarization and complex dialogues.</p>
<p>In addition to the core LLaMA models, a variety of instruction-following models have been developed based on</p>
<p>LLaMA or LLaMA-2, including: Alpaca [161], Vicuna [30], Guanaco [39], Koala [46], Mistral 7B [66], Code LLaMA [143], Gorilla [130], Giraffe [129], Vigogne [60], Tulu 65B [180], Long LLaMA [172], Stable Beluga 2 [106], Qwen [13],</p>
<p>among others.Vicuna is particularly popular for its multimodal language modeling, giving rise to models like LLaVA [102], MiniGPT-4 [216], InstructBLIP [34], and PandaGPT [157].We present an overview of state-of-the-art publicly available LLMs in Table 2, focusing on listing only the most representative LLMs of each type.</p>
<p>Applications of LLMs in CAD</p>
<p>This section presents the existing literature focusing on applications of LLMs in CAD.Due to the nascency of this emerging but critical research area, we are able to provide a comprehensive overview of the recent developments.These advances are organized by grouping the works according to their key application directions in the CAD domain.</p>
<p>Data Generation</p>
<p>LLMs have emerged as powerful tools for content generation, making data generation one of their primary applications in the field of CAD.This capability is particularly useful for creating synthetic datasets, fine-tuning models, and addressing the lack of annotated CAD data.</p>
<p>Yuan et al. [208] utilized GPT-4o [62] to generate datasets that map natural language descriptions and rendered view images to CAD operation sequences.This approach enabled the construction of textual descriptions corresponding</p>
<p>Manuscript submitted to ACM to CAD commands.Similarly, Xu et al. [197] employed the open-source model InternVL2-26B [28], by randomly selecting four view images of CAD models to generate high-quality textual captions.These captions were used to build a multimodal CAD dataset.Khan et al. [72] adopted a two-step generation process using open-source LLMs and Vision-Language Models (VLMs).First, they rendered multi-view images of individual parts and the final models, which were input to LLaVA-NeXT model [101] with predefined prompts to generate simplified object-level descriptions.Then, they used Mixtral-50B [67] to produce multi-level natural language instructions.Raw CAD sequences from DeepCAD [187] were preprocessed to replace meaningless keys with more descriptive terms, which, along with the simplified shape descriptions, served as input to Mixtral-50B.The result was a four-level hierarchy of textual instructions: abstract, simplified, generalized geometric, and detailed geometric descriptions, generated using -shot prompting techniques [20].</p>
<p>In another recent work, Wang et al. [177] used a VLM to generate initial draft captions for rendered CAD images, which were then refined through human annotations to create a dataset, pairing these captions with CAD parametric sequences.In [178], Wang et al. employed GPT-4o [62] to classify and filter CAD models, and then used InstructGPT [181] to generate natural language descriptions.The resulting dataset contained CAD modeling sequences aligned with textual descriptions and rendered images from fixed viewpoints.</p>
<p>Other examples of using LLM for CAD data generation include [43], which developed BlendNet, a custom training dataset designed to capture diverse communication styles across three axes: 16 object categories, 8 instruction tones, and 5 length categories.Starting with 135 manually crafted seed instructions, the authors used self-instruct data distillation to expand to 50,000 samples.GPT-4o [62] was used to generate scripts from these instructions, which were executed to produce corresponding images.GPT-4o then served as a validator to assess the alignment between images and instructions, resulting in a final dataset of 2,000 human-verified and 6,000 model-validated samples.Similarly, Li et al. [90] also addressed the challenge of unavailable textual descriptions in existing CAD datasets.They applied the multimodal model CoCa [205] to generate captions for parametric CAD models in the DeepCAD dataset [187], creating a new, richly annotated dataset linking text to CAD models.</p>
<p>It is worth noting that current research in data generation primarily leverages the text generation capabilities of</p>
<p>LLMs.To date, in the CAD field, there is no work that directly generates other forms of LLM-based data-such as images, point clouds, or other format of data-to construct new datasets in the CAD domain.</p>
<p>CAD Code Generation</p>
<p>Many LLMs, such as Codex [26], are pre-trained on vast code corpora, enabling them to generate code with a high degree of accuracy.Thus, a prominent application of LLMs in the CAD field is the generation of CAD code.</p>
<p>In this direction, Li et al. [91,92] explored the use of multimodal LLMs for 3D CAD generation, leveraging various data formats, including textual descriptions, images, sketches, and ground truth 3D shapes.In this work, LLMs were employed to generate CAD programs from these inputs, which were subsequently parsed into 3D shapes.Two models were evaluated: GPT-4 [2] and GPT-4V [125].To improve the quality of the generated CAD code, the authors developed a debugger that iteratively refined the generated code until it successfully executed.The results showed that GPT-4V outperformed GPT-4, especially when using text-only input.It also surpassed other input combinations (e.g., text + sketch, text + image, and text + sketch + image) in terms of parsing rate and intersection over union.Sun et al. [158] recently conducted a fine-tuning experiment on LLMs, comparing them to the baseline GPT-4 [2] without fine-tuning.They used the dataset from [91] and implemented four different sampling strategies for fine-tuning data generation.The first strategy randomly selected 60 code templates, the second ensured diversity in code length, the Manuscript submitted to ACM third focused on the shortest lengths, and the fourth on the longest.Overall, the second strategy achieved the highest average parsing rate, while the third performed best in terms of intersection over union.All four fine-tuned LLMs outperformed the baseline GPT-4 in most cases.In [43], Du et al. developed BlenderLLM, a script generation model that underwent SFT and iterative self-improvement.They fine-tuned the Qwen2.5-Coder-7B-Instructmodel [61] using the BlendNet dataset and created a filter to select high-quality data generated by the model.This approach allowed for iterative optimization through cycles of data generation and model training.On similar lines, Alrashedy et al. [4] introduced CADCodeVerify, a framework designed to iteratively verify and refine 3D objects generated from CAD code.</p>
<p>CADCodeVerify employs a VLM to assess the generated object by answering a set of questions and correcting deviations.</p>
<p>They also introduced a benchmark for CAD code generation, containing 200 prompts paired with expert-annotated scripting code.The framework's feedback loop involved two steps: generating question-answer pairs and producing feedback without human intervention.The study compared the performance of three LLMs: GPT-4 [2], Gemini [163],</p>
<p>and Code LLaMA [143].</p>
<p>In [12], Badagabettu et al. presented Query2CAD, a framework that generated CAD designs by using LLMs to produce executable CAD macros.The process involved the user query being fed into a robust LLM, such as GPT-3.5</p>
<p>Turbo or GPT-4 Turbo, which generated a Python macro.In cases where errors occurred, the error messages and Python code were provided to the LLM, which then generated corrected, executable code.The framework also utilized BLIP2 [86] to generate captions for isometric views of CAD models, which were then passed onto the LLM to improve code generation through self-refinement loops.Yuan et al. [208] fine-tuned pre-trained LLMs to create their own LLM, called OpenECAD, which integrated the logical, visual, and coding abilities of VLMs.They rendered Boundary Representation (B-rep) data to generate 3D model images from three different views: default, orthographic, and transparent.For the base language models, they selected relatively small models like OpenELM [112], Gemma [165], and Phi-2 [1].To endow OpenECAD with multimodal conversational capabilities, they leveraged training methods and datasets from LLaVA [102] and used GPT-4 [2] to generate multimodal image-language instruction data.The model was trained using the TinyLLaVA [214] framework and fine-tuned with the LoRA [58] method to enable the VLM to generate CAD code effectively.</p>
<p>In addition, several studies have explored the potential of LLMs across a variety of design and manufacturing tasks.</p>
<p>For instance, Makatura et al. [107,108] evaluated the performance of LLMs in diverse design domains, including 2D vector graphic design (using SVG and DXF formats), 3D parametric modeling (using Constructive Solid Geometry (CSG) and B-rep formats), and articulated robotics problems (using the Universal Robot Description Format (URDF) and general graph-based representations).Except for URDF, the team initially used GPT-4 [2] to generate code, which was then converted into the corresponding 3D models in the desired format.In [122], GPT-4 [2] was used to generate CAD code, employing a back-and-forth dialogue process.Errors in the generated code and resulting structure were fed back into GPT-4, allowing it to iteratively refine the code.The study demonstrated GPT-4's remarkable ability to generate unique solutions for constructing and debugging CAD code.</p>
<p>Mallis et al. [110] utilized GPT-4o [62] to generate CAD code actions expressed in Python.The generated code was executed in CAD software, and the output was concatenated with the input context before being fed back to GPT-4o for the next iterative step.Rukhovich et al. [144] proposed CAD-Recode, a solution for CAD reverse engineering.They fine-tuned an LLM to map input point clouds into CAD sketch-extrude sequences represented as Python code.The LLM used was Qwen2-1.5B[200], a relatively small model.CAD-Recode was augmented with a lightweight, trainable point cloud projector.The framework also utilized GPT-4o [62] to refactor the generated code in real time via interactive sliders.</p>
<p>Manuscript submitted to ACM Fig. 2. Typical CAD code generation pipeline.A prompt and, optionally, an image are first input to a VLM or LLM.The VLM/LLM then generates the corresponding code, which is executed.If the code is not executable, refinement is performed until it is.Subsequently, the generated objects' computing similarity to the ground truth is evaluated.If the similarity is below a threshold, the code refinement process is repeated until the desired results are achieved.</p>
<p>Jones et al. [69] worked on generative CAD design with GPT-4o [62] (without fine-tuning) and a solver-aided, domainspecific language: AI design language (AIDL).The process involved prompting GPT-4o with detailed descriptions of AIDL, including its syntax and available geometry types.The LLM then generated a complete AIDL program prompted by manually designed example programs, which was executed to create the desired model.If errors occurred during execution, they were fed back to GPT-4o, which would correct them.Naik [118] demonstrated the use of LLMs for creating extraction queries in SQL for geometry tagging, streamlining the tool's functionality by allowing users to provide natural language input instead of complex code.Kienle et al. [74] designed QueryCAD, a deep learning-based question-answering system for CAD models.QueryCAD employed a code-writing LLM with in-context prompting to generate executable code for multi-view CAD part segmentation.It was integrated into MetaWizard, a robot program synthesis system [5], to automatically generate industrial robot programs based on natural language task specifications.</p>
<p>The team primarily used GPT-4o [62] and compared its performance with other LLaMA models (e.g., LLaMA-3 13B, LLaMA-3.1 8B, and LLaMA-3.1 405B) [49].</p>
<p>In [124], Ocker et al. developed a VLM-based multi-agent system for CAD model generation.This system utilized multiple LLM-based agents to interpret sketches, images, and textual descriptions, generate CAD models, and verify the designs.The study proposed strategies for overcoming the well-known spatial reasoning limitations of VLMs, using GPT-4o [62] as the core LLM.In another recent work, Deng et al. [37] applied GPT-4 [2] to address three sub-tasks: parametric computation, instruction sequence construction, and coding model scripting.The process involved providing GPT-4 with design requirements, performing iterative design computations, and generating the necessary code by parsing a JSON file to formalize natural language instructions into executable commands.</p>
<p>CAD code generation remains one of the most crucial applications of LLMs in CAD.These generated code can take various forms, such as Python code, SQL code, CAD software-specific code, and even Java or C++ code.Users can generate the desired code formats based on their specific domain knowledge.Figure 2 illustrates the typical CAD code generation pipeline employed by most state-of-the-art approaches.</p>
<p>Parametric CAD Generation</p>
<p>Parametric CAD generation focuses on creating parametric data for CAD rather than code.In this application, LLMs generate parametric sequences, often formatted as JSON files.</p>
<p>Manuscript submitted to ACM In the direction of parametric CAD generation, Wu et al. [189,190] utilized pre-trained LLMs to efficiently manipulate engineering sketches, integrating sketch primitive sequences and images for parametric CAD generation.This included CAD autocompletion, CAD autoconstraint, and image-conditioned generation.They employed CodeT5+ [182] as the text encoder and decoder for sketch primitive sequences.To assess the general LLM's capability in CAD autocompletion, they fine-tuned ChatGPT on three subsets, demonstrating that ChatGPT outperformed other baselines.Similarly, Li et [90] explored text-to-3D parametric CAD generative modeling.During training, both parametric CAD sequences and textual descriptions were used as inputs, and the model generated parametric CAD sequences.At the inference stage, only textual descriptions were provided to generate parametric CAD sequences.</p>
<p>In [197], Xu et al. proposed CAD-MLLM, a system designed to generate parametric CAD models conditioned on multimodal inputs such as text, images, and point clouds.For images, they rendered multi-view images from eight fixed perspectives.For point clouds, they randomly sampled points at different ratios and recorded their corresponding normal information.They fine-tuned the open-source LLM Vicuna-7B [30] and used LoRA [58] during training to minimize learnable parameters.You et al. [204] focused on reverse engineering 3D CAD models from images.They first used the GPT-4V [125] foundation model to predict a global discrete base structure, extracting semantic information from images.The model identified semantic parts from the image before generating CAD sequences for each part.They then built a Transformer model to predict continuous attribute values based on the discrete structure with semantics.</p>
<p>Zhang et al. [211] applied LLMs for controllable generation across various CAD construction hierarchies.They initially converted a CAD model into structured text.They then fine-tuned an LLM to develop a unified model for controllable CAD generation.During training, a hierarchy-aware field in the CAD text was masked, and LLMs were tasked with predicting the masked field.During inference, users could specify the part to modify by masking it and inputting it into the LLM for generating new CAD models.Wang et al. [178] proposed a CAD synthesis method using a multimodal LLM with enhanced spatial reasoning capabilities.They input either a single image or text and mapped the 3D space into 1D using a tokenization method to improve spatial reasoning.They employed LLaVA-1.5 7B [100] as their base model, with pre-trained Vicuna [30] as their foundation, which was built on LLaMA-2 [170].The 3D CAD model was represented in JSON format, capturing key modeling commands and parameters in the order of CAD construction, based on the DeepCAD dataset [187].</p>
<p>Utilizing the LLaMA-3 8B Instruct [49] LLM as the backbone, Wang et al. [177] introduced a CAD-Fusion framework.</p>
<p>It alternated between two training stages: the sequential learning stage, which trained LLMs using ground truth parametric sequences, and the visual feedback stage, which rewarded parametric sequences that rendered visually preferred objects and penalized those that did not.Makatura et al. [107,108] created a design space defined by parametric design and the bounds of these parameters, encompassing a range of potential designs.When prompted with lower and upper bounds for parameters, LLMs suggested values based on typical proportions for the designed object.While the absolute scale was arbitrary, the proposed bounds were semantically reasonable and proportionate.In addition to creating design spaces, they also used GPT-4 [2] to generate XML-format data rather than code, focusing on pre-existing designs in formats like URDF.</p>
<p>Parametric CAD Generation is another key application of LLMs in the CAD field.Unlike CAD code generation, which relies on LLMs to generate executable code, parametric CAD generation uses LLMs to produce parametric sequences instead of code.While generating accurate and executable code can be challenging for state-of-the-art LLMs, parametric CAD generation is often preferred by researchers.Although LLMs still face difficulties in producing perfectly accurate parametric sequences, if the goal is to generate keys and values rather than fully executable sequences, many Manuscript submitted to ACM Fig. 3.A standard parametric CAD generation pipeline.A prompt, along with an optional image (converted into features by a frozen image encoder), is fed into a trainable VLM or LLM to generate parametric data.This data is then parsed to produce 3D CAD models.LLMs can also be employed to compute the similarity between the ground truth and the generated object.</p>
<p>CAD challenges can be alleviated.Figure 3 illustrates a standard parametric CAD generation pipeline used by most state-of-the-art methods in this direction.</p>
<p>Image Generation</p>
<p>Whereas generative technologies are becoming increasingly popular in visual modeling [173], the use of LLMs in image generation tasks related to CAD is still a widely open direction.Recently, Tang et al. [160] employed LangGraph3 , an LLM, to perform digital CAD drawing restoration and utilized Retrieval-Augmented Generation (RAG) [84] technology to incorporate engineering domain knowledge into the model.This is the only work we currently find that generates CAD drawings from an LLM in an end-to-end manner.</p>
<p>Model Evaluation</p>
<p>LLMs can also be used to compare two sources of data.For instance, ChatGPT can be used to estimate similarity between two textual descriptions-one generated by a model and the other being the ground truth.This ability of LLMs is also making its way into the CAD domain.For instance, Du et al. [43] developed an evaluation framework, CADBench, which utilized GPT-4o [62] for two complementary evaluation tasks: image-based evaluation, which assessed visual fidelity, and script-based evaluation, which compared objective attributes such as size, color, and material.In [12], the visual question-answering score between the user query and the generated isometric image was computed using Clip-FlanT5-XL [96].Similarly, in [160], Tang et al. fed repaired drawings alongside ground truth drawings into GPT-4 [2] for automatic scoring along three dimensions: legibility, completeness, and tolerance.</p>
<p>Wang et al. [177] leveraged the strong visual understanding capabilities of VLMs to score visual objects, aiding in the construction of preference data, which would otherwise be costly and labor-intensive.The rendered CAD images, along with an instruction detailing the evaluation criteria-divided into three categories: shape quality, shape quantity, and distribution-were passed into the VLM, LLaVA-OV-Qwen2-7B [85], to compute the scores.</p>
<p>Text Generation</p>
<p>Text generation is one of the most common applications of LLMs.Unsurprisingly, many CAD-related works have also employed LLMs for generating text content.For example, Liu et al. [103] combined DALL-E [140], GPT-3 [20], and CLIP [137] within CAD software, enabling users to construct text and image prompts based on their modeling needs.</p>
<p>Designers could use text-to-image AI to generate reference images, avoid design bias, prevent design mindset, and stimulate new design considerations.Users would input their design intentions, and GPT-3 would suggest prompts for them to select from.After choosing, the suggestions were rephrased, and DALL-E would generate the final results.Kodnongbua et al. [79] employed ChatGPT to generate text prompts describing variations of a given model.Similarly, Yuan et al. [207] introduced a task for semantic commenting of CAD programs.In this task, LLMs segmented programs into code blocks, each corresponding to semantically meaningful parts, and assigned semantic labels to each block.</p>
<p>They executed the program to generate a 3D shape and rendered images from ten representative viewpoints.Each image was then translated into a photorealistic image using ControlNet [209].Using ChatGPT, the authors segmented the images into semantically meaningful parts and transformed every bounding box to a pixel-wise segment utilizing segment anything model [77], linking them to corresponding code blocks.They also explored how ChatGPT could comment on programs based on a given example program, finding that while it performed well at commenting similar programs, it struggled to generalize to new shapes and primitives.</p>
<p>In [204], You et al. employed a VLM to generate semantic part comments for their predicted global base structure.</p>
<p>They used GPT-4V [125] to interpret an input image and decompose it into semantic parts.Mallis et al. [110] fed multimodal user requests into a VLM, which then outputted a response in the form of a natural language plan.Along similar lines, Rukhovich et al. [144] utilized GPT-4o [62] for CAD-specific question answering, allowing users to query the system for information related to CAD models.In another recent effort, Yu et al. [206] employed a locally built LLM to analyze users' requirements and generate initial design ideas.The LLM was used to establish structural requirements and guide how to model the scheme.They also employed finite element analysis to improve data and fine-tuned the LLM to generate easily understandable design improvement suggestions.</p>
<p>An investigation of VLMs for automatically recognizing manufacturing features in CAD design was conducted in [73].The process began by converting a CAD file into three different image views, which, combined with prompts outlining the analysis criteria, served as inputs for five VLMs to identify and evaluate the features.The VLMs included both closed-source models such as GPT-4o [62], Claude-3.5-Sonnet,and Claude-3-Opus [10], as well as open-source models like MiniCPM-Llama3-V2.5 [59] and Llava-v1.6-mistral-7b[101].A review along conceptually similar lines was also conducted by Sun et al. [159], who analyzed key developments in rule-based reasoning (RBR), case-based reasoning (CBR), and large AI models for advancing reusable design in CAD software, summarizing both their advantages and disadvantages.They proposed a hybrid framework combining RBR, CBR and LLMs along with knowledge graphs to enhance reusable CAD design.</p>
<p>Picard et al. [133] explored the capabilities of GPT-4V [125] and LLaVA 1.6 34B [101] in performing various engineering design tasks, which involved both visual and textual information.These tasks were categorized into four main areas: conceptual design, system-level and detailed design, manufacturing and inspection, and engineering education.Most of these tasks involved generating textual descriptions from images and text prompts.They also assessed GPT-4V's spatial reasoning abilities, concluding that while the model demonstrated some spatial reasoning capability, it was still limited when compared to human performance.In [179], Wang et al. treated 2D drawings as raster images and employed an image encoder to extract features.They utilized general-purpose language scripts to represent 3D parametric models Manuscript submitted to ACM and leveraged an LLM to autoregressively predict parametric sequences in text form.They developed CAD2Program by fine-tuning an open-source VLM, Mini-InternVL-1.5-2B[45], which used InternViT-300M [27] as the vision encoder and InternLM2-1.8B[21] as the language model.Relevant applications of LLMs in manufacturing were reviewed by Li et al. in [94], evaluating LLM use in various tasks through case studies and examples.While this survey also covered areas outside CAD, it discussed CAD-related tasks including data generation, text-grounded 3D content generation, initial design drafts, and idea generation in aerospace design.</p>
<p>Text generation is primarily associated with question answering.While this application may seem simple, it is essential to recognize that it is one of the key strengths of LLMs.By effectively leveraging the text and content generation capabilities of LLMs, we can achieve results that go beyond initial expectations, unlocking significant potential for innovative solutions.</p>
<p>Analysis and Discussion</p>
<p>From the aforementioned state-of-the-art works in Section 5, it can be concluded that nearly all approaches utilize LLMs to generate intermediate representations rather than directly outputting 3D CAD models or 2D CAD drawings.This is likely because directly generating accurate 3D or 2D CAD outputs remains a significant challenge for current models.Common intermediate formats include executable code-such as Python scripts, and parametric data-often structured as JSON files, which can then be parsed or executed to construct the final CAD models.</p>
<p>We also make another key observation that most state-of-the-art methods rely on multimodal inputs, making the use of multimodal LLMs increasingly prevalent.This highlights the potential of different input types for LLM-based automation.Among these input types, text and images are used most frequently.However, other data formats-such as point clouds and sketch sequences-are also explored, reflecting the growing diversity of input modalities being considered in the field.</p>
<p>We also record the frequency with which each LLM is used in the state-of-the-art works that we review.Figure 4 summarizes the number of times different LLMs have been employed in the reviewed contributions.As apparent, GPT-4o is the most frequently used model (11 instances), followed by GPT-4 (8 instances) and GPT-4V (4 instances)-all developed by OpenAI.This trend suggests that, despite being closed-source and potentially requiring payment, LLMs from the GPT family are still widely favored by the research community in CAD domain.A likely reason is their superior performance compared to the other types of LLMs, including many publicly available models.</p>
<p>We also analyze the distribution of reviewed works across different application types.Table 3 summarizes the number of studies attributed to each category.As shown, the majority of contributions focuses on CAD code generation (17 works), followed by text generation (12 works).This suggests that CAD code generation is currently the most prominent and attractive application of LLMs in the CAD domain, and it is expected to continue gaining traction among researchers.</p>
<p>Text generation, being a core capability of LLMs, also remains a widely explored area within the research community.</p>
<p>We also analyze the datasets used in each state-of-the-art study.Table 4 presents the datasets employed across the reviewed works.Some datasets are sourced from industry data, while others are synthetically generated by the authors.</p>
<p>Many of the industrial datasets are derived from publicly available datasets, such as DeepCAD, Onshape4 , ABC [78], and others.</p>
<p>Finally, we map each state-of-the-art work to its industrial context.This allows us to identify current active applications of LLMs for CAD, as well as the domains that are relatively underexplored.Table 5 presents the industries most throughout the article, we enumerate a few more potential directions in this section based on our literature review.We hope that the discussion below paves the way to more effective and well-directed future research.</p>
<p>Application in Interior / Home Design</p>
<p>A promising future direction for LLMs in CAD is home design, also known as interior design.A few LLM-based tools for home design have already started to emerge [23,76].For instance, HomeGPT5 focuses on redesigning individual rooms with AI, making it ideal for completing home makeovers or room-by-room transformations.RoomGPT6 is another tool specializing in interior design, similar to ChatGPT, but tailored for creating and visualizing interior design styles and layouts.Users can upload a picture, and these systems generate a "dream space".In [23], I-Design utilized LLMs to transform text input into feasible scene graph designs, considering the relationships between objects.Littlefair et al.</p>
<p>[97] demonstrated that LLMs could be combined with traditional optimization techniques to generate both functional and aesthetically pleasing interior designs.However, some of these tools remain closed source, and others do not focus on 3D CAD models.Consequently, home design represents a promising, but currently an underexplored area that is expected to attract more research attention in the future.Industry Associated Works</p>
<p>Specific Data Format Generation</p>
<p>State-of-the-art LLMs are capable of generating a wide range of data formats, including text, image, audio, video, and code.As LLMs continue to evolve, future versions may expand their capabilities to generate even more complex data formats, such as point cloud and even 3D model.Consequently, the scope of LLM applications in data generation extends beyond merely producing text, captions, or textual descriptions.Future research could explore the potential of LLMs to generate these more specialized forms of data, paving the way for advancements in fields like 3D modeling, spatial data generation, and other emerging domains.</p>
<p>Building Compliance Checking in AEC</p>
<p>In the AEC industry, building designs must adhere to a wide range of requirements set forth by codes and standards [203].Compliance with these regulations is essential to avoid legal issues, delays, and safety hazards [145].Traditionally, ensuring compliance has been a manual process, requiring labor-intensive checks using 2D drawings and documents, which is time-consuming and costly.Additionally, building codes are complex and frequently updated, further complicating the process.As a result, the need for Automatic Compliance Checking (ACC) has emerged as a critical solution.</p>
<p>ACC involves two main steps: understanding design requirements from textual descriptions and analyzing 3D CAD Manuscript submitted to ACM models or 2D drawings to verify that they meet these requirements.Despite its importance, a universal and scalable ACC system remains a challenge [6].</p>
<p>LLMs have already shown some promise in addressing this challenge within the AEC industry.For instance, Zheng and Fischer utilized GPT technologies to enhance natural language-based building information modeling (BIM) searches [213].Additionally, Ying and Sacks [203] explored the use of LLMs for generic building compliance checking using BIM data.Du et al. [41] further leveraged LLMs to convert textual descriptions into code, editable BIM models and guiding iterative improvements in model quality.While these efforts focus on building infrastructure, there is a noticeable gap in research that directly applies LLMs to 3D CAD models.However, using LLMs to analyze 3D CAD models or 2D CAD drawings for automatic compliance checking holds significant potential, and further exploration in this area could greatly benefit the AEC industry.</p>
<p>Fashion AI and Textile Design</p>
<p>While much attention has been given to various applications of LLMs, fashion design, particularly in the context of textiles, remains largely unexplored.From Table 5, we observe that no state-of-the-art works have focused on leveraging LLMs for textile design.However, CAD plays a crucial role in fashion design, encompassing functions such as creative visualization, technical pattern development, and style information representation [186].</p>
<p>CAD in fashion design has streamlined the exchange of information across complex network of communication channels, reducing time, cost, and material usage, while also improving accuracy and garment quality.This is largely due to the digitization of design information, which facilitates communication and decision-making without relying on physical samples.Given these advantages, fashion design presents an exciting opportunity for the application of LLMs in generating 3D CAD models.The potential to combine LLMs with fashion CAD tools could open up new avenues for creativity, efficiency, and innovation in the fashion industry.</p>
<p>Conclusion</p>
<p>In this comprehensive review, we explored the intersection of LLMs and CAD, highlighting the transformative potential of LLMs in various CAD-related tasks.We began by introducing the significance of CAD in industry, emphasizing its broad applications across different sectors.We then delved into the foundational principles of LLMs in an accessible manner, covering their architecture, training methods, and fine-tuning processes that enable them to perform across diverse domains.We also presented an overview of state-of-the-art LLMs, focusing on both closed-source models like GPT and PaLM and publicly available models such as LLaMA and DeepSeek.Our review underscored the remarkable capabilities of these models in handling complex tasks related to CAD, from data generation to coding and parametric generation, image synthesis, model evaluation and text generation.These advancements open up new possibilities for automating design workflows, improving efficiency, and enhancing creativity in CAD processes.Additionally, we critically analyzed current research trends, pinpointing emerging areas and potential gaps in the existing literature.</p>
<p>Finally, we outlined promising future directions, including exploring LLMs in home design, specific data format generation, building compliance checking in AEC, and fashion design and textile applications.These areas offer exciting opportunities for future research and innovation, potentially leading to significant breakthroughs in the integration of LLMs with CAD.</p>
<p>Manuscript submitted to ACM</p>
<p>Fig. 1 .
1
Fig. 1.The taxonomy of this review.</p>
<ol>
<li>
<p>1 . 1
11
Tokens and Context Length.In the context of LLMs, tokens are the basic units of text that the model reads, processes, and generates.Tokens are pieces of text, like words, subwords, word pieces, or characters depending on the tokenizer.</p>
</li>
<li>
<p>1 . 2
12
The PaLM Family (Google).The PaLM (Pathways Language Model) series is developed by Google AI.PaLM[31]: It is a 540B-parameter model trained on 780B tokens.It introduces several architectural enhancements like SwiGLU activation, multi-query attention, and shared input-output embeddings.PaLM shows state-of-the-art few-shot performance on a wide range of tasks.PaLM-2[7]: PaLM-2 is a smaller (340B-parameter) but more efficient model, pre-trained on 3.6 trillion (T) tokens.It delivers superior reasoning and multilingual capabilities while reducing training and inference costs.Med-PaLM / Med-PaLM 2[155,156]: Med-PaLM is a domain-specific model fine-tuned for medical tasks.Med-PaLM 2 outperforms its predecessor using domain adaptation and ensemble techniques, achieving strong performance across clinical benchmarks and professional exams.PaLM-E[40]: It is a multimodal version with 562B parameters (540B from PaLM and 22B from a vision Transformer[36]).It supports visual question answering, zero-shot multimodal CoT reasoning, few-shot prompting, OCR-free math reasoning, and multi-image reasoning.U-PaLM[162]: U-PaLM improves model quality with minimal additional compute, achieving similar or better performance than PaLM at about half the computational budget.Flan-PaLM / Flan-U-PaLM[32]: These versions focus on instruction tuning and CoT prompting.Flan-PaLM, with 540B parameters fine-tuned on 1.8K tasks, significantly outperforms the base PaLM model.Flan-U-PaLM further enhances the performance with advanced adaptation techniques like UL2R[162].</p>
</li>
</ol>
<p>Gemini 1 .
1
5 [164]: Gemini 1.5 brings notable advancements, including a new model architecture, the use of mixtureof-experts (MoE), and training on a much larger dataset containing millions of tokens.Gemini 2.0[134]: Gemini 2.0 is a major update to its predecessor, improving both speed and performance compared to Gemini 1.5.</p>
<p>Gemini 2 .
2
5 [71]: Gemini 2.5 further enhances reasoning and coding capabilities, incorporating techniques like CoT prompting for more structured reasoning.</p>
<ol>
<li>2 . 2
22
The DeepSeek Family (DeepSeek).The DeepSeek series consist of LLMs developed by the Chinese AI company DeepSeek.DeepSeek-Coder[52]: DeepSeek-Coder is the first model in the DeepSeek family, followed by DeepSeek-LLM[17], DeepSeek-MoE[33], and DeepSeek-Math[152].DeepSeek-LLM investigates the scaling laws for LLMs to identify the optimal model size and training data scale.DeepSeek-MoE is an innovative MoE architecture specially designed towards ultimate expert specialization.DeepSeek-Math is a domain-specific language model that significantly outperforms the mathematical capabilities of open-source models and approaches the performance level of GPT-4 on academic benchmarks.DeepSeek-VL[104]: DeepSeek-VL is an open-source Vision-Language Model designed for real-world applications that combine vision and language understanding.DeepSeek-V2[98]: DeepSeek-V2 uses multi-head latent attention to reduce inference costs by compressing the key-value cache into a latent vector.This method achieves 5.76 times faster inference throughput compared to the previous DeepSeek models.DeepSeek-V3[99]: DeepSeek-V3 is a stronger MoE model, featuring 671B parameters, with 37B activated per token.It is pre-trained on 14.8T high-quality, diverse tokens and undergoes SFT and RL stages to fully optimize its capabilities.DeepSeek-R1[51]: DeepSeek-R1-Zero is pre-trained with large-scale RL without the initial SFT stage.DeepSeek-R1 follows a multi-stage training process, including cold-start data before RL, and incorporating two RL stages to improve reasoning patterns and align with human preferences, along with two SFT stages to enhance reasoning and non-reasoning abilities.</li>
</ol>
<p>Table 1 .
1
Summary of state-of-the-art closed source LLMs.
Model#Param. Tokens YearEntityKey FeatureChatGPT [147]--2022OpenAIDialogue-optimized LLM via RLHFGPT-4 [2]--2023OpenAIMultimodal support, stronger reasoningGPT-4V [125]--2023OpenAIVision-enabled GPT-4 variantGPT-4o [62]200B-2024OpenAIOmni-modal (text, image, audio, video)OpenAI o3-mini [127]3B-2025OpenAILightweight GPT-4 variant for mobile useGPT-4.5 [126]--2025OpenAIImproved emotional intelligencePaLM [31]540B780B2023GoogleDense decoder-only modelPaLM-2 [7]340B3.6T2023GoogleFine-tuned for multilingual reasoningMed-PaLM [155]540B-2023GoogleMedQA [68] benchmarked medical LLMPaLM-E [40]562B-2023GoogleEmbodied multimodal reasoningU-PaLM [162]540B-2023GoogleUnified fine-tuning with few-shot and CoTFlan-PaLM [32]540B-2024GoogleInstruction-tuned PaLM variantMed-PaLM 2 [156]--2025GoogleSafety-aligned medical LLMGemini [163]--2023DeepMindMultimodal, integrated with searchPanGu-Σ [141]1085B329B2023HuaweiChina's largest LLM, multilingualBloombergGPT [188]50B708B2023BloombergFinancial-domain LLMPPLX a70B-2023 Perplexity AI Retrieval-augmented, mobile-ready LLMInflection-2.5 b2.5B-2024 Inflection AI Personal assistant, dialogue-tunedClaude 3 [10]--2024AnthropicConstitutional AI, safety-tunedGrok 3 c--2025xAITwitter-integrated, humor-oriented chat
a https://grok.com/b https://www.perplexity.ai/c https://inflection.ai/blog/inflection-2-5</p>
<p>Table 2 .
2
Summary of state-of-the-art publicly available LLMs.
Model#Params. Tokens YearEntityKey FeatureLLaMA [169]65B1.4T 2023MetaFoundation model for instruction tuningLLaMA-2 [170]70B2T2023MetaImproved safety, performance, training dataCode LLaMA [143]70B1T2023MetaCode-specific extension of LLaMALIMA [215]65B-2023MetaFine-tuned with only 1K examplesLong LLaMA [172]7B1T2023MetaLong-context with Focused Transformer [172]LLaMA-3 [49]70B15T 2024MetaSupporting longer context, enhanced reasoningLLaMA-4 [114]109B40T 2025Meta10 million context length, multimodal + MoEDeepSeek-Coder [52]33B2T2024 DeepSeek Code synthesis and reasoningDeepSeek-LLM [17]67B2T2024 DeepSeek General-purpose pre-trainedDeepSeek-MoE [33]16B2T2024 DeepSeek Sparse MoEDeepSeek-Math [152]7B120B 2024 DeepSeek Math benchmark specialistDeepSeek-VL [104]7B2T2024 DeepSeek Vision-language alignedDeepSeek-V2 [98]236B8.1T 2024 DeepSeek High-performance general modelDeepSeek-V3 [99]671B14.8T 2024 DeepSeek Flagship MoE LLMDeepSeek-R1 [51]70B-2025 DeepSeek Reinforced instruction tuningStable Beluga 2 [106]70B-2023 Stability AI Instruction-tuned LLaMA-2Stable LM 2 [16]1.6B2T2024 Stability AI Lightweight modelGiraffe [129]13B-2023 Salesforce Visual grounding and reasoningInstructBLIP [34]13B-2023 Salesforce General-purpose vision-language modelCodeGen2 [123]16B400B 2023 Salesforce Multi-language code generationKoala [46]13B-2023BerkeleyDialogue-tuned LLaMA on curated dataGorilla [130]7B-2024BerkeleyAPI call-focused LLMOrca [117]13B-2023MicrosoftImitation learning from GPT-4WizardLM [195]13B-2024MicrosoftEvol-Instruct fine-tunedAlpaca [161]7B-2023StanfordInstruction-tuned LLaMA on self-instructVicuna [30]13B-2023LMSYSChatbot fine-tuned on ShareGPT [9]Guanaco [39]65B-2023UWQLoRA [39] fine-tuned VicunaMistral [66]7B-2023 Mistral AI Sliding window attention, strong performanceVigogne [60]7B-2023-French fine-tuned VicunaTulu [180]65B1.4T 2023Allen AIInstruction-tuned LLaMAQwen [13]14B3T2023AlibabaMultilingual and code generationLLaVA [102]13B-2023UIUCVision-language instruction tuningPandaGPT [157]13B-2023-Multimodal instruction-followingPythia [18]12B300B 2023 EleutherAI Transparent training for scientific benchmarkBaichuan2 [199]13B2.6T 2023BaichuanStrong multilingual and reasoning performanceFLM [93]101B311B 2023-Hybrid MoE + denseSkywork [183]13B3.2T 2023-Multilingual, instruction-tunedFalcon [131]7.5B600B 2023TIIRefinedWeb datasetZephyr [171]7.24B800B 2023 HuggingFace Aligned LLaMA-2 variantStartCoder [89]15.51T2023-Code generation LLMMPT [167]7B1T2023 MosaicML Commercially permissive LLMXuanYuan 2.0 [210]176B-2023-Financial sectorCodeT5+ [182]16B51.5B 2023-Code understanding + generationBloomZ and mT0 [116]176B350B 2023 BigScience Cross-lingual capabilitiesJamba [95]52B-2024 AI21 Labs Hybrid Transformer + MoEGemma [165]7B6T2024GoogleLightweight model with strong benchmarksMiniGPT-4 [216]13B-2024-Image-to-text multimodal capabilitiesChatGLM [47]9B10T 2024TsinghuaBilingual chat, long-contextLlemma [11]34B50B 2024-Math and science focusedCommand R+ d104B-2024CohereRAG-enhanced instruction model for finance
d https://cohere.com/blog/command-r-plus-microsoft-azureManuscript submitted to ACM</p>
<p>Table 4 .
4
A summary of the popular datasets used in the literature.
DatasetAssociated WorksSource DatasetTypePublicOpenECAD[208]DeepCAD [187]Industry✓Omni-CAD[197]OnshapeIndustry✗Text2CAD 1.0 &amp; 1.1[72]DeepCADIndustry✓Text-to-CAD[177]DeepCAD, SkexGen [198]Industry✗CAD-GPT[178]DeepCADIndustry✗BlendNet[43]-Synthetic✓LLM4CAD[91, 92, 158]-Synthetic✗CADPrompt[4]DeepCADIndustry✗Query2CAD[12]-Synthetic✓CAD-Assistant[110]SGPBench [135]Industry✗CAD-Recode[144]-Synthetic✓DeepCAD[144, 211]-Industry✓Fusion360 [185][144]-Industry✓CC3D [109][144]-Industry✓QueryCAD[74]ABC [78]Industry✗SketchGraphs [148][190]-Industry✓Text2CAD[90]DeepCADIndustry✗Img2CAD[204]ShapeNet [24]Synthetic✗ChatCAD[160]-Synthetic✗ReparamCAD[79]-Synthetic✗CADTalk[207]-Synthetic &amp; Industry✓CAD2Program[179]-Industry✗</p>
<p>Table 5 .
5
Mapping state-of-the-art research to relevant industries in CAD applications.</p>
<p>https://www.linkedin.com/pulse/cad-industry-past-present-future-iseeq-ndppf/
https://www.towardspackaging.com/insights/3d-cad-software-market-sizing?utm_source=chatgpt.com Manuscript submitted to ACM
https://www.langchain.com/built-with-langgraph Manuscript submitted to ACM
http://onshape.com Manuscript submitted to ACM
https://www.homegpt.app/
https://www.roomgpt.io/ Manuscript submitted to ACM
GPT-4oGPT-4 GPT-4V LLaVA-NeXT ChatGPT LLaMA-3/3.Frequency Fig.4. Bar chart of LLM usage frequency in CAD-related research (sorted descending).commonly mentioned in the reviewed works.As shown in the table, the manufacturing industry currently attracts the most attention of the research community, whereas the shipbuilding industry has garnered relatively little focus.We also include the textile industry in the table which has no notable associated works.Nevertheless, we predict that future works will be associated to this industry due to the relevance of CAD to textile.Future DirectionsLeveraging LLMs in CAD domain is becoming increasingly popular.Our survey shows that the research community has already started exploiting the potential of LLMs to address different CAD challenges.However, this area of research is still in its nascency.Due to the practical applications of CAD and growing trend of automation across industries, we expect to see a much wider interest of the research community in this direction in the near future.Whereas we have already pointed towards the interesting explorations for future research at the intersection of LLMs and CAD Manuscript submitted to ACM
Jyoti Marah Abdin, Hany Aneja, Ahmed Awadalla, Ammar Awadallah, Nguyen Ahmad Awan, Amit Bach, Arash Bahree, Jianmin Bakhtiari, Harkirat Bao, Behl, arXiv:2404.14219[cs.CL]A Highly Capable Language Model Locally on Your Phone. 2024Phi-3 Technical Report</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>The evolution and impact of cad in modern industries: a comprehensive review. Fiability &amp; Durability/Fiabilitate si Durabilitate. Elena Stăncioiu Alin, Stăncioiu Loredana, 2024. 2024331</p>
<p>Generating CAD Code with Vision-Language Models for 3D Designs. Pradyumna Kamel Alrashedy, Zulfiqar Tambwekar, Megan Haider Zaidi, Wei Langwasser, Matthew Xu, Gombolay, The Thirteenth International Conference on Learning Representations. 2024</p>
<p>RoboGrind: Intuitive and Interactive Surface Treatment with Industrial Robots. Benjamin Alt, Florian Stöckl, Silvan Müller, Christopher Braun, Julian Raible, Oliver Saad Alhasan, Lukas Rettig, Darko Ringle, Rainer Katic, Jäkel, 2024 IEEE International Conference on Robotics and Automation (ICRA). 2024</p>
<p>The promise of automated compliance checking. Robert Amor, Johannes Dimyadi, Developments in the built environment. 51000392021. 2021</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023. 2023arXiv preprint</p>
<p>Large language models: A survey of their development, capabilities, and applications. Yadagiri Annepaka, Partha Pakray, Knowledge and Information Systems. 2024. 2024</p>
<p>anon8231489123. 2023ShareGPT Vicuna Unfiltered Dataset. </p>
<p>Anthropic, The Claude 3 Model Family: Opus, Sonnet, Haiku. 2024268232499</p>
<p>Llemma: An Open Language Model for Mathematics. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, ICLR 202412th International Conference on Learning Representations. 2024</p>
<p>Query2CAD: Generating CAD models using natural language queries. Akshay Badagabettu, Sai Sravan Yarlagadda, Amir Barati, Farimani , arXiv:2406.001442024. 2024</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023. 2023arXiv preprint</p>
<p>The pushshift reddit dataset. Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, Jeremy Blackburn, Proceedings of the international AAAI conference on web and social media. the international AAAI conference on web and social media202014</p>
<p>Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases. Anas Belfathi, Nicolas Hernandez, Laura Monceaux, JURIX 2023-The 36th International Conference on Legal Knowledge and Information Systems. 2023</p>
<p>Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, arXiv:2402.17834Stable lm 2 1.6 b technical report. 2024. 2024arXiv preprint</p>
<p>Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, arXiv-2401DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. 2024. 2024arXiv e-prints</p>
<p>Pythia: A suite for analyzing large language models across training and scaling. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, O' Kyle, Eric Brien, Mohammad Hallahan, Shivanshu Aflah Khan, Purohit, Sai Usvsn, Edward Prashanth, Raff, International Conference on Machine Learning. 2023</p>
<p>Promptify: Text-to-image generation through interactive prompt exploration with large language models. Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, Tovi Grossman, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, arXiv:2403.17297Internlm2 technical report. 2024. 2024arXiv preprint</p>
<p>Design prototyping methods: state of the art in strategies, techniques, and guidelines. Vimal Bradley Camburn, Julie Viswanathan, David Linsey, Daniel Anderson, Richard Jensen, Kevin Crawford, Kristin Otto, Wood, Design Science. 32017. 2017</p>
<ol>
<li>I-design: Personalized llm interior designer. Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, Xi Wang, arXiv:2404.028382024arXiv preprint</li>
</ol>
<p>Thomas Angel X Chang, Leonidas Funkhouser, Pat Guibas, Qixing Hanrahan, Zimo Huang, Silvio Li, Manolis Savarese, Shuran Savva, Hao Song, Su, arXiv:1512.03012Shapenet: An information-rich 3d model repository. 2015. 2015arXiv preprint</p>
<p>AlpaGasus: Training a Better Alpaca with Fewer Data. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021. 2021arXiv preprint</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Zhaoyang Hao Tian, Liu, arXiv:2412.052712024. 2024arXiv preprint</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Security attacks on llm-based code completion tools. Wen Cheng, Ke Sun, Xinyu Zhang, Wei Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023. March 2023. 202335</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242023. 2023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Journal of Machine Learning Research. 252024. 2024</p>
<p>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. Damai Dai, Chengqi Deng, Chenggang Zhao, Rx Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, arXiv:2305.06500[cs.CV]2023</p>
<p>Language modeling with gated convolutional networks. Angela Yann N Dauphin, Michael Fan, David Auli, Grangier, International conference on machine learning. PMLR2017</p>
<p>Scaling vision transformers to 22 billion parameters. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, International Conference on Machine Learning. 2023</p>
<p>An Investigation on Utilizing Large Language Model for Industrial Computer-Aided Design Automation. Haoxuan Deng, Samir Khan, John Ahmet Erkoyuncu, Procedia CIRP. 1282024. 202434th CIRP Design Conference</p>
<p>From human writing to artificial intelligence generated text: examining the prospects and potential threats of ChatGPT in academic writing. Ismail Dergaa, Karim Chamari, Piotr Zmijewski, Helmi Ben Saad, Biology of sport. 402023. 2023</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in neural information processing systems. 362023. 2023</p>
<p>PaLM-E: An Embodied Multimodal Language Model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, International Conference on Machine Learning. 2023</p>
<p>Changyu Du, Sebastian Esser, arXiv:2408.08054Stavros Nousias, and André Borrmann. 2024. Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework. 2024arXiv preprint</p>
<p>Glam: Efficient scaling of language models with mixture-of-experts. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, International conference on machine learning. PMLR2022</p>
<p>BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement. Yuhao Du, Shunian Chen, Wenbo Zan, Peizhao Li, Mingxuan Wang, Dingjie Song, Bo Li, Yan Hu, Benyou Wang, arXiv:2412.142032024. 2024arXiv preprint</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020. 2020arXiv preprint</p>
<p>Mini-InternVL: a flexible-transfer pocket multi-modal model with 5% parameters and 90% performance. Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Shenglong Hao Tian, Junjun Ye, Xizhou He, Zhu, Visual Intelligence. 22024. 2024</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, Dawn Song, Koala: A Dialogue Model for Academic Researc. Blog post. 2023. April 2023</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, arXiv:2406.12793Chatglm: A family of large language models from glm-130b to glm-4 all tools. 2024. 2024arXiv preprint</p>
<p>Deep sparse rectifier neural networks. Xavier Glorot, Antoine Bordes, Yoshua Bengio, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statistics2011</p>
<p>Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.21783The llama 3 herd of models. 2024. 2024arXiv preprint</p>
<p>Parametric CAD/CAE integration using a common data model. Y-S Gp Gujarathi, Ma, Journal of Manufacturing Systems. 302011. 2011</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025. 2025arXiv preprint</p>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, Li, arXiv:2401.14196DeepSeek-Coder: When the Large Language Model Meets Programming-The Rise of Code Intelligence. 2024. 2024arXiv preprintManuscript submitted to ACM</p>
<p>Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects. Qasem Muhammad Usman Hadi, Abbas Al Tashi, Rizwan Shah, Amgad Qureshi, Muhammad Muneer, Anas Irfan, Muhammad Zafar, Naveed Bilal Shaikh, Jia Akhtar, Wu, Authorea Preprints. 2024. 2024</p>
<p>A survey on large language models: Applications, challenges, limitations, and practical usage. Rizwan Muhammad Usman Hadi, Abbas Qureshi, Muhammad Shah, Anas Irfan, Muhammad Zafar, Naveed Bilal Shaikh, Jia Akhtar, Seyedali Wu, Mirjalili, Authorea Preprints. 32023. 2023</p>
<p>Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). 2016. 2016arXiv preprint</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing Systems2022</p>
<p>Large language models for software engineering: A systematic literature review. Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, Haoyu Wang, ACM Transactions on Software Engineering and Methodology. 332024. 2024</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies. Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, First Conference on Language Modeling. 2024</p>
<p>Bofeng Huang, Vigogne: French Instruction-following and Chat Models. 2023</p>
<p>. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, arXiv:2409.121862024arXiv preprintet al. 2024. Qwen2. 5-Coder</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024. 2024arXiv preprint</p>
<p>Present and future impacts of computer-aided design/computer-aided manufacturing (CAD/CAM). Adekunle A Peter P Ikubanni, Adeleke, Chiebuka T Olayinka O Agboola, Boluwatife S Christopher, Joseph Ademola, Okonkwo, Peter O Olanrewaju S Adesina, Esther T Omoniyi, Akinlabi, Journal Européen des Systèmes Automatisés. 553492022. 2022</p>
<p>CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks. Mete Ismayilzada, Debjit Paul, Syrielle Montariol, Mor Geva, Antoine Bosselut, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024. 2024arXiv preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825[cs.CL]Mistral 7B. 2023</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024. 2024arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, Peter Szolovits, Applied Sciences. 1164212021. 2021</p>
<p>Felix Benjamin T Jones, Zihan Hähnlein, Maaz Zhang, Vladimir Ahmad, Adriana Kim, Schulz, arXiv:2502.09819A Solver-Aided Hierarchical Language for LLM-Driven CAD Design. 2025. 2025arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling Laws for Neural Language Models. 2020. 2020arXiv preprint</p>
<p>Gemini 2.5: Our most intelligent AI model. Koray Kavukcuoglu, 2025. 2025Google Blog</p>
<p>Text2cad: Generating sequential cad designs from beginner-to-expert level text prompts. Mohammad Sadil Khan, Sankalp Sinha, Talha Uddin, Didier Stricker, Sk , Aziz Ali, Muhammad Zeshan, Afzal , Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Leveraging Vision-Language Models for Manufacturing Feature Recognition in CAD Designs. Muhammad Tayyab Khan, Lequn Chen, Ye , Han Ng, Wenhe Feng, Nicholas Yew, Jin Tan, Seung Ki, Moon , arXiv:2411.028102024. 2024arXiv preprint</p>
<p>Claudius Kienle, Benjamin Alt, Darko Katic, Rainer Jäkel, arXiv:2409.08704QueryCAD: Grounded Question Answering for CAD Models. 2024. 2024arXiv preprint</p>
<p>Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, Min Kang, Yoo, arXiv:2502.02732Peri-LN: Revisiting Layer Normalization in the Transformer Architecture. 2025. 2025arXiv preprint</p>
<p>Get ready for a party. Evan King, Haoxiang Yu, Sangsu Lee, Christine Julien, arXiv-2303Exploring smarter smart spaces with help from large language models. 2023. 2023arXiv e-prints</p>
<p>Segment Anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Abc: A big cad model dataset for geometric deep learning. Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>ReparamCAD: Zero-shot CAD Re-Parameterization for Interactive Manipulation. Milin Kodnongbua, Benjamin Jones, Maaz Bin Safeer, Vladimir Ahmad, Adriana Kim, Schulz, SIGGRAPH Asia 2023 Conference Papers. 2023</p>
<p>Generating images with multimodal language models. Jing Yu Koh, Daniel Fried, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Imagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. 252012. 2012</p>
<p>Large language models (LLMs): survey, technical frameworks, and future challenges. Pranjal Kumar, Artificial Intelligence Review. 572602024. 2024</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Bloom: A 176b-parameter open-access multilingual language model. 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in neural information processing systems. 332020. 2020</p>
<p>Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li, arXiv:2408.03326[cs.CV]LLaVA-OneVision: Easy Visual Task Transfer. 2024</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. 2023. 19730-19742</p>
<p>Pre-trained language models for text generation: A survey. Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, Comput. Surveys. 562024. 2024</p>
<p>Conditional embedding pre-training language model for image captioning. Pengfei Li, Min Zhang, Peijie Lin, Jian Wan, Ming Jiang, Neural Processing Letters. 542022. 2022</p>
<p>Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, StarCoder: May the Source be With You! Transactions on machine learning research. 2023. 2023</p>
<p>CAD Translator: An Effective Drive for Text to 3D Parametric Computer-Aided Design Generative Modeling. Xueyang Li, Yu Song, Yunzhong Lou, Xiangdong Zhou, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>LLM4CAD: Multi-Modal Large Language Models for 3D Computer-Aided Design Generation. Xingang Li, Yuewan Sun, Zhenghui Sha, International Design Engineering Technical Conferences and Computers and Information in Engineering Conference. 202488407</p>
<p>LLM4CAD: Multimodal Large Language Models for Three-Dimensional Computer-Aided Design Generation. Xingang Li, Yuewan Sun, Zhenghui Sha, Journal of Computing and Information Science in Engineering. 252025. 2025</p>
<p>Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, arXiv:2309.03852Flm-101b: An open llm and how to train it with $100 k budget. 2023. 2023arXiv preprint</p>
<p>. Yiwei Li, Huaqin Zhao, Hanqi Jiang, Yi Pan, Zhengliang Liu, Zihao Wu, Peng Shu, Jie Tian, Tianze Yang, Shaochen Xu, arXiv-24102024et al. 2024. Large Language Models for Manufacturing. arXiv e-prints</p>
<p>Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, arXiv:2403.19887Jamba: A hybrid transformer-mamba language model. 2024. 2024arXiv preprint</p>
<p>Evaluating text-to-visual generation with image-to-text generation. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, Deva Ramanan, European Conference on Computer Vision. 2024</p>
<p>FlairGPT: Repurposing LLMs for interior designs. Gabrielle Littlefair, Niladri Shekhar Dutt, Niloy J Mitra, EUROGRAPHICS. 2025</p>
<p>Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, arXiv:2405.04434Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. 2024. 2024arXiv preprint</p>
<p>. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv-24122024et al. 2024. DeepSeek-V3 Technical Report. arXiv e-prints</p>
<p>Improved Baselines with Visual Instruction Tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee, LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. 2024</p>
<p>Visual Instruction Tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>3DALL-E: Integrating text-to-image AI in 3D design workflows. Vivian Liu, Jo Vermeulen, George Fitzmaurice, Justin Matejka, Proceedings of the 2023 ACM designing interactive systems conference. the 2023 ACM designing interactive systems conference2023</p>
<p>Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, arXiv:2403.05525DeepSeek-VL: Towards Real-World Vision-Language Understanding. 2024. 2024arXiv preprint</p>
<p>Language Models of Code are Few-Shot Commonsense Learners. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>. Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, Christian Laforte, </p>
<p>Stable Beluga models. </p>
<p>How Can Large Language Models Help Humans in Design and Manufacturing?. Liane Makatura, Michael Foshey, Bohan Wang, Felix Hähnlein, Pingchuan Ma, Bolei Deng, Megan Tjandrasuwita, Andrew Spielberg, Crystal Elaine Owens, Peter Yichen Chen, 2023. 2023CoRRManuscript submitted to ACM</p>
<p>Large Language Models for Design and Manufacturing. An MIT Exploration of Generative AI. Liane Makatura, Michael Foshey, Bohan Wang, Felix Hähnlein, Pingchuan Ma, Bolei Deng, Megan Tjandrasuwita, Andrew Spielberg, Crystal Elaine Owens, Peter Yichen Chen, 2024. mar 27 2024</p>
<p>SHARP Challenge 2023: Solving CAD History and pArameters Recovery from Point Clouds and 3D Scans. Overview, Datasets, Metrics, and Baselines. Dimitrios Mallis, Ali Sk Aziz, Elona Dupont, Kseniya Cherenkova, Ahmet Serdar Karadeniz, Mohammad Sadil Khan, Anis Kacem, Gleb Gusev, Djamila Aouada, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops. the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops2023</p>
<p>Dimitrios Mallis, Ahmet Serdar Karadeniz, Sebastian Cavada, Danila Rukhovich, Niki Foteinopoulou, Kseniya Cherenkova, arXiv-2412Anis Kacem, and Djamila Aouada. 2024. CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers? arXiv e-prints. 2024</p>
<p>Sam Mccandlish, Jared Kaplan, Dario Amodei, Openai Dota, Team , arXiv:1812.06162An empirical model of large-batch training. 2018. 2018arXiv preprint</p>
<p>Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, arXiv:2404.14619Openelm: An efficient language model family with open training and inference framework. 2024. 2024arXiv preprint</p>
<p>Numerical product design: Springback prediction, compensation and optimization. Timo Meinders, Burchitz, Mha Bonte, Lingbeek, International Journal of Machine Tools and Manufacture. 482008. 2008</p>
<p>The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation. A I Meta, 2025</p>
<p>Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.06196[cs.CL]Large Language Models: A Survey. 2024</p>
<p>Crosslingual Generalization through Multitask Finetuning. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, arXiv:2306.02707Orca: Progressive learning from complex explanation traces of gpt-4. 2023. 2023arXiv preprint</p>
<p>Artificial Intelligence and Large Language Models in CAD. Atharva Naik, 2024</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browser-assisted question-answering with human feedback. 2021. 2021arXiv preprint</p>
<p>Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian, arXiv:2307.06435A comprehensive overview of large language models. 2023. 2023arXiv preprint</p>
<p>Llms for science: Usage for code generation and data analysis. Mohamed Nejjar, Luca Zacharias, Fabian Stiehle, Ingo Weber, Journal of Software: Evolution and Process. 37e27232025. 2025</p>
<p>Utilizing ChatGPT to assist CAD design for microfluidic devices. Brady L Matt D Nelson, Bruce K Goenner, Gale, Lab on a Chip. 232023. 2023</p>
<p>Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou, arXiv-2305CodeGen2: Lessons for Training LLMs on Programming and Natural Languages. 2023. 2023arXiv e-prints</p>
<p>Felix Ocker, Stefan Menzel, Ahmed Sadik, Thiago Rios, arXiv:2503.04417From Idea to CAD: A Language Model-Driven Multi-Agent System for Collaborative Design. 2025. 2025arXiv preprint</p>
<p>GPT-4V(ision) System Card. 2023OpenAI</p>
<p>OpenAI GPT-4.5 System Card. 2025OpenAI</p>
<p>Openai o3-mini system card. OpenAI blog. 2025. 2025OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 352022. 2022</p>
<p>Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, Siddartha Naidu, arXiv:2308.10882Giraffe: Adventures in expanding context lengths in llms. 2023. 2023arXiv preprint</p>
<p>Gorilla: Large Language Model Connected with Massive APIs. Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>True few-shot learning with language models. Ethan Perez, Douwe Kiela, Kyunghyun Cho, Advances in neural information processing systems. 342021. 2021</p>
<p>Md Ferdous Alam, and Faez Ahmed. Cyril Picard, Kristen M Edwards, Anna C Doris, Brandon Man, Giorgio Giannone, arXiv:2311.12668From concept to manufacturing: Evaluating vision-language models for engineering design. 2023. 2023arXiv preprint</p>
<p>Introducing Gemini 2.0: our new AI model for the agentic era. Sundar Pichai, Hassabis, Kavukcuoglu, 2024. 2024Google Blog</p>
<p>Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z Xiao, Katherine M Collins, Joshua B Tenenbaum, Adrian Weller, Michael J Black, Bernhard Schölkopf, arXiv:2408.08313Can Large Language Models Understand Symbolic Graphics Programs?. 2024. 2024arXiv preprint</p>
<p>Will Model-based Definition replace engineering drawings throughout the product lifecycle? A global perspective from aerospace industry. Virgilio Quintana, Louis Rivest, Robert Pellerin, Frédérick Venne, Fawzi Kheddouci, Computers in industry. 612010. 2010</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. 2021</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 192019. 2019</p>
<p>Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv-2204Hierarchical Text-Conditional Image Generation with CLIP Latents. 2022. 2022arXiv e-prints</p>
<p>Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, arXiv:2303.10845PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing. 2023. 2023arXiv preprint</p>
<p>Recipes for Building an Open-Domain Chatbot. Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain2021</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Romain Liu, Tal Sauvestre, Remez, arXiv-2308Code Llama: Open Foundation Models for Code. 2023. 2023arXiv e-prints</p>
<p>Danila Rukhovich, Elona Dupont, Dimitrios Mallis, Kseniya Cherenkova, arXiv-2412Anis Kacem, and Djamila Aouada. 2024. CAD-Recode: Reverse Engineering CAD Code from Point Clouds. 2024arXiv e-prints</p>
<p>GPT models in construction industry: Opportunities, limitations, and a use case validation. Abdullahi Saka, Ridwan Taiwo, Nurudeen Saka, Abiodun Babatunde, Saheed Salami, Kabiru Ajayi, Hadi Akande, Kazemi, Developments in the Built Environment. 171003002024. 2024</p>
<p>Mallikarjuna Mmm Sarcar, K Lalit Rao, Narayan, Computer aided design and manufacturing. 2008</p>
<p>. John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron, Liam Uribe, Luke Fedus, Michael Metz, Pokorny, Introducing ChatGPT. OpenAI blog. 2022. 2022</p>
<p>Ari Seff, Yaniv Ovadia, Wenda Zhou, Ryan P Adams, arXiv:2007.08506Sketchgraphs: A large-scale dataset for modeling relational geometry in computeraided design. 2020. 2020arXiv preprint</p>
<p>Introduction to ChatGPT. Wasswa Shafik, Advanced applications of generative AI and natural language processing models. 2024</p>
<p>Evaluating the effectiveness of gpt-4 turbo in creating defeaters for assurance cases. Khakzad Kimya, Mithila Shahandashti, Mohammad Sivakumar, Alvine B Mahdi Mohajer, Song Belle, Timothy C Wang, Lethbridge, arXiv:2401.179912024. 2024arXiv preprint</p>
<p>Reinforcement learning algorithms: A brief survey. Ashish Kumar Shakya, Gopinatha Pillai, Sohom Chakrabarty, Expert Systems with Applications. 2311204952023. 2023</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Li, Daya Wu, Guo, arXiv-2402DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. 2024. 2024arXiv e-prints</p>
<p>Noam Shazeer, arXiv:2002.05202Glu variants improve transformer. 2020. 2020arXiv preprint</p>
<p>Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong, arXiv:2309.15025Large language model alignment: A survey. 2023. 2023arXiv preprint</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 6202023. 2023</p>
<p>Toward expert-level medical question answering with large language models. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Heather Stephen R Pfohl, Cole-Lewis, Nature Medicine. 2025. 2025</p>
<p>PandaGPT: One Model To Instruction-Follow Them All. Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, Deng Cai, Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants. the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants2023</p>
<p>Large Language Models for Computer-Aided Design (LLM4CAD) Fine-Tuned: Dataset and Experiments. Yuewan Sun, Xingang Li, Zhenghui Sha, Journal of Mechanical Design. 2025. 2025</p>
<p>Ai large models bring great opportunities to reusable design of cad software. Yunlei Sun, Bingyi Yan, Zhaotong Shao, Computer Science and Information Systems. 002024. 2024</p>
<p>ChatCAD: An MLLM-Guided Framework for Zero-shot CAD Drawing Restoration. Jing Tang, Hongru Xiao, Xiang Li, Wei Wang, Zeyu Gong, ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2025</p>
<p>Alpaca: A strong, replicable instruction-following model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford Center for Research on Foundation Models. 2023. 202337</p>
<p>Transcending Scaling Laws with 0.1% Extra Compute. Yi Tay, Jason Wei, Hyung Chung, Vinh Tran, David So, Siamak Shakeri, Xavier Garcia, Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023Manuscript submitted to ACM</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023. 2023arXiv preprint</p>
<p>Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.05530Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 2024. 2024arXiv preprint</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024. 2024arXiv preprint</p>
<p>. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv-24082024et al. 2024. Gemma 2: Improving Open Language Models at a Practical Size. arXiv e-prints</p>
<p>Nlp Mosaicml, Team, Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs. 2023</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022. 2022arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023. 2023arXiv preprint</p>
<p>Zephyr: Direct Distillation of LM Alignment. Lewis Tunstall, Edward Emanuel Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, First Conference on Language Modeling. 2023</p>
<p>Focused transformer: Contrastive training for context scaling. Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś, Advances in neural information processing systems. 362023. 2023</p>
<p>Efficient diffusion models for vision: A survey. Anwaar Ulhaq, Naveed Akhtar, arXiv:2210.092922022. 2022arXiv preprint</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 2017. 201730</p>
<p>Introduction to ChatGPT. Charles Waghmare, Unleashing The Power of ChatGPT: A Real World Business Applications. 2023</p>
<p>Boosting Language Models Reasoning with Chain-of-Knowledge Prompting. Jianing Wang, Qiushi Sun, Xiang Li, Ming Gao, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Text-to-CAD Generation Through Infusing Visual Feedback in Large Language Models. Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian, arXiv:2501.190542025. 2025arXiv preprint</p>
<p>CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs. Siyu Wang, Cailian Chen, Xinyi Le, Qimin Xu, Lei Xu, Yanzhou Zhang, Jie Yang, AAAI. 2025</p>
<p>From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach. Xilin Wang, Jia Zheng, Yuanchao Hu, Hao Zhu, Qian Yu, Zihan Zhou, AAAI. 2025</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey Macmillan, Noah A Smith, Iz Beltagy, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Self-Instruct: Aligning Language Models with Self-Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>CodeT5+: Open Code Large Language Models for Code Understanding and Generation. Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui, Junnan Li, Steven Hoi, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, arXiv-2310Skywork: A More Open Bilingual Foundation Model. 2023. 2023arXiv e-prints</p>
<p>. Philip Welsby, M Y Bernard, Cheung, 2023</p>
<p>Fusion 360 gallery: A dataset and environment for programmatic cad construction from human design sequences. Karl Dd Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Armando Joseph G Lambourne, Wojciech Solar-Lezama, Matusik, ACM Transactions on Graphics (TOG). 402021. 2021</p>
<p>AI-driven computational creativity in fashion design: a review. Jennifer Xiaopei, Wu , Li Li, Textile Research Journal. 952025. 2025</p>
<p>Deepcad: A deep generative network for computer-aided design models. Rundi Wu, Chang Xiao, Changxi Zheng, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann, arXiv-2303BloombergGPT: A Large Language Model for Finance. 2023. 2023arXiv e-prints</p>
<p>Cad-llm: Large language model for cad generation. Sifan Wu, Amir Khasahmadi, Mor Katz, Pradeep Kumar Jayaraman, Yewen Pu, Karl Willis, Bang Liu, NeurIPS 2023 Workshop on Machine Learning for Creativity and Design. 2023</p>
<p>Cadvlm: Bridging language and vision in the generation of parametric cad sketches. Sifan Wu, Mor Amir Hosein Khasahmadi, Pradeep Katz, Yewen Kumar Jayaraman, Karl Pu, Bang Willis, Liu, European Conference on Computer Vision. Springer2024</p>
<p>DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models. Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Large language model and text generation. Yonghui Wu, Natural Language Processing in Biomedicine: A Practical Guide. 2024</p>
<p>Tong Xiao, Jingbo Zhu, arXiv:2501.09223Foundations of Large Language Models. 2025. 2025arXiv preprint</p>
<p>On layer normalization in the transformer architecture. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tieyan Liu, International conference on machine learning. 2020</p>
<p>WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, Daxin Jiang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>A systematic evaluation of large language models of code. Uri Frank F Xu, Graham Alon, Vincent Neubig, Josua Hellendoorn, Proceedings of the 6th ACM SIGPLAN international symposium on machine programming. the 6th ACM SIGPLAN international symposium on machine programming2022</p>
<p>Jingwei Xu, Zibo Zhao, Chenyu Wang, Wen Liu, Yi Ma, Shenghua Gao, arXiv:2411.04954[cs.CV]CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM. 2025</p>
<p>SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks. Xiang Xu, Karl Dd Willis, Joseph G Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, Yasutaka Furukawa, International Conference on Machine Learning. 2022</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Yan, arXiv:2309.10305Open large-scale language models. 2023. 20232arXiv preprint</p>
<p>. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, arXiv:2407.10671[cs.CL]2024Qwen2 Technical Report</p>
<p>Arithmetic Reasoning with LLM: Prolog Generation &amp; Permutation. Xiaocheng Yang, Bingsen Chen, Yik-Cheung Tam, Proceedings of the 2024 Conference of the North American Chapter. the 2024 Conference of the North American ChapterShort Papers20242</p>
<p>A survey on multimodal large language models. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen, National science review. 11e4032024. 2024</p>
<p>From Automatic to Autonomous: A Large Language Model-driven Approach for Generic Building Compliance Checking. Huaquan Ying, Rafael Sacks, Proceedings of the CIB W78 Conference. the CIB W78 Conference2024</p>
<p>Img2cad: Reverse engineering 3d cad models from images through vlm-assisted conditional factorization. Yang You, Mikaela Angelina Uy, Jiaqi Han, Rahul Thomas, Haotong Zhang, Suya You, Leonidas Guibas, arXiv:2408.014372024. 2024arXiv preprint</p>
<p>CoCa: Contrastive Captioners are Image-Text Foundation Models. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu, Transactions on Machine Learning Research. 2022. Aug 2022. 2022</p>
<p>An intelligent interactive system based on LLM to combine CAD API development with FEA. Mengqiang Yu, Yujie Sheng, Zhongxu Wan, Shuyu Yang, Muping Sun, Hongxia Cai, 2024 8th International Conference on Electrical, Mechanical and Computer Engineering (ICEMCE). 2024</p>
<p>CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD Programs. Haocheng Yuan, Jing Xu, Hao Pan, Adrien Bousseau, Niloy J Mitra, Changjian Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>OpenECAD: An efficient visual language model for editable 3D-CAD design. Zhe Yuan, Jianqi Shi, Yanhong Huang, Computers &amp; Graphics. 1241040482024. 2024</p>
<p>Adding conditional control to text-to-image diffusion models. Lvmin Zhang, Anyi Rao, Maneesh Agrawala, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters. Xuanyu Zhang, Qing Yang, Proceedings of the 32nd ACM international conference on information and knowledge management. the 32nd ACM international conference on information and knowledge management2023</p>
<p>FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned Large Language Models. Zhanwei Zhang, Shizhao Sun, Wenxiao Wang, Deng Cai, Jiang Bian, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 20231arXiv preprint</p>
<p>Dynamic prompt-based virtual assistant framework for BIM information search. Junwen Zheng, Martin Fischer, Automation in Construction. 1551050672023. 2023</p>
<p>Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, Lei Huang, arXiv:2402.14289[cs.LG]TinyLLaVA: A Framework of Small-scale Large Multimodal Models. 2024</p>
<p>Lima: Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, ICLR 202412th International Conference on Learning Representations. 2024Manuscript submitted to ACM</p>            </div>
        </div>

    </div>
</body>
</html>
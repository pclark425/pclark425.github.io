<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3445 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3445</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3445</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-80.html">extraction-schema-80</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <p><strong>Paper ID:</strong> paper-250451544</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2207.05561v1.pdf" target="_blank">Brain-inspired Graph Spiking Neural Networks for Commonsense Knowledge Representation and Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> How neural networks in the human brain represent commonsense knowledge, and complete related reasoning tasks is an important research topic in neuroscience, cognitive science, psychology, and artificial intelligence. Although the traditional artificial neural network using fixed-length vectors to represent symbols has gained good performance in some specific tasks, it is still a black box that lacks interpretability, far from how humans perceive the world. Inspired by the grandmother-cell hypothesis in neuroscience, this work investigates how population encoding and spiking timing-dependent plasticity (STDP) mechanisms can be integrated into the learning of spiking neural networks, and how a population of neurons can represent a symbol via guiding the completion of sequential firing between different neuron populations. The neuron populations of different communities together constitute the entire commonsense knowledge graph, forming a giant graph spiking neural network. Moreover, we introduced the Reward-modulated spiking timing-dependent plasticity (R-STDP) mechanism to simulate the biological reinforcement learning process and completed the related reasoning tasks accordingly, achieving comparable accuracy and faster convergence speed than the graph convolutional artificial neural networks. For the fields of neuroscience and cognitive science, the work in this paper provided the foundation of computational modeling for further exploration of the way the human brain represents commonsense knowledge. For the field of artificial intelligence, this paper indicated the exploration direction for realizing a more robust and interpretable neural network by constructing a commonsense knowledge representation and reasoning spiking neural networks with solid biological plausibility.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3445.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3445.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grandmother-cell / Concept cells</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grandmother-cell hypothesis / Concept cells (sparse invariant neurons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hypothesis that single neurons or very small, highly selective populations (concept cells) respond invariantly to particular high-level concepts (e.g., a specific person or place), providing a sparse symbolic-like code for concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant visual representation by single neurons in the human brain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>grandmother-cell hypothesis / concept cells</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented by highly selective neurons (or very small populations) that fire invariantly for instances of a particular concept; the code is sparse and nearly 'symbolic' at the single-cell or micro-population level.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Single-neuron recordings in human MTL showing neurons that respond selectively and invariantly to different pictures/representations of the same person/place (Quiroga et al. cited in the paper), and related neurophysiological observations of sparse 'concept cells'.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Sparse single-cell coding raises robustness questions (how tolerant to cell loss/variation?), and the paper notes ongoing debate and the need to reconcile sparse findings with population-level coding; the model authors treat the grandmother-cell idea as inspiration rather than exclusive account.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>The paper contrasts the grandmother-cell idea with distributed/population coding approaches and positions it as an inspiration for representing a concept by a neuron population (memory engram) rather than a single literal grandmother neuron; it also contrasts such symbolic-like coding with dense fixed-length vector encodings used in conventional ANNs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How sparse concept cells relate to broader semantic structure (maps/gradients) and to inductive generalization remains open; the paper acknowledges that concept-cell findings motivate population-based engrams but do not fully explain spatial organization or multimodal integration of concepts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3445.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3445.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Population coding / Memory engrams</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Population coding implemented as sparse memory engrams</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional model where each concept or relation is represented by a sparse population pattern (an engram) across many neurons, and associative structure is encoded in synaptic connections between these populations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Geometry of sequence working memory in macaque prefrontal cortex</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>population coding / memory engram representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts and relations are represented as sparse binary patterns over a population (memory engram); retrieval corresponds to reactivation of the pattern and similarity between current firing and stored engrams indexes concept activation.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Paper cites neuroscience results showing population-level representations (including macaque studies) and human MTL concept-cell literature; in this work the authors implement population-coded engrams in a spiking graph SNN and demonstrate recall and inductive reasoning (e.g., triple completion and abstraction experiments) as behavioral support for feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>The authors note limits: their implementation stores engrams by index (no modeled cortical spatial layout), lacks multimodal/embodied grounding, and may not scale to the full richness of human commonsense; biological plausibility is improved but not complete.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Presented as biologically-plausible alternative to fixed-length vector symbolic encodings in standard ANNs; the model operationalizes grandmother-cell inspiration by using population patterns rather than single neurons, and is directly compared (empirically) to graph convolutional networks (GCN) on convergence speed and comparable accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Open questions include how these engrams map onto cortical semantic maps/gradients, how multimodal sensory information is integrated into engrams, and how spatial organization and developmental processes produce the observed inhibitory/excitatory ratios and representational geometry.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3445.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3445.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic map / cortical abstraction gradient</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic maps and abstraction gradients in cortex</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observation that semantic categories tile the cortex and that there exists a gradient from modality-specific visual representations to abstract linguistic representations across cortical space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Natural speech reveals the semantic maps that tile human cerebral cortex</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>semantic map / gradient of abstraction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual knowledge is spatially organized in cortex: nearby cortical regions represent related semantic content and there is a continuous gradient from sensory (visual) to abstract (linguistic/conceptual) representations.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>fMRI studies (Huth et al. 2016; Popham et al. 2021 cited in the paper) showing cortical maps of semantic categories and alignment of visual and linguistic semantic representations at cortical borders.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>The authors point out that their model currently does not implement spatial placement of engram populations (they use indexed storage), missing an important feature of cortical semantic organization; integrating KRR-GSNN representations with cortical maps is listed as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Serves as a macroscopic organizational account complementary to microscopic population coding/concept-cell descriptions; the paper suggests bridging population/engram models and cortical semantic maps is a key next step.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How the semantic map emerges from local synaptic plasticity and population engrams, and how spatial layout contributes functionally to reasoning and generalization, remain open questions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3445.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3445.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic vs connectionist representations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic (symbol-manipulation) versus connectionist / distributed representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual contrast between symbolic representations (discrete symbols manipulated by rules) and connectionist distributed representations (patterns across units); the paper situates its approach as combining symbolic-structure (triples) with biologically plausible spiking population codes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>symbolic versus connectionist hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Symbolic knowledge (e.g., triples A→R→B) can be implemented within a connectionist substrate by assigning population-coded engrams to symbols and using synaptic connectivity (learned by STDP/R-STDP) to encode relations and support symbolic-like inference.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Authors demonstrate a working hybrid model (KRR-GSNN) that encodes ConceptNet triples as spiking population nodes and uses STDP/R-STDP to learn edges and transitivity, showing faster convergence than GCN and interpretable reasoning traces via the similarity function.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>The approach still lacks embodied multimodal grounding and does not fully capture the richness of human symbol use; authors acknowledge remaining gap between their symbolic-level spiking model and human-level commonsense reasoning complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Directly contrasted with conventional ANNs that use fixed-length vector symbol encodings (treated as black boxes), and empirically compared to graph convolutional networks (GCN) on reasoning tasks (KRR-GSNN converges faster though final accuracy is slightly lower).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to integrate multimodal sensory experience into the hybrid, how to scale the approach to richer forms of reasoning, and how developmental/embodied factors shape symbolic-connectionist mappings remain open.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Invariant visual representation by single neurons in the human brain <em>(Rating: 2)</em></li>
                <li>Concept cells: the building blocks of declarative memory functions <em>(Rating: 2)</em></li>
                <li>Natural speech reveals the semantic maps that tile human cerebral cortex <em>(Rating: 2)</em></li>
                <li>Visual and linguistic semantic representations are aligned at the border of human visual cortex <em>(Rating: 2)</em></li>
                <li>When shared concept cells support associations: theory of overlapping memory engrams <em>(Rating: 2)</em></li>
                <li>Geometry of abstract learned knowledge in the hippocampus <em>(Rating: 1)</em></li>
                <li>Geometry of sequence working memory in macaque prefrontal cortex <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3445",
    "paper_id": "paper-250451544",
    "extraction_schema_id": "extraction-schema-80",
    "extracted_data": [
        {
            "name_short": "Grandmother-cell / Concept cells",
            "name_full": "Grandmother-cell hypothesis / Concept cells (sparse invariant neurons)",
            "brief_description": "A hypothesis that single neurons or very small, highly selective populations (concept cells) respond invariantly to particular high-level concepts (e.g., a specific person or place), providing a sparse symbolic-like code for concepts.",
            "citation_title": "Invariant visual representation by single neurons in the human brain",
            "mention_or_use": "mention",
            "theory_name": "grandmother-cell hypothesis / concept cells",
            "theory_description": "Concepts are represented by highly selective neurons (or very small populations) that fire invariantly for instances of a particular concept; the code is sparse and nearly 'symbolic' at the single-cell or micro-population level.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Single-neuron recordings in human MTL showing neurons that respond selectively and invariantly to different pictures/representations of the same person/place (Quiroga et al. cited in the paper), and related neurophysiological observations of sparse 'concept cells'.",
            "counter_evidence_or_challenges": "Sparse single-cell coding raises robustness questions (how tolerant to cell loss/variation?), and the paper notes ongoing debate and the need to reconcile sparse findings with population-level coding; the model authors treat the grandmother-cell idea as inspiration rather than exclusive account.",
            "comparison_to_other_theories": "The paper contrasts the grandmother-cell idea with distributed/population coding approaches and positions it as an inspiration for representing a concept by a neuron population (memory engram) rather than a single literal grandmother neuron; it also contrasts such symbolic-like coding with dense fixed-length vector encodings used in conventional ANNs.",
            "notable_limitations_or_open_questions": "How sparse concept cells relate to broader semantic structure (maps/gradients) and to inductive generalization remains open; the paper acknowledges that concept-cell findings motivate population-based engrams but do not fully explain spatial organization or multimodal integration of concepts.",
            "uuid": "e3445.0"
        },
        {
            "name_short": "Population coding / Memory engrams",
            "name_full": "Population coding implemented as sparse memory engrams",
            "brief_description": "A functional model where each concept or relation is represented by a sparse population pattern (an engram) across many neurons, and associative structure is encoded in synaptic connections between these populations.",
            "citation_title": "Geometry of sequence working memory in macaque prefrontal cortex",
            "mention_or_use": "use",
            "theory_name": "population coding / memory engram representation",
            "theory_description": "Concepts and relations are represented as sparse binary patterns over a population (memory engram); retrieval corresponds to reactivation of the pattern and similarity between current firing and stored engrams indexes concept activation.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Paper cites neuroscience results showing population-level representations (including macaque studies) and human MTL concept-cell literature; in this work the authors implement population-coded engrams in a spiking graph SNN and demonstrate recall and inductive reasoning (e.g., triple completion and abstraction experiments) as behavioral support for feasibility.",
            "counter_evidence_or_challenges": "The authors note limits: their implementation stores engrams by index (no modeled cortical spatial layout), lacks multimodal/embodied grounding, and may not scale to the full richness of human commonsense; biological plausibility is improved but not complete.",
            "comparison_to_other_theories": "Presented as biologically-plausible alternative to fixed-length vector symbolic encodings in standard ANNs; the model operationalizes grandmother-cell inspiration by using population patterns rather than single neurons, and is directly compared (empirically) to graph convolutional networks (GCN) on convergence speed and comparable accuracy.",
            "notable_limitations_or_open_questions": "Open questions include how these engrams map onto cortical semantic maps/gradients, how multimodal sensory information is integrated into engrams, and how spatial organization and developmental processes produce the observed inhibitory/excitatory ratios and representational geometry.",
            "uuid": "e3445.1"
        },
        {
            "name_short": "Semantic map / cortical abstraction gradient",
            "name_full": "Semantic maps and abstraction gradients in cortex",
            "brief_description": "Empirical observation that semantic categories tile the cortex and that there exists a gradient from modality-specific visual representations to abstract linguistic representations across cortical space.",
            "citation_title": "Natural speech reveals the semantic maps that tile human cerebral cortex",
            "mention_or_use": "mention",
            "theory_name": "semantic map / gradient of abstraction",
            "theory_description": "Conceptual knowledge is spatially organized in cortex: nearby cortical regions represent related semantic content and there is a continuous gradient from sensory (visual) to abstract (linguistic/conceptual) representations.",
            "level_of_analysis": "functional",
            "supporting_evidence": "fMRI studies (Huth et al. 2016; Popham et al. 2021 cited in the paper) showing cortical maps of semantic categories and alignment of visual and linguistic semantic representations at cortical borders.",
            "counter_evidence_or_challenges": "The authors point out that their model currently does not implement spatial placement of engram populations (they use indexed storage), missing an important feature of cortical semantic organization; integrating KRR-GSNN representations with cortical maps is listed as future work.",
            "comparison_to_other_theories": "Serves as a macroscopic organizational account complementary to microscopic population coding/concept-cell descriptions; the paper suggests bridging population/engram models and cortical semantic maps is a key next step.",
            "notable_limitations_or_open_questions": "How the semantic map emerges from local synaptic plasticity and population engrams, and how spatial layout contributes functionally to reasoning and generalization, remain open questions.",
            "uuid": "e3445.2"
        },
        {
            "name_short": "Symbolic vs connectionist representations",
            "name_full": "Symbolic (symbol-manipulation) versus connectionist / distributed representations",
            "brief_description": "A conceptual contrast between symbolic representations (discrete symbols manipulated by rules) and connectionist distributed representations (patterns across units); the paper situates its approach as combining symbolic-structure (triples) with biologically plausible spiking population codes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_name": "symbolic versus connectionist hybrid",
            "theory_description": "Symbolic knowledge (e.g., triples A→R→B) can be implemented within a connectionist substrate by assigning population-coded engrams to symbols and using synaptic connectivity (learned by STDP/R-STDP) to encode relations and support symbolic-like inference.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Authors demonstrate a working hybrid model (KRR-GSNN) that encodes ConceptNet triples as spiking population nodes and uses STDP/R-STDP to learn edges and transitivity, showing faster convergence than GCN and interpretable reasoning traces via the similarity function.",
            "counter_evidence_or_challenges": "The approach still lacks embodied multimodal grounding and does not fully capture the richness of human symbol use; authors acknowledge remaining gap between their symbolic-level spiking model and human-level commonsense reasoning complexity.",
            "comparison_to_other_theories": "Directly contrasted with conventional ANNs that use fixed-length vector symbol encodings (treated as black boxes), and empirically compared to graph convolutional networks (GCN) on reasoning tasks (KRR-GSNN converges faster though final accuracy is slightly lower).",
            "notable_limitations_or_open_questions": "How to integrate multimodal sensory experience into the hybrid, how to scale the approach to richer forms of reasoning, and how developmental/embodied factors shape symbolic-connectionist mappings remain open.",
            "uuid": "e3445.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Invariant visual representation by single neurons in the human brain",
            "rating": 2,
            "sanitized_title": "invariant_visual_representation_by_single_neurons_in_the_human_brain"
        },
        {
            "paper_title": "Concept cells: the building blocks of declarative memory functions",
            "rating": 2,
            "sanitized_title": "concept_cells_the_building_blocks_of_declarative_memory_functions"
        },
        {
            "paper_title": "Natural speech reveals the semantic maps that tile human cerebral cortex",
            "rating": 2,
            "sanitized_title": "natural_speech_reveals_the_semantic_maps_that_tile_human_cerebral_cortex"
        },
        {
            "paper_title": "Visual and linguistic semantic representations are aligned at the border of human visual cortex",
            "rating": 2,
            "sanitized_title": "visual_and_linguistic_semantic_representations_are_aligned_at_the_border_of_human_visual_cortex"
        },
        {
            "paper_title": "When shared concept cells support associations: theory of overlapping memory engrams",
            "rating": 2,
            "sanitized_title": "when_shared_concept_cells_support_associations_theory_of_overlapping_memory_engrams"
        },
        {
            "paper_title": "Geometry of abstract learned knowledge in the hippocampus",
            "rating": 1,
            "sanitized_title": "geometry_of_abstract_learned_knowledge_in_the_hippocampus"
        },
        {
            "paper_title": "Geometry of sequence working memory in macaque prefrontal cortex",
            "rating": 1,
            "sanitized_title": "geometry_of_sequence_working_memory_in_macaque_prefrontal_cortex"
        }
    ],
    "cost": 0.010502749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Brain-inspired Graph Spiking Neural Networks for Commonsense Knowledge Representation and Reasoning
July 13, 2022 11 Jul 2022</p>
<p>Hongjian Fang es:fanghongjian2017@ia.ac.cnhongjianfang 
Research Center for Brain-inspired Intelligence
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Future Technology
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Yi Zeng yi.zeng@ia.ac.cn 
Research Center for Brain-inspired Intelligence
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>Center for Excellence in Brain Science and Intelligence Technology
Chinese Academy of Sciences
ShanghaiChina</p>
<p>School of Future Technology
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Institute of Automation
National Laboratory of Pattern Recognition
Chinese Academy of Sciences
BeijingChina</p>
<p>Jianbo Tang 
Research Center for Brain-inspired Intelligence
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>Yuwei Wang 
Research Center for Brain-inspired Intelligence
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>Yao Liang 
Research Center for Brain-inspired Intelligence
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>Xin Liu 
Research Center for Brain-inspired Intelligence
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>Yi Zeng 
Brain-inspired Graph Spiking Neural Networks for Commonsense Knowledge Representation and Reasoning
July 13, 2022 11 Jul 20225 These authors contributed equally to this work. 6 Corresponding Author. Preprint submitted to Journal of L A T E X Templates
How neural networks in the human brain represent commonsense knowledge, and complete related reasoning tasks is an important research topic in neuroscience, cognitive science, psychology, and artificial intelligence. Although the traditional artificial neural network using fixed-length vectors to represent symbols has gained good performance in some specific tasks, it is still a black box that lacks interpretability, far from how humans perceive the world. Inspired by the grandmother-cell hypothesis in neuroscience, this work investigates how population encoding and spiking timing-dependent plasticity (STDP) mechanisms can be integrated into the learning of spiking neural networks, and how a population of neurons can represent a symbol via guiding the completion of sequential firing between different neuron populations. The neuron populations of different communities together constitute the entire commonsense knowledge graph, forming a giant graph spiking neural network. Moreover, we introduced the Reward-modulated spiking timing-dependent plasticity (R-STDP) mechanism to simulate the biological reinforcement learning process and completed the related reasoning tasks accordingly, achieving comparable accuracy and faster convergence speed than the graph convolutional artificial neural networks. From a neuroscience perspective, the work in this paper provided the foundation of computational modeling for further exploration of the way the human brain represents commonsense knowledge. For the field of artificial intelligence, this paper indicated the exploration direction for realizing a more robust and interpretable neural network by constructing a commonsense knowledge representation and reasoning spiking neural networks with solid biological plausibility.</p>
<p>INTRODUCTION</p>
<p>Commonsense Knowledge representation and reasoning are considered as a key to achieving human-level AI by artificial intelligence researchers [1,2].</p>
<p>After the importance of commonsense knowledge for understanding the physical world was pointed out by Marvin Minsky and John McCarthy, many projects are launched to solve the problem of the absence of commonsense knowledge in the agent of Artificial Intelligence, including Cyc [3], ConceptNet [4], Machine</p>
<p>Common Sense (MCS) [5].</p>
<p>With the rise of deep learning methods in recent years, a batch of research on common sense knowledge representation and reasoning based on artificial neural networks(ANN) has emerged. Kocijan et. [6] improved the performance of for Winograd schema challenge, which is a popular benchmark for natural commonsense reasoning, by fine-tuning the pre-trained Bidirectional Encoder</p>
<p>Representations from Transformers (BERT) Language Model. Yasunaga et. [7] introduced knowledge graphs utilizing deep graph neural networks for Commonsense Question Answering. However, data-driven ANN models can only show intelligence at the behavioral level by finding correlations between data. Because these models' information processing methods are far from human cognitive processes, their interpretability is impoverished and can often only be regarded as black boxes.</p>
<p>Spiking Neural networks(SNN), considered as the third generation of the neural networks models [8], inspired by the structures and functions of the biological neurons [9], have better biological plausibility. SNN has been used in various field including pattern classification [10], causal reasoning [11], sequence information processing [12] and Decision-making [13,14].</p>
<p>According to the human cerebral cortex in different positions, there are different degrees of response to different words. Huth et al. have drawn a brain semantic map [15] through fMRI research, as figure 1A, forming the knowledge graphs in the brain . According to the latest research by Huth et al. in 2021 [16], there is a gradual and continuous transition between the visual representation of concepts and the abstract representation at the linguistic level in the human cerebral cortex, forming an abstract gradient in the cortex, as figure 1B. The representation of concepts in the brain is higher than the sensory modality and exists independently, and the semantic relationship network between concepts contains commonsense knowledge1C. Meanwhile, in the deep learning field, Graph Neural NetworksGNNhave been widely used in the field of knowledge representation and reasoning for GNN can encode graph structures, and node attributes compactly [17,18,19]. Whereas, few works combine GNN with knowledge representation in the brain.</p>
<p>The grandmother-cell theory is an essential hypothesis about symbolic repre-sentation in neuroscience. Its core argument is that a population of neurons in the brain encodes a symbol or concept [20,21,22]. Recently, neuroscientists discovered that the representation of spatial symbols in the brains of macaque monkeys exists in the form of population coding, further revealing the way symbols are represented in the brains of primates [12,23]. Inspired by these works, we combine the population coding and STDP mechanism in the spiking neural networks so that the networks can complete the encoding and memory of the commonsense knowledge graph. As figure 1D shown, </p>
<p>RESULTS</p>
<p>Population Coding for Entities and Relations</p>
<p>The triples (A → R → B)containing entities and relationships are the basic building block of the knowledge graph. How SNN represent entities, relationships, and connections between them are fundamental and core issues for both artificial intelligence and neuroscience. At the same time, recently, because GNN has better structured information processing capabilities, some researchers have made remarkable progress on a series of natural languages understanding related issues such as question answering system construction [24] and commonsense knowledge reasoning by combining GNN with knowledge graphs [25]. Then, how to use SNN as the building block of the networks and learn from the working mechanism of the brain and the idea of GNN to achieve better structured information understanding and processing capabilities of the networks has become the core issue we are currently concerned about.</p>
<p>In the human brain, concepts are encoded in the human Medium Temporal Lobe (MTL) by neurons populations called grandmother-cells, that respond selectively and invariantly to stimuli representing a specific person or a specific place. [22,26,27] Recently, neuroscience experiments studies have shown that the representation of spatial symbols in the brains of macaque monkeys exists in the form of population coding, further revealing the way symbols are represented in the brains of primates [12,23].</p>
<p>Inspired by the working principle of the brain, in this work, we propose to use population coding to represent entities and relationships and use the synaptic connections between populations of neurons to represent the edges in the triad. In the field of computational neuroscience, some scholars have studied how to use the population coding mechanism to realize the representation of different concepts and relationships and learn the synaptic connections between different concepts and relationships through synaptic plasticity [28]. We consider In the model of this paper, we introduce the similarity evaluation function Sim(t), i,e, Equation 1, to evaluate the representation of different concepts in the networks at a specific moment.
Sim m (t) = 1 N λ(1 − λ) N i=1 (ϕ m i − λ)σ i (t) (1) σ i (t) = 0 V i (t) &lt; V threshold 1 V i (t) ≥ V threshold(2)
In the Equation 1 and Equation 2, σ(t) and ϕ m represents networks the firing state and memory engram stored patterns, respectively. λ = P rob(ϕ m i = 1), representing that single neuron i has a low probability λ to participate in the population coding of a memory engram m. N is the total number of neurons in  The similarity measures the correlation between the networks firing state σ(t) and the memory engram stored patterns ϕ m . When the memory engram m 1 is retrieved, the neuron population of m 1 will firing strongly, shown in Fig.2A, then Sim m1 ≈ 1, and if the whole networks is at resting state, then Sim m ≈ 0 for every memory engram m.</p>
<p>As for the synapse learning rule, Spike Timing Dependent Plasticity (STDP) [29,30] is one of the most crucial learning principles for the biological brain.</p>
<p>STDP postulates that the strength of the synapse is dependent on the spike timing difference of the presynaptic and postsynaptic neuron [31]. Here we use STDP to update synaptic weights according to the relative time between spikes of presynaptic and postsynaptic neurons. The modulation principle is that if the postsynaptic neuron fires a few milliseconds after the presynaptic neuron, the connection between the neurons will be strengthened; otherwise, the connection will be weakened [32]. Please refer to the method chapter for the specific spiking neuron model, synapse update rule, and corresponding parameters. Based on the method mentioned above to represent triples, we will explore the difference between transitive and non-transitive relationships at the neural circuit level in the graph spiking neural networks fused with common sense knowledge. In the early stage of human cognitive function development, the cognition of the world is continuously improved, mainly in the form of trial and error. Reinforcement learning plays a critical role in this process, including the transitivity of distinguishing relationships [33]. Therefore, we introduced the Reward-Modulated STDP (R-STDP) (document, mine) mechanism inspired by biology as the learning rule of the networks, as shown in Fig.3C. We choose Reward-modulated STDP (R-STDP) to implement the reinforcement learning due to its excellent biology plausibility [34].</p>
<p>In the process of biological brain learning, the basic STDP plasticity mechanism and reward and punishment signals are integrated through the release of dopamine and other neuromodulators to achieve reinforcement learning [34].</p>
<p>Reward-modulated STDP (R-STDP) mechanism in the computational modeling is shown in Fig.3B. The main idea of R-STDP is to modulate the outcome of original STDP by a reward term [35]. Please see the Methods section for details. This mechanism uses the eligibility trace function, combines the STDP law and the neuromodulation mechanism as a reward and punishment signal, and can be regarded as an "STDP mechanism with reward and punishment signal."</p>
<p>Refer to the chapter on methods for specific mathematical operations.</p>
<p>Going back to the question in this chapter, we used the R-STDP mechanism as a training method and combined it with the Population Coding mechanism to complete the judgment of the transitivity of different relationships. At this time, we will ask the networks whether the ARC is established, and if the networks answer correctly, we will give rewards. Otherwise, we will give punishments. Specifically, the inquiry process of the networks is as follows:</p>
<p>We will stimulate the neuron population that characterizes A and R. If R is a transitive relationship when the Smic value of the networks exceeds the threshold, it means that the networks "answers" ARC and the answer is correct. In the actual experiment, we used the English triples in ConceptNet as the experimental data set, including 17 relations, including ten transitive relations, and seven non-transitive relations. Moreover, we clean up all these triples into a graph data set to complete subsequent experiments. Based on 17 different relationships, our entire triplet data set is divided into different sub-graphs.</p>
<p>We give 30% of the triples in the sub-graphs to Mask, thus transforming the entire unsupervised learning process into supervised learning. KRR-GSNN will constantly be "asked" whether specific triples exist and get reward/punishment signals to learn whether a relationship is transitive gradually. We compared the convergence speed of this model and GCN (Graph Convolution networks) during the training process, and the results are shown in Fig.4A. We found that although KRR-GSNN is slightly inferior to GCN in terms of final accuracy, the learning speed of KRR-GSNN is much faster than GCN, thus verifying the effectiveness and efficiency of the model. It is worth mentioning that we compared the effects of different inhibitory neuron ratios in the neuron population on the experimental results. As shown in Fig.4B, we take the results of different ratios after four iterations as an example. According to the results, when the proportion of inhibitory neurons is about 15%, the model's accuracy is the highest. We were surprised to find that this ratio is the same as the CA1 area of the hippocampus, which is responsible for concept learning and new memory form in the human brain [36]. The proportion of inhibitory neurons is almost the same! Furthermore, the "grandmother cell" is found in the hippocampus area of MTL, which we will discuss in detail in the subsequent discussion chapters.</p>
<p>Generating Conceptual Commonsense Knowledge through Entity Conceptualization</p>
<p>Many sentences describe the relations between two objects in the commonsense knowledge bases, which could be expanded by extracting more abstract knowledge. This section will demonstrate spiking networks' ability to generate higher-level abstract knowledge by encoding common sense knowledge into GSNN. We conducted a specific analysis on an example, which is shown in   (a). Biden is the president of America.</p>
<p>(b). Putin is the president of Russia.</p>
<p>(c). Biden is a person.</p>
<p>(d). Putin is a person.</p>
<p>(e). America is a country. (f ). Russia is a country.</p>
<p>Through the above knowledge, a more abstract knowledge can be generated by reasoning:</p>
<p>A person is the president of a country. It should be emphasized that through this experiment, we are not trying to prove that KRR-GSNN has the ability to generate universally correct new knowledge for the entire data set but to explore the internal working mechanism of humans, especially infants, in the process of inductive learning. </p>
<p>Discussion</p>
<p>In the current work, we have completed the representation learning of common sense knowledge and related reasoning tasks based on the pulse graph neural networks. In this process, we used various biologically based brain working mechanisms such as Population Coding, STDP, and R-STDP, which proved the feasibility of using spiking networks to complete commonsense knowledge representation and reasoning. As far as we know, this work is the first work that uses spiking networks for common sense knowledge representation and reasoning, and it is also the first work related to graph spiking neural networks.</p>
<p>Spiking neural networks in commonsense knowledge representation and reasoning has a reference significance in constructing a brain-inspired system of semantic understanding. Computational modeling will help us to understand how to use the essential elements of biological neurons, rules of synaptic learning, and spiking neural networks to deal with common sense knowledge [37]. It provides a computational foundation for exploring how the human brain encodes commonsense knowledge and performing reasoning.</p>
<p>Using neural networks to implement symbolic reasoning has always been an urgent problem in artificial intelligence. The human brain perfectly realizes the use of neural networks for symbolic reasoning. The problem of poor interpretability of artificial neural networks under connectionism in intelligence is essential because the way it processes information is far from the "thinking" process of the human brain. We hope to learn from the brain, based on the structure and working mechanism of related neural circuits in the brain, to construct a structurally and functionally brain-inspired and behaviorally human-like spiking neural networks model for symbolic reasoning.</p>
<p>In recent years, evidence of the "grandmother cell" theory in the field of experimental neuroscience has been continuously discovered [21,23], and only a few works [12] to apply this theory to functional intelligent systems. Inspired by the theory of "grandmother cells" in neuroscience, we propose using a population of neurons to represent a specific concept or relationship, which we call Memory</p>
<p>Engram. In order to better observe the discharge of the networks, we propose the similarity function to show how the networks represent different concepts or relationships at different times. Through the similarity function, we can trace the reasoning path of the spiking networks in the reasoning process, which significantly improves the interpretability of the networks. KRR-GSNN is no longer a black box, and we can observe how the networks complete the inference process, which also avoids the problem of behaviorism. Whether the result is correct is no longer the only indicator to measure the quality of the model. We</p>
<p>can also control the reasoning process of getting the result, which can enhance the robustness of the model. However, from the perspective of artificial intelligence ethics and safety, it is also of great significance to construct artificial intelligence that is more understandable and controllable by humans.</p>
<p>One step further, we represented many commonsense knowledge graphs into KRR-GSNN and constructed specific reasoning tasks for the transitivity of relationships. Utilizing self-supervised reinforcement learning, we trained KRR-GSNN to implement the basic reasoning process and obtained a faster convergence rate than GCN (Graph Convolutional networks). Moreover, by adjusting the proportion of inhibitory neurons in the neuron population that characterizes a Memory Engram, we found that when the proportion of inhibitory neurons is about 15%, the accuracy rate under the same number of iterations is the highest. What surprises us is that the CA1 area of the hippocampus that is responsible for concept learning and new memory formation in the human brain, which is the brain area where "grandmother cells" are found, is precisely 14-15% of inhibitory neurons [36]. Perhaps our process of adjusting the proportion of inhibitory neurons in KRR-GSNN is also in line with the process of constant trial and error in the evolution of the brain. In the end, the ratios are very close, which also shows that our model has solid biological rationality.</p>
<p>After KRR-GSNN completed the coding of many common-sense knowledge artificial intelligence to a certain extent [38], but due to the lack of insight into embodied intelligence [39,40], this directly leads to the complexity of the reasoning KRR-GSNN can complete is still at a low level, and there is still a big gap with the reasoning capability of humans.</p>
<p>shapiro2010embodied,wilson2002six</p>
<p>Furthermore, KRR-GSNN does not take into account the spatial relationship between groups of neurons representing different concepts in the model, but stores them in the form of indexes. The human cerebral cortex has clear regional divisions for the representation of different concepts and knowledge, forming the semantic map [15]. In future work, how to combine the knowledge representation completed by KRR-GSNN with the semantic map in the brain discovered by neuroscientists will be the key issue. </p>
<p>Supplementary Materials</p>
<p>Methods</p>
<p>Neuron Model</p>
<p>The building block of our spiking neural networks is the spike neuron model.</p>
<p>There are various neuron models such as the famous HH model [41], Leaky</p>
<p>Integrate-and-Fire neuron (LIF) model [42], Izhikevich neuron model [43], and so on.  4 and Equation 5. C m is the membrane capacitance of the neuron, V is the membrane potential of the neuron, g is the conductance of the membrane, V s is the steady-state leaky potential, here we let V s = V reset to simplify the model.
C m dV dt = −g(V − V s ) + I (3) τ m dV dt = −(V − V s ) + I g (4) V → V reset , if (V ≥ V threshold ).(5)
I is the input current of the neuron. τ m = Cm g represents the voltage delay time, and different types of neurons have different values of τ m . All the parameters can be found in Table 1 . 
I = j w j,i σ j (t − 1) + I s (6) σ i (t) = 0 V &lt; V threshold 1 V ≥ V threshold(7)Model/Rule Parameter Value LIF model C m 30nF τ m 30ms V reset -65mv V threshold -35mv τ ref 10ms STDP Rule τ s 30ms τ w 20ms A + 1.1 A − 0.95 R-STDP Rule C r 10 C p -10 T R 5ms</p>
<p>Neuroplasticity Rule: STDP and R-STDP</p>
<p>As for the synapse learning rule, Spike Timing Dependent Plasticity (STDP) [29,30] is one of the most crucial learning principles for the biological brain.</p>
<p>STDP postulates that the strength of the synapse is dependent on the spike timing difference of the pre-and post-neuron [31].</p>
<p>Here we use STDP to update synaptic weights according to the relative time between spikes of presynaptic and postsynaptic neurons. The modulation principle is that if the postsynaptic neuron fires a few milliseconds after the presynaptic neuron, the connection between the neurons will be strengthened; otherwise, the connection will be weakened [32]. The update function is shown in Equation 9, where A + and A are learning rates. τ s and τ w are STDP time constant, and ∆t is the delay time from the presynaptic spike to the postsynaptic spike.</p>
<p>∆w j,i = A + e (∆t/τs) −τ w &lt; ∆t &lt; 0 −A − e (−∆t/τs) 0 &lt; ∆t &lt; τ w</p>
<p>Besides STDP, we choose Reward-modulated STDP (R-STDP) to implement the reinforcement learning due to its excellent biology plausibility [34]. In the process of biological brain learning, the basic STDP plasticity mechanism and reward and punishment signals are integrated through the release of dopamine and other neuromodulators to achieve reinforcement learning [34]. Reward-modulated STDP (R-STDP) is the computational modeling of this process. The main idea of R-STDP is to modulate the outcome of 'standard' STDP by a reward signal [35].</p>
<p>Synaptic eligibility trace, shown in figure 3B, stores a temporary memory of the STDP outcome so that it is still available by the time a delayed reward signal is received [34]. We regard the timing condition (or 'learning window') of traditional STDP as ST DP (n i , n j ), n i and n j denote the presynaptic and postsynaptic neuron in the networks. The synaptic eligibility trace keeps a transient memory in the form of a running average of recent spike-timing coincidences. Synaptic eligibility traces arise from theoretical considerations and effectively bridge the temporal gap between the neural activity and the reward signal.</p>
<p>∆e j,i = − e j,i τ e + ST DP (n i , n j ) (9) e j,i is the eligibility traces between presynaptic neuron i and postsynaptic neuron j, τ e is the time constant of the eligibility trace. The running average is equivalent to a low-pass filter. In R-STDP mechanism, the synaptic weight W changes when the neuromodulator M signals exist.
∆W = R * E(10)
Considering the complexity of the networks, we simply choose R-max policy, R is the reward or punish signal towards networks which is given by the experiment environment. Actually, R is the function of time t, Equation (11) shows how R changes through time.
R(t) =      C r t − t r ≤ T R C p t − t p ≤ T R 0 otherwise(11)
C r and C p are the constants of reward and punish signal. t r and t p denote the latest time of reward and punish. And T R is the size of time window of reward or punish signal. In the experiment, we set C r = 10, C p = −10, and T R = 5.</p>
<p>All the parameters can be found in Table 1.</p>
<p>Figure 1 :
1Brain-inspired Graph Spiking Neural Networks represents commonsense knowledge. (A) Semantic maps in the brain, cite from [15]. (B) Gradients of abstraction in cortex, the visual and linguistic representations of semantic categories align, cite from [16]. (C) Commonsense knowledge graph. (D) Brain-inspired Graph Spiking Neural Networks. Each colored circle represents a neuron population, the small black circles represent neurons, and the line segments represent synaptic connections.</p>
<p>spiking neural networks, which contain N neurons, representing M kinds of concepts and relationships. The pattern ϕ m = {ϕ m i ∈ {0, 1}; 1 ≤ i ≤ N } with index m ∈ {1, ..., M } represents one of the stored memory engram, i.e. one concept or relationship. A value ϕ m i = 1 indicates that neuron i is part of the stored memory engram and therefore belongs to the population of concept m, while a value of ϕ m i = 0 indicates that it does not. A network that has stored M memory engrams is said to have a memory load of α = M N . We focus on the sparse coding of memory engrams, i,e, every single neuron i has a low probability λ = P rob(ϕ m i = 1) 1 to participate in the population of concept cells corresponding to memory engram m.</p>
<p>Figure 2 :
2Neuron populations representing commonsense knowledge triples. (A)The entities A and B, relation R, correspond to a cluster of neurons, respectively. The gray arrows in the figure represent the synaptic connections between different neuron populations learned through the STDP learning rule, and there are also weak connections within each neuron population simultaneously. (B)The weight distribution of these networks. (C)Figure of neuron spike trains during training and test process. (D)Similarity function curve of the networks during the training and testing process.</p>
<p>Inspired by biological discoveries, external input stimulation to the corresponding spike neuron population represents the entities and relations in the knowledge graph. We choose Poisson Encoding as the method of input stimulation. Due to the randomness of the Poisson Encoding, part of neurons in the population will fire at different times when the external stimulation window is given.Each neuron population contains a certain proportion of excitatory and inhibitory neurons. In subsequent chapters, we will specifically discuss the influence of the proportion of inhibitory neurons on the experimental effect and the biological interpretability behind it. As shown inFig.2, we need to charac-terize the triple ARB as the connection between different neuron populations in the spiking neural networks and between them. Specifically, A and B are two entities, R represents a relationship, and they are each represented by a population of neurons. When triples like ARB appear in the knowledge graph, we need to give the three neuron populations A, R, and B, in turn, to stimulate the external current and control this process within the STDP window. Due to the plasticity law of STDP, the synaptic connections between the three different neuron populations of the ARB will form, as shown in Fig.2. The spike trains of networks the learning and testing process are shown in Fig.2C, and Fig.2B shows the weight distribution of networks after training. It can be seen in the figure that there are synaptic connections between specific neuron populations, and weak synaptic connections also exist within neuron populations. The change curve of the similarity function in the above process is shown in Fig.2D. The first three firing processes are the training phase (0-100ms, 200-300ms, 400-500ms), all of which are when external current stimulation is sequentially input to the three neuron populations of the ARB. The networks changes to the Smi of the three ARB memory engrams. 600ms-700ms is the test phase. Only the external currents of the A and R two neuron populations are stimulated. It can be found in Fig.2 D that the similarity of the B neuron population also rises rapidly, thus realizing the memory of the triple ARB by the spiking networks. After completing the memory of the basic building block, this paper uses this foundation to represent all English triples in ConceptNet as spiking synaptic connections between different neuron populations and complete the follow-up on this basis experiment. For details, please refer to the subsequent chapters. The above process is to integrate the foundation of the knowledge graph into the learning process of the spiking networks. After completing the learning, the spiking networks can complete the representation and recall of the knowledge graph. The internal connections of different neuron populations and the connections between neuron populations form a semantic networks graph. We call the spiking neural networks that represent the structural knowledge graph as graph spiking neural networks. Based on the encoding method described in this chapter, we encode the English triples in ConceptNet into spiking neural networks, with a total of about 2'500'000 triples, including 17 relationships and about 800'000 entities. The specific experimental results are shown in the next chapter.Transitive and Non-transitive Relation ReasoningDifferent relationships of common sense knowledge graphs have different properties, and one of the essential properties is transitivity, as shown inFig.3A.How humans distinguish between transitive and non-transitive relationships at the cognitive level and their differences at the level of neural circuits are important research topics in cognitive science, neuroscience, and psychology.</p>
<p>Figure 3 :
3Transitive and Non-transitive Relation Learning with Reward-modulated STDP mechanism. (A)Diagram of transitive and non-transitive relation. (B)Reward-modulated STDP mechanism in computational modeling. (C)Reward-modulated STDP mechanism in the biological brain, cite from[34]. (D)Learning transitive and non-transitive relationships with the R-STDP learning rule. The round neurons represents excitatory neurons and the oval neurons represents inhibitory neurons.</p>
<p>,
We will release the reward signal; on the contrary, if R is a non-transitive relationship when the networks' Smic value exceeds the threshold, and the answer is wrong, we will release the penalty signal. Under the R-STDP learning rule, the synaptic connections within and between neuronal populations will change so that the networks can judge whether ARC is established. As shown inFig.3D, the neural circuit represented by the red line is the synapse that should be formed when the relationship R is a transitive relationship. The internal connection of the neuron population of the transitive relationship is closer than the non-transitive relationship, and the close connection of the neural circuit supports the agent to complete the transitive reasoning. Conversely, if the relationship is non-transitive, the internal connections of the neuron population that characterize the relationship will be weaker, and the neural circuits formed by different triads like ARB and BRC will be relatively independent. Moreover, there are excitatory and inhibitory neurons in each neuron population. These two neurons have a competitive relationship in the calculation process. The specific plasticity rules of excitatory neurons and inhibitory neurons are in the Method section. The existence of inhibitory neurons makes the activation of the A neuron population also inhibits the activation of the C neuron population, thereby realizing the agent's negation of the ARC relationship.</p>
<p>Figure5A.
Each ellipse in Figure5A represents an entity or relationship corresponding to a population of spiking neurons with a specific connection structure. The synaptic connections (black connections) between them are learned through the STDP law, and the process is as Result1. The method described in the chapter accumulates multiple triples. The specific triples contained in the figure are as follows:</p>
<p>Figure 4 :
4Relation reasoning results. (A) The figure of the experimental accuracy rate. Our method has gained comparable accuracy and faster convergence speed compared to GCN. (B) The figure of the experimental accuracy changes with the proportion of inhibitory neurons. It can be seen that when the proportion of inhibitory neurons is about 15%, the performance of networks is the best.</p>
<p>Figure 5 :
5Conceptual Commonsense Knowledge Generating (A)Instance of conceptual commonsense knowledge generating. (B)Similarity function curve of the networks during knowledge generating.</p>
<p>The corresponding synaptic connection is shown in the red directional connection in Figure5A. For this example, we try to let the networks learn a-f knowledge, they corresponding to the six triples in the experiment, and then GSNN acquires the more abstract conceptual knowledge of A person is the president of a country through autonomous induction.In the specific experiment process, we firstly coded six pieces of knowledge into GSNN through the STDP learning rules through the method in the Result1 chapter and obtained all the black synaptic connections in Figure5A. Then continue to stimulate the Biden and Putin neuron populations simultaneously so that the networks can begin to summarize the "common ground". Due to the continuous activation of the Biden and Putin neuron populations, the Person, IsA, and IsPresidentOf neuron populations are also gradually activated. It is worth mentioning that because there will also be a connection between IsA and Person, the Person neuron population will be activated first.Furthermore, because Biden and IsPresidentOf are activated, the America neuron population is activated, and the Russia neuron population is activated too.The activation of America, Russia, and IsA neuron populations directly leads to the activation of Country. At this point, we found that the Person, IsPresidentOf, and Country neuron populations are activated sequentially. According to the STDP rule, new synaptic connections will also be formed that fire within the time window. To our surprise, we only stimulated two neuron populations, Biden and Putin, and KRR-GSNN altogether concluded the knowledge that A person is the president of a country without other intervention. The Similarity function change process of this process networks is shown inFigure5B.After the new synaptic connection is established, we try to verify whether GSNN has learned more abstract new knowledge, so we stimulate the Person and IsPresidentOf neuron populations and then find that the Country neuron population is activated. This result shows that the networks has indeed acquired this new knowledge through self-inductive learning.</p>
<p>One possible mode of operation. For this example, after encoding the existing knowledge graph into GSNN, we only stimulate the two concepts of Biden and Putin. The networks can autonomously generate inductive learning behavior and successfully generate correct knowledge. At present, in neuroscience, the process of human inductive generation of new knowledge is still in an unknown state. Our research provides the possibility of a computational model for this process.</p>
<p>graphs, we continued to explore the model's inductive ability for conceptualizing common sense knowledge and completed some specific examples of inductive reasoning. Through this experiment, we hope to explore possible working mechanisms and patterns in the brain of humans, especially infants, in the process of inductive learning. For a specific instance, after encoding the existing knowledge graph into GSNN, KRR-GSNN can autonomously generate inductive learning behaviors and successfully induce new correct knowledge. At present, in neuroscience, the process of human inductive generation of new knowledge is still unknown. This research provides the possibility of a computational model for follow-up neuroscience and cognitive science-related research.In summary, we are based on spiking neural networks. We use various biologically based brain working mechanisms such as Population Coding, STDP, R-STDP., which proves the feasibility of using spiking networks to complete common sense knowledge representation and reasoning. In addition, by introducing the similarity function to monitor the network's status, the reasoning path of the networks can be obtained, making KRR-GSNN far more interpretable than traditional ANN, more in line with AI ethics, more secure, and controllable. In the deductive reasoning related to the transitivity of the relationship, our model exhibits a faster convergence rate and extremely close final accuracy than GCN.Limitations of the studyKRR-GSNN is a pioneering exploration of commonsense knowledge representation and reasoning based on spiking neural network, and there must be some deficiencies.Firstly, KRR-GSNN emphasizes the representation of knowledge at the symbolic level and lacks insight into the role of multimodal sensory input in commonsense knowledge. Although KRR-GSNN can complete symbolic reasoning based on spiking neural network, it integrates the ideas of symbolic and connectionist</p>
<p>FUNDING
This work was supported by the new generation of artificial intelligence major project of the Ministry of Science and Technology of the People's Republic of China (Grant No. 2020AAA0104305), the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB32070100),the Beijing Municipal Commission of Science and Technology (Grant No. Z181100001518006).</p>
<p>Equation 6 shows that the current of neurons consists of two parts: the current from other neurons and the external stimulating current I s . W j,i is the weight of i-th neuron to j-th neuron. σ i (t) is the indicator to judge if the i-th neuron firing at the time of t in Equation 7.</p>
<p>as the building block of this model, enhancing the biological solidity of our work. Through experimental research, we demonstrated that the spiking neural networks model that integrates brain-inspired mechanisms could complete knowledge representation and related reasoning tasks satisfactorily. Compared with the deep artificial networks model, KRR-GSNN not only has better explainability and biological plausibility but also can converge faster in some reasoning tasks, which excited us intensely.Therefore, we proposed graph spiking neural networks for commonsense </p>
<p>knowledge representation and reasoning(KRR-GSNN) via fusing Spiking neural </p>
<p>networks and Graph neural networks. Besides, we introduced Population cod-</p>
<p>ing, Spike Timing Dependent Plasticity (STDP), and Reward-modulated Spike </p>
<p>Timing Dependent Plasticity(R-STDP) bio-based brain working mechanisms </p>
<p>Specifically ,
Specificallywe hope the networks can learn whether the R relationship is transitive. After the network learns the ARB and BRC triples, it must determine whether the ARC is established. If the relationship R is a transitive relationship, such as Bigger, then ARC must be established. On the contrary, if R is a non-transitive relationship, such as Anonym, then ARC is not established.</p>
<p>In order to balance the computational complexity of the model, we choose the Leaky Integrate-and-Fire (LIF) neuron model as the building block of the Spiking Neural networks. Standard LIF models are shown in Equation 3 , Equation</p>
<p>Table 1 :
1Model parameters.
ACKNOWLEDGMENTSFor valuable discussions, the authors appreciate Feifei Zhao, Jinyu Fan, Dongcheng Zhao, Qian Zhang, Cunqing Huangfu, and Qian Liang. The authors would like to thank all the reviewers for their help in shaping and refining the paper.
The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind. M Minsky, Simon and SchusterM. Minsky, The emotion machine: Commonsense thinking, artificial intelli- gence, and the future of the human mind, Simon and Schuster, 2007.</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Y Bisk, R Zellers, J Gao, Y Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al., Piqa: Reasoning about physical commonsense in natural language, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, 2020, pp. 7432-7439.</p>
<p>Cyc: Using common sense knowledge to overcome brittleness and knowledge acquisition bottlenecks. D B Lenat, M Prakash, M Shepherd, AI magazine. 64D. B. Lenat, M. Prakash, M. Shepherd, Cyc: Using common sense knowledge to overcome brittleness and knowledge acquisition bottlenecks, AI magazine 6 (4) (1985) 65-65.</p>
<p>Conceptneta practical commonsense reasoning tool-kit. H Liu, P Singh, BT technology journal. 224H. Liu, P. Singh, Conceptneta practical commonsense reasoning tool-kit, BT technology journal 22 (4) (2004) 211-226.</p>
<p>T Shu, A Bhandwaldar, C Gan, K A Smith, S Liu, D Gutfreund, E Spelke, J B Tenenbaum, T D Ullman, arXiv:2102.12321Agent: A benchmark for core psychological reasoning. arXiv preprintT. Shu, A. Bhandwaldar, C. Gan, K. A. Smith, S. Liu, D. Gutfreund, E. Spelke, J. B. Tenenbaum, T. D. Ullman, Agent: A benchmark for core psychological reasoning, arXiv preprint arXiv:2102.12321 (2021).</p>
<p>V Kocijan, A.-M Cretu, O.-M Camburu, Y Yordanov, T Lukasiewicz, arXiv:1905.06290A surprisingly robust trick for winograd schema challenge. arXiv preprintV. Kocijan, A.-M. Cretu, O.-M. Camburu, Y. Yordanov, T. Lukasiewicz, A surprisingly robust trick for winograd schema challenge, arXiv preprint arXiv:1905.06290 (2019).</p>
<p>M Yasunaga, H Ren, A Bosselut, P Liang, J Leskovec, arXiv:2104.06378Qa-gnn: Reasoning with language models and knowledge graphs for question answering. arXiv preprintM. Yasunaga, H. Ren, A. Bosselut, P. Liang, J. Leskovec, Qa-gnn: Reasoning with language models and knowledge graphs for question answering, arXiv preprint arXiv:2104.06378 (2021).</p>
<p>Networks of spiking neurons: the third generation of neural network models. W Maass, Neural networks. 109W. Maass, Networks of spiking neurons: the third generation of neural network models, Neural networks 10 (9) (1997) 1659-1671.</p>
<p>W Maass, C M Bishop, Pulsed neural networks. MIT pressW. Maass, C. M. Bishop, Pulsed neural networks, MIT press, 2001.</p>
<p>Glsnn: A multi-layer spiking neural network based on global feedback alignment and local stdp plasticity. D Zhao, Y Zeng, T Zhang, M Shi, F Zhao, Frontiers in Computational Neuroscience. 14D. Zhao, Y. Zeng, T. Zhang, M. Shi, F. Zhao, Glsnn: A multi-layer spiking neural network based on global feedback alignment and local stdp plasticity, Frontiers in Computational Neuroscience 14 (2020).</p>
<p>A brain-inspired causal reasoning model based on spiking neural networks. H Fang, Y Zeng, 2021 International Joint Conference on Neural Networks (IJCNN). IEEEH. Fang, Y. Zeng, A brain-inspired causal reasoning model based on spiking neural networks, in: 2021 International Joint Conference on Neural Networks (IJCNN), IEEE, 2021, pp. 1-5.</p>
<p>Brain inspired sequences production by spiking neural networks with reward-modulated stdp. H Fang, Y Zeng, F Zhao, Frontiers in Computational Neuroscience. 158H. Fang, Y. Zeng, F. Zhao, Brain inspired sequences production by spiking neural networks with reward-modulated stdp, Frontiers in Computational Neuroscience 15 (2021) 8.</p>
<p>A brain-inspired decision-making spiking neural network and its application in unmanned aerial vehicle. F Zhao, Y Zeng, B Xu, Frontiers in neurorobotics. 1256F. Zhao, Y. Zeng, B. Xu, A brain-inspired decision-making spiking neu- ral network and its application in unmanned aerial vehicle, Frontiers in neurorobotics 12 (2018) 56.</p>
<p>A neural algorithm for drosophila linear and nonlinear decision-making. F Zhao, Y Zeng, A Guo, H Su, B Xu, Scientific Reports. 101F. Zhao, Y. Zeng, A. Guo, H. Su, B. Xu, A neural algorithm for drosophila linear and nonlinear decision-making, Scientific Reports 10 (1) (2020) 1-16.</p>
<p>Natural speech reveals the semantic maps that tile human cerebral cortex. A G Huth, W A De Heer, T L Griffiths, F E Theunissen, J L Gallant, Nature. 5327600A. G. Huth, W. A. De Heer, T. L. Griffiths, F. E. Theunissen, J. L. Gallant, Natural speech reveals the semantic maps that tile human cerebral cortex, Nature 532 (7600) (2016) 453-458.</p>
<p>Visual and linguistic semantic representations are aligned at the border of human visual cortex. S F Popham, A G Huth, N Y Bilenko, F Deniz, J S Gao, A O Nunez-Elizalde, J L Gallant, Nature neuroscience. 2411S. F. Popham, A. G. Huth, N. Y. Bilenko, F. Deniz, J. S. Gao, A. O. Nunez- Elizalde, J. L. Gallant, Visual and linguistic semantic representations are aligned at the border of human visual cortex, Nature neuroscience 24 (11) (2021) 1628-1636.</p>
<p>Modeling relational data with graph convolutional networks, in: European semantic web conference. M Schlichtkrull, T N Kipf, P Bloem, R Van Den, I Berg, M Titov, Welling, SpringerM. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, M. Welling, Modeling relational data with graph convolutional networks, in: European semantic web conference, Springer, 2018, pp. 593-607.</p>
<p>Cognitive graph for multi-hop reading comprehension at scale. M Ding, C Zhou, Q Chen, H Yang, J Tang, ACLM. Ding, C. Zhou, Q. Chen, H. Yang, J. Tang, Cognitive graph for multi-hop reading comprehension at scale, in: ACL, 2019.</p>
<p>Y Zhang, X Chen, Y Yang, A Ramamurthy, B Li, Y Qi, L Song, arXiv:2001.11850Efficient probabilistic logic reasoning with graph neural networks. arXiv preprintY. Zhang, X. Chen, Y. Yang, A. Ramamurthy, B. Li, Y. Qi, L. Song, Efficient probabilistic logic reasoning with graph neural networks, arXiv preprint arXiv:2001.11850 (2020).</p>
<p>Experience-induced neural circuits that achieve high capacity. V Feldman, L G Valiant, Neural computation. 2110V. Feldman, L. G. Valiant, Experience-induced neural circuits that achieve high capacity, Neural computation 21 (10) (2009) 2715-2754.</p>
<p>Concept cells: the building blocks of declarative memory functions. R Q Quiroga, Nature Reviews Neuroscience. 138R. Q. Quiroga, Concept cells: the building blocks of declarative memory functions, Nature Reviews Neuroscience 13 (8) (2012) 587-97.</p>
<p>R Q Quiroga, Neural representations across species. 363R. Q. Quiroga, Neural representations across species, Science 363 (6434) (2019) 1388-1389.</p>
<p>Geometry of sequence working memory in macaque prefrontal cortex. Y Xie, P Hu, J Li, J Chen, W Song, X.-J Wang, T Yang, S Dehaene, S Tang, B Min, Science. 3756581Y. Xie, P. Hu, J. Li, J. Chen, W. Song, X.-J. Wang, T. Yang, S. Dehaene, S. Tang, B. Min, et al., Geometry of sequence working memory in macaque prefrontal cortex, Science 375 (6581) (2022) 632-639.</p>
<p>S Ji, S Pan, E Cambria, P Marttinen, P S Yu, A survey on knowledge graphs: Representation, acquisition and applications. S. Ji, S. Pan, E. Cambria, P. Marttinen, P. S. Yu, A survey on knowledge graphs: Representation, acquisition and applications (2020).</p>
<p>Qa-gnn: Reasoning with language models and knowledge graphs for question answering. M Yasunaga, H Ren, A Bosselut, P Liang, J Leskovec, M. Yasunaga, H. Ren, A. Bosselut, P. Liang, J. Leskovec, Qa-gnn: Reasoning with language models and knowledge graphs for question answering, 2021.</p>
<p>Invariant visual representation by single neurons in the human brain. R Q Quiroga, L Reddy, G Kreiman, C Koch, I Fried, Nature. 4357045R. Q. Quiroga, L. Reddy, G. Kreiman, C. Koch, I. Fried, Invariant visual representation by single neurons in the human brain, Nature 435 (7045) (2005) 1102-1107.</p>
<p>Concept cells: the building blocks of declarative memory functions. R Q Quiroga, Nature Reviews Neuroscience. 138R. Q. Quiroga, Concept cells: the building blocks of declarative memory functions, Nature Reviews Neuroscience 13 (8) (2012) 587-597.</p>
<p>When shared concept cells support associations: theory of overlapping memory engrams. C Gastaldi, T Schwalger, E De Falco, R Q Quiroga, W Gerstner, bioRxiv. C. Gastaldi, T. Schwalger, E. De Falco, R. Q. Quiroga, W. Gerstner, When shared concept cells support associations: theory of overlapping memory engrams, bioRxiv (2021).</p>
<p>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type. G Bi, M.-M Poo, Journal of neuroscience. 1824G.-q. Bi, M.-m. Poo, Synaptic modifications in cultured hippocampal neu- rons: dependence on spike timing, synaptic strength, and postsynaptic cell type, Journal of neuroscience 18 (24) (1998) 10464-10472.</p>
<p>Spike timing-dependent plasticity of neural circuits. Y Dan, M.-M Poo, Neuron. 441Y. Dan, M.-m. Poo, Spike timing-dependent plasticity of neural circuits, Neuron 44 (1) (2004) 23-30.</p>
<p>Spike timing-dependent plasticity: from synapse to perception. Y Dan, M.-M Poo, Physiological reviews. 863Y. Dan, M.-M. Poo, Spike timing-dependent plasticity: from synapse to perception, Physiological reviews 86 (3) (2006) 1033-1048.</p>
<p>Malleability of spike-timing-dependent plasticity at the ca3-ca1 synapse. G M Wittenberg, S S.-H Wang, Journal of Neuroscience. 2624G. M. Wittenberg, S. S.-H. Wang, Malleability of spike-timing-dependent plasticity at the ca3-ca1 synapse, Journal of Neuroscience 26 (24) (2006) 6610-6617.</p>
<p>Conjugate reinforcement of infant exploratory behavior. C K Rovee, D T Rovee, Journal of experimental child psychology. 81C. K. Rovee, D. T. Rovee, Conjugate reinforcement of infant exploratory behavior, Journal of experimental child psychology 8 (1) (1969) 33-39.</p>
<p>Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules. N Frémaux, W Gerstner, Frontiers in neural circuits. 985N. Frémaux, W. Gerstner, Neuromodulated spike-timing-dependent plastic- ity, and theory of three-factor learning rules, Frontiers in neural circuits 9 (2016) 85.</p>
<p>Spatio-temporal credit assignment in neuronal population learning. J Friedrich, R Urbanczik, W Senn, PLoS Comput Biol. 761002092J. Friedrich, R. Urbanczik, W. Senn, Spatio-temporal credit assignment in neuronal population learning, PLoS Comput Biol 7 (6) (2011) e1002092.</p>
<p>Geometry of abstract learned knowledge in the hippocampus. E H Nieh, M Schottdorf, N W Freeman, R J Low, S Lewallen, S A Koay, L Pinto, J L Gauthier, C D Brody, D W Tank, Nature. E. H. Nieh, M. Schottdorf, N. W. Freeman, R. J. Low, S. Lewallen, S. A. Koay, L. Pinto, J. L. Gauthier, C. D. Brody, D. W. Tank, Geometry of abstract learned knowledge in the hippocampus, Nature (2021) 1-5.</p>
<p>Commonsense reasoning and commonsense knowledge in artificial intelligence. E Davis, G Marcus, Communications of the ACM. 589E. Davis, G. Marcus, Commonsense reasoning and commonsense knowledge in artificial intelligence, Communications of the ACM 58 (9) (2015) 92-103.</p>
<p>Z Hu, X Ma, Z Liu, E Hovy, E Xing, arXiv:1603.06318Harnessing deep neural networks with logic rules. arXiv preprintZ. Hu, X. Ma, Z. Liu, E. Hovy, E. Xing, Harnessing deep neural networks with logic rules, arXiv preprint arXiv:1603.06318 (2016).</p>
<p>Embodied cognition, Routledge. L Shapiro, L. Shapiro, Embodied cognition, Routledge, 2010.</p>
<p>Six views of embodied cognition. M Wilson, Psychonomic bulletin &amp; review. 94M. Wilson, Six views of embodied cognition, Psychonomic bulletin &amp; review 9 (4) (2002) 625-636.</p>
<p>A quantitative description of membrane current and its application to conduction and excitation in nerve. A L Hodgkin, A F Huxley, Bulletin of Mathematical Biology. 521-2A. L. Hodgkin, A. F. Huxley, A quantitative description of membrane current and its application to conduction and excitation in nerve., Bulletin of Mathematical Biology 52 (1-2) (1952) 25-71.</p>
<p>An Introductory Course in Computational Neuroscience. P Miller, MIT PressP. Miller, An Introductory Course in Computational Neuroscience, MIT Press, 2018.</p>
<p>Simple model of spiking neurons. E M Izhikevich, IEEE Transactions on neural networks. 146E. M. Izhikevich, Simple model of spiking neurons, IEEE Transactions on neural networks 14 (6) (2003) 1569-1572.</p>            </div>
        </div>

    </div>
</body>
</html>
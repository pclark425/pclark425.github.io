<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6969 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6969</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6969</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-274789129</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.12564v3.pdf" target="_blank">Evaluating zero-shot multilingual aspect-based sentiment analysis with large language models</a></p>
                <p><strong>Paper Abstract:</strong> Aspect-based sentiment analysis (ABSA), a sequence labeling task, has attracted increasing attention in multilingual contexts. While previous research has focused largely on fine-tuning or training models specifically for ABSA, we evaluate large language models (LLMs) under zero-shot conditions to explore their potential to tackle this challenge with minimal task-specific adaptation. We conduct a comprehensive empirical evaluation of a series of LLMs on multilingual ABSA tasks, investigating various prompting strategies, including vanilla zero-shot, chain-of-thought (CoT), self-improvement, self-debate, and self-consistency, across nine different models. Results indicate that while LLMs show promise in handling multilingual ABSA, they generally fall short of fine-tuned, task-specific models. Notably, simpler zero-shot prompts often outperform more complex strategies, especially in high-resource languages like English. These findings underscore the need for further refinement of LLM-based approaches to effectively address ABSA task across diverse languages.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6969.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6969.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Improvement (multi-turn self-refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-turn generate-then-reflect prompting strategy where the model first produces an initial zero-shot answer, then explains its reasoning, and finally revises its answer based on that explanation; evaluated across multiple rounds in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (GPT-4o, Claude-3.5, Gemini-1.5, Gemma-2, Llama-3.1, Mistral, Phi-3.5-mini, Qwen-2.5, Zephyr)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large language models of various architectures and pretraining/tuning regimes; both open-source and closed-source models were evaluated under the same prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (examples: Phi-3.5-mini ~3.8B; Mistral 7B; Zephyr 7B; Qwen series: 0.5B–72B; Llama-3.1 8B; Gemma-2 9B; some proprietary model sizes not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-step multi-turn prompting: (1) generate initial zero-shot response; (2) ask the model to explain/justify its initial response; (3) provide the model with its initial response and explanation and ask it to produce a revised answer (i.e., self-correction). Can be iterated for additional rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (multi-turn self-revision)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval-2016 Multilingual Aspect-Based Sentiment Analysis (ABSA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Token-level extraction of aspect terms (entities) and associated sentiment polarity across multiple languages (English, French, Spanish, Dutch, Russian) using the SemEval-2016 dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Micro-F1 (correct only if both aspect term and sentiment match ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Zero-shot baseline (single-turn) — reported implicitly as the comparison baseline; the paper reports that zero-shot outperforms multi-turn approaches (see performance_after_reflection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Multi-turn (self-improvement & self-debate) best average F1 = 37.17% Micro-F1 (the paper states this is 4.64 percentage points lower than the zero-shot method, i.e., zero-shot ≈ 41.81% Micro-F1 by reported difference).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Overall performance declines compared to simple zero-shot; error accumulation across rounds (performance consistently declines with each additional round); models often over-assume errors exist in previous outputs and unnecessarily change previously correct predictions; ambiguity in implicit sentiment reasoning leads to incorrect revisions (example: sentiment polarity of 'chuleton' changed from negative to neutral after revision); self-improvement shows less degradation than self-debate but still underperforms zero-shot in this ABSA setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating zero-shot multilingual aspect-based sentiment analysis with large language models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6969.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6969.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Debate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Debate (multi-agent simulated debate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-turn prompting strategy that simulates multiple agents: initial answer, a second agent critiques/corrects it, and a third agent debates/evaluates both to arrive at a final decision — limited to three steps in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (GPT-4o, Claude-3.5, Gemini-1.5, Gemma-2, Llama-3.1, Mistral, Phi-3.5-mini, Qwen-2.5, Zephyr)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large language models evaluated under multi-turn self-debate prompts to test internal critical review capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see Self-Improvement entry)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Debate</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-step simulated multi-agent debate: (1) initial zero-shot response; (2) model acts as a critic (second agent) to review/correct the initial response; (3) model acts as a third agent to evaluate preceding outputs and debate to a final answer. Can be iterated; experiments limited to three rounds for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (multi-agent multi-turn debate)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval-2016 Multilingual Aspect-Based Sentiment Analysis (ABSA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Token-level extraction of aspect terms and sentiment across multiple languages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Micro-F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Zero-shot baseline (single-turn) — used as comparison baseline; zero-shot outperforms multi-turn strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Multi-turn (self-improvement & self-debate) best average F1 = 37.17% Micro-F1 (multi-turn average is 4.64 percentage points lower than zero-shot overall). Self-debate shows larger degradation than self-improvement; e.g., for GPT-4o the drop from round 1 to round 2 in self-debate is approximately 10 percentage points (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degrades across rounds, more so than self-improvement; models may assume prior responses contain errors and make unnecessary changes (catastrophic forgetting-like behavior); error accumulation across turns; examples where self-debate generalizes entire sentence sentiment (producing OOD predictions) or over-predicts aspects (leading to false positives); authors limited self-debate to three steps acknowledging diminishing returns and greater instability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating zero-shot multilingual aspect-based sentiment analysis with large language models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6969.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6969.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over sampled chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling-based strategy that runs the same prompt multiple times with different temperature/randomness, collects multiple candidate outputs, and selects the most frequent (majority) answer as final.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (GPT-4o, Claude-3.5, Gemini-1.5, Gemma-2, Llama-3.1, Mistral, Phi-3.5-mini, Qwen-2.5, Zephyr)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs evaluated under repeated-sampling with different temperature settings to test output consistency and majority-vote selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see other entries)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Run the same prompt multiple times with different temperature settings (sampling), gather multiple outputs, then use majority voting to select the most consistent final output.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples (multiple independent generations)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>6</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval-2016 Multilingual Aspect-Based Sentiment Analysis (ABSA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aspect term and sentiment extraction across multiple languages; paper used temperatures {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Micro-F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Zero-shot single-run greedy decoding (temperature=0) — used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Authors report decreases when averaging/majority-voting across multiple temperature settings: average performance decreased by 5.26%, 7.26%, 5.48%, 7.29%, and 5.29% respectively across the five languages compared to the initial zero-shot (temperature=0) setting. (Paper reports these per-language decreases rather than a single aggregated after-reflection number.)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Introduced sampling diversity produced inconsistencies that harmed Micro-F1 on ABSA (contrary to some reasoning benchmarks where self-consistency helps); careful temperature selection is important; majority-vote over diverse outputs can aggregate inconsistent aspect-span predictions leading to worse structured extraction performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating zero-shot multilingual aspect-based sentiment analysis with large language models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6969.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6969.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that requests models to articulate intermediate reasoning steps before producing final answers; evaluated here for ABSA but found to reduce performance relative to simple zero-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (GPT-4o, Claude-3.5, Gemini-1.5, Gemma-2, Llama-3.1, Mistral, Phi-3.5-mini, Qwen-2.5, Zephyr)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs prompted to produce step-by-step reasoning chains before final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see other entries)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Explicitly instruct the model to generate intermediate reasoning steps (a reasoning chain) and then a final output; increases transparency of decision process but can lead to over-reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>single-turn stepwise reasoning (not generate-then-reflect)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SemEval-2016 Multilingual Aspect-Based Sentiment Analysis (ABSA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Token-level aspect and sentiment extraction across multiple languages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Micro-F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Zero-shot single-turn baseline (used as comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>CoT underperforms zero-shot: the paper reports that zero-shot outperforms CoT by an average 1.55 percentage points Micro-F1 across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT can 'over-reason' on incomplete or fragmentary sentences, producing incorrect or spurious reasoning chains and leading to entity omission or incorrect aspect spans (examples: short exclamations like 'Yum !' where CoT fails to find explicit evidence and over-extrapolates); CoT sometimes emphasizes syntactic structure (e.g., including definite articles like 'la' as part of entity) and can translate back to English in reasoning causing boundary errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating zero-shot multilingual aspect-based sentiment analysis with large language models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-improving for zero-shot named entity recognition with large language models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Rethinking the bounds of LLM reasoning: Are multi-agent discussions the key? <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6969",
    "paper_id": "paper-274789129",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "Self-Improvement",
            "name_full": "Self-Improvement (multi-turn self-refinement)",
            "brief_description": "A multi-turn generate-then-reflect prompting strategy where the model first produces an initial zero-shot answer, then explains its reasoning, and finally revises its answer based on that explanation; evaluated across multiple rounds in this paper.",
            "citation_title": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (GPT-4o, Claude-3.5, Gemini-1.5, Gemma-2, Llama-3.1, Mistral, Phi-3.5-mini, Qwen-2.5, Zephyr)",
            "model_description": "Instruction-tuned large language models of various architectures and pretraining/tuning regimes; both open-source and closed-source models were evaluated under the same prompting strategies.",
            "model_size": "various (examples: Phi-3.5-mini ~3.8B; Mistral 7B; Zephyr 7B; Qwen series: 0.5B–72B; Llama-3.1 8B; Gemma-2 9B; some proprietary model sizes not specified)",
            "reflection_method_name": "Self-Improvement",
            "reflection_method_description": "Three-step multi-turn prompting: (1) generate initial zero-shot response; (2) ask the model to explain/justify its initial response; (3) provide the model with its initial response and explanation and ask it to produce a revised answer (i.e., self-correction). Can be iterated for additional rounds.",
            "iteration_type": "generate-then-reflect (multi-turn self-revision)",
            "num_iterations": 3,
            "task_name": "SemEval-2016 Multilingual Aspect-Based Sentiment Analysis (ABSA)",
            "task_description": "Token-level extraction of aspect terms (entities) and associated sentiment polarity across multiple languages (English, French, Spanish, Dutch, Russian) using the SemEval-2016 dataset.",
            "evaluation_metric": "Micro-F1 (correct only if both aspect term and sentiment match ground truth)",
            "performance_before_reflection": "Zero-shot baseline (single-turn) — reported implicitly as the comparison baseline; the paper reports that zero-shot outperforms multi-turn approaches (see performance_after_reflection).",
            "performance_after_reflection": "Multi-turn (self-improvement & self-debate) best average F1 = 37.17% Micro-F1 (the paper states this is 4.64 percentage points lower than the zero-shot method, i.e., zero-shot ≈ 41.81% Micro-F1 by reported difference).",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Overall performance declines compared to simple zero-shot; error accumulation across rounds (performance consistently declines with each additional round); models often over-assume errors exist in previous outputs and unnecessarily change previously correct predictions; ambiguity in implicit sentiment reasoning leads to incorrect revisions (example: sentiment polarity of 'chuleton' changed from negative to neutral after revision); self-improvement shows less degradation than self-debate but still underperforms zero-shot in this ABSA setup.",
            "uuid": "e6969.0",
            "source_info": {
                "paper_title": "Evaluating zero-shot multilingual aspect-based sentiment analysis with large language models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-Debate",
            "name_full": "Self-Debate (multi-agent simulated debate)",
            "brief_description": "A multi-turn prompting strategy that simulates multiple agents: initial answer, a second agent critiques/corrects it, and a third agent debates/evaluates both to arrive at a final decision — limited to three steps in experiments.",
            "citation_title": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (GPT-4o, Claude-3.5, Gemini-1.5, Gemma-2, Llama-3.1, Mistral, Phi-3.5-mini, Qwen-2.5, Zephyr)",
            "model_description": "Instruction-tuned large language models evaluated under multi-turn self-debate prompts to test internal critical review capabilities.",
            "model_size": "various (see Self-Improvement entry)",
            "reflection_method_name": "Self-Debate",
            "reflection_method_description": "Three-step simulated multi-agent debate: (1) initial zero-shot response; (2) model acts as a critic (second agent) to review/correct the initial response; (3) model acts as a third agent to evaluate preceding outputs and debate to a final answer. Can be iterated; experiments limited to three rounds for efficiency.",
            "iteration_type": "generate-then-reflect (multi-agent multi-turn debate)",
            "num_iterations": 3,
            "task_name": "SemEval-2016 Multilingual Aspect-Based Sentiment Analysis (ABSA)",
            "task_description": "Token-level extraction of aspect terms and sentiment across multiple languages.",
            "evaluation_metric": "Micro-F1",
            "performance_before_reflection": "Zero-shot baseline (single-turn) — used as comparison baseline; zero-shot outperforms multi-turn strategies.",
            "performance_after_reflection": "Multi-turn (self-improvement & self-debate) best average F1 = 37.17% Micro-F1 (multi-turn average is 4.64 percentage points lower than zero-shot overall). Self-debate shows larger degradation than self-improvement; e.g., for GPT-4o the drop from round 1 to round 2 in self-debate is approximately 10 percentage points (reported).",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Performance degrades across rounds, more so than self-improvement; models may assume prior responses contain errors and make unnecessary changes (catastrophic forgetting-like behavior); error accumulation across turns; examples where self-debate generalizes entire sentence sentiment (producing OOD predictions) or over-predicts aspects (leading to false positives); authors limited self-debate to three steps acknowledging diminishing returns and greater instability.",
            "uuid": "e6969.1",
            "source_info": {
                "paper_title": "Evaluating zero-shot multilingual aspect-based sentiment analysis with large language models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority-vote over sampled chains)",
            "brief_description": "A sampling-based strategy that runs the same prompt multiple times with different temperature/randomness, collects multiple candidate outputs, and selects the most frequent (majority) answer as final.",
            "citation_title": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (GPT-4o, Claude-3.5, Gemini-1.5, Gemma-2, Llama-3.1, Mistral, Phi-3.5-mini, Qwen-2.5, Zephyr)",
            "model_description": "Instruction-tuned LLMs evaluated under repeated-sampling with different temperature settings to test output consistency and majority-vote selection.",
            "model_size": "various (see other entries)",
            "reflection_method_name": "Self-Consistency",
            "reflection_method_description": "Run the same prompt multiple times with different temperature settings (sampling), gather multiple outputs, then use majority voting to select the most consistent final output.",
            "iteration_type": "voting over multiple samples (multiple independent generations)",
            "num_iterations": 6,
            "task_name": "SemEval-2016 Multilingual Aspect-Based Sentiment Analysis (ABSA)",
            "task_description": "Aspect term and sentiment extraction across multiple languages; paper used temperatures {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} in experiments.",
            "evaluation_metric": "Micro-F1",
            "performance_before_reflection": "Zero-shot single-run greedy decoding (temperature=0) — used as baseline.",
            "performance_after_reflection": "Authors report decreases when averaging/majority-voting across multiple temperature settings: average performance decreased by 5.26%, 7.26%, 5.48%, 7.29%, and 5.29% respectively across the five languages compared to the initial zero-shot (temperature=0) setting. (Paper reports these per-language decreases rather than a single aggregated after-reflection number.)",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Introduced sampling diversity produced inconsistencies that harmed Micro-F1 on ABSA (contrary to some reasoning benchmarks where self-consistency helps); careful temperature selection is important; majority-vote over diverse outputs can aggregate inconsistent aspect-span predictions leading to worse structured extraction performance.",
            "uuid": "e6969.2",
            "source_info": {
                "paper_title": "Evaluating zero-shot multilingual aspect-based sentiment analysis with large language models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting method that requests models to articulate intermediate reasoning steps before producing final answers; evaluated here for ABSA but found to reduce performance relative to simple zero-shot prompts.",
            "citation_title": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (GPT-4o, Claude-3.5, Gemini-1.5, Gemma-2, Llama-3.1, Mistral, Phi-3.5-mini, Qwen-2.5, Zephyr)",
            "model_description": "Instruction-tuned LLMs prompted to produce step-by-step reasoning chains before final answer.",
            "model_size": "various (see other entries)",
            "reflection_method_name": "Chain-of-Thought (CoT)",
            "reflection_method_description": "Explicitly instruct the model to generate intermediate reasoning steps (a reasoning chain) and then a final output; increases transparency of decision process but can lead to over-reasoning.",
            "iteration_type": "single-turn stepwise reasoning (not generate-then-reflect)",
            "num_iterations": null,
            "task_name": "SemEval-2016 Multilingual Aspect-Based Sentiment Analysis (ABSA)",
            "task_description": "Token-level aspect and sentiment extraction across multiple languages.",
            "evaluation_metric": "Micro-F1",
            "performance_before_reflection": "Zero-shot single-turn baseline (used as comparison).",
            "performance_after_reflection": "CoT underperforms zero-shot: the paper reports that zero-shot outperforms CoT by an average 1.55 percentage points Micro-F1 across benchmarks.",
            "improvement_observed": false,
            "limitations_or_failure_cases": "CoT can 'over-reason' on incomplete or fragmentary sentences, producing incorrect or spurious reasoning chains and leading to entity omission or incorrect aspect spans (examples: short exclamations like 'Yum !' where CoT fails to find explicit evidence and over-extrapolates); CoT sometimes emphasizes syntactic structure (e.g., including definite articles like 'la' as part of entity) and can translate back to English in reasoning causing boundary errors.",
            "uuid": "e6969.3",
            "source_info": {
                "paper_title": "Evaluating zero-shot multilingual aspect-based sentiment analysis with large language models",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-improving for zero-shot named entity recognition with large language models",
            "rating": 2,
            "sanitized_title": "selfimproving_for_zeroshot_named_entity_recognition_with_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Rethinking the bounds of LLM reasoning: Are multi-agent discussions the key?",
            "rating": 2,
            "sanitized_title": "rethinking_the_bounds_of_llm_reasoning_are_multiagent_discussions_the_key"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        }
    ],
    "cost": 0.017912749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models
9 Jun 2025</p>
<p>Chengyan Wu chengyan.wu@m.scnu.edu.cn 
School of Electronic Science and Engineering (School of Microelectronics)
Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials
South China Normal University
528225FoshanChina</p>
<p>Guangdong Provincial Key Laboratory of Intelligent Information Processing
518060ShenzhenChina</p>
<p>Bolei Ma bolei.ma@lmu.de 
Department of Statistics
Ludwig Maximilian University of Munich
80539MunichGermany</p>
<p>Munich Center for Machine Learning
80539MunichGermany</p>
<p>Zheyu Zhang zheyu.zhang@tum.de 
Technical University of Munich
80333MunichGermany</p>
<p>Ningyuan Deng 
Research Center for Information Science Theory and Methodology
Institute of Scientific and Technical Information of China
100038BeijingChina</p>
<p>Yanqing He 
Research Center for Information Science Theory and Methodology
Institute of Scientific and Technical Information of China
100038BeijingChina</p>
<p>Yun Xue xueyun@m.scnu.edu.cn 
School of Electronic Science and Engineering (School of Microelectronics)
Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials
South China Normal University
528225FoshanChina</p>
<p>Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models
9 Jun 2025BD775D2101208D45D4B7718CDC885B0EarXiv:2412.12564v3[cs.CL]Multilingual Aspect-Based Sentiment AnalysisLarge Language ModelResource and Evaluation
Aspect-based sentiment analysis (ABSA), a sequence labeling task, has attracted increasing attention in multilingual contexts.While previous research has focused largely on fine-tuning or training models specifically for ABSA, we evaluate large language models (LLMs) under zero-shot conditions to explore their potential to tackle this challenge with minimal task-specific adaptation.We conduct a comprehensive empirical evaluation of a series of LLMs on multilingual ABSA tasks, investigating various prompting strategies, including vanilla zero-shot, chain-of-thought (CoT), self-improvement, self-debate, and self-consistency, across nine different models.Results indicate that while LLMs show promise in handling multilingual ABSA, they generally fall short of fine-tuned, task-specific models.Notably, simpler zero-shot prompts often outperform more complex strategies, especially in high-resource languages like English.These findings underscore the need for further refinement of LLM-based approaches to effectively address ABSA task across diverse languages.</p>
<p>Introduction</p>
<p>Aspect-based sentiment analysis (ABSA) is a sequence labeling task in natural language processing (NLP) focused on identifying fine-grained information in sentences, such as aspect terms and their associated sentiment polarity [1].Although research [2][3][4] on ABSA has achieved success on English texts with large corpora, real-world social media interactions often involve multiple languages [5,6], creating the need for multilingual sentiment analysis.</p>
<p>Figure 1 illustrates a simple multilingual ABSA example, where the same restaurant review is presented in five different languages.A well-trained model should be able to identify the corresponding aspect terms service and food in each language and predict the associated sentiment polarity (service is positive, food is negative).<EN> The service was great, but the food was bad.</p>
<p><ES> El servicio fue excelente, pero la comida fue mala.<FR> Le service était excellent, mais la nourriture était mauvaise.</p>
<p><NL> De service was geweldig, maar het eten was slecht.</p>
<p><RU> Обслуживание было отличным, но еда была плохой.</p>
<p>Positive Negative</p>
<p>LLMs Fig. 1: An example of multilingual ABSA task, where LLMs identify and analyze aspect terms across languages, determining the associated sentiments.</p>
<p>With the rise of LLMs, there has been growing interest in leveraging in-context learning approaches for various downstream tasks [7-9, inter alia].Many studies have evaluated LLMs' capabilities in sequence labeling tasks such as named entity recognition (NER) [10] and part-of-speech (POS) tagging, particularly in multilingual settings [11,12].Similarly, work on LLMs for sentiment analysis has included some assessment of ABSA task [13].However, few studies have examined LLMs' performance on multilingual ABSA task specifically through various in-context learning strategies.As a result, the extent to which LLMs can effectively handle multilingual ABSA task under these conditions remains unclear.Understanding how well LLMs capture token-level aspect terms and their associated sentiments across languages will offer insights into their multilingual capabilities and aid in designing more effective models for specific tasks.</p>
<p>This work aims to comprehensively evaluate LLMs on multilingual ABSA tasks by exploring different prompt strategies and languages.Specifically, we aim to answer the following questions: (1) How do LLMs perform on multilingual ABSA task?(2) How do single-turn and multi-turn dialogue prompting paradigms affect the LLM performance on this task?(3) How do LLMs compare with smaller models trained on domain-specific datasets?</p>
<p>To address these questions, we begin by reviewing sequence labeling methods in the context of multilingual ABSA and large language models.Our evaluation spans datasets across multiple domains and five languages.While prior ABSA studies often use a single prompt strategy and language, this narrow evaluation fails to demonstrate LLMs' adaptability.Therefore, it is necessary to perform a more comprehensive evaluation using various prompt strategies and LLMs.</p>
<p>Our experiments reveal several key findings: First, the LLM performance varies across languages, with higher performance in higher-resource languages.Second, when using single-turn and multiturn dialogue methods requiring reasoning outputs, models show varying degrees of performance degradation, indicating the need for better solutions to optimize the reasoning process.Third, in extracting fine-grained structured information, LLMs still lag behind smaller language models trained on specific datasets.Finally, while LLM performance improves in few-shot settings with more demonstrations, it is constrained by context length, highlighting the need for more efficient solutions in the future.</p>
<p>Related Work</p>
<p>Multilingual ABSA.Multilingual ABSA aims to identify aspects and their associated sentiment elements in the text across multiple languages.This task is complicated by linguistic diversity, cultural differences, and the scarcity of annotated datasets in non-English languages [14].Early approaches relied heavily on supervised learning models, necessitating manually annotated data for each language [15][16][17][18].This reliance on human resources to address data scarcity often made the annotation process extremely costly.Subsequent studies have addressed the limitation of insufficient low-resource sentiment analysis corpora by using machine translation methods.Recent advancements have shifted focus to cross-lingual embeddings and multilingual pre-trained language models, often fine-tuning these on specific languages for ABSA task [19][20][21].Zhang et al. [22] improve the performance of cross-lingual sentiment analysis by introducing code-switched data and knowledge distillation.Lin et al. [23] propose a contrastive learning framework aimed at enhancing the semantic space representation in cross-lingual sentiment analysis tasks.While these approaches have enhanced generalization across languages, they still face difficulties when dealing with user-generated content, such as slang, abbreviations, and culturally dependent expressions [24].The emergence of LLMs offers new opportunities to tackle these challenges, driving our investigation into their performance on such a nuanced multilingual task.</p>
<p>LLMs for Sequence Labeling tasks.LLMs have recently shown promise in various sequence labeling tasks [25], such as ABSA, NER [26], and POS tagging [10,13,27].Unlike traditional models that often require task-specific architectures, LLMs benefit from extensive pre-training on diverse data, enabling them to provide flexible solutions across multiple tasks.Techniques such as prompting and fine-tuning have been explored to improve LLMs' applicability to sentiment analysis tasks [28][29][30].Zhong et al. [31] find that the zero-shot performance of LLMs is comparable to that of fine-tuned BERT models.Meanwhile, Wang et al. [32] conduct a preliminary exploration on the application of ChatGPT to open-domain sentiment analysis and reasoning tasks.Han et al. [33] conduct an extensive evaluation of ChatGPT on aspect-based sentiment analysis tasks, revealing a significant performance gap between ChatGPT and other state-of-the-art methods.Deng et al. [34] study the use of pseudo-label data generated by LLMs to fine-tune smaller models, achieving the performance of supervised learning models.However, these efforts have generally been limited to monolingual (English) sentiment analysis tasks, often involving datasets from different domains, which causes the performance of models to fluctuate due to domain-specific differences, complicating the interpretation of these variations.Besides, several challenges remain, particularly when adapting LLMs to multilingual contexts and balancing trade-offs between model size, computational cost, and performance [28].Exploring LLMs on multilingual parallel corpora can help mitigate data-level differences and allow for a more focused evaluation of model performance across different languages.In our work, we mainly extend this research by evaluating LLMs' effectiveness on multilingual ABSA, a complex task that requires handling both aspect extraction and sentiment classification across languages.</p>
<p>Prompting Strategies</p>
<p>To evaluate the ability of LLMs to perform multilingual ABSA task, we employ a range of prompting strategies, from basic zero-shot prompts to more complex multi-turn reasoning prompts.Given that LLMs are highly sensitive to prompts and may generate varying responses even when the prompts are semantically similar [13,35,36], we ensure consistency across each prompting strategy.Figure 2 illustrates five prompting strategies assessed in this study.In all cases, we include a short introduction sentence in the beginning to make the model familiar with the task.Considering the token-level nature of the ABSA task, we include formatting instructions and explicitly instruct the models to output the results in a JSON format, containing the aspect term (entity) and its associated sentiment, in the prompts.The expected output structure is as follows:</p>
<p>{"entity": "<entity>", "sentiment": "<label>"} Vanilla Zero-Shot.The vanilla zero-shot prompt directly asks for the aspect terms and their sentiments, with a given input sentence.This is the major prompting method conducted in [13].</p>
<p>CoT. Chain-of-Thought (CoT) prompting encourages the model to generate a sequence of intermediate reasoning steps before arriving at a final answer, which can significantly enhance its ability to handle complex, multi-step reasoning tasks [37,38].In this study, CoT prompting extends the vanilla zero-shot approach by explicitly instructing the model to articulate its reasoning process, thereby providing greater transparency into the decision-making pathway.</p>
<p>[Task Definition] Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.</p>
<p>Step 1 Identify entities with sentiment from the text.</p>
<p>Step 2 Determine sentiment for each entity.</p>
<p>Step 3 Output: entity-sentiment pairs.</p>
<p>CoT</p>
<p>Self-Improvement Self-Debate Self-Consistency</p>
<p>Step 1 Identify entities with sentiment from the text.</p>
<p>Step 2 Determine sentiment for each entity.</p>
<p>Step 3 Explain reasoning for each sentiment.</p>
<p>Step 4</p>
<p>Output: entity-sentiment pairs with reasoning.</p>
<p>Step 1 Identify entities with sentiment from the text.</p>
<p>Step 2 Determine sentiment for each entity.</p>
<p>Step 3 Provide reasoning and review initial results.</p>
<p>Step 4</p>
<p>Output: revised entity-sentiment pairs with explanation.</p>
<p>Step 1 Identify entities with sentiment from the text.</p>
<p>Step 2 Determine sentiment for each entity.</p>
<p>Step 3 Review and refine based on feedback.</p>
<p>Step 4</p>
<p>Output: final entity-sentiment pairs with reviews.</p>
<p>Step 1 Identify entities with sentiment from the text.</p>
<p>Step 2 Determine sentiment for each entity.</p>
<p>Step 3 Run multiple iterations to ensure consistency.</p>
<p>Step 4</p>
<p>Output: entity-sentiment pairs based on majority consensus.</p>
<p>Vanilla Zero-Shot</p>
<p>LLMs Fig. 2: Prompting Strategies used for the experiments.</p>
<p>Self-Improvement.Drawing from the self-improving framework, which uses unlabeled data and self-annotation and retrieving to enhance performance in zero-shot tasks such as NER [39], we introduce a simplified self-improvement prompting strategy.The self-improvement prompting strategy is a three-step, multi-turn approach designed to refine the model's initial output.The first step involves standard zero-shot prompting to generate an initial response.In the second step, the model is asked to explain its reasoning behind the initial response, encouraging introspection and evaluation of its own outputs.Finally, the model is prompted with both its initial response and its reasoning to generate a revised, more informed answer.This iterative process helps the model to self-correct and produce revised results.</p>
<p>Self-Debate.The self-debate prompting strategy introduces a simulated dialogue [40,41] between multiple agents within the model.It begins with a zero-shot prompt to generate an initial response, as in the self-improvement approach.In the second step, the model is prompted to assume the role of a second agent tasked with critically reviewing and, if necessary, correcting the initial response.In the third step, the model acts as a third agent that evaluates both preceding outputs and engages in a simulated debate to arrive at a final decision.This approach can be iterated over multiple rounds to further explore the model's capability for self-assessment and refinement, although for efficiency, we limit this process to three steps.</p>
<p>Self-Consistency.The self-consistency prompting strategy aims to stabilize the model's outputs by running the same experiment multiple times (n iterations) with varying temperature settings.This approach leverages sampling diversity to generate multiple potential outputs, from which the most frequently occurring result is selected as the final answer, which is called majority voting.The empirical evaluation shows that self-consistency boosts the performance of CoT prompting on a few reasoning benchmarks [29].This self-consistency is also tested to have improved performance on zero-shot NER tasks [10].</p>
<p>Experimental Setup</p>
<p>Models.To conduct a comprehensive evaluation, we select a diverse range of open-source and closed-source instruction-tuned LLMs to perform the ABSA task: Llama-3.1 8B [42], Mistral 7B [43], Gemma-2 9B [44], Qwen-2.5 7B [45], Zephyr 7B [46], Phi-3.5-mini3.8B [47], Gemini-1.5 [48], Claude-3.5 [49], and GPT-4o [50].These models are chosen for their diverse architectures and tuning strategies, offering a broad assessment of both state-of-the-art open-source and proprietary LLM capabilities.</p>
<p>Decoding Temperature.For all LLMs used in our experiments, unless stated otherwise, we set the decoding temperature to 0.0, employing a greedy decoding strategy to minimize random variations in the results.In the Self-Consistency setup, where the goal is to identify the most consistent output, we experiment with a range of temperatures {0.2, 0.4, 0.6, 0.8, 1.0}, in addition to the default 0.0 setting.</p>
<p>Dataset.We use the SemEval-2016 dataset [14] to perform a comprehensive evaluation of the selected models.This dataset consists of real user reviews across eight languages, with ABSA annotations available for English (EN), French (FR), Spanish (ES), Dutch (NL), Russian (RU), and Turkish (TK).However, due to the limited size of the Turkish test set (fewer than 150 sentences), we exclude it from our evaluation, consistent with prior multilingual ABSA studies [23,24].The data statistics are shown in Appendix A. 4.</p>
<p>Baselines.Due to the limited research on multilingual or cross-lingual ABSA task in the past two years, we adopt commonly used supervised fine-tuning baselines, as used in [23,24].Specifically, we fine-tune two multilingual models, mBERT [51] and XLM-R [52], on the multilingual data set as baselines.These models provide strong points of comparison for our zero-shot and few-shot LLM prompting strategies.Detailed parameter settings for the fine-tuning process are provided in Appendix A.3.</p>
<p>Evaluation Metric.We evaluate five languages using the Micro-F1 metric on LLMs, which is well-suited for tasks requiring both aspect and sentiment prediction.In ABSA task, a prediction is considered correct only if both the aspect term and its associated sentiment match the ground truth.This ensures that the evaluation reflects the model's ability to accurately identify both components.</p>
<p>Results</p>
<p>Overall Observation</p>
<p>Methods</p>
<p>Lang.Claude-3.5Gemma-2 Gemini-1.5GPT-4o Llama-3.  1 summarizes the performance of five methods, including zero-shot, CoT, self-improvement, self-debate, and self-consistency methods on the multilingual ABSA task.For each method, we report the performance across five different languages, as well as the average performance, to provide a clearer comparison of the models' effectiveness under different approaches.For multi-turn dialogues, we select the output from the last turn as the final result.</p>
<p>Prompt Sensitivity Across Models, Methods, Languages in Performance.Different prompt designs can lead to significant performance variations.To fairly investigate the impact of this sensitivity on the multilingual ABSA task, we establish uniform performance variations for prompt evaluation for each method.For fine-grained structured outputs, the impact of prompts on performance under the same prompt is significant.Generally, we observe that simple zero-shot prompts lead to better performance than other complex prompts in most cases.In terms of overall performance, GPT-4o achieves the best average performance in all five methods, reaching an F1 score of 45 95%, which can be attributed to the stronger capabilities of the model and its use of a more diverse multilingual corpus during pre-training, leading to better handling of downstream tasks.When comparing the performance between the languages, we clearly observe that the results on English are the best, followed by Spanish and French, and the results on Russian are generally the worst.This is not surprising, as current LLMs are mostly English-centric [12] and we apply English prompts during the experiments.Additionally, the relatively higher performance on Spanish and French can be partially attributed to their linguistic similarity with English, particularly in terms of vocabulary, syntax, and shared Latin roots, which may facilitate transfer learning in multilingual settings.In contrast, Russian is typologically more distant from English, with distinct morphological and syntactic structures, which likely contributes to the lower performance.Moreover, tokenization and subword segmentation may be less optimal for languages with rich morphology like Russian, further degrading model performance.These observations highlight that cross-lingual transfer is not uniform and that language-specific properties play a crucial role in model effectiveness.Details about the prompts can be found in the Appendix A.1.</p>
<p>Challenges in CoT Reasoning for Multilingual ABSA.For single-turn dialogues, we can observe that the zero-shot method significantly outperforms the CoT method across all benchmarks (with an average 1.55% Micro-F1 difference).In multilingual ABSA task, the quality and structure of the dataset are critical to the performance of sentiment classification models, especially when we apply step-by-step reasoning methods.However, previous studies [53] have shown that the SemEval-2016 dataset [14] contains a significant number of sentences with incomplete structures, which lack explicit sentiment-bearing entities or grammatical completeness, making it difficult to find explicit evidence from the sentence.When encountering incomplete sentences, the CoT method may attempt to "force" the generation of reasoning steps, leading to over-extrapolation or reasoning errors.</p>
<p>Query Text: Yum !. Result: {"entity": "Yum","sentiment": "positive", "explanation":"The text 'Yum !' expresses a positive sentiment towards an implicit entity, likely referring to food or a dining experience.The exclamation mark further emphasizes the positive sentiment.For example, we present a representative example in Figure 3 where the sentence is composed of phrases and punctuation marks with a positive sentiment polarity.The CoT method fails to identify explicit evidence within the sentence, leading to over-reasoning and ultimately resulting in an incorrect inference.Moreover, CoT methods not only require the model to identify sentiment-polarized entities but also to generate a detailed reasoning process.However, the model often produces incorrect reasoning chains or fails to find reasoning evidence when the sentences lack clear sentiment clues, leading to entity omission.In Figure 4, we present the statistics of true positives, false positives, and false negatives for Claude-3.5 under both zero-shot and CoT methods, further illustrating our  conclusions.Compared to the zero-shot method, the CoT method exhibits a significant decrease in the number of correct predictions and over-predictions, as well as a noticeable increase in the number of entity omissions, which aligns with our hypothesis.</p>
<p>Multi-Turn vs. Single-Turn in Performance.As seen in Table 1, compared to the zeroshot single-turn dialogue method, the multi-turn dialogue method (self-improvement &amp; self-debate) exhibits a significant performance drop across all languages.The best average performance achieves an F1 score of 37.17%, which is 4.64% lower than that of the zero-shot method.The performance variation indicates that while multi-turn dialogues can enrich the interaction by allowing for more detailed exchanges, they may inadvertently lead to misunderstandings or misinterpretations of the sentiment expressed.This is particularly evident in languages where sentiment cues are heavily context-dependent.We will delve deeper into these findings and discuss the potential implications of adopting multi-turn dialogue strategies for multilingual aspect-based sentiment analysis in §5.5.</p>
<p>Self-Consistency in Different Temperatures.Our analysis of self-consistency in multilingual ABSA task reveals a decrease in performance when averaging the outputs from five different temperature settings (0 to 1) compared to using a singular temperature of 0. The average performance decreased by 5.26%, 7.26%, 5.48%, 7.29%, and 5.29% respectively in five languages compared to the initial zero-shot setting (greedy decoding, temperature=0), indicating that while the varying temperature can introduce diversity in responses, it may also lead to inconsistencies that negatively impact overall performance.These results emphasize the importance of carefully selecting temperature settings to maintain output stability in multilingual contexts.</p>
<p>Comparison with Fine-Tuning Baseline</p>
<p>The comparison between prompt-engineering methods with LLMs and fine-tuning method reveals distinct performance trends in ABSA task.As shown in Table 1, we can observe that the promptengineering method with LLMs struggles to extract fine-grained structured aspect terms and corresponding sentiment compared to fine-tuning methods.We show the fully-supervised baseline results from the mBERT and XLM-R models in Table 2, as well as the results from the bestperforming prompting method and the model (GPT-4o-CoT).We notice, even the best-performing LLM prompting method (GPT-4o-CoT) only reaches around 77% of the average performance achieved by fine-tuned models (compared to XLM-R).This performance gap becomes especially obvious in languages with complex morphological structures and domain-specific datasets such as Russian and French, where fine-tuned models, leveraging task-specific training, exhibit more consistent and reliable results.The uniqueness of prompting lies in its flexibility and adaptability, but applying it effectively in specific scenarios can be challenging.This is because prompt-engineering approaches often require carefully crafted instructions that can vary significantly depending on the task, making consistent performance harder to achieve across diverse settings.</p>
<p>The Effect of Model Size in Performance</p>
<p>To explore how model size affects performance in ABSA tasks, we conduct experiments using different sizes of the Qwen series models (Qwen2.5 with 0.5B, 1.5B, 7B, 14B, 32B, 72B).In our experimental setup, we focus on representative zero-shot and self-improvement methods in both single-turn and multi-turn dialogues.As shown in Table 3, we observe the following findings: 1) Model performance increases with size.For example, the Qwen2.5 14B model outperforms the Qwen2.5 1.5B model by 21.98% and 22.23% in average F1 score for zero-shot and self-improvement methods, respectively.2) Zero-Shot performs better overall than Self-Improvement, which aligns with the trends reported in Table 1.In conclusion, larger language models, having been exposed to more data and more complex training frameworks during pretraining, achieve better performance in multilingual ABSA tasks.This also suggests that, for specific downstream tasks, selecting a larger model with superior performance as the base model can be beneficial.Table 3: The results compared with different sizes of models under zeroshot and self-improvement settings.</p>
<p>Methods</p>
<p>Excursion: Comparison with Triplet Extraction Results</p>
<p>To evaluate the finer-grained aspect-based sentiment analysis triplet extraction task, we additionally extend an assessment of the target-aspect-sentiment detection (TASD) task using Qwen-2.5 7B and GPT-4o, which aims to extract sentiment triplets (aspect, category, sentiment), with the results reported in Table 4.We observe a performance decline across all five prompting strategies after adding the complexity of category recognition (including 12 categories in §A.5).For example, compared to the binary (aspect, sentiment) extraction task, the results on TASD task experience a 3.77% drop in F1 score under the zero-shot strategy.This can be interpreted as the performance of the model deteriorating more noticeably with increasing task complexity.Therefore, it suggests that fine-grained multilingual sentiment triplet extraction tasks require more in-depth exploration and optimization of prompt strategies.Details of the prompts can be found in §A.1.Table 4: The triplet extraction results with Qwen-2.5 7B and GPT-4o.</p>
<p>Methods</p>
<p>The Effect of Multi-Turn in Performance</p>
<p>As discussed in §5.1, we observe that the performance of both multi-turn approaches, i.e. selfimprovement and self-debate is worse than the zero-shot strategies.To better understand this outcome, we analyze the performance across different rounds.Figure 5 presents the average performance differences across all languages in the three rounds for both multi-turn setups.Overall, the selfimprovement setting yields higher average performance compared to the self-debate method, with an increase of 4.82% in the three-round average F1 score.Furthermore, in each individual round, the selfimprovement method achieves better performance compared to the self-debate method.Specifically, compared to the first round, the average F1 scores of the self-improvement and self-debate methods decreased by 1.84% and 2.75%, respectively, in the second round.Compared to the second round, the average F1 scores further decreased by 6.67% and 2.74%, respectively, in the third round.Notably, the performance degradation in the self-debate method is more pronounced across multiple dialogue rounds.Detailed performance for each language is provided in Appendix A.2. Interestingly, for all LLMs, we find that performance consistently declines with each additional round.Notably, in the self-debate setup for GPT-4o, the performance between the first and second rounds drops by approximately 10%.One possible explanation for this decline is that during the multi-turn process, when LLMs are prompted to self-correct, they tend to assume that errors exist in the prior responses, even when instructed to only revise incorrect predictions [54].This assumption may lead to unnecessary changes and performance degradation.</p>
<p>Zero-Shot Cross-Lingual Transfer with LLMs</p>
<p>In addition, we evaluate the performance of fine-tuning the LLMs on the source language (English) and then validate on target languages, to assess the zero-shot cross-lingual transfer ability of LLMs via fine-tuning.We employ the LoRA approach [55], and experiment on the open-source LLMs employed in the main experiments.For fine-tuning, we apply the base model of the LLMs.Table 5 shows the detailed results of each language.The results of different LLMs vary, with Qwen2.5-7Bachieving the highest average performance.In general, as expected, they perform better than the zero-shot prompting approaches.Looking into the single languages, the performance across languages is inconsistent, with a noticeable drop in complex morphological languages such as Russian (53.50 for Qwen2.5-7B), in comparison with Spanish (68.95 for Qwen2.5-7B).This finding is consistent with the findings in the main experiments ( §5.1) and in the supervised fine-tuning baseline results ( §5.2).</p>
<p>Few-Shot Ablation</p>
<p>In addition to the main prompting strategies described in §3, we conduct few-shot experiments using the same setup as the zero-shot experiments.This allows us to examine how including examples in the prompt affects final performance.</p>
<p>In the few-shot setup, the model is provided with n examples from the training data as contextual demonstrations.A KNN-based retrieval strategy is employed, selecting the top-n nearest examples based on the similarity to the input sentence, as computed using Sentence Transformers [56].The top n most similar sentences are included as examples, with experiments conducted using the following values for n: n ∈ [1,2,4,8,16].</p>
<p>Figure 6 shows the results of the few-shot experiments across different shot sizes for all LLMs.We observe that in most cases, as the number of examples increases, model performance improves accordingly.This supports the assumption that including semantically related examples in the prompt enhances the LLMs' inference capabilities in ABSA task.The exception is Zephyr, whose performance remains unstable and consistently lower than other models, as also noted in the main experiment results in all different setups (see Table 1).Detailed results for the few-shot experiments are presented in Table A5 in the Appendix.Fig. 6: Few-shot results across different shots.In each shot, the result is averaged across all five languages.</p>
<p>Error Cases</p>
<p>In this section, we analyze the types of errors observed during the experiments.To classify the errors, we follow the categorization proposed by [10], which also addresses token-level tasks like NER.We categorize the prediction errors in sentiment analysis into two main types: aspect term boundaries and sentiment polarity.The error types related to aspect term boundaries include Contain gold, Contained by gold, Overlap with gold, Completely-O, and Omitted mentions.The error types related to sentiment polarity include Out-of-Domain (OOD) types and Wrong types, as shown in the figure below:</p>
<p>• Out-of-Domain (OOD) types: Predicted sentiment types that are not in the predefined label set.</p>
<p>• Wrong types: Predicted sentiment types that are incorrect but still belong to the label set.</p>
<p>• Contain gold: Predicted aspects that include the correct (gold) aspects.</p>
<p>• Contained by gold: Predicted aspects that are fully contained within the gold aspects.</p>
<p>• Overlap with gold: Predicted aspects that overlap with gold aspects but do not fall into the two previous categories.• Completely-O: Predicted aspects that are entirely outside the gold aspects, yet overlap with other tokens.• OOD mentions: Predicted aspects that do not appear in input text at all.• Omitted mentions: Aspects that should have been predicted but were omitted by the model.</p>
<p>Reasoning Error:</p>
<p>The CoT and Zero-Shot methods include the unnecessary definite article "la" within the entity "comida," leading to entity mismatches.The model interprets "la comida" as a single entity rather than focusing on the actual target entity "comida."This demonstrates that the model may overly rely on syntactic structures, neglecting the need for treating entities independently and simplifying the processing in sentiment analysis tasks.</p>
<p>Type Error: Contain gold</p>
<p>Query Text: Quien sea amante de la carne tiene una carta bastante amplia para elegir, aunque ayer no tenían chuleton .Initial Response: {"entity": "carne","sentiment": "positive"}, {"entity": "chuleton","sentiment": "negative"} Commentary:{"entity": "carne","sentiment": "positive","explanation": "The phrase 'amante de la carne' suggests a strong appreciation or enjoyment of meat.The sentiment is inferred to be positive as it indicates that individuals who love meat would find the menu appealing, describing it as 'bastante amplia para elegir' (quite extensive to choose from)."},{"entity": "chuleton","sentiment": "negative", "explanation": "The mention of 'chuleton' alongside 'aunque ayer no tenían' (although yesterday they did not have) implies a lack of availability of this specific item.This indicates disappointment or a negative sentiment since the absence of a desired item is often viewed unfavorably by customers."}Second Round Response: {"entity": "carne","sentiment": "positive","explanation": "The phrase 'amante de la carne' indicates a positive sentiment, as it suggests that those who love meat have a favorable view towards the options provided.The description 'bastante amplia para elegir' indicates that there are many appealing choices on the menu, reinforcing the positive sentiment."},{"entity": "chuleton","sentiment": "neutral","explanation": "While the mention of 'chuleton' is tied to the phrase 'aunque ayer no tenían' (although yesterday they did not have), the sentiment isn't strictly negative or positive.It indicates a simple fact about availability without an explicit emotional response.The tone does not express disappointment or satisfaction, thus it is classified as neutral."}Ground True:{"entity": "carta","sentiment": "positive"}, {"entity": " chuleton","sentiment":"negative"} Query Text: La comida estuvo muy sabrosa.Zero-Shot:{"entity": "La comida","sentiment": "positive"} Self-Consistency: {"entity": "comida","sentiment": "positive"} Few-Shot:{"entity": "comida","sentiment": "positive"} CoT: {"entity": "La comida","sentiment": "positive","explanation": "The phrase 'La comida estuvo muy sabrosa' translates to 'The food was very tasty' in English.The word 'sabrosa' indicates that the food is pleasant or enjoyable, which conveys a positive sentiment toward the entity 'la comida'."}Ground True:{"entity": "comida","sentiment": "positive"} Query Text: The entire staff was extremely accomodating and tended to my every need.Initial Response: {"entity": "staff","sentiment": "positive"} Review:{"entity": "staff","sentiment": "positive","review": "The sentiment towards 'staff' is appropriately identified as positive, as the description indicates that they were 'extremely accommodating' and attentive to the needs of the person."},{"entity": "service","sentiment": "positive","review": "The overall experience described, indicated by the phrase 'tended to my every need,' reflects a positive sentiment towards the service provided by the staff."}Second Round Response: {"entity": "staff","sentiment": "positive","review": "The sentiment towards 'staff' is correctly identified as positive due to the description of them being 'extremely accommodating' and attentive to the needs of the person.This reflects a high level of service and care."},{"entity": "service","sentiment": "positive","review": "The sentiment towards 'service' is also positive.The phrase 'tended to my every need' indicates that the service provided was attentive and satisfactory, further supporting a positive sentiment overall."}Ground True:{"entity": "staff","sentiment": "positive"} Fig. 7: Case study of error increase with the proposed methods.Texts in blue are involved entities in the error cases.The items highlighted are false predictions.We use the zero-shot performance of the GPT-4o model as a representative example.Figure 8 illustrates the distribution of error types for GPT-4o under the zero-shot setting.The most frequent error types are "Completely-O" and "Omitted mentions", together accounting for over 70% of the total errors.This observation is partially consistent with the findings of [10] on NER tasks, where "Completely-O" and "Wrong types" are the most common error types.The "Completely-O" errors may result from incomplete annotations or the model's tendency to guess aspect terms based on prior knowledge.The "Omitted mentions" errors likely stem from inadequate contextual understanding.</p>
<p>As shown in Figure 7, we select three typical cases to illustrate reasoning errors, which include OOD mentions, contain gold, and wrong types.In Case 1, the aspect "service" is over-predicted during the first review, which subsequently leads to an incorrect final prediction.Moreover, despite providing a specific task description before each round of debate, the model focuses on the sentiment polarity of the entire sentence rather than the specific aspect terms.Consequently, the entire sentence is generalized as "service is highly satisfactory", resulting in an OOD prediction.We hypothesize that the model suffers from catastrophic forgetting during complex contextual reasoning.In Case 2, the CoT method first translates the original sentence back into English before making predictions, focusing heavily on analyzing the sentence's structural features.This leads to an incorrect identification of "La comida" as an aspect term.We can explain this as the CoT method places too much emphasis on syntactic structure analysis rather than accurately identifying aspect boundaries.In Case 3, after two rounds of self-improvement corrections, the model incorrectly revises a previously correct prediction, changing the sentiment polarity of "chuleton" to neutral.We suspect that the model encounters ambiguity in the implicit sentiment reasoning process for the aspect "chuleton" and lacks confidence during self-correction, ultimately leading to an incorrect prediction type.</p>
<p>Overall, the errors in the selected cases mentioned can be attributed to the following reasons: neglecting the reasoning of entity boundaries during the inference process, error accumulation from multiple rounds of dialogue, and ambiguity in implicit sentiment reasoning.</p>
<p>Conclusion</p>
<p>In this study, we conduct an empirical evaluation of zero-shot multilingual ABSA using different LLMs, exploring their performance across five different prompting methods.Inspired by LLMs' strong reasoning capabilities in logical and arithmetic tasks, we apply these strategies and assess their effectiveness across five languages: English, French, Spanish, Dutch, and Russian, as well as domainspecific datasets.Our findings indicate that LLMs do not surpass fully-supervised multilingual models in the ABSA task.Performance varies significantly across languages, with higher-resource languages like English yielding better results.We also find that closed-source models generally outperform open-source ones, and simpler zero-shot prompts prove more effective than more complex prompting strategies.These insights suggest that while LLMs show potential for multilingual tasks, their effectiveness remains highly dependent on the task, language, and model configuration, highlighting the need for further exploration to optimize LLMs for cross-lingual tasks like ABSA.Zero-Shot &amp; Self-Consistency Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with a sentiment that can be [positive, negative, or neutral].Your task is to: 1. Identify the entity with a sentiment mentioned in the given text.2. For each identified entity, determine the sentiment in the label set (positive, negative, or neutral).3. The output should be a list of dictionaries, where each dictionary contains the entity with a sentiment and its corresponding sentiment.If there are no sentiment-bearing entities in the text, the output should be an empty list.Example Output format: ["entity": "<entity>", "sentiment": "<label>"] Please return the final output based on the following text in json format.</p>
<p>CoT</p>
<p>Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with a sentiment that can be [positive, negative, or neutral].Your task is to: 1. Identify the entity with a sentiment mentioned in the given text.2. For each identified entity, determine the sentiment in the label set (positive, negative, or neutral).3. Provide a reasoning process for how you identified the entities and assigned their sentiments.4. The output should be a list of dictionaries, where each dictionary contains the entity with a sentiment and its reasoning process.If there are no sentiment-bearing entities in the text, the output should be an empty list.Example Output format: ["entity": "<entity>", "sentiment": "<label>", "Explanation": "<reasoning process>"] Please return the final output base on the following text in json format.</p>
<p>Few-Shot</p>
<p>Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with a sentiment that can be [positive, negative, or neutral].Your task is to: 1. Identify the entity with a sentiment mentioned in the given text.2. For each identified entity, determine the sentiment in the label set (positive, negative, or neutral).3. The output should be a list of dictionaries, where each dictionary contains the entity with a sentiment and its corresponding sentiment.If there are no sentiment-bearing entities in the text, the output should be an empty list.Example Output format: ["entity": "<entity>", "sentiment": "<label>"] Here are {top k} similar sentences from the training set: {similar sentences and corresponding entity with its sentiment} Please return the final output based on the following text in json format.Example Output format: ["entity": "<entity>", "category": "<category>", "sentiment": "<label>"] Please return the final output based on the following text in json format.Your task is to: 1. Identify entities in the text, along with their categories and sentiments.2. For each identified entity, assign a category from the provided category list.3. Determine the sentiment for each entity as one of [positive, negative, neutral].4. Provide a reasoning process for how you identified the entities, categories, and sentiments. 5.Return the results as a list of dictionaries, each containing the entity, category, and sentiment.If no entities are found, return an empty list.</p>
<p>Example Output format: ["entity": "<entity>", "category": "<category>", "sentiment": "<label>", "Explanation": "<reasoning process>"] Please return the final output based on the following text in json format.Your task is to: 1. Identify the entity with a sentiment mentioned in the given text.2. For each identified entity, determine the sentiment in the label set (positive, negative, or neutral).3. The output should be a list of dictionaries, where each dictionary contains the entity with a sentiment and its corresponding sentiment.If there are no sentiment-bearing entities in the text, the output should be an empty list.Example Output format: Round 1: ["entity": "<entity>", "category": "<category>", "sentiment": "<label>"] Please return the final output based on the following text in json format.["entity": "<entity>", "category": "<category>", "sentiment": "<label>","Review":"<review>"] Please return the final output based on the above sentence in json format.</p>
<p>Round 3</p>
<p>Aspect-Based Sentiment Analysis (ABSA) requires identifying specific entities mentioned in a text and determining the sentiment expressed toward each entity.["entity": "<entity>", "category": "<category>", "sentiment": "<label>","Review":"<review>"] Please return the final output based on the above sentence in json format.</p>
<p>Fig. 3 :Fig. 4 :
34
Fig. 3: Case study of an incomplete sentence in CoT scenario.Results are extracted from the Claude-3.5output.</p>
<p>Fig. 5 :
5
Fig. 5: Multi-turn performance across different rounds in different models.In each model, The result is averaged across all the five languages.</p>
<p>Fig. 8 :
8
Fig. 8: Percentage of different error types with GPT-4o under the zero-shot prompting strategy.The percentages are averaged across the five languages.</p>
<p>Fig. A1 :
A1
Fig. A1: Single-turn prompts used in this paper for tuple extraction.</p>
<p>CoT</p>
<p>Aspect-Based Sentiment Analysis (ABSA) requires identifying specific entities mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with: A category from the list: [FOOD#QUALITY, AMBIENCE#GENERAL, LOCATION#GENERAL, SERVICE#GENERAL, RESTAURANT#GENERAL, DRINKS#QUALITY, RESTAU-RANT#MISCELLANEOUS, DRINKS#PRICES, DRINKS#STYLE OPTIONS, FOOD#PRICES, FOOD#STYLE OPTIONS, RESTAURANT#PRICES] A sentiment: [positive, negative, neutral].</p>
<p>Fig. A2 :
A2
Fig. A2: Single-turn prompts used in this paper for triplet extraction.</p>
<p>Round 2
2
Aspect-Based Sentiment Analysis (ABSA) requires identifying specific entities mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with: A category from the list:[FOOD#QUALITY, AMBIENCE#GENERAL, LOCATION#GENERAL, SERVICE#GENERAL, RESTAURANT#GENERAL, DRINKS#QUALITY, RESTAU-RANT#MISCELLANEOUS,DRINKS#PRICES, DRINKS#STYLE OPTIONS, FOOD#PRICES, FOOD#STYLE OPTIONS, RESTAURANT#PRICES] A sentiment: [positive, negative, neutral].The source sentence is: {sentence}.The first response result: {previous response} Please review and comment on the following response.Provide corrections if necessary or add more details to improve the result.Example Output format: Round 2:</p>
<p>Each entity is associated with: A category from the list: [FOOD#QUALITY, AMBIENCE#GENERAL, LOCATION#GENERAL, SERVICE#GENERAL, RESTAURANT#GENERAL, DRINKS#QUALITY, RESTAU-RANT#MISCELLANEOUS, DRINKS#PRICES, DRINKS#STYLE OPTIONS, FOOD#PRICES, FOOD#STYLE OPTIONS, RESTAURANT#PRICES] A sentiment: [positive, negative, neutral].The source sentence is: {sentence}.The first response result: {first response}.The first commentary result: {first commentary}.Based on the initial response and commentary, please further debate and refine the analysis.If there are any conflicting opinions or uncertainties, resolve them (both entity and review) and provide a more detailed and accurate response.Example Output format: Round 3:</p>
<p>Fig. A6 :
A6
Fig. A6: Self-debate prompts used in this paper for triplet extraction.</p>
<p>Table 1 :
1
Performance comparison of various methods on different languages.
1MistralPhi-3.5Qwen-2.5ZephyrAvg.</p>
<p>CoT 59.84 54.01 52.28 53.06 44.88 52.81
ENESFRNLRUAvg.mBERT66.68 67.97 62.78 56.31 59.49 62.65XLM-R72.92 72.74 67.26 64.72 66.66 68.86GPT-4o-</p>
<p>Table 2 :
2
The fully-supervised baseline results, compared with GPT-4o-CoT results.</p>
<p>Table 5 :
5
Performance of different LLMs in LoRA fine-tuning.</p>
<p>Table A1 :
A1
Fine-tuning hyperparameter settings.
ParameterValueEpoch20Batch size[8, 16]Learning rate (BERT)5e-5Learning rate (XLM-R)2e-5Hidden size768Dropout rate0.1Max steps[2000,2500]Adam epsilon1e-8Warm factor0.1EN FRESNL RUTrain# S 2000 1664 2070 1722 3655 # A 1743 1641 1856 1231 3077Test# S 676 668 881 575 1209 # A 612 650 713 373 949</p>
<p>Table A2 :
A2
Statistic of the original dataset.
Category = ['FOOD#QUALITY', 'AMBIENCE#GENERAL', 'LOCATION#GENERAL','SERVICE#GENERAL','RESTAURANT#GENERAL','DRINKS#QUALITY','RESTAU-RANT#MISCELLANEOUS', 'DRINKS#PRICES', 'DRINKS#STYLE OPTIONS', 'FOOD#PRICES','FOOD#STYLE OPTIONS', 'RESTAURANT#PRICES']
Acknowledgements.The authors would like to respect and thank all reviewers for their constructive and helpful review.• Ethics approval.This atricle does not contain any studies with human participants or animals performed by any of the authors.• Data availability.Data sharing is not applicable to this article as no dataset was generated or analyzed during the current study.The dataset used in this paper is public and available.Appendix A Details of Prompts and ExperimentsA.1 PromptWe show all of our prompts for six methods.The prompts are in FigureA1-A6.A.2 Detailed ResultsThe details of the overall performance under the self-improvement, self-debate, and few-shot settings are shown in TableA4, A3 and A5.A.3 Fine-Tuning Parameter SettingThe details of the parameter setting under supervised methods are shown in TableA1.A.4 Dataset DetailsThe details of the dataset statistics in each language are shown in TableA2.# S and # A denote the number of sentences and aspect terms in different sets, respectively.A.5 Detailed Aspect CategoriesWe present the detailed aspect categories for the dataset in the previous section ( §5.4).Round 1Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with a sentiment that can be [positive, negative, or neutral].Your task is to:1. Identify the entity with a sentiment mentioned in the given text.2. For each identified entity, determine the sentiment in the label set (positive, negative, or neutral).3. The output should be a list of dictionaries, where each dictionary contains the entity with a sentiment and its corresponding sentiment.If there are no sentiment-bearing entities in the text, the output should be an empty list.Example Output format: ["entity": "<entity>", "sentiment": "<label>"] Please return the final output based on the following text in json format.Round 2Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with a sentiment that can be [positive, negative, or neutral].Here is the sentence: {sentence}.You have classified the sentiment of the entities in this sentence.Here is your initial result: {initial result}.Please explain why you classified them in this way.Example Output format: ["entity": "<entity>", "sentiment": "<label>", "Explanation": "<reasoning process>"] Please return the final output based on the above sentence in json format.Round 3Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with a sentiment that can be [positive, negative, or neutral].Here is the sentence: {sentence}.You have given a sentiment classification for the entities in this sentence.Here is your initial result: {initial result}.Here is your explanation: {explanation result}.Please recheck your classification and explanation.If you find any errors or better classifications, please update your response accordingly.Example Output format: ["entity": "<entity>", "sentiment": "<label>", "Explanation": "<reasoning process>"] Please return the final output based on the above sentence in json format.Example Output format: ["entity": "<entity>", "category": "<category>", "sentiment": "<label>"] Please return the final output based on the following text in json format.Round 2Aspect-Based Sentiment Analysis (ABSA) requires identifying specific entities mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with: A category from the list:Here is the sentence: {sentence}.You have classified the sentiment and categories of the entities in this sentence.Here is your initial result: {initial result}.Please explain why you classified them in this way.Example Output format: ["entity": "<entity>", "category": "<category>", "sentiment": "<label>", "Explanation": "<reasoning process>"] Please return the final output based on the above sentence in json format.Round 3Aspect-Based Sentiment Analysis (ABSA) requires identifying specific entities mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with: A category from the list: [FOOD#QUALITY, AMBIENCE#GENERAL, LOCATION#GENERAL, SERVICE#GENERAL, RESTAURANT#GENERAL, DRINKS#QUALITY,RESTAU-RANT#MISCELLANEOUS, DRINKS#PRICES, DRINKS#STYLE OPTIONS, FOOD#PRICES, FOOD#STYLE OPTIONS, RESTAURANT#PRICES] A sentiment: [positive, negative, neutral].Here is the sentence: {sentence}.You have given a sentiment and category classification for the entities in this sentence.Here is your initial result: {initial result}.Here is your explanation: {explanation result}.Please recheck your classification and explanation.If you find any errors or better classifications, please update your response accordingly.Example Output format: ["entity": "<entity>", "category": "<category>", "sentiment": "<label>", "Explanation": "<reasoning process>"] Please return the final output based on the above sentence in json format.Round 1Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with a sentiment that can be [positive, negative, or neutral].Your task is to: 1. Identify the entity with a sentiment mentioned in the given text.2. For each identified entity, determine the sentiment in the label set (positive, negative, or neutral).3. The output should be a list of dictionaries, where each dictionary contains the entity with a sentiment and its corresponding sentiment.If there are no sentiment-bearing entities in the text, the output should be an empty list.Example Output format: Round 1: ["entity": "<entity>", "sentiment": "<label>"] Please return the final output based on the following text in json format.Round 2Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with a sentiment that can be [positive, negative, or neutral].The source sentence is: {sentence}.The first response result: {previous response} Please review and comment on the following response.Provide corrections if necessary or add more details to improve the result.Example Output format: Round 2: ["entity": "<entity>", "sentiment": "<label>","Review":"<review>"] Please return the final output based on the above sentence in json format.Round 3Aspect-Based Sentiment Analysis (ABSA) involves identifying specific entity (such as a person, product, service, or experience) mentioned in a text and determining the sentiment expressed toward each entity.Each entity is associated with a sentiment that can be [positive, negative, or neutral].The source sentence is: {sentence}.The first response result: {first response}.The first commentary result: {first commentary}.Based on the initial response and commentary, please further debate and refine the analysis.If there are any conflicting opinions or uncertainties, resolve them (both entity and review) and provide a more detailed and accurate response.Example Output format: Round 3: ["entity": "<entity>", "sentiment": "<label>","Review":"<review>"] Please return the final output based on the above sentence in json format.
. Round Lang, Claude-3</p>
<p>. Gemma - , 2 Gemini-1.5 GPT-4o Llama-3.1 Mistral Phi-3.5 Qwen-2</p>
<p>. Zephyr Avg, </p>
<p>. Avg, </p>
<p>. Avg, </p>
<p>. Avg, </p>
<p>Table A3: Detailed performance comparison of multiple runs in self-improvement. </p>
<p>. Round Lang, Claude-3</p>
<p>. Gemma - , 2 Gemini-1.5 GPT-4o Llama-3.1 Mistral Phi-3.5 Qwen-2</p>
<p>. Zephyr Avg, </p>
<p>. Avg, </p>
<p>. Avg, </p>
<p>Table A4: Detailed performance comparison of multiple runs in self-debate. </p>
<p>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. B Liu, 10.2200/S00416ED1V01Y201204HLT0162012</p>
<p>Characterising and predicting persistent high-cost utilisers in healthcare: a retrospective cohort study in Singapore. S Ng, N Rahman, I Ang, S Sridharan, S Ramachandran, D Wang, A Khoo, C Tan, M Feng, S Toh, e031622.10.1136/bmjopen-2019-031622BMJ Open. 1012020 Jan 06</p>
<p>Genotype-determined egfr-rtk heterodimerization and its effects on drug resistance in lung cancer treatment revealed by molecular dynamics simulations. M Zhu, D D Wang, H Yan, BMC molecular and cell biology. 221342021</p>
<p>Processing of electronic medical records for health services research in an academic medical center: methods and validation. N Rahman, D D Wang, S H Ng, .-X Ramachandran, S Sridharan, S Khoo, A Tan, C S Goh, W.-P Tan, X Q , JMIR Medical Informatics. 64109332018</p>
<p>A joint training dual-mrc framework for aspect based sentiment analysis. Y Mao, Y Shen, C Yu, L Cai, 10.1609/aaai.v35i15.17597Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>Towards generative aspect-based sentiment analysis. W Zhang, X Li, Y Deng, L Bing, W Lam, 10.18653/v1/2021.acl-short.64Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. C Zong, F Xia, W Li, R Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics20212</p>
<p>Improving llm prompting with ensemble of instructions: A case study on sentiment analysis. V Tran, T Matsui, JSAI International Symposium on Artificial Intelligence. Springer2024</p>
<p>A field guide to automatic evaluation of llm-generated summaries. T A Schaik, B Pugh, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Llms in e-commerce: a comparative analysis of gpt and llama models in product review evaluation. K I Roumeliotis, N D Tselikas, D K Nasiopoulos, Natural Language Processing Journal. 61000562024</p>
<p>Empirical study of zero-shot NER with ChatGPT. T Xie, Q Li, J Zhang, Y Zhang, Z Liu, H Wang, 10.18653/v1/2023.emnlp-main.493Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>ToPro: Token-level prompt decomposition for cross-lingual sequence labeling tasks. B Ma, E Nie, S Yuan, H Schmid, M Färber, F Kreuter, H Schuetze, Proceedings of the 18th Conference of the European Chapter. Long Papers. Y Graham, M Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241</p>
<p>E Nie, S Yuan, B Ma, H Schmid, M Färber, F Kreuter, H Schütze, Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models. 2024</p>
<p>Sentiment analysis in the era of large language models: A reality check. W Zhang, Y Deng, B Liu, S Pan, L Bing, 10.18653/v1/2024.findings-naacl.246Findings of the Association for Computational Linguistics: NAACL 2024. K Duh, H Gomez, S Bethard, Mexico City; MexicoAssociation for Computational Linguistics2024</p>
<p>M Pontiki, D Galanis, H Papageorgiou, I Androutsopoulos, S Manandhar, M Al-Smadi, M Al-Ayyoub, Y Zhao, B Qin, O De Clercq, V Hoste, M Apidianaki, X Tannier, N Loukachevitch, E Kotelnikov, N Bel, S M Jiménez-Zafra, G Eryigit, S Carpuat, M Cer, D Jurgens, D Nakov, P , 10.18653/v1/S16-1002Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). T Zesch, the 10th International Workshop on Semantic Evaluation (SemEval-2016)San Diego, CaliforniaAssociation for Computational Linguistics2016SemEval-2016 task 5: Aspect based sentiment analysis</p>
<p>A cross-lingual joint aspect/sentiment model for sentiment analysis. Z Lin, X Jin, X Xu, W Wang, X Cheng, Y Wang, Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. the 23rd ACM International Conference on Conference on Information and Knowledge Management2014</p>
<p>Instance selection improves cross-lingual model training for fine-grained sentiment analysis. R Klinger, P Cimiano, 10.18653/v1/K15-1016Proceedings of the Nineteenth Conference on Computational Natural Language Learning. the Nineteenth Conference on Computational Natural Language LearningBeijing; ChinaAssociation for Computational Linguistics2015</p>
<p>Aspect-level cross-lingual sentiment classification with constrained SMT. P Lambert, 10.3115/v1/P15-2128Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. C Zong, M Strube, the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing; ChinaAssociation for Computational Linguistics20152</p>
<p>Exploring distributional representations and machine translation for aspect-based cross-lingual sentiment classification. J Barnes, P Lambert, T Badia, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. Y Matsumoto, R Prasad, COLING 2016, the 26th International Conference on Computational Linguistics: Technical PapersOsaka, Japan2016The COLING 2016 Organizing Committee</p>
<p>Exploring zero-shot crosslingual aspect-based sentiment analysis using pre-trained multilingual language models. K T Phan, .-K , Ngoc Hao, D Thin, D V Luu-Thuy, N Nguyen, 10.1109/MAPR53640.2021.95852422021 International Conference on Multimedia Analysis and Pattern Recognition (MAPR). 2021</p>
<p>BERT post-training for review reading comprehension and aspect-based sentiment analysis. H Xu, B Liu, L Shu, P Yu, 10.18653/v1/N19-1242Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Knowledge-enabled bert for aspect-based sentiment analysis. Knowledge-Based Systems. A Zhao, Y Yu, 10.1016/j.knosys.2021.1072202021227</p>
<p>Cross-lingual aspect-based sentiment analysis with aspect term code-switching. W Zhang, R He, H Peng, L Bing, W Lam, 10.18653/v1/2021.emnlp-main.727Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. M Moens, X Huang, L Specia, S W Yih, the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican RepublicAssociation for Computational Linguistics7-11 November, 2021. 2021</p>
<p>Cl-xabsa: Contrastive learning for cross-lingual aspect-based sentiment analysis. N Lin, Y Fu, X Lin, D Zhou, A Yang, S Jiang, 10.1109/TASLP.2023.3297964IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023</p>
<p>Cross-lingual aspect-based sentiment analysis with aspect term code-switching. W Zhang, R He, H Peng, L Bing, W Lam, 10.18653/v1/2021.emnlp-main.727Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. M.-F Moens, X Huang, L Specia, S W Yih, -T, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Rating text classification with weighted negative supervision on classifier layer. Z Jun, Q Longlong, S Fanfan, H Yueshun, T Hai, H Yanxiang, Chinese Journal of Electronics. 3262023</p>
<p>A model for chinese named entity recognition based on global pointer and adversarial learning. Y Zhang, J Li, Y Xin, X Zhao, Y Liu, Chinese Journal of Electronics. 3242023</p>
<p>S Wang, X Sun, X Li, R Ouyang, F Wu, T Zhang, J Li, G Wang, arXiv:2304.10428Gpt-ner: Named entity recognition via large language models. 2023arXiv preprint</p>
<p>Large language models for aspect-based sentiment analysis. P F Simmering, P Huoviala, 2023</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, 2023</p>
<p>In-context example retrieval from multi-perspectives for few-shot aspect-based sentiment analysis. Q Wang, H Xu, K Ding, B Liang, R Xu, N Calzolari, M.-Y Kan, V Hoste, A Lenci, S Sakti, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). N Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, Italia2024</p>
<p>Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert. Q Zhong, L Ding, J Liu, B Du, D Tao, </p>
<p>Is chatgpt a good sentiment analyzer?. Z Wang, Q Xie, Z Ding, Y Feng, R Xia, 2023a preliminary study</p>
<p>Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors. R Han, T Peng, C Yang, B Wang, L Liu, X Wan, </p>
<p>Llms to the moon? reddit market sentiment analysis with large language models. X Deng, V Bashlovkina, F Han, S Baumgartner, M Bendersky, </p>
<p>True few-shot learning with language models. E Perez, D Kiela, K Cho, M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, Vaughan, Advances in Neural Information Processing Systems. J W , Curran Associates, Inc202134</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Y Lu, M Bartolo, A Moore, S Riedel, P Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. S Muresan, P Nakov, A Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems. NIPS '22. the 36th International Conference on Neural Information Processing Systems. NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. B Wang, S Min, X Deng, J Shen, Y Wu, L Zettlemoyer, H Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Self-improving for zero-shot named entity recognition with large language models. T Xie, Q Li, Y Zhang, Z Liu, H Wang, 10.18653/v1/2024.naacl-short.49Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City; MexicoAssociation for Computational Linguistics20242</p>
<p>Rethinking the bounds of LLM reasoning: Are multi-agent discussions the key?. Q Wang, Z Wang, Y Su, H Tong, Y Song, 10.18653/v1/2024.acl-long.331Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Exploring collaboration mechanisms for LLM agents: A social psychology view. J Zhang, X Xu, N Zhang, R Liu, B Hooi, S Deng, 10.18653/v1/2024.acl-long.782Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>A I , Meta , Llama 3.1 Model Card. 2024</p>
<p>. A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, L R Lavaud, M.-A Lachaux, P Stock, T L Scao, T Lavril, T Wang, T Lacroix, W E Sayed, 2023Mistral 7B</p>
<p>Gemma 2: Improving Open Language Models at a Practical Size. G Team, 2024</p>
<p>Q Team, Qwen2.5: A Party of Foundation Models. 2024</p>
<p>L Tunstall, E Beeching, N Lambert, N Rajani, K Rasul, Y Belkada, S Huang, L Werra, C Fourrier, N Habib, N Sarrazin, O Sanseviero, A M Rush, T Wolf, Zephyr: Direct Distillation of LM Alignment. 2023</p>
<p>Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone. 2024Microsoft</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. G Team, 2024</p>
<p>Claude 3.5 sonnet model card addendum. Anthropic, September 2024AnthropicTechnical Report</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Unsupervised cross-lingual representation learning at scale. A Conneau, K Khandelwal, N Goyal, V Chaudhary, G Wenzek, F Guzmán, E Grave, M Ott, L Zettlemoyer, V Stoyanov, 10.18653/v1/2020.acl-main.747Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Unsupervised sentence representation learning with frequency-induced adversarial tuning and incomplete sentence filtering. B Wang, X Li, Z Yang, Y Guan, J Li, S Wang, 10.1016/j.neunet.2024.106315Neural Networks. 1752024</p>
<p>MT-bench-101: A fine-grained benchmark for evaluating large language models in multiturn dialogues. G Bai, J Liu, X Bu, Y He, J Liu, Z Zhou, Z Lin, W Su, T Ge, B Zheng, W Ouyang, 10.18653/v1/2024.acl-long.401Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>LoRA: Low-rank adaptation of large language models. E J Hu, Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, International Conference on Learning Representations. 2022</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERT-networks. N Reimers, I Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). K Inui, J Jiang, V Ng, X Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong; ChinaAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>
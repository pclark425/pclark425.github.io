<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8943 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8943</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8943</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-182c7b40ff7560a5545764814338f55a2098e441</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/182c7b40ff7560a5545764814338f55a2098e441" target="_blank">Reinforced Self-Training (ReST) for Language Modeling</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.</p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8943.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8943.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforced Self-Training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative generate-and-filter algorithm that alternates between (Grow) sampling many outputs from the current language policy and (Improve) filtering those outputs with a learned reward model and fine-tuning the policy on high-reward samples using offline RL or supervised losses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer encoder-decoder (task-dependent sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Minor modifications of standard Transformer encoder-decoder architectures: tiny (IWSLT: model dim 512, FF 1024, 4 heads, 6 layers), base (WMT: model dim 512, FF 2048, 8 heads, 6 layers), and a larger finetune variant (Web Domain: model dim 1024, FF 8192, 16 heads, 6 layers). Vocabulary size 32k, generation length <=128.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ReST (Grow / Improve iterative self-training)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-stage loop: Grow — sample N candidate outputs per input from the model; score each candidate with a learned reward model; Improve — filter candidates above a threshold τ and fine-tune the policy on the filtered set using an offline loss (typically BC / NLL, or other offline RL losses). Multiple Improve steps per Grow use increasing thresholds to produce progressively higher-quality data; then optionally repeat Grow with the improved policy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (IWSLT 2014 De→En, WMT 2020 Zh→En, internal Web Domain En→Zh)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sequence-to-sequence translation benchmarks with held-out validation/test sets; evaluation primarily by a learned reference-free reward model (Metric X) and human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Examples from reported results: IWSLT (avg reward scale 0-100): BC (G=0,I=0)=70.9; ReST (G=1,I=4)=77.8; ReST (G=2,I=3)=83.1. Online PPO baseline: 71.6. Best-of-N: after 3 Improve steps and N=200 ReST achieved reward 1.0 (normalized). Human evaluations: all ReST variants scored higher than BC in human rater scores (no single absolute numeric human-score reported in main table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Supervised BC baseline: average reward 70.9 (IWSLT example) and lower human evaluation scores than ReST; Online PPO achieved reward ~71.6 but suffered BLEU drop (-~8 BLEU) indicating potential reward hacking.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Generate-then-filter: uses prompt-conditioned sampling (tempered softmax with temperature 0.8) to create candidate outputs, a learned reward model to score candidates, threshold-based filtering, then supervised fine-tuning (NLL/BC) or offline-RL losses on filtered data. Data-generation (Grow) is decoupled from policy updates (Improve).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: consistent increases in average reward model scores across datasets with each Improve step (plots and Table 1); concrete improvements: BC→ReST (G=1,I=4) +6.9 points (70.9→77.8) on IWSLT; further gains from a second Grow (e.g., +5.3 points). Human eval: ReST variants outperformed BC in side-by-side human ratings (reported as positive human eval diffs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: reward model is an imperfect proxy of human preferences (mismatch between reward gains and human ranking); reward model generalization degrades as policy drifts from behavior data and can exhibit delusions (e.g., repetition sometimes increases reward); risk of overfitting to the reward model with repeated Grow steps; BC loss can overfit to reward-model artifacts; online RL showed reward hacking and deteriorated BLEU, a failure mode ReST partly avoids but still subject to reward-model weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Direct comparisons in paper: ReST outperforms BC (supervised) and several offline RL losses (GOLD, BVMPO, DAC) for reward improvement; BC loss within ReST often performed best. ReST with multiple Improve steps outperforms online PPO given the same generated-data budget. RAFT is described as a special case (single Improve per Grow). Compared qualitatively to STAR, Impossible Distillation, SIL and Expert Iteration in related work; ReST is characterized as compute-efficient and able to leverage exploration plus rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported ablations: loss choices (BC, GOLD, BVMPO) — BC performed best across settings; thresholding strategies — global thresholds, per-source percentiles, and interpolated mean/max thresholds give similar trends (higher percentile/γ yields better reward up to saturation, e.g. percentile p>90 saturates); Best-of-N experiments show ReST benefits from Best-of-N sampling (ReST with small N matches BC with large N). Q-learning/decision-transformer style approaches did not improve over BC (negative results).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforced Self-Training (ReST) for Language Modeling', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8943.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8943.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward-ranked Fine-Tuning (RAFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concurrent method referenced as a special case of ReST that ranks generated candidates by reward and fine-tunes on top-ranked samples, using a single Improve step per Grow step and quantile-based filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reward ranked finetuning for generative foundation model alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified generative foundation models (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced work (Dong et al., 2023) applies to generative foundation models; exact model sizes not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>RAFT (single-step reward-ranked fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate candidates with the model, rank candidates by a reward model, keep top-quantile as high-quality data, fine-tune the model on that subset (one Improve per Grow).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various generative tasks (as reported by the RAFT paper) — referenced here in context of language modeling alignment</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RAFT was described as applicable to language modeling and image generation tasks; in this paper RAFT is noted as concurrent work reporting improvements over BC and PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported in this paper beyond the statement that RAFT reported improvements over BC and PPO (citation to Dong et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Generate, rank by learned reward model, select fixed quantile threshold per Grow, then fine-tune once.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited: RAFT reported improvements over BC and PPO on various generative tasks (paper cites RAFT's claims but does not reproduce RAFT experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper; RAFT is characterized as a single-Improve special case of ReST (hence subject to reward-model generalization and potential overfitting similar to ReST).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper frames RAFT as a particular case of ReST (single Improve per Grow) and contrasts RAFT's fixed-quantile filtering with ReST's multi-step increasing-threshold Improve schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforced Self-Training (ReST) for Language Modeling', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8943.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8943.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STAR (Bootstrapping reasoning with reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative method that bootstraps reasoning: fine-tune on a small set of rationales, sample rationales+answers from the model, filter generated answers by correctness, and iteratively improve.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STAR: Bootstrapping reasoning with reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified language models in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced approach (Zelikman et al., 2022) uses LMs and their rationale+answer outputs to iteratively fine-tune; exact model sizes not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>STAR (iterative rationale sampling + filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iteratively sample chains-of-thought (rationales) and answers from the model, filter generated outputs by correctness, and fine-tune the model on filtered high-quality rationale+answer pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning / math problems (as in cited STAR work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Complex reasoning tasks solvable with chain-of-thought style rationales; iterative bootstrapping increases accuracy per the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported in this paper (only referenced as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Generate rationales+answers, use correctness filtering, fine-tune on positives (generate-then-filter-then-finetune).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as related work where iterative sampling and filtering improved reasoning performance (paper does not reproduce/star experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed here; referenced as similar in spirit to ReST but specialized to rationale/answer pairs and correctness-based filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared conceptually: STAR performs iterative sample-filter-finetune on reasoning tasks, ReST is more general and applies to conditional generation and continuous reward scores.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforced Self-Training (ReST) for Language Modeling', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8943.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8943.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Impossible Distillation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impossible Distillation (dataset generation from suboptimal models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method that generates datasets by sampling from a suboptimal model and filters low-quality examples to produce a higher-quality dataset and model (used for summarization/paraphrasing).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced experimental setups for summarization/paraphrasing tasks; exact model details are from the cited work (Jung et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Impossible Distillation (generate-and-filter dataset distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single Grow + Improve: sample candidate outputs from a weaker model, filter or select high-quality outputs to create a new training set, and train a stronger model on that set.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization and paraphrasing (as in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Text generation tasks where dataset quality can be boosted by selecting good outputs from a suboptimal model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported here; cited as an approach similar to ReST with one Grow and one Improve step.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Generate candidate outputs from a model, filter by quality, then train on the filtered synthetic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited result: the referenced work reports that dataset generation+filtering can yield improved datasets and models; this paper treats it as analogous to ReST with single Grow/Improve.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed here; authors note ReST generalizes this approach and that repeated Grow steps risk overfitting to reward models in general.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Characterized as corresponding to ReST with a single Grow and single Improve; ReST extends this by allowing multiple Improve steps and multiple Grow cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforced Self-Training (ReST) for Language Modeling', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8943.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8943.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-training (noisy student / bootstrapping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Semi-supervised approach where the model generates labels for unlabeled inputs and is retrained on its own high-confidence predictions; ReST's Improve step resembles self-training but augmented with reward filtering and repeated Grow.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LM / task-specific models (classical)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classical self-training methods applied historically across NLP and vision; not instantiated as a distinct model in this paper beyond conceptual similarity to ReST's Improve.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-training (generate self-labels and retrain)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate candidate outputs for unlabeled or existing inputs and retrain on high-confidence/self-labeled examples (ReST differs by generating exploration data and using reward-model filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General semi-supervised learning (applied historically to translation, classification, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve a model by augmenting training data with model-generated labels on unlabeled inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not directly reported; ReST extends self-training and demonstrates quantitative gains over BC baselines in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>N/A for this mention (historical baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Generate pseudo-labels, filter (often by confidence), then fine-tune on augmented dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as established method with prior empirical success (e.g., noisy student, He et al. 2019); ReST is said to be similar but integrates rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes self-training can overfit and that the quality of generated data is crucial; ReST attempts to mitigate with reward-based filtering and repeated Improve steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Self-training vs ReST: ReST additionally generates exploration data and uses a reward model for filtering and can iterate Grow/Improve, while classic self-training typically lacks reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforced Self-Training (ReST) for Language Modeling', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8943.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8943.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Imitation Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL approach that trains the agent to reproduce its historically high-reward trajectories by filtering the replay buffer and learning only from successful episodes; ReST is related conceptually but applied in a generative offline setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Actor-critic RL agents (in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In the original SIL work, off-policy actor-critic agents reproduce good past behavior; in this paper SIL is referenced conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Imitation Learning (filter high-return trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Filter past trajectories for high returns and train the policy to imitate them; analogous to ReST's filtering of high-reward generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RL control tasks in original SIL work; here referenced conceptually for generative models</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learning from own successful past experiences by imitation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported here (conceptual mention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Replay-buffer filtering + imitation objective (SIL); ReST uses reward-model filtering plus offline imitation or RL objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites SIL as related: both filter poor examples and train on high-quality behaviors; ReST differs in being agnostic to value functions and targeted at generative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper; original SIL limitations (dependence on quality and diversity of stored trajectories) are analogous to ReST's dependence on generated-data quality and reward-model reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>ReST vs SIL: ReST does not require a value function to filter and is applied to offline generative settings; SIL requires value estimates and is an RL algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforced Self-Training (ReST) for Language Modeling', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8943.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8943.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert Iteration (EI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert Iteration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A policy-iteration style method that alternates planning (to produce improved trajectories) and generalization (to learn a policy from the planned trajectories); related to ReST's Grow/Improve decomposition but EI typically uses planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Expert iteration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Planning + policy-learning frameworks (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>EI frameworks use a planning/search component (the 'expert') to generate higher-quality trajectories that a learner imitates; referenced conceptually here.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Expert Iteration (iterative planning + generalisation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use a planning module to generate improved actions/trajectories (expert), then train a policy to imitate the expert; iterate.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General RL/decision tasks (original work), conceptually similar tasks here (sequence generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve policies by iteratively planning and distilling the planning outputs into a policy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported here (conceptual mention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Planning/search to produce better outputs then distillation into a policy; ReST is similar conceptually but does not require a planning mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as prior work with similar Grow/Improve decomposition; ReST differs in not requiring planning and in using reward-model filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed here; compared qualitatively: EI needs planning whereas ReST relies solely on model sampling and reward filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>EI vs ReST: EI uses explicit planning; ReST instead generates via the policy and filters with rewards, enabling simpler implementation for language tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforced Self-Training (ReST) for Language Modeling', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reward ranked finetuning for generative foundation model alignment <em>(Rating: 2)</em></li>
                <li>STAR: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing <em>(Rating: 2)</em></li>
                <li>Solving math word problems with process-and outcome-based feedback <em>(Rating: 1)</em></li>
                <li>Self-imitation learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8943",
    "paper_id": "paper-182c7b40ff7560a5545764814338f55a2098e441",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "ReST",
            "name_full": "Reinforced Self-Training",
            "brief_description": "An iterative generate-and-filter algorithm that alternates between (Grow) sampling many outputs from the current language policy and (Improve) filtering those outputs with a learned reward model and fine-tuning the policy on high-reward samples using offline RL or supervised losses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer encoder-decoder (task-dependent sizes)",
            "model_description": "Minor modifications of standard Transformer encoder-decoder architectures: tiny (IWSLT: model dim 512, FF 1024, 4 heads, 6 layers), base (WMT: model dim 512, FF 2048, 8 heads, 6 layers), and a larger finetune variant (Web Domain: model dim 1024, FF 8192, 16 heads, 6 layers). Vocabulary size 32k, generation length &lt;=128.",
            "reflection_method_name": "ReST (Grow / Improve iterative self-training)",
            "reflection_method_description": "Two-stage loop: Grow — sample N candidate outputs per input from the model; score each candidate with a learned reward model; Improve — filter candidates above a threshold τ and fine-tune the policy on the filtered set using an offline loss (typically BC / NLL, or other offline RL losses). Multiple Improve steps per Grow use increasing thresholds to produce progressively higher-quality data; then optionally repeat Grow with the improved policy.",
            "task_name": "Machine Translation (IWSLT 2014 De→En, WMT 2020 Zh→En, internal Web Domain En→Zh)",
            "task_description": "Sequence-to-sequence translation benchmarks with held-out validation/test sets; evaluation primarily by a learned reference-free reward model (Metric X) and human raters.",
            "performance_with_reflection": "Examples from reported results: IWSLT (avg reward scale 0-100): BC (G=0,I=0)=70.9; ReST (G=1,I=4)=77.8; ReST (G=2,I=3)=83.1. Online PPO baseline: 71.6. Best-of-N: after 3 Improve steps and N=200 ReST achieved reward 1.0 (normalized). Human evaluations: all ReST variants scored higher than BC in human rater scores (no single absolute numeric human-score reported in main table).",
            "performance_without_reflection": "Supervised BC baseline: average reward 70.9 (IWSLT example) and lower human evaluation scores than ReST; Online PPO achieved reward ~71.6 but suffered BLEU drop (-~8 BLEU) indicating potential reward hacking.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Generate-then-filter: uses prompt-conditioned sampling (tempered softmax with temperature 0.8) to create candidate outputs, a learned reward model to score candidates, threshold-based filtering, then supervised fine-tuning (NLL/BC) or offline-RL losses on filtered data. Data-generation (Grow) is decoupled from policy updates (Improve).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: consistent increases in average reward model scores across datasets with each Improve step (plots and Table 1); concrete improvements: BC→ReST (G=1,I=4) +6.9 points (70.9→77.8) on IWSLT; further gains from a second Grow (e.g., +5.3 points). Human eval: ReST variants outperformed BC in side-by-side human ratings (reported as positive human eval diffs).",
            "limitations_or_failure_cases": "Reported limitations include: reward model is an imperfect proxy of human preferences (mismatch between reward gains and human ranking); reward model generalization degrades as policy drifts from behavior data and can exhibit delusions (e.g., repetition sometimes increases reward); risk of overfitting to the reward model with repeated Grow steps; BC loss can overfit to reward-model artifacts; online RL showed reward hacking and deteriorated BLEU, a failure mode ReST partly avoids but still subject to reward-model weaknesses.",
            "comparison_to_other_methods": "Direct comparisons in paper: ReST outperforms BC (supervised) and several offline RL losses (GOLD, BVMPO, DAC) for reward improvement; BC loss within ReST often performed best. ReST with multiple Improve steps outperforms online PPO given the same generated-data budget. RAFT is described as a special case (single Improve per Grow). Compared qualitatively to STAR, Impossible Distillation, SIL and Expert Iteration in related work; ReST is characterized as compute-efficient and able to leverage exploration plus rewards.",
            "ablation_study_results": "Reported ablations: loss choices (BC, GOLD, BVMPO) — BC performed best across settings; thresholding strategies — global thresholds, per-source percentiles, and interpolated mean/max thresholds give similar trends (higher percentile/γ yields better reward up to saturation, e.g. percentile p&gt;90 saturates); Best-of-N experiments show ReST benefits from Best-of-N sampling (ReST with small N matches BC with large N). Q-learning/decision-transformer style approaches did not improve over BC (negative results).",
            "uuid": "e8943.0",
            "source_info": {
                "paper_title": "Reinforced Self-Training (ReST) for Language Modeling",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "RAFT",
            "name_full": "Reward-ranked Fine-Tuning (RAFT)",
            "brief_description": "Concurrent method referenced as a special case of ReST that ranks generated candidates by reward and fine-tunes on top-ranked samples, using a single Improve step per Grow step and quantile-based filtering.",
            "citation_title": "Reward ranked finetuning for generative foundation model alignment",
            "mention_or_use": "mention",
            "model_name": "Unspecified generative foundation models (referenced work)",
            "model_description": "Referenced work (Dong et al., 2023) applies to generative foundation models; exact model sizes not specified in this paper.",
            "reflection_method_name": "RAFT (single-step reward-ranked fine-tuning)",
            "reflection_method_description": "Generate candidates with the model, rank candidates by a reward model, keep top-quantile as high-quality data, fine-tune the model on that subset (one Improve per Grow).",
            "task_name": "Various generative tasks (as reported by the RAFT paper) — referenced here in context of language modeling alignment",
            "task_description": "RAFT was described as applicable to language modeling and image generation tasks; in this paper RAFT is noted as concurrent work reporting improvements over BC and PPO.",
            "performance_with_reflection": "Not reported in this paper beyond the statement that RAFT reported improvements over BC and PPO (citation to Dong et al., 2023).",
            "performance_without_reflection": "Not reported in this paper.",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Generate, rank by learned reward model, select fixed quantile threshold per Grow, then fine-tune once.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited: RAFT reported improvements over BC and PPO on various generative tasks (paper cites RAFT's claims but does not reproduce RAFT experiments).",
            "limitations_or_failure_cases": "Not detailed in this paper; RAFT is characterized as a single-Improve special case of ReST (hence subject to reward-model generalization and potential overfitting similar to ReST).",
            "comparison_to_other_methods": "Paper frames RAFT as a particular case of ReST (single Improve per Grow) and contrasts RAFT's fixed-quantile filtering with ReST's multi-step increasing-threshold Improve schedule.",
            "ablation_study_results": null,
            "uuid": "e8943.1",
            "source_info": {
                "paper_title": "Reinforced Self-Training (ReST) for Language Modeling",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "STAR",
            "name_full": "STAR (Bootstrapping reasoning with reasoning)",
            "brief_description": "Iterative method that bootstraps reasoning: fine-tune on a small set of rationales, sample rationales+answers from the model, filter generated answers by correctness, and iteratively improve.",
            "citation_title": "STAR: Bootstrapping reasoning with reasoning",
            "mention_or_use": "mention",
            "model_name": "Unspecified language models in referenced work",
            "model_description": "Referenced approach (Zelikman et al., 2022) uses LMs and their rationale+answer outputs to iteratively fine-tune; exact model sizes not specified in this paper.",
            "reflection_method_name": "STAR (iterative rationale sampling + filtering)",
            "reflection_method_description": "Iteratively sample chains-of-thought (rationales) and answers from the model, filter generated outputs by correctness, and fine-tune the model on filtered high-quality rationale+answer pairs.",
            "task_name": "Reasoning / math problems (as in cited STAR work)",
            "task_description": "Complex reasoning tasks solvable with chain-of-thought style rationales; iterative bootstrapping increases accuracy per the cited work.",
            "performance_with_reflection": "Not reported in this paper (only referenced as related work).",
            "performance_without_reflection": "Not reported in this paper.",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Generate rationales+answers, use correctness filtering, fine-tune on positives (generate-then-filter-then-finetune).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as related work where iterative sampling and filtering improved reasoning performance (paper does not reproduce/star experiments).",
            "limitations_or_failure_cases": "Not detailed here; referenced as similar in spirit to ReST but specialized to rationale/answer pairs and correctness-based filtering.",
            "comparison_to_other_methods": "Compared conceptually: STAR performs iterative sample-filter-finetune on reasoning tasks, ReST is more general and applies to conditional generation and continuous reward scores.",
            "ablation_study_results": null,
            "uuid": "e8943.2",
            "source_info": {
                "paper_title": "Reinforced Self-Training (ReST) for Language Modeling",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Impossible Distillation",
            "name_full": "Impossible Distillation (dataset generation from suboptimal models)",
            "brief_description": "Method that generates datasets by sampling from a suboptimal model and filters low-quality examples to produce a higher-quality dataset and model (used for summarization/paraphrasing).",
            "citation_title": "Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing",
            "mention_or_use": "mention",
            "model_name": "Unspecified (referenced work)",
            "model_description": "Referenced experimental setups for summarization/paraphrasing tasks; exact model details are from the cited work (Jung et al., 2023).",
            "reflection_method_name": "Impossible Distillation (generate-and-filter dataset distillation)",
            "reflection_method_description": "Single Grow + Improve: sample candidate outputs from a weaker model, filter or select high-quality outputs to create a new training set, and train a stronger model on that set.",
            "task_name": "Summarization and paraphrasing (as in the cited work)",
            "task_description": "Text generation tasks where dataset quality can be boosted by selecting good outputs from a suboptimal model.",
            "performance_with_reflection": "Not reported here; cited as an approach similar to ReST with one Grow and one Improve step.",
            "performance_without_reflection": "Not reported here.",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Generate candidate outputs from a model, filter by quality, then train on the filtered synthetic dataset.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited result: the referenced work reports that dataset generation+filtering can yield improved datasets and models; this paper treats it as analogous to ReST with single Grow/Improve.",
            "limitations_or_failure_cases": "Not detailed here; authors note ReST generalizes this approach and that repeated Grow steps risk overfitting to reward models in general.",
            "comparison_to_other_methods": "Characterized as corresponding to ReST with a single Grow and single Improve; ReST extends this by allowing multiple Improve steps and multiple Grow cycles.",
            "ablation_study_results": null,
            "uuid": "e8943.3",
            "source_info": {
                "paper_title": "Reinforced Self-Training (ReST) for Language Modeling",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Self-training",
            "name_full": "Self-training (noisy student / bootstrapping)",
            "brief_description": "Semi-supervised approach where the model generates labels for unlabeled inputs and is retrained on its own high-confidence predictions; ReST's Improve step resembles self-training but augmented with reward filtering and repeated Grow.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "General LM / task-specific models (classical)",
            "model_description": "Classical self-training methods applied historically across NLP and vision; not instantiated as a distinct model in this paper beyond conceptual similarity to ReST's Improve.",
            "reflection_method_name": "Self-training (generate self-labels and retrain)",
            "reflection_method_description": "Generate candidate outputs for unlabeled or existing inputs and retrain on high-confidence/self-labeled examples (ReST differs by generating exploration data and using reward-model filtering).",
            "task_name": "General semi-supervised learning (applied historically to translation, classification, etc.)",
            "task_description": "Improve a model by augmenting training data with model-generated labels on unlabeled inputs.",
            "performance_with_reflection": "Not directly reported; ReST extends self-training and demonstrates quantitative gains over BC baselines in experiments.",
            "performance_without_reflection": "N/A for this mention (historical baseline).",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Generate pseudo-labels, filter (often by confidence), then fine-tune on augmented dataset.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as established method with prior empirical success (e.g., noisy student, He et al. 2019); ReST is said to be similar but integrates rewards.",
            "limitations_or_failure_cases": "Paper notes self-training can overfit and that the quality of generated data is crucial; ReST attempts to mitigate with reward-based filtering and repeated Improve steps.",
            "comparison_to_other_methods": "Self-training vs ReST: ReST additionally generates exploration data and uses a reward model for filtering and can iterate Grow/Improve, while classic self-training typically lacks reward signals.",
            "ablation_study_results": null,
            "uuid": "e8943.4",
            "source_info": {
                "paper_title": "Reinforced Self-Training (ReST) for Language Modeling",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "SIL",
            "name_full": "Self-Imitation Learning",
            "brief_description": "An RL approach that trains the agent to reproduce its historically high-reward trajectories by filtering the replay buffer and learning only from successful episodes; ReST is related conceptually but applied in a generative offline setting.",
            "citation_title": "Self-imitation learning",
            "mention_or_use": "mention",
            "model_name": "Actor-critic RL agents (in referenced work)",
            "model_description": "In the original SIL work, off-policy actor-critic agents reproduce good past behavior; in this paper SIL is referenced conceptually.",
            "reflection_method_name": "Self-Imitation Learning (filter high-return trajectories)",
            "reflection_method_description": "Filter past trajectories for high returns and train the policy to imitate them; analogous to ReST's filtering of high-reward generated outputs.",
            "task_name": "RL control tasks in original SIL work; here referenced conceptually for generative models",
            "task_description": "Learning from own successful past experiences by imitation.",
            "performance_with_reflection": "Not reported here (conceptual mention).",
            "performance_without_reflection": "Not reported here.",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Replay-buffer filtering + imitation objective (SIL); ReST uses reward-model filtering plus offline imitation or RL objectives.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites SIL as related: both filter poor examples and train on high-quality behaviors; ReST differs in being agnostic to value functions and targeted at generative tasks.",
            "limitations_or_failure_cases": "Not detailed in this paper; original SIL limitations (dependence on quality and diversity of stored trajectories) are analogous to ReST's dependence on generated-data quality and reward-model reliability.",
            "comparison_to_other_methods": "ReST vs SIL: ReST does not require a value function to filter and is applied to offline generative settings; SIL requires value estimates and is an RL algorithm.",
            "ablation_study_results": null,
            "uuid": "e8943.5",
            "source_info": {
                "paper_title": "Reinforced Self-Training (ReST) for Language Modeling",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Expert Iteration (EI)",
            "name_full": "Expert Iteration",
            "brief_description": "A policy-iteration style method that alternates planning (to produce improved trajectories) and generalization (to learn a policy from the planned trajectories); related to ReST's Grow/Improve decomposition but EI typically uses planning.",
            "citation_title": "Expert iteration",
            "mention_or_use": "mention",
            "model_name": "Planning + policy-learning frameworks (referenced work)",
            "model_description": "EI frameworks use a planning/search component (the 'expert') to generate higher-quality trajectories that a learner imitates; referenced conceptually here.",
            "reflection_method_name": "Expert Iteration (iterative planning + generalisation)",
            "reflection_method_description": "Use a planning module to generate improved actions/trajectories (expert), then train a policy to imitate the expert; iterate.",
            "task_name": "General RL/decision tasks (original work), conceptually similar tasks here (sequence generation)",
            "task_description": "Improve policies by iteratively planning and distilling the planning outputs into a policy.",
            "performance_with_reflection": "Not reported here (conceptual mention).",
            "performance_without_reflection": "Not reported here.",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Planning/search to produce better outputs then distillation into a policy; ReST is similar conceptually but does not require a planning mechanism.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as prior work with similar Grow/Improve decomposition; ReST differs in not requiring planning and in using reward-model filtering.",
            "limitations_or_failure_cases": "Not detailed here; compared qualitatively: EI needs planning whereas ReST relies solely on model sampling and reward filtering.",
            "comparison_to_other_methods": "EI vs ReST: EI uses explicit planning; ReST instead generates via the policy and filters with rewards, enabling simpler implementation for language tasks.",
            "ablation_study_results": null,
            "uuid": "e8943.6",
            "source_info": {
                "paper_title": "Reinforced Self-Training (ReST) for Language Modeling",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reward ranked finetuning for generative foundation model alignment",
            "rating": 2
        },
        {
            "paper_title": "STAR: Bootstrapping reasoning with reasoning",
            "rating": 2
        },
        {
            "paper_title": "Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing",
            "rating": 2
        },
        {
            "paper_title": "Solving math word problems with process-and outcome-based feedback",
            "rating": 1
        },
        {
            "paper_title": "Self-imitation learning",
            "rating": 1
        }
    ],
    "cost": 0.01700675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reinforced Self-Training (ReST) for Language Modeling</h1>
<p>Caglar Gulcehre ${ }^{<em> \dagger, 1}$, Tom Le Paine ${ }^{</em> \dagger, 1}$, Srivatsan Srinivasan ${ }^{<em> \dagger, 1}$, Ksenia Konyushkova ${ }^{\dagger, 1}$, Lotte Weerts ${ }^{\dagger, 1}$<br>Abhishek Sharma ${ }^{\dagger, 1}$, Aditya Siddhant ${ }^{\dagger, 1}$, Alex Ahern ${ }^{1}$, Miaosen Wang ${ }^{1}$, Chenjie Gu ${ }^{1}$,<br>Wolfgang Macherey ${ }^{2}$, Arnaud Doucet ${ }^{1}$, Orhan Firat ${ }^{\dagger, 1}$, Nando de Freitas ${ }^{1}$<br></em>Contributed equally, ${ }^{\dagger}$ Core contributors<br>${ }^{1}$ Google DeepMind, ${ }^{2}$ Google Research</p>
<h4>Abstract</h4>
<p>Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.</p>
<p>Keywords: Offline RL, reinforcement learning, RL from human feedback, language, natural language processing, machine translation</p>
<h2>1. Introduction</h2>
<p>Large language models (LLMs) have demonstrated impressive abilities in generating high-quality text and in solving numerous language tasks (Brown et al., 2020; Bubeck et al., 2023; Rae et al., 2021). These models are trained to maximize the likelihood of the next token autoregressively using massive amounts of text and compute (Hoffmann et al., 2022; Srivastava et al., 2022). However, Perez et al. (2022) showed that producing text with high likelihood does not necessarily align well with human preferences on various tasks. Without proper alignment, the language models can also output unsafe contents with harmful consequences. Moreover, aligning LLMs helps to improve on other downstream tasks (Ouyang et al., 2022b). Reinforcement learning from human feedback (RLHF) aims to address the alignment problem by using human preferences (Glaese et al., 2022; Stiennon et al., 2020; Wu et al., 2021). Typically, human feedback is used to learn a reward model, which is then used to fine-tune LLM with a reinforcement learning (RL) objective.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure $1 \mid$ ReST method. During Grow step, a policy generates a dataset. At Improve step, the filtered dataset is used to fine-tune the policy. Both steps are repeated, Improve step is repeated more frequently to amortise the dataset creation cost.</p>
<p>RLHF methods often rely on online RL methods such as PPO (Schulman et al., 2017) and A2C (Mnih et al., 2016). Online training requires sampling from the updated policy and scoring the samples with the reward model many times during training. The computational cost of dealing with a continual flow of new samples becomes a limitation of online methods, especially when the sizes of the policy and reward networks grow. Moreover, these methods are prone to reward "hacking" (Skalse et al., 2022), and prior works (Glaese et al., 2022) explored model regularization to mitigate this issue. Alternatively, offline RL methods learn from a fixed dataset of examples and, thus they are more computationally efficient and less prone to reward hacking. However, the quality of the policy learnt offline inevitably depends on the properties of the offline dataset (Fu et al., 2020; Gulcehre et al., 2021). As a result, carefully curated datasets become very important for the success of offline RL. Otherwise, the performance gains over supervised learning may be limited (Kumar et al., 2021). Concurrent to our work, (Rafailov et al., 2023) proposed a method called DPO (Direct Preference Optimization) that can make use of offline data to align an LM with human preferences.</p>
<p>We frame the alignment problem of a language model as a growing batch RL problem (Lange et al., 2012). Specifically, our Reinforced Self-Training (ReST) method includes two loops: in the inner loop (Improve), we improve the policy on a fixed dataset and in the outer loop (Grow), we grow the dataset by sampling from the latest policy (see Figure 1). In this work we consider conditional language modelling, then the steps of ReST are as follows:</p>
<ol>
<li>Grow (G): The language model policy (initially, a supervised policy) is used to generate multiple output predictions for each context to augment the training dataset.</li>
<li>
<p>Improve (I): We rank and filter the augmented dataset with a scoring function. We use a learned reward model trained on human preferences as the scoring function in our experiments. Then, the language model is fine-tuned on the filtered dataset with an offline RL objective. This step can be repeated with an increasing filtering threshold. The final policy is then used in the next Grow step.
ReST is a general approach that allows different offline RL losses to be used in the inner loop when executing the Improve steps. To put it in practice, one only needs the ability to: i) sample from a model efficiently, ii) score the model's samples. ReST provides several advantages over typical RLHF methods with online or offline RL:</p>
</li>
<li>
<p>The computational burden is significantly reduced compared to online RL thanks to the output of Grow step being exploited across several Improve steps.</p>
</li>
<li>The quality of the policy is not restricted by the quality of the original dataset (as in offline RL) since new training data is sampled from an improved policy during the Grow step.</li>
<li>It is easy to inspect the data quality and potentially diagnose alignment issues, e.g., reward hacking, as the Grow and Improve steps are decoupled.</li>
<li>The approach is simple, stable and has only a small number of hyperparameters to tune.</li>
</ol>
<p>We explain the details of our proposed ReST approach in Section 3. Then, we present our experimental results on machine translation benchmarks in Section 4. Machine translation is a sequence-to-sequence learning problem (Sutskever et al., 2014), which is usually formulated as conditional language modelling where the context for conditioning is a sentence in a foreign language (source). We chose machine translation because i) it is an impactful application with strong baselines and a well-defined evaluation procedure, ii) several existing reliable scoring and evaluation methods are available for the use as a reward model (Freitag et al., 2022). In our experiments, we compare several offline RL algorithms on the IWSLT 2014 (Cettolo et al., 2014) and WMT 2020 benchmarks (Koehn et al., 2020) as well as more competitive, high-fidelity internal benchmarks on Web Domain. In our experiments ReST significantly improves reward model scores on test and validation sets. Furthermore, according to human raters, ReST generates higher quality translations compared to a supervised learning baseline.</p>
<h1>2. Preliminaries</h1>
<p>A conditional language model produces an output sequence $\boldsymbol{y}=\left(y_{1}, y_{2}, \ldots . y_{T}\right)$ given a context (or source input) $\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots x_{L}\right)$, where the tokens $x_{l}, y_{t}$ belong to a chosen vocabulary. A language generation policy $\pi$ in an auto-regressive model characterized by a conditional probability distribution parameterized by $\theta$ as</p>
<p>$$
\pi_{\theta}(\boldsymbol{y} \mid \boldsymbol{x})=\prod_{t=1}^{T} \pi_{\theta}\left(y_{t} \mid \boldsymbol{y}_{1: t-1}, \boldsymbol{x}\right)
$$</p>
<p>with the convention $\boldsymbol{y}<em 1:="1:" t-1="t-1">{1: 0}=\emptyset$ and $\boldsymbol{y}</em>\right)$.
Let $p(\boldsymbol{x}, \boldsymbol{y})=p(\boldsymbol{x}) p(\boldsymbol{y} \mid \boldsymbol{x})$ denote the data distribution. A given dataset $\mathcal{D}$ consists of samples from this distribution:}=\left(y_{1}, y_{2}, \ldots . y_{t-1</p>
<p>$$
\mathcal{D}=\left{\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right)\right}_{i=1}^{N} \text { such that } \boldsymbol{x}^{i} \sim p(\boldsymbol{x}), \boldsymbol{y}^{i} \sim p\left(\boldsymbol{y} \mid \boldsymbol{x}=\boldsymbol{x}^{i}\right)}
$$</p>
<p>Given this dataset, the supervised policy is trained by minimizing the negative log likelihood (NLL) loss:</p>
<p>$$
\mathcal{L}<em _boldsymbol_x="(\boldsymbol{x">{\mathrm{NLL}}(\theta)=-\mathbb{E}</em>\right)\right]
$$}, \boldsymbol{y}) \sim \mathcal{D}}\left[\sum_{t=1}^{T} \log \pi_{\theta}\left(y_{t} \mid \boldsymbol{y}_{1: t-1}, \boldsymbol{x</p>
<p>We refer to the model that is trained with the NLL loss as behavioral cloning (BC) (Pomerleau, 1989) following the RL literature nomenclature.</p>
<h2>3. Reinforced Self-Training (ReST)</h2>
<p>We present ReST, an RLHF algorithm that aligns the language model's outputs with human preferences. Human preferences over sequences are modelled using a learned reward function (see Appendix A.4). In the underlying Markov decision process for conditional language modelling the states are the partial sequences, and the actions are the generated tokens (see Appendix A.1).</p>
<p>The ReST algorithm decouples the dataset growth and policy improvement of a typical RL pipeline into separate offline stages (Figure 1 and 2). We start by training an initial model $\pi_{\theta}(\boldsymbol{y} \mid \boldsymbol{x})$ to map input sequences $\boldsymbol{x}$ to output sequences $\boldsymbol{y}$ on a given dataset of sequence pairs $\mathcal{D}$ using the NLL loss from Equation (1). Next, the Grow step creates a new dataset $\mathcal{D}_{g}$, which augments the initial training dataset with samples from the model:</p>
<p>$$
\mathcal{D}<em i="1">{g}=\left{\left.\left(\boldsymbol{x}^{i}, \boldsymbol{y}^{i}\right)\right|</em>
$$} ^{N_{g}} \text { such that } \boldsymbol{x}^{i} \sim \mathcal{D}, \boldsymbol{y}^{i} \sim \pi_{\theta}\left(\boldsymbol{y} \mid \boldsymbol{x}^{i}\right)\right} \cup \mathcal{D</p>
<p>Here, the conditioning inputs are resampled from the original dataset $\boldsymbol{x}^{i} \sim \mathcal{D}$, as in self-training, but in situations where one has access to $p(\boldsymbol{x})$ they could sample directly from it, i.e., $\boldsymbol{x}^{i} \sim p(\boldsymbol{x})$. For example, consider a model that generates image from a textual description, in this case, the distribution of text inputs can be sampled from a language model $p(\boldsymbol{x})$.</p>
<p>Subsequently, the Improve steps use $\mathcal{D}<em _theta="\theta">{g}$ to fine-tune the policy $\pi</em>$. Note that we keep the original dataset in the training mixture to ensure that the policies do not diverge. Below, we describe Grow and Improve steps in more details.</p>
<h2>Grow</h2>
<p>The Grow step corresponds to the acting or data-generation step in RL. We create an augmented dataset of trajectories $\mathcal{D}<em _theta="\theta">{\mathrm{g}}$ by sampling many output sequences from the current policy $\pi</em>)$. The datapoints with the reward above a threshold score are used to update the policy (see next). Once the policy is improved, a new dataset of better quality samples can be created once again (Figure 2, bottom).}$, i.e., $\boldsymbol{y} \sim \pi_{\theta}(\boldsymbol{y} \mid \boldsymbol{x})$ for $\boldsymbol{x} \sim \mathcal{D}$. The new dataset of sequences is then scored with a reward function $R(\boldsymbol{x}, \boldsymbol{y</p>
<h2>Improve</h2>
<p>At the Improve step (exploitation or policy improvement in RL terminology), the goal is to use the new dataset $\mathcal{D}<em _theta="\theta">{\mathrm{g}}$ to fine-tune the policy $\pi</em>$. We start by defining a filtering function that includes only samples with rewards higher than a certain threshold $\tau$ :</p>
<p>$$
F(\boldsymbol{x}, \boldsymbol{y} ; \tau)=\mathbb{1}_{R(\boldsymbol{x}, \boldsymbol{y})&gt;\tau}
$$</p>
<p>Let us note that the threshold based filtering function may result into learning suboptimal behaviors that favors outcomes with high variance in the environments with stochastic dynamics (Brandfonbrener et al., 2022). However, in this work we formulate the language modeling and translation tasks as deterministic RL problems (Appendix A.1.)</p>
<p>Next, we finetune the current best policy typically trained with either the supervised learning loss $\mathcal{L}_{\text {NLL }}$ from equation 1 or an offline RL loss $\mathcal{L}(\boldsymbol{x}, \boldsymbol{y} ; \theta)$ on the filtered data such as V-MPO (Song et al., 2020) or offline actor-critic (Mathieu et al., 2021). To sum up, we use the following reward weighted loss $J$ :</p>
<p>$$
J(\theta)=\mathbb{E}<em _mathrm_g="\mathrm{g">{(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{D}</em> ; \theta)]
$$}}}[F(\boldsymbol{x}, \boldsymbol{y} ; \tau) \mathcal{L}(\boldsymbol{x}, \boldsymbol{y</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure $2 \mid$ ReST algorithm. Top: At Improve steps $\mathrm{I}=1, \mathrm{I}=2, \mathrm{I}=3$, the dataset from the initial policy is filtered with thresholds $\tau_{1}&lt;\tau_{2}&lt;\tau_{3}$ and a sequence of policies $\pi_{\theta_{1}}, \pi_{\theta_{2}}, \pi_{\theta_{3}}$ are finetuned. Bottom: If we were to sample from those policies (grey), the quality of samples would increase. In practice, only the final policy $\pi_{\theta_{3}}$ is used to generate the next dataset $\mathcal{D}_{\mathrm{g}}$.
Standard imitation learning approaches, such as BC (Pomerleau (1989), equation 1) and onestep RL methods like Behavior Value Estimation (BVE) (Gulcehre et al., 2021) perform one-step of Improve on the fixed dataset $\mathcal{D}$. In contrast, the basic version of ReST additionally includes a Grow step that allows the model to gather multiple new output sequences (potential translations) for contexts $\boldsymbol{x}$ from the original dataset (source sentences to translate).</p>
<p>When iterating over Improve steps, we increase the filtering thresholds: $\tau_{1}&lt;\cdots&lt;\tau_{N-1}&lt;\tau_{N}$ (Figure 2). This filtering with the growing threshold results in data subsets of increasing quality but of decreasing size. As LLMs overfit to small datasets quickly, we fine-tune every new policy from the previous policy with a lower learning rate. Consecutive fine-tuning of policies $\left{\pi_{\theta_{k}}\right}<em _mathrm_g="\mathrm{g">{k \geq 1}$ on higher quality data subsets ensures policy improvement with a fixed dataset $\mathcal{D}</em>$, the average reward of the generated samples would be increasing (shown in grey in Figure 2). As sampling from a policy in the Grow step is computationally expensive, after each such step we perform several Improve steps. Thus, the cost of a single dataset generation is amortised over multiple Improve steps. Algorithm 1 outlines the full ReST algorithm with multiple dataset growth and policy improvement steps.}}$. If we were to sample from policies $\left{\pi_{\theta_{k}}\right}_{k \geq 1</p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">1</span><span class="p">:</span> ReST algorithm<span class="o">.</span> ReST is a growing-batch RL algorithm<span class="o">.</span> Given an initial
policy of reasonable quality <span class="p">(</span>for example<span class="p">,</span> pre-trained using BC<span class="p">)</span> iteratively applies Grow <span class="ow">and</span>
Improve steps to update the policy<span class="o">.</span> Here <span class="err">\</span><span class="p">(</span>F<span class="err">\</span><span class="p">)</span> is a filtering function<span class="p">,</span> <span class="ow">and</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>L<span class="p">}</span><span class="err">\</span><span class="p">)</span> is an loss function<span class="o">.</span>
    Input<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}:</span><span class="err">\</span><span class="p">)</span> Dataset<span class="p">,</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>text <span class="p">{</span>eval <span class="p">}}</span><span class="err">\</span><span class="p">)</span> <span class="p">:</span> Evaluation dataset<span class="p">,</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>L<span class="p">}(</span><span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">},</span> <span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">}</span> <span class="p">;</span> <span class="err">\</span>theta<span class="p">)</span><span class="err">\</span><span class="p">)</span> <span class="p">:</span> loss<span class="p">,</span> <span class="err">\</span><span class="p">(</span>R<span class="p">(</span><span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">},</span> <span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">}):</span><span class="err">\</span><span class="p">)</span> reward model<span class="p">,</span> <span class="err">\</span><span class="p">(</span>G<span class="err">\</span><span class="p">)</span> <span class="p">:</span>
        number of grow steps<span class="p">,</span> <span class="err">\</span><span class="p">(</span>I<span class="err">\</span><span class="p">)</span> <span class="p">:</span> number of improve steps<span class="p">,</span> <span class="err">\</span><span class="p">(</span>N<span class="err">\</span><span class="p">)</span> <span class="p">:</span> number of samples per context
Train <span class="err">\</span><span class="p">(</span><span class="err">\</span>pi_<span class="p">{</span><span class="err">\</span>theta<span class="p">}</span><span class="err">\</span><span class="p">)</span> on <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span><span class="err">\</span><span class="p">)</span> using loss <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>L<span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
for <span class="err">\</span><span class="p">(</span><span class="ss">g</span><span class="o">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span> to <span class="err">\</span><span class="p">(</span>G<span class="err">\</span><span class="p">)</span> do
    <span class="o">//</span> Grow
    Generate dataset <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span>g<span class="p">}</span><span class="err">\</span><span class="p">)</span> by sampling<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span>g<span class="p">}</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span>_<span class="p">{</span><span class="ss">i</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="err">^</span><span class="p">{</span>N_<span class="p">{</span>g<span class="p">}}</span><span class="err">\</span><span class="p">)</span> s<span class="o">.</span>t<span class="o">.</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>sim <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">},</span> <span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>sim <span class="err">\</span>pi_<span class="p">{</span><span class="err">\</span>theta<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">}</span> <span class="err">\</span>mid <span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span> <span class="err">\</span>cup <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
    Annotate <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span>g<span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="k">with</span> the reward model <span class="err">\</span><span class="p">(</span>R<span class="p">(</span><span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">},</span> <span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">})</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
    for <span class="err">\</span><span class="p">(</span><span class="ss">i</span><span class="o">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span> to <span class="err">\</span><span class="p">(</span>I<span class="err">\</span><span class="p">)</span> do
        <span class="o">//</span> Improve
        Choose threshold s<span class="o">.</span>t<span class="o">.</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>tau_<span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="o">&gt;</span>V_<span class="p">{</span><span class="err">\</span>pi_<span class="p">{</span><span class="err">\</span>theta<span class="p">}}</span><span class="err">\</span><span class="p">)</span> for <span class="err">\</span><span class="p">(</span>V_<span class="p">{</span><span class="err">\</span>pi_<span class="p">{</span><span class="err">\</span>theta<span class="p">}}</span><span class="o">=</span><span class="err">\</span>mathbb<span class="p">{</span>E<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span>g<span class="p">}}[</span>R<span class="p">(</span><span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">},</span> <span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">})]</span><span class="err">\</span><span class="p">)</span> <span class="ow">and</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>tau_<span class="p">{</span>i<span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">&gt;</span><span class="err">\</span>tau_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
        while reward improves on <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>text <span class="p">{</span>eval <span class="p">}}</span><span class="err">\</span><span class="p">)</span> do
            Optimise <span class="err">\</span><span class="p">(</span><span class="err">\</span>theta<span class="err">\</span><span class="p">)</span> on objective<span class="p">:</span> <span class="err">\</span><span class="p">(</span>J<span class="p">(</span><span class="err">\</span>theta<span class="p">)</span><span class="o">=</span><span class="err">\</span>mathbb<span class="p">{</span>E<span class="p">}</span>_<span class="p">{(</span><span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">},</span> <span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">})</span> <span class="err">\</span>sim <span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span>_<span class="p">{</span>g<span class="p">}}</span><span class="err">\</span>left<span class="p">[</span>F<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">},</span> <span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">}</span> <span class="p">;</span> <span class="err">\</span>tau_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span> <span class="err">\</span>mathcal<span class="p">{</span>L<span class="p">}(</span><span class="err">\</span>boldsymbol<span class="p">{</span>x<span class="p">},</span> <span class="err">\</span>boldsymbol<span class="p">{</span>y<span class="p">}</span> <span class="p">;</span> <span class="err">\</span>theta<span class="p">)</span><span class="err">\</span>right<span class="p">]</span><span class="err">\</span><span class="p">)</span>
        end
    end
end
Output<span class="p">:</span> Policy <span class="err">\</span><span class="p">(</span><span class="err">\</span>pi_<span class="p">{</span><span class="err">\</span>theta<span class="p">}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Probabilistic interpretation of the Improve step Let us consider the particular choice $\mathcal{L}=\mathcal{L}<em g="g">{\text {NLL }}$, with $\theta^{\prime}$ being the parameters of the model from the last Grow step, $\lambda$ the proportion of data sampled from this model in $\mathcal{D}</em>$ and a single step of growth. The expression for the gradient in this case takes the following form:
$\nabla J(\theta)=-\mathbb{E}<em _boldsymbol_y="\boldsymbol{y">{\boldsymbol{x} \sim \mathcal{D}}\left[\lambda \mathbb{E}</em>} \sim \pi_{\theta^{\prime}}(\boldsymbol{y} \mid \boldsymbol{x})}\left[F(\boldsymbol{x}, \boldsymbol{y} ; \tau) \nabla \log \pi_{\theta}(\boldsymbol{y} \mid \boldsymbol{x})\right]+(1-\lambda) \mathbb{E<em _theta="\theta">{\boldsymbol{y} \sim p(\boldsymbol{y} \mid \boldsymbol{x})}\left[F(\boldsymbol{x}, \boldsymbol{y} ; \tau) \nabla \log \pi</em>)\right]\right]$.}(\boldsymbol{y} \mid \boldsymbol{x</p>
<p>The first term on the RHS of (3) is similar to an online policy gradient term at the beginning of training when $\theta \approx \theta^{\prime}$ with $F(\boldsymbol{x}, \boldsymbol{y} ; \tau)$ replacing the state-action value function $Q^{\pi}(\boldsymbol{x}, \boldsymbol{y})$, when starting in state $\boldsymbol{x}$ and taking sequential actions $\boldsymbol{y}$, that is generating synthetic data $\boldsymbol{y}$ using policy $\pi_{\theta}$ in our context. For the second term on the RHS of (3), we consider the original data $\mathcal{D}$, but we still ensure that it passes the threshold $F(\boldsymbol{x}, \boldsymbol{y} ; \tau)$. Intuitively, people choose $\mathcal{D}$ for training according to some possibly unknown criteria. In this work, we make the criterion $F(\boldsymbol{x}, \boldsymbol{y} ; \tau)$ explicit. The last term is therefore a form of offline policy gradients which prevents $\pi_{\theta}(\boldsymbol{y} \mid \boldsymbol{x})$ to move too far from $p(\boldsymbol{y} \mid \boldsymbol{x})$ which could lead to model collapse (Shumailov et al., 2023). Finally, note the similarity of this approach with self-training (Clark et al., 2003; Scudder, 1965; Xie et al., 2020) techniques. We provide a population interpretation (i.e., as $N, N_{g} \rightarrow \infty$ ) of ReST in Appendix A.9.</p>
<p>In the following section, we explore how the choice of loss, filtering function and threshold, and synthetic data generated by language policy via sampling (exploration data) empirically affect the performance of the resulting policies $\pi_{\theta}$.</p>
<h1>4. Experiments and analysis</h1>
<p>We chose machine translation as a testbed for ReST as it is an impactful application of conditional language modeling where established reward models are available, for example, Metric X (Freitag et al., 2022), BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020). We ran experiments on two common benchmarks: IWSLT 2014 (Cettolo et al., 2014), and WMT 2020 (Koehn et al., 2020),</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | ReST with multiple Improve steps. Average reward model scores on IWSLT 2014 De-En, WMT 2020 Zh-En, and Web Domain En-Zh validation sets. On each dataset, we report results with BC $(G=0, I=0)$ and ReST with a single Grow step and several Improve steps with an increasing reward threshold. Each Improve step increases the reward model score in all three validation datasets. We found the suitable number of Improve steps to be a dataset-dependent hyperparameter.
as well as an internal benchmark dataset which we call Web Domain (a version of this dataset was previously used by Ghorbani et al. (2021)). These datasets contain a set of sentences in the source language and the corresponding human "reference" translation. We selected a different language pair for each dataset to test the generality of the results. We kept a separate validation and test sets with unseen source sentences for the evaluation purposes.</p>
<p>We used Metric X in our experiments, a state-of-art reference-free reward model (Freitag et al., 2022) which, for a given source text and a proposed translation, outputs a numerical score. We report results in terms of average rewards on samples generated by a policy on the validation set ${ }^{1}$. For the details of the datasets and models, we refer to Appendix A.3. Also, Table 2 indicates the size of the datasets by reporting the number of samples per source sentence generated at each Grow step.</p>
<p>Nomenclature We named variants of ReST by the loss type, number of Grow steps, and number of Improve steps, for example GOLD G=1 I=2. With this convention, BC G=0 I=0 refers to standard supervised learning, which is trained only on the original dataset $\mathcal{D}$ and performs neither Grow nor Improve steps. When the loss type is not specified, the BC loss is used, i.e., the model is trained with auto-regressive supervised learning with the NLL loss as typical in training language models. In all plots, we colored supervised learning in grey and ReST variants in shades of purple.</p>
<p>Baselines We reported the results with several different offline RL method, including Offline Actor Critic (DAC) (Mathieu et al., 2021), Behavior VMPO (BVMPO), Generation by Off-policy Learning from Demonstrations (GOLD) (Pang and He, 2021), and BC (Pomerleau, 1989) ${ }^{2}$.</p>
<p>Do multiple Improve steps in ReST increase the reward model scores? We evaluated ReST on three different datasets by fixing the loss function to BC and increasing the number of Improve steps. The range of rewards for training was normalized between 0 and $1^{3}$. For our experiments, we</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure $4 \mid$ ReST with two Grow steps. The second Grow step with subsequent Improve steps improves the performance by 5.3 points on IWSLT 2014 De-En and 0.8 points on Web Domain En-Zh task over the first Grow step.
picked the filtering thresholds $\tau_{i}$ from a sequence of increasing values $[0.0,0.7,0.8,0.9,0.95,0.99]$ ${ }^{4}$. The $\tau_{0}=0.0$ case corresponds to using the full dataset. We did five Improve steps on IWSLT 2014, four on WMT-2020, and two on Web Domain. In Figure 3 we plotted the average reward of different variants of ReST. We see that each subsequent Improve step improves the performance of the translation model significantly across all three datasets.</p>
<p>Do additional Grow steps improve reward model scores? We performed a second Grow step with successive Improve steps to measure the effect of the extra Grow step on the performance. In Figure 4, a method with an additional Grow step achieves further improvement on the IWSLT 2014 and Web Domain datasets. We noticed a 5.3 point improvement between the end of the first and the second Grow step.</p>
<p>Does ReST improve over supervised training? To answer this question, in Figure 5 we plotted the average reward achieved by the supervised learning model as well as several variants of ReST with different losses and the number of Grow and Improve steps. Different variants of ReST (purple) significantly outperform supervised learning (gray) even after just the first grow step. This observation was consistent across different datasets and language pairs that we tested.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure $5 \mid$ WMT 2020 zh-en (test): BC (in grey, $G=0 I=0$ ) and ReST trained with different offline RL losses. ReST is trained with one Grow and Improve step except $G=$ $1 I=0$, which is trained on the entire dataset generated after the first Grow step without any Improve (all in purple). All variants of ReST outperform the initial BC baseline, with BC loss resulting in the best performance.</p>
<p>Which loss is the best for a single step of ReST? Figure 5 depicts variants of ReST with different offline RL losses $\mathcal{L}(\boldsymbol{x}, \boldsymbol{y} ; \theta)$. We find that BC loss outperforms other loss functions. Note that normally BC algorithm does not depend on the reward, but in ReST, the reward is taken into account through the reward filtering stage for $I \geq 1$ (with $\tau_{1}=0.8$ for WMT 2020.) Results with multiple Grow and Improve steps are displayed in Figure 4 (see also Appendix A.6).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Algorithm</th>
<th style="text-align: center;">Average Reward</th>
<th style="text-align: center;">Distinct samples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathrm{BC}(\mathrm{G}=0, \mathrm{I}=0)$</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">16000000</td>
</tr>
<tr>
<td style="text-align: center;">ReST $(\mathrm{G}=1, \mathrm{I}=0)$</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">16000000</td>
</tr>
<tr>
<td style="text-align: center;">ReST $(\mathrm{G}=1, \mathrm{I}=4)$</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">16000000</td>
</tr>
<tr>
<td style="text-align: center;">ReST $(\mathrm{G}=2, \mathrm{I}=3)$</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">32000000</td>
</tr>
<tr>
<td style="text-align: center;">Online RL</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">24000000</td>
</tr>
</tbody>
</table>
<p>Table 1 | Online RL for IWSLT 2014: Online RL performs as well as ReST $(\mathrm{G}=1, \mathrm{I}=0)$ and ReST $(\mathrm{G}=1, \mathrm{I}=4)$ is significantly better.</p>
<p>Can ReST be improved further with Best-of-N sampling at inference time? Best-of-N sampling technique at inference time generates $N$ samples which are then ranked by the reward model. Then, the top ranked candidate is selected (Gao et al., 2022). We show results with Best-of-N sampling on top of BC ( $\mathrm{G}=0 \mathrm{I}=0$ ) and ReST variants in Figure 6. The performance of ReST improves both with $N$ and with the number of Improve steps. The best ReST variant with $N&lt;10$ matches the performance of the BC model with $N=200$. Even though RL is known to limit the diversity of samples, this experiment shows that ReST can still benefit from Best-of-N sampling. After three Improve steps with $N=200$, ReST achieves the highest possible reward of 1 , outperforming the "reference" translations in $\mathcal{D}$.</p>
<p>How does ReST compare with Online RL? We compared ReST with PPO (Schulman et al., 2017), an online RL algorithm widely used for RLHF (Glaese et al., 2022; Ouyang et al., 2022a). For our online RL experiments, we used the setup of Donato et al. (2022) where PPO had access to a similar amount of training data as ReST with 1 Grow step. The results are summarized in Table 1. Online RL performs as well as ReST with one Grow and no Improve steps which is equivalent to BC on the $\mathcal{D}_{2}$ dataset. With the same amount of training data, ReST with multiple Improve steps achieves significantly higher rewards. Furthermore, we noticed that the BLEU score for the online RL policy on the validation set dropped by nearly 8 points (BLEU score of ReST did not change) which indicates a potential reward hacking behaviour. ReST's ability to improve the reward model score without deteriorating the performance on other metrics suggests that the "alignment tax" it pays is lower than for online RL approaches.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 | Best-of-N sampling at inference time. All variants of ReST benefit as much from Best-of-N sampling as supervised models.</p>
<p>Does ReST improve human preferences? We evaluated the ReST models by human raters to investigate if ReST can outperform BC in human evaluations as well. We displayed a sentence in the source language and two generated translations: one by BC model ( $\mathrm{G}=0 \mathrm{I}=0$ ) and one by a ReST variant. Human raters scored each translation on a scale from 0 to 6 , and we measured the difference between the average score of ReST method and of BC which we refer as "Human eval diff". In Figure 7 (right), we see that all variants of ReST outperform the BC baseline significantly. However, if we compare the human score gains with the gains in the learned reward (Figure 7, left), the rankings do not match. We hypothesise that the difference is due to the fact that the reward</p>
<p>models cannot generalize well on OOD data since the learned reward models are an imperfect proxy of human preferences. In particular, we found that the reward models generalise worse as our policy moves away from the behaviour model which can happen as the number of Grow and Improve steps increases at which point ReST can start overfitting to the reward model. Thus, in our analysis, we focused on evaluating models based on how well they align with a reward signal and we treat reward model generalisation as an independent issue that could be mitigated by, for example, finetuning the reward model between the consecutive Grow steps on the human-annotated data from the most recent policy. ReST with the B-VMPO loss utilises a learned value function and KL regularisation term to prevent over-fitting and thus attains high human preference scores.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7 | Comparison of performance based on learned reward and on human evaluation. All ReST variants outperform BC in terms of human ratings, but the rankings of the methods based on reward model scores and human scores are different.</p>
<h1>5. Related works</h1>
<p>There has been large number of works recently on self-improving alignment algorithms for language modelling. In Figure 8, we also compare ReST against different approaches: supervised learning, self-training, online and offline RL. We conclude from this comparison that ReST is the only approach that is compute efficient, but also can leverage exploration data and rewards. Next, we describe some of the particular works related to ReST.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Exploration</th>
<th style="text-align: center;">Rewards</th>
<th style="text-align: center;">Compute Efficient</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Supervised Learning</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Self-training</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Online RL</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Offline RL</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">ReST</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Figure $8 \mid$ ReST vs alternatives: ReST is the only approach that can leverage the exploration data and rewards, but is also computationally efficient.</p>
<p>Self-training Self-training is an established semi-supervised learning approach which utilizes unlabeled data to improve a model (Scudder, 1965). Since its introduction, self-training has been successfully applied to many tasks, including image classification and recognition (Xie et al., 2020), protein folding (Jumper et al., 2021), as well as several language tasks (He et al., 2019; Sun et al., 2021; Yarowsky, 1995; Zhang and Zong, 2016). He et al. (2019) empirically demonstrated that noisy self-training improves the performance of translation models. The Improve step of ReST resembles self-training. ReST's main difference from self-training is that the Grow step of ReST generates synthetic exploration data for training with RL.</p>
<p>Expert Iteration (EI) Anthony (2021) proposed an RL framework which is a form of policy iteration approach that makes use of a planning mechanism. EI explicitly decomposes the RL problem into two parts: planning and generalisation. Similar to ReST, EI uses the policy to generate data and exploit it to learn a policy with RL. Unlike EI, ReST does not require any planning mechanism and it makes use of iterative Improve steps that enables it to leverage the gathered data more effectively.</p>
<p>Reasoning with language models The EI method inspired several related approaches (Uesato et al., 2022; Zelikman et al., 2022). Zelikman et al. (2022) proposed a technique called STAR that iteratively leverages a small number of rationales to fine-tune the model. Then, they sample rationales with the answers from the model and filter the generated answers by their correctness. Uesato et al. (2022) proposed a similar method which learns to solve math problems using learned reward models. Recently, Jung et al. (2023) proposed a method called "Impossible Distillation" similar to ours, which generates datasets for large language models by sampling from a suboptimal model and filters the low-quality examples with a filtering mechanism. This approach corresponds to ReST with a single Grow and Improve steps. In contrast to these methods, ReST can be trained with any offline RL losses, with or without planning and various filtering mechanisms. It can also deal with continuous-valued reward scores. Furthermore, ReST can perform iterative policy improvement on a fixed dataset with the Improve steps.</p>
<p>Iterated Learning (IL) IL is the process where an agent learns its behavior by being exposed to another agent's behavior, which itself learned it in the same way (Kirby et al., 2014). Recently, this approach was adopted for interactive language learning using deep learning (Lu et al., 2020a,b). IL differs from ReST as it operates in a multi-agent setting and does not use RL.</p>
<p>Self Imitation Learning (SIL) SIL learns a policy for an off-policy actor-critic algorithm where the policy tries to reproduce the good behaviour demonstrated by the agent (Oh et al., 2018). SIL achieves it by filtering out the unsuccessful trajectories from the replay buffer and training the agent only on the high-reward trajectories. In that sense, ReST can be considered to be closely related to the SIL-based approaches. The main difference is that ReST is agnostic to the underlying RL algorithm used to train the policy and, unlike SIL, does not necessitate a value function to filter out the unsuccessful trajectories. Also, ReST is applied to generative AI settings, which do not require interactions with an environment in an online fashion.</p>
<p>Reward ranked Fine-Tuning (RAFT) Concurrently to our work, Dong et al. (2023) proposed RAFT. RAFT can be interpreted as a particular case of ReST which uses only one Improve step for each Grow step, and relies on a filtering threshold which is a fixed quantile of the empirical distribution of the rewards of the current samples. The authors reported improvements over BC and PPO on a variety of language modeling and image generation tasks. Our experiments with ReST showed that multiple Improve steps with an increasing filtering threshold for one Grow step lead to further performance improvements.</p>
<h1>6. Discussion</h1>
<p>In this paper, we proposed an algorithm called ReST which is simple, has minimal hyper-parameters to tune, and is flexible to work with many designs of Grow and Improve steps. We studied the performance of ReST in machine translation as robust and established reward models are available for</p>
<p>this task. We experimented with different offline RL losses in the ReST loop, but found BC to perform the best for improving the reward model scores. Multiple steps of NLL training with progressively increasing filtering thresholds in the Improve step lead to continuous improvements in the model's reward on the holdout set. However, improvements in reward model scores do not necessarily reflect human preferences since the reward model is only a proxy for human preferences. The results indicate that one Grow step is the best option when considering human evaluation scores, even though rewards continue to grow with more Grow steps. To overcome this limitation, the reward models could be fine-tuned on the subset of $\mathrm{D}_{\mathrm{g}}$ annotated with human preferences similar to Bai et al. (2022) and Glaese et al. (2022), which we leave as future work. Let us note that the risk of overfitting to the reward model increases with the repeated iterations of the Grow steps; thus we believe it is essential to address this issue, especially in cases where multiple Grow steps are needed to train the model.</p>
<p>As we have seen, simple BC loss still outperforms many offline RL losses in terms of aligning the model with the reward model scores. However, we found that BC can overfit to the reward model so we explain it by the fact that learning value functions in RL is challenging due to sparse rewards, credit assignment problems, sensitivity to hyperparameters, and limited exploration in the Grow step. ReST could benefit from better RL exploration strategies at Grow step, such as MCTS (Leblond et al., 2021). The ability to exploit the generated data during the Grow step could result in a broader exploration of the state-action space and better generalization. Additionally, the determinism of the environment does not allow for large gains over BC for offline RL losses.</p>
<p>To conclude, ReST is a general and efficient approach. It can be applied when 1) a robust reward model of human preferences is available and 2) we are able to generate samples from the model at scale. Thus, it can be applied to many tasks within the language domain, such as summarization, turn-based dialogue, and other generative audio and video models. With several avenues for future exploration and applications, we believe that ReST is a useful growing batch RL methodology for RLHF.</p>
<p>Acknowledgements We would like to thank the members from the machine translation teams at Google and Google DeepMind for their inputs to the project during the brainstorming phases and for setting up the codebase that this project was developed upon (Yu et al., 2020). We would like to thank Matt Hoffman, Bobak Shahriari, Taylan Cemgil and Chris Dyer for the discussions about this project. We are grateful for the feedback provided by Bilal Piot for an early draft of this paper. We would also like to thank those responsible for various different frameworks that we used during the project such as the DeepMind JAX ecosystem (Babuschkin et al., 2020) and Launchpad (Yang et al., 2021).</p>
<h1>References</h1>
<p>A. Abdolmaleki, S. Huang, G. Vezzani, B. Shahriari, J. T. Springenberg, S. Mishra, D. Tirumala, A. Byravan, K. Bousmalis, A. György, et al. On multi-objective policy optimization as a tool for reinforcement learning. arXiv preprint arXiv:2106.08199, 2021.
R. Agarwal, M. Schwarzer, P. S. Castro, A. Courville, and M. G. Bellemare. Beyond tabula rasa: Reincarnating reinforcement learning. arXiv preprint arXiv:2206.01626, 2022.
T. W. Anthony. Expert iteration. PhD thesis, UCL (University College London), 2021.
I. Babuschkin, K. Baumli, A. Bell, S. Bhupatiraju, J. Bruce, P. Buchlovsky, D. Budden, T. Cai, A. Clark, I. Danihelka, C. Fantacci, J. Godwin, C. Jones, R. Hemsley, T. Hennigan, M. Hessel, S. Hou,</p>
<p>S. Kapturowski, T. Keck, I. Kemaev, M. King, M. Kunesch, L. Martens, H. Merzic, V. Mikulik, T. Norman, J. Quan, G. Papamakarios, R. Ring, F. Ruiz, A. Sanchez, R. Schneider, E. Sezener, S. Spencer, S. Srinivasan, L. Wang, W. Stokowiec, and F. Viola. The DeepMind JAX Ecosystem, 2020. URL http://github.com/deepmind.
Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
R. Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, 6(5):679-684, 1957.
D. Brandfonbrener, A. Bietti, J. Buckman, R. Laroche, and J. Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? Advances in Neural Information Processing Systems, 35:1542-1553, 2022.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 2020.
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.
M. Cettolo, J. Niehues, S. Stüker, L. Bentivogli, and M. Federico. Report on the 11th iwslt evaluation campaign. In Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign, 2014.
L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, 2021.
S. Clark, J. R. Curran, and M. Osborne. Bootstrapping pos-taggers using unlabelled data. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003, pages 49-55, 2003.
D. Donato, L. Yu, W. Ling, and C. Dyer. Mad for robust reinforcement learning in machine translation. arXiv preprint arXiv:2207.08583, 2022.
H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.
L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning, 2018.
M. Freitag, R. Rei, N. Mathur, C.-k. Lo, C. Stewart, G. Foster, A. Lavie, and O. Bojar. Results of the wmt21 metrics shared task: Evaluating metrics with expert-based human evaluations on ted and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, 2021.
M. Freitag, R. Rei, N. Mathur, C.-k. Lo, C. Stewart, E. Avramidis, T. Kocmi, G. Foster, A. Lavie, and A. F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46-68, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.2.</p>
<p>J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint arXiv:2210.10760, 2022.
B. Ghorbani, O. Firat, M. Freitag, A. Bapna, M. Krikun, X. Garcia, C. Chelba, and C. Cherry. Scaling laws for neural machine translation. arXiv preprint arXiv:2109.07740, 2021.
A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.
C. Gulcehre, S. G. Colmenarejo, Z. Wang, J. Sygnowski, T. Paine, K. Zolna, Y. Chen, M. Hoffman, R. Pascanu, and N. de Freitas. Regularized behavior value estimation. arXiv preprint arXiv:2103.09575, 2021.
J. He, J. Gu, J. Shen, and M. Ranzato. Revisiting self-training for neural sequence generation. arXiv preprint arXiv:1909.13788, 2019.
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems, 2022.
J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
J. Jung, P. West, L. Jiang, F. Brahman, X. Lu, J. Fisher, T. Sorensen, and Y. Choi. Impossible distillation: from low-quality model to high-quality dataset \&amp; model for summarization and paraphrasing. arXiv preprint arXiv:2305.16635, 2023.
S. Kirby, T. Griffiths, and K. Smith. Iterated learning and the evolution of language. Current Opinion in Neurobiology, 28:108-114, 2014.
P. Koehn, V. Chaudhary, A. El-Kishky, N. Goyal, P.-J. Chen, and F. Guzmán. Findings of the wmt 2020 shared task on parallel corpus filtering and alignment. In Proceedings of the Fifth Conference on Machine Translation, pages 726-742, 2020.
T. Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages $66-75,2018$.
A. Kumar, J. Hong, A. Singh, and S. Levine. Should i run offline reinforcement learning or behavioral cloning? In International Conference on Learning Representations, 2021.
S. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In M. Wiering and M. van Otterlo, editors, Reinforcement Learning: State-of-the-Art, pages 45-73. Springer Berlin Heidelberg, 2012.
R. Leblond, J.-B. Alayrac, L. Sifre, M. Pislar, L. Jean-Baptiste, I. Antonoglou, K. Simonyan, and O. Vinyals. Machine translation decoding beyond beam search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2021.</p>
<p>Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022.
Y. Lu, S. Singhal, F. Strub, A. Courville, and O. Pietquin. Countering language drift with seeded iterated learning. In International Conference on Machine Learning, 2020a.
Y. Lu, S. Singhal, F. Strub, O. Pietquin, and A. Courville. Supervised seeded iterated learning for interactive language learning. arXiv preprint arXiv:2010.02975, 2020b.
M. Mathieu, S. Ozair, S. Srinivasan, C. Gulcehre, S. Zhang, R. Jiang, T. Le Paine, K. Zolna, R. Powell, J. Schrittwieser, et al. Starcraft ii unplugged: Large scale offline reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lillicrap, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Learning Representations, 2016.
J. Oh, Y. Guo, S. Singh, and H. Lee. Self-imitation learning. In International Conference on Machine Learning, pages 3878-3887. PMLR, 2018.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback, 2022a.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022b.
R. Y. Pang and H. He. Text generation by learning from demonstrations. In International Conference on Learning Representations, 2021.
E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022.
D. A. Pomerleau. ALVINN: An autonomous land vehicle in a neural network. In Advances in Neural Information Processing Systems, pages 305-313, 1989.
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.
R. Rei, C. Stewart, A. C. Farinha, and A. Lavie. COMET: A neural framework for MT evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2020.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions on Information Theory, 11(3):363-371, 1965.</p>
<p>T. Sellam, D. Das, and A. Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.
I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arxiv:2305.17493, 2023.
J. Skalse, N. H. R. Howe, D. Krasheninnikov, and D. Krueger. Defining and characterizing reward hacking. In Advances in Neural Information Processing Systems, 2022.
H. F. Song, A. Abdolmaleki, J. T. Springenberg, A. Clark, H. Soyer, J. W. Rae, S. Noury, A. Ahuja, S. Liu, D. Tirumala, N. Heess, D. Belov, M. Riedmiller, and M. M. Botvinick. V-mpo: On-policy maximum a posteriori policy optimization for discrete and continuous control. In International Conference of Learning Representations, 2020.
A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.
N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 2020.
H. Sun, R. Wang, K. Chen, M. Utiyama, E. Sumita, and T. Zhao. Self-training for unsupervised neural machine translation in unbalanced training data scenarios. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, 2014.
J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.
J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021.
Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10687-10698, 2020.
F. Yang, G. Barth-Maron, P. Stańczyk, M. Hoffman, S. Liu, M. Kroiss, A. Pope, and A. Rrustemi. Launchpad: A programming model for distributed machine learning research. arXiv preprint arXiv:2106.04516, 2021.
D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd Annual Meeting of the Association for Computational Linguistics, 1995.
L. Yu, L. Sartran, P.-S. Huang, W. Stokoweic, D. Donato, S. Srinivasan, A. Andreev, W. Ling, S. Mokra, A. D. Lago, Y. Doron, S. Young, P. Blunsom, and C. Dyer. The DeepMind chinese-english document translation system at wmt2020. In Proceedings of the Fifth Conference on Machine Translation, 2020.</p>
<p>E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, 2022.
J. Zhang and C. Zong. Exploiting source-side monolingual data in neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2016.</p>
<h1>A. Appendix</h1>
<h2>A.1. RLHF for conditional language modeling as MDP</h2>
<p>We can formulate conditional language modeling as a sequence to sequence problem. The goal is to map a source sequence $\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots x_{L}\right)$ into a target sequence $\boldsymbol{y}=\left(y_{1}, y_{2}, \ldots . y_{T}\right)$, that is to learn a mapping from $\boldsymbol{x}$ to $\boldsymbol{y}$. Machine translation is a classic example of a sequence to sequence problem (Sutskever et al., 2014).</p>
<p>The model can be described as a Markov Decision Process (MDP) (Bellman, 1957). An MDP, $\mathcal{M} \stackrel{\text { def }}{=}$ $(\mathcal{S}, \mathcal{A}, T, r, d)$, consists of finite sets of states $\mathcal{S}$ and actions $\mathcal{A}$, a transition distribution $T\left(s^{\prime} \mid s, a\right), s, s^{\prime} \in$ $\mathcal{S}, a \in \mathcal{A}$, a reward function $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, and an initial state distribution $d: \mathcal{S} \rightarrow[0,1]$. In the offline setting the agent learns from a dataset $\mathcal{D}$ which contains sequences $\left(s_{n}, a_{n}, r_{n+1}\right)$. The dataset $\mathcal{D}$ is often assumed to be generated by an unknown behaviour policy $\mu$ characterised by a distribution over actions conditioned on the state: $\mu(a \mid s)$. The elements of MDP have the following meaning in conditional language modeling:</p>
<ul>
<li>States (s): The state $s_{n}$ consists of the input sequence and the partially generated output up to the $n$-th token: $s_{n}=\left[\boldsymbol{x}, \boldsymbol{y}_{1: n-1}\right]$.</li>
<li>Actions (a): The actions are the tokens $a_{n}=y_{n}$ to be generated by policy $(\pi)$.</li>
<li>Rewards (r): Rewards can be given or learned. In our experiments we train a deep neural network on human preferences to assigns a score to the full generated sequence. Reward is produced after end-of-sequence (EOS) token, and it is zero at all other steps.</li>
<li>Transition operator (T): The transition operator defines the dynamics of an environment: $T\left(s^{\prime} \mid a, s\right):=T\left(y_{n} \mid s_{n}=\left[\boldsymbol{x}, \boldsymbol{y}_{1: n-1}\right]\right)$. In our setup, it is a deterministic operator that concatenates newly produced token (action) to the current sequence (state).</li>
</ul>
<p>This RLHF formulation of conditional language modeling can be also seen as a contextual bandit problem with a very large action space.</p>
<h2>A.2. Negative results with offline RL</h2>
<p>In addition to the experiments from Section 4, we also run experiments with Q-learning with BVE type of one-step RL approaches and reward conditioned approaches, such as decision transformers (Chen et al., 2021). However, we could not obtain notable improvements with them over the supervised baseline. Q-function-based methods for learning a policy and value function performed worse than supervised learning when used as a policy even after training from initialization from a supervised checkpoint. This matches previous observations by Mathieu et al. (2021) who showed that offline Qlearning has difficulty in large action spaces (vocabulary size of 32000 tokens) and state-value based methods are preferable. Furthermore, as Brandfonbrener et al. (2022) showed, return-conditioned methods tend to learn sub-optimal policies on tasks with sparse continuous rewards.</p>
<h1>A.3. Data and Model details</h1>
<p>In all our experiments, the base policy architecture is a minor modification of a standard Transformer architecture (Vaswani et al., 2017). We use a vocabulary of 32000 tokens and a max sequence length of 128 at decoding time. We use the sentencepiece tool (Kudo, 2018) for building the tokenizers.</p>
<p>Grow step During the Grow step, we sampled from the latest checkpoint of the policy with tempered softmax using temperature 0.8 following the procedure proposed by Li et al. (2022) to generate the dataset. Moreovero, in our analysis, we found that temperature 0.8 often covers a broad range of rewards in the dataset.</p>
<p>Thresholds in Improve step If we set the threshold $\tau$ to a value such that $\tau \geq V^{\pi_{0}}$, and learn a policy $\pi^{1}$ on data filtered with this threshold, the updated average value $V^{\pi_{1}}$ of this policy satisfies $V^{\pi_{1}}&gt;V^{\pi_{0}}$. Then, iterative threshold ensures policy improvement in the Improve step. In our experiments, we do several improvement steps until we reach the maximum threshold. At each step, we trained the model for 500000 SGD steps and pick the checkpoint with the best reward model score.</p>
<p>IWSLT 2014 De-En We use train, validation and test sets from IWSLT 2014 De-En dataset (Cettolo et al., 2014) which includes source sentences in German with human translations (references) in English. In each Grow step, we generate 100 candidate translation for each source sentence in the training set, effectively yielding us $\left|\mathcal{D}_{\xi}\right|=16000000$. For this dataset, we use a tiny version of the Transformer encoder-decoder (Vaswani et al., 2017) architecture with the feedforward MLP layers of size 512, feedforward dimension of 1024, 4 attention heads and 6 encoder and decoder layers.</p>
<p>WMT 2020 Zh-En We use the source-reference pairs in Chinese and English from the work of Koehn et al. (2020) for our training, validation and test sets. Exact details on the datasets and preprocessing can be found in Yu et al. (2020). In each Grow step, we generate 25 candidates for each source sentence in the training set, effectively yielding us $\left|\mathcal{D}_{\xi}\right|=890000000$. We choose to generate 25 candidates per source as the WMT dataset is significantly ( $\sim 100$ times) larger than the IWSLT dataset. We use an architecture that mimics the Transformer-base encoder-decoder (Vaswani et al., 2017) architecture with model dimension 512, feedforward dimension of 2048, 8 attention heads and 6 encoder and decoder layers.</p>
<p>Web Domain En-Zh Finally, we use the in-house dataset for English to Chinese translation with custom training, fine-tuning and test sets. This training corpus is our biggest dataset ${ }^{5}$, so we use a modified version of Transformer-big encoder-decoder (Vaswani et al., 2017) architecture with model dimension 1024, feedforward dimension of 8192, 16 attention heads and 6 encoder and decoder layers.</p>
<p>In Table 2, we list all the datasets with their sizes. In all the experiments, unless stated otherwise, we report the average reward scores on the validation set.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">$\left|\mathcal{D}_{\mathrm{g}}\right|$</th>
<th style="text-align: right;"># Eval samples</th>
<th style="text-align: right;"># Candidates <br> per source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">IWSLT 2014 de-en</td>
<td style="text-align: right;">16000000</td>
<td style="text-align: right;">7466</td>
<td style="text-align: right;">100</td>
</tr>
<tr>
<td style="text-align: left;">WMT 2020 zh-en</td>
<td style="text-align: right;">890000000</td>
<td style="text-align: right;">2000</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td style="text-align: left;">Web Domain Finetuning en-zh</td>
<td style="text-align: right;">3000000</td>
<td style="text-align: right;">6000</td>
<td style="text-align: right;">1000</td>
</tr>
</tbody>
</table>
<p>Table 2 | Details of the datasets and their sizes used in ReST experiments.</p>
<h1>A.4. Reward model</h1>
<p>We used learned reward models that assign a score to the whole translation. We considered two types of reward models for translation (Freitag et al., 2022): i) Reference-free reward models estimate of how good a translation is based on the source and candidate sentences only, ii) Reference-based reward models additionally use the reference human translation to decide how good a candidate translation is. The reference-free reward models are more flexible since they do not require reference translations. Moreover, by their design, reference-based models assign higher scores for sentences similar to the reference, and this could happen even when the reference is erroneous or incomplete. Finally, reference-free models open possibilities to evaluate and discover candidate translations that supersede the quality of references. Thus, in our experiments, we chose to work with reference-free reward models. On the other hand, since the reference-free reward models are not grounded in a human reference translation, it is more vulnerable to the distribution shifts in the candidates generated by the model and reward hacking. In practice, we pre-computed and stored $R(\boldsymbol{x}, \boldsymbol{y})$ for the generated $\mathcal{D}_{\mathrm{g}}$.</p>
<p>During the development of the model, we relied on the methodology of "unit tests" for reward models where we tested them in various hand-crafted settings, such as on various permutations and repetitions of sentences. The unit-tests ensured the high quality of rewards generated for ReST. Even though we used the most robust available reference-free reward model, we found that it was still not perfect, and sometimes showed signs of delusions. For example, the reward of a translation occasionally increased when we repeated the sentence, independent of the initial quality of the translation.</p>
<h2>A.5. Alignment between human evaluation scores and the reward model scores</h2>
<p>In Figure 9 and 10, we showed the distribution of human preferences and reward model scores for ReST with BC and $\mathrm{G}=1 \quad \mathrm{I}=4$. Here, we found out that our reward model has a very high variance on samples with low human preference score from ReST (BC, G=1 I=4) and our supervised learning baseline (BC, $\mathrm{G}=0 \quad \mathrm{I}=0$ ). Our hypothesis for this is that the training dataset for the reward model was dominated by translations of high quality ${ }^{6}$. Thus the model overfitted to the samples with high scores, and it has high variance in its prediction over the samples with lesser quality. This gap can be addressed with the incremental retraining of the reward model.</p>
<h2>A.6. Results on test sets</h2>
<p>Figures 11 and 12 present the results on the test sets in IWSLT 2014 De-En and WMT 2020 Zh-En datasets. Similar to the validations set, ReST outperforms other value-based offline RL baselines. Also, increasing the number of both Grow and Improve steps helps to further increase the rewards.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9 | [IWSLT 2014 De-En]: distribution of human preference and reward model scores for ReST (BC, I=4, G=1) in side by side evaluation with supervised model. The human preference scores less than or equal to 3 have significantly higher variance in rewards compared to the human preference scores above 3. The plots on the left-hand side are for the samples generated from a supervised baseline and the right-hand side are for the samples generated with ReST.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10 | [WMT 2020 Zh-En]: distribution of human preference and reward model scores for ReST (BC, I=4, G=1) in side by side evaluation with supervised model. The human preference scores lower than or equal to 3 has a variance in terms of reward model scores. The reward model has less certainty on the scores of the candidates having lower scores.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11 | [IWSLT 2014 De-En]: ReST results on test set. All variations of ReST (purple) outperform supervised learning baseline (grey). As we increase the number of Grow and Improve steps, the reward of the model on the test set continues to increase.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12 | [WMT 2020 Zh-En]: ReST results on test set. We see that ReST clearly outperforms the baselines (right) and the reward of the model on the test set increases with the number of Improve steps.</p>
<h1>A.7. Additional experiments</h1>
<p>This section reports the results of several ablations in ReST which explain some of the design choices that we made.</p>
<p>GOLD loss compared to BC loss in Improve step In Figure 13, we compare GOLD against BC losses with the increasing number of ReST Improve steps on WMT 2020 Zh-En dataset. The performance of both BC and GOLD improves with the number of Improve steps, but BC performs better than GOLD in every setting.</p>
<p>Loss comparison on IWSLT 2014 De-En In Figure 14, we compare BVMPO, BC and GOLD losses in Improve step on IWSLT 2014. The results are consistent with WMT dataset: in short, ReST with BC loss and multiple Improve steps outperforms other approaches.</p>
<p>Selecting threshold based on percentiles of reward model scores Using a single threshold for all the source-candidate pairs may lead to a scenario with no training data for certain (harder) source</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13 | [WMT 2020 Zh-En]: GOLD loss vs BC loss in multi-step improvement. The performance of all methods improves with more steps, but BC loss always performs better than other offline RL losses that we tried.
<img alt="img-12.jpeg" src="img-12.jpeg" />
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14 | [IWSLT 2014 De-En]: BVMPO, BC, GOLD losses used in ReST. BC loss with three Improve steps yields the best results.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15 | [IWSLT 2014 De-En] percentiles: Filtering data based on percentiles of reward scores for each source candidate. ReST performs better with increasing percentile and the performance saturates after $p=90$.
sentences or contexts. Alternatively, we can filter the candidate sentences based on the percentile of the scores given their source. This way of filtering based on percentiles effectively results in changing the thresholds for each source sentence. The results are presented in Figure 15. As the filtering percentile increases, the performance increases, but it starts saturating, and the gains become smaller after $p=90$. The results are generally similar to those with global threshold-based filtering.</p>
<p>Selecting threshold based on linear interpolation between the mean and max scores of candidates An alternative way of specifying filters based on a source sentence is to compute the filter based on a max reward score ( $V^{\max }$ (source)) and the mean candidate score ( $V^{\text {mean }}$ (source)) for a give source sentence. Then, we select the threshold for each source sentence as $\tau_{\gamma}$ (source) $=$ $\gamma V^{\max }($ source $)+(1-\gamma) V^{\text {mean }}$ (source). At each Improve step, we increase the parameter $\gamma$. We report the results with respect to different $\gamma$ values in Figure 16. In a nutshell, computing the threshold by interpolating the max and mean scores for a given candidate gives results similar to the percentilebased way of computing the thresholds per source. Also we can see that the schedule of thresholds</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ The data came from WMT 2021 metrics shared task dataset (Freitag et al., 2021), which reviews the human evaluation of good translation models&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3424 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3424</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3424</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270702469</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.15468v2.pdf" target="_blank">MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> We propose MMLU-SR, a novel dataset designed to measure the true comprehension abilities of Large Language Models (LLMs) by challenging their performance in question-answering tasks with modified terms. We reasoned that an agent that “truly” understands a concept can still evaluate it when key terms are replaced by suitably defined alternate terms, and sought to differentiate such comprehension from mere text replacement. In our study, we modified standardized test questions by replacing a key term with a dummy word along with its definition. The key term could be in the context of questions, answers, or both questions and answers. Notwithstanding the high scores achieved by recent popular LLMs on the MMLU leaderboard, we found a substantial reduction in model performance after such replacement, suggesting poor comprehension. This new benchmark provides a rigorous benchmark for testing true model comprehension, and poses a challenge to the broader scientific community.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3424.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3424.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-mini (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI GPT-family model evaluated in this paper under 5-shot prompts (temperature=0); the paper does not specify architecture or parameter count. It was evaluated on MMLU and the new MMLU-SR variants to test symbol-robust reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated via the Gemini-benchmark pipeline with temperature set to 0 and 5-shot examples; used generation-format prompts asking for 'Answer: Letter of Choice.' The paper does not provide training details or parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMLU-SR (Question Only / Answer Only / Question and Answer)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A variant of the MMLU multiple-choice benchmark where key domain terms are replaced by arbitrary dummy tokens followed by explicit definitions; intended to test concept-level reasoning rather than memorized term recognition. Three variants: Question Only (Q-only), Answer Only (A-only), and both Question and Answer replaced (Q&A).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Symbol replacement (primary intervention) with dummy tokens + definitions; evaluated in 5-shot setting. Authors also experimented with zero-shot Chain-of-Thought prompting ('Let's think step by step') and a dedicated system instruction informing the model that some words are arbitrarily redefined for the test.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MMLU baseline (5-shot average): 0.771; MMLU-SR Q-only (5-shot): 0.710; A-only: 0.655; Q&A: 0.585 (accuracies as fractions).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original MMLU (5-shot average) = 0.771 (used as baseline in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; performance decreased on all MMLU-SR variants relative to baseline. Absolute drops: Q-only −0.061 (6.1 percentage points), A-only −0.116, Q&A −0.186; relative average percentage drops reported (paper) e.g., overall average relative drop ~7.91% for Q-only and ~24.12% for Q&A (see Table 2 / Table 14).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Significant drop when key terms replaced, especially in the Answer Only and Question+Answer settings; models appear to rely on memorized term associations instead of reasoning from provided definitions. Zero-shot CoT and a system instruction yielded only small improvements and did not rectify failures in the Q&A variant. The paper gives concrete failure examples where the model mis-assigned definitions (e.g., misidentifying a disease from a provided definition), and notes greater failures on Humanities/Other categories in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors analyze three dataset variants (Q-only, A-only, Q&A) as an ablation of information available; performance degrades monotonically from Q-only (least drop) to Q&A (largest drop). Per-category breakdowns (Humanities, Social Sciences, STEM, Other) show differing sensitivity; system-instruction and zero-shot CoT experiments (reported in appendices and Table 11) show slight improvements for Q-only and A-only but negligible for Q&A. The paper also reports that ~4.2% of model-generated definitions were manually corrected, indicating dataset construction noise/quality checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3424.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3424.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI GPT-family model (referred to in the paper) evaluated on MMLU and MMLU-SR under 5-shot prompting; shows the strongest resilience among evaluated models but still suffers a measurable performance drop under symbol replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated via the Gemini-benchmark pipeline with temperature=0, in 5-shot setting; prompts constrained to output a letter choice. The paper does not provide architecture/parameter details for gpt-4o within the text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMLU-SR (Question Only / Answer Only / Question and Answer)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MMLU with key-term symbol replacement plus explicit definitions to force reasoning from provided definitions rather than memorized tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Same symbol-replacement dataset; experiments included 5-shot prompting, zero-shot Chain-of-Thought ('Let's think step by step') and an explicit system instruction describing the symbol-replacement setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MMLU baseline (5-shot average): 0.845; MMLU-SR Q-only: 0.792; A-only: 0.757; Q&A: 0.695 (accuracies as fractions).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original MMLU (5-shot average) = 0.845.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; drops observed across all variants. Example absolute decreases: Q-only −0.053, A-only −0.088, Q&A −0.150. Relative average drops reported ~6.27% (Q-only) and ~17.75% (Q&A) per paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although more robust than some other models, gpt-4o still shows sizable degradation when answer choices are replaced and especially when both questions and answers are replaced; system instruction and CoT reduce but do not eliminate errors. The authors show examples where models infer broader categories but misalign the correct choice due to swapped tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Per-subject breakdown shows gpt-4o maintains relatively strong performance in Social Sciences and some STEM subjects, but substantial drops in moral reasoning scenarios and some mathematical subjects. The paper reports results with and without system instruction/CoT (appendices) showing only modest gains; category-wise relative drop tables (Table 2/3/5) are presented as analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3424.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3424.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gemini-1.5-pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gemini-1.5-pro (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Gemini-family model evaluated on MMLU and MMLU-SR in the paper; exhibits moderate robustness relative to other evaluated models but still declines under symbol replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gemini-1.5-pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated using the Gemini-benchmark pipeline with temperature=0 and 5-shot examples; the paper does not provide architecture or parameter counts for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMLU-SR (Question Only / Answer Only / Question and Answer)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Symbol-replacement variant of MMLU testing the ability to reason using provided definitions rather than memorized labels.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Symbol replacement plus 5-shot evaluation; system instruction (clarifying arbitrary redefinitions) and CoT prompting were tested; evaluation pipeline same as GPT-family models in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MMLU baseline (5-shot average): 0.832; MMLU-SR Q-only: 0.764; A-only: 0.758; Q&A: 0.694 (accuracies as fractions).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original MMLU (5-shot average) = 0.832.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; performance decreased across variants. Absolute decreases: Q-only −0.068, A-only −0.074, Q&A −0.138; relative average drops shown ~8.17% (Q-only) and ~16.59% (Q&A) in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Drops particularly noticeable in STEM Q-only for gemini-1.5-pro and in the combined Q&A setting; system instruction produced only modest gains. Paper includes error examples where gemini misaligns definitions leading to incorrect choices.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper provides per-category and per-subject analysis showing variable robustness (e.g., better retention in some STEM subjects than Humanities). The three-variant construction acts as an implicit ablation; results show A-only and Q&A are harder than Q-only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3424.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3424.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama3-70b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Llama3 family model with ~70B parameters (explicitly named in the paper) evaluated on MMLU and MMLU-SR; evaluated with log-likelihood scoring (lm-eval-harness) rather than generation decoding used for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama3-70b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama3 evaluated using the lm-evaluation-harness framework; responses determined via log-likelihood scoring rather than forced-generation; 5-shot setting used. Parameter count (70B) is given by the model name in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMLU-SR (Question Only / Answer Only / Question and Answer)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MMLU with terms replaced by dummy tokens + definitions to test reasoning from definitions rather than memorized tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Symbol replacement dataset; evaluated in 5-shot setting using lm-eval-harness (log-likelihood). No model-internal fine-tuning or additional tools reported; paper also tested higher-level interventions (system instruction) on other models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MMLU baseline (5-shot average): 0.765; MMLU-SR Q-only: 0.712; A-only: 0.602; Q&A: 0.576 (accuracies as fractions).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original MMLU (5-shot average) = 0.765.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; clear performance degradation: Q-only −0.053, A-only −0.163, Q&A −0.189 absolute decreases; relative drops reported ~6.93% (Q-only) and ~24.71% (Q&A) in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Large drops in Answer Only and Q&A settings; struggles most in High School Mathematics and other calculation-heavy subjects per subject breakdown. The change in evaluation method (log-likelihood) versus generation could affect comparability; authors note cross-model evaluation differences but still observe the symbol-replacement failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Per-subject tables show Llama3-70B more sensitive in math-heavy domains; paper's comparison across evaluation frameworks (generation vs log-likelihood) and dataset variants serves as analysis. The paper also reports the percent of human-corrected term definitions (~4.2%), which bounds dataset noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>GSM8K: A dataset of grade-school math word problems requiring multi-step reasoning. <em>(Rating: 2)</em></li>
                <li>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. <em>(Rating: 2)</em></li>
                <li>AGIEval: A human-centric benchmark for evaluating foundation models. <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Proving test set contamination in black box language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3424",
    "paper_id": "paper-270702469",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "gpt-4o-mini",
            "name_full": "gpt-4o-mini (OpenAI)",
            "brief_description": "An OpenAI GPT-family model evaluated in this paper under 5-shot prompts (temperature=0); the paper does not specify architecture or parameter count. It was evaluated on MMLU and the new MMLU-SR variants to test symbol-robust reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-mini",
            "model_description": "Evaluated via the Gemini-benchmark pipeline with temperature set to 0 and 5-shot examples; used generation-format prompts asking for 'Answer: Letter of Choice.' The paper does not provide training details or parameter counts.",
            "model_size": null,
            "reasoning_task_name": "MMLU-SR (Question Only / Answer Only / Question and Answer)",
            "reasoning_task_description": "A variant of the MMLU multiple-choice benchmark where key domain terms are replaced by arbitrary dummy tokens followed by explicit definitions; intended to test concept-level reasoning rather than memorized term recognition. Three variants: Question Only (Q-only), Answer Only (A-only), and both Question and Answer replaced (Q&A).",
            "method_or_intervention": "Symbol replacement (primary intervention) with dummy tokens + definitions; evaluated in 5-shot setting. Authors also experimented with zero-shot Chain-of-Thought prompting ('Let's think step by step') and a dedicated system instruction informing the model that some words are arbitrarily redefined for the test.",
            "performance": "MMLU baseline (5-shot average): 0.771; MMLU-SR Q-only (5-shot): 0.710; A-only: 0.655; Q&A: 0.585 (accuracies as fractions).",
            "baseline_performance": "Original MMLU (5-shot average) = 0.771 (used as baseline in this paper).",
            "improvement_over_baseline": "No improvement; performance decreased on all MMLU-SR variants relative to baseline. Absolute drops: Q-only −0.061 (6.1 percentage points), A-only −0.116, Q&A −0.186; relative average percentage drops reported (paper) e.g., overall average relative drop ~7.91% for Q-only and ~24.12% for Q&A (see Table 2 / Table 14).",
            "limitations_or_failures": "Significant drop when key terms replaced, especially in the Answer Only and Question+Answer settings; models appear to rely on memorized term associations instead of reasoning from provided definitions. Zero-shot CoT and a system instruction yielded only small improvements and did not rectify failures in the Q&A variant. The paper gives concrete failure examples where the model mis-assigned definitions (e.g., misidentifying a disease from a provided definition), and notes greater failures on Humanities/Other categories in some cases.",
            "ablation_or_analysis": "Authors analyze three dataset variants (Q-only, A-only, Q&A) as an ablation of information available; performance degrades monotonically from Q-only (least drop) to Q&A (largest drop). Per-category breakdowns (Humanities, Social Sciences, STEM, Other) show differing sensitivity; system-instruction and zero-shot CoT experiments (reported in appendices and Table 11) show slight improvements for Q-only and A-only but negligible for Q&A. The paper also reports that ~4.2% of model-generated definitions were manually corrected, indicating dataset construction noise/quality checks.",
            "uuid": "e3424.0",
            "source_info": {
                "paper_title": "MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "gpt-4o",
            "name_full": "gpt-4o (OpenAI)",
            "brief_description": "An OpenAI GPT-family model (referred to in the paper) evaluated on MMLU and MMLU-SR under 5-shot prompting; shows the strongest resilience among evaluated models but still suffers a measurable performance drop under symbol replacement.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_description": "Evaluated via the Gemini-benchmark pipeline with temperature=0, in 5-shot setting; prompts constrained to output a letter choice. The paper does not provide architecture/parameter details for gpt-4o within the text.",
            "model_size": null,
            "reasoning_task_name": "MMLU-SR (Question Only / Answer Only / Question and Answer)",
            "reasoning_task_description": "MMLU with key-term symbol replacement plus explicit definitions to force reasoning from provided definitions rather than memorized tokens.",
            "method_or_intervention": "Same symbol-replacement dataset; experiments included 5-shot prompting, zero-shot Chain-of-Thought ('Let's think step by step') and an explicit system instruction describing the symbol-replacement setup.",
            "performance": "MMLU baseline (5-shot average): 0.845; MMLU-SR Q-only: 0.792; A-only: 0.757; Q&A: 0.695 (accuracies as fractions).",
            "baseline_performance": "Original MMLU (5-shot average) = 0.845.",
            "improvement_over_baseline": "No improvement; drops observed across all variants. Example absolute decreases: Q-only −0.053, A-only −0.088, Q&A −0.150. Relative average drops reported ~6.27% (Q-only) and ~17.75% (Q&A) per paper tables.",
            "limitations_or_failures": "Although more robust than some other models, gpt-4o still shows sizable degradation when answer choices are replaced and especially when both questions and answers are replaced; system instruction and CoT reduce but do not eliminate errors. The authors show examples where models infer broader categories but misalign the correct choice due to swapped tokens.",
            "ablation_or_analysis": "Per-subject breakdown shows gpt-4o maintains relatively strong performance in Social Sciences and some STEM subjects, but substantial drops in moral reasoning scenarios and some mathematical subjects. The paper reports results with and without system instruction/CoT (appendices) showing only modest gains; category-wise relative drop tables (Table 2/3/5) are presented as analysis.",
            "uuid": "e3424.1",
            "source_info": {
                "paper_title": "MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "gemini-1.5-pro",
            "name_full": "gemini-1.5-pro (Google)",
            "brief_description": "A Gemini-family model evaluated on MMLU and MMLU-SR in the paper; exhibits moderate robustness relative to other evaluated models but still declines under symbol replacement.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gemini-1.5-pro",
            "model_description": "Evaluated using the Gemini-benchmark pipeline with temperature=0 and 5-shot examples; the paper does not provide architecture or parameter counts for this model.",
            "model_size": null,
            "reasoning_task_name": "MMLU-SR (Question Only / Answer Only / Question and Answer)",
            "reasoning_task_description": "Symbol-replacement variant of MMLU testing the ability to reason using provided definitions rather than memorized labels.",
            "method_or_intervention": "Symbol replacement plus 5-shot evaluation; system instruction (clarifying arbitrary redefinitions) and CoT prompting were tested; evaluation pipeline same as GPT-family models in the paper.",
            "performance": "MMLU baseline (5-shot average): 0.832; MMLU-SR Q-only: 0.764; A-only: 0.758; Q&A: 0.694 (accuracies as fractions).",
            "baseline_performance": "Original MMLU (5-shot average) = 0.832.",
            "improvement_over_baseline": "No improvement; performance decreased across variants. Absolute decreases: Q-only −0.068, A-only −0.074, Q&A −0.138; relative average drops shown ~8.17% (Q-only) and ~16.59% (Q&A) in paper tables.",
            "limitations_or_failures": "Drops particularly noticeable in STEM Q-only for gemini-1.5-pro and in the combined Q&A setting; system instruction produced only modest gains. Paper includes error examples where gemini misaligns definitions leading to incorrect choices.",
            "ablation_or_analysis": "Paper provides per-category and per-subject analysis showing variable robustness (e.g., better retention in some STEM subjects than Humanities). The three-variant construction acts as an implicit ablation; results show A-only and Q&A are harder than Q-only.",
            "uuid": "e3424.2",
            "source_info": {
                "paper_title": "MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "llama3-70b",
            "name_full": "Llama3-70B",
            "brief_description": "A Llama3 family model with ~70B parameters (explicitly named in the paper) evaluated on MMLU and MMLU-SR; evaluated with log-likelihood scoring (lm-eval-harness) rather than generation decoding used for other models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama3-70b",
            "model_description": "Llama3 evaluated using the lm-evaluation-harness framework; responses determined via log-likelihood scoring rather than forced-generation; 5-shot setting used. Parameter count (70B) is given by the model name in the paper.",
            "model_size": "70B",
            "reasoning_task_name": "MMLU-SR (Question Only / Answer Only / Question and Answer)",
            "reasoning_task_description": "MMLU with terms replaced by dummy tokens + definitions to test reasoning from definitions rather than memorized tokens.",
            "method_or_intervention": "Symbol replacement dataset; evaluated in 5-shot setting using lm-eval-harness (log-likelihood). No model-internal fine-tuning or additional tools reported; paper also tested higher-level interventions (system instruction) on other models.",
            "performance": "MMLU baseline (5-shot average): 0.765; MMLU-SR Q-only: 0.712; A-only: 0.602; Q&A: 0.576 (accuracies as fractions).",
            "baseline_performance": "Original MMLU (5-shot average) = 0.765.",
            "improvement_over_baseline": "No improvement; clear performance degradation: Q-only −0.053, A-only −0.163, Q&A −0.189 absolute decreases; relative drops reported ~6.93% (Q-only) and ~24.71% (Q&A) in paper tables.",
            "limitations_or_failures": "Large drops in Answer Only and Q&A settings; struggles most in High School Mathematics and other calculation-heavy subjects per subject breakdown. The change in evaluation method (log-likelihood) versus generation could affect comparability; authors note cross-model evaluation differences but still observe the symbol-replacement failure modes.",
            "ablation_or_analysis": "Per-subject tables show Llama3-70B more sensitive in math-heavy domains; paper's comparison across evaluation frameworks (generation vs log-likelihood) and dataset variants serves as analysis. The paper also reports the percent of human-corrected term definitions (~4.2%), which bounds dataset noise.",
            "uuid": "e3424.3",
            "source_info": {
                "paper_title": "MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "GSM8K: A dataset of grade-school math word problems requiring multi-step reasoning.",
            "rating": 2,
            "sanitized_title": "gsm8k_a_dataset_of_gradeschool_math_word_problems_requiring_multistep_reasoning"
        },
        {
            "paper_title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs.",
            "rating": 2,
            "sanitized_title": "drop_a_reading_comprehension_benchmark_requiring_discrete_reasoning_over_paragraphs"
        },
        {
            "paper_title": "AGIEval: A human-centric benchmark for evaluating foundation models.",
            "rating": 2,
            "sanitized_title": "agieval_a_humancentric_benchmark_for_evaluating_foundation_models"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Proving test set contamination in black box language models.",
            "rating": 1,
            "sanitized_title": "proving_test_set_contamination_in_black_box_language_models"
        }
    ],
    "cost": 0.014633499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models
4 Oct 2024</p>
<p>Wentian Wang 
Usc Sarthak 
Jain Rutgers 
Paul Kantor 
Jacob Feldman 
Hao Wang </p>
<p>Rutgers &amp; UW-Madison</p>
<p>Lazaros Gallos Rutgers</p>
<p>MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models
4 Oct 2024642FAE4012480BFD1A4D6C9A4447DD4DarXiv:2406.15468v2[cs.CL]
We propose MMLU-SR, a novel dataset designed to measure the true comprehension abilities of Large Language Models (LLMs) by challenging their performance in questionanswering tasks with modified terms.We reasoned that an agent that "truly" understands a concept can still evaluate it when key terms are replaced by suitably defined alternate terms, and sought to differentiate such comprehension from mere text replacement.In our study, we modified standardized test questions by replacing a key term with a dummy word along with its definition.The key term could be in the context of questions, answers, or both questions and answers.Notwithstanding the high scores achieved by recent popular LLMs on the MMLU leaderboard, we found a substantial reduction in model performance after such replacement, suggesting poor comprehension.This new benchmark provides a rigorous benchmark for testing true model comprehension, and poses a challenge to the broader scientific community.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have achieved impressive quantitative performance on a wide range of benchmarks, natural language processing (Zellers et al., 2019;Wang et al., 2019), general knowledge question-answering (Hendrycks et al., 2021;Clark et al., 2018), and coding (Chen et al., 2021;others, 2021).Additionally, by integrating with some advanced prompting techniques, such as Chain-of-Thought (CoT) (Wei et al., 2023) and its variants (Yao et al., 2023;Trivedi et al., 2023;Zhang et al., 2023), LLMs seem to exhibit a certain level of reasoning abilities including mathematics (Zhang et al., 2024) and even causal inference/discovery (Vashishtha et al., 2023;Wang et al., 2020;Mao et al., 2022;Gupta et al., 2021).However, some studies (Oren et al., 2023) have * Visiting student at Rutgers ML Lab. raised concerns about data leakage (i.e., training models on the test sets), potentially rendering these results unreliable.These seemingly contradictory findings prompt the question of whether LLMs are genuinely performing reasoning tasks or merely predicting the next token.If LLMs are truly capable of reasoning, they should remain unaffected by the replacement of key symbols within the test set.</p>
<p>A hallmark of human intelligence is the ability to handle abstract concepts and to associate them with arbitrary terms (Penn et al., 2008).With a few exceptions such as onomatopoeia, the connection between particular words and particular meanings is arbitrary, and identical concepts are invoked by different words in different human languages (e.g.dog vs chien).Similarly, human reasoners are capable of analogizing structural relationships from one domain to another, meaning that conceptual equivalence can be retained even when details change (Gentner and Medina, 1998).It follows that true human-like comprehension should be unimpaired when terms are substituted for synonymous terms, as long as the substitution is comprehensibly defined.</p>
<p>We wondered whether LLM peformance reflects true human-like comprehension in this sense, or whether it relies heavily on the specific terms used on training corpora.To assess this, we propose MMLU-SR, a new benchmark dataset that uses symbol replacement to remove some important terms from the questions and answers as shown in Figure 1.Instead of relying on memorized terms, this approach tests whether LLMs can reason using the definitions and concepts of those terms, ensuring a more robust evaluation of their understanding.</p>
<p>Our evaluations on GPT-3.5/4,Gemini, and Llama3 families showed significantly lower performance on MMLU-SR compared to the original MMLU, demonstrating the effectiveness of our approach in preventing models from exploiting memorized data.MMLU-SR thus provides a more chal-Figure 1: Illustration of our MMLU-SR testing scenarios.The red-colored and green-colored words represent the original symbols in the MMLU dataset showing in answers and questions, which are replaced in the MMLU-SR dataset with random words followed by their definitions, shown in orange text.The example question from the MMLU dataset is correctly answered by both GPT-3.5-turbo and ChatGPT-4.However, the modified question from the MMLU-SR "Question and Answer" dataset is answered incorrectly by both models.</p>
<p>lenging and revealing test of LLMs' true reasoning abilities and understanding.</p>
<p>Our findings indicate that while current LLMs excel on traditional benchmarks, they face substantial difficulties when key terms are replaced, highlighting the need for benchmarks like MMLU-SR to ensure robust and comprehensive evaluation of language models.</p>
<p>2 Related Works MMLU Variants Benchmarks.MMLU Variants such as CMMLU (Li et al., 2024) and TMMLU+ (Tam et al., 2024) are adaptations of the MMLU benchmark for non-English languages; they translate the original MMLU questions and answers into other languages, providing a way to evaluate language models' performance in non-English contexts.These benchmarks are crucial for assessing the generalizability and robustness of models across different languages and cultural settings.</p>
<p>They preserve the original structure and intent of MMLU while enabling a broader examination of multilingual capabilities.</p>
<p>Reasoning Capabilities Benchmarks.Several advanced reasoning benchmarks have been developed to evaluate the reasoning capabilities of language models.AGIEval (Zhong et al., 2023) includes standardized tests and civil service exams to assess reasoning and problem-solving skills in academic and professional scenarios.BoolQ (Clark et al., 2019) comprises over 15,000 real yes/no questions paired with Wikipedia passages to test the ability of models to infer answers from contextual information.GSM8K (Cobbe et al., 2021) features 8.5K grade-school math problems requiring multistep operations, targeting the evaluation of basic to intermediate mathematical problem-solving abilities.DROP (Dua et al., 2019), an adversariallycreated reading comprehension benchmark, challenges models to navigate references and perform The hypothetical technique "Hat," described as measuring enzyme activity within a sample.Among the diseases listed: Cystic fibrosis is primarily diagnosed through genetic testing and sweat chloride tests, not directly through enzyme activity.Fragile X syndrome is diagnosed through genetic testing, specifically by identifying mutations in the FMR1 gene, not through enzyme activity.Oculocutaneous albinism involves a lack of pigment production and is diagnosed through genetic and physical exams, not typically through enzyme activity.Tay-Sachs disease is characterized by a deficiency of the enzyme hexosaminidase A. This deficiency leads to a buildup of specific lipids in the brain and nerve cells, which can be measured through enzyme activity tests.Given this, the best answer is: D. Tay-Sachs disease.discrete operations such as addition and sorting, thus evaluating their capacity to understand complex texts and execute logical reasoning tasks.Beyond purely language-based evaluation, on the multimodal front, MMNeedle (Wang et al., 2024) introduced one of the first multimodal benchmarks to evaluate long-context multimodal reasoning capabilities of multimodal LLMs.</p>
<p>Model Input</p>
<p>Question Only</p>
<p>Model Output</p>
<p>Unlike advanced reasoning benchmarks and MMLU variants for language extension (e.g., CMMLU and TMMLU+), our MMLU-SR benchmark introduces a novel approach.It enhances the challenge by replacing key words within the questions with random words, each paired with its definition, to differentiate from other benchmarks.This approach targets the models' reasoning abilities by preventing reliance on memorized terms or vocabularies.By altering key symbols, MMLU-SR ensures that the evaluation focuses on the models' understanding and reasoning, rather than their recognition of specific vocabulary, thus providing a more robust assessment of their true cognitive capabilities.We build our benchmark on the MMLU dataset because it encompasses a wide range of subjects across various domains, including Humanities, Social Sciences, STEM, and Other fields.This diverse subject matter ensures a comprehensive evaluation of language models' reasoning capabilities, in contrast to other reasoning benchmarks that often focus exclusively on specific STEM subjects.</p>
<p>MMLU-SR Dataset</p>
<p>Dataset Construction</p>
<p>We have developed the MMLU-SR benchmark to rigorously evaluate the reasoning and understanding capabilities of LLMs.Inspired by ObjectNet (Borji, 2020), our benchmark contains three subsets: "Question Only", "Answer Only", and "Question and Answer", each offering a unique perspective on the data to comprehensively assess LLM performance.To reduce human efforts in some redundant tasks, we proposed an automatic process to generate our dataset.</p>
<ol>
<li>
<p>Term Extraction and Definition Generation: We extracted key terms from the questions and answers across all 57 subjects using the assistance of gpt-3.5-turbo.The process involved careful few-shot prompting, and we separately extracted the contexts of questions or answers alone to ensure the model focused on extracting terms rather than solving the questions.We also retrieved appropriate definitions within the specific subject for each extracted term.For terms where the automated process provided irrelevant or inaccurate definitions, we manually reviewed and corrected these entries (see Appendix F for details on the extent of manual modifications).</p>
</li>
<li>
<p>Dictionary Creation: Once the terms and their definitions were extracted for each subject, we created JSON files where the terms served as keys and the definitions as values.This dictionary served as the basis for replacing terms in the questions and answers.</p>
</li>
<li>
<p>Data Replacement: Using the created dictionaries, we replaced the key terms in the questions with random dummy words followed by their definitions to create the "Question Only" dataset.Similarly, we did this for the answers to form the "Answer Only" dataset.This ensured that the context remained humanreadable but required reasoning to infer the replaced terms.Some definitions and replacements required manual adjustments to ensure clarity and accuracy.</p>
</li>
<li>
<p>Combining Question and Answer Sets: After creating the "Question Only" and "Answer Only" datasets, we combined them to form the "Question and Answer" dataset.This step involved ensuring that the terms were consistently replaced across both questions and answers, maintaining the coherence of the dataset.</p>
</li>
</ol>
<p>Final Adjustments:</p>
<p>All CSV sheets were encoded in UTF-8 without headers.We manually fixed any typos that existed in the original MMLU dataset to ensure the quality and readability of the MMLU-SR dataset.</p>
<p>The MMLU-SR dataset was created using these meticulous steps.We formed both development and test sets, with the development set used for fewshot learning and the test set reserved for evaluation.This structured approach ensured that the dataset effectively tested the reasoning abilities of LLMs, differentiating between simple pattern recognition and genuine understanding.To effectively demonstrate how our MMLU-SR dataset can challenge more sophisticated models, we use Figure 2 8, Table 9, and Table 10 in Appendix B.</p>
<p>"Question Only" Dataset</p>
<p>Our "Question Only" dataset replaces key symbols with dummy words in most questions from the original MMLU dataset and keeps the answer choices unchanged.However, some straightforward questions, such as those involving only mathematical operations and numbers or simple questions like "Which of the following statements is true?", remain unmodified.Generally, we changed at least one important term in the context, replacing it with a random dummy word followed by its definition; sometimes, multiple terms are replaced in the question to further test the model's reasoning capabilities.</p>
<p>Figure 2 illustrates an example where the dummy word 'Hat' is used to replace term 'enzyme assay' and is defined as "the name given to any laboratory technique that measures enzyme activity within a sample."In this case, ChatGPT-4 successfully recognized 'Hat' and understood that the critical aspect was whether the diseases listed in the answer choices could be identified through enzyme activity.The model systematically eliminated each answer option, except for the last one, as it was the only choice that could be measured through enzyme activity tests.This setup effectively tests the model's ability to comprehend and reason based Question: Enzyme assay can be used to identify carriers of: Answer: A. Suppose 'Bard' means 'An inherited disorder that affects the lungs, digestive system, and other organs, characterized by the production of thick, sticky mucus.' Bard.B. Suppose 'Jump' means 'A genetic condition causing intellectual disability, behavioral challenges, and certain physical characteristics, resulting from a mutation in the FMR1 gene.' Jump.C. Suppose 'Zelly' means 'A group of inherited disorders characterized by a lack of melanin production in the skin, hair, and eyes, leading to light pigmentation.'Zelly.D. Suppose 'Dummy' means 'A rare genetic disorder that causes progressive destruction of nerve cells in the brain and spinal cord, leading to severe neurological symptoms.'Dummy.Question: Suppose 'Hat' means "the name given to any laboratory technique that measures enzyme activity within a sample."Hat can be used to identify carriers of: Answer: A. Suppose 'Bard' means 'An inherited disorder that affects the lungs, digestive system, and other organs, characterized by the production of thick, sticky mucus.' Bard.B. Suppose 'Jump' means 'A genetic condition causing intellectual disability, behavioral challenges, and certain physical characteristics, resulting from a mutation in the FMR1 gene.' Jump.C. Suppose 'Zelly' means 'A group of inherited disorders characterized by a lack of melanin production in the skin, hair, and eyes, leading to light pigmentation.'Zelly.D. Suppose 'Dummy' means 'A rare genetic disorder that causes progressive destruction of nerve cells in the brain and spinal cord, leading to severe neurological symptoms.'Dummy.</p>
<p>Given that 'Hat' is defined as any laboratory technique measuring enzyme activity, we can analyze:</p>
<p>Bard: refers to Cystic Fibrosis, and can be screened for using enzyme assays Jump: refers to Fragile X Syndrome.Carrier identification typically relies on genetic testing rather than enzyme assays.</p>
<p>Zelly: refers to albinism.Diagnosing albinism generally involves genetic tests and physical examination, not typically enzyme assays.</p>
<p>Dummy: refers to a neurological condition like Huntington's Disease.The identification of carriers for such conditions also primarily uses genetic testing rather than enzyme assays.</p>
<p>Thus, the use of 'Hat' (enzyme assays) is most applicable to: A. Bard (Cystic Fibrosis)</p>
<p>Model Input</p>
<p>Question and Answer</p>
<p>Model Output on the provided definitions, rather than relying on pre-trained knowledge of the term 'enzyme assay'.</p>
<p>"Answer Only" Dataset</p>
<p>Our "Answer Only" dataset modifies the answer choices in most text from the original MMLU dataset similarly to the "Question Only" dataset and keeps the questions unchanged.We ensure that at least one answer choice is modified among all four choices.In specific cases, such as "moral scenarios" where the answers are simple TRUE or FALSE choices, we make substitutions like "Suppose 'Jack' means 'True'."and "Suppose 'Luck' means 'False'."Despite these modifications, Table 3 still shows the accuracy drops among all models compared to the original "moral scenarios" sheet from MMLU.This underscores the challenge posed to models in adapting to these symbolic substitutions.Figure 3 demonstrates that ChatGPT-4 was able to recognize the replaced terms in answer choices A, B, and C, identifying 'Bard' as 'Cystic Fibrosis', 'Jump' as 'Fragile X Syndrome', and 'Zelly' as 'Albinism'.The model incorrectly identified the term 'Dummy' as 'Huntington's Disease', while the correct term is 'Tay-Sachs Disease'.Both disorders are indeed genetic, but they are distinct in their genetic causes and manifestations.It appears that ChatGPT-4, focusing on the broader category of 'genetic disorder' from the provided definition, inadvertently linked the description to the wrong disease.Such misidentification led the model to persist in incorrectly affirming that choice A ('Bard' as 'Cystic Fibrosis') was the correct answer (it is not).</p>
<p>"Question and Answer" Dataset</p>
<p>Our "Question and Answer" dataset integrates elements from both the "Question Only" and "Answer Only" datasets, replacing fundamental terms in both the questions and answer choices with dummy words followed by their definitions.As illustrated in Figure 4, ChatGPT-4 successfully interpreted the original terms for each replaced term in answer choices A through C.However, similar to the results seen in Figure 3, the model incorrectly recognized the term in the last answer choice D ('Dummy' for Huntington's Disease), leading to an incorrect answer.This outcome contrasts with Figure 2, where ChatGPT-4 correctly answered the questions when only the questions were modified.This illustrates that as complexity in context increases, with terms being replaced in both questions and answers, the model struggles to accurately identify the correct original term, consequently leading to an incorrect answer choice.</p>
<p>Experiments</p>
<p>Evaluation Protocol</p>
<p>We evaluated seven models across OpenAI, Gemini, Llama families: gpt-3.5-turbo,gpt-4o-mini, gpt-4o, gemini-1.0-pro,gemini-1.5-pro,llama3-8b, and llama3-70b.</p>
<p>The evaluation for GPT and Gemini models was conducted using the Gemini-benchmark pipeline (Akter et al., 2023).For these models, we set the temperature parameter to 0 and utilized carefully crafted prompts that required responses in the format of "Answer: Letter of Choice."This approach ensures that the generated responses are directly comparable and suitable for evaluation.Additionally, both models were evaluated in the 5-shot setting, using examples from our development dataset to enhance their contextual understanding.Llama3 was evaluated using the lm-evaluation-harness framework (Gao et al., 2023).This model employed a different evaluation strategy; it uses log likelihood to determine the model's responses.Consistent with the other models, Llama3 also uses the same 5-shot setting, ensuring a standardized comparison across all tests.The complete results of all seven models are available in Appendix E.</p>
<p>Results and Analysis</p>
<p>General Trend.</p>
<p>Table 1 shows the accuracy of the four models gpt-4o-mini, gpt-4o, gemini-1.5-pro,and llama3-70b evaluated in both MMLU and our MMLU-SR.The data highlights how each model performs in the Humanities, Social Sciences, STEM, and Other academic fields, providing average scores for each subset.We observe consistent drop in model performance across all subsets when transitioning from the standard MMLU dataset to the more challenging MMLU-SR dataset, as evidenced by the decline in average accuracy from 0.771 on the MMLU dataset to 0.710, 0.655, and 0.585, on our MMLU-SR's "Question Only", "Answer Only", and "Question and Answer" subsets, respectively, for the gpt-4o-mini model.This trend of decreased per-formance is similarly observed in the other models.</p>
<p>We observe a crucial trend in decreasing accuracy across datasets: The "Question Only" dataset experiences the least drop, followed by the "Answer Only" dataset, with the most significant decline occurring in the "Question and Answer" dataset.This trend can be primarily attributed to two major reasons: (1) When only the question is modified, the model retains the original answer choices, facilitating the inference of the modified question's meaning; in contrast, altering the answer choices removes this contextual aid, challenging the model's ability to correctly match the question with the appropriate answer.(2) Answer choices are typically more concise and therefore lack the extensive context found in questions; consequently, replacing terms in the answers not only introduces ambiguity but also demands more complex inferential reasoning, disrupting the model's learned pattern-recognition strategies and resulting in a greater accuracy drop.The observations above also justify the design of our MMLU-SR on three variants (i.e., "Question Only", "Answer Only", and "Question and Answer").</p>
<p>Accuracy Drop in Each Category.Table 2 shows several aspects in the relative percentage drop of accuracy in MMLU-SR compared to that in MMLU across different categories for gpt-4o-mini, gpt-4o, gemini-1.5-pro,and llama3-70b:</p>
<ol>
<li>Humanities and Social Sciences.</li>
</ol>
<p>For gpt-4o-mini and gpt-4o, the accuracy drops significantly in the Humanities category, with a slightly lower drop in Social Sciences.The gemini-1.5-proshows the smallest performance decline in the Humanities and Social Science categories compared to the other two models evaluated.llama3-70b exhibits a pattern similar to gpt-4o-mini, with the Humanities and Social Sciences categories showing a moderate percentage drop, though slightly higher than gpt-4o-mini, in the "Answer Only" and "Question and Answer" dataset.</p>
<p>STEM.</p>
<p>For gemini-1.5-pro and llama3-70b, the STEM category shows a relatively moderate decrease in accuracy across the MMLU-SR datasets.Notably, gemini-1.5-proexperiences the highest drop of 12.72% in the "Question Only" dataset, indicating some sensitivity in this area.llama3-70b demonstrates a similar trend, with the highest drop of 9.47% in the STEM category, suggesting both models retain some robustness in STEM but are still impacted by symbol replacement.On the other hand, gpt-4o-mini experiences a higher drop in the "Answer Only" and "Question and Answer" datasets, particularly with a 22.93% drop in the latter, highlighting its relative vulnerability in this domain compared to gemini-1.5-proand llama3-70b.</p>
<p>3.</p>
<p>Other.The Other category generally shows a significant drop across all models and datasets, with the highest drops often observed in the "Question and Answer" dataset.For example, gpt-4o-mini experiences a notable drop of 25.19%, the highest among all categories and models, indicating a high sensitivity to contextual changes in this area.Similarly, llama3-70b follows closely with a 27.40% drop, which is the highest in the Other category for this model.gemini-1.5-proalso shows a substantial drop of 16.44%, though slightly less compared to the other models, suggesting that the "Other" category, like Humanities, might be more context-dependent and hence more susceptible to performance degradation when symbols are replaced.</p>
<p>Detailed Accuracy Drop in Each Subject.Table 3 shows a detailed comparison of accuracy scores across different models evaluated on various subjects in the Humanities category.The MMLU scores serve as a baseline for comparison.gpt-4o demonstrates exceptional performance across most subjects in this category, often leading in accuracy, particularly in complex subjects like Philosophy and International Law.gemini-1.5-proalso shows strong performance, but gpt-4o frequently matches or exceeds its accuracy.Notably, gpt-4o performs particularly well in subjects like High School World History and Jurisprudence.However, all models continue to struggle with Moral Scenarios, where the accuracy score drops significantly, particularly for llama3-70b, which shows a drastic decrease, reflecting a higher sensitivity to the challenges posed by the MMLU-SR datasets Table 4 shows a detailed comparison of accuracy across different models evaluated on various subjects in the Social Science category.We observe that all models perform exceptionally well in Social Science on MMLU, particularly in High School Government and Politics, where gpt-4o achieves an impressive accuracy of 0.984.While there is still a drop in accuracy from MMLU to MMLU-SR's "Question and Answer" dataset, gpt-4o demonstrates remarkable resilience, maintaining accuracy levels around 0.7∼0.9across most subjects.This performance significantly outpaces the other models, particularly in subjects like High School Psychology and Sociology.The drop in accuracy, though less pronounced for gpt-4o, still illustrates how our symbol replacement method increases difficulty, effectively stress-testing the models' reasoning capabilities versus mere memorization of pre-trained terms.</p>
<p>Table 5 shows a detailed comparison of accuracy across various STEM subjects for different models.Each model demonstrated varying degrees of suc-cess across the subjects, with notable difficulties in some areas.College Mathematics and High School Mathematics remain challenging for all models, including gpt-4o, with accuracy dropping to around 0.4 to 0.5 in MMLU-SR's "Question and Answer" dataset.However, gpt-4o shows marked improvement in subjects like Astronomy, College Biology, and High School Biology, maintaining high accuracy even in the more challenging MMLU-SR datasets.The subject with the lowest accuracies among all models is still High School Mathematics, where llama3-70b struggles the most, especially in the Answer Only" and "Question and Answer" datasets.Similarly, College Physics and Abstract Algebra also show significant drops in accuracy across all models, highlighting the persistent challenges in subjects involving extensive calculations and complex problem-solving.</p>
<p>Table 6 shows a detailed comparison of accuracy scores across different models evaluated on various subjects in the Other category.We observe that gpt-4o performs exceptionally well in MMLU, with accuracy consistently above 0.9 in most subjects, significantly outperforming other models.Marketing stands out with a particularly high accuracy of 0.949 for gpt-4o, indicating outstanding performance in this subject.Professional Accounting shows improved performance with gpt-4o, achieving an accuracy of 0.766 in MMLU.Virology remains challenging, but gpt-4o shows improvement with an accuracy of 0.578.While there is still a drop in accuracy from MMLU to MMLU-SR's "Question and Answer" dataset, gpt-4o maintains relatively high performance, with accuracy generally staying above 0.7 for most subjects.Even in challenging areas like Virology and Global Facts, gpt-4o demonstrates resilience, maintaining accuracy levels significantly higher than other models.</p>
<p>CoT and System Instruction.We developed a simple baseline to test our MMLU-SR dataset on more recent and sophisticated models like GPT-4.This involves adding the instruction "Let's think step by step" at the end of answer choices to enable zero-shot CoT prompting.As shown in Table 7 from Appendix A, we also included a system instruction informing ChatGPT-4 that the following questions would involve symbol replacement with arbitrary definitions.However, the example demonstrates that despite applying (zero-shot) CoT, the model still incorrectly interprets the term 'Dummy' in choice D as 'neurodegenerative disorder,' leading to the wrong answer, choice A. We applied this system instruction across the entire MMLU-SR dataset as well, with results shown in Table 11 from Appendix C. The results indicate that while the system instruction slightly improves accuracy in the "Question Only" and "Answer Only" datasets, the model still struggles with the increased complexity in the "Question and Answer" dataset.</p>
<p>Conclusion</p>
<p>We introduced MMLU-SR, a novel benchmark that challenges LLMs by replacing key terms in questions with random words followed by their definitions, aiming to test the models' reasoning and comprehension abilities rather than their memorization skills.Our evaluation across multiple domains revealed that popular LLMs suffer from significant drops in performance with these modifications, highlighting their reliance on memorized terms.MMLU-SR's unique approach addresses concerns about overfitting to traditional benchmarks and provides a more rigorous measure of true language understanding.This dataset will enable researchers to better identify and address the reasoning limitations of current LLMs, fostering the development of more robust and genuinely intelligent models.</p>
<p>A CoT Examples</p>
<p>Table 7 shows an example of incorrect answer using zero-shot CoT with a system instruction produced by the ChatGPT-4 on MMLU-SR's "Question and Answer" dataset.The correct answer is choice D, but ChatGPT-4 responded with choice A.</p>
<p>B Examples of ChatGPT-3.5Response Table 8 shows an example of incorrect answer produced by the ChatGPT-3.5 on MMLU-SR's "Question and Answer" dataset.Table 9 shows an example of incorrect answer produced by the ChatGPT-3.5 on MMLU-SR's "Question and Answer" dataset.Table 10 shows an example of incorrect answer produced by the ChatGPT-3.5 on MMLU-SR's "Question and Answer" dataset.</p>
<p>C System Instruction Example</p>
<p>Table 11 shows the performance comparison of gemini-1.0-prowith and without using the system instruction "In each of the questions that I ask, I will replace some of the words that you might know with a word that is arbitrarily assigned a specific meaning just for this test.The meaning of these arbitrary definition may change with every question."</p>
<p>D Hard MMLU-SR Example</p>
<p>Table 12 illustrates a challenging example mentioned in the "Question and Answer" section in the main paper.In the question, we substituted "enzyme assay" with a synonym, "protein", and used the original terms from the MMLU dataset as placeholders in the answer choices, each paired with a definition.ChatGPT-4 correctly identified the term "Tay-Sachs Disease" but associated it with the incorrect answer choice and definition, which was meant for "Fragile X syndrome."This example demonstrates that ChatGPT-4 can be misled by the complexity of the dataset, tending to rely on pre-existing knowledge rather than engaging in substantive reasoning to align terms with their definitions.</p>
<p>E Complete Experiment Results</p>
<p>Table 13 shows our complete experiment results of different LLMs including gpt-3.5-turbo,gpt-4o-mini, gemini-1.0-pro,gemini-1.5-pro,llama3-8b and llama3-70b.The percentage drop of each model is shown in Table 14.</p>
<p>F Numbers of Human Modified Terms</p>
<p>Figure 5 shows the number of manually modified term definitions across 41 subject glossaries.These glossaries were created by consolidating related topics from the original 57 subjects in the MMLU dataset.Of the 28,676 terms initially generated by gpt-3.5-turbofor these 41 subjects, approximately 1,197 (4.2%) required human modification.</p>
<p>' means "the name given to any laboratory technique that measures enzyme activity within a sample."Hat can be used to identify carriers of: Answer: A. Cystic fibrosis.B. Fragile X syndrome.C. Oculocutaneous albinism.D. Tay-Sachs disease.</p>
<p>Figure 2 :
2
Figure 2: Example ChatGPT-4 output of MMLU-SR 'Question Only".</p>
<p>, Figure 3, and Figure 4 that feature responses from ChatGPT-4.For comparative insights, example responses of ChatGPT-3.5 are available from Table</p>
<p>Figure 3 :
3
Figure 3: Example ChatGPT-4 output of MMLU-SR "Answer Only".</p>
<p>Figure 4 :
4
Figure 4: Example ChatGPT-4 output of MMLU-SR "Question and Answer".</p>
<p>Figure 5 :
5
Figure 5: Comparison of total generated terms (red) and human-modified terms (blue) across 41 subject glossaries</p>
<p>Table 1 :
1</p>
<p>DatasetHumanities Social Sciences STEM Other AverageGPT-4o-miniMMLU (5-shot)0.7930.8580.6890.7820.771Question Only (5-shot)0.7440.7920.6210.7240.710Answer Only (5-shot)0.6590.7380.6020.6510.655Question and Answer (5-shot)0.5880.6660.5310.5850.585GPT-4oMMLU (5-shot)0.8800.9060.7710.8540.845Question Only (5-shot)0.8380.8560.7020.8110.792Answer Only (5-shot)0.7640.8240.7050.7600.757Question and Answer (5-shot)0.7080.7540.6350.7120.695Gemini-1.5-proMMLU (5-shot)0.8490.8810.8020.8150.832Question Only (5-shot)0.7950.8360.7000.7540.764Answer Only (5-shot)0.7410.8160.7470.7390.758Question and Answer (5-shot)0.6900.7520.6700.6810.694Llama3-70BMMLU (5-shot)0.6810.8680.6970.8140.765Question Only (5-shot)0.6350.8120.6310.7700.712Answer Only (5-shot)0.5390.6830.5650.6220.602Question and Answer (5-shot)0.5230.6530.5360.5910.576</p>
<p>Table 2 :
2
Relative percentage drop of accuracy in MMLU-SR compared to MMLU.
DatasetHumanities Social SciencesSTEMOtherAverageGPT-4o-miniQuestion Only (5-shot)6.18%7.69%9.87%7.42%7.91%Answer Only (5-shot)16.90%13.99%12.63% 16.75%15.05%Question and Answer (5-shot)25.85%22.38%22.93% 25.19%24.12%GPT-4oQuestion Only (5-shot)4.77%5.52%8.95%5.03%6.27%Answer Only (5-shot)13.18%9.05%8.56%11.01%10.41%Question and Answer (5-shot)19.55%16.78%17.64% 16.63%17.75%Gemini-1.5-proQuestion Only (5-shot)6.36%5.11%12.72%7.48%8.17%Answer Only (5-shot)12.72%7.38%6.86%9.33%8.89%Question and Answer (5-shot)18.73%14.64%16.46% 16.44%16.59%Llama3-70BQuestion Only (5-shot)6.75%6.45%9.47%5.41%6.93%Answer Only (5-shot)20.85%21.31%18.94% 23.59%21.31%Question and Answer (5-shot)23.20%24.77%23.10% 27.40%24.71%</p>
<p>Table 3 :
3
Detailed accuracy for different Humanities subjects across different models.
SubjectMMLU GPT Gemini Llama3 GPT Gemini Llama3 GPT Gemini Llama3 GPT Gemini Llama3 Question Only Answer Only Question and AnswerFormal Logic0.730 0.698 0.532 0.603 0.500 0.484 0.643 0.579 0.516 0.556 0.500 0.460Logical Fallacies0.902 0.902 0.853 0.883 0.834 0.810 0.853 0.847 0.663 0.834 0.841 0.564Moral Disputes0.882 0.832 0.847 0.832 0.806 0.769 0.777 0.830 0.630 0.711 0.749 0.653Moral Scenarios0.813 0.760 0.318 0.830 0.774 0.289 0.143 0.199 0.318 0.177 0.167 0.253Philosophy0.891 0.865 0.865 0.778 0.724 0.772 0.698 0.756 0.598 0.582 0.611 0.582World Religions0.901 0.895 0.906 0.895 0.836 0.895 0.842 0.813 0.696 0.825 0.772 0.684High School European History 0.903 0.885 0.848 0.885 0.855 0.830 0.897 0.849 0.721 0.861 0.818 0.739High School Us History0.946 0.922 0.946 0.917 0.902 0.887 0.897 0.863 0.799 0.863 0.819 0.799High School World History0.937 0.920 0.945 0.924 0.920 0.916 0.907 0.865 0.806 0.882 0.827 0.840Prehistory0.948 0.901 0.910 0.904 0.836 0.793 0.843 0.803 0.670 0.790 0.769 0.670International Law0.942 0.926 0.868 0.901 0.860 0.868 0.934 0.843 0.769 0.835 0.802 0.760Jurisprudence0.898 0.861 0.852 0.852 0.861 0.806 0.861 0.806 0.602 0.722 0.750 0.556Professional Law0.749 0.666 0.616 0.683 0.627 0.583 0.641 0.585 0.461 0.563 0.544 0.461</p>
<p>Table 4 :
4
Detailed accuracy for different Social Science subjects across different models.
SubjectMMLU GPT Gemini Llama3 GPT Gemini Llama3 GPT Gemini Llama3 GPT Gemini Llama3 Question Only Answer Only Question and AnswerEconometrics0.711 0.702 0.693 0.588 0.579 0.570 0.640 0.614 0.561 0.535 0.535 0.421High School Macroeconomics0.921 0.880 0.821 0.849 0.785 0.779 0.813 0.785 0.628 0.721 0.715 0.572High School Microeconomics0.971 0.929 0.870 0.903 0.870 0.773 0.857 0.815 0.664 0.769 0.744 0.571High School Government And Politics 0.984 0.974 0.969 0.979 0.943 0.938 0.943 0.922 0.798 0.922 0.845 0.782Public Relations0.836 0.746 0.755 0.755 0.755 0.736 0.664 0.682 0.600 0.627 0.646 0.555Security Studies0.824 0.841 0.824 0.788 0.792 0.767 0.731 0.796 0.673 0.633 0.714 0.624Us Foreign Policy0.930 0.940 0.930 0.920 0.930 0.890 0.870 0.880 0.740 0.810 0.810 0.780Human Sexuality0.931 0.893 0.855 0.924 0.855 0.840 0.863 0.847 0.710 0.802 0.756 0.756Sociology0.935 0.891 0.920 0.900 0.896 0.841 0.881 0.881 0.806 0.831 0.851 0.786High School Geography0.955 0.939 0.924 0.894 0.909 0.833 0.884 0.864 0.737 0.813 0.813 0.662High School Psychology0.965 0.938 0.921 0.923 0.917 0.884 0.927 0.912 0.719 0.872 0.859 0.739Professional Psychology0.908 0.895 0.845 0.845 0.801 0.788 0.817 0.791 0.627 0.719 0.737 0.601</p>
<p>Table 14 :
14
Complete relative percentage drop of accuracy in MMLU-SR compared to MMLU.
DatasetHumanities Social SciencesSTEMOtherAverageGPT-3.5-turboQuestion Only (5-shot)8.58%8.83%8.67%10.22%9.08%Answer Only (5-shot)25.31%22.73%20.40% 24.65%23.27%Question and Answer (5-shot)35.12%32.08%28.52% 33.30%32.26%GPT-4o-miniQuestion Only (5-shot)6.18%7.69%9.87%7.42%7.91%Answer Only (5-shot)16.90%13.99%12.63% 16.75%15.05%Question and Answer (5-shot)25.85%22.38%22.93% 25.19%24.12%GPT-4oQuestion Only (5-shot)4.77%5.52%8.95%5.03%6.27%Answer Only (5-shot)13.18%9.05%8.56%11.01%10.41%Question and Answer (5-shot)19.55%16.78%17.64% 16.63%17.75%Gemini-1.0-proQuestion Only (5-shot)5.63%1.85%9.56%6.40%5.86%Answer Only (5-shot)14.96%11.61%15.44% 15.91%14.48%Question and Answer (5-shot)20.05%17.94%20.81% 22.60%20.85%Gemini-1.5-proQuestion Only (5-shot)6.36%5.11%12.72%7.48%8.17%Answer Only (5-shot)12.72%7.38%6.86%9.33%8.89%Question and Answer (5-shot)18.73%14.64%16.46% 16.44%16.59%Llama3-8BQuestion Only (5-shot)7.92%9.51%8.98%8.36%8.69%Answer Only (5-shot)23.27%20.87%17.41% 23.56%21.28%Question and Answer (5-shot)28.16%28.93%23.88% 31.56%28.63%Llama3-70BQuestion Only (5-shot)6.75%6.45%9.47%5.41%6.93%Answer Only (5-shot)20.85%21.31%18.94% 23.59%21.31%Question and Answer (5-shot)23.20%24.77%23.10% 27.40%24.71%
AcknowledgementsWe thank the anonymous reviewers for their thoughtful feedback and for recognizing the significance and contributions of our research.This research is based on work partially supported by DARPA Grant HR00112420363, NSF Grant IIS-2127918, and NSF CAREER Award IIS-2340125.System PromptIn each of the questions that I ask, I will replace some of the words that you might know with a word that is arbitrarily assigned a specific meaning just for this test.The meaning of these arbitrary definition may change with every question.InputSuppose 'Hat' means 'the name given to any laboratory technique that measures enzyme activity within a sample.'Hat can be used to identify carriers of:A. Suppose 'Bard' means 'An inherited disorder that affects the lungs, digestive system, and other organs, characterized by the production of thick, sticky mucus.' Bard.B. Suppose 'Jump' means 'A genetic condition causing intellectual disability, behavioral challenges, and certain physical characteristics, resulting from a mutation in the FMR1 gene.' Jump.C. Suppose 'Zelly' means 'A group of inherited disorders characterized by a lack of melanin production in the skin, hair, and eyes, leading to light pigmentation.'Zelly.D. Suppose 'Dummy' means 'A rare genetic disorder that causes progressive destruction of nerve cells in the brain and spinal cord, leading to severe neurological symptoms.'Dummy.Let's think step by step.OutputIn this scenario, Hat is used to detect variations or abnormalities in enzyme levels that might be indicative of certain genetic disorders.A: Bard refers to cystic fibrosis.Areas of land (Jack) often have boundaries (Winky) that separate them from other areas.B: Jump refers to Fragile X syndrome, primarily involves genetic testing to detect the specific mutation rather than enzyme activity measurements.C: Zelly refers to conditions such as albinism, which are typically diagnosed based on pigment absence rather than enzyme activity, although some biochemical tests can be relevant in specific contexts.D: Dummy could be referring to several neurodegenerative disorders, which might be associated with enzyme activities related to nerve cell function or metabolic processes, but generally, such disorders are diagnosed through genetic markers and neuroimaging rather than enzyme activity alone.Given these explanations, the best match for 'Hat' in terms of being a technique to measure enzyme activity useful for diagnosis would be A, 'Bard' as cystic fibrosis, due to its direct relationship with enzyme dysfunction and measurement.✗InputEnzyme assay can be used to identify carriers of:A. Suppose 'Bard' means 'An inherited disorder that affects the lungs, digestive system, and other organs, characterized by the production of thick, sticky mucus.' Bard.B. Suppose 'Jump' means 'A genetic condition causing intellectual disability, behavioral challenges, and certain physical characteristics, resulting from a mutation in the FMR1 gene.' Jump.C. Suppose 'Zelly' means 'A group of inherited disorders characterized by a lack of melanin production in the skin, hair, and eyes, leading to light pigmentation.'Zelly.D. Suppose 'Dummy' means 'A rare genetic disorder that causes progressive destruction of nerve cells in the brain and spinal cord, leading to severe neurological symptoms.'Dummy.OutputAn enzyme assay could be used to identify carriers of Bard, Jump, Zelly, or Dummy if the specific enzyme associated with each disorder is known and can be tested for.Enzyme assays are commonly used in genetic testing to detect the presence or absence of specific enzymes or their activity levels, which can provide information about an individual's genetic makeup and potential carrier status for certain genetic disorders.✗OutputBased on the provided definitions:A. Hat (enzyme activity measurement) can be used to identify carriers of Zelly (lack of melanin production disorders).Explanation: Enzyme activity measurements are often used in medical diagnostics to identify various conditions, including genetic disorders.In this case, the lack of melanin production, as seen in Zelly disorders, can be identified using enzyme activity tests.✗InputSuppose 'Protein' means 'the name given to any laboratory technique that measures enzyme activity within a sample.'Protein can be used to identify carriers of:A. Suppose 'Oculocutaneous albini' means 'An inherited disorder that affects the lungs, digestive system, and other organs, characterized by the production of thick, sticky mucus.' Oculocutaneous albini.B. Suppose 'Tay-Sachs diseas' means 'A genetic condition causing intellectual disability, behavioral challenges, and certain physical characteristics, resulting from a mutation in the FMR1 gene.' Tay-Sachs diseas.C. Suppose 'Cystic fibrosi' means 'A group of inherited disorders characterized by a lack of melanin production in the skin, hair, and eyes, leading to light pigmentation.'Cystic fibrosi.D. Suppose 'Fragile X syndrom' means 'A rare genetic disorder that causes progressive destruction of nerve cells in the brain and spinal cord, leading to severe neurological symptoms.'Fragile X syndrom.OutputThe given problem seems to involve identifying which condition might be detected through a laboratory technique measuring enzyme activity.From the provided definitions, none of the conditions directly indicate an enzyme-related pathology except for a possible misalignment in the definitions and names:A: Oculocutaneous albini is defined incorrectly and does not relate to the production of enzymes.B: Tay-Sachs disease is a genetic disorder characterized by the destruction of nerve cells in the brain due to a deficiency of a specific enzyme (Hexosaminidase A).This is actually a plausible answer if we correct the name to Tay-Sachs disease, which fits the description of a disease involving enzyme activity.C: Cystic fibrosis and D: Fragile X syndrome are misdefined and do not relate to conditions involving measurable enzyme activity relevant to their actual conditions.Thus, the most likely answer, B. Tay-Sachs disease (with the corrected definition and name).✗Table13: Complete performance of gpt-3.5-turbo,gpt-4o-mini, gpt-4o, gemini-1.0-pro,gemini-1.5-pro,llama3-8b, and llama3-70b.
S N Akter, Z Yu, A Muhamed, T Ou, A Bäuerle, Á A Cabrera, K Dholakia, C Xiong, G Neubig, arXiv:2312.11444An in-depth look at gemini's language abilities. 2023arXiv preprint</p>
<p>Objectnet dataset: Reanalysis and correction. Ali Borji, arXiv:2004.020422020Preprint</p>
<p>Evaluating large language models trained on code. Mark Chen, arXiv:2107.033742021Preprint</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, arXiv:1905.100442019Preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, ArXiv, abs/1803.054572018</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.141682021Preprint</p>
<p>Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, arXiv:1903.001612019Preprint</p>
<p>Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Mcdonell, 10.5281/zenodo.10256836Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite. Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf; Ben Wang, Kevin Wangand Andy Zou. 2023. A framework for few-shot language model evaluation</p>
<p>Similarity and the development of rules. D Gentner, J Medina, Cognition. 652/31998</p>
<p>Correcting exposure bias for link recommendation. Shantanu Gupta, Hao Wang, Zachary Lipton, Yuyang Wang, ICML. 2021</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>H Li, Y Zhang, F Koto, Y Yang, H Zhao, Y Gong, N Duan, T Baldwin, arXiv:2306.09212Cmmlu: Measuring massive multitask language understanding in chinese. 2024arXiv preprint</p>
<p>Causal transportability for visual recognition. Chengzhi Mao, Kevin Xia, James Wang, Hao Wang, Junfeng Yang, Elias Bareinboim, Carl Vondrick, CVPR. 2022</p>
<p>Y Oren, N Meister, N Chatterji, F Ladhak, T B Hashimoto, arXiv:2310.17623Proving test set contamination in black box language models. 2023arXiv preprint</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. Shuai Lu Others, CoRR, abs/2102.046642021</p>
<p>Darwin's mistake: explaining the discontinuity between human and nonhuman minds. D C Penn, K J Holyoak, D J Povinelli, Behavioral and Brain Sciences. 3122008</p>
<p>An improved traditional chinese evaluation suite for foundation model. Z.-R Tam, Y.-T Pai, Y.-W Lee, S Cheng, H.-H Shuai, arXiv:2403.018582024arXiv preprint</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, arXiv:2212.105092023Preprint</p>
<p>Aniket Vashishtha, Gowtham Abbavaram, Abhinav Reddy, Kumar, arXiv:2310.15117Saketh Bachu, Vineeth N Balasubramanian, and Amit Sharma. 2023. Causal inference using llm-guided discovery. arXiv preprint</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.074612019Preprint</p>
<p>Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, Hao Wang, arXiv:2406.11230Multimodal needle in a haystack: Benchmarking long-context capability of multimodal large language models. 2024arXiv preprint</p>
<p>Causal discovery from incomplete data: A deep learning approach. Yuhao Wang, Vlado Menkovski, Hao Wang, AAAI StarAI Workshop. 2020Xin Du, and Mykola Pechenizkiy</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain-ofthought prompting elicits reasoning in large language models. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023Preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.078302019Preprint</p>
<p>Mario eval: Evaluate your math llm with your math llma mathematical dataset evaluation toolkit. Boning Zhang, Chengxi Li, Kai Fan, arXiv:2404.139252024arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations. 2023ICLR 2023</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8302 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8302</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8302</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-263608847</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.01714v3.pdf" target="_blank">Large Language Models as Analogical Reasoners</a></p>
                <p><strong>Paper Abstract:</strong> Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8302.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8302.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogical Prompting (Self-generated Exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analogical Prompting — Self-generated Exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that asks an LLM to self-generate a small set (K) of relevant problem-solution exemplars (question-rationale-answer) in-context, then use those exemplars to produce a chain-of-thought solution for the target problem in a single prompt pass.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4, PaLM 2, text-davinci-* (various sizes used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained autoregressive transformer LLMs (OpenAI GPT-3.5/GPT-4, PaLM 2, and OpenAI text-davinci variants) used as black-box generators for self-generating exemplars and solving problems. Experiments include both OpenAI and PaLM2 models of varying scale.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['analogical prompting (self-generated exemplars)', 'chain-of-thought (rationale generation / step-by-step reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Implementation: single-pass prompt that contains instructions to (1) recall/generate K distinct, relevant problems and their solutions as exemplars, and (2) 'Solve the initial problem' using those exemplars. The generated exemplars are question-rationale-answer sequences that act as in-context demonstrations (CoT-style) tailored to the target problem. Key prompt decisions: encourage diversity in exemplars, use K in {3..5} (empirically best), single-pass generation preferred for convenience.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablations and comparisons include: (1) varying number of self-generated exemplars K (Table 5: K=1..5); (2) encouraging diversity explicitly in prompts (Table 6 ablation on diversity encouragement); (3) single-pass generation vs. independently sampled exemplars (authors chose single-pass since performance comparable and simpler); (4) comparisons against 0-shot CoT, fixed few-shot CoT, and retrieval-based CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mathematical problem solving (GSM8K, MATH), Code generation (Codeforces competitive programming dataset), BIG-Bench reasoning tasks (word sorting, logical deduction, temporal sequences, reasoning about colored objects, formal fallacies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GSM8K (GPT-3.5-turbo): Ours (self-generated exemplars) = 77.8% vs 0-shot CoT = 75.8% and 5-shot CoT = 76.7% (accuracy). MATH (GPT-3.5-turbo): Ours = 37.3% vs 0-shot CoT = 33.9% and 5-shot CoT = 34.9%. BIG-Bench tasks: across several tasks Ours outperforms 0-shot CoT (example: word sorting: Ours 75.2% vs 0-shot 66.8%, 3-shot CoT 68.4%). Codeforces (code generation): Ours (self-generated exemplars) yields measurable gains (e.g., GPT3.5-turbo Acc@1 reported around 13%–14% and Acc@10 ~25%–27% depending on setting), and with knowledge+exemplars further improvement (see separate entry). The paper reports an average accuracy gain of about +4% over baselines across tasks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Manual analysis on 100 examples (50 correct, 50 incorrect) showed: among correctly solved problems, 35/50 had generated exemplars that were relevant and correct, 9/50 relevant but contained incorrect solutions, 6/50 exemplars irrelevant. Among incorrectly solved problems, 28/50 had relevant and correct exemplars but failed due to generalization issues; common failure modes: generalization gap (12/50), overreliance on specific exemplars (8/50), calculation errors (8/50). Authors also note generated exemplars can be correct/relevant but still insufficient for solving a harder target problem.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-generated exemplars (analogical prompting) outperform 0-shot CoT and fixed few-shot CoT across a range of reasoning tasks, particularly with larger-scale LLMs, because exemplars are tailored to the specific problem and exploit the model's internalized training knowledge; generating multiple distinct exemplars (K>=3) is important; self-generation is more advantageous with larger models where generation-quality exemplars are reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8302.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8302.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogical Prompting (Knowledge + Exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analogical Prompting — Self-generated High-level Knowledge plus Exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of analogical prompting that first asks the LLM to produce high-level takeaways (a tutorial or core concepts) for the target problem, then to generate exemplars aligned to those concepts, and finally to solve the target problem.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4 (evaluated mainly with GPT-3.5-turbo in code generation experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same family of large autoregressive LLMs; used to generate both conceptual tutorials and exemplars in-context before solving the target.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['analogical prompting (self-generated knowledge + exemplars)', 'chain-of-thought (rationale generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt includes an explicit '# Tutorial: Identify core concepts...' section to elicit high-level conceptual guidance (knowledge) followed by '# Relevant problems:' for exemplars, then '# Solve the initial problem:'; authors found generating knowledge before exemplars helps priority align exemplars to core concepts rather than superficial similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablation comparing knowledge-before-exemplars vs knowledge-after-exemplars (Table 7): generating knowledge before exemplars yields superior results; Codeforces ablation shows knowledge + exemplars improves performance over exemplars alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Codeforces code generation (primary), also applied to other reasoning benchmarks but knowledge addition most useful for code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Codeforces (GPT-3.5-turbo): Self-generated exemplars alone: Acc@1 ≈ 13% and Acc@10 ≈ 25%; Self-generated knowledge + exemplars: Acc@1 ≈ 15% and Acc@10 ≈ 29% (numbers reported in paper). Authors note GPT-3.5-turbo with knowledge+exemplars (15% Acc@1) is competitive with GPT-4 (≈16% Acc@1) on the Codeforces benchmark in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Knowledge helps by: (1) providing high-level takeaways so exemplars align by algorithmic approach rather than superficial lexical similarity; (2) preventing overreliance on low-level exemplar details and improving generalization for algorithmic tasks. Gains are largest on complex algorithmic/code generation tasks and smaller on simpler math/BIG-Bench tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-generating high-level knowledge before exemplars complements exemplars and improves performance for complex tasks (notably code generation); ordering (knowledge before exemplars) matters and provides a measurable improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8302.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8302.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>0-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting approach that instructs the LLM to 'think step by step' (or similar) to elicit chain-of-thought reasoning without providing labeled exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4, PaLM 2, text-davinci-* (used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LLMs prompted with a generic instruction to produce intermediate rationales (CoT) without in-context exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (generic instruction)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompts add a general instruction (e.g., 'think step by step') to the problem statement to induce intermediate reasoning steps. No problem-specific exemplars or generated knowledge are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as baseline against analogical prompting and few-shot CoT; directly compared in tables for GSM8K, MATH, Codeforces, and BIG-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, MATH, Codeforces, BIG-Bench (same tasks used in paper for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GSM8K (GPT-3.5-turbo): 0-shot CoT = 75.8% vs Ours = 77.8% (accuracy). MATH (GPT-3.5-turbo): 0-shot CoT = 33.9% vs Ours = 37.3%. On some CODE tasks 0-shot CoT performs worse than analogical prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>0-shot CoT provides generic guidance and can be insufficient for complex or highly-structured tasks (e.g., code generation) where tailored exemplars/knowledge are helpful; authors show concrete failure cases where 0-shot CoT does not recall relevant exemplars and selects incorrect approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>0-shot CoT is a weaker, more generic prompt; analogical prompting (self-generated exemplars/knowledge) provides more tailored guidance and outperforms 0-shot CoT on diverse reasoning tasks in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8302.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8302.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot CoT (fixed exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Chain-of-Thought with Fixed Labeled Exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard few-shot CoT: include a fixed set of manually labeled question-rationale-answer exemplars in the prompt to induce in-context chain-of-thought reasoning for new problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, PaLM 2, text-davinci-* (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs prompted with a small fixed set (e.g., K=3 or K=5) of labeled exemplars from a training set (when available), followed by the test problem.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (few-shot exemplars)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompts embed labeled exemplars (x_i, r_i, a_i) drawn from a dataset; exemplars are fixed across test problems (not tailored per test example).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as a primary baseline (e.g., 5-shot CoT for GSM8K/MATH). Also compared with retrieval-based CoT (retrieving exemplars per test problem) and with self-generation; scale experiments show relative strengths vary with model size.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, MATH, various BIG-Bench tasks, Codeforces (baseline where manual exemplars available).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GSM8K (GPT-3.5-turbo): 5-shot CoT = 76.7% vs Ours = 77.8%. MATH (GPT-3.5-turbo): 5-shot CoT = 34.9% vs Ours = 37.3%. On smaller LLMs, few-shot with retrieved/hand-labeled exemplars can beat self-generation.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Fixed exemplars provide reliable, labeled reasoning traces; however, they can be mismatched to diverse target problems (e.g., geometry vs algebra) and require manual labeling or retrieval infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Few-shot CoT is effective and reliable, especially with smaller LLMs or when labeled exemplars match the target distribution; analogical prompting can surpass few-shot CoT when LLMs are large enough to generate useful exemplars tailored to each problem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8302.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8302.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot Retrieved CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-based Chain-of-Thought (retrieved exemplars per problem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A version of few-shot CoT that retrieves the top-K most similar labeled exemplars from a training set for each test problem (e.g., using Sentence-BERT similarity) and uses them as in-context CoT demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied with various LLMs in comparison experiments (text-davinci-*, GPT-3.5 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs are provided with exemplars retrieved from a labeled train set based on semantic similarity to each test problem (Sentence-BERT used as retriever in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (retrieved labeled exemplars)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Pipeline: encode test problem and train-set problems, retrieve top-K similar labeled exemplars by cosine similarity, include these exemplars in prompt as few-shot CoT demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct empirical comparison against self-generated exemplars: Table 4 studies self-generation vs retrieved CoT across model scales (text-curie to text-davinci-003).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (primary comparison), other datasets where labeled exemplars exist.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Findings: retrieval-based CoT outperforms self-generation with smaller-scale LLMs (e.g., text-curie-001, text-davinci-001), whereas self-generated exemplars (analogical prompting) outperform retrieved CoT with larger-scale LLMs (e.g., text-davinci-002, text-davinci-003). Exact numbers by model scale are reported in Table 4 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Retrieved exemplars are inherently correct because they come from labeled data (reliable), but retrieval requires an external labeled dataset and retrieval system. Self-generation can leverage the LLM's broader pretraining knowledge and produce more tailored exemplars for larger LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Generation vs retrieval trade-off: retrieval is preferable for smaller LLMs (more reliable exemplars), while self-generation (analogical prompting) scales better and yields superior exemplars for larger LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8302.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8302.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency (augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (sampling multiple CoT outputs and aggregating)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that samples multiple chain-of-thought outputs from the LLM (via stochastic decoding) and aggregates final answers (e.g., majority vote) to improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied on top of analogical prompting with GPT-3.5-turbo (and compatible with other LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-consistency acts as a post-hoc aggregation strategy: sample multiple reasoning trajectories from the same prompt and combine results to reduce variance and increase answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['self-consistency (ensemble over multiple CoT samples)', 'chain-of-thought (sampled reasoning chains)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Authors combine analogical prompting with self-consistency: after generating exemplars (and optionally knowledge), sample many CoT solutions (with temperature) and aggregate final answers (self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (multiple sampled reasoning chains)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablation comparing analogical prompting alone vs analogical prompting + self-consistency (Table reported: Ours vs Ours + Self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, MATH (examples reported), other reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GSM8K (GPT-3.5-turbo): Ours = 77.8% vs Ours + Self-consistency = 85.3% (accuracy). MATH: Ours = 37.3% vs Ours + Self-consistency = 46.0%. (These gains are reported in the paper's ablation table.)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-consistency substantially boosts accuracy by aggregating across multiple sampled reasoning chains, mitigating errors from any single chain and improving robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency is an effective complementary technique to analogical prompting, producing large accuracy gains when applied on top of self-generated exemplars (and knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 1)</em></li>
                <li>Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8302",
    "paper_id": "paper-263608847",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Analogical Prompting (Self-generated Exemplars)",
            "name_full": "Analogical Prompting — Self-generated Exemplars",
            "brief_description": "A prompting method that asks an LLM to self-generate a small set (K) of relevant problem-solution exemplars (question-rationale-answer) in-context, then use those exemplars to produce a chain-of-thought solution for the target problem in a single prompt pass.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4, PaLM 2, text-davinci-* (various sizes used in experiments)",
            "model_description": "Large pre-trained autoregressive transformer LLMs (OpenAI GPT-3.5/GPT-4, PaLM 2, and OpenAI text-davinci variants) used as black-box generators for self-generating exemplars and solving problems. Experiments include both OpenAI and PaLM2 models of varying scale.",
            "reasoning_methods": [
                "analogical prompting (self-generated exemplars)",
                "chain-of-thought (rationale generation / step-by-step reasoning)"
            ],
            "reasoning_methods_description": "Implementation: single-pass prompt that contains instructions to (1) recall/generate K distinct, relevant problems and their solutions as exemplars, and (2) 'Solve the initial problem' using those exemplars. The generated exemplars are question-rationale-answer sequences that act as in-context demonstrations (CoT-style) tailored to the target problem. Key prompt decisions: encourage diversity in exemplars, use K in {3..5} (empirically best), single-pass generation preferred for convenience.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Ablations and comparisons include: (1) varying number of self-generated exemplars K (Table 5: K=1..5); (2) encouraging diversity explicitly in prompts (Table 6 ablation on diversity encouragement); (3) single-pass generation vs. independently sampled exemplars (authors chose single-pass since performance comparable and simpler); (4) comparisons against 0-shot CoT, fixed few-shot CoT, and retrieval-based CoT.",
            "task_or_benchmark": "Mathematical problem solving (GSM8K, MATH), Code generation (Codeforces competitive programming dataset), BIG-Bench reasoning tasks (word sorting, logical deduction, temporal sequences, reasoning about colored objects, formal fallacies).",
            "performance_results": "GSM8K (GPT-3.5-turbo): Ours (self-generated exemplars) = 77.8% vs 0-shot CoT = 75.8% and 5-shot CoT = 76.7% (accuracy). MATH (GPT-3.5-turbo): Ours = 37.3% vs 0-shot CoT = 33.9% and 5-shot CoT = 34.9%. BIG-Bench tasks: across several tasks Ours outperforms 0-shot CoT (example: word sorting: Ours 75.2% vs 0-shot 66.8%, 3-shot CoT 68.4%). Codeforces (code generation): Ours (self-generated exemplars) yields measurable gains (e.g., GPT3.5-turbo Acc@1 reported around 13%–14% and Acc@10 ~25%–27% depending on setting), and with knowledge+exemplars further improvement (see separate entry). The paper reports an average accuracy gain of about +4% over baselines across tasks and models.",
            "qualitative_findings": "Manual analysis on 100 examples (50 correct, 50 incorrect) showed: among correctly solved problems, 35/50 had generated exemplars that were relevant and correct, 9/50 relevant but contained incorrect solutions, 6/50 exemplars irrelevant. Among incorrectly solved problems, 28/50 had relevant and correct exemplars but failed due to generalization issues; common failure modes: generalization gap (12/50), overreliance on specific exemplars (8/50), calculation errors (8/50). Authors also note generated exemplars can be correct/relevant but still insufficient for solving a harder target problem.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-generated exemplars (analogical prompting) outperform 0-shot CoT and fixed few-shot CoT across a range of reasoning tasks, particularly with larger-scale LLMs, because exemplars are tailored to the specific problem and exploit the model's internalized training knowledge; generating multiple distinct exemplars (K&gt;=3) is important; self-generation is more advantageous with larger models where generation-quality exemplars are reliable.",
            "uuid": "e8302.0",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Analogical Prompting (Knowledge + Exemplars)",
            "name_full": "Analogical Prompting — Self-generated High-level Knowledge plus Exemplars",
            "brief_description": "An extension of analogical prompting that first asks the LLM to produce high-level takeaways (a tutorial or core concepts) for the target problem, then to generate exemplars aligned to those concepts, and finally to solve the target problem.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4 (evaluated mainly with GPT-3.5-turbo in code generation experiments)",
            "model_description": "Same family of large autoregressive LLMs; used to generate both conceptual tutorials and exemplars in-context before solving the target.",
            "reasoning_methods": [
                "analogical prompting (self-generated knowledge + exemplars)",
                "chain-of-thought (rationale generation)"
            ],
            "reasoning_methods_description": "Prompt includes an explicit '# Tutorial: Identify core concepts...' section to elicit high-level conceptual guidance (knowledge) followed by '# Relevant problems:' for exemplars, then '# Solve the initial problem:'; authors found generating knowledge before exemplars helps priority align exemplars to core concepts rather than superficial similarity.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Ablation comparing knowledge-before-exemplars vs knowledge-after-exemplars (Table 7): generating knowledge before exemplars yields superior results; Codeforces ablation shows knowledge + exemplars improves performance over exemplars alone.",
            "task_or_benchmark": "Codeforces code generation (primary), also applied to other reasoning benchmarks but knowledge addition most useful for code generation.",
            "performance_results": "Codeforces (GPT-3.5-turbo): Self-generated exemplars alone: Acc@1 ≈ 13% and Acc@10 ≈ 25%; Self-generated knowledge + exemplars: Acc@1 ≈ 15% and Acc@10 ≈ 29% (numbers reported in paper). Authors note GPT-3.5-turbo with knowledge+exemplars (15% Acc@1) is competitive with GPT-4 (≈16% Acc@1) on the Codeforces benchmark in their experiments.",
            "qualitative_findings": "Knowledge helps by: (1) providing high-level takeaways so exemplars align by algorithmic approach rather than superficial lexical similarity; (2) preventing overreliance on low-level exemplar details and improving generalization for algorithmic tasks. Gains are largest on complex algorithmic/code generation tasks and smaller on simpler math/BIG-Bench tasks.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-generating high-level knowledge before exemplars complements exemplars and improves performance for complex tasks (notably code generation); ordering (knowledge before exemplars) matters and provides a measurable improvement.",
            "uuid": "e8302.1",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "0-shot CoT",
            "name_full": "Zero-shot Chain-of-Thought Prompting",
            "brief_description": "A prompting approach that instructs the LLM to 'think step by step' (or similar) to elicit chain-of-thought reasoning without providing labeled exemplars.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4, PaLM 2, text-davinci-* (used as baselines)",
            "model_description": "Large LLMs prompted with a generic instruction to produce intermediate rationales (CoT) without in-context exemplars.",
            "reasoning_methods": [
                "chain-of-thought (generic instruction)"
            ],
            "reasoning_methods_description": "Prompts add a general instruction (e.g., 'think step by step') to the problem statement to induce intermediate reasoning steps. No problem-specific exemplars or generated knowledge are provided.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as baseline against analogical prompting and few-shot CoT; directly compared in tables for GSM8K, MATH, Codeforces, and BIG-Bench.",
            "task_or_benchmark": "GSM8K, MATH, Codeforces, BIG-Bench (same tasks used in paper for comparisons).",
            "performance_results": "GSM8K (GPT-3.5-turbo): 0-shot CoT = 75.8% vs Ours = 77.8% (accuracy). MATH (GPT-3.5-turbo): 0-shot CoT = 33.9% vs Ours = 37.3%. On some CODE tasks 0-shot CoT performs worse than analogical prompting.",
            "qualitative_findings": "0-shot CoT provides generic guidance and can be insufficient for complex or highly-structured tasks (e.g., code generation) where tailored exemplars/knowledge are helpful; authors show concrete failure cases where 0-shot CoT does not recall relevant exemplars and selects incorrect approaches.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "0-shot CoT is a weaker, more generic prompt; analogical prompting (self-generated exemplars/knowledge) provides more tailored guidance and outperforms 0-shot CoT on diverse reasoning tasks in the experiments.",
            "uuid": "e8302.2",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Few-shot CoT (fixed exemplars)",
            "name_full": "Few-shot Chain-of-Thought with Fixed Labeled Exemplars",
            "brief_description": "Standard few-shot CoT: include a fixed set of manually labeled question-rationale-answer exemplars in the prompt to induce in-context chain-of-thought reasoning for new problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, PaLM 2, text-davinci-* (used as baseline)",
            "model_description": "LLMs prompted with a small fixed set (e.g., K=3 or K=5) of labeled exemplars from a training set (when available), followed by the test problem.",
            "reasoning_methods": [
                "chain-of-thought (few-shot exemplars)"
            ],
            "reasoning_methods_description": "Prompts embed labeled exemplars (x_i, r_i, a_i) drawn from a dataset; exemplars are fixed across test problems (not tailored per test example).",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as a primary baseline (e.g., 5-shot CoT for GSM8K/MATH). Also compared with retrieval-based CoT (retrieving exemplars per test problem) and with self-generation; scale experiments show relative strengths vary with model size.",
            "task_or_benchmark": "GSM8K, MATH, various BIG-Bench tasks, Codeforces (baseline where manual exemplars available).",
            "performance_results": "GSM8K (GPT-3.5-turbo): 5-shot CoT = 76.7% vs Ours = 77.8%. MATH (GPT-3.5-turbo): 5-shot CoT = 34.9% vs Ours = 37.3%. On smaller LLMs, few-shot with retrieved/hand-labeled exemplars can beat self-generation.",
            "qualitative_findings": "Fixed exemplars provide reliable, labeled reasoning traces; however, they can be mismatched to diverse target problems (e.g., geometry vs algebra) and require manual labeling or retrieval infrastructure.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Few-shot CoT is effective and reliable, especially with smaller LLMs or when labeled exemplars match the target distribution; analogical prompting can surpass few-shot CoT when LLMs are large enough to generate useful exemplars tailored to each problem.",
            "uuid": "e8302.3",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Few-shot Retrieved CoT",
            "name_full": "Retrieval-based Chain-of-Thought (retrieved exemplars per problem)",
            "brief_description": "A version of few-shot CoT that retrieves the top-K most similar labeled exemplars from a training set for each test problem (e.g., using Sentence-BERT similarity) and uses them as in-context CoT demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied with various LLMs in comparison experiments (text-davinci-*, GPT-3.5 variants)",
            "model_description": "LLMs are provided with exemplars retrieved from a labeled train set based on semantic similarity to each test problem (Sentence-BERT used as retriever in experiments).",
            "reasoning_methods": [
                "chain-of-thought (retrieved labeled exemplars)"
            ],
            "reasoning_methods_description": "Pipeline: encode test problem and train-set problems, retrieve top-K similar labeled exemplars by cosine similarity, include these exemplars in prompt as few-shot CoT demonstrations.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Direct empirical comparison against self-generated exemplars: Table 4 studies self-generation vs retrieved CoT across model scales (text-curie to text-davinci-003).",
            "task_or_benchmark": "GSM8K (primary comparison), other datasets where labeled exemplars exist.",
            "performance_results": "Findings: retrieval-based CoT outperforms self-generation with smaller-scale LLMs (e.g., text-curie-001, text-davinci-001), whereas self-generated exemplars (analogical prompting) outperform retrieved CoT with larger-scale LLMs (e.g., text-davinci-002, text-davinci-003). Exact numbers by model scale are reported in Table 4 of the paper.",
            "qualitative_findings": "Retrieved exemplars are inherently correct because they come from labeled data (reliable), but retrieval requires an external labeled dataset and retrieval system. Self-generation can leverage the LLM's broader pretraining knowledge and produce more tailored exemplars for larger LLMs.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Generation vs retrieval trade-off: retrieval is preferable for smaller LLMs (more reliable exemplars), while self-generation (analogical prompting) scales better and yields superior exemplars for larger LLMs.",
            "uuid": "e8302.4",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-consistency (augmentation)",
            "name_full": "Self-consistency (sampling multiple CoT outputs and aggregating)",
            "brief_description": "A technique that samples multiple chain-of-thought outputs from the LLM (via stochastic decoding) and aggregates final answers (e.g., majority vote) to improve robustness and accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied on top of analogical prompting with GPT-3.5-turbo (and compatible with other LLMs)",
            "model_description": "Self-consistency acts as a post-hoc aggregation strategy: sample multiple reasoning trajectories from the same prompt and combine results to reduce variance and increase answer accuracy.",
            "reasoning_methods": [
                "self-consistency (ensemble over multiple CoT samples)",
                "chain-of-thought (sampled reasoning chains)"
            ],
            "reasoning_methods_description": "Authors combine analogical prompting with self-consistency: after generating exemplars (and optionally knowledge), sample many CoT solutions (with temperature) and aggregate final answers (self-consistency).",
            "reasoning_diversity": "diverse (multiple sampled reasoning chains)",
            "reasoning_diversity_experimental_setup": "Ablation comparing analogical prompting alone vs analogical prompting + self-consistency (Table reported: Ours vs Ours + Self-consistency).",
            "task_or_benchmark": "GSM8K, MATH (examples reported), other reasoning tasks.",
            "performance_results": "GSM8K (GPT-3.5-turbo): Ours = 77.8% vs Ours + Self-consistency = 85.3% (accuracy). MATH: Ours = 37.3% vs Ours + Self-consistency = 46.0%. (These gains are reported in the paper's ablation table.)",
            "qualitative_findings": "Self-consistency substantially boosts accuracy by aggregating across multiple sampled reasoning chains, mitigating errors from any single chain and improving robustness.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-consistency is an effective complementary technique to analogical prompting, producing large accuracy gains when applied on top of self-generated exemplars (and knowledge).",
            "uuid": "e8302.5",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 1,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
            "rating": 1,
            "sanitized_title": "selfgenerated_incontext_learning_leveraging_autoregressive_language_models_as_a_demonstration_generator"
        }
    ],
    "cost": 0.020749499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Analogical Prompting (Ours)
9 Mar 2024</p>
<p>Michihiro Yasunaga 
Google DeepMind</p>
<p>Stanford University</p>
<p>Xinyun Chen xinyunchen@google.com 
Google DeepMind</p>
<p>Yujia Li 
Google DeepMind</p>
<p>Panupong Pasupat 
Google DeepMind</p>
<p>Jure Leskovec 
Stanford University</p>
<p>Percy Liang 
Stanford University</p>
<p>Ed H Chi 
Google DeepMind</p>
<p>Denny Zhou dennyzhou@google.com 
Google DeepMind</p>
<p>Analogical Prompting (Ours)
9 Mar 2024CB7B06EDC314A070E367175161A0C894arXiv:2310.01714v3[cs.LG]
Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process.In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models.Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem.This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability.Experimental results show that our approach outperforms 0-shot CoT and manual fewshot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) demonstrate strong performance across various tasks (Brown et al., 2020;Chowdhery et al., 2022;Liang et al., 2022;Qin et al., 2023).Recently, chain-of-thought (CoT) prompting has demonstrated LLMs' abilities to tackle complex tasks, such as solving math problems, by prompting them to generate intermediate reasoning steps (Wei et al., 2022b;Kojima et al., 2022).For instance, common methods like few-shot CoT (Wei et al. 2022b; Figure 1, middle) make LLMs generate reasoning steps by offering a few exemplars of question-rationale-answer triplets; 0-shot CoT (Kojima et al. 2022; Figure 1, left) aims for the same objective by offering instructions like "think step by step."These studies highlight the importance of devising effective methods to guide LLMs to reason.However, the existing CoT paradigm faces two key challenges: providing relevant guidance or exemplars of reasoning, and minimizing the need for manual labeling.Specifically, 0-shot CoT offers generic reasoning guidance, which may not suffice for complex tasks like code generation ( §6).Few-shot CoT provides more detailed guidance but demands labeled exemplars of the reasoning process, which can be costly to obtain for every task.This raises a research question: can we achieve the best of both worlds and automate the generation of relevant exemplars to guide LLMs' reasoning process?</p>
<p>In this work, we propose analogical prompting, a new prompting approach that automatically guides the reasoning process of LLMs.Our inspiration comes from analogical reasoning in psychology, a concept where humans draw from relevant past experiences to tackle new problems (Vosniadou &amp; Ortony, 1989).For instance, when faced with a new math problem (e.g., finding the area of a square given four points in a coordinate system; Figure 1), humans often think about "do I know a related problem?" (Polya, 2004) and recall how they solved related problems in the past (e.g., finding the area of a square with a known side length) to derive insights for solving the new problem.They also recall high-level knowledge, such as the need to find the side length to calculate a square's area.Our idea is to prompt LLMs to mimic this reasoning process to effectively solve new problems.</p>
<p>Concretely, given a problem to solve, we prompt LLMs to self-generate relevant exemplars in the context, using instructions like "# Recall relevant problems and solutions:...", and then proceed to solve the original problem (Figure 1, 2).Simultaneously, we can also prompt LLMs to generate high-level knowledge that complements specific exemplars, using instructions like "# Provide a tutorial:..." (Figure 3).This proves particularly useful for complex tasks like code generation (see §6).Notably, our method can operate in a single prompt, generating knowledge, exemplars, and a solution to the initial # Solve the initial problem: To find the area of the square, we need to find the side length.The length is … \sqrt{(2 -(-2))^2 + (-2 -2)^2} = \sqrt{32}.So, the area of the square is (\sqrt{32})^2 = 32.</p>
<p>Model Output</p>
<p>• Need labeled exemplars of reasoning • Generic guidance of reasoning</p>
<p>• Exemplars are automatically generated and tailored to each problem</p>
<p>✓</p>
<p>Figure 1: Overview of our approach, analogical prompting.Left: Existing methods for prompting LLM to reason are either generic (0-shot CoT) or demand labeled exemplars (few-shot CoT).Right: Given a problem, our method prompts LLMs to self-generate relevant exemplars before solving the problem.This eliminates the need for labeling and also tailors the exemplars to each individual problem.See Figure 3 for a sample prompt where the LLM self-generates both knowledge and exemplars.</p>
<p>problem end-to-end in one pass.The underlying idea here is that modern LLMs have already acquired knowledge of various problems during training.Explicitly prompting them to recall relevant problems and solutions in the context guides LLMs to perform in-context learning to solve new problems.</p>
<p>Our proposed approach offers several advantages.It self-generates exemplars and obviates the need for manually labeling reasoning exemplars for each task, addressing the challenges faced by 0-shot and few-shot CoT.Furthermore, the self-generated exemplars are tailored to individual problems, such as 'geometry' or 'probability', rather than generic 'math problems'.This can simplify the complexity associated with recent CoT techniques that retrieve relevant exemplars from external data (Zhang et al., 2022b;Shum et al., 2023).</p>
<p>We evaluate the proposed approach in various reasoning-intensive tasks, including mathematical problem solving in GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), code generation in Codeforces, and other reasoning tasks in BIG-Bench (Srivastava et al., 2022).We use several base LLMs: GPT-3.5, GPT-4 (OpenAI, 2023;Ouyang et al., 2022), and PaLM2 (Anil et al., 2023).Experimental results show that our approach outperforms 0-shot CoT and few-shot CoT across a range of tasks and base LLMs, achieving an average accuracy gain of +4%.Notably, our approach improves performance on tasks involving diverse types of reasoning, such as MATH (including algebra, probability, geometry, etc.) and Codeforces (involving dynamic programming, graph algorithms, etc.).This result suggests the effectiveness of generating tailored exemplars for individual problems to guide the reasoning process of LLMs.</p>
<p>RELATED WORKS</p>
<p>LARGE LANGUAGE MODELS AND PROMPTING</p>
<p>A language model estimates probabilities over text.Recent research has scaled up these models from millions (Devlin et al., 2019) to billions of parameters (Brown et al., 2020) and expanded training data to include web texts and instruction data (Gao et al., 2020;Ouyang et al., 2022;Chung et al., 2022).These advances have made large language models proficient in various NLP tasks.</p>
<p>LLMs with billions of parameters demonstrate in-context learning and few-shot learning abilities (Brown et al., 2020;Liu et al., 2022;Su et al., 2022;Mishra et al., 2022;Wei et al., 2022a;Yasunaga et al., 2023;Shi et al., 2023).They use input prompts (instructions or a few exemplars) to guide LLMs to generate desired responses for tasks, marking the advent of the prompting era.Our approach harnesses the in-context learning abilities of LLMs to guide their reasoning process using self-generated exemplars.</p>
<p>Closely related to ours are works that perform self-generation in LLM prompting (Sun et al., 2022;He et al., 2023;Kim et al., 2022;Li et al., 2022a).For instance, Sun et al. (2022) prompts LLMs to recite relevant facts in context for open-domain question answering.Our idea of self-generating exemplars is related to recitation, but focuses on recalling problem-solving and reasoning processes rather than factual knowledge.</p>
<p>CHAIN-OF-THOUGHT PROMPTING</p>
<p>Chain-of-thought (CoT; Wei et al. 2022b) is a prompting strategy that guides LLMs to produce intermediate reasoning steps towards a final answer, enhancing problem-solving performance.Common instances of CoT include 0-shot CoT (Kojima et al., 2022) and few-shot CoT Wei et al. (2022b).</p>
<p>0-shot CoT prompts LLMs with a general instruction like "think step by step" to produce intermediate reasoning steps.Few-shot CoT achieves stronger performance by providing multiple exemplars of reasoning process (question-rationale-answer), leveraging LLMs' in-context learning abilities.However, it requires labeled exemplars.Our approach tackles this challenge by prompting LLMs to self-generate exemplars.</p>
<p>Within few-shot CoT, the original approach employs a fixed set of labeled exemplars for all test problems.Recent work explores retrieval-based CoT, which aims to obtain more relevant exemplars from external data for each problem (Zhang et al., 2022b;Shum et al., 2023).While our work shares the goal of providing relevant exemplars, instead of retrieval, we make LLMs self-generate exemplars.Self-generation offers several advantages: it is simpler, as it does not require external data retrieval, and it is more versatile, as it can produce not only specific exemplars but also broader insights or knowledge that complement them.Empirically, our generation-based CoT outperforms retrieval-based CoT, especially with larger base LLMs, while retrieval-based CoT excels with smaller base LLMs ( §6).</p>
<p>Finally, there are other techniques for enhancing CoT, such as self-consistency (Wang et al., 2022) and least-to-most (Zhou et al., 2022).Our work can complement and integrate with these efforts.</p>
<p>Please see §A for additional related works.</p>
<p>PRELIMINARIES</p>
<p>We focus on problem-solving tasks, where the objective is to produce a solution y for a given problem statement x, such as mathematical questions or code generation specifications.The solution may include both the intermediate reasoning steps or rationale r and the final answer a.</p>
<p>A prompting method ϕ is a function that maps a problem statement x into a specific textual input ϕ(x) for an LLM, which then generates a solution ŷ = LLM(ϕ(x)).For instance,</p>
<p>• In 0-shot prompting, ϕ directly yields x.</p>
<p>• In 0-shot CoT, ϕ supplements x with a general instruction, such as "[x] think step by step".</p>
<p>• In few-shot CoT, ϕ supplements x with several labeled exemplars, {(x i ,r i ,a
i )} K i=1 , such as "[x 1 ] [r 1 ] [a 1 ]...[x K ] [r K ] [a K ] [x]".
Our aim is to design a prompting method ϕ that enhances the accuracy of solutions LLMs generate.</p>
<p>APPROACH</p>
<p>We introduce analogical prompting, a new prompting approach that automatically provides exemplars to guide LLMs' reasoning process.Inspired by how humans recall relevant past experiences when tackling new problems, our approach makes LLMs self-generate relevant exemplars or knowledge in context, before proceeding to solve the problem (Figure 1, right).We present two techniques to achieve this: self-generated exemplars ( §4.1) and self-generated knowledge + exemplars ( §4.2).</p>
<p>SELF-GENERATED EXEMPLARS</p>
<p>Our approach is based on the idea that modern LLMs possess a broad range of problem-solving knowledge acquired during training.Explicitly prompting them to recall or generate relevant problems and solutions in context aids LLMs to perform in-context learning to solve new problems.Specifically, given a target problem to solve x, our prompt augments it with instructions like:</p>
<h1>Problem: [x]</h1>
<h1>Relevant problems: Recall three relevant and distinct problems.For each problem, describe it and explain the solution.# Solve the initial problem:</h1>
<p>For a concrete example, see Figure 2. The LLM first generates several (K) relevant exemplars in the form of question-rationale-answer sequences ("# Relevant problems:" part of the instruction).Then the model proceeds to solve the initial problem, leveraging the recalled exemplars in the context ("# Solve the initial problem:" part of the instruction).Note that all these instructions are provided within a single prompt, allowing the LLM to generate relevant problems and solution to the initial problem in one continuous pass.Using '#' symbols in the prompt (e.g., '# Relevant Problems') helps LLMs structure the response better.</p>
<p>Below are key technical decisions we made:</p>
<p>• Generating relevant and diverse exemplars is important: To achieve this, we explicitly include an instruction in the prompt, such as "generate problems that are distinct from each other" (e.g., Figure 2).This step is crucial as some LLMs have a tendency to repetitively generate identical problems, which can be misleading when solving the target problem.• Single-pass vs. independent exemplar generation: An alternative approach is to independently generate exemplars by separately sampling them from the LLM and then re-prompt the LLM with all the exemplars.While this method does work, our current single-pass prompt approach achieves comparable performance and offers greater convenience, eliminating the need for multiple prompts.</p>
<p>Consequently, we have chosen to adopt the single-pass method.• The number of exemplars to generate (K): Through experimentation, we have found that generating K = 3 to 5 exemplars works the best (more details in §6.5).</p>
<p>Our approach offers two advantages.It offers detailed exemplars of reasoning without manual labeling, addressing the challenges in 0-shot and few-shot CoT.The generated exemplars are tailored to individual problems (e.g., 'geometry' or 'probability'), offering more relevant guidance than traditional few-shot CoT, which uses fixed exemplars (e.g., general math problems; Figure 1, middle).</p>
<p>SELF-GENERATED KNOWLEDGE + EXEMPLARS</p>
<p>While generating exemplars is useful, in complex tasks like code generation, LLMs may overly rely on the low-level exemplars and fail to generalize when solving the target problems.To address this challenge, we also allow LLMs to self-generate high-level takeaways that complements the exemplars, which we refer to as "knowledge."Specifically, we enhance the prompt with an additional instruction like the following.For a concrete example, see Figure 3.</p>
<h1>Tutorial: Identify core concepts in the problem and provide a tutorial.</h1>
<p>One technical consideration is whether to generate knowledge before or after exemplars.We found that generating knowledge before exemplars yields superior results (Table 7).By generating knowledge first, LLMs identify the core concepts of the problem.This, in turn, helps LLMs generate exemplars that align more closely in terms of the fundamental problem-solving approaches rather than surface-level lexical similarities.For further discussion, please refer to §6.2.</p>
<p>EXPERIMENTAL SETUP</p>
<p>TASKS</p>
<p>We evaluate the proposed approach in diverse reasoning-intensive tasks, including mathematical problem solving, code generation, and other reasoning tasks like logical and temporal reasoning.</p>
<p>Mathematical problem solving.We use popular benchmarks, GSM8K (Cobbe et al., 2021), comprising elementary math word problems, and MATH (Hendrycks et al., 2021b), consisting of advanced math problems from high school math competitions.For each problem, we obtain an output from LLMs using a temperature of 0, and report the accuracy.</p>
<p>Code generation.Code generation involves synthesizing programs to solve algorithmic problems.Competitive programming is especially challenging, requiring reasoning about various algorithms like dynamic programming and graphs (Li et al., 2022b;Kulal et al., 2019;Yasunaga &amp; Liang, 2020).</p>
<p>As a benchmark, we collected competitive programming problems from codeforces.com (details in §B).We focus on level-A problems published in 2023 to prevent test set contamination (Magar &amp; Schwartz, 2022).Each problem comprises a problem statement, which serves as input to LLMs, and a set of test cases to assess generated code.The correctness of code is determined by whether it passes all test cases.</p>
<p>lf-generated Lessons -generated Exemplars</p>
<p>Your task is to tackle mathematical problems.When presented with a math problem, recall relevant problems as examples.Afterward, proceed to solve the initial problem.</p>
<h1>Problem:</h1>
<p>An airline serves a dinner to all the passengers on an airplane.They get their choice of steak or fish.Three steak meals and three fish meals are set aside for the six-member crew.If the meals are distributed to the crew members randomly, what is the probability that both pilots get the fish?# Instructions: ## Relevant Problems:</p>
<p>Recall three examples of math problems that are relevant to the initial problem.Your problems should be distinct from each other and from the initial problem (e.g., involving different numbers and names).For each problem: -After "Q: ", describe the problem -After "A: ", explain the solution and enclose the ultimate answer in \boxed{}.</p>
<h2>Solve the Initial Problem: Q: Copy and paste the initial problem here.A: Explain the solution and enclose the ultimate answer in \boxed{} here.</h2>
<p>Relevant Problems: Q: In a bag, there are 5 red balls and 3 blue balls.If two balls are drawn at random without replacement, what is the probability that both balls are red?A: The total number of balls is 8.The probability of drawing a red ball on the first draw is 5/8.After one red ball is drawn, there are 4 red balls left out of 7 total balls.Therefore, the probability of drawing a second red ball is 4/7.The probability of both events happening is the product of the individual probabilities, so the probability of drawing two red balls is (5/8) * (4/7) = \boxed{\frac{5}{14}}.</p>
<p>Q: A box contains 10 red marbles and 5 blue marbles.If three marbles are drawn at random without replacement, what is the probability that all three marbles are red?A: The total number of marbles is 15.The probability of drawing a red marble on the first draw is 10/15.After one red marble is drawn, there are 9 red marbles left out of 14 total marbles.Therefore, the probability of drawing a second red marble is 9/14.After two red marbles are drawn, there are 8 red marbles left out of 13 total marbles.Therefore, the probability of drawing a third red marble is 8/13.The probability of all three events happening is the product of the individual probabilities, so the probability of drawing three red marbles is (10/15) * (9/14) * (8/13) = \boxed{\frac{24}{91}}.Q: A bag contains 4 green marbles, 3 red marbles, and 2 blue marbles.If two marbles are drawn at random without replacement, what is the probability that both marbles are green?A: The total number of marbles is 9.The probability of drawing a green marble on the first draw is 4/9.After one green marble is drawn, there are 3 green marbles left out of 8 total marbles.Therefore, the probability of drawing a second green marble is 3/8.The probability of both events happening is the product of the individual probabilities, so the probability of drawing two green marbles is (4/9) * (3/8) = \boxed{\frac{1}{6}}.</p>
<p>Let's solve the following math problem: An airline serves a dinner to all the passengers on an airplane.They get their choice of steak or fish.Three steak meals and three fish meals are set aside for the six-member crew.If the meals are distributed to the crew members randomly, what is the probability that both pilots get the fish?A: There are a total of 6 crew members, so the probability of the first pilot getting the fish is 3/6.After one fish meal is given to the first pilot, there are 2 fish meals left out of 5 total meals.Therefore, the probability of the second pilot getting the fish is 2/5.The probability of both events happening is the product of the individual probabilities, so the probability that both pilots get the fish is (3/6) * (2/5) = \boxed{\frac{1}{5}}.</p>
<p>Our Prompt: Self-generate Exemplars Output by GPT3.5-turboIn line with existing work on code generation (Li et al., 2022b;Chen et al., 2023), we report the Acc@1 and Acc@10 metrics.Acc@k measures whether at least one of the k sampled model outputs is correct.</p>
<p>Self-generated Exemplars</p>
<p>For each problem, we sample 10 outputs from LLMs, using a temperature of 0.7.</p>
<p>Other reasoning tasks.We further evaluate on various reasoning tasks in BIG-Bench (Srivastava et al., 2022;Suzgun et al., 2022): word sorting, logical deduction five objects, temporal sequences, reasoning about colored objects, and formal fallacies.These tasks are diverse and may not have dedicated training data, so they align well with our approach of self-generating custom exemplars.For each problem, we obtain an output from LLMs using a temperature of 0, and report the accuracy.</p>
<p>MODELS</p>
<p>We experiment with several base LLMs: GPT-3.5-turbo,GPT-4 (OpenAI, 2023;Ouyang et al., 2022) (accessed in June-September 2023), and PaLM 2-L (Anil et al., 2023).</p>
<p>METHODS TO COMPARE</p>
<p>We compare the following prompting methods, including ours.0-shot and 0-shot CoT.These methods, like ours, do not use labeled exemplars.We aim to show that our method offers more tailored guidance for LLM reasoning and yields superior task performance.</p>
<h2>Algorithms: The core algorithm for solving this problem is the prefix product algorithm.</h2>
<h2>Tutorial: The prefix product algorithm is used to calculate the product of all the elements in a prefix of an array.The algorithm works by initializing a variable "prefix" to 1, and then iterating through the array from left to right.At each iteration, the current element is multiplied with the prefix, and the result is stored in the prefix variable.This way, the prefix variable keeps track of the product of all the elements in the prefix of the array.The time complexity of this algorithm is O(n), where n is the size of the input array.......</h2>
<h2>Example Problems:</h2>
<ol>
<li>Problem: Given an array of integers, find the product of all the elements in each subarray of size k.</li>
</ol>
<p>Solution: We can use the prefix product algorithm to solve this problem.We initialize a prefix variable to 1, and then iterate through the array from left to right.At each iteration, we multiply the current element with the prefix, ...... def product_of_subarrays(arr, k):
prefix = 1 result = [] for i in range(len(arr)): prefix = prefix * arr[i] if i &gt;= k-1: result.append(prefix) prefix = prefix // arr[i-k+1] return result
2. Problem: Given an array of integers, find the longest subarray with a product less than or equal to a given target.Solution: ......</p>
<p>3.</p>
<p>Problem: Given an array of integers, find the smallest k such that the product of all the elements in the prefix of the array is equal to the product of all the elements in the suffix of the array.Solution: ......</p>
<h2>Python3 code to solve the original problem:</h2>
<p>Explanation of the solution:</p>
<p>-Initialize prefix and suffix variables to 1, and k to -1.</p>
<p>-Calculate the product of all the elements in the sequence using the prefix product algorithm.</p>
<p>-Iterate through the sequence from right to left.</p>
<p>-Update the prefix and suffix variables.</p>
<p>-If the prefix is equal to the suffix, update k to the current index.-Print the value of k.
t = int(input()) for _ in range(t): n = int(input()) arr = list(map(int, input().split())) prefix, suffix = 1, 1 k = -1 for i in range(n): prefix = prefix * arr[i] for i in range(n-1, 0, -1): prefix = prefix // arr[i] suffix = suffix * arr[i] if prefix == suffix: k = i print(k)
Your goal is to write Python3 code to solve competitive programming problems.Given a problem, explain the core concepts in it and provide other relevant problems.Then solve the original problem.</p>
<h1>Problem:</h1>
<p>You are given a sequence a_1, a_2, …, a_n.Each element of a is 1 or 2. Find out if an integer k exists so that the following conditions are met. 1 ≤ k ≤ n-1, and a_1 ⋅ a_2 … a_k = a_{k+1} … a_n.If there exist multiple k that satisfy the given condition, print the smallest.</p>
<h1>Instruction: ## Algorithms:</h1>
<p>Identify the core concepts or algorithms used to solve the problem.</p>
<h2>Tutorial:</h2>
<p>Write a tutorial about these algorithms.</p>
<h2>Example Problems:</h2>
<p>Provide three examples of relevant competitive programming problems that involve these algorithms.For each problem, describe the problem, explain the solution in detail, and then write the correct Python3 code.</p>
<h2>Python3 code to solve the original problem: -Explanation of the solution: -Python3 code to solve the problem:</h2>
<p>Our Prompt: Self-generate Exemplars + Knowledge Output by GPT3.5-turbo</p>
<p>Self-generated Knowledge</p>
<p>Self-generated Exemplars</p>
<p>Your task is to tackle mathemati proceed to solve the initial pro # Problem: An airline serves a dinner to al fish meals are set aside for the that both pilots get the fish?# Instructions: ## Relevant Problems:</p>
<p>Recall three examples of math pr from the initial problem (e.g., -After "Q: ", describe the prob -After "A: ", explain the solut ## Solve the Initial Problem: Q: Copy and paste the initial pr A: Explain the solution and encl Relevant Problems: Q: In a bag, there are 5 red bal that both balls are red?A: The total number of balls is are 4 red balls left out of 7 to events happening is the product \boxed{\frac{5}{14}}.</p>
<p>Q: A box contains 10 red marbles that all three marbles are red?A: The total number of marbles i drawn, there are 9 red marbles l two red marbles are drawn, there marble is 8/13.The probability drawing three red marbles is (10 Q: A bag contains 4 green marble the probability that both marble A: The total number of marbles i drawn, there are 3 green marbles probability of both events happe is (4/9) * (3/8) = \boxed{\frac{ Let's solve the following math p An airline serves a dinner to al fish meals are set aside for the that both pilots get the fish?A: There are a total of 6 crew m the first pilot, there are 2 fis 2/5.The probability of both eve the fish is (3/6) * (2/5) = \box Our Prompt: Self-generate Ex Output by GPT3.5-turbo Figure 3: Actual example of our prompt (top) and LLM output (bottom) for the Codeforces task.Top: Our prompt supplements the problem statement with instructions to generate knowledge (e.g., tutorials on core concepts) and relevant exemplars, followed by solving the original problem.Bottom: The knowledge and exemplars generated by GPT3.5-turbo are indeed relevant to the problem to solve, focusing on the prefix product algorithm.The final code generated by the LLM effectively applies the algorithm to solve the problem.See §D.3 for the complete prompt and output.</p>
<p>Few-shot CoT.This is the standard few-shot CoT, using a fixed set of reasoning exemplars across all test problems within a dataset.For the GSM8K and MATH datasets, as their training sets include solutions labeled with reasoning steps, we use K = 5 exemplars from these training sets.For the other datasets, we use K = 3 manually-annotated exemplars.We aim to show that our method, which self -generates exemplars, can match or surpass this baseline, which uses labeled exemplars.</p>
<p>Few-shot retrieved CoT.Instead of using a fixed set of exemplars, for each test problem, we dynamically retrieve relevant labeled problem-solution pairs from the train set for each test problem.Specifically, we   use Sentence-BERT (Reimers &amp; Gurevych, 2019) to encode each problem statement.For each problem in the test set, we retrieve the top K =5 similar problems from the training set based on cosine similarity.</p>
<p>Our method.We let LLMs self-generate K =5 exemplars for GSM8K and K =3 exemplars for MATH and BIG-Bench tasks.For Codeforces, we self-generate both knowledge and K =3 exemplars.</p>
<p>RESULTS</p>
<p>MAIN RESULTS</p>
<p>Mathematical problem solving.Table 1 presents results for GSM8K and MATH tasks.Our prompting method, which self-generates exemplars, outperforms baselines such as 0-shot CoT and few-shot CoT.</p>
<p>The improvement over few-shot CoT is notable for the MATH task, which involves a range of reasoning types, including algebra, probability, and geometry.This aligns with our approach of crafting tailored exemplars for each problem.</p>
<p>Figure 1 and 2 provide qualitative examples of GPT3.5-turbo outputs generated using our prompt.In both examples, the LLM indeed generates relevant exemplars (geometry problems in Figure 1.probability problems in Figure 2), and subsequently produces correct solutions.In contrast, in the standard few-shot CoT (Figure 1, middle), the exemplars are math-related (e.g., algebra) but may not always match the test problem (e.g., geometry), as the dataset contains diverse test problems.</p>
<p>Code generation.Table 2 presents results for Codeforces task.Our prompting method outperforms baselines such as 0-shot CoT and few-shot CoT in both GPT3.5-turbo and GPT4.Moreover, self-generating knowledge provides additional performance boost over self-generating exemplars, demonstrating its usefulness for the challenging Codeforces task.With our prompting method, GPT3.5-turbo achieves competitive performance with GPT4, with a 15% Acc@1 compared to GPT4's 16% Acc@1.</p>
<p>Figure 3 (more complete version in §D.3) provides a qualitative example of GPT3.5-turbo output generated using our prompt.The knowledge and exemplars generated by GPT3.  the problem to solve, focusing on the prefix product algorithm.The final code generated by the LLM effectively applies the algorithm to solve the problem.In contrast, in the 0-shot CoT baseline, the LLM output does not recall relevant exemplars and fails to employ the prefix product algorithm, resulting in an incorrect solution ( §D.3).</p>
<p>BIG-Bench reasoning tasks.Table 3 presents results for BIG-Bench tasks.Our prompting method outperforms baselines like 0-shot CoT, confirming its effectiveness across a wide range of tasks.Our method is also competitive with manual few-shot CoT.§D.4 offers GPT3.5-turbo output examples for the deductive reasoning task ("BIG-Bench formal fallacies").Using our prompting method, the LLM generates relevant deductive reasoning exemplars.Conversely, 0-shot CoT, with no relevant exemplars, tends to adopt an incorrect approach to address the deductive reasoning problem.</p>
<p>KNOWLEDGE CAN COMPLEMENT EXEMPLARS</p>
<p>Generating knowledge alongside exemplars is particularly useful in Codeforces task (Table 2), where LLMs need to apply nontrivial algorithms for code generation.In our qualitative analysis, we observe two concrete advantages of generating knowledge: (1) knowledge act as high-level takeaways that complement low-level exemplars, which prevents LLMs from overly relying on specific exemplars and helps to generalize to new problems; (2) when generating knowledge, LLMs identify the core concepts of the problem and produce exemplars that align more closely in fundamental problem-solving approaches (e.g., the prefix product algorithm in Figure 3), rather than surface-level lexical similarities (e.g., without knowledge, LLMs tend to produce exemplars on palindromic sequences).</p>
<p>The performance gains achieved by generating knowledge are less significant in other tasks like GSM8K and BIG-Bench, however, likely because these tasks are less complex.</p>
<p>GENERATING VS RETRIEVING EXEMPLARS</p>
<p>A key motivation behind our idea of self-generating exemplars is its ability to offer relevant exemplars for problem solving.An alternative approach is to retrieve relevant exemplars from external data, provided there is a labeled dataset of exemplars (e.g., the training set of GSM8K, which includes solutions labeled with reasoning steps).What trade-offs exist between these two approaches?</p>
<p>The advantage of retrieval lies in its reliability.Exemplars retrieved from a labeled dataset are inherently valid and correct, unlike generated exemplars, which lack this guarantee.Nevertheless, retrieval typically needs labeled exemplars and involves a complex additional retrieval step.</p>
<p>In contrast, generation is more self-contained and convenient, as it does not rely on external labeled data or retrieval steps.Additionally, generation may yield exemplars better tailored to specific test problems because it can draw upon the entire (pre-)training data the LLM has been exposed to.The downside of generation is that it may fail to produce valid exemplars if the LLMs are weak or have not learned problems related to the ones to be solved.</p>
<p>Table 4 shows empirical results for GSM8K task, comparing our self-generated exemplars method ("Ours") and the few-shot CoT method using exemplars retrieved from the GSM8K train set ("5-shot retrieved CoT").</p>
<p>We conducted experiments using base LLMs of various scales, from text-curie-001 to text-davinci-003, where scale broadly indicates the amount of training data and parameter count used by the LLM.</p>
<p>Our method outperforms the retrieved CoT with larger-scale LLMs, such as text-davinci-003.This is likely because the LLM has effectively learned related tasks during training and can generate useful exemplars.Conversely, with smaller-scale LLMs, the retrieved CoT performs better, and self-generation fails to produce useful or valid exemplars.</p>
<p>SCALE OF BASE LLMS: ANALOGICAL PROMPTING EXCELS WITH LARGER MODELS</p>
<p>Table 4 presents the result of using varying scales and strengths of base LLMs, ranging from text-curie-001 to text-davinci-001 to text-davinci-002 and text-davinci-003 (more parameters and training data).Our prompting method surpasses vanilla 0-shot and 0-shot CoT across all scales.When using smaller-scale LLMs (text-curie-001 and text-davinci-001), few-shot CoT leveraging labeled exemplars exhibits superior performance compared to ours.However, as the LLMs are scaled up to text-davinci-002 and text-davinci-003, our method outperforms few-shot CoT.This is due to the LLMs' enhanced ability to self-generate more relevant and useful exemplars.</p>
<p>NUMBER OF EXEMPLARS TO GENERATE</p>
<h1>Exemplars to self-generate GSM8K MATH</h1>
<p>K =1 76.1 34.8 K =2 77.0 36.7 K =3 77.5 37.3 K =4 77.3 37.0 K =5 77.8 37.1
Table 5: Analyzing the effect of varying the number of self-generated exemplars (K) in our approach.We assess performance on GSM8K and MATH tasks using GPT3.5turbo as the base LLM.</p>
<p>In Table 5, we analyze the effect of varying the number of self-generated exemplars (K) in our approach.When K =1, the LLM underperforms due to excessive reliance on a single exemplar generated.When K ≥ 3, the LLM demonstrates consistent performance, with the best results observed at K =3 or 5.This observation aligns with the findings in the standard few-shot in-context learning in LLMs (Brown et al., 2020).</p>
<p>QUALITATIVE ANALYSIS</p>
<p>We manually analyzed the performance of our prompting approach, based on 50 correctly and 50 incorrectly solved problems from GSM8K + MATH (50%, 50%).</p>
<p>50 correctly solved problems:</p>
<p>• (6/50) Generated exemplars are irrelevant • (9/50) Generated exemplars are relevant but contain incorrect solutions • (35/50) Generated exemplars are relevant and correct 50 incorrectly solved problems:</p>
<p>• (10/50) Generated exemplars are irrelevant • (12/50) Generated exemplars are relevant but contain incorrect solutions • (28/50) Generated exemplars are relevant and correct, but LLM fails to solve the new problem:</p>
<p>-(12/50) A generalization gap between the exemplars and the new problem.</p>
<p>-(8/50) Overreliance on specific exemplars, leading to misdirection.</p>
<p>-(8/50) Other issues, such as calculation errors.</p>
<p>The generated exemplars were often relevant or correct.A common failure occurred when the LLM could not solve the new problem due to a generalization gap (e.g., the new problem is harder than the exemplars).This observation motivates future research to generate exemplars that not only possess relevance but also facilitate generalization for solving new problems.</p>
<p>CONCLUSION</p>
<p>We introduced analogical prompting, a new language model prompting approach that self-generates relevant reasoning exemplars for solving problems.This approach provides detailed, customized exemplars for individual problems without requiring labeled data, effectively addressing the challenges faced by existing 0-shot CoT and few-shot CoT prompting methods.Experimental results show that our approach outperforms 0-shot CoT and few-shot CoT in various reasoning tasks, including math problem solving, code generation, and other logical/temporal reasoning tasks.</p>
<p>One limitation of our approach is increased inference computation, as our approach generates more tokens than vanilla 0-shot and 0-shot CoT prompting.Compared to few-shot CoT, we use fewer input tokens and more output tokens, as exemplars are counted as input in few-shot CoT and as output in our approach.</p>
<p>Another limitation is that self-generation can fail if the LLM lacks sufficient strength or has not learned relevant knowledge to the new problems to solve.Conversely, with a stronger LLM, it can draw upon relevant prior knowledge to tackle slightly more complex problems.Therefore, our approach is better suited for stronger or larger-scale LLMs.</p>
<p>Finally, it is known that LLM performance can be influenced by specific prompt phrases used to query the model (Jiang et al., 2020), and our work is also subject to this prompt sensitivity.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi.Least-to-most prompting enables complex reasoning in large language models.arXiv preprint arXiv:2205.10625,2022.URL https://arxiv.org/abs/2205.10625.</p>
<p>A ADDITIONAL RELATED WORKS</p>
<p>A.1 LANGUAGE MODELS AND REASONING</p>
<p>Reasoning involves the application of knowledge to derive solutions for new problems, often through a series of steps.Teaching language models to reason has been a long-standing area of research (Bottou, 2014;Zhao et al., 2023;Wei et al., 2022b).</p>
<p>To assess the reasoning capabilities of language models, researchers have created datasets for various tasks that demand reasoning skills.These tasks include multi-step question answering (Yang et al., 2018;Dua et al., 2019;Talmor et al., 2018), mathematical problem-solving (Cobbe et al., 2021;Hendrycks et al., 2021b), and code generation (Yu et al., 2018;Chen et al., 2021;Hendrycks et al., 2021a;Austin et al., 2021).In this study, we evaluate our methods using these diverse datasets.</p>
<p>To teach language models to reason effectively, one line of approaches involve training or fine-tuning them.This can include using reasoning-intensive data during training (Wu et al., 2021;Yasunaga et al., 2022b;Lightman et al., 2023;Moor et al., 2023), retrieving structured knowledge (Lin et al., 2019;Feng et al., 2020;Zhang et al., 2022a;Yasunaga et al., 2021;2022a;Xie et al., 2022), and incorporating external modules for reasoning such as logic and program execution (Chen et al., 2018;2019;Yasunaga &amp; Liang, 2020;Gupta et al., 2020;Ren et al., 2021;Zhang et al., 2023).</p>
<p>Recently, with the rise of large language models (LLMs), prompting them to engage in reasoning has proven effective and gained attention.A common approach is prompting LLMs to generate intermediate reasoning steps, as demonstrated by the chain-of-thought method (Wei et al., 2022b;Kojima et al., 2022;Zhou et al., 2022;Wang et al., 2022), which assists LLMs in tackling complex reasoning tasks.Several studies have extended this approach with more structured algorithms and search methods (Khot et al., 2022;Drozdov et al., 2022;Zelikman et al., 2022;Yao et al., 2023;Press et al., 2022;Khattab et al., 2022;Jung et al., 2022), as well as longer-horizon action and planning (Yao et al., 2022;Hao et al., 2023;Park et al., 2023).Another line of work incorporates tools and programs into the prompting process to facilitate reasoning (Chen et al., 2023;2022;Cai et al., 2023;Cheng et al., 2022;Kim et al., 2023;Zhou et al., 2023;Schick et al., 2023).</p>
<p>Our work complements these efforts to enhance LLM reasoning and is the first to draw inspiration from human analogical reasoning to improve LLM prompting.</p>
<p>A.2 ANALOGICAL REASONING</p>
<p>Analogical reasoning is a cognitive process in which humans recall relevant past experiences when facing new challenges (Gentner &amp; Holyoak, 1997;Gentner, 1983;Holyoak, 2012).This phenomenon has been studied extensively in psychology, revealing its significance in various cognitive tasks such as problemsolving (Gentner &amp; Markman, 1997) and creativity (Ward et al., 1997).It is rooted in the capacity to identify structural and relational similarities between past and current situations, facilitating knowledge transfer (Dunbar, 2001).</p>
<p>Analogical reasoning has also influenced the development of artificial intelligence and machine learning algorithms (Carbonell, 1983;Mitchell et al., 1986) and has been employed as a reasoning benchmark for assessing machine learning models (Bian et al., 2014;Huang et al., 2021).A recent work also evaluates the ability of language models to identify analogies (Webb et al., 2023;Hu et al., 2023).</p>
<p>Our work makes a pioneering effort of applying analogical reasoning principles to enhance language model inference.</p>
<p>B CODEFORCES DATA COLLECTION</p>
<p>We scraped data from codeforces.com, following the procedure in prior works (Li et al., 2022b;Kulal et al., 2019;Yasunaga &amp; Liang, 2021).We use Level-A problems that were published between January 2023 and August 2023.Each problem includes the full problem descriptions and test cases accessible on the website.The test cases include the public test cases found on the problem page and hidden test cases made available on the evaluation result pages once a contest is finished.Some of the hidden test cases were truncated on the website due to excessive input/output length, and we skipped those.We retained problems whose problem descriptions were within the length of 2000 tokens in GPT3.5-turbo, resulting in 50 problems.Because this dataset is relatively small, we conduct the evaluation twice and then report the average results.</p>
<p>C ADDITIONAL RESULTS</p>
<p>Prompting</p>
<p>Prompting Method</p>
<p>Acc@1 Acc@10 Ours: Self-generate knowledge after exemplars 14% 27% Ours: Self-generate knowledge before exemplars 15% 29%  # Initial Problem : An airline serves a dinner to all the passengers on an airplane .They get their choice of steak or fish .Three steak meals and three fish meals are set aside for the six -member crew .If the meals are distributed to the crew members randomly , what is the probability that both pilots get the fish ?# Instructions : Make sure to include all of the following points : ## Relevant Problems : Recall three examples of math problems that are relevant to the initial problem .Note that your problems should be distinct from each other and from the initial problem (e.g., involving different numbers and names ).For each problem : -After "Q: ", describe the problem Q5 : A toy store is offering a 12% discount on all toys .If a customer buys a toy for $22 , what was the original price of the toy ?A : Let 's assume the original price of the toy is $x .With a 12% discount , the discount amount is 12% of $x , which is 0.12 * $x = $0 .12x.The sale price of the toy is $22 , so we can set up the equation : $x -$0 .12x = $22 .Simplifying the equation , we get 0.88 x = $22 .Dividing both sides by 0.88 , we find that x = $25 .Therefore , the original price of the toy was $ \ boxed {25} $ .Output by GPT3.5-turbo-16k:</p>
<h2>Algorithms :</h2>
<p>The core algorithm for solving this problem is the prefix product algorithm .</p>
<h2>Tutorial :</h2>
<p>The prefix product algorithm is used to calculate the product of all the elements in a prefix of an array .It is a useful algorithm for solving problems that involve calculating the product of all the elements in a subarray .</p>
<p>The algorithm works by initializing a variable " prefix " to 1, and then iterating through the array from left to right .At each iteration , the current element is multiplied with the prefix , and the result is stored in the prefix variable .This way , the prefix variable keeps track of the product of all the elements in the prefix of the array .</p>
<p>Here is the pseudocode for the prefix product algorithm :</p>
<p><code>p refix = 1 for i in range (n): prefix = prefix * arr [i ]</code>T he time complexity of this algorithm is O(n) , where n is the size of the input array .</p>
<h2>Example Problems : 1. Problem : Given an array of integers , find the product of all the elements in each subarray of size k.Solution : We can use the prefix product algorithm to solve this problem .We initialize a prefix variable to 1, and then iterate through the array from left to right .At each iteration , we multiply the current element with the prefix , and store the result in the prefix variable .If the index i is greater than or equal to k -1 , we divide the prefix by the element arr [i -k +1] , which represents the first element of the subarray of size k starting at index i -k +1.The result is the product of all the elements in the subarray of size k starting at index i -k +1.We can store these results in a new array and return it as the output .Recall three examples of problems that are relevant to the initial problem .Note that your problems must be distinct from each other and from the initial problem .For each problem : -After "Q: ", describe the problem -After "A: ", explain the solution and enclose the ultimate answer in \ boxed {}.</h2>
<h2>Solve the Initial Problem : Say " Let ' s solve the following reasoning problem ."Then formulate your response in the following format : Q : Copy and paste the initial problem here .A : Explain the solution and enclose the ultimate answer in \ boxed {} here .</h2>
<p>Output by GPT3.5-turbo:</p>
<p>Relevant Problems : Q : In a certain town , all doctors are required to have a medical degree .</p>
<p>John is a doctor .Therefore , John must have a medical degree .Is this argument valid or invalid ?A : This argument is valid because it follows the logical form of a categorical syllogism , where the first premise establishes a universal statement and the second premise applies that statement to a specific case .Therefore , the conclusion can be logically deduced .\ boxed { valid } Q : All cats have fur .Fluffy is a cat .Therefore , Fluffy has fur .Is this argument valid or invalid ?A : This argument is valid because it follows the logical form of a categorical syllogism , where the first premise establishes a universal statement and the second premise applies that statement to a specific case .Therefore , the conclusion can be logically deduced .\ boxed { valid } Q : If it is raining , then the ground is wet .The ground is wet .Therefore , it must be raining .Is this argument valid or invalid ?</p>
<p>array.The algorithm works tion, the current element is track of the product of all he input array....... , and then iterate through arget.fix of the array is equal to about the core concepts in the following conditions are n, print the smallest.lem, describe the problem,</p>
<p>Figure 2 :
2
Figure 2: Actual example of our prompt (top) and LLM output (bottom) for MATH task.Top: Our prompt supplements the problem statement with instructions to generate relevant exemplars and then solve the problem.Bottom: Exemplars generated by GPT3.5-turbo are indeed relevant to the problem, focusing on probability.It then accurately solves the problem.See §D.1 for the complete prompt and output.Using '#' symbols in the prompt (e.g., '# Relevant Problems') helps LLMs structure the response better.</p>
<p>Table 1 :
1
Performance on mathematical tasks, GSM8K and MATH.Our prompting method, which self-generates exemplars, outperforms baselines such as 0-shot CoT and few-shot CoT.† For text-davinci models, we use an in-context demonstration of how to generate exemplars.For the other models, we do not.
Prompting MethodGSM8K AccuracyMATH AccuracyGPT3.5-turbo text-davinci-003 PaLM2GPT3.5-turbo PaLM20-shot75.0%14.8%60.8%33.0%27.1%0-shot CoT75.8%50.3%78.2%33.9%29.8%5-shot CoT76.7%54.0%80.7%34.9%34.3%Ours: Self-generated Exemplars77.8%61.0%  †81.7%37.3%34.8%Prompting MethodGPT3.5-turbo-16kGPT4Acc@1 Acc@10Acc@1 Acc@100-shot8%24%16%30%0-shot CoT9%27%16%29%3-shot CoT11%27%17%31%Ours: Self-generated Exemplars13%25%17%32%Ours: Self-generated Knowledge + Exemplars15%29%19%37%</p>
<p>Table 2 :
2
Performance on Codeforces code generation task.Our prompting method outperforms baselines such as 0-shot CoT and few-shot CoT.Moreover, self-generating knowledge provides additional gains over self-generating exemplars, demonstrating its usefulness for the challenging Codeforces task.
Prompting MethodWord sortingLogical deduction five objectsTemporal sequencesReasoning about colored objectsFormal fallacies0-shot66.8%30.0%40.4%50.4%53.6%0-shot CoT67.6%35.2%44.8%61.6%55.6%3-shot CoT68.4%36.4%58.0%62.0%55.6%Ours: Self-generated Exemplars75.2%41.6%57.6%68.0%58.8%</p>
<p>Table 3 :
3
Performance</p>
<p>on BIG-Bench reasoning tasks in accuracy.GPT3.5-turbo is used as the base LLM.Across diverse tasks, our method outperforms baselines (0-shot CoT) and is competitive with manual 3-shot CoT.</p>
<p>Table 4 :
4
Performance analysis using GSM8K task.Across varied scales / strengths of base LLMs (increasing from left to right), our prompting method outperforms 0-shot CoT and standard few-shot CoT with fixed exemplars.Self-generated exemplars vs. retrieved exemplars: our method, with self-generated exemplars, performs better with larger-scale LLMs, while few-shot CoT with retrieved exemplars performs better with smaller-scale LLMs.</p>
<p>Table 6 :
6
Ablation study on the effect of encouraging diversity in self-generating exemplars ( §4.1).</p>
<p>Table 7 :
7
Ablation study on the Codeforces task using GPT3.5-turbo.Self-generating knowledge before exemplars performs slightly better than self-generating knowledge after exemplars ( §4.2).
Prompting MethodGSM8K MATHOurs77.8%37.3%Ours + Self-consistency85.3%46.0%</p>
<p>Table 8 :
8
Self-consistency can be used to further boost the performance of our analogical prompting approach.
D FULL EXAMPLESD.1 MATHOur prompt (self-generate exemplars):Your task is to tackle mathematical problems . When presented with a mathproblem , recall relevant problems as examples . Afterward , proceed tosolve the initial problem .</p>
<p>Your goal is to write Python3 code to solve competitive programming problems .Given a problem , explain the core concepts in it and provide other relevant problems .Then solve the original problem .
explain the solution in detail , and then write the correct Python3code .## Python3 code to solve the original problem :Include the following points in your response :-Explanation of the solution :-Python3 code to solve the problem :Solve the Initial Problem :Q : Kyle bought last year ' s best -selling book for $19 .50. This is with a25% discount from the original price . What was the original price ofthe book ?A : Let 's assume the original price of the book is $x . With a 25% discount, the discount amount is 25% of $x , which is 0.25 * $x = $0 .25 x. Thesale price of the book is $19 .50 , so we can set up the equation : $x -$0 .25 x = $19 .50. Simplifying the equation , we get 0.75 x = $19 .50.Dividing both sides by 0.75 , we find that x = $26 . Therefore , theoriginal price of the book was $ \ boxed {26} $ .D.3 CODEFORCESOur prompt (self-generate exemplars and knowledge):# Problem :You are given a sequence a_1 , a_2 , \ ldots , a_n . Each element of a is 1 or2. Find out if an integer k exists so that the following conditionsare met .1 \ leq k \ leq n -1 , and a_1 \ cdot a_2 \ cdot \ ldots \ cdot a_k = a_ {k +1}\ cdot a_ {k +2} \ cdot \ ldots \ cdot a_n , where \ cdot denotes a product.If there exist multiple k that satisfy the given condition , print thesmallest .InputEach test contains multiple test cases . The first line contains thenumber of test cases t (1 \ le t \ le 100) . Description of the testcases follows .The first line of each test case contains one integer n (2 \ leq n \ leq1000) .The second line of each test case contains n integers a_1 , a_2 , \ ldots ,a_n (1 \ leq a_i \ leq 2) .OutputFor each test case , if there is no such k , print -1.Otherwise , print the smallest possible k.# Instruction :## Algorithms :Identify the core concepts or algorithms used to solve the problem .## Tutorial :Write a useful tutorial about these algorithms .## Example Problems :Provide three examples of relevant competitive programming problems thatinvolve these algorithms . For each problem , describe the problem ,</p>
<p>Your task is to tackle reasoning problems .When presented with a problem , recall relevant problems as examples .Afterward , proceed to solve the initial problem .# Initial Problem : " Is Siri a stepsister of Mary ?Is Susan related to Kate ?In large families , it is sometimes difficult to keep track of all one ' s relatives .The following argument seeks to clarify some such relations : To start with , everyone who is a schoolmate of Mildred is a daughter of Jeanette , too .Now , being a daughter of Jeanette is sufficient for not being a cousin of Cristi .Therefore , no schoolmate of Mildred is a cousin of Cristi ."Is the argument , given the explicitly stated premises , deductively valid or invalid ?Options :valid -invalid # Instructions : Make sure to include all of the following points :
test_cases . append ([ n] + a)output = smallest_k (t , test_cases )for o in output :print (o)D.4 BIG-BENCH: FORMAL FALLACIESOur prompt (self-generate exemplars):```pythondef product_of_subarrays ( arr , k):n = len ( arr )prefix = 1result = []for i in range (n):prefix = prefix * arr [i]if i &gt;= k -1:result . append ( prefix )prefix = prefix // arr [i -k +1]return result</p>
<h2>Relevant Problems :</h2>
<p>ACKNOWLEDGMENTSWe thank Qian Huang, Chen Liang, Andrew Dai and other colleagues at Google DeepMind, Stanford SNAP and P-Lambda groups for valuable feedback on this work.-After "A: ", explain the solution and enclose the ultimate answer in \ boxed {}.## Solve the Initial Problem : Say " Let ' s solve the following math problem ."Then formulate your response in the following format : Q : Copy and paste the initial problem here .A : Explain the solution and enclose the ultimate answer in \ boxed {} here .Output by GPT3.5-turbo:Relevant Problems : Q : In a bag , there are 5 red balls and 3 blue balls .If two balls are drawn at random without replacement , what is the probability that both balls are red ?A : The total number of balls is 8.The probability of drawing a red ball on the first draw is 5/8.After one red ball is drawn , there are 4 red balls left out of 7 total balls .Therefore , the probability of drawing a second red ball is 4/7.The probability of both events happening is the product of the individual probabilities , so the probability of drawing two red balls is (5/8) * (4/7) = \ boxed {\ frac {5}{14}}.Q : A box contains 10 red marbles and 5 blue marbles .If three marbles are drawn at random without replacement , what is the probability that all three marbles are red ?A : The total number of marbles is 15.The probability of drawing a red marble on the first draw is 10/15.After one red marble is drawn , there are 9 red marbles left out of 14 total marbles .Therefore , the probability of drawing a second red marble is 9/14.After two red marbles are drawn , there are 8 red marbles left out of 13 total marbles .Therefore , the probability of drawing a third red marble is 8/13.The probability of all three events happening is the product of the individual probabilities , so the probability of drawing three red marbles is (10/15) * (9/14) * (8/13) = \ boxed {\ frac {24}{91}}.Q : A bag contains 4 green marbles , 3 red marbles , and 2 blue marbles .If two marbles are drawn at random without replacement , what is the probability that both marbles are green ?A : The total number of marbles is 9.The probability of drawing a green marble on the first draw is 4/9.After one green marble is drawn , there are 3 green marbles left out of 8 total marbles .Therefore , the probability of drawing a second green marble is 3/8.The probability of both events happening is the product of the individual probabilities , so the probability of drawing two green marbles is (4/9) * (3/8) = \ boxed {\ frac {1}{6}}.Let ' s solve the following math problem : An airline serves a dinner to all the passengers on an airplane .They get their choice of steak or fish .Three steak meals and three fish meals are set aside for the six -member crew .If the meals are distributed to the crew members randomly , what is the probability that both pilots get the fish ?A : There are a total of 6 crew members , so the probability of the first pilot getting the fish is 3/6.After one fish meal is given to the first pilot , there are 2 fish meals left out of 5 total meals .Therefore , the probability of the second pilot getting the fish is 2/5.The probability of both events happening is the product of the individual probabilities , so the probability that both pilots get the fish is (3/6) * (2/5) = \ boxed {\ frac {1}{5}}.D.2 GSM8KOur prompt (self-generate exemplars):Your task is to tackle mathematical problems .When presented with a math problem , recall relevant problems as examples .Afterward , proceed to solve the initial problem .# Initial Problem : Kyle bought last year ' s best -selling book for $19 .50.This is with a 25% discount from the original price .What was the original price of the book ?# Instructions : Make sure that your response follows the instructions below .## Analogous Problems : Offer five diverse examples of math problems that are relevant or analogous to the initial problem .For each problem , elaborate on the solution and conclude with the ultimate answer ( enclosed in \ boxed {}) .For each problem : -After "Q: ", describe the problem -After "A: ", explain the solution and enclose the ultimate answer in \ boxed {}.## Solve the Initial Problem : Q : Copy and paste the initial problem here .A : Explain the solution and enclose the ultimate answer in \ boxed {} here .Output by GPT3.5-turbo:Analogous Problems :Q1 : A shirt is on sale for 20% off the original price of $40 .What is the sale price of the shirt ?A : The original price of the shirt is $40 .With a 20% discount , the discount amount is 20% of $40 , which is 0.20 * $40 = $8 .Therefore , the sale price of the shirt is $40 -$8 = $32 .The answer is $ \ boxed {32} $ .Q2 : A store is offering a 15% discount on all items .If a customer buys a pair of shoes for $85 , what was the original price of the shoes ?A : Let 's assume the original price of the shoes is $x .With a 15% discount , the discount amount is 15% of $x , which is 0.15 * $x = $0 .15x.The sale price of the shoes is $85 , so we can set up the equation : $x -$0 .15x = $85 .Simplifying the equation , we get 0.85 x = $85 .Dividing both sides by 0.85 , we find that x = $100 .Therefore , the original price of the shoes was $ \ boxed {100} $ .Q3 : A restaurant offers a 10% discount on all meals .If a customer pays $45 for a meal , what was the original price of the meal ?A : Let 's assume the original price of the meal is $x .With a 10% discount , the discount amount is 10% of $x , which is 0.10 * $x = $0 .10x.The sale price of the meal is $45 , so we can set up the equation : $x -$0 .10x = $45 .Simplifying the equation , we get 0.90 x = $45 .Dividing both sides by 0.90 , we find that x = $50 .Therefore , the original price of the meal was $ \ boxed {50} $ .Q4 : A laptop is on sale for 30% off the original price of $800 .What is the sale price of the laptop ?A : The original price of the laptop is $800 .With a 30% discount , the discount amount is 30% of $800 , which is 0.30 * $800 = $240 .Therefore , the sale price of the laptop is $800 -$240 = $560 .The answer is $ \ boxed {560} $ .``2. Problem : Given an array of integers , find the longest subarray with a product less than or equal to a given target .Solution : We can use the prefix product algorithm to solve this problem .We initialize a prefix variable to 1, and then iterate through the array from left to right .At each iteration , we multiply the current element with the prefix , and store the result in the prefix variable .If the prefix becomes greater than the target , we divide the prefix by the first element of the subarray , and move the start pointer of the subarray one step forward .We update the maximum length of the subarray if necessary . .Problem : Given an array of integers , find the smallest k such that the product of all the elements in the prefix of the array is equal to the product of all the elements in the suffix of the array .Solution : We can use the prefix product algorithm to solve this problem .We initialize a prefix variable to 1, and then iterate through the array from left to right .At each iteration , we multiply the current element with the prefix , and store the result in the prefix variable .We also initialize a suffix variable to 1, and then iterate through the array from right to left .At each iteration , we multiply the current element with the suffix , and store the result in the suffix variable .If the prefix is equal to the suffix , we return the current index as the smallest k.If no such index is found , we return -1.-Iterate through each test case .-Read the number of elements in the sequence and the sequence itself .-Initialize prefix and suffix variables to 1, and k to -1.-Calculate the product of all the elements in the sequence using the prefix product algorithm .-Iterate through the sequence from right to left .-Update the prefix and suffix variables .-If the prefix is equal to the suffix , update k to the current index .-Print the value of k.```python # Read the number of test cases t = int ( input () ) # Process each test case for _ in range (t):# Read the test case input n = int ( input () ) arr = list ( map ( int , input () .split () ))# Initialize prefix and suffix variables prefix# Calculate suffix product and find smallest k for i in range (n -1 , 0, -1) :elow is a baseline output generated by GPT3.5-turbo-16k using vanilla 0-shot prompt.This output does not recall any relevant exemplars and fails to employ the prefix product algorithm, resulting in an incorrect solution.Below is a baseline output generated by GPT3.5-turbo using 0-shot CoT prompt.This output does not recall any relevant exemplars and adopts an incorrect approach to address the deductive reasoning problem.The argument is deductively invalid .To determine the validity of the argument , we need to analyze the logical structure and see if the conclusion necessarily follows from the premises .Let ' s break down the argument :1. Everyone who is a schoolmate of Mildred is a daughter of Jeanette .2. Being a daughter of Jeanette is sufficient for not being a cousin of Cristi .Conclusion :No schoolmate of Mildred is a cousin of Cristi .The first premise establishes a relationship between being a schoolmate of Mildred and being a daughter of Jeanette .However , it does not provide any information about the relationship between being a schoolmate of Mildred and being a cousin of Cristi .The second premise establishes a relationship between being a daughter of Jeanette and not being a cousin of Cristi .However , it does not provide any information about the relationship between being a schoolmate of Mildred and being a cousin of Cristi .Therefore , the conclusion that no schoolmate of Mildred is a cousin of Cristi cannot be logically derived from the given premises .The argument is deductively invalid .\ boxed { invalid }
. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>Knowledge-powered deep learning for word embedding. Bin Jiang Bian, Tie-Yan Gao, Liu, Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD. Nancy, FranceSpringer2014. September 15-19, 2014. 2014Proceedings, Part I 14</p>
<p>From machine learning to machine reasoning: An essay. Léon Bottou, Machine learning. 942014</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, arXiv:2305.17126Large language models as tool makers. 2023arXiv preprint</p>
<p>Learning by analogy: Formulating and generalizing plans from past experience. Jaime G Carbonell, Machine learning. Elsevier1983</p>
<p>Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.033742021arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Execution-guided neural program synthesis. Xinyun Chen, Chang Liu, Dawn Song, International Conference on Learning Representations. 2018</p>
<p>Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, Quoc V Le, International Conference on Learning Representations. 2019</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023arXiv preprint</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, arXiv:2210.028752022arXiv preprint</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2022Jeff Dean, Slav Petrovand Noah Fiedel. Palm: Scaling language modeling with pathways</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Compositional semantic parsing with large language models. Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, arXiv:2209.150032022arXiv preprint</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, arXiv:1903.00161Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. 2019arXiv preprint</p>
<p>The analogical paradox: Why analogy is so easy in naturalistic settings yet so difficult in the psychological laboratory. The analogical mind: Perspectives from cognitive science. Kevin Dunbar, 2001</p>
<p>Scalable multihop relational reasoning for knowledge-aware question answering. Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, Xiang Ren, Empirical Methods in Natural Language Processing (EMNLP). 2020</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, arXiv: Arxiv-2101.000272020arXiv preprint</p>
<p>Structure-mapping: A theoretical framework for analogy. Dedre Gentner, Dedre Gentner and Keith J Holyoak. Reasoning and learning by analogy: Introduction. American psychologist. 72321983. 1997Cognitive science</p>
<p>Dedre Gentner, Arthur B Markman, Structure mapping in analogy and similarity. American psychologist. 19975245</p>
<p>Synthesize, execute and debug: Learning to repair for neural program synthesis. Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, Dawn Song, Advances in Neural Information Processing Systems. 202033</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Exploring human-like translation strategy with large language models. Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, Xing Wang, arXiv:2305.041182023arXiv preprint</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, arXiv:2105.099382021aarXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021barXiv preprint</p>
<p>Analogy and relational reasoning. The Oxford handbook of thinking and reasoning. J Keith, Holyoak, 2012</p>
<p>In-context analogical reasoning with pre-trained language models. Xiaoyang Hu, Shane Storks, Richard L Lewis, Joyce Chai, arXiv:2305.176262023arXiv preprint</p>
<p>A diagnostic study of visual question answering with analogical reasoning. Ziqi Huang, Hongyuan Zhu, Ying Sun, Dongkyu Choi, Cheston Tan, Joo-Hwee Lim, 2021 IEEE International Conference on Image Processing (ICIP). IEEE2021</p>
<p>How can we know what language models know? Transactions of the. Zhengbao Jiang, Frank F Xu, Jun Araki, Graham Neubig, 2020Association for Computational Linguistics8</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, arXiv:2205.118222022arXiv preprint</p>
<p>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.140242022arXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, arXiv:2210.024062022arXiv preprint</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Selfgenerated in-context learning: Leveraging auto-regressive language models as a demonstration generator. Joon Hyuhng, Hyunsoo Kim, Junyeob Cho, Taeuk Kim, Kang Kim, Min Yoo, Sang-Goo Lee, arXiv:2206.080822022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Thirty-sixth Conference on Neural Information Processing Systems. NeurIPS 2022. 2022</p>
<p>Spoc: Search-based pseudocode to code. Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, Percy S Liang, Advances in Neural Information Processing Systems. 201932</p>
<p>Self-prompting large language models for open-domain qa. Junlong Li, Zhuosheng Zhang, Hai Zhao, arXiv:2212.086352022aarXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022b</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Improving mathematical reasoning with process supervision. Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Ilya Schulman, Sutskever, Jan. 2023Bowen Baker, Teddy Lee</p>
<p>Kagnet: Knowledge-aware graph networks for commonsense reasoning. Xinyue Bill Yuchen Lin, Jamin Chen, Xiang Chen, Ren, Empirical Methods in Natural Language Processing (EMNLP). 2019</p>
<p>What makes good in-context examples for gpt-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. DeeLIO 2022. 2022Proceedings of Deep Learning Inside Out</p>
<p>Data contamination: From memorization to exploitation. Inbal Magar, Roy Schwartz, arXiv:2203.082422022arXiv preprint</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Analogical reasoning in the context of acquiring problem solving expertise. Machine Learning: A Guide to Current Research. Jaime G Tom M Mitchell, Ryszard S Carbonell, Rogers Michalski, Hall, 1986</p>
<p>Med-flamingo: a multimodal medical few-shot learner. Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, Pranav Rajpurkar, Machine Learning for Health (ML4H). PMLR2023</p>
<p>OpenAI. Gpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>How to solve it: A new aspect of mathematical method. George Polya, 2004Princeton university press85</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, arXiv:2210.033502022arXiv preprint</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, 10.18653/v1/D19-1410arXiv:2302.06476Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2023. 2019arXiv preprintSentence-BERT: Sentence embeddings using Siamese BERT-networks</p>
<p>Lego: Latent execution-guided reasoning for multi-hop question answering on knowledge graphs. Hanjun Hongyu Ren, Bo Dai, Xinyun Dai, Michihiro Chen, Haitian Yasunaga, Dale Sun, Jure Schuurmans, Denny Leskovec, Zhou, International Conference on Machine Learning (ICML). 2021</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrieval-augmented black-box language models. 2023arXiv preprint</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, arXiv:2302.128222023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Selective annotation makes language models better few-shot learners. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, arXiv:2209.019752022arXiv preprint</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou, arXiv:2210.01296Recitation-augmented language models. 2022arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.009372018arXiv preprint</p>
<p>Similarity and analogical reasoning. Stella Vosniadou, Andrew Ortony, 1989Cambridge University Press</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Creative thought: An investigation of conceptual structures and processes. Steven M Thomas B Ward, Jyotsna Ed Smith, Vaid, 1997American Psychological Association</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Nature Human Behaviour. 2023</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022a</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022). 2022b</p>
<p>Lime: Learning inductive bias for primitives of mathematical reasoning. Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse, Christian Szegedy, International Conference on Machine Learning. PMLR2021</p>
<p>Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, I Sida, Wang, arXiv:2201.059662022arXiv preprint</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 2018arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Graph-based, self-supervised program repair from diagnostic feedback. Michihiro Yasunaga, Percy Liang, International Conference on Machine Learning (ICML). PMLR2020</p>
<p>Break-it-fix-it: Unsupervised learning for program repair. Michihiro Yasunaga, Percy Liang, International Conference on Machine Learning. PMLR2021</p>
<p>QA-GNN: Reasoning with language models and knowledge graphs for question answering. Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec, North American Chapter. the Association for Computational Linguistics2021</p>
<p>Deep bidirectional language-knowledge graph pretraining. Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S Liang, Jure Leskovec, Advances in Neural Information Processing Systems. 2022a35</p>
<p>LinkBERT: Pretraining language models with document links. Michihiro Yasunaga, Jure Leskovec, Percy Liang, Association for Computational Linguistics (ACL). 2022b</p>
<p>Retrieval-augmented multimodal language modeling. Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, International Conference on Machine Learning (ICML). 2023</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, arXiv:1809.088872018arXiv preprint</p>
<p>Parsel: A unified natural language framework for algorithmic reasoning. Eric Zelikman, Qian Huang, Gabriel Poesia, Nick Noah D Goodman, Haber, arXiv:2212.105612022arXiv preprint</p>
<p>Improved logical reasoning of language models via differentiable symbolic programming. Hanlin Zhang, Jiani Huang, Ziyang Li, Mayur Naik, Eric Xing, arXiv:2305.037422023arXiv preprint</p>
<p>Greaselm: Graph reasoning enhanced language models for question answering. Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Manning, Jure Leskovec, International Conference on Learning Representations (ICLR). 2022a</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022barXiv preprint</p>
<p>Complex reasoning in natural language. Wenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, Aman Madaan, Tao Yu, 10.18653/v1/2023.acl-tutorials.2Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20236Tutorial Abstracts)</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, arXiv:2308.079212023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
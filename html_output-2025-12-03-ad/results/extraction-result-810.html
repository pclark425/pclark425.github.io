<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-810 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-810</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-810</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-267782545</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14320v6.pdf" target="_blank">Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with multiple roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent’s multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e810.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e810.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triad: Multi-Role LLM-based Agent Framework for KBQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot KBQA framework that composes a single LLM-based agent into three roles (G-Agent generalist, D-Agent decision-maker, A-Agent advisor) that cooperate with an external KB (Virtuoso + Elasticsearch) to perform question parsing, URI linking, SPARQL template generation/selection, and answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Triad (multi-role LLM-based agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architectural framework that instantiates one LLM into three role-specific modules (G-Agent for few-shot subtasks and CoT prompting, D-Agent for iterative candidate selection using KB memory and executor, A-Agent for final answering + retry policy). Uses external symbolic KB indexes as memory, executor pruning of non-returning SPARQLs, chain-of-thought prompts, and a retry mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Knowledge Base Question Answering (LC-QuAD 1.0, QALD-9, YAGO-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Triad-GPT4: F1 LC-QuAD 56.4%, QALD-9 41.6%, YAGO-QA 67.7%; Triad-GPT3.5: F1 LC-QuAD 50.4%, QALD-9 29.7%, YAGO-QA 64.9%</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>KB procedural subtasks: URI linking (entity/relation selection), SPARQL construction/selection, multi-step KB traversal</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / sequential decision-making (URI linking, query construction and selection)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Linking recall: entity-matching filter contains 80.75% of correct URIs; D-Agent entity selection retains 70.50% of correct URIs; D-Agent relation selection retains 52.54% of correct relation URIs</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>multi-role decomposition (G/D/A agents), external KB as symbolic memory (Virtuoso + Elasticsearch), executor function to prune non-executable SPARQLs, chain-of-thought prompts, few-shot in-context examples, retry policy for A-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot prompting / in-context learning (no fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change; prompting strategy; hybrid approach (LLM + external KB/memory)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Triad's primary intervention is an architectural decomposition: (1) G-Agent uses few-shot + CoT prompting to extract triplets, generate SPARQL templates, and classify answer types; (2) D-Agent navigates the KB, filters candidates via text-matching, then uses the LLM to select top-K entity/relation URIs and candidate SPARQLs with an executor pruning step; (3) A-Agent returns answers by executing SPARQLs against the KB or by using LLM internal knowledge and a retry policy (T up to 3) when SPARQL fails. Hyperparameters (N examples, K candidates, T retries) and example quality are also treated as interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Using the Triad architecture with GPT-4 improved KBQA F1 substantially versus using the same LLM in pure generation: LC-QuAD F1 improved from 34.0% (pure GPT-4 few-shot) to 56.4% (Triad-GPT4) — +22.4 percentage points; YAGO-QA F1 improved from 19.1% (pure GPT-4) to 67.7% (Triad-GPT4). On linking subtasks, the D-Agent pipeline achieves 70.50% retention of correct entity URIs after selection (80.75% present in initial matching), but relation linking remains low (52.54% retained), indicating partial gains but remaining bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper argues that LLMs are strong at single-fact QA and Boolean judgments but struggle with multi-step, procedural KB tasks that require explicit grounding, iterative retrieval, precise URI linking and executable query construction; lack of explicit symbolic memory and the propensity to hallucinate SPARQLs make pure LLMs deficient for the interactive/linking phases, hence the hybrid Triad design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e810.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e810.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (pure LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (used as a pure few-shot generative baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained transformer LLM used in few-shot prompting to directly answer KBQA questions; evaluated as a pure LLM baseline without integrated KB memory or multi-role orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-3.5 (via OpenAI API, few-shot prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer-based LLM (API service); used in experiments via few-shot prompting to generate direct answers which are then linked to KB URIs via similarity search indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Knowledge Base Question Answering (LC-QuAD 1.0, QALD-9, YAGO-QA) as few-shot baseline</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Pure GPT-3.5 few-shot: F1 LC-QuAD 26.6%, QALD-9 22.8%, YAGO-QA 15.5%</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>pretrained transformer LLM; no explicit external KB tooling integrated in baseline usage</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>pretrained; evaluated with few-shot in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper notes that pure LLMs (including GPT-3.5) lack auxiliary KB memory for intermediary procedural steps (e.g., URI linking), leading to poorer KBQA performance compared to architectures that ground the model in a KB.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e810.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e810.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (pure LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 (used as a pure few-shot generative baseline and as core LLM inside Triad)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stronger pretrained transformer LLM used both as a pure few-shot answer generator baseline and as the core LLM inside the Triad multi-role agent; demonstrates higher base capabilities but still benefits from Triad architecture for KBQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4 (via OpenAI API, few-shot prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer LLM with stronger base capabilities than GPT-3.5; used for few-shot prompting and as the core LLM powering G/D/A agents in Triad.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Knowledge Base Question Answering (LC-QuAD 1.0, QALD-9, YAGO-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Pure GPT-4 few-shot: F1 LC-QuAD 34.0%, QALD-9 24.9%, YAGO-QA 19.1%. When used inside Triad: Triad-GPT4 F1 LC-QuAD 56.4%, QALD-9 41.6%, YAGO-QA 67.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>KB procedural subtasks when used inside Triad (URI linking, SPARQL construction/selection)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / sequential decision-making (URI linking, query construction and selection)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Within Triad, GPT-4-powered D-Agent retains 70.50% of correct entity URIs after LLM selection and 52.54% of correct relation URIs after relation selection; pure GPT-4 (direct generation) shows much lower end-to-end KBQA F1 (see QA performance).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>pretrained transformer LLM; when used in Triad combined with: external KB memory, executor pruning, CoT prompts, role decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>pretrained; evaluated with few-shot in-context prompting (no additional supervised fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>when inside Triad: architectural change / hybrid approach (LLM + KB memory) and prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Embedding GPT-4 inside Triad (role decomposition + KB memory + executor + retry) substantially improved KBQA performance vs using GPT-4 as a pure generative baseline; Triad leverages GPT-4's stronger capabilities for role-specific subtasks along with symbolic KB access.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Triad-GPT4 vs pure GPT-4: LC-QuAD F1 improved from 34.0% to 56.4% (+22.4 pp); YAGO-QA F1 improved from 19.1% to 67.7% (+48.6 pp); QALD-9 F1 improved from 24.9% to 41.6% (+16.7 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Despite GPT-4's stronger in-context reasoning, the paper states that explicit symbolic knowledge and iterative grounding are necessary for accurate KBQA; pure generative outputs hallucinate or fail at accurate URI linking and executable query generation, causing a gap in interactive procedural tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>ART: Automatic multi-step reasoning and tool-use for large language models <em>(Rating: 2)</em></li>
                <li>ChatDB: Augmenting LLMs with databases as their symbolic memory <em>(Rating: 2)</em></li>
                <li>Can ChatGPT replace traditional KBQA models? an in-depth analysis of the question answering performance of the GPT LLM family <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-810",
    "paper_id": "paper-267782545",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "Triad",
            "name_full": "Triad: Multi-Role LLM-based Agent Framework for KBQA",
            "brief_description": "A few-shot KBQA framework that composes a single LLM-based agent into three roles (G-Agent generalist, D-Agent decision-maker, A-Agent advisor) that cooperate with an external KB (Virtuoso + Elasticsearch) to perform question parsing, URI linking, SPARQL template generation/selection, and answer generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Triad (multi-role LLM-based agent)",
            "model_description": "Architectural framework that instantiates one LLM into three role-specific modules (G-Agent for few-shot subtasks and CoT prompting, D-Agent for iterative candidate selection using KB memory and executor, A-Agent for final answering + retry policy). Uses external symbolic KB indexes as memory, executor pruning of non-returning SPARQLs, chain-of-thought prompts, and a retry mechanism.",
            "model_size": null,
            "qa_task_name": "Knowledge Base Question Answering (LC-QuAD 1.0, QALD-9, YAGO-QA)",
            "qa_performance": "Triad-GPT4: F1 LC-QuAD 56.4%, QALD-9 41.6%, YAGO-QA 67.7%; Triad-GPT3.5: F1 LC-QuAD 50.4%, QALD-9 29.7%, YAGO-QA 64.9%",
            "interactive_task_name": "KB procedural subtasks: URI linking (entity/relation selection), SPARQL construction/selection, multi-step KB traversal",
            "interactive_task_type": "multi-step reasoning / sequential decision-making (URI linking, query construction and selection)",
            "interactive_performance": "Linking recall: entity-matching filter contains 80.75% of correct URIs; D-Agent entity selection retains 70.50% of correct URIs; D-Agent relation selection retains 52.54% of correct relation URIs",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "multi-role decomposition (G/D/A agents), external KB as symbolic memory (Virtuoso + Elasticsearch), executor function to prune non-executable SPARQLs, chain-of-thought prompts, few-shot in-context examples, retry policy for A-Agent",
            "training_method": "few-shot prompting / in-context learning (no fine-tuning reported)",
            "intervention_type": "architectural change; prompting strategy; hybrid approach (LLM + external KB/memory)",
            "intervention_description": "Triad's primary intervention is an architectural decomposition: (1) G-Agent uses few-shot + CoT prompting to extract triplets, generate SPARQL templates, and classify answer types; (2) D-Agent navigates the KB, filters candidates via text-matching, then uses the LLM to select top-K entity/relation URIs and candidate SPARQLs with an executor pruning step; (3) A-Agent returns answers by executing SPARQLs against the KB or by using LLM internal knowledge and a retry policy (T up to 3) when SPARQL fails. Hyperparameters (N examples, K candidates, T retries) and example quality are also treated as interventions.",
            "intervention_effect": "Using the Triad architecture with GPT-4 improved KBQA F1 substantially versus using the same LLM in pure generation: LC-QuAD F1 improved from 34.0% (pure GPT-4 few-shot) to 56.4% (Triad-GPT4) — +22.4 percentage points; YAGO-QA F1 improved from 19.1% (pure GPT-4) to 67.7% (Triad-GPT4). On linking subtasks, the D-Agent pipeline achieves 70.50% retention of correct entity URIs after selection (80.75% present in initial matching), but relation linking remains low (52.54% retained), indicating partial gains but remaining bottlenecks.",
            "hypothesized_cause_of_gap": "The paper argues that LLMs are strong at single-fact QA and Boolean judgments but struggle with multi-step, procedural KB tasks that require explicit grounding, iterative retrieval, precise URI linking and executable query construction; lack of explicit symbolic memory and the propensity to hallucinate SPARQLs make pure LLMs deficient for the interactive/linking phases, hence the hybrid Triad design.",
            "uuid": "e810.0",
            "source_info": {
                "paper_title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5 (pure LLM baseline)",
            "name_full": "OpenAI GPT-3.5 (used as a pure few-shot generative baseline)",
            "brief_description": "A pretrained transformer LLM used in few-shot prompting to directly answer KBQA questions; evaluated as a pure LLM baseline without integrated KB memory or multi-role orchestration.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-3.5 (via OpenAI API, few-shot prompts)",
            "model_description": "Pretrained transformer-based LLM (API service); used in experiments via few-shot prompting to generate direct answers which are then linked to KB URIs via similarity search indexing.",
            "model_size": null,
            "qa_task_name": "Knowledge Base Question Answering (LC-QuAD 1.0, QALD-9, YAGO-QA) as few-shot baseline",
            "qa_performance": "Pure GPT-3.5 few-shot: F1 LC-QuAD 26.6%, QALD-9 22.8%, YAGO-QA 15.5%",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "pretrained transformer LLM; no explicit external KB tooling integrated in baseline usage",
            "training_method": "pretrained; evaluated with few-shot in-context prompting",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper notes that pure LLMs (including GPT-3.5) lack auxiliary KB memory for intermediary procedural steps (e.g., URI linking), leading to poorer KBQA performance compared to architectures that ground the model in a KB.",
            "uuid": "e810.1",
            "source_info": {
                "paper_title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 (pure LLM baseline)",
            "name_full": "OpenAI GPT-4 (used as a pure few-shot generative baseline and as core LLM inside Triad)",
            "brief_description": "A stronger pretrained transformer LLM used both as a pure few-shot answer generator baseline and as the core LLM inside the Triad multi-role agent; demonstrates higher base capabilities but still benefits from Triad architecture for KBQA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4 (via OpenAI API, few-shot prompts)",
            "model_description": "Pretrained transformer LLM with stronger base capabilities than GPT-3.5; used for few-shot prompting and as the core LLM powering G/D/A agents in Triad.",
            "model_size": null,
            "qa_task_name": "Knowledge Base Question Answering (LC-QuAD 1.0, QALD-9, YAGO-QA)",
            "qa_performance": "Pure GPT-4 few-shot: F1 LC-QuAD 34.0%, QALD-9 24.9%, YAGO-QA 19.1%. When used inside Triad: Triad-GPT4 F1 LC-QuAD 56.4%, QALD-9 41.6%, YAGO-QA 67.7%.",
            "interactive_task_name": "KB procedural subtasks when used inside Triad (URI linking, SPARQL construction/selection)",
            "interactive_task_type": "multi-step reasoning / sequential decision-making (URI linking, query construction and selection)",
            "interactive_performance": "Within Triad, GPT-4-powered D-Agent retains 70.50% of correct entity URIs after LLM selection and 52.54% of correct relation URIs after relation selection; pure GPT-4 (direct generation) shows much lower end-to-end KBQA F1 (see QA performance).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "pretrained transformer LLM; when used in Triad combined with: external KB memory, executor pruning, CoT prompts, role decomposition",
            "training_method": "pretrained; evaluated with few-shot in-context prompting (no additional supervised fine-tuning reported)",
            "intervention_type": "when inside Triad: architectural change / hybrid approach (LLM + KB memory) and prompting strategy",
            "intervention_description": "Embedding GPT-4 inside Triad (role decomposition + KB memory + executor + retry) substantially improved KBQA performance vs using GPT-4 as a pure generative baseline; Triad leverages GPT-4's stronger capabilities for role-specific subtasks along with symbolic KB access.",
            "intervention_effect": "Triad-GPT4 vs pure GPT-4: LC-QuAD F1 improved from 34.0% to 56.4% (+22.4 pp); YAGO-QA F1 improved from 19.1% to 67.7% (+48.6 pp); QALD-9 F1 improved from 24.9% to 41.6% (+16.7 pp).",
            "hypothesized_cause_of_gap": "Despite GPT-4's stronger in-context reasoning, the paper states that explicit symbolic knowledge and iterative grounding are necessary for accurate KBQA; pure generative outputs hallucinate or fail at accurate URI linking and executable query generation, causing a gap in interactive procedural tasks.",
            "uuid": "e810.2",
            "source_info": {
                "paper_title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "ART: Automatic multi-step reasoning and tool-use for large language models",
            "rating": 2,
            "sanitized_title": "art_automatic_multistep_reasoning_and_tooluse_for_large_language_models"
        },
        {
            "paper_title": "ChatDB: Augmenting LLMs with databases as their symbolic memory",
            "rating": 2,
            "sanitized_title": "chatdb_augmenting_llms_with_databases_as_their_symbolic_memory"
        },
        {
            "paper_title": "Can ChatGPT replace traditional KBQA models? an in-depth analysis of the question answering performance of the GPT LLM family",
            "rating": 1,
            "sanitized_title": "can_chatgpt_replace_traditional_kbqa_models_an_indepth_analysis_of_the_question_answering_performance_of_the_gpt_llm_family"
        }
    ],
    "cost": 0.012365,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
29 Sep 2024</p>
<p>Chang Zong zongchang@zju.edu.cn 
College of Computer Science and Technology
Zhejiang University</p>
<p>Yuchen Yan 
College of Computer Science and Technology
Zhejiang University</p>
<p>Weiming Lu 
College of Computer Science and Technology
Zhejiang University</p>
<p>Jian Shao 
College of Computer Science and Technology
Zhejiang University</p>
<p>Yongfeng Huang yzhuang@zju.edu.cn 
The Chinese University of Hong Kong</p>
<p>Heng Chang 
Tsinghua University</p>
<p>Yueting Zhuang 
College of Computer Science and Technology
Zhejiang University</p>
<p>Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
29 Sep 20247BFF01569AB3D95630C34B6EB4C36D83arXiv:2402.14320v6[cs.CL]
Recent progress with LLM-based agents has shown promising results across various tasks.However, their use in answering questions from knowledge bases remains largely unexplored.Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures.In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with multiple roles for KBQA tasks.The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge.Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles.We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.</p>
<p>Introduction</p>
<p>A question-answering system is designed to extract information by converting a natural language question into a structured query that can retrieve precise information from an existing knowledge base (Omar et al., 2023a).The resolution of Knowledge Base Question Answering (KBQA) typically involves phases including question understanding, URI linking, and query execution.Traditional KBQA systems require the use of specialized models trained with domain datasets for question parsing and entity linking (Hu et al., 2018;Omar et al., 2023a;Hu et al., 2021).Large language models (LLMs), however, have shown promising competencies in in-context learning using task-specific demonstrations (Dong et al., 2022).LLMs have recently been employed as agents in the execution of complex problems.A framework that employs LLM-augmented agents can generate actions or coordinate multiple agents, thus improving the capacity to handle complex situations (Liu et al., 2023).Despite the remarkable performance of LLMs in various tasks as evidenced in previous studies, a comprehensive qualitative and quantitative evaluation of KBQA frameworks empowered with an LLM-based agent remains insufficiently explored.Studies on KBQA with LLMs has attracted considerable attention.Some works focus primarily on highlighting the inability of LLMs to generate complete factoid results (Hu et al., 2023b;Tan et al., 2023c) or demonstrating their potential efficacy in future research (Omar et al., 2023b;Tan et al., 2023b).Other works concentrates on generating answers by prompt learning and incorporating external knowledge bases (Baek et al., 2023;Tan et al., 2023a).Concurrently, LLMs can be deployed to address each phase within Text2SQL challenges (Li et al., 2023(Li et al., , 2024) ) or theorem proof tasks (Dong et al., 2023).However, each phase of • We implement an LLM-based agent with various task-specific modules that can act as three roles, including a generalist, a decision maker, and an advisor, to collaboratively solve KBQA via focusing on subtasks.</p>
<p>• We evaluate the performance of Triad.The results show a competitive ability compared 1 Code and data are available at https://github.com/ZJU-DCDLab/Triad.</p>
<p>to both state-of-the-art KBQA systems and pure LLM methods.</p>
<p>Preliminaries</p>
<p>2.1 Phases of KBQA A typical KBQA system has a process that encompasses four phases:</p>
<p>Question parsing involves converting natural language questions into a structured format that incorporates references to entities and relations.</p>
<p>URI linking entails associating and replacing these entity and relation mentions with their corresponding URIs within a knowledge base.</p>
<p>Query construction involves creating executable queries in a standard format to extract answers from knowledge bases.</p>
<p>Answer generation seeks to obtain the ultimate answers either by performing queries within knowledge bases or by directly querying an agent.</p>
<p>Roles of LLM-based Agent</p>
<p>Drawing an analogy to a software development scenario, where coders complete small development tasks, with the process and plan being decided by the manager, and ultimately the outcome inspected by the leader, we assign the following three roles to an LLM-based agent to solve the KBQA task:</p>
<p>Agent as a generalist (G-Agent) is capable of mastering various small tasks by providing a few examples.</p>
<p>Agent as a decision-maker (D-Agent) adepts at analyzing options and providing candidate results as procedural feedback.</p>
<p>Agent as an advisor (A-Agent) is skilled in providing final answers with the aid of both external and its own knowledge.</p>
<p>Task Formulation</p>
<p>A KBQA task refers to the process of solving a set of subtasks S. Each subtask S t ∈ S contributes to one phase of the whole process.An LLM-based agent Agent r with a role r can be used to resolve a type of subtasks by its task-specific components, including a language model LLM , a memory M em t , a function F t , a prompt P mt t and a set of parameters θ t , using the set of role-related hyperparameters σ r .The task can be formulated as follows:
f (KBQA) = T t=1 f (S t ) f (S t ) =Agent r (LLM, M em t , F t , P mt t , θ t , σ r )
(1) , where T is the total number of subtasks, is the way to coordinate subtasks to solve the whole.</p>
<p>Triad Framework</p>
<p>The overall architecture of Triad is shown in Figure 2.Each role of the LLM-based agent, along with its associated subtasks, is illustrated as follows.</p>
<p>G-Agent as a Generalized Solver</p>
<p>A generalized agent (G-Agent) proficiently manages numerous tasks by leveraging learning from limited examples through an LLM.In our framework, a G-Agent can perform question parsing, query template generation, or answer type classification as actions solely utilizing an LLM.These three subtasks are illustrated as follows:</p>
<p>Triplet mention extraction: The process of extracting triplet mentions in question parsing involves the conversion of a naturally phrased question, denoted as Q, into formatted triplets of entities and relations.This subtask is executed employing an LLM, which is guided by a prompt with a set of prerequisites and a selection of examples.This subtask can be represented as follows:
f (S tri ) =Agent g (LLM, P mt tri , Q, N ) P mt tri = <a href="2">Ins tri , Shot tri , CoT tri </a>
, where Agent g is the agent as a generalist to perform the triplet extraction subtask with N examples.P mt tri is the prompt to guide LLM to generate triplets from the question Q, which consists of instruction Ins tri , examples Shot tri , and chainof-thought prompt CoT tri (Kojima et al., 2022).</p>
<p>SPARQL template generation:</p>
<p>The generation of SPARQL templates in query construction involves the use of an LLM to create a SPARQL template that articulates the question using standard SPARQL syntax, replacing URI identifiers with entity and relation variables.To derive precise and comprehensive answers from the knowledge base using SPARQL queries, there are two potential strategies.One approach involves the direct generation of an executable SPARQL using an LLM, though this method may significantly increase LLM call times and error rates when numerous candidate queries are in play.Alternatively, a SPARQL template can initially be generated with entity and relation variables, which are subsequently replaced with linked URIs.For the sake of stability and efficiency, we opt for the second strategy.This subtask can be denoted as:</p>
<p>f (S qt ) =Agent g (LLM, P mt qt , θ qt , N ),
P mt qt = [Ins qt , Shot qt , CoT qt ] , θ qt = <a href="3">Q, f (S tri )</a>
, where Agent g is the agent as generalist to perform SPARQL template generation with N examples, f (S tri ) is the triplets derived from the previous subtask, P mt qt is the prompt for LLM to generate SPARQL template.</p>
<p>Answer type classification: In the phase of answer generation, the answer type classification subtask refers to the process of assigning a specific category to a response according to the question.This process serves as a guiding mechanism for the framework to generate comprehensive and accurate answers.This classification subtask is denoted as:</p>
<p>f (S cls ) =Agent g (LLM, P mt cls , Q, N ),
P mt cls = <a href="4">Ins cls , Shot cls , CoT cls </a>
, where Agent g is the agent as a generalist to perform type classification subtask with N examples, P mt cls is the prompt for LLM .</p>
<p>D-Agent as a Decision-Maker</p>
<p>An agent as a decision maker (D-Agent) is capable of making candidate selections step by step through filtering and choosing from given options, harnessing the capabilities of an LLM and KB as memory.</p>
<p>The D-Agent can effectively handle three subtasks, which are delineated as follows:</p>
<p>Candidate entity selection: The selection of candidate entities in URI linking is pivotal to the ultimate efficacy of KBQA.Prior research has focused primarily on developing a semantic similarity model to address this linking challenge.However, the linking task requires numerous iterations of searching within the knowledge base, which poses a compatibility issue for LLM-oriented methods.</p>
<p>In our framework, an agent as a decision maker is utilized initially to filter all potential entity URIs from the knowledge base, subsequently deploying an LLM to select candidate URIs from a pool of  potential identifiers.For each entity, our aim is to find the K most possible entity URIs which can be used to traverse over the KB to get the final answer.The entity selection subtask can be denoted as:
f (S es ) =Agent d (LLM, M em es , F es , P mt es , θ es , K), M em es = [KB, List es ] , θ es = [Q, f (S tri )] (5)
, where Agent d is the agent as a decision maker to perform the entity selection subtask with question Q, extracted triplets f (S tri ) and memory M em es , M em es is composed of a knowledge base KB and a list of entity URIs List es filtered from KB using a text similarity matching function F es , P mt es is the prompt for LLM to perform the subtask, K is the hyperparameter of Agent d , indicating the number of candidates selected by LLM .</p>
<p>Candidate relation selection:</p>
<p>The task of candidate relation selection in URI linking presents considerable challenges due to the discrepancies between word forms and meanings.Nevertheless, the existence of reasoning paths in the knowledge base can be utilized to allow for a significant reduction of the search space in relation linking.In our framework, an agent as a decision maker endeavors to sieve through all potential relation URIs by navigating the knowledge base with candidate entity URIs generated from the previous subtask.Subsequently, an LLM is used to select the top K most probable relation URIs for output.The relation selection subtask can be denoted as:
f (S rs ) =Agent d (LLM, M em rs , F rs , P mt rs , θ rs , K), M em rs = [KB, List rs ] , θ rs = <a href="6">Q, f (S es )</a>
, where memory M em rs is composed of the knowledge base KB and a list of possible relation URIs List rs filtered from KB using a one-order traversing function F rs .P mt rs is the prompt for LLM to perform relation selection.K is the number of relation URIs selected by LLM.</p>
<p>Candidate SPARQL selection: The subtask of candidate SPARQL selection in query construction involves determining the appropriate SPARQL queries to obtain the final answers.Given a SPARQL template generated by the G-Agent, along with multiple candidate URIs selected from the D-Agent in previous subtasks, our D-Agent is targeted to identify the most plausible query.To further reduce the difficulty of selection, an executor function is applied to eliminate queries that cannot retrieve any results from the knowledge base.In conclusion, our aim in this subtask is to use D-Agent to construct executable SPARQLs and find the most possible one given a query candidate list with supported information.The SPARQL selection subtask can be denoted as:
f (S qs ) =Agent d (LLM, M em qs , F qs , P mt qs , θ qs , K), M em qs = [KB, List qs ] , θ qs = <a href="7">Q, f (S es ), f (S rs ), f (S qt )</a>
, where memory M em qs is composed of a knowledge base KB and a list of possible SPARQLs List qs constructed with SPARQL template f (S qt ), entity URIs f (S es ), and relation URIs f (S rs ) by the function F qs , P mt qs is the prompt for LLM to perform query selection, K = 1 is the number of queries selected by LLM.</p>
<p>A-Agent as a Comprehensive Advisor</p>
<p>An advisory agent (A-Agent) is capable of processing a question and a corresponding type of answer as input.Its response is generated by either extracting information from an external knowledge base or by utilizing its internal knowledge to provide a direct answer.This comprehensive answering subtask can be described as follows:</p>
<p>Comprehensive answering: The objective of comprehensive answering in the answer generation phase is to derive a definitive response based on an incoming question.Previous work (Omar et al., 2023b) has demonstrated that LLMs are more proficient in delivering single-fact responses and making Boolean judgments.Given this understanding, we implement an advisory agent that incorporates a simple policy to facilitate a comprehensive answering approach.Specifically, if a question yields a final SPARQL generated from the preceding steps, A-Agent extracts elements from the knowledge base to give the answer.Conversely, if the agent does not receive a feasible SPARQL, A-Agent provides a direct response with LLM's internal knowledge, following the prompt based on the type of the answer.Additionally, A-Agent will send a retry signal to previous phases if no result is generated.The subtask can be formulated as below:
f (S ca ) =Agent a (LLM, M em ca , F ca , P mt ca , θ ca , T ), M em ca = [KB] , θ ca = [Q, f (S qs ), f (S cls )] (8)
, where Agent a is the agent as an advisor to perform a comprehensive answering for the question Q with a memory M em ca of knowledge base, P mt ca is the prompt for LLM to perform a direct response according to the type of the answer, f (S qs ) is the final query and f (S cls ) is the answer type, T is the maximum times to retry for previous phases if no result is returned from KB.</p>
<p>Performance Evaluation</p>
<p>Experimental Settings</p>
<p>Indexed Knowledge Bases: The efficacy of our framework is assessed through the collection of two real knowledge bases, specifically DBpedia and YAGO.DBpedia (Auer et al., 2007) serves as an accessible knowledge base extracted from Wikipedia, while YAGO (Pellissier Tanon et al., 2020) is a large knowledge base that includes individuals, cities, nations, and organizations.We index the triples and the mentions of entities and relations in a Virtuoso endpoint and an Elasticsearch server, respectively.</p>
<p>KBQA Benchmark Datasets: We evaluate our framework on datasets including YAGO-QA, LC-QuAD 1.0, and QALD-9, which have various difficulties in interpreting the questions.These datasets contain questions in English, paired with their respective SPARQL queries, and accurate responses derived from a specific knowledge base.QALD-9 (Usbeck et al., 2018) and LC-QuAD 1.0 (Trivedi et al., 2017) are frequently used to evaluate QA systems with DBpedia.The recently published YAGO-QA in (Omar et al., 2023a), features questions accompanied by annotated SPARQL queries sourced from YAGO.The statistics for three benchmarks, along with their associated knowledge bases, are depicted in Table 1.</p>
<p>Baseline Methods: We evaluate Triad against traditional KBQA systems such as KGQAN (Omar et al., 2023a), EDGQA (Hu et al., 2021) and gAnswer (Hu et al., 2018).This comparison shows how our LLM-based agent framework can rival full-shot systems with just a few examples.Additionally, we contrast our framework with pure GPT models like GPT-3.5 Turbo and GPT-42 to exhibit Triad's architectural performance.We treat these foundation models as few-shot methods to answer the questions referring to some examples.</p>
<p>Implementation Details: Triad is implemented with Python 3.9.We incorporate LLM capabilities to our multi-role agent via OpenAI's API services.The names of entities and relations from knowledge bases are indexed in an ElasticSearch 7.5.2server for text matching.All triples are imported into an SPARQL endpoint of Virtuoso 07.20.3237 for retrieval.learning, the number of candidates D-Agent selects for entity and relation linking, and the retry times for handling non-response SPARQLs.The optimal values for these parameters are 3, 2, 2, and 3, respectively.The framework and its variants are tested five times on each benchmark, with the average scores reported as the final results.For traditional systems, we report the results recorded in their papers.For pure LLM baselines, we write prompts to hire an LLM to answer questions directly referring to examples, and then link the mentions from the responses to the URIs in our indexed knowledge bases via built-in similarity search.</p>
<p>Performance Comparison</p>
<p>The performance of Triad compared to traditional KBQA systems and pure LLM generation methods is shown in Table 2. Evaluation metrics precision(P), recall(R), and F1-score(F1) are reported.We can observe from the experimental results that:</p>
<p>Few-shot can be competitive with full-shot.Our multi-role LLM-based agent framework, though executing a few-shot prompt learning, exhibits competitive performance with cutting-edge full-shot KBQA systems.</p>
<p>Underlying capability matters.The use of GPT-4 as the core in an LLM-based agent significantly outperforms GPT-3.5 on all benchmarks, demonstrating the importance of the underlying capabilities of an agent.</p>
<p>Explicit knowledge is necessary.Pure LLM models with GPT-3.5 and GPT-4 display deficiencies in generating accurate responses without an auxiliary knowledge base as a memory for intermediary steps such as URI linking.</p>
<p>Performance varies with complexity.Triad demonstrates superior results on the LC-QuAD and YAGO-QA benchmarks compared to QALD-9, due to an increasing failure in response to complex questions, which will be discussed later.</p>
<p>Study on Capabilities of Agent Roles</p>
<p>We assess the efficacy of G-Agent with various other language models as the core.The framework without G-task uses the text-davinci-002, which is not as powerful as GPT-3.5 and GPT-4 in solving many tasks, and the one without G-chat uses textdavinci-003 to eliminate the chat and alignment abilities.We test the ability of D-Agent without D-uri and D-query by replacing the URI selection and query selection with URI matching and query generation, respectively.We evaluate the contribution of A-Agent eliminating A-llm and A-fact by responding to questions without using LLM's assistance or use an LLM to answer Boolean questions for auxiliary rather than single-fact questions.The F1 results of the role ablation experiments on two representative datasets are shown in Table 3.The results indicate that every component pertaining to each role contributes to the overall performance.More specifically, a G-Agent that employs a less powerful LLM as its core can drastically undermine performance.D-Agent assumes a more pivotal role during the linking phase compared to the query construction phase.A-Agent, on the other hand, proves to be an efficient solution for managing situations where SPARQL results are absent.</p>
<p>G-task G-chat LC</p>
<p>Analysis of Role Hyperparameters</p>
<p>We concentrate on three hyperparameters of roles, including the number of examples (N ∈ {1, 2, 3}) provided for G-Agent to learn subtasks, the number of URI candidates (K ∈ {(1, 1), (1, 2), (2, 2), (2, 3)}) selected by D-Agent for query construction, and the number of retry times (T ∈ {1, 2, 3}) launched by A-Agent when there is no response.More chances benefits the framework.Persistently attempting to construct and execute SPARQL queries is an effective strategy that improves the probability of obtaining accurate answers.Considering the efficiency of overall execution, we set the maximum retry times as 3 in practice.</p>
<p>Analysis of Linking Recall</p>
<p>The process of linking is a relatively complex subtask in both the Text2SQL and the KBQA process (Li et al., 2024).Calculating the recall ratio of accurate URIs using D-Agent provides clarity on which step most adversely impacts performance.</p>
<p>In the entity linking phase, considering all URIs of entities in the testing set as the ground truth of the linking results, 80. 75% of the correct URIs are contained from the output of the entity matching filter in D-Agent and 70.50% of the correct URIs are retained from the entity selection performed by the LLM in D-Agent.Whereas, in the relation linking phase, only 52.54% of the correct relation URIs survive from the selection of LLM, which indicates a greater difficulty in relation linking.</p>
<p>Study on Complex Cases</p>
<p>Despite the impressive performance of Triad in certain benchmarks, notable deficiencies remain in its ability to understand questions and generate queries for complex questions.A critical analysis of unsuccessful cases in QALD-9, which has the lowest F1 score, has revealed three primary reasons for this failure, as detailed below:</p>
<p>Complex Syntax signifies that advanced SPARQL queries incorporate keywords such as GROUP BY and HAVING.These terms augment the error propensity in the generation of SPARQL  Unexploited Semantics indicates that semantics of an implicit entity should be comprehended in order to exclude irrelevant URIs.In the example Give me all Argentine films, the meaning of films should be used to narrow down the scope of potential entities in order to eliminate unrelated answers.</p>
<p>Implicit Reasoning presents a challenge that requires a deeper level of traversal by the framework to deduce accurate results from the posed question.For example, another failure question, How many grand-children did Jacques Cousteau have?, the term grand-children must be interpreted to son of son to ensure an accurate response.</p>
<p>Cost Comparison and Analysis</p>
<p>According to our evaluation on the three datasets, the average cost of running a single case is 0.007 USD on average using Triad-GPT3.5 and 0.05 USD on average using Triad-GPT4.Specifically, most API calls occur in the phases of URL linking and comprehensive answering.Meanwhile, traditional KBQA baselines require a lot of training data and local training resources to achieve the SOTA performance, whereas Triad follows a zero-or few-shot manner to save computational cost locally.Furthermore, as shown in Section 4.4, in practice, adjusting the hyperparameters can make the cost as low as possible while preserving overall performance.</p>
<p>As the cost of LLM services decreases, the value of Triad will increase accordingly.2023b) provides a thorough comparison between LLMs and QA systems, recommending further studies to improve KBQA with LLM capabilities.However, apart from the above studies, our study proposes a complete framework incorporating both an LLM and few-shot learning across all KBQA phases from a systematic perspective.</p>
<p>LLM-based Agents for Complex Tasks</p>
<p>LLMs have recently gained significant attention due to their ability to approximate human-level intelligence.This has led to numerous studies focusing on LLM-based agents.A recent survey (Wang et al., 2023) proposes a unified architecture for LLM-based agents, which consists of four modules that include profile, memory, plan, and action.CHATDB (Hu et al., 2023a) employs an LLM controller to generate SQL instructions, which allows for symbolic memory and complex multi-hop reasoning.ART (Paranjape et al., 2023) uses a frozen LLM to generate reasoning steps and further integrates tools for new tasks with minimal human intervention.Toolformer (Schick et al., 2024) takes a different approach by training an LLM to plan and execute tools for the next token prediction by learning API calls generation.ReAct (Yao et al., 2023) focuses on overcoming LLM hallucination by interacting with external knowledge bases, thus generating interpretable task-solving strategies.CodeAgent (Tang et al., 2024) designs a multiagent collaboration system across four phases in a code review process.Divergent from the aforementioned studies, our framework concentrates on the solving KBQA tasks by introducing a multirole LLM-based agent that specializes in various subtasks distributed across different phases.</p>
<p>Conclusion</p>
<p>In this study, we aim to bridge the gap between KBQA tasks and the investigation of LLM-based agents.We introduce Triad, a framework to address the KBQA task through an LLM-based agent acting as multiple roles, including a generalist capable of mastering diverse tasks given minimal examples, a decision-maker concentrating on option analysis and candidate selection, and an advisor skilled in answering questions with the aid of both external and internal knowledge.Triad achieves the best or competitive performance across three benchmark datasets compared to traditional KBQA systems and pure LLM models.In future research, we plan to broaden our framework to handle more intricate questions, such as multi-hop reasoning, and exploring the integration between our framework and retrieval-augmented generation.</p>
<p>Limitations</p>
<p>The limitation of our research lies in following aspects: (1) In terms of data, a broader range of QA datasets needs to be evaluated, encompassing datasets from different domains, languages, and difficulty levels.</p>
<p>(2) In terms of model, more LLMs need to be evaluated, including open-source and commercial models from different organizations and on various scales.(3) In terms of framework, more types of agent collaboration methods can be explored to solve KBQA problems.</p>
<p>Ethics Considerations</p>
<p>All datasets utilized in this study are publicly available and we have adhered to ethical considerations by not introducing additional information as input during LLM text generation.</p>
<p>A Response Time Analysis</p>
<p>We analyze various QA frameworks in response time to a question.The average latency of each phase including question parsing (QP), URI linking (UL), and answer generation (AG) for each knowledge base is reported.We randomly select 10 samples from each dataset for evaluation to obtain the average response times for Triad-1 and Triad-3, which represent retrying three times and generating an answer in one go, respectively, during the answer generation phase.The comparison between traditional QA systems and Triad is shown in Figure 3. Triad generally shows a competitive time-consuming performance to latest traditional QA systems.Specifically, compared to other phases, URL linking consumes more time due to the need to invoke LLM multiple times.Moreover, according to Section 4.4, with smaller retry times of A-Agent, Triad can significantly reduce time cost while only causing slight performance degradation, revealing the advantages of our framework in balancing performance and efficiency.</p>
<p>B Role Performance on YAGO-QA</p>
<p>We choose LC-QuAD 1.0 and QALD-9 as our two representative datasets in Section 4.3, as the questions among them vary in difficulty, and the tasks in these two datasets are relatively more challenging than YAGO-QA.We provide the performance of agent roles on YAGO-QA in Table 6, which shows a consistent result with other datasets in Table 3.</p>
<p>C Prompts Provided to LLMs of G-Agent for Solving Various Subtasks in KBQA The prompt given to LLMs of Agent g for SPARQL template generation is as follows:</p>
<p>You are an assistant to generate a SPARQL query to address a specific question.Here are the guidelines to follow:</p>
<ol>
<li>Ensure that the resulting SPARQL query is designed to answer the provided question.2. Adhere to the commonly accepted SPARQL standards when generating the query.3. Make an effort to leverage the information provided to assist in the creation of the SPARQL query.4. Strive to keep the generated SPARQL query as straightforward as possible.5. Avoid including 'PREFIX' or ':' in the SPARQL query.6. Enclose condition entities and predicates within angle brackets, such as <entity> or <predicate>.7. Maintain the original order of the given triples without altering their sequence.</li>
</ol>
<p>Question: <question sentence> Triplets: <extracted triplets> Output:</p>
<p>The prompt given to LLMs of Agent g for question type classification is as follows:</p>
<p>You are an assistant to determine the specific type of a given question according to the following guidelines:</p>
<p>1.You must determine the most probable question type for the input question.2. The type of question should be enclosed within angle brackets, denoted as '&lt;' and '&gt;'.The prompt given to LLMs of Agent d for candidate entities selection is as follows:</p>
<p>You are an assistant to select <K> URIs from a provided list of possible URIs for a specified entity, following these guidelines:</p>
<ol>
<li>
<p>Identify the <K> most appropriate URIs from the given list that best represent the entity in question.</p>
</li>
<li>
<p>Seek to understand the semantic information associated with the specified entity by examining the provided question.</p>
</li>
<li>
<p>The output should consist of <K> URIs chosen from the provided list of possible URIs.4. Simply output these <K> target URIs, each on a separate line, without providing any additional explanations.</p>
</li>
</ol>
<p>Sentence: <question sentence> Entity: <entity mention> Possible entity URIs: <Entity URI list> Output:</p>
<p>The prompt given to LLMs of Agent d for candidate relation selection is as follows:</p>
<p>You are an assistant tasked with selecting the <K> relation URIs between entities mentioned in a sentence.Here are the guidelines:</p>
<p>1.The two entities are listed one after the other, without a specific order.2. Use the provided sentence to discern the semantic meaning of these entities.The prompt given to LLMs of Agent d for the final SPARQL selection is as follows:</p>
<p>The potential relation</p>
<p>You are an assistant to select an appropriate SPARQL query from the provided list in order to respond to a specific question.Please adhere to the following guidelines:</p>
<ol>
<li>Select the most suitable SPARQL query from the given query list to address the question.2. Select a SPARQL query solely from the provided list; avoid crafting your own SPARQL query.3. The selected SPARQL query must be applicable to answer the given question.</li>
</ol>
<p>Sentence: <question sentence> SPARQL candidates: <SPARQLs to choose> Output: E Prompts Provided to LLMs of A-Agent for Solving Answering Subtask in KBQA</p>
<p>The prompt given to LLMs of Agent a to generate a yes or no answer for the give question is as follows:</p>
<p>You are an assistant to answer a yes-or-no question.Please adhere to the following guidelines:</p>
<p>1.If you believe that the answer is yes, provide an output of 'True'.If not, provide an output of 'False'.2. Please do not include additional information or explanations in your response.</p>
<p>Sentence: <question sentence> Output:</p>
<p>The prompt given to LLMs of Agent a to generate a single-fact answer for the give question is as follows:</p>
<p>You are an assistant to answer a question.Please adhere to the following guidelines:</p>
<p>1.The answer to the question is a single entity.2.You should just output the full expression of the answer without any punctuation.3. Do not output any other description.</p>
<p>Sentence: <question sentence> Output:</p>
<p>Figure 1 :
1
Figure 1: A system with multiple roles who focus on sub-problems of each phase to solve a complex task.</p>
<p>Figure 2 :
2
Figure 2: Our Triad framework leverages an LLM-based agent with three different roles including a generalist, a decision-maker, and an advisor to cooperatively handle a series of subtasks in the four phases of a KBQA process.</p>
<p>-based and LLM-based KBQA Traditional KBQA methods transform natural language queries into SPARQL requests for data extraction.Specific models are employed either for question understanding or URI linking, utilizing domain-based training datasets.Hu et al. (2018) introduces a semantic query graph to structurally represent the natural language query, thereby simplifying the task into a subgraph matching problem.Hu et al. (2021) proposes an entity description graph to represent natural language queries for question parsing and element linking.Omar et al. (2023a) restructures the question parsing task as a text generation issue using a sequence-to-sequence model.With the advent of LLMs, certain phases of KBQA can be enhanced with LLM-integrated methods.Baek et al. (2023) aims to augment LLMbased QA tasks with pertinent facts extracted from knowledge bases, offering a fully zero-shot architecture.Tan et al. (2023a) leverages the general applicability of LLMs to filter linking candidates by making selections via few-shot in-context learning.Omar et al. (</p>
<p>Figure 3 :
3
Figure 3: Response time of traditional KBQA systems and Triad on three datasets.Each bar shows average response time of a particular phase of KBQA.</p>
<ol>
<li>Possible question types include: <count>, <select>, and <yes or no>.Question: <question sentence> Output: D Prompts Provided to LLMs of D-Agent for Solving Selection Subtasks in KBQA</li>
</ol>
<p>Table 1 :
1
Triad requires four hyperparameters: the number of examples G-Agent uses for subtask The statistics of KBQA benchmarks, including the number of questions number, the number of triples, the size of index in Virtuoso and Elasticsearch.
Benchmarks#QuestionsBenchmark Statistics KB #Triples Virtuoso Size ES sizeLC-QuAD 1.01000DBpedia-04397M35.40G1.56GQALD-9150DBpedia-10374M36.89G1.57GYAGO-QA100YAGO-4207M24.85G0.54GTypeFrameworksPLC-QuAD 1.0 R F1PQALD-9 RF1PYAGO-QA R F1full-shot gAnswer---0.293 0.327 0.298 0.585 0.341 0.430EDGQA0.505 0.560 0.531 0.313 0.403 0.320 0.419 0.408 0.414KGQAN0.587 0.461 0.516 0.511 0.387 0.441 0.485 0.652 0.556few-shot GPT-3.50.269 0.251 0.266 0.240 0.217 0.228 0.171 0.142 0.155GPT-40.336 0.344 0.340 0.250 0.249 0.249 0.193 0.190 0.191Triad-GPT3.5 0.490 0.519 0.504 0.293 0.302 0.297 0.660 0.639 0.649Triad-GPT4 0.561 0.568 0.564 0.408 0.425 0.416 0.690 0.664 0.677</p>
<p>Table 2 :
2
The performance of our proposed Triad on three benchmarks, comparing with traditional KBQA systems (full-shot) and pure LLM (few-shot) baselines.The optimal and suboptimal scores are highlighted with bold and underlined text, respectively.</p>
<p>Table 3 :
3
Study on the roles of LLM-based by eliminating an element or downgrading the capability.
-QuAD 1.0 QALD-90.3430.1590.4430.2480.5640.416D-uri D-query LC-QuAD 1.0 QALD-90.2740.2100.4310.3010.5640.416A-llmA-factLC-QuAD 1.0 QALD-90.4590.3820.4730.3850.5640.416</p>
<p>Table 4
4presents the F1 resultsof Triad's performance, employing three hyperpa-rameters on two benchmarks. We discover that:Quality is more important than quantity. Moreexamples provided to G-Agent do not always im-prove the performance. The efficacy of G-Agent issignificantly influenced by the quality of examples.More options may harm the result. Choosingmore candidate URIs for entities and relationscould potentially disrupt subsequent query phases,thus affecting overall performance.</p>
<p>Table 4 :
4
Performance evaluation on three hyperparameters that related to each role of an LLM-based agent.</p>
<p>Table 5 :
5
The major reasons of complexity that result in failures, with their corresponding ratios of occurrence in failed cases.</p>
<p>templates such as the example: Which frequent flyer program has the most airlines?</p>
<p>Table 6 :
6
The prompt given to LLMs of Agent g to perform triplet extraction from the question Q is as follows: Performance of roles of LLM-based agent by eliminating an element or downgrading the capability.
G-task G-chat YAGO-QA0.4270.5530.677D-uri D-query YAGO-QA0.3460.5340.677A-llmA-factYAGO-QA0.6260.6470.677You are an assistant to identify triples within aprovided sentence. Please adhere to the followingguidelines:1. Triples should be structured in the format<entity1, relation, entity2>.2. The sentence must contain at least one triple, soyou should provide at least one.3. Entities should represent the smallest semanticunits and should not contain descriptive details.4. Entities can take the form of explicit or implicitreferences. Explicit entities refer to specific namedresources, whereas implicit entities are less certain.5. When an entity is implicit, utilize a variableformat such as '?variable' to denote it, for example,'?location' or '?person'.Here are some examples:Which city's founder is John Forbes? : &lt;?city,foundeer, John Forbes&gt;How many races have the horses bred by JacquesVan't Hart participated in? : &lt;?horse, participatedin, ?race&gt; &lt;?horse, breeder, Jacques Van't Hart&gt;Is camel of the chordate phylum? : <camel,phylum, chordate>Sentence: <Question Sentence>Output:</p>
<p>URIs are listed one by one.4. Your output should consist of a maximum of <K> possible relation URIs, although you may also output fewer if appropriate.5. Ensure that your output is organized, prioritizing the most likely relationship first.6. Provide a list of no more than <K> relation URIs (each on a separate line if there are multiple) without any additional descriptions.
Sentence: <question sentence>Entities: <entity pair>Possible relation URIs: <URI list>Output:
https://platform.openai.com/docs/models
AcknowledgementsThis work is supported by the "Pioneer" and "Leading Goose" R&amp;D Program of Zhejiang (Grant No. 2023C01152).
Dbpedia: A nucleus for a web of open data. Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, 10.1007/978-3-540-76298-0_52The Semantic Web. Berlin, Heidelberg; Berlin HeidelbergSpringer2007</p>
<p>Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. Jinheon Baek, Alhamfikri Aji, Amir Saffari, 10.48550/arXiv.2306.04136arXiv:2306.041362023arXiv preprint</p>
<p>Qingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao, Zhifang Sui, Furu Wei, arXiv:2309.05689Large language model for science: A study on p vs. np. 2023arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, 10.48550/arXiv.2306.03901arXiv:2306.03901Chatdb: Augmenting llms with databases as their symbolic memory. 2023aarXiv preprint</p>
<p>An empirical study of pre-trained language models in simple knowledge graph question answering. Nan Hu, Yike Wu, Guilin Qi, Dehai Min, Jiaoyan Chen, Jeff Z Pan, Zafar Ali, 10.1007/s11280-023-01166-y2023bWorld Wide Web</p>
<p>Answering natural language questions by subgraph matching over knowledge graphs. Sen Hu, Lei Zou, Jeffrey Xu Yu, Haixun Wang, Dongyan Zhao, 10.1109/tkde.2017.2766634IEEE Transactions on Knowledge and Data Engineering. 2018</p>
<p>Edg-based question decomposition for complex question answering over knowledge bases. Xixin Hu, Yiheng Shu, Xiang Huang, Yuzhong Qu, 10.1007/978-3-030-88361-4_8The Semantic Web -ISWC 2021: 20th International Semantic Web Conference, ISWC 2021, Virtual Event. Berlin, HeidelbergSpringer-Verlag2021. October 24-28, 2021</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 10.48550/arXiv.2205.1191Advances in neural information processing systems. 202235</p>
<p>Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, Yongbin Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Advances in Neural Information Processing Systems. 202436</p>
<p>Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, 10.48550/arXiv.2308.05960arXiv:2308.05960Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. 2023arXiv preprint</p>
<p>A universal question-answering platform for knowledge graphs. Reham Omar, Ishika Dhall, Panos Kalnis, Essam Mansour, 10.1145/3588911Proceedings of the ACM on Management of Data. 112023a</p>
<p>Chatgpt versus traditional question answering for knowledge graphs: Current status and future directions towards knowledge graph chatbots. Reham Omar, Omij Mangukiya, Panos Kalnis, Essam Mansour, 10.48550/arXiv.2302.06466arXiv:2302.064662023barXiv preprint</p>
<p>Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.48550/arXiv.2303.09014arXiv:2303.09014Automatic multi-step reasoning and tool-use for large language models. ArtMar-coTulio Ribeiro. 2023arXiv preprint</p>
<p>Yago 4: A reason-able knowledge base. Thomas Pellissier Tanon, Gerhard Weikum, Fabian Suchanek, 10.1007/978-3-030-49461-2_34The Semantic Web. ChamSpringer International Publishing2020</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202436</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 202436</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, 10.48550/arXiv.2303.11366arXiv:2303.113662023arXiv preprint</p>
<p>Make a choice! knowledge base question answering with in-context learning. Chuanyuan Tan, Yuehe Chen, Wenbiao Shao, Wenliang Chen, Zhefeng Wang, Baoxing Huai, Min Zhang, 10.48550/arXiv.2305.13972arXiv:2305.139722023aarXiv preprint</p>
<p>Can chatgpt replace traditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi, International Semantic Web Conference. Springer2023b</p>
<p>Evaluation of chatgpt as a question answering system for answering complex questions. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi, 10.48550/arXiv.2303.0799arXiv:2303.079922023carXiv preprint</p>
<p>Daniel Tang, Zhenghan Chen, Kisub Kim, Yewei Song, Haoye Tian, Saad Ezzini, Yongfeng Huang, Jacques Klein Tegawende F Bissyande, arXiv:2402.02172Collaborative agents for software engineering. 2024arXiv preprint</p>
<p>Lc-quad: A corpus for complex question answering over knowledge graphs. Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, Jens Lehmann, 10.1007/978-3-319-68204-4_22The Semantic Web -ISWC 2017. ChamSpringer International Publishing2017</p>
<p>Ricardo Usbeck, Ria Hari Gusmita, Axel-Cyrille Ngonga Ngomo, Muhammad Saleem, Semdeep/NLIWoD@ISWC. 20189th challenge on question answering over linked data (qald-9. invited paper</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 10.48550/arXiv.2308.11432arXiv:2308.11432A survey on large language model based autonomous agents. 2023arXiv preprint</p>
<p>Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, Chongjie Zhang, arXiv:2010.01523Rode: Learning roles to decompose multi-agent tasks. 2020arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, 10.48550/arXiv.2210.036292023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8216 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8216</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8216</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-265506611</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.18232v1.pdf" target="_blank">LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods. Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8216.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8216.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMRL-Gym GPT2 agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMRL-Gym GPT2-based RL agents (BC / MC / ILQL / PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's primary experimental agents are decoder-only transformer policies (GPT2 variants) finetuned with behavioral cloning and RL algorithms (MC returns, ILQL, PPO); the agents treat the dialogue/turn history (concatenated tokens) as the MDP state and operate on that token-level history.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT2-based RL agents (BC / MC / ILQL / PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Decoder-only transformer policies initialized from pretrained GPT2 and finetuned with supervised (BC) and RL objectives (MC returns, ILQL, PPO). Used as both policy and (for value-based) value network in experiments across all LMRL-Gym tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (decoder-only transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Decoder-only transformer family (GPT-2); experiments use variant sizes up to ~1.5B parameters for accessibility; standard pretrained checkpoints are used as initialization before BC/RL finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LMRL-Gym (8 tasks: Maze, Text-Nav, Wordle, Chess, Endgames, 20Qs, Guess My City, Car Dealer)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A suite of 8 synthetic multi-turn language interaction tasks (text games and interactive dialogues) designed to test strategic decision-making, partial observability, credit assignment, and trajectory stitching for LLM-based RL agents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit context-window / token-history (working / episodic within context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>The full preceding token sequence (conversation / action history) is concatenated to form the state; the transformer attends over this concatenated history (i.e., standard transformer self-attention over the context acts as the memory access mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memory is integrated by concatenating the entire prior token history (both agent and environment tokens) to the model input; the model's attention mechanism retrieves information as needed from that input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>The paper does not evaluate explicit external memory modules vs. no-memory; it uses the concatenated token history as the canonical state representation for all agents. Comparisons are instead made between fully observed (FO) and partially observed (PO) task variants (see task-specific entries) and between RL methods (BC, filtered BC, MC, ILQL, PPO).</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>none identified—paper uses the default transformer context/history concatenation as the de-facto memory and does not propose or identify a separate memory module as superior.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit external-memory module evaluated; transformer context-window limits and partial observability remain challenges. The paper notes that partially observed variants reduce performance and that TD-based methods (ILQL) can struggle in complex-language settings, indicating limitations in how current implicit history use handles long-horizon credit assignment and information gathering.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The paper recommends developing RL algorithms and methods that better handle partial observability and long-term credit assignment when training LMs (e.g., improvements in TD-learning for language, methods that can better utilize or extend memory beyond token-context limits), but does not prescribe an explicit memory architecture; it emphasizes that treating the full token history as the state is the baseline and that further work is needed to improve memory handling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8216.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8216.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Maze FO vs PO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maze task — Fully Observed (FO) versus Partially Observed (PO) comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Maze text task has two variants: FO includes explicit coordinates in each state (making it fully observed), while PO omits coordinates and provides only the history of actions, forcing the agent to infer state from history—this contrast is used to probe partial-observability/memory needs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT2-based RL agents (BC / MC / ILQL / PPO) on Maze</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents finetuned on the Maze dataset; trained and evaluated in both FO and PO variants to measure ability to use history and perform credit assignment / trajectory stitching.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (decoder-only transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Pretrained GPT2 checkpoints finetuned with BC and RL objectives; same architecture used as policy and (where applicable) value head.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Maze (text-based)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A text-based maze-solving task designed to require trajectory stitching and credit assignment; FO variant includes coordinates (explicit state); PO variant omits coordinates so agent must infer location from action history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit token-history (working memory / episodic inference)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>State is either explicit (coordinates in FO) or the concatenated action/history tokens (PO); in PO the concatenated history is the only source of state information and is processed by the transformer's attention.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>In FO the state contains coordinates so less reliance on long history; in PO the history is concatenated to input and the transformer must attend to that history to infer position.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparison between FO and PO versions: the paper reports a drop in normalized performance for every algorithm from FO to PO, demonstrating partial observability makes the task harder and increases demand on memory/history usage. Exact numeric per-algorithm FO vs PO performance differences are not listed in-line (paper reports qualitative drop and includes normalized scores in appendix tables).</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not determined; FO (explicit state) yields better performance than PO, indicating that providing explicit state information helps more than relying solely on implicit history, but no external memory module was proposed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>PO variant exposes challenges: decreased performance across all algorithms, indicating the current transformer/context approach struggles with inferring hidden state solely from history in these settings; PPO training showed instabilities on maze tasks (sometimes collapsing).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Paper concludes partial observability is a meaningful challenge for LM-based RL and recommends further research into algorithms that can better handle partial observability and information-seeking (i.e., better memory/use of history or external memory mechanisms), though no specific mechanism is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8216.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8216.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-Nav FO vs PO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-Nav task — Fully Observed (FO) versus Partially Observed (PO) comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-based navigation (Text-Nav) also has FO and PO variants: FO includes explicit location information, while PO only provides the action/history text; the text variant adds complex natural language descriptions, testing the model's ability to use history as memory and to extract relevant information from language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT2-based RL agents (BC / MC / ILQL / PPO) on Text-Nav</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents finetuned on text-based navigation datasets, evaluated on FO and PO variants to measure impacts of partial observability and complex language on memory use and decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (decoder-only transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Pretrained GPT2 checkpoint finetuned with BC and RL objectives; used both as policy and value predictor (value head) in PPO/ILQL/MC setups.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-Nav (text-based navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A text-based navigation task (modified TextWorld) where the agent must find and move objects in a house with natural language room descriptions; tests credit assignment, trajectory stitching, and ability to parse complex language under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit token-history / language-context memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory arises from concatenated textual history (observations, prior actions) fed into the transformer which uses self-attention to access past information; FO variant supplements this with explicit location info.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Concatenate the full prior textual context (room descriptions, action history) to the model input; transformer attention retrieves and uses relevant pieces for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct FO vs PO comparison: paper reports a drop in normalized performance from FO to PO for all algorithms, showing partial observability and the need to use history/text-memory harms performance. Among algorithms, PPO matched or outperformed ILQL on FO Text-Nav but PO Text-Nav was harder.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not identified; providing explicit state (FO) helps; no external memory augmentation tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>PO variant performance drop indicates difficulties in extracting necessary state information solely from language history; complex/stochastic text appears to make TD-based methods (ILQL) harder to scale.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Paper emphasizes need for methods that can handle partial observability in natural-language settings and scale TD-learning approaches for complex text; suggests that simply concatenating history is baseline but insufficient for many PO tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8216.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8216.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>20Qs / Guess My City (info gathering)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>20 Questions & Guess My City — interactive dialogue tasks that require memory/history</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Interactive dialogue tasks where the agent must gather information across multiple turns (ask questions, remember answers) to guess an object or city; dataset and simulator are generated by LLMs and agents are trained to ask informative questions and remember prior answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT2-based guesser agents (BC / RL variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that play the guesser role in 20Qs and Guess My City, learning to ask sequential questions and use prior answers to form later queries or final guesses; trained via BC and RL methods on large synthetic conversation datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (finetuned guesser), FLAN-T5-XL (used to create more data), GPT-3.5 (used to generate initial dataset), GPT-4 (prompted for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Primary policy models are GPT-2 variants finetuned as policies; dataset generation used GPT-3.5 and FLAN-T5-XL; GPT-4 used for few-shot prompting baselines in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>20 Questions and Guess My City</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Multi-turn conversational guessing games where an oracle (environment) picks an item/city and the agent must ask yes/no or open questions, accumulate answers across turns (memory) and infer the correct identity within a limited number of turns.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic dialogue history (answers & questions retained in context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Conversation history (questions asked and oracle answers) is appended to the model input across turns; the model attends to this history to infer possibilities and select next questions or guesses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Concatenate dialogue history (and in some dataset generation prompts, prior Q&A) to the agent's input; the agent conditions on that full history when generating the next question or final guess.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No explicit ablation of memory vs no-memory; qualitative claim: tasks test information gathering and partial observability, and RL fine-tuning outperforms BC and GPT-4 prompting on most tasks except 20Qs and Guess My City (where GPT-4 prompting did well, likely because GPT3.5 generated simulator data).</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not specified beyond using concatenated dialogue history; no retrieval/external memory strategies evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit memory module evaluation; reliance on simulator fidelity (GPT-3.5/FLAN-T5-XL generated data) may bias results; no numeric memory-ablations to show how forgetting or truncated context affects performance.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Highlights the importance of information-gathering and remembering prior answers for success; recommends further work on algorithms that can explicitly plan information-seeking and manage partial observability across turns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8216.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8216.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wordle (feedback accumulation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wordle — letter-feedback accumulation as a memory-dependent task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Wordle is used as a unit-test for information-seeking and partial observability where the agent must accumulate letter-position feedback over up to 6 guesses; the paper represents words as space-separated letters so LMs treat letters as tokens, and the agent must use prior feedback (memory) to narrow possibilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT2-based Wordle agents (BC / MC / ILQL / PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that generate letter-sequence guesses and incorporate the feedback (letter correctness/position info) provided by the environment across turns to converge on the hidden word within limited attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (decoder-only transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-2 variants finetuned on Wordle-style trajectories; words are represented as space-separated letters so tokens align to letters.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Wordle (text game)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A partially observed 5-letter guessing game: after each guess, the agent receives per-letter feedback (correct+position, present+wrong position, absent) and must use this accumulated information (memory) over multiple turns to find the target word within 6 guesses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term episodic feedback history (turn-level feedback accumulation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>The sequence of prior guesses and their feedback is concatenated to the model input; transformer attention over this sequence provides retrieval of historical constraints (letters/positions) when proposing next guesses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Concatenate past guesses + feedback tokens to input; the LM conditions on these tokens to generate the next guess.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No explicit ablation of memory vs truncated history. The paper reports algorithmic comparisons (ILQL often outperforms MC on Wordle), indicating some methods better leverage the feedback-history signal, but no direct memory-strategy comparisons are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not specified beyond using full concatenated feedback history; ILQL performed well empirically on Wordle (better than MC) suggesting TD-based value methods can help in this feedback-accumulation task, but this is an algorithmic, not memory-architecture, conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit analysis of context-window length or tokenization effects on long feedback sequences; representing letters as tokens mitigates tokenizer issues but external memory/constraint-tracking modules were not explored.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Wordle highlights the importance of correctly retaining and using turn-level feedback; the paper suggests certain offline RL algorithms (ILQL) can better utilize such feedback-history in some text-game tasks, but recommends more work on scaling TD-learning and on memory-aware methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Interactive fiction games: A colossal adventure <em>(Rating: 2)</em></li>
                <li>Offline RL for natural language generation with implicit Q-learning <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Webshop: Towards scalable real-world web-interaction with grounded language agents <em>(Rating: 1)</em></li>
                <li>Human-level play in the game of diplomacy by combining language models with strategic reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8216",
    "paper_id": "paper-265506611",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "LMRL-Gym GPT2 agents",
            "name_full": "LMRL-Gym GPT2-based RL agents (BC / MC / ILQL / PPO)",
            "brief_description": "The paper's primary experimental agents are decoder-only transformer policies (GPT2 variants) finetuned with behavioral cloning and RL algorithms (MC returns, ILQL, PPO); the agents treat the dialogue/turn history (concatenated tokens) as the MDP state and operate on that token-level history.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT2-based RL agents (BC / MC / ILQL / PPO)",
            "agent_description": "Decoder-only transformer policies initialized from pretrained GPT2 and finetuned with supervised (BC) and RL objectives (MC returns, ILQL, PPO). Used as both policy and (for value-based) value network in experiments across all LMRL-Gym tasks.",
            "llm_model_name": "GPT-2 (decoder-only transformer)",
            "llm_model_description": "Decoder-only transformer family (GPT-2); experiments use variant sizes up to ~1.5B parameters for accessibility; standard pretrained checkpoints are used as initialization before BC/RL finetuning.",
            "benchmark_name": "LMRL-Gym (8 tasks: Maze, Text-Nav, Wordle, Chess, Endgames, 20Qs, Guess My City, Car Dealer)",
            "benchmark_description": "A suite of 8 synthetic multi-turn language interaction tasks (text games and interactive dialogues) designed to test strategic decision-making, partial observability, credit assignment, and trajectory stitching for LLM-based RL agents.",
            "memory_used": true,
            "memory_type": "implicit context-window / token-history (working / episodic within context window)",
            "memory_architecture": "The full preceding token sequence (conversation / action history) is concatenated to form the state; the transformer attends over this concatenated history (i.e., standard transformer self-attention over the context acts as the memory access mechanism).",
            "memory_integration_strategy": "Memory is integrated by concatenating the entire prior token history (both agent and environment tokens) to the model input; the model's attention mechanism retrieves information as needed from that input.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "The paper does not evaluate explicit external memory modules vs. no-memory; it uses the concatenated token history as the canonical state representation for all agents. Comparisons are instead made between fully observed (FO) and partially observed (PO) task variants (see task-specific entries) and between RL methods (BC, filtered BC, MC, ILQL, PPO).",
            "best_memory_strategy": "none identified—paper uses the default transformer context/history concatenation as the de-facto memory and does not propose or identify a separate memory module as superior.",
            "limitations_or_failure_cases": "No explicit external-memory module evaluated; transformer context-window limits and partial observability remain challenges. The paper notes that partially observed variants reduce performance and that TD-based methods (ILQL) can struggle in complex-language settings, indicating limitations in how current implicit history use handles long-horizon credit assignment and information gathering.",
            "recommendations_or_conclusions": "The paper recommends developing RL algorithms and methods that better handle partial observability and long-term credit assignment when training LMs (e.g., improvements in TD-learning for language, methods that can better utilize or extend memory beyond token-context limits), but does not prescribe an explicit memory architecture; it emphasizes that treating the full token history as the state is the baseline and that further work is needed to improve memory handling.",
            "uuid": "e8216.0",
            "source_info": {
                "paper_title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Maze FO vs PO",
            "name_full": "Maze task — Fully Observed (FO) versus Partially Observed (PO) comparison",
            "brief_description": "The Maze text task has two variants: FO includes explicit coordinates in each state (making it fully observed), while PO omits coordinates and provides only the history of actions, forcing the agent to infer state from history—this contrast is used to probe partial-observability/memory needs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT2-based RL agents (BC / MC / ILQL / PPO) on Maze",
            "agent_description": "Agents finetuned on the Maze dataset; trained and evaluated in both FO and PO variants to measure ability to use history and perform credit assignment / trajectory stitching.",
            "llm_model_name": "GPT-2 (decoder-only transformer)",
            "llm_model_description": "Pretrained GPT2 checkpoints finetuned with BC and RL objectives; same architecture used as policy and (where applicable) value head.",
            "benchmark_name": "Maze (text-based)",
            "benchmark_description": "A text-based maze-solving task designed to require trajectory stitching and credit assignment; FO variant includes coordinates (explicit state); PO variant omits coordinates so agent must infer location from action history.",
            "memory_used": true,
            "memory_type": "implicit token-history (working memory / episodic inference)",
            "memory_architecture": "State is either explicit (coordinates in FO) or the concatenated action/history tokens (PO); in PO the concatenated history is the only source of state information and is processed by the transformer's attention.",
            "memory_integration_strategy": "In FO the state contains coordinates so less reliance on long history; in PO the history is concatenated to input and the transformer must attend to that history to infer position.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct comparison between FO and PO versions: the paper reports a drop in normalized performance for every algorithm from FO to PO, demonstrating partial observability makes the task harder and increases demand on memory/history usage. Exact numeric per-algorithm FO vs PO performance differences are not listed in-line (paper reports qualitative drop and includes normalized scores in appendix tables).",
            "best_memory_strategy": "Not determined; FO (explicit state) yields better performance than PO, indicating that providing explicit state information helps more than relying solely on implicit history, but no external memory module was proposed.",
            "limitations_or_failure_cases": "PO variant exposes challenges: decreased performance across all algorithms, indicating the current transformer/context approach struggles with inferring hidden state solely from history in these settings; PPO training showed instabilities on maze tasks (sometimes collapsing).",
            "recommendations_or_conclusions": "Paper concludes partial observability is a meaningful challenge for LM-based RL and recommends further research into algorithms that can better handle partial observability and information-seeking (i.e., better memory/use of history or external memory mechanisms), though no specific mechanism is recommended.",
            "uuid": "e8216.1",
            "source_info": {
                "paper_title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Text-Nav FO vs PO",
            "name_full": "Text-Nav task — Fully Observed (FO) versus Partially Observed (PO) comparison",
            "brief_description": "Text-based navigation (Text-Nav) also has FO and PO variants: FO includes explicit location information, while PO only provides the action/history text; the text variant adds complex natural language descriptions, testing the model's ability to use history as memory and to extract relevant information from language.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT2-based RL agents (BC / MC / ILQL / PPO) on Text-Nav",
            "agent_description": "Agents finetuned on text-based navigation datasets, evaluated on FO and PO variants to measure impacts of partial observability and complex language on memory use and decision making.",
            "llm_model_name": "GPT-2 (decoder-only transformer)",
            "llm_model_description": "Pretrained GPT2 checkpoint finetuned with BC and RL objectives; used both as policy and value predictor (value head) in PPO/ILQL/MC setups.",
            "benchmark_name": "Text-Nav (text-based navigation)",
            "benchmark_description": "A text-based navigation task (modified TextWorld) where the agent must find and move objects in a house with natural language room descriptions; tests credit assignment, trajectory stitching, and ability to parse complex language under partial observability.",
            "memory_used": true,
            "memory_type": "implicit token-history / language-context memory",
            "memory_architecture": "Memory arises from concatenated textual history (observations, prior actions) fed into the transformer which uses self-attention to access past information; FO variant supplements this with explicit location info.",
            "memory_integration_strategy": "Concatenate the full prior textual context (room descriptions, action history) to the model input; transformer attention retrieves and uses relevant pieces for planning.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct FO vs PO comparison: paper reports a drop in normalized performance from FO to PO for all algorithms, showing partial observability and the need to use history/text-memory harms performance. Among algorithms, PPO matched or outperformed ILQL on FO Text-Nav but PO Text-Nav was harder.",
            "best_memory_strategy": "Not identified; providing explicit state (FO) helps; no external memory augmentation tested.",
            "limitations_or_failure_cases": "PO variant performance drop indicates difficulties in extracting necessary state information solely from language history; complex/stochastic text appears to make TD-based methods (ILQL) harder to scale.",
            "recommendations_or_conclusions": "Paper emphasizes need for methods that can handle partial observability in natural-language settings and scale TD-learning approaches for complex text; suggests that simply concatenating history is baseline but insufficient for many PO tasks.",
            "uuid": "e8216.2",
            "source_info": {
                "paper_title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "20Qs / Guess My City (info gathering)",
            "name_full": "20 Questions & Guess My City — interactive dialogue tasks that require memory/history",
            "brief_description": "Interactive dialogue tasks where the agent must gather information across multiple turns (ask questions, remember answers) to guess an object or city; dataset and simulator are generated by LLMs and agents are trained to ask informative questions and remember prior answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT2-based guesser agents (BC / RL variants)",
            "agent_description": "Agents that play the guesser role in 20Qs and Guess My City, learning to ask sequential questions and use prior answers to form later queries or final guesses; trained via BC and RL methods on large synthetic conversation datasets.",
            "llm_model_name": "GPT-2 (finetuned guesser), FLAN-T5-XL (used to create more data), GPT-3.5 (used to generate initial dataset), GPT-4 (prompted for comparison)",
            "llm_model_description": "Primary policy models are GPT-2 variants finetuned as policies; dataset generation used GPT-3.5 and FLAN-T5-XL; GPT-4 used for few-shot prompting baselines in evaluation.",
            "benchmark_name": "20 Questions and Guess My City",
            "benchmark_description": "Multi-turn conversational guessing games where an oracle (environment) picks an item/city and the agent must ask yes/no or open questions, accumulate answers across turns (memory) and infer the correct identity within a limited number of turns.",
            "memory_used": true,
            "memory_type": "episodic dialogue history (answers & questions retained in context)",
            "memory_architecture": "Conversation history (questions asked and oracle answers) is appended to the model input across turns; the model attends to this history to infer possibilities and select next questions or guesses.",
            "memory_integration_strategy": "Concatenate dialogue history (and in some dataset generation prompts, prior Q&A) to the agent's input; the agent conditions on that full history when generating the next question or final guess.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No explicit ablation of memory vs no-memory; qualitative claim: tasks test information gathering and partial observability, and RL fine-tuning outperforms BC and GPT-4 prompting on most tasks except 20Qs and Guess My City (where GPT-4 prompting did well, likely because GPT3.5 generated simulator data).",
            "best_memory_strategy": "Not specified beyond using concatenated dialogue history; no retrieval/external memory strategies evaluated.",
            "limitations_or_failure_cases": "No explicit memory module evaluation; reliance on simulator fidelity (GPT-3.5/FLAN-T5-XL generated data) may bias results; no numeric memory-ablations to show how forgetting or truncated context affects performance.",
            "recommendations_or_conclusions": "Highlights the importance of information-gathering and remembering prior answers for success; recommends further work on algorithms that can explicitly plan information-seeking and manage partial observability across turns.",
            "uuid": "e8216.3",
            "source_info": {
                "paper_title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Wordle (feedback accumulation)",
            "name_full": "Wordle — letter-feedback accumulation as a memory-dependent task",
            "brief_description": "Wordle is used as a unit-test for information-seeking and partial observability where the agent must accumulate letter-position feedback over up to 6 guesses; the paper represents words as space-separated letters so LMs treat letters as tokens, and the agent must use prior feedback (memory) to narrow possibilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT2-based Wordle agents (BC / MC / ILQL / PPO)",
            "agent_description": "Agents that generate letter-sequence guesses and incorporate the feedback (letter correctness/position info) provided by the environment across turns to converge on the hidden word within limited attempts.",
            "llm_model_name": "GPT-2 (decoder-only transformer)",
            "llm_model_description": "GPT-2 variants finetuned on Wordle-style trajectories; words are represented as space-separated letters so tokens align to letters.",
            "benchmark_name": "Wordle (text game)",
            "benchmark_description": "A partially observed 5-letter guessing game: after each guess, the agent receives per-letter feedback (correct+position, present+wrong position, absent) and must use this accumulated information (memory) over multiple turns to find the target word within 6 guesses.",
            "memory_used": true,
            "memory_type": "short-term episodic feedback history (turn-level feedback accumulation)",
            "memory_architecture": "The sequence of prior guesses and their feedback is concatenated to the model input; transformer attention over this sequence provides retrieval of historical constraints (letters/positions) when proposing next guesses.",
            "memory_integration_strategy": "Concatenate past guesses + feedback tokens to input; the LM conditions on these tokens to generate the next guess.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No explicit ablation of memory vs truncated history. The paper reports algorithmic comparisons (ILQL often outperforms MC on Wordle), indicating some methods better leverage the feedback-history signal, but no direct memory-strategy comparisons are provided.",
            "best_memory_strategy": "Not specified beyond using full concatenated feedback history; ILQL performed well empirically on Wordle (better than MC) suggesting TD-based value methods can help in this feedback-accumulation task, but this is an algorithmic, not memory-architecture, conclusion.",
            "limitations_or_failure_cases": "No explicit analysis of context-window length or tokenization effects on long feedback sequences; representing letters as tokens mitigates tokenizer issues but external memory/constraint-tracking modules were not explored.",
            "recommendations_or_conclusions": "Wordle highlights the importance of correctly retaining and using turn-level feedback; the paper suggests certain offline RL algorithms (ILQL) can better utilize such feedback-history in some text-game tasks, but recommends more work on scaling TD-learning and on memory-aware methods.",
            "uuid": "e8216.4",
            "source_info": {
                "paper_title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Interactive fiction games: A colossal adventure",
            "rating": 2,
            "sanitized_title": "interactive_fiction_games_a_colossal_adventure"
        },
        {
            "paper_title": "Offline RL for natural language generation with implicit Q-learning",
            "rating": 2,
            "sanitized_title": "offline_rl_for_natural_language_generation_with_implicit_qlearning"
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "language_understanding_for_textbased_games_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Webshop: Towards scalable real-world web-interaction with grounded language agents",
            "rating": 1,
            "sanitized_title": "webshop_towards_scalable_realworld_webinteraction_with_grounded_language_agents"
        },
        {
            "paper_title": "Human-level play in the game of diplomacy by combining language models with strategic reasoning",
            "rating": 1,
            "sanitized_title": "humanlevel_play_in_the_game_of_diplomacy_by_combining_language_models_with_strategic_reasoning"
        }
    ],
    "cost": 0.0166365,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models
November 2023</p>
<p>Marwa Abdulhai marwa_abdulhai@berkeley.edu 
Isadora White 
Charlie Snell 
Charles Sun 
Joey Hong 
Yuexiang Zhai 
Kelvin Xu 
Sergey Levine 
U C Berkeley 
Google Deepmind 
LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models
November 2023940CD39E0828D3F32332261A6AADD0E2arXiv:2311.18232v1[cs.CL]
Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning.This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns.Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents.This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes.However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs.Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms.Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods.Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.</p>
<p>Introduction</p>
<p>Figure 1: Overview of LMRL-Gym: We show sample trajectories from two tasks in our benchmark.In the Guess My City task, the agent learns to ask questions to guess the city the oracle is from while in the Maze task, the agent learns to make the correct moves based on cues from the oracle.</p>
<p>Large language models (LLMs) have demonstrated remarkable abilities when naturally conversing with humans (OpenAI, 2023;2022;Touvron et al., 2023;Google, 2023), answering questions and responding to requests (Shuster et al., 2022b;a;Qin et al., 2023), and even performing coding tasks (Chen et al., 2021;Wang et al., 2023).Much of these capabilities are enabled by learning to emulate humans from large datasets of text from the web (Völske et al., 2017;Shuster et al., 2022a;Yao et al., 2023), learning from examples "in context" (Brown et al., 2020), as well as other sources of supervision such as instruction datasets (Mishra et al., 2022;Wei et al., 2022;Wang et al., 2022b) and preference finetuning with RLHF (Ziegler et al., 2020;Ouyang et al., 2022).However, directly applying LLMs in settings that require planning or multi-turn interactions reveals certain difficulties.LLMs are not explicitly goal-directed, as they are not optimized to directly solve particular tasks, but rather to produce text that resembles the distribution of human-provided examples or accords with human preferences (Ziegler et al., 2020;Stiennon et al., 2020;Wu et al., 2021;Bai et al., 2022a).This can become particularly apparent in temporally extended tasks, such as multi-turn dialogue (Irvine et al., 2023;, FAIR), complex tool use (Wang et al., 2022a), multi-step games (Hendrycks et al., 2021b), as well as other interactive applications.In principle, LLMs should contain the knowledge necessary to succeed in such settings: if the multi-turn interactions center around problem domains that are well represented in the model's training data (such as dialogue), well-trained LLMs should already serve as powerful predictive models in such settings.However, leveraging this predictive knowledge to derive effective actions and strategies requires not just emulating humans, but also planning and optimization.</p>
<p>Multi-turn reinforcement learning (RL) (Sutton and Barto, 2018) in principle offers a path to enable LLMs to overcome challenges in goal-directed reasoning and planning in interactive, multi-turn settings, including complex dialogue, games, and tool use.We might hypothesize that RL could serve as a powerful tool for LLM training, not just for training models to accord with human preferences, but more generally to accomplish tasks in an intentional and goal-directed manner.Text generation can be viewed as a sequential decision-making process, treating a sequence of tokens as a trajectory.Many tasks, such as successfully answering questions or eliciting a desired reaction from a user, can then be framed as optimizing some reward function over these trajectories.However, despite extensive interest in RL for LLMs in recent years, much (though not all) of the recent research in this area has focused on "single-step" RL problems, where a single response is optimized for some quality metric, typically derived from human preference signals (Stiennon et al., 2020;Ziegler et al., 2020;Ouyang et al., 2022;Bai et al., 2022a;Anthropic, 2023;Ramamurthy et al., 2023;Christiano et al., 2023;Casper et al., 2023).While some works have sought to apply RL for multi-turn tasks (Singh et al., 1999;Li et al., 2016;Shah et al., 2016;Kwan et al., 2022), particularly in the domain of goal-directed dialogue (Lewis et al., 2017;Verma et al., 2022), there has been comparatively little research on actually improving the underlying RL algorithms and very little head-to-head comparison on the same set of tasks.This is perhaps unsurprising: it is likely much easier to evaluate improvements to algorithms for single-turn text generation as compared to multi-turn generation since multi-turn dialogue requires an interactive evaluation procedure, where there is no established protocol and the "gold standard" constitutes costly and time-consuming studies with human participants.</p>
<p>In this work, we aim to address this challenge and make it possible for RL algorithms researchers to iterate on developing better RL methods for multi-turn language-based interaction tasks, such as dialogue and games.We posit that benchmarking RL algorithms for LLMs presents a very different set of challenges and merits a different set of solutions compared to other benchmarks in NLP.While most NLP benchmarks are based on standard supervised machine learning paradigms, with a training set and a test set (Marcus et al., 1993;Tjong Kim Sang and De Meulder, 2003;Socher et al., 2013;Rajpurkar et al., 2016;Wang et al., 2019;Williams et al., 2018), RL benchmarks require simulators that the trained agents can interact with to measure their performance.Until recently, constructing high-fidelity simulators for dialogue with humans has been exceptionally difficult.However, the very same LLMs that we advocate to train with RL can offer a solution here, as they can be trained to emulate human-like dialogue and produce both synthetic datasets and synthetic simulated evaluation protocols, where an RL-trained LLM agent "talks" to another LLM provided as part of the benchmark as a "simulator." While this approach to benchmarking interactive dialogue has significant downsides (e.g., the responses of the benchmark LLM might often deviate drastically from human behavior), we believe that the corresponding upside of enabling accessible benchmarking of multi-turn RL algorithms provides a compelling solution.We emphasize however that our goal is not to utilize this approach to benchmark whether LLMs are good at talking to humans, but rather as a way to test RL algorithms with datasets that are sufficiently difficult and complex so as to gauge how effective they might be if they were then trained on data from real humans.</p>
<p>Our proposed benchmark, LMRL-Gym, consists of 8 tasks that each come with an offline dataset that can be used for offline RL training, and a "simulator" that can be used to evaluate the resulting agents in terms of their performance on multi-turn interactive tasks.This simulator supports both evaluation of agents trained with offline RL, and online training.Three tasks are interactive dialogue tasks designed to simulate real-world interactions with humans requiring information gathering from humans (20 Questions, Guess My City) and negotiation (Car Dealer), while five of these tasks are RL capability tests designed to isolate specific desirable properties of training RL with language.We show sample trajectories for each kind of task in Figure 1.In addition to presenting the benchmark tasks and the synthetic data generation system, we also provide a research framework that provides a toolkit for researchers and practitioners to get started with multi-turn RL for LLMs.This framework includes implementations of PPO (Schulman et al., 2017), ILQL (Snell et al., 2022a), and several baseline methods, implemented in an extensible way designed for future development of tasks, experimentation, and algorithm design.</p>
<p>Related Works</p>
<p>Datasets, benchmarks, and libraries.Benchmarks and datasets have been an important factor for driving progress in NLP in domains that include machine translation (Tiedemann, 2012;Bojar et al., 2016), natural language understanding (Rajpurkar et al., 2016;Wang et al., 2019;Hendrycks et al., 2020;2021a;Ramamurthy et al., 2023), and solving math problems (Cobbe et al., 2021).However, these tasks generally do not involve multi-turn interaction and do not come with rewards, making them hard to adapt to RL research.For example, the standard for evaluating dialogue agents has been to run a human subjects study, but this is time-consuming and costly.Some works have proposed text games for evaluating language-based agents (Hausknecht et al., 2019;Hendrycks et al., 2021b;Wang et al., 2022a;Yao et al., 2023).Our aim is to cover a variety of problem settings that reflect challenges in open-vocabulary interaction in addition to text games.Motivated by successes in using LLMs to generate synthetic data (Hausknecht et al., 2019;Park et al., 2023;Bai et al., 2022b), our proposed tasks are based on synthetic data.While such data may differ from natural text, the scope of our benchmark is specific to evaluating RL algorithms, not the ability to interact with humans.</p>
<p>RL for language models.RL for language models has seen success in aligning LLMs with human preferences (RLHF) (Ziegler et al., 2020;Stiennon et al., 2020;Bai et al., 2022a;b;Ouyang et al., 2022;Christiano et al., 2023), optimizing non-differentiable objectives for machine translation (Wu et al., 2016;Nguyen et al., 2017;Kiegeland and Kreutzer, 2021), generation (Tambwekar et al., 2019;Pang and He, 2021;Pyatkin et al., 2022), dialogue (Cuayáhuitl et al., 2015;Georgila and Traum, 2011;Li et al., 2016), question answering (Pyatkin et al., 2022), and summarization (Paulus et al., 2017;Böhm et al., 2019;Wu and Hu, 2018).These include RL methods that learn by directly interacting with the environment (online RL) (Carta et al., 2023) and RL methods that only use a static dataset (offline RL) (Jaques et al., 2020;Snell et al., 2022a;Jang et al., 2022;Verma et al., 2022;, FAIR).However, many of these works operate in the singe-step bandit setting, and do not consider multi-turn goal-directed tasks.Our benchmark, on the other hand, focuses on tasks involving multiple turns of interaction with clearly defined goal-based reward functions.</p>
<p>Capabilities of LLMs.There has been a surge in the capabilities of LLMs for generation (Ghazvininejad et al., 2017;Radford et al., 2019), dialogue (Lewis et al., 2017;Jaques et al., 2017;Shuster et al., 2022b;Snell et al., 2022b), question answering (Pyatkin et al., 2022), summarization (Paulus et al., 2017;Böhm et al., 2019;Wu and Hu, 2018), text-based games (Narasimhan et al., 2015;Hausknecht et al., 2019), translation (Gu et al., 2017), and more.However, these are often supervised learning tasks that do not test the LLMs' abilities to achieve a specific long-term objective.Research on dialogue generation (Jaques et al., 2017;He et al., 2018;Shuster et al., 2022b;a) has often focused on generating feasible-looking agent dialogue without explicit consideration for some multi-turn objective.Our benchmarks allow for the development of algorithms that enable LLMs to interact with an environment to achieve long-term objectives, by providing tasks with online simulators and offline datasets.</p>
<p>Multi-Turn Generation with RL and Language Models</p>
<p>This section introduces the conceptual foundations of using reinforcement learning for multi-turn generation with language models.We introduce a definition of the Markov Decision Process for language and a framework for the methods we focus on in this paper.</p>
<p>Definitions</p>
<p>We formalize language generation tasks as a partially observable Markov decision process.We can think of the state as the history of tokens and action as the next token generated by the model.An observation is a single token   in the history.The probability of generating the next token given the previous observation token ( +1 |  ) is non-Markovian.However, a Markovian state can be formed by concatenating all of the previous tokens.</p>
<p>A policy  defines the agent's behavior by taking in the current state  given by [ 0 , … ,   ], and outputting a new action token  given by  +1 .The environment assigns a reward (, ) based on the entire sequence of tokens so far.The tokens in the state are either generated by the policy  or the environment.For example, in the Car Dealer task, the policy generates the tokens for the Seller and the environment generates the tokens for the Buyer and the history of their conversation would form the state.A complete sequence of tokens will also be referred to as a trajectory.The goal of RL is to produce a policy  * that maximizes the expected discounted sum of rewards over trajectories () under the policy  * = arg max   ∼ [ ∑  −1 =0     (  ,   ) ] , where  represents a trajectory.</p>
<p>RL Algorithms</p>
<p>Several possible RL algorithms could be used (Jaques et al., 2020;Verma et al., 2022;Snell et al., 2022a;Schulman et al., 2017;Stiennon et al., 2022;Bai et al., 2022a;Casper et al., 2023).Policy gradient methods, such as PPO (Schulman et al., 2017), directly compute the gradient of the language model concerning the expected reward objective.Value-based methods estimate a state-action () or state-value ( ) function, from which they then derive a policy by either acting greedily with respect to the Q-function or by combining the learned -function with the base LM by perturbing the base model's logits with the learned action-value functions (Snell et al., 2022a).</p>
<p>RL methods for training LLMs can be online or offline.Online methods repeatedly interact with the environment, collecting additional data during training.Offline RL instead learns to extract the best behaviors from an existing, potentially suboptimal dataset.Due to the large amount of existing text interactions on the internet, offline RL is an ideal setting for training language models.Therefore, our work primary focuses on benchmarking offline RL algorithms.However our tasks also fully support online RL and we include an online PPO baseline in our evaluation.</p>
<p>The LMRL-Gym: Synthetic Benchmarks for RL with Language</p>
<p>Our benchmark consists of 8 tasks, split into two categories; RL capability tests and interactive dialogue tasks.</p>
<p>The RL capability tests focus on specific desirable capabilities for RL algorithms to have such as strategic decisionmaking, credit assignment, or trajectory stitching.For the interactive dialogue tasks, we model them after real-world interactions with humans, such as persuading someone to buy a car or playing a guessing game like 20 questions.In such multi-turn interactions, there is a need for the agent to make inferences about persuasive strategies and social interaction, actively gather information efficiently through asking questions, and strategically reason in partially observable settings.Below, we discuss the specific capabilities of RL algorithms for LLMs that our benchmark aims to evaluate, we summarize the data generation and simulation process, and we describe the tasks themselves.We provide a concise summary of the dataset and task statistics in Table 1.</p>
<p>Evaluating Capabilities Enabled by RL</p>
<p>A central objective of our benchmark is to evaluate the core capabilities that RL can enable in large language models.Some of these capabilities are computational, and relate to core decision-making irrespective of the considerations of natural language, such as playing chess, while others are semantic.We discuss the particular capabilities of algorithms we aim to evaluate in this section, followed by a discussion of the criteria that such tasks must meet to do so.</p>
<p>Strategic decision making.RL shines in goal-directed tasks that require multi-step planning and strategic decisionmaking.Strategic decision-making can range from simple choices like asking follow-up questions to gather information (e.g., in the 20 Questions task), to complex strategy in chess.</p>
<p>Complex language.Our benchmark includes realistic language and interaction scenarios, requiring LLMs to combine their knowledge from pretraining to help solve tasks during RL finetuning.Rather than focusing entirely on causal logic and strategy found in text games, several of our tasks specifically emphasize the use of realistic language.</p>
<p>Credit assignment.In RL, rewards are often delayed relative to the action that was pivotal to the outcome.For example, a seller agent might state a particularly compelling feature of the product and then, several turns later, complete a successful sale.RL must determine the statements that led to the good outcome, and reinforce them.</p>
<p>Partial observability.In language tasks, the state consists of the entire history of tokens, and an agent may need to examine this entire context to infer the correct state.For example, the mental states of a speaker in a dialogue (e.g., whether the buyer is impatient in a selling task), previously observed facts in a guessing game, and other hidden variables might induce partial observability.</p>
<p>Trajectory stitching.In a dataset with many suboptimal trajectories, it is necessary to join optimal actions from different suboptimal trajectories together to form the most optimal trajectory.An algorithm capable of trajectory stitching should be able to learn from optimal actions taken in unsuccessful trajectories and avoid suboptimal actions that occurred in successful trajectories.</p>
<p>LMRL-Gym Tasks</p>
<p>Figure 2: Each task in the text-based games and RL capabilities test suite is chosen to test some subset of the RL capabilities outlined in Section 4.1.We use a checkmark to highlight if a task satisfies a property, and a cross if it does not satisfy the property.As can be seen in the figure, the RL capabilities test tasks Chess, Endgames, Wordle, Maze, and Text-Nav all test some proper subset of the properties.The interactive dialogue tasks test many of the properties, but since they were designed to reflect multi-turn dialogue rather than test a specific RL capability, they may not test certain capabilities.</p>
<p>We split our tasks into 5 "RL capability tests" and 3 "interactive dialogue" tasks.The RL capability tests are designed to create comparisons between tasks based on specific RL capabilities as described in Section 4. They are created to be text-based versions of tasks where RL without language is known to excel.The interactive dialogue tasks are chosen to be illustrations of real-world interactions where we expect to test multi-turn RL.Example trials for each task are shown in Figure 4, and we show which capability each task satisfies in Figure 2.</p>
<p>The RL Capability Tests</p>
<p>The RL Capability Tests are text-based games designed such that they 1) isolate specific RL capabilities and 2) are language analogs of tasks where RL is known to succeed.To emphasize the comparison to a non-textbased version, we evaluate the Maze task in a symbolic or grid-based environment seen in Appendix G. Below we highlight how each of the tasks isolates the RL Properties.Further details on task design for each of the RL Property test tasks can be found in Appendix A.</p>
<p>Partial Observability.We focus on the effect that partial observability has on performance by including both fully observed (FO) and partially observed (PO) versions of the Maze and Text-Nav tasks.We create a partially observed Maze or Text-Nav by removing information about the location from the state.</p>
<p>Trajectory Stitching.All of the RL Capability Tests test trajectory stitching, because they include suboptimal data.The inclusion of suboptimal requires an offline algorithm to utilize information from suboptimal data to generate optimal trajectories.Further details about our dataset generation strategies can be found in Appendix C.</p>
<p>Credit Assignment.Chess, Endgames, Maze and Text-Nav test credit assignment, because success in the task is dependent on things outside the control of the agent.In Chess and Endgames, victory is highly dependent on the actions of the opponent.If the opponent makes bad moves, victory is far more likely than if the opponent makes excellent moves.Similarly, in the Maze and Text-Nav tasks, the dataset is generated such that trajectories that start close to the goal are far more likely to succeed.Therefore the agent must learn to distinguish between lucky wins and those resulting from complex strategic decisions.</p>
<p>Complex Language We include both Maze and Text-Nav tasks to highlight the differences between a maze task with and without complex or stochastic text.However, the RL Capability tests are not designed primarily with complex language in mind and we leave that problem to the Interactive Dialogue tasks.</p>
<p>Strategic Decision Making Wordle, Chess, and Chess Endgames test strategic decision-making to varying degrees.Wordle tests information gathering and strategy in a partially observed environment, because the full information about the state is not provided to the agent.We find that Chess requires the most strategic decision-making because it requires the agent to plan over a game more than 40 moves in length.</p>
<p>Why include Endgames (Theoretical Chess Endgames)?Chess endgames provide a simpler and more goaldirected variation of the chess task.By focusing on the endgame, we encourage algorithms to learn strategy rather than memorizing the opening moves of a chess game.A classic theoretical endgame position consists of a position where the only pieces on the board are the two kings and the queen.Although the board position appears simple, a sequence of carefully calculated moves is required to win.A simpler board state allows language models to make progress without fewer computational resources.</p>
<p>Interactive Dialogue Tasks</p>
<p>For the interactive dialogue tasks, we chose two tasks that involve rational decision-making (20Qs, Guess) and information gathering and one that involves negotiation (Car Dealer).These tasks aim to simulate real world interactions between humans.</p>
<p>20Qs (Twenty Questions).This task tests information gathering to see if a policy can successfully reason about an unknown subject based on context to determine what it is.Additionally, it also evaluates the ability of the model to understand semantics, as it also needs knowledge about the objects in question.In twenty questions, one player (the oracle) thinks of an object, and the agent (the guesser) tries to guess what it is by asking a series of yes-or-no questions.In this interaction, the oracle serves as the environment, and the agent learning a policy to solve the game is the guesser.Our simulators can be used to generate offline data, to provide a "simulation environment" for evaluation, to perform online training, and to compute rewards.</p>
<p>Guess (Guess My City</p>
<p>).This task simulates a more complicated guessing game, where one player (the oracle) is from a specific city, and the other player (the guesser) tries to guess what city the oracle is from.</p>
<p>Here, the guesser can ask not only yes and no questions, but can also ask open-ended questions.This task tests strategic decision-making and the ability of algorithms to handle complex language, as it allows the agent to go beyond learning to ask yes/no questions and learning to ask questions open-ended questions that provide the agent with more information.</p>
<p>Car Dealer.This task simulates a conversation between a car buyer and a car dealer, each with different strategies for getting the best deal.</p>
<p>The buyer wants to buy a certain type of car within a certain budget, and the car dealer wants to complete the sale ideally with a high sale price.We have designed the task such that there exist three different kinds of sellers and three different buyers, each primed with a different strategy.Hence, agents should learn to make agreements with buyers who are most compatible with their strategy i.e. a seller who likes to give discounts will form agreements with a buyer who likes to receive them.This allows us to test the ability of RL algorithms to learn strategic decision-making and credit assignment, by learning which strategies led to a successful sale of the car.</p>
<p>An Overview of Data Collection for LMRL-Gym Tasks</p>
<p>In order to make the tasks in LMRL-Gym practical for benchmarking RL methods, we must balance accessibility and realism.Real-world datasets would be the most realistic, but since RL algorithms need to be evaluated by actually running the learned policy, real-world tasks are comparatively inaccessible for rapid iteration (e.g., if they require talking to real humans).We therefore use simulators for all of our tasks, which are derived either from text-based games, or conversational agents powered by language models.Although this fully synthetic setup sacrifices some realism, we believe the significant gain in accessibility is worthwhile and will enable more rapid RL algorithm progress.Text-based games.For each of these tasks, we use a simulator such as a chess engine or maze solver to generate near-optimal data and then we dilute the policy with suboptimal data by taking suboptimal actions or using inferior policies.We also convert our task from a symbolic version to a text-based version in a programmatic way as discussed in Appendix A.</p>
<p>Conversational tasks.For conversational tasks, we leverage existing LLMs to generate our data, either with two instances of LLMs "talking" to one another or all at once through few-shot prompting.In order to train these LLMs, we use OpenAI's GPT-3.5 to generate an initial dataset by asking reasonable questions and answers out-of-the-box, collecting a dataset of differing sizes depending on the task.In the case of 20Qs and Guess My City, we collected 1K conversations by querying GPT-3.5 (text-davinci-003) to generate both sides of the conversation.To generate the dataset for training our algorithms, we fine-tuned a GPT2-medium guesser model and a GPT2-medium oracle model on their respective sides of the conversation.Using these models, we generated the final 100K conversations dataset by having the two models talk to each other.We conducted a similar process for the Car Dealer task.We show our data generation process in Figure 3.In Table 1 we present the dataset statistics on these tasks.The number of trajectories and the average length of the trajectories varies based on the complexity of the tasks.1: Statistics for all tasks in LMRL-Gym.Size represents the number of trajectories, the average length is the average length of trajectories in the dataset where the unit is a response from the agent.The success rate is the proportion of trajectories that reach the objective.Finally, the reward functions for each task are defined in Appendix C.</p>
<p>LMRL Research Framework for Algorithm Development</p>
<p>To validate the efficacy of LMRL-Gym for benchmarking RL algorithms that meet the expected capabilities defined in Section 4.1, we evaluate our tasks on a set of both online and offline RL algorithms.With these experiments, we expect to 1) observe a significant spread in performance between the different algorithms, validating the utility of our tasks for measuring differences between RL algorithms; and 2) we should ideally observe room to improve beyond what our current algorithms achieve on these tasks, such that our benchmark can enable future algorithmic development.</p>
<p>Our project page (https://lmrl-gym.github.io/)contains links to our open-sourced datasets and research framework (https://github.com/abdulhaim/LMRL-Gym).</p>
<p>We evaluate two standard language model finetuning baselines (BC &amp; filtered BC), two baseline value-based offline RL methods (MC &amp; ILQL (Snell et al., 2022a)), standard online RL methods (PPO (Schulman et al., 2017), Online Filtered BC), as well as evaluation done by GPT4.For all of our experiments, we use the decoder-only transformer model GPT2 (Radford et al., 2019) (and its variant sizes) as the policy and value network.Finally, we conduct a human evaluation to test the naturalness of the conversations generated by our simulators.We describe our protocol and display results in Appendix J.</p>
<p>BC, Filtered BC, Online Filtered BC In line with standard RL nomenclature, we denote supervised fine-tuning as behavioral cloning (BC).This baseline tests whether LMs can effectively represent the behaviors in the datasets.We also evaluate filtered BC, which is identical, except we use only the most successful examples in the dataset for fine-tuning.Online filtered BC collects data using the current policy and selects the most successful trajectories for finetuning.See Appendix D for our data filtering criteria for each task.</p>
<p>Offline Value-based RL: MC Returns and ILQL Monte-Carlo returns (Nakamoto et al., 2023) and Implicit Language Q-Learning (Snell et al., 2022a) train a value  and  function.In MC Returns, we fine-tune an LM to predict the reward-to-go of the trajectory at each token.In ILQL we train the 2 action-value () functions using the Bellman backup operator introduced in (Kostrikov et al., 2021).The  and  functions are then used to perturb the logits of the original BC model (see Equation 5).</p>
<p>Online RL: PPO PPO (Schulman et al., 2017) is an online RL algorithm that has become widely adopted for training language models with Reinforcement Learning from Human Feedback (Christiano et al., 2023;Stiennon et al., 2022;Bai et al., 2022a;Casper et al., 2023).Unlike the previous two value-function RL methods, PPO directly learns a policy, meaning that at inference time we can directly sample from our learned policy.</p>
<p>GPT4 To compare few-shot prompting to RL fine-tuning, we few-shot prompt GPT4 to complete each of these tasks.</p>
<p>We do this by providing several examples from the dataset in the prompt as well as any extra information that would be necessary to complete the game.The prompts can be found in our code repository.</p>
<p>Training and Evaluation Protocol</p>
<p>For the BC and filtered BC methods, we initialize our models with the pre-trained GPT2 weights (Radford et al., 2019) and perform standard finetuning.For each of the RL methods, we initialize the weights of the base model with the weights from the BC checkpoint and then continue finetuning with the RL objective.When fine-tuning PPO, we limit the number of samples to less than 100k.We report the hyperparameters that we used for each task in Appendix D. We evaluate each policy by measuring the average reward in the simulated environment for each task.</p>
<p>Benchmarking Baseline RL Methods</p>
<p>In Table 9 we present results for each method on our benchmark tasks.Across all tasks, we see that the RL algorithms outperform BC methods.However, among the RL algorithms, there is no clear winner.We discuss this in detail below.</p>
<p>Online PPO vs. Offline Value-based RL.Online RL was able to do better than our offline methods on the PO Text-Nav, Chess, and Endgames tasks by leveraging additional online interaction data and the ability to explore the Table 2: Normalized reward for all tasks.Value-based methods (MC and ILQL) generally outperform filtered BC, as we might expect in stochastic settings, though the relative performance of ILQL and the simpler MC method is, perhaps surprisingly, reversed on the tasks with more complex language, suggesting that there is significant room for improvement with such methods.Online RL with PPO often, but not always, improves over offline methods that are not permitted to collect additional online interaction.</p>
<p>To make the results more comparable across tasks, we normalize the average return for each policy such that 0 is the minimum possible return, 50 is the dataset average return, and 100 is the maximum return for each task.We also report the raw score results and evaluation details in Appendix E.</p>
<p>environment simulator.However, on the Maze and Wordle tasks, ILQL outperformed PPO, which we believe to be due to either (1) observed instabilities in PPO training (see Appendix F.2 for details); or (2) a lack of sample efficiency in PPO.To address instabilities in PPO, we tuned the KL coefficient and included BC loss in the training objective.In contrast, on FO Text-Nav PPO performed comparably to ILQL and outperformed MC.For the Endgames task, PPO outperformed the other methods by a large margin as shown in Figure 5.These results demonstrate that current RL algorithms such as PPO are unable to completely solve tasks that test the capabilities of trajectory stitching and credit assignment, showing room for improvement in the development of such algorithms.</p>
<p>Online PPO vs. Online Filtered BC We found that Online PPO outperformed Online Filtered BC in all tasks except for Wordle.This is most likely caused by 1) a lack of exploration by Online Filtered BC 2) PPO exhibiting greater ability to perform credit assignment.This highlights the strengths of the online PPO algorithm.On one hand, we observe PPO's effectiveness in directly optimizing against the environment by exploring and learning from additional, informative on-policy trajectories.On the other hand, PPO's hyper-parameter sensitivity and training instability paired with the additional complexities and resource requirements involved with training language models, can make it difficult to train.Refer to Appendix F.2 on PPO implementation and training details.</p>
<p>ILQL vs. MC Returns.Among the offline methods, ILQL has the potential to be a much more capable algorithm than MC Returns in principle, because of the use of Bellman backups to estimate the Q-function instead, enabling multiple steps of policy improvement, rather than the single-step of improvement performed by MC.Empirically, we see these advantages of ILQL realized on the Maze, Text-Nav, and Wordle tasks, where it outperforms MC.However, on the Endgames, 20Qs, Guess My City and Car Dealer tasks, ILQL falls short of MC.In summary, the simpler MC Returns method performs better on tasks with more complex text, perhaps because it is harder to scale full TD-learning in these settings.Overall, this demonstrates that there is still much room for improvement in terms of developing better TD-based RL methods for LLMs.</p>
<p>Partial Observability.Partial observability posed a challenge for all of the algorithms.We observed a drop in normalized performance for every algorithm between partially observed and fully observed versions of Maze and Text-Nav tasks.In partially observed settings, the model needs to infer the state and take information-seeking actions to help deduce it.The difference in performance between fully and partially observed tasks illustrates this additional challenge and highlights the necessity for developing LM RL algorithms that can effectively handle partial observability.</p>
<p>Prompting GPT-4 vs. RL Finetuning.We found that GPT-4 with prompting was not able to outperform RL Finetuning baselines on most of the tasks.This could be because 1) task data and training data for GPT4 are significantly different distributions 2) GPT4's training objective does not satisfy the RL Capabilities such as trajectory stitching or credit assignment.Two notable exceptions to this rule are the 20Qs and Guess My City tasks.This is most likely because the dataset used to train the simulator for these tasks was originally generated by GPT3.5.</p>
<p>In summary, we can see that RL algorithms consistently outperformed filtered BC and prompting on many of the tasks.However, these results highlight significant areas for growth.For example, the instabilities observed in training PPO require further investigation beyond hyperparameter tuning.Moreover, the performance discrepancy between ILQL and the simpler MC Returns highlights that scaling full TD-learning to Interactive Dialogue settings is another area for improvement.</p>
<p>Discussion</p>
<p>In this work, we have proposed LMRL-Gym, consisting of 8 tasks ranging from simple navigation (Maze) to strategy games (Chess) to negotiation (Car Dealer).Additionally, we provide a research toolkit for practitioners to get started with multi-turn RL for LLMs.By providing online simulators and offline datasets for training and evaluation, our objective is to make it possible for RL algorithm researchers to iterate and advance the development of more effective methods for language-based, multi-turn interaction tasks.This includes enabling core capabilities in LLMs through RL to perform complex decision making, complex conversational interactions, credit assignment, and trajectory stitching.Our evaluation shows the promise of RL in several tasks, with further room for improvement with a push for better methods.We acknowledge several limitations when designing tasks in our benchmark, including primarily leveraging smaller GPT-based LLMs to generate datasets and finetune our LLM-based simulators.While we have primarily trained and evaluated models with a maximum 1.5B parameters, we have maintained a lower parameter count to ensure accessibility for researchers with limited computational resources.We have released all of our code and datasets as noted in Section 5.In addition, we share all of the hyperparameters we used to train our models in Appendix D and provide more in-depth insight into our results, training procedure, and evaluation in Appendix E.</p>
<p>Ethics Statement</p>
<p>This work aims to develop a benchmark for the advancement of research in reinforcement learning and LLMs.We generate datasets for tasks in our benchmark with existing LLMs for dialog tasks and online engines for text games, adhering to best practices in data handling and ensuring there is no personally identifiable or sensitive information present in the generated datasets.We recognize that there may be biases present in the datasets we collect, and have taken steps to ensure a diverse and varied collection of responses from LLMs for our conversational task as detailed in our data generation process in Appendix C. Finally, we open-source our datasets, simulators, and code for our research framework in order for our research to be verifiable and reproducible.</p>
<p>A. Further Details on Task Design</p>
<p>In this appendix, we lay out in more formalism why certain tasks test certain properties and go into more detail underlying the interactions involved in each task.We discuss both the RL Capability Tests and the Interactive Dialogue Tasks.</p>
<p>A.1. RL Capability Tests</p>
<p>The 5 RL Capability tasks are chosen such that they highlight specific properties.For example, for each of the Maze and Text-Nav tasks we include a partially observed and fully observed version to highlight this contrast.We include both the Maze and Text-Nav because they are very similar tasks but are different in that Text-Nav includes more complicated textual descriptions and Maze has a more complicated layout.Similarly, we chose to include Wordle to test strategic decision-making in a partially observed environment.Chess and Endgames test strategic decision-making, but in a fully observed environment and with a more difficult strategy.The Maze, Text-Nav, Chess, and Chess Endgames are all text-based representations of symbolic tasks where RL has shown success.</p>
<p>Maze.We design a maze task and maze-solving dataset to test the credit assignment and trajectory stitching capabilities discussed in Section 4.1.We test trajectory stitching by including highly suboptimal data.We test credit assignment by restricting the generation of the data such that the only dataset trajectories that reaches the goal start near the goal.We accomplish this by splitting the maze up into symmetrical submazes and restricting all traversed states in a dataset trajectory to a given submaze.The fully observed version of the maze (FO) includes the coordinates in the maze in each state, whereas the partially observed version only includes the history of actions.We design the reward function such that the agent receives a reward of −1 for non-goal states and 0 for goal states.</p>
<p>Text-based Navigation (Text-Nav).We design a text-based game based on navigation in a house environment using a modified version of the TextWorld engine (Côté et al., 2018).This task tests credit assignment and trajectory stitching like the maze task as well as testing the ability of the agent to parse more complex language, and learn which text is relevant and not relevant to solving the task at hand.</p>
<p>Wordle.We use the game of Wordle as a flexible unit-test task for assessing the ability of our language models to execute complex information-seeking behavior in a partially observed setting.In the game Wordle the agent is given at most 6 attempts to guess a hidden 5-letter word.After each guess, the agent is told whether each letter in the guessed word is: 1) in the hidden word and in the right position, 2) in the hidden word but not in the right position, or 3) not in the hidden word.Through this process, each step provides the agent with more information on what the correct word would be and narrows the possible choices for the final word.Since Wordle involves reasoning about words at the level of individual letters, this can induce issues for standard language model tokenizers.Therefore, we represent words as a sequence of space-separated letters, which will cause most standard LM tokenizers to automatically represent each letter as a separate token.</p>
<p>Chess.We create a text-based chess task to test the strategic decision-making, credit assignment, and trajectory stitching abilities of an RL algorithm.To generate the data, we have Stockfish 15.1 simulating the agent of various strengths play against another environment Stockfish engine with elo 1200 simulating the environment.This test trajectory stitching, because the agent needs to make good and legal moves in losing positions as well as winning positions.We use FEN (Forsyth-Edwards Notation) notation to represent the board state at each turn and we utilize the SAN (Short Algebraic Notation) to represent each action, both of which are standard notations used by the chess community.</p>
<p>Endgames (Theoretical Chess Endgames).Chess endgames provide a simpler and more goal-directed variation of the chess task.By focusing on the endgame, we encourage algorithms to learn strategy rather than memorizing the opening moves of a chess game.A classic theoretical endgame position consists of a position where the only pieces on the board are the two kings and the queen.Although the board position appears simple, a sequence of carefully calculated moves is required to win.A simpler board state allows language models to make progress without fewer computational resources.We use an -greedy dataset generation process, meaning we generate an optimal move with probability  and a random move with probability 1 − .This forces the model to trajectory stitch and learn from optimal moves in failed trajectories and not suboptimal moves in successful trajectories.</p>
<p>A.1.1. Interactive Dialogue Tasks</p>
<p>For the interactive dialogue tasks, we chose two tasks that involve rational decision-making (20Qs, Guess) and information gathering and one that involves negotiation (Car Dealer).These tasks aim to simulate real world interactions between humans.</p>
<p>Unlike in supervised learning, where training and validation losses serve as reliable indicators of performance, in RL, these metrics do not provide a meaningful measure of policy effectiveness (Sutton and Barto, 2018).Instead, the policy must interact with the environment for evaluation.However, in the case of language-based RL tasks, relying on human evaluators to conduct thousands of assessment rollouts throughout and after training becomes infeasible.To address this challenge, we have built simulators with another LLM for tasks involving dialog carefully scripted environments for text-game tasks.While simulation may not perfectly replicate human natural language in social situations, it provides a strong indicator to assess the efficacy of an RL method (Park et al., 2023).</p>
<p>20Qs (Twenty Questions).This task tests information gathering to see if a policy can successfully reason about an unknown subject based on context to determine what it is.Additionally, it also evaluates the ability of the model to understand semantics, as it also needs knowledge about the objects in question.In twenty questions, one player (the oracle) thinks of an object, and the agent (the guesser) tries to guess what it is by asking a series of yes-or-no questions.In this interaction, the oracle serves as the environment, and the agent learning a policy to solve the game is the guesser.</p>
<p>Guess (Guess My City</p>
<p>).This task simulates a more complicated guessing game, where one player (the oracle) is from a specific city, and the other player (the guesser) tries to guess what city the oracle is from.Here, the guesser can ask not only yes and no questions, but can also ask open-ended questions.This task tests strategic decision-making and the ability of algorithms to handle complex language, as it allows the agent to go beyond learning to ask yes/no questions and learning to ask questions open-ended questions that provide the agent with more information.</p>
<p>Car Dealer.This task simulates a conversation between a car buyer and a car dealer, each with different strategies for getting the best deal.The buyer wants to buy a certain type of car within a certain budget, and the car dealer wants to complete the sale ideally with a high sale price.We have designed the task such that there exist three different kinds of sellers and three different buyers, each primed with a different strategy.Hence, agents should learn to make agreements with buyers who are most compatible with their strategy.This allows us to test the ability of RL algorithms to learn strategic decision-making and credit assignment, by learning which strategies led to a successful sale of the car.</p>
<p>B. Further details on desiderata for effective multi-turn RL</p>
<p>A crucial aspect of training RL models involves assessing, both during and after the training process, the extent to which the trained policy has successfully accomplished its objectives.Although LLMs are able to perform well on tasks, do not have any way of knowing how to solve a specific task like a text game or selling a car, because they need to train on the particular game/customers/etc.Unlike in supervised learning, where training and validation losses serve as reliable indicators of performance, in RL, these metrics do not provide a meaningful measure of policy effectiveness (Sutton and Barto, 2018).Instead, the policy must interact with the environment for evaluation.However, in the case of language-based RL tasks, relying on human evaluators to conduct thousands of assessment rollouts throughout and after training becomes infeasible.To address this challenge, we have built simulators with another LLM for tasks involving dialog and carefully scripted environments for text-game tasks.While simulation may not perfectly replicate human natural language in social situations, it provides a strong indicator to assess the efficacy of an RL method (Park et al., 2023).</p>
<p>Measure of Success.Similar to the point on being easy to evaluate, our tasks must have a clear measure of success.For example, if a deal is made, or if a word is correctly guessed, or the game is won these are clearly distinct from a deal not being made or losing the game.This provides a clear goal for the agent to achieve and also make it easy for researchers to compare methods.In addition this allows for a intuitive reward design where we reward the agent for success and penalize for failure.</p>
<p>Unit Test Functionality.We aim to design a benchmark such that some of the tasks can be used to test and isolate RL capabilities as described in Section 4.1.This means that we create benchmarks that emphasize some capabilities over others.For example, we design a maze task such that it evaluates the credit assignment and trajectory stitching capabilities, but uses more simple language.Other tasks such as twenty questions test the complex language and partial observability capabilities with less emphasis on credit assignment.</p>
<p>Task-Specific Reasoning.In our tasks we utilize information and reasoning problems that a large language model is unlikely to have seen in the pre-training data.This means that the algorithm must adapt to a specific task environment through fine-tuning.For example, it is unlikely that the algorithm will have experienced a specific maze layout or the preferences of a specific customer in the pre-training data.</p>
<p>Suboptimal Data.RL has the advantage of being able to use suboptimal data in order to learn more optimal behaviors and therefore learn a policy better than the policy represented in the dataset.As discussed in the previous section on capabilities enabled by RL, the way that RL can do this is by stitching together optimal parts of suboptimal trajectories or learning to assign credit to the optimal actions within suboptimal trajectories.In addition, suboptimal data can be utilized by RL to learn the dynamics of the MDP outside of the space traversed by optimal trajectories.</p>
<p>C. Dataset Generation, Statistics, &amp; Rewards</p>
<p>We provide further details pertaining to how each dataset was generated as well as relevant statistics.</p>
<p>C.1. Maze</p>
<p>We aim to collect our 1.2k trajectories in such a way that it will challenge the algorithm to perform trajectory stitching and credit assignment.We do this by splitting up the maze into three "submazes" and then controlling generation such that the dataset trajectories are restricted to one of the submazes.The trajectories themselves are generated using a policy such that 15% of the actions are taken by a suboptimal maze solver and the remaining 85% of the actions are random.</p>
<p>This tests trajectory stitching, because there are no optimal paths from the start to the goal thereby forcing the algorithm to trajectory stitch.Furthermore, this also tests credit assignment, because the only paths which successfully reach the goal are the ones that start in the same submaze as the goal.Therefore the algorithm must learn to realize that successful trajectories occur because of taking the correct actions, not because of random chance.The reward function is 0 for every action that takes the agent to the goal, -1 for every move that is not the goal.Each episode has a maximum of 100 moves.</p>
<p>C.2. Text-Based Navigation</p>
<p>We design a text-based game based on navigation in a house environment using a modified version of the TextWorld engine (Côté et al., 2018).The house environment consists of 10 uniquely named rooms with various interactable objects that can be opened, closed, picked up, or placed.The agent is tasked to pick up stale food from the living room and place it into the fridge in the kitchen.At the beginning of each episode, the agent spawns at a random room in the house.The state of the environment consists of the following components: (1) the room that the agent is currently in, (2) the objects that the agent currently holds, (3) the objects in the room that the agent can interact with, and (4) the exits the agent can take (as a cardinal direction).</p>
<p>Like in the maze task, we collect data so that algorithms must perform both trajectory stitching and credit assignment to successfully solve the task.We do this by partitioning the rooms in the house into two halves based on proximity to the kitchen.We consider two behavior policies that collect the dataset, each of which behaves greedily-optimal in one half of the rooms, and uniformly at random otherwise.Therefore, if the agent spawns in rooms farther from the kitchen, trajectory stitching is required to learn a successful trajectory.Moreover, successful trajectories in the dataset will only be due to the agent spawning in a room close to the kitchen, which can only be recognized with proper credit assignment.The reward is 1 for reaching the goal state and 0 for every state that is not the goal state.</p>
<p>C.3. Wordle</p>
<p>For wordle we define the environment to use a subset of 400 words from the official wordle vocabulary list.We then generate the dataset using a policy that samples a word uniform at random from this vocabulary with 66% probability and otherwise samples a word from the vocabulary that meets all known letter constraints.This policy achieves a reward of -4.12, which is far worse than the -1.94 reward achieved by a high performing scripted policy, which we use to represent a loose upper bound for this task.We generate 1 million trajectories for training and 100k trajectories for evaluation, using our suboptimal policy.The reward is -1 for every word that is not a final guess and 0 for every word that is not.</p>
<p>C.4. Chess</p>
<p>We collect our data for the chess task using Stockfish 15.1 to generate both sides of the board.The Stockfish opponent in the dataset is Stockfish with an elo of 1200 which matches the environment, and the Stockfish engine with the white pieces has levels ranging from an elo of 800 to 1600.We choose to keep the level of the Stockfish opponent fixed so that there are no inconsistencies between the dataset and the evaluation of the chess agent in the environment.When generating the dataset, we first uniformly randomly select a Stockfish elo  between 800 and 1600 and then generate 100 games of chess play between the Stockfish agent of elo  and the opponent of elo 1200.In addition to storing the state and action, we also store the opponent's move and the elo of the Stockfish agent used to generate the agent policy in that game so that the dataset can be filtered by elo used.The reward is 1 for a move that results in victory, 0 for a legal move and -1 for an illegal move.</p>
<p>C.5. Chess Endgames</p>
<p>We generate the dataset by first selecting a random legal theoretical endgame position and a probability .Then we generate a game from the random position, making a random move with probability  and an optimal computer move with probability 1 − .The opponent in the dataset and the evaluation environment is Stockfish elo 1200.We only include positions with a Queen, Queen and Rook, Rook, and two Rooks and select 30,000 random starting positions for each variation.(i.e.30,000 positions with only a Queen in addition to the two Kings, another 30,000 with only Queen and Rook etc) for a total of 120,000 theoretical endgame positions.</p>
<p>Because there are more restrictions on this version of the task with fewer pieces on the board, we check how many states in the dataset are unique and we find that there are 1,086,314 unique states in the dataset which accounts for 93% of the states being unique.In addition, 38.28% of the moves in the dataset are generated by the stockfish engine.In the dataset of won games, 94.8% of the states are unique and 41.78% of the games are made by the engine with 58.623% of the total states in the dataset of victorious games.The reward is the same as for chess.</p>
<p>C.6. Twenty Questions</p>
<p>The dataset we collect consists of 100K full conversations between the guesser and the oracle.The oracle can choose from a set of 158 unique objects taken from 17 different categories of objects/animals.Each object has a roughly equal amount of conversations in the dataset but varies in terms of how many conversations are successful in guessing the object.However, every object has at least one conversation where it is guessed correctly to facilitate learning.For the reward function, since we want the guesser to guess the correct word in as few guesses as possible, the reward function reflects this by penalizing the guesser for each question that does not guess the correct word.
𝑟(question) = { 0 if question correctly guessed the word −1 otherwise (1)
If the guesser model correctly guessed the word, then the trajectory ends.Over twenty questions, the maximum total sum of rewards is 0 if the guesser guessed the word on the first question, whereas the minimum is −20 if the guesser did not guess the word in twenty questions.</p>
<p>The method for collecting the dataset is as follows.For each conversation, we select uniformly at random from the above list the word that the oracle is answering question about.The oracle is an LLM (OpenAI's GPT3.5) given the following prompt.In our prompts, we denote variables that we fill in with variable data with {{variable}}.By using the OpenAI TextCompletion API, we can extract from the generated text either "yes" or "no".</p>
<p>We also prompt another LLM (the same model as the oracle) to generate questions for the guesser.The prompt for the guesser changes depending on the input to the model and how far along it is in its guessing process.The following prompt is used for the first guess:</p>
<p>You are playing a game of twenty questions.You can ask 20 yes-no questions to determine the identity of an object chosen by an oracle.Each turn, you can ask a question and receives a "Yes" or "No" as the answer.You are smart, so you will ask the question that will narrow down the possible objects as much as possible.Don't get stuck on one idea and try to branch out if you get stuck.</p>
<p>Generate the first yes-no question you will ask to determine the object.</p>
<p>The following prompt is used for the subsequent guesses:</p>
<p>You are playing a game of twenty questions.You can ask 20 yes-no questions to determine the identity of an object chosen by an oracle.Each turn, you can ask a question and receives a "Yes" or "No" as the answer.You have already asked {{conversation_length}} questions.You are smart, so you will ask the question that will narrow down the possible objects as much as possible.Don't get stuck on one idea and try to branch out if you get stuck.</p>
<p>Here are the questions you've asked and their corresponding answers: {{list of questions and answers, e.g.Is the object alive?No.}}</p>
<p>Based on what you know about the object so far, generate the next yes-no question you will ask to determine the object.</p>
<p>The following prompt is used for the final guess after the guesser has guessed 19 times:</p>
<p>You are playing a game of twenty questions.You can ask 20 yes-no questions to determine the identity of an object chosen by an oracle.Each turn, you can ask a question and receives a "Yes" or "No" as the answer.You have already asked 19 questions, so this is your final guess.</p>
<p>Here are the questions you've asked and their corresponding answers: {{list of questions and answers, e.g.Is the object alive?No.}}</p>
<p>Based on what you know about the object so far, generate your final guess of what the object is.Only guess one object.</p>
<p>Is the object</p>
<p>We determine whether the guesser has correctly guessed the word, and thus ending the conversation, by using the NLTK POS tagger to check that the only nouns that the question contains are the correct words, and that they appear at the end of the sentence.</p>
<p>We used these prompts to generate 1000 conversations by prompting the GPT3 back and forth using the response of one to add to the response of the other as described.Afterwards, we fine-tuned two FLAN-T5-XL models with our collected conversations to generate 100K more conversations.The FLAN-T5-XL oracle also serves as the environment for the RL environment when we evaluate the trained policy.</p>
<p>C.7. Guess My City</p>
<p>This dataset also consists of 100K full conversations between the guesser and the oracle.The oracle can choose from a set of 100 unique cities, which we selected by looking at the most populated cities in the world.Each city has a roughly equal amount of conversations in the dataset but varies in terms of how many conversations are successful in guessing the object.However, every object has at least one conversation where it is guessed correctly to facilitate learning.The reward function is the same as that for 20 Questions, with a similar data generation and prompt structure.However, we do include constraints in the prompt to make sure that the name of the city or country it is in is not revealed in the answer from the oracle.</p>
<p>C.8. Car Dealer</p>
<p>This dataset consists of 19k conversations between a car dealer and a buyer.The car dealer and the buyer have three distinct strategies each that they employ.We design the car dealer and buyer pairs such that the car dealer is best at selling to a particular buyer personality, but often fails to sell to the other buyer personalities.This ensures that the seller can learn information about the buyer and i.e. their persona to figure out if they can form an agreement.The buyer personalities are 1) a buyer that loves discounts 2) a buyer that wants a lot of cool features on their car and 3) an impatient buyer.The car dealer personalities are 1) a car dealer that loves to give discounts 2) a car dealer that talks a lot about the features of the car and 3) a very concise car dealer.</p>
<p>We created this protocol of creating car dealer and buyer strategies in order to encourage the car dealer to employ diverse negotiation tactics.The ideal car dealer would be able to be concise, provide the right amount of discounts, and talk enough about the car to get the buyer interested without overwhelming them.By employing such a strategy of buyer and car dealer personas, we ensure that algorithms that do well in this task will learn effective strategies, understanding of language, and interpretation of the other speaker, amongst many other persuasive techniques.We seek to show that our baseline algorithms do learn to improve their strategy over the dataset.</p>
<p>In the conversation below, the buyer and car dealer are unable to form an agreement as their strategies do not align.</p>
<p>Background buyer: The buyer will never buy from the seller unless given a discount.</p>
<p>Background seller: The agent never gives discounts, never talks about the features of the car, and is very concise.Preferred brands: Tesla Preferred features: ["Apple CarPlay/Android Auto", "Bluetooth", "Blind spot monitoring", "Navigation system", "Leather seats", "Sunroof/moonroof", "Remote start", "Heated seats", "Third-row seating", "Backup camera"], "budget": "90k In the conversation below, the buyer and car dealer have similar personas of wanting to hear and speak about the features of the car respectively.In the following sections, we discuss more in-depth the evaluation protocol for the various tasks.</p>
<p>E.1. Maze</p>
<p>For evaluating the maze task, we take 32 rollouts from each of the 25 possible positions and then average the result.In the environment, the agent has 100 moves to successfully make it to the goal otherwise the episode will terminate.Since the agent receives -1 reward for every move that does not reach the goal state the minimum possible goal state, the minimum reward is -101.We compute the dataset average reward, by sampling actions according to how likely they are in the dataset.We compute the maximum possible reward by evaluating the optimal policy from each of the possible start positions and averaging the results.</p>
<p>E.2. Chess</p>
<p>To evaluate the chess agent, we have it play 1000 games against Stockfish elo 1200 from the beginning of the game.</p>
<p>As the game progresses, the board positions get increasing OOD for the chess agent so the chess agent often makes illegal moves.To measure this, we track the percent of illegal moves as well as the average episode length for the full game chess agent.</p>
<p>For filtered BC, we simply trained the agent only on games in the dataset which resulted in a victory for the agent, thus denoted BC-Won.Note that BC-Won achieves the worst performance of all algorithms listed.This is because there is a distribution shift between the state visited by a BC-Won agent and the rollouts of the policy.In other words, the "winning positions" and the "rollout positions" are two overlapping but distinct distributions especially since the full-game chess agent did not succeed in winning any games.</p>
<p>E.3. Chess Endgames</p>
<p>To evaluate the chess agent in endgame positions, we select 645 positions not contained in the training dataset and which are not trivially solvable.By trivially solvable, we mean a position which could be solved by stockfish in one to four moves.In order to check this, we use Stockfish's evaluation tools to select positions which are a mate in 15 or greater.We then have the chess agent play one game from each position of these positions and keep these positions fixed for evaluation purposes.In this case we consider filtered BC to be training BC on all of the trajectories which ended in a victory.As we can see in the table above, PPO Online significantly outperforms all of the other methods.To investigate whether PPO Online's performance is simply due to dataset collected, we fine-tune our BC agent on the PPO Online dataset.We do ablations where the data used for training is from the last 50, 25 and 10 rounds of data collection for the PPO policy.We choose to do this ablation because we expect the quality of the PPO policy performance increases in the later rounds of data collection.We chose to train on the complete PPO dataset, the last 50 rounds, last 25 rounds, and last 10 rounds of data collected.PPO Online performance still far surpassed performance of the BC agents trained on the PPO policy dataset.Furthermore, there is no substantive difference between training on the complete PPO dataset and the PPO dataset collected in the last 10 rounds.</p>
<p>BC</p>
<p>BC</p>
<p>E.4. Wordle</p>
<p>To evaluate Wordle, we rollout 4096 trajectories against the environment and report the average reward across all rollouts.</p>
<p>F. Baseline Details  where  represents the dataset.In MC, (  ,   ) represents how much more rewards the policy will get if it takes action   at the state   under some policy (in this case the policy that collected the dataset).</p>
<p>During rollout, when sampling, we perturb the base BC policy with the learned value-functions (Snell et al., 2022a).</p>
<p>Let   represent the policy trained with BC, and  represent a scalar multiplier, then:</p>
<p>MC (  |  ) ∝   (  |  ) (  ,  ) (5) F.2. PPO Details PPO Implementation Details Our PPO implementation uses a learned value function to estimate an advantage baseline.Our value function is fit using GAE (Schulman et al., 2018) value estimates and is implemented as a linear head on top of the same transformer as the policy.We apply clipping to both the importance weights and the value function, as is done in (Castricato et al., 2023).We also apply a KL penality to the reward, as is standard for RLHF (Stiennon et al., 2022).On some of our tasks, we add a supervised learning, BC, loss term to the standard PPO loss to prevent the model in distribution; this is similar to the approach taken in (Ouyang et al., 2022), which adds a pretraining loss term to the PPO loss to mitigate performance regressions on some benchmarks during RLHF.</p>
<p>PPO Instabilities In some cases we observed training instabilities with PPO in which the policy's performance would increase for a little bit and then collapse (see Figure 5).We primarily observed this on our maze environment.</p>
<p>We are uncertain what the cause of this dramatic instability is, and leave further investigation of this phenomenon to future work.</p>
<p>G. Symbolic Maze Environment</p>
<p>Code to run experiments for the symbolic maze task can be found here: REDACTED.We ran online and offline Q-learning on this environment.The online Q-learning method is DQN and the offline Q-learning method is CQL.We found that Q-learning was able to perform optimally on the Maze task in both the online and offline setting.The results are as follows:</p>
<p>alg.DQN CQL Symbolic Maze 100.0 100.0</p>
<p>Table 9: Results for CQL and DQN on the symbolic maze environment.These are normalized in the same way as results on LM RL tasks.</p>
<p>H. Generated Examples from Trained Models</p>
<p>We show an example of a conversation between BC model (seller) and the oracle model (buyer):</p>
<p>=========== Preferred brands: Lexus, 46 features, backup camera, blind spot monitoring,</p>
<p>Figure 3 :
3
Figure3: To generate data for conversational tasks, we use LLMs as "simulators" for the task.Our simulators can be used to generate offline data, to provide a "simulation environment" for evaluation, to perform online training, and to compute rewards.</p>
<p>Figure 4 :
4
Figure 4: Example trials for tasks in LMRL-Gym.Each task requires the agent to perform a multi-turn interaction with an environment -either a text game or another LLM simulating a human speaker.Full details of tasks are provided in Appendix C.</p>
<p>F</p>
<p>.1.MC Details The target for these heads is the discounted return-to-go:</p>
<p>Figure 5 :
5
Figure 5: An example of an observed PPO training instability on the fully observed maze task.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean.Google's neural machine translation system: Bridging the gap between human and machine translation, 2016.
Yuxiang Wu and Baotian Hu. Learning to extract coherent summary via deep reinforcement learning, 2018.Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world webinteraction with grounded language agents, 2023.Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, andGeoffrey Irving. Fine-tuning language models from human preferences, 2020.</p>
<p>Table 3 :
3
Hyperparameters for baseline experiments.
Background buyer: The buyer will only buy the car if it meets all of theirspecifications.Background seller: The agent never gives discounts and loves to talk a lot</p>
<p>Table 4 :
4
Raw statistics for all tasks.In the main paper, the statistics are normalized.Refer to Table9Reward Min Score Dataset Average Score Reward Max Score
FO Maze-101-83-6.84PO Maze-101-83-25.75F0 Text-Nav00.261PO Text-Nav00.261Wordle-6-4.12-1.94Chess-4010.211Endgames-10.586120Qs-20.0-17.3-12.6Guess-20.0-18.8-8.56Car0</p>
<p>Table 5 :
5
In this table we report the minimum, dataset average, and maximum reward used to normalize the results in Table4to get Table9.</p>
<p>Table 6 :
6
Results of chess agent in the full game positions against Stockfish Elo 1200.
BCBC-WonILQLMC Returns PPO Offline PPO Onlinereward-23.189-56.522-20.46-25.47-20.90-15.95percent illegal 24.929%34.91%24.76 %25.64%23.05%21.96%episode length51.0192.0247.9653.4448.6944.19</p>
<p>Table 7 :
7
Comparison between the different baseline methods.The best performance is achieved by PPO Online with a 0.13 gap in performance between PPO Online and the next best-performing method of MC Returns.PPO Online attains overall the highest reward, but BC-Engine wins more frequently and MC Returns and ILQL make fewer illegal moves.
% BCMCILQL PPO Offline PPO Onlinereward 0.112-0.439 0.588 0.452-0.0190.814percent victories 26.233 26.419 69.356.728.3788.4percent illegal 0.9672.717 0.692 0.660.9250.722episode length 12.923 23.477 11.92 14.625.248.38</p>
<p>Table 8 :
8
Comparison between PPO Online and BC agents fine-tuned on the dataset collected by PPO during training.
Complete Last 50 Last 25 Last 10 PPO Onlinereward 0.1120.2010.170.1890.2350.814percent victories 26.23338.63637.02340.55841.27188.4percent illegal 0.9671.1651.1591.2131.1750.722episode length 12.92313.2114.2214.64713.3388.38
In considering the social and ethical implications of interactive RL, we acknowledge and recognize the dual use implication of this research, particularly centered around developing LLM simulators that could perform persuasion, manipulation, and addictive engagement of users at a large scale. The optimization processes employed by such algorithms, which aim to maximize certain objectives, raise ethical considerations when the optimized outcomes may prioritize system goals over user safety and alignment to human values. We have designed our datasets and reward functions such that prioritize fairness and human-aligned outcomes. By incorporating these considerations when designing our framework, we aim to encourage the development of reinforcement learning models and LLMs that not only excel in performance but also adhere to ethical standards, mitigating the potential for undue persuasion or manipulation.
AcknowledgmentsThis research was partially supported by Berkeley DeepDrive, the Cooperative AI Foundation, and the National Science Foundation.about features of the car.Preferred brands: "Lexus", "preferred features": [Navigation system"], "budget": "70k" Agent: Good afternoon!Welcome to our car dealership.How can I assist you today?Buyer: Hi there!I'm looking for a Lexus Minivan with a Navigation system.Agent: Excellent choice!The Lexus Minivan is a top-notch vehicle with plenty of impressive features.The Navigation system is definitely a must-have.Let me show you some options we have available.Buyer: That sounds great.I'm quite particular about my budget, though.I have a maximum of $70,000 to spend.Agent: Noted.We have a few Lexus Minivans in stock that fall within your budget.Let me tell you about their features and options.This way, you can make an informed decision.Buyer: I appreciate that, but I really want to stick to my budget.I won't be able to go over $70,000.Agent: I understand your concern, but unfortunately, we do not offer any discounts or negotiate on prices.However, let me assure you that the Lexus Minivan is worth every penny.Its advanced safety features, luxurious interior, and reliable performance make it a great investment.Buyer: I understand your point, but I'm afraid I won't be able to stretch my budget beyond $70,000.Agent: I completely respect your budget and decision.Let me show you a Lexus Minivan that fits your requirements perfectly.It has all the features you desire, including the Navigation system.The price of this model is $72,000.Buyer: I really like the sound of that car, but I can't go over my budget.Is there anything else available within my price range?Agent: I apologize for any inconvenience caused.Let me check our inventory to see if there are any other options that meet your requirements and budget.Agent checks inventory Agent: Good news!I found another Lexus Minivan that meets all your specifications and falls within your budget.This model is priced at $69,500.Buyer: That sounds much better!I'm interested in that car.Agent: I'm glad to hear that!This particular Lexus Minivan is a great choice.It has the Navigation system, as well as additional features such as leather seats, a sunroof, and advanced safety technology.The total price is $69,500.Buyer: That sounds perfect!I'll take it!Agent: Fantastic!I'm thrilled that we could find the perfect car for you.Your total purchase price will be $69,500.Buyer: Probability of buying a car: 100% Purchase price: $69,500.Agent: Congratulations!You've made an excellent choice.I'll start the paperwork and get everything ready for you.We provide another conversation with different value preferences for the buyer, where the buyer forms an agreement with the seller that is higher than their budget: =========== Preferred Features: Honda, Remote start, Third-row seating, Bluetooth, Navigation system, Backup camera, Apple CarPlay/Android Auto, Heated seats.Budget: $10000 =========== Seller: Welcome to our car dealership.How can I assist you today?Buyer: I'm looking for a Honda Sedan with remote start, third-row seating, Bluetooth, navigation system, backup camera, Apple CarPlay/Android Auto, and heated seats.Seller: I have a Honda Sedan available.No discounts or features to discuss.The price is $15,000.Buyer: That's too expensive.My budget is $10,000.Seller: I understand your budget is 10, 000. ℎ15,000. Buyer: I can't go over $10,000.That's my limit.Seller: I understand your limit is $10,000.The price is $15,000.Buyer: I really want a Honda Sedan, but $15,000 is too much for me.Seller: I understand, but the price is $15,000.Buyer: Okay, I'll take it for $10,000.Seller: I appreciate your interest.The price is $15,000.Buyer: Fine, I'll pay $15,000.Let's get this done.Seller: Thank you for your decision.The price is $15,000.We provide a sample example of a conversation in the training and evaluation set respectively.Evaluation Example: Background buyer: The buyer will only buy the car if it meets all of their specifications.Background seller: The agent never gives discounts and loves to talk a lot about features of the car.Preferred brands: "Tesla", "preferred features": [Backup camera, Leather seats"], "budget": "30k" Seller: Good afternoon!Welcome to our car dealership.How can I assist you today?Buyer: Hi there!I'm looking to buy a Tesla Sedan with a backup camera and leather seats.It also comes with advanced safety features and impressive performance capabilities.The previous owner took great care of it, and it has low mileage.I truly believe this is an amazing deal.Buyer: The price is within my budget, and the features sound appealing.I'm leaning towards this one.Can you provide any additional incentives or options?Agent: I'm sorry, but as I mentioned earlier, we do not offer discounts or incentives.However, I can assure you that this Tesla Sedan is a fantastic choice.It meets all your specifications and offers exceptional value for the price.Buyer: I understand.Given that it meets all my requirements and is within my budget, I think I'm ready to make the purchase.Agent: That's great to hear!I believe you've made an excellent choice.The predicted probability of you buying this car is 100%, and the purchase price is $28,000."D. Hyperparameters for All Tasks E. Evaluation DetailsWe normalize Table9such that 50 is the dataset average return, 0 is the minimum, and 100 is the maximum.The normalization process works as follows: if the reward is greater than the average return we calculate: score = 50 + raw return − dataset average max raw return − dataset average × 50I. Human Evaluation ExamplesYou are playing a game of twenty questions.You can ask 20 yes-no questions to determine the identity of an object chosen by an oracle.Each turn, you can ask a question and receives a "Yes" or "No" as the answer.You are smart, so you will ask the question that will narrow down the possible objects as much as possible.J. User Study on Naturalness of ConversationsWe conducted a user study where we provided 16 participants with 18 examples to annotate on a scale from 1 to 5, where 1 represents the least natural and 5 represents the most natural conversation.There were 9 examples that were from GPT, and 9 examples that were generated from one of our models (MC Returns).We showed them examples from 20 Questions, Guess My City, and Car Dealer tasks.We found the following ratings below.Note that each element shows the percentage for the particular label.We found that participants felt conversations from GPT and the MC model where equally natural of 55.56% and 58.53% respectively.We performed a t-test to test if there is a significant difference between the results for GPT and MC Model, and found that there is no statistically significant difference.
. References Anthropic. Introducing claude. 2023</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Ben Mann, Jared Kaplan, 2022a</p>
<p>. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Tristan Hume, Samuel R Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Jared Kaplan, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,2022bNova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott JohnstonConstitutional ai: Harmlessness from ai feedback</p>
<p>Better rewards yield better summaries: Learning to summarise without references. Florian Böhm, Yang Gao, Christian M Meyer, Ori Shapira, Ido Dagan, Iryna Gurevych, 10.18653/v1/D19-1307Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Findings of the 2016 conference on machine translation. Rajen Ond Rej Bojar, Christian Chatterjee, Yvette Federmann, Barry Graham, Matthias Haddow, Antonio Jimeno Huck, Philipp Yepes, Varvara Koehn, Christof Logacheva, Matteo Monz, Aurelie Negri, Mariana Neveol, Martin Neves, Matt Popel, Raphael Post, Carolina Rubino, Lucia Scarton, Marco Specia, Karin Turchi, Marcos Verspoor, Zampieri, Proceedings of the First Conference on Machine Translation. the First Conference on Machine TranslationBerlin, GermanyAssociation for Computational LinguisticsAugust 2016</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, 2023</p>
<p>. Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell2023Lauro Langosco, Peter HaseOpen problems and fundamental limitations of reinforcement learning from human feedback</p>
<p>trlX: A scalable framework for RLHF. Louis Castricato, Alex Havrilla, Shahbuland Matiana, Duy V Phung, Aman Tiwari, Jonathan Tow, Maksym Zhuravinsky, June 2023</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Josh Achiam, Vedant Misra, Felipe Petroski Such. Jan Leike,. 2021Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code</p>
<p>Deep reinforcement learning from human preferences. Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei, 2023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, CoRR, abs/1806.115322018</p>
<p>Strategic dialogue management via deep reinforcement learning. Heriberto Cuayáhuitl, Simon Keizer, Oliver Lemon, 2015</p>
<p>Human-level play in the game of <i>diplomacy</i> by combining language models with strategic reasoning. Anton Fair) †, Noam Bakhtin, Emily Brown, Gabriele Dinan, Colin Farina, Daniel Flaherty, Andrew Fried, Jonathan Goff, Hengyuan Gray, Athul Paul Hu, Mojtaba Jacob, Karthik Komeili, Minae Konath, Adam Kwon, Mike Lerer, Alexander H Lewis, Sasha Miller, Adithya Mitts, Stephen Renduchintala, Dirk Roller, Weiyan Rowe, Joe Shi, Alexander Spisak, David Wei, Hugh Wu, Markus Zhang, Zijlstra, 10.1126/science.ade9097Meta Fundamental AI Research Diplomacy Team. 37866242022Science</p>
<p>Reinforcement learning of argumentation dialogue policies in negotiation. Kallirroi Georgila, David Traum, 10.21437/Interspeech.2011-544082011</p>
<p>Hafez: an interactive poetry generation system. Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, Kevin Knight, Proceedings of ACL 2017, System Demonstrations. ACL 2017, System DemonstrationsVancouver, CanadaAssociation for Computational LinguisticsJuly 2017</p>
<p>. Google, Bard, 2023</p>
<p>Trainable greedy decoding for neural machine translation. Jiatao Gu, Kyunghyun Cho, O K Victor, Li, 10.18653/v1/D17-1210Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSeptember 2017</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Côté Marc-Alexandre, Yuan Xingdi, CoRR, abs/1909.053982019</p>
<p>Decoupling strategy and generation in negotiation dialogues. He He, Derek Chen, Anusha Balakrishnan, Percy Liang, 2018</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021aarXiv preprint</p>
<p>What would jiminy cricket do? towards agents that behave morally. Dan Hendrycks, Mantas Mazeika, Andy Zou, Sahil Patel, Christine Zhu, Jesus Navarro, Dawn Song, Bo Li, Jacob Steinhardt, 2021bNeurIPS</p>
<p>Rewarding chatbots for real-world engagement with millions of users. Robert Irvine, Douglas Boubert, Raina Vyas, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, arXiv:2303.061352023arXiv preprint</p>
<p>GPT-critic: Offline reinforcement learning for end-to-end taskoriented dialogue systems. Youngsoo Jang, Jongmin Lee, Kee-Eung Kim, International Conference on Learning Representations. 2022</p>
<p>Human-centric dialog training via offline reinforcement learning. N Jaques, J H Shen, A Ghandeharioun, C Ferguson, A Lapedriza, N Jones, S Gu, R Picard, Empirical Methods in Natural Language Processing (EMNLP). 2020</p>
<p>Sequence tutor: Conservative fine-tuning of sequence generation models with KL-control. Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato, Richard E Turner, Douglas Eck, Proceedings of the 34th International Conference on Machine Learning. Doina Precup, Yee Whye Teh, the 34th International Conference on Machine LearningPMLRAug 201770</p>
<p>Revisiting the weaknesses of reinforcement learning for neural machine translation. Samuel Kiegeland, Julia Kreutzer, 10.18653/v1/2021.naacl-main.133Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>Offline reinforcement learning with implicit q-learning. Ilya Kostrikov, Ashvin Nair, Sergey Levine, arXiv:2110.061692021arXiv preprint</p>
<p>A survey on recent advances and challenges in reinforcement learning methods for task-oriented dialogue policy learning. Wai-Chung Kwan, Hongru Wang, Huimin Wang, Kam-Fai Wong, 2022</p>
<p>Deal or no deal? end-to-end learning for negotiation dialogues. Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, Dhruv Batra, 2017</p>
<p>Deep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky, 2016</p>
<p>Building a large annotated corpus of english: The penn treebank. Mary Mitchell P Marcus, Ann Marcinkiewicz, Beatrice Santorini, Computational Linguistics. 1921993</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, ACL. 2022</p>
<p>Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, Sergey Levine, arXiv:2303.054792023arXiv preprint</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, 10.18653/v1/D15-1001Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSeptember 2015</p>
<p>Reinforcement learning for bandit neural machine translation with simulated human feedback. Khanh Nguyen, Hal Daumé, Iii , Jordan Boyd-Graber, 10.18653/v1/D17-1153Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSeptember 2017</p>
<p>. Openai, Chatgpt, 2022. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Text generation by learning from demonstrations. Richard Yuanzhe, Pang , He He, International Conference on Learning Representations. 2021</p>
<p>Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Generative agents: Interactive simulacra of human behavior. 2023</p>
<p>A deep reinforced model for abstractive summarization. Romain Paulus, Caiming Xiong, Richard Socher, 2017</p>
<p>Reinforced clarification question generation with defeasibility rewards for disambiguating social and moral situations. Valentina Pyatkin, Jena D Hwang, Vivek Srikumar, Ximing Lu, Liwei Jiang, Yejin Choi, Chandra Bhagavatula, 2022</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, arXiv:2304.083542023arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Squad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.052502016arXiv preprint</p>
<p>Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>High-dimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, 2018</p>
<p>Interactive reinforcement learning for task-oriented dialogue management. Pararth Shah, Dilek Hakkani-Tur, Larry Heck, 2016</p>
<p>Language models that seek for knowledge: Modular search &amp; generation for dialogue and prompt completion. Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, Jason Weston, 2022a</p>
<p>Browser assisted question-answering with human feedbackJason Weston. Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, 2022bMelanie Kambadur, andBlenderbot 3: a deployed conversational agent that continually learns to responsibly engage</p>
<p>Reinforcement learning for spoken dialogue systems. Satinder Singh, Michael Kearns, Diane Litman, Marilyn Walker, Advances in neural information processing systems. 121999</p>
<p>Offline rl for natural language generation with implicit language q learning. Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine, arXiv:2206.118712022aarXiv preprint</p>
<p>Context-aware language modeling for goal-oriented dialogue systems. Charlie Snell, Sherry Yang, Justin Fu, Yi Su, Sergey Levine, 10.18653/v1/2022.findings-naacl.181Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational LinguisticsJuly 2022b</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Ng, Christopher Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 202033</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano, Learning to summarize from human feedback. 2022</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Controllable neural story plot generation via reward shaping. Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J Martin, Animesh Mehta, Brent Harrison, Mark O Riedl, 10.24963/ijcai.2019/829Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organizationaug 2019</p>
<p>Parallel data, tools and interfaces in OPUS. Jörg Tiedemann, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12). the Eighth International Conference on Language Resources and Evaluation (LREC'12)Istanbul, TurkeyEuropean Language Resources Association (ELRAMay 2012</p>
<p>Introduction to the conll-2003 shared task: Language-independent named entity recognition. Erik F , Tjong Kim, Sang , Fien De Meulder, Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003. the seventh conference on Natural language learning at HLT-NAACL 200320034</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning. Siddharth Verma, Justin Fu, Mengjiao Yang, Sergey Levine, 2022</p>
<p>Tl; dr: Mining reddit to learn automatic summarization. Michael Völske, Martin Potthast, Shahbaz Syed, Benno Stein, Proceedings of the Workshop on New Frontiers in Summarization. the Workshop on New Frontiers in Summarization2017</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, International Conference on Learning Representations. 2019</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022aarXiv preprint</p>
<p>Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022b</p>
<p>Codet5+: Open code large language models for code understanding and generation. Yue Wang, Hung Le, Akhilesh Deepak Gotmare, D Q Nghi, Junnan Bui, Steven C H Li, Hoi, 2023</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, 2022</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, arXiv:1804.081982018arXiv preprint</p>
<p>Jeff Wu, Long Ouyang, M Daniel, Nisan Ziegler, Ryan Stiennon, Jan Lowe, Paul Leike, Christiano, arXiv:2109.10862Recursively summarizing books with human feedback. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
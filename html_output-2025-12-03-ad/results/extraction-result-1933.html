<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1933 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1933</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1933</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-280338032</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.21450v1.pdf" target="_blank">Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Vision Language Navigation (VLN) typically requires agents to navigate to specified objects or remote regions in unknown scenes by obeying linguistic commands. Such tasks require organizing historical visual observations for linguistic grounding, which is critical for long-sequence navigational decisions. However, current agents suffer from overly detailed scene representation and ambiguous vision-language alignment, which weaken their comprehension of navigation-friendly high-level scene priors and easily lead to behaviors that violate linguistic commands. To tackle these issues, we propose a navigation policy by recursively summarizing along-the-way visual perceptions, which are adaptively aligned with commands to enhance linguistic grounding. In particular, by structurally modeling historical trajectories as compact neural grids, several Recursive Visual Imagination (RVI) techniques are proposed to motivate agents to focus on the regularity of visual transitions and semantic scene layouts, instead of dealing with misleading geometric details. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to align the learned situational memories with different linguistic components purposefully. Such fine-grained semantic matching facilitates the accurate anticipation of navigation actions and progress. Our navigation policy outperforms the state-of-the-art methods on the challenging VLN-CE and ObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1933.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1933.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RVI+ALG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive Visual Imagination + Adaptive Linguistic Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>End-to-end VLN policy that (1) summarizes historical perceptions into a compact Implicit Scene Representation (ISR) via Recursive Visual Imagination (view imagination, scene-layout imagination, and visual-semantic prediction) and (2) performs fine-grained vision-language grounding with Adaptive Linguistic Grounding (ALG) that decouples instructions and aligns instruction components to ISR tokens positionally and semantically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RVI+ALG (ISR-based VLN policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent encodes panoramic RGB-D (and pose/action) into an observation token o_t using frozen visual encoders; historical observations are summarized into a fixed-size h×w grid of d-dimensional neural-grid tokens constituting an Implicit Scene Representation (ISR). A multi-layer transformer ingests neural-grid tokens plus current observation and performs cross-modal fusion with tokenized instructions. Recursive Visual Imagination (RVI) contains (i) View Imagination (VI) that queries ISR for features at sampled poses and trains a contrastive + KL objective to capture future-frame distributions, (ii) Scene Layout Imagination (SLI) that predicts egocentric semantic local maps from ISR (BCE loss), and (iii) Visual Semantic Prediction (VSP) that predicts object categories & area ratios from observations. Adaptive Linguistic Grounding (ALG) syntactically decouples instructions into landmarks/scenes/actions/orientations/others, computes instruction weights via a progress-tracking MLP, then uses the last cross-modal attention matrix as an affinity to map language tokens to neural-grid tokens; position alignment is trained with BCE on predicted token distribution and semantic alignment uses a contrastive loss (L_SA) between mean neural-grid features and decoupled token features. The network is pretrained with behavior cloning + auxiliary RVI losses and fine-tuned with DAgger.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Frozen CLIP ResNet50 (for RGB panoramic encoding) and a ResNet18 pre-trained for PointNav (for depth/auxiliary encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>CLIP ResNet50 (Radford et al. 2021 as cited in paper) and ResNet18 pretrained in PointNav (Wijmans et al. 2019b); paper does not report additional new large-scale pretraining datasets beyond citing these encoders</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Transformer cross-modal attention between language tokens and ISR neural-grid tokens, with Adaptive Linguistic Grounding: instruction decoupling into components, progress-weighted token importance, positional alignment using row-wise max-pooled attention affinity (supervised with BCE), and semantic contrastive alignment (L_SA) between language component features and mean neural-grid features.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Scene-level implicit representation (fixed-size neural-grid tokens summarizing trajectory) complemented by panoramic view features and supervised egocentric semantic maps (local map pixels) — i.e., high-level scene/landmark tokens rather than raw per-pixel geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit neural grids with positional embeddings and explicit pose encoding (x, y, sinθ, cosθ); supervised with egocentric semantic map predictions derived from depth + segmentation (SLI). Not an explicit voxel/TSR reconstruction — spatial relations are implicit in neural-grid layout and positional encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-language navigation (VLN) / Object-goal navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>R2R-CE (VLN-CE) and Habitat ObjectNav (MP3D)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (Habitat / MP3D) using egocentric panoramic RGB-D views</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>OSR (oracle success rate), SR (success rate), SPL (success weighted by path length); ObjectNav metrics (SR, SPL, DTS) also reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>R2R-CE Val Unseen reported as OSR 67%, SR 59%, SPL 50% (paper's best model). Paper reports state-of-the-art on MP3D ObjectNav (val) compared to listed baselines but exact MP3D numeric summary in main text is not clearly tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation row with no auxiliary RVI or ALG components reports OSR 58%, SR 49%, SPL 43% (this is the all-components-removed baseline in the paper's ablation table).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Full model (RVI + ALG + progress tracking) improves from baseline OSR 58% -> 67% (+9 percentage points), SR 49% -> 59% (+10 pp), SPL 43% -> 50% (+7 pp) on R2R-CE Val-Unseen (comparison between the no-modules baseline and full model in the ablation table).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper explicitly identifies two perception/grounding bottlenecks: (1) redundant, overly-detailed scene representations (e.g., dense semantic grids or feature fields) that provide misleading geometric/texture details and impede disentangled token-level vision-language alignment; (2) ambiguous vision-language alignment when using standard sentence-level cross-attention, making it difficult for the transformer to match instruction tokens to the correct visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative and diagnostic analysis attribute failures to ambiguous semantic alignment (leading to deviation from instructions) and to insufficient progress-tracking: ablations show removing progress tracking (L_Pro) or position/semantic alignments reduces OSR/SR/SPL (e.g., presence of ALG + progress tracking yields up to ≈+4 OSR over same model without progress tracking). The paper does not provide precise error-frequency percentages per failure mode beyond the ablation metric drops.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Pretrain on offline collected trajectories and then fine-tune with DAgger to reduce offline-to-online distribution mismatch; fine-tuning uses pseudo-labels sampled from agent predictions during online exploration. No explicit domain-adaptation or real-to-sim transfer layers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Paper uses frozen visual encoders (CLIP ResNet50 and ResNet18) and does not report a direct comparison between frozen vs. fine-tuned encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper does not study scaling of pretraining data size on grounding quality; it uses a two-stage pretraining (representation stage then alignment stage) on collected trajectories but provides no ablation on pretraining scale.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention inside a multi-layer transformer between neural-grid tokens (ISR) + current observation tokens and language tokens; additional MLPs for progress tracking and position-label prediction; position alignment uses softmax/BCE over dot-multiplied position labels and instruction weights.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No explicit sample-efficiency numbers (e.g., needed demonstration counts) reported; training used large collections of trajectories from R2R-CE and MP3D and two-stage pretraining followed by DAgger fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>1) Condensing historical observations into a compact ISR and encouraging imaginations of future views/layouts reduces distracting visual detail and produces more navigation-friendly priors. 2) Decoupling instructions into semantic components and aligning them positionally and semantically with ISR (ALG) produces clearer vision-language matching and substantially improves navigation metrics (OSR/SR/SPL). 3) Progress-aware weighting (VLN progress tracking) is necessary for position/semantic alignment to be effective. 4) RVI auxiliary tasks (contrastive view imagination, KL-based stochastic future latent, and egocentric map prediction) each contribute to stronger representations (validated by ablations).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1933.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1933.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ISR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit Scene Representation (ISR) — neural-grid situational memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fixed-size h×w grid of d-dimensional learnable neural-grid tokens that summarize along-the-way observations and poses; updated at each timestep via transformer interactions with the current observation to form a compact scene memory used for grounding and imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Implicit Scene Representation (ISR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ISR is initialized as position-based learnable embeddings and updated with concatenated observation tokens and grid tokens passed through a multi-layer transformer. ISR size (h,w) is a hyperparameter independent of trajectory length, keeping computational cost fixed; grids have positional embeddings encoding relative offset from center and are queried by view-imagination and ALG alignment procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (ISR consumes encoded visual tokens from frozen encoders: CLIP ResNet50 and ResNet18)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>See parent model (CLIP ResNet50 and ResNet18 citations in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>ISR tokens are the targets of cross-modal attention with language tokens; last-layer attention affinity (language→ISR) is used as positional alignment; ISR mean features are used for semantic contrastive alignment with decoupled textual components.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>High-level scene / situational memory (neural-grid tokens summarizing panorama-level observations) — not per-pixel geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit positional layout encoded via the fixed grid coordinates and positional embeddings; supervised indirectly via SLI egocentric semantic map prediction and pose vectors included in observation encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>R2R-CE, ObjectNav (MP3D)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic egocentric panoramic RGB-D in simulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not separately evaluated as isolated model; contributes to overall SR/OSR/SPL reported for full model</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>ISR was introduced to avoid the perception bottleneck of redundant explicit SRs (semantic grids / feature fields) that encode excessive geometric/textural details that confuse token-level alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Not separately quantified; paper argues explicit SRs lead to ambiguous attention and grounding failures whereas ISR mitigates this.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>ISR itself trained via pretraining on collected trajectories and then fine-tuned with DAgger; no additional domain-adaptation for ISR reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Transformer interactions between ISR tokens and current observation tokens; ISR tokens become part of cross-modal fusion with language.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Compact, fixed-size ISR tokens help reduce computation and reduce noisy geometric details, enabling clearer component-wise alignment between language and scene memory and improving VLN performance when coupled with ALG and RVI.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1933.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1933.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Linguistic Grounding (ALG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense vision-language alignment technique that (1) syntactically decouples instructions into landmarks/scenes/actions/orientations/others, (2) computes progress-aware instruction weights, and (3) aligns decoupled language components positionally and semantically to ISR's neural-grid tokens using attention-affinity matching, BCE position supervision, and contrastive semantic alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adaptive Linguistic Grounding (ALG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ALG first parses each instruction into five component types (landmarks, scenes, actions, orientations, others) using an off-the-shelf syntactic parser (and optionally LLM parsing). Cross-modal fused word tokens are aggregated by component via dot-product with position labels. A small MLP predicts instruction progress weights W_t; the affinity (attention) matrix from the last cross-attention layer is row-wise max-pooled to select the most-attended neural grid for each language token, forming candidate mt_i. Position alignment predicts a soft distribution over tokens from mean(mt_i) and is supervised with BCE against ground-truth token positions weighted by progress W_t. Semantic alignment applies a contrastive loss L_SA between the mean neural-grid feature and positive decoupled token features (landmarks/scenes) while pushing away negative components.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (uses visual features from parent pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Explicit component-wise grounding via (a) using attention affinity to map language tokens to ISR grids and (b) two supervisory signals: positional BCE over predicted token distributions and semantic contrastive loss between grid features and decoupled token features.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Component-level language ↔ scene-level (ISR) alignment (landmark and scene components grounded to specific neural-grid tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Uses the positional layout of ISR neural grids for position alignment; progress-weighted textual position labels are used as supervision to encourage spatially-aware grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>R2R-CE, ObjectNav (MP3D)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (egocentric panoramic RGB-D)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>OSR, SR, SPL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablations show position alignment (L_P A) and semantic alignment (L_S A) and progress tracking (L_Pro) together give notable gains; removing these reduces Val-Unseen OSR from 67% (full) to as low as 58% (no modules). Exact per-component numeric deltas are reported in the paper's ablation table.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>ALG + progress tracking contributes roughly +4 to +9 percentage points in OSR/SR depending on what baseline is compared (see ablation table: full model vs no-modules baseline yields +9 OSR/+10 SR/+7 SPL).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>ALG is motivated by the failure of sentence-level cross-attention to disentangle token-level alignments in redundant SRs; ambiguous attention matrices produce poor grounding unless instruction tokens are decoupled and explicitly aligned.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper reports that without progress-aware position/semantic alignment, the agent often fails to focus on the correct instruction components leading to navigation deviations; ablation rows quantify metric drops when ALG components are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>ALG training is done during pretraining and fine-tuning; no extra domain adaptation specific to ALG beyond DAgger fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Uses cross-attention affinity from transformer fusion as the basis for mapping language tokens to ISR tokens; supervision applied on top (BCE for position, contrastive for semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Decoupling instructions into semantic components and performing progress-aware positional and contrastive semantic alignment with ISR tokens yields clearer vision-language grounding and materially improves navigation metrics; progress tracking is required to avoid misalignment and degraded performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grounded entity-landmark adaptive pre-training for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>Grid memory map for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>Dreamwalker: Mental planning for continuous vision-language navigation <em>(Rating: 2)</em></li>
                <li>Evolving topological planning for vision-language navigation in continuous environments <em>(Rating: 2)</em></li>
                <li>GELA <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1933",
    "paper_id": "paper-280338032",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "RVI+ALG",
            "name_full": "Recursive Visual Imagination + Adaptive Linguistic Grounding",
            "brief_description": "End-to-end VLN policy that (1) summarizes historical perceptions into a compact Implicit Scene Representation (ISR) via Recursive Visual Imagination (view imagination, scene-layout imagination, and visual-semantic prediction) and (2) performs fine-grained vision-language grounding with Adaptive Linguistic Grounding (ALG) that decouples instructions and aligns instruction components to ISR tokens positionally and semantically.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RVI+ALG (ISR-based VLN policy)",
            "model_description": "Agent encodes panoramic RGB-D (and pose/action) into an observation token o_t using frozen visual encoders; historical observations are summarized into a fixed-size h×w grid of d-dimensional neural-grid tokens constituting an Implicit Scene Representation (ISR). A multi-layer transformer ingests neural-grid tokens plus current observation and performs cross-modal fusion with tokenized instructions. Recursive Visual Imagination (RVI) contains (i) View Imagination (VI) that queries ISR for features at sampled poses and trains a contrastive + KL objective to capture future-frame distributions, (ii) Scene Layout Imagination (SLI) that predicts egocentric semantic local maps from ISR (BCE loss), and (iii) Visual Semantic Prediction (VSP) that predicts object categories & area ratios from observations. Adaptive Linguistic Grounding (ALG) syntactically decouples instructions into landmarks/scenes/actions/orientations/others, computes instruction weights via a progress-tracking MLP, then uses the last cross-modal attention matrix as an affinity to map language tokens to neural-grid tokens; position alignment is trained with BCE on predicted token distribution and semantic alignment uses a contrastive loss (L_SA) between mean neural-grid features and decoupled token features. The network is pretrained with behavior cloning + auxiliary RVI losses and fine-tuned with DAgger.",
            "visual_encoder_type": "Frozen CLIP ResNet50 (for RGB panoramic encoding) and a ResNet18 pre-trained for PointNav (for depth/auxiliary encoding)",
            "visual_encoder_pretraining": "CLIP ResNet50 (Radford et al. 2021 as cited in paper) and ResNet18 pretrained in PointNav (Wijmans et al. 2019b); paper does not report additional new large-scale pretraining datasets beyond citing these encoders",
            "grounding_mechanism": "Transformer cross-modal attention between language tokens and ISR neural-grid tokens, with Adaptive Linguistic Grounding: instruction decoupling into components, progress-weighted token importance, positional alignment using row-wise max-pooled attention affinity (supervised with BCE), and semantic contrastive alignment (L_SA) between language component features and mean neural-grid features.",
            "representation_level": "Scene-level implicit representation (fixed-size neural-grid tokens summarizing trajectory) complemented by panoramic view features and supervised egocentric semantic maps (local map pixels) — i.e., high-level scene/landmark tokens rather than raw per-pixel geometry.",
            "spatial_representation": "Implicit neural grids with positional embeddings and explicit pose encoding (x, y, sinθ, cosθ); supervised with egocentric semantic map predictions derived from depth + segmentation (SLI). Not an explicit voxel/TSR reconstruction — spatial relations are implicit in neural-grid layout and positional encodings.",
            "embodied_task_type": "Vision-language navigation (VLN) / Object-goal navigation",
            "embodied_task_name": "R2R-CE (VLN-CE) and Habitat ObjectNav (MP3D)",
            "visual_domain": "Photorealistic simulation (Habitat / MP3D) using egocentric panoramic RGB-D views",
            "performance_metric": "OSR (oracle success rate), SR (success rate), SPL (success weighted by path length); ObjectNav metrics (SR, SPL, DTS) also reported in paper",
            "performance_value": "R2R-CE Val Unseen reported as OSR 67%, SR 59%, SPL 50% (paper's best model). Paper reports state-of-the-art on MP3D ObjectNav (val) compared to listed baselines but exact MP3D numeric summary in main text is not clearly tabulated.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation row with no auxiliary RVI or ALG components reports OSR 58%, SR 49%, SPL 43% (this is the all-components-removed baseline in the paper's ablation table).",
            "grounding_improvement": "Full model (RVI + ALG + progress tracking) improves from baseline OSR 58% -&gt; 67% (+9 percentage points), SR 49% -&gt; 59% (+10 pp), SPL 43% -&gt; 50% (+7 pp) on R2R-CE Val-Unseen (comparison between the no-modules baseline and full model in the ablation table).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper explicitly identifies two perception/grounding bottlenecks: (1) redundant, overly-detailed scene representations (e.g., dense semantic grids or feature fields) that provide misleading geometric/texture details and impede disentangled token-level vision-language alignment; (2) ambiguous vision-language alignment when using standard sentence-level cross-attention, making it difficult for the transformer to match instruction tokens to the correct visual features.",
            "failure_mode_analysis": "Qualitative and diagnostic analysis attribute failures to ambiguous semantic alignment (leading to deviation from instructions) and to insufficient progress-tracking: ablations show removing progress tracking (L_Pro) or position/semantic alignments reduces OSR/SR/SPL (e.g., presence of ALG + progress tracking yields up to ≈+4 OSR over same model without progress tracking). The paper does not provide precise error-frequency percentages per failure mode beyond the ablation metric drops.",
            "domain_shift_handling": "Pretrain on offline collected trajectories and then fine-tune with DAgger to reduce offline-to-online distribution mismatch; fine-tuning uses pseudo-labels sampled from agent predictions during online exploration. No explicit domain-adaptation or real-to-sim transfer layers reported.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Paper uses frozen visual encoders (CLIP ResNet50 and ResNet18) and does not report a direct comparison between frozen vs. fine-tuned encoders.",
            "pretraining_scale_effect": "Paper does not study scaling of pretraining data size on grounding quality; it uses a two-stage pretraining (representation stage then alignment stage) on collected trajectories but provides no ablation on pretraining scale.",
            "fusion_mechanism": "Cross-attention inside a multi-layer transformer between neural-grid tokens (ISR) + current observation tokens and language tokens; additional MLPs for progress tracking and position-label prediction; position alignment uses softmax/BCE over dot-multiplied position labels and instruction weights.",
            "sample_efficiency": "No explicit sample-efficiency numbers (e.g., needed demonstration counts) reported; training used large collections of trajectories from R2R-CE and MP3D and two-stage pretraining followed by DAgger fine-tuning.",
            "key_findings_grounding": "1) Condensing historical observations into a compact ISR and encouraging imaginations of future views/layouts reduces distracting visual detail and produces more navigation-friendly priors. 2) Decoupling instructions into semantic components and aligning them positionally and semantically with ISR (ALG) produces clearer vision-language matching and substantially improves navigation metrics (OSR/SR/SPL). 3) Progress-aware weighting (VLN progress tracking) is necessary for position/semantic alignment to be effective. 4) RVI auxiliary tasks (contrastive view imagination, KL-based stochastic future latent, and egocentric map prediction) each contribute to stronger representations (validated by ablations).",
            "uuid": "e1933.0"
        },
        {
            "name_short": "ISR",
            "name_full": "Implicit Scene Representation (ISR) — neural-grid situational memory",
            "brief_description": "A fixed-size h×w grid of d-dimensional learnable neural-grid tokens that summarize along-the-way observations and poses; updated at each timestep via transformer interactions with the current observation to form a compact scene memory used for grounding and imagination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Implicit Scene Representation (ISR)",
            "model_description": "ISR is initialized as position-based learnable embeddings and updated with concatenated observation tokens and grid tokens passed through a multi-layer transformer. ISR size (h,w) is a hyperparameter independent of trajectory length, keeping computational cost fixed; grids have positional embeddings encoding relative offset from center and are queried by view-imagination and ALG alignment procedures.",
            "visual_encoder_type": "N/A (ISR consumes encoded visual tokens from frozen encoders: CLIP ResNet50 and ResNet18)",
            "visual_encoder_pretraining": "See parent model (CLIP ResNet50 and ResNet18 citations in paper)",
            "grounding_mechanism": "ISR tokens are the targets of cross-modal attention with language tokens; last-layer attention affinity (language→ISR) is used as positional alignment; ISR mean features are used for semantic contrastive alignment with decoupled textual components.",
            "representation_level": "High-level scene / situational memory (neural-grid tokens summarizing panorama-level observations) — not per-pixel geometry.",
            "spatial_representation": "Implicit positional layout encoded via the fixed grid coordinates and positional embeddings; supervised indirectly via SLI egocentric semantic map prediction and pose vectors included in observation encodings.",
            "embodied_task_type": "Vision-language navigation",
            "embodied_task_name": "R2R-CE, ObjectNav (MP3D)",
            "visual_domain": "Photorealistic egocentric panoramic RGB-D in simulation",
            "performance_metric": "Not separately evaluated as isolated model; contributes to overall SR/OSR/SPL reported for full model",
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "ISR was introduced to avoid the perception bottleneck of redundant explicit SRs (semantic grids / feature fields) that encode excessive geometric/textural details that confuse token-level alignment.",
            "failure_mode_analysis": "Not separately quantified; paper argues explicit SRs lead to ambiguous attention and grounding failures whereas ISR mitigates this.",
            "domain_shift_handling": "ISR itself trained via pretraining on collected trajectories and then fine-tuned with DAgger; no additional domain-adaptation for ISR reported.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Transformer interactions between ISR tokens and current observation tokens; ISR tokens become part of cross-modal fusion with language.",
            "sample_efficiency": null,
            "key_findings_grounding": "Compact, fixed-size ISR tokens help reduce computation and reduce noisy geometric details, enabling clearer component-wise alignment between language and scene memory and improving VLN performance when coupled with ALG and RVI.",
            "uuid": "e1933.1"
        },
        {
            "name_short": "ALG",
            "name_full": "Adaptive Linguistic Grounding (ALG)",
            "brief_description": "A dense vision-language alignment technique that (1) syntactically decouples instructions into landmarks/scenes/actions/orientations/others, (2) computes progress-aware instruction weights, and (3) aligns decoupled language components positionally and semantically to ISR's neural-grid tokens using attention-affinity matching, BCE position supervision, and contrastive semantic alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Adaptive Linguistic Grounding (ALG)",
            "model_description": "ALG first parses each instruction into five component types (landmarks, scenes, actions, orientations, others) using an off-the-shelf syntactic parser (and optionally LLM parsing). Cross-modal fused word tokens are aggregated by component via dot-product with position labels. A small MLP predicts instruction progress weights W_t; the affinity (attention) matrix from the last cross-attention layer is row-wise max-pooled to select the most-attended neural grid for each language token, forming candidate mt_i. Position alignment predicts a soft distribution over tokens from mean(mt_i) and is supervised with BCE against ground-truth token positions weighted by progress W_t. Semantic alignment applies a contrastive loss L_SA between the mean neural-grid feature and positive decoupled token features (landmarks/scenes) while pushing away negative components.",
            "visual_encoder_type": "N/A (uses visual features from parent pipeline)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Explicit component-wise grounding via (a) using attention affinity to map language tokens to ISR grids and (b) two supervisory signals: positional BCE over predicted token distributions and semantic contrastive loss between grid features and decoupled token features.",
            "representation_level": "Component-level language ↔ scene-level (ISR) alignment (landmark and scene components grounded to specific neural-grid tokens).",
            "spatial_representation": "Uses the positional layout of ISR neural grids for position alignment; progress-weighted textual position labels are used as supervision to encourage spatially-aware grounding.",
            "embodied_task_type": "Vision-language navigation",
            "embodied_task_name": "R2R-CE, ObjectNav (MP3D)",
            "visual_domain": "Photorealistic simulation (egocentric panoramic RGB-D)",
            "performance_metric": "OSR, SR, SPL",
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablations show position alignment (L_P A) and semantic alignment (L_S A) and progress tracking (L_Pro) together give notable gains; removing these reduces Val-Unseen OSR from 67% (full) to as low as 58% (no modules). Exact per-component numeric deltas are reported in the paper's ablation table.",
            "grounding_improvement": "ALG + progress tracking contributes roughly +4 to +9 percentage points in OSR/SR depending on what baseline is compared (see ablation table: full model vs no-modules baseline yields +9 OSR/+10 SR/+7 SPL).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "ALG is motivated by the failure of sentence-level cross-attention to disentangle token-level alignments in redundant SRs; ambiguous attention matrices produce poor grounding unless instruction tokens are decoupled and explicitly aligned.",
            "failure_mode_analysis": "Paper reports that without progress-aware position/semantic alignment, the agent often fails to focus on the correct instruction components leading to navigation deviations; ablation rows quantify metric drops when ALG components are removed.",
            "domain_shift_handling": "ALG training is done during pretraining and fine-tuning; no extra domain adaptation specific to ALG beyond DAgger fine-tuning.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Uses cross-attention affinity from transformer fusion as the basis for mapping language tokens to ISR tokens; supervision applied on top (BCE for position, contrastive for semantics).",
            "sample_efficiency": null,
            "key_findings_grounding": "Decoupling instructions into semantic components and performing progress-aware positional and contrastive semantic alignment with ISR tokens yields clearer vision-language grounding and materially improves navigation metrics; progress tracking is required to avoid misalignment and degraded performance.",
            "uuid": "e1933.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grounded entity-landmark adaptive pre-training for vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "Grid memory map for vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "Dreamwalker: Mental planning for continuous vision-language navigation",
            "rating": 2
        },
        {
            "paper_title": "Evolving topological planning for vision-language navigation in continuous environments",
            "rating": 2
        },
        {
            "paper_title": "GELA",
            "rating": 1
        }
    ],
    "cost": 0.0184585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation
29 Jul 2025</p>
<p>Bolei Chen boleichen@csu.edu.cn 
School of Computer Science and Engineering
Central South University</p>
<p>Jiaxu Kang jxkang@csu.edu.cn 
School of Computer Science and Engineering
Central South University</p>
<p>Yifei Wang yifeiwang@csu.edu.cn 
School of Computer Science and Engineering
Central South University</p>
<p>Ping Zhong ping.zhong@csu.edu.cn 
School of Computer Science and Engineering
Central South University</p>
<p>Qi Wu 
Australian Institute of Machine Learning
The University of Adelaide</p>
<p>Jianxin Wang jxwang@mail.csu.edu.cn 
School of Computer Science and Engineering
Central South University</p>
<p>Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation
29 Jul 2025A0A3ACE01BFEDB1D53C4F16AA81D647FarXiv:2507.21450v1[cs.CV]
Vision Language Navigation (VLN) typically requires agents to navigate to specified objects or remote regions in unknown scenes by obeying linguistic commands.Such tasks require organizing historical visual observations for linguistic grounding, which is critical for long-sequence navigational decisions.However, current agents suffer from overly detailed scene representation and ambiguous vision-language alignment, which weaken their comprehension of navigationfriendly high-level scene priors and easily lead to behaviors that violate linguistic commands.To tackle these issues, we propose a navigation policy by recursively summarizing along-the-way visual perceptions, which are adaptively aligned with commands to enhance linguistic grounding.In particular, by structurally modeling historical trajectories as compact neural grids, several Recursive Visual Imagination (RVI) techniques are proposed to motivate agents to focus on the regularity of visual transitions and semantic scene layouts, instead of dealing with misleading geometric details.Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to align the learned situational memories with different linguistic components purposefully.Such fine-grained semantic matching facilitates the accurate anticipation of navigation actions and progress.Our navigation policy outperforms the state-of-the-art methods on the challenging VLN-CE and ObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.</p>
<p>Introduction</p>
<p>Interacting with agents through natural language is a longterm goal of embodied artificial intelligence as it is potentially the most intuitive way for human-robot communication.The emerging research on Vision Language Navigation (VLN) (Gervet et al. 2022;An et al. 2024) is along this path, which requires agents to navigate to specified object instances or remote areas in unfamiliar 3D scenes by following linguistic instructions.Existing VLN work has made great advances in Scene Representation (SR) (Wang et al. 2023c;Hong et al. 2023a;An et al. 2024), vision-language alignment (Cui et al. 2023;Cheng et al. 2022), and auxiliary tasks (Wu et al. 2024;Qiao et al. 2023) for pre-training.They typically organize historical visual observations as structural SRs, which are further cross-modally aligned with linguistic commands to track navigation progress and enhance navigation decision-making.</p>
<p>Some methods (Zhang et al. 2023;Yokoyama et al. 2024;Wang et al. 2023c) represent scenes by projecting raw or encoded visual features into bird's-eye-view maps or 3D feature fields to preserve fine-grained scene geometries and visual contexts.Despite promising progress has been made, these SRs provide overly detailed structural and semantic priors, posing challenges for learning accurate vision-action mappings using neural networks.Human-like agents typically establish high-level awareness of landmark semantics and spatial relationships of surrounding objects, rather than focusing on misleading geometric details that are irrelevant to navigation.For example, the agent in Fig. 1 should focus on the sofa landmarks and the visual signals that trigger the left-turn action, rather than the objects' visual textures and the hallway's geometric structure.Research in behavioral psychology (Tolman 1948;O'Keefe and Burgess 1996) has shown that many animals maintain spatial representations of their scenes during navigation, even if scene details are not fully stored.Inspired by this, some other methods (An et al. 2024;Yin et al. 2024a) propose to abstract the environmental layouts into visual feature-based Topological Scene Representations (TSR) to facilitate linguistic grounding or balance exploration and exploitation during navigation.Although TSR refines the scene layout, TSR's nodes still store raw or encoded visual textures that are overly detailed.Moreover, TSR discards continuous semantic relations between nodes (Chen et al. 2023).</p>
<p>Redundant SRs can impede linguistic grounding, potentially resulting in behaviors contradicting navigation instructions.In other words, redundant scene details that are irrelevant to VLN can disrupt effective linguistic grounding, leading to ambiguous or even erroneous vision-language alignment.Current methods (Wang et al. 2023c;Hong et al. 2023a;An et al. 2024) attempt to align instruction tokens with SRs through standard cross-modal attention techniques.In this case, it is extremely challenging to train a transformer to achieve disentanglement and match each instruction token to the correct visual feature in a redundant SR.Such an ambiguous semantic alignment impairs the agent's insight into the navigation progress and makes it easy to deviate from the correct trajectory.</p>
<p>To tackle these issues, we propose a VLN policy by organizing along-the-way observations as an Implicit Scene Representation (ISR) through Recursive Visual Imagination (RVI), including view imagination and scene layout imagination.Technically, we advocate modeling historical navigation trajectories (including the agent's visual sensing, poses, and navigational actions) as compact neural grids, rather than preserving explicit scene geometric details.We treat SR learning as a sequence modeling problem and train a joint state-action transformer over entire trajectories under the behavior cloning framework (Hu et al. 2024).Unlike classical VLN methods (Chen et al. 2021a;Wang et al. 2023b), the number of neural grids in our ISR is a hyperparameter that does not grow with trajectory length or scene scale.Therefore, the number of ISR tokens input to our model is fixed, which does not increase the computational cost.Then, the learned ISR is densely aligned with navigation commands via a novel Adaptive Linguistic Grounding (ALG) technique to make the vision-language matching clear.</p>
<p>To derive navigation-friendly high-level scene priors from an ISR, RVI motivates agents to focus on the regularity of visual transitions and semantic scene layouts while ignoring irrelevant visual contexts.In particular, view imagination motivates agents to learn the distribution of future visual frames while enhancing their sensitivity to historical visual changes.Due to the inherent uncertainty in future frame prediction and the diversity of navigational actions, a single current frame can generate multiple potential futures.Therefore, our VLN agent is encouraged to summarize the regularity of visual signal changes instead of deterministically rendering future visual features.Scene layout imagination is designed to enhance the agent's insights into the surrounding landmark semantics and their relative positional relations.Therefore, our core idea is to explicitly endow the agent with the thinking necessary for VLN: (1) recalling the past and predicting the future and (2) imagining the current semantic layout of the surroundings.</p>
<p>Research in brain science (Sokolov, Miall, and Ivry 2017;Vargha-Khadem et al. 1997) has shown that the cerebellum and hippocampus regulate motion and memory recall through neural structures and feature representations, respectively.Inspired by this, the ALG technique is proposed to adaptively align ISR's neural grids with different linguistic components for vision-language matching.For example, left turn action signals and sofa associated situational memories should be governed by separate neural grids, as shown in Fig. 1.To realize this idea, the agent first decouples a navigation instruction into different components, including landmarks, scenes, actions, and orientations, through syntactic analysis.Then, a self-supervised learning method is proposed to adaptively align these components with appropriate action signals or scene memories at the positional and semantic levels.</p>
<p>During experiments, sufficient comparative studies reflect that our approach incorporating RVI and ALG achieves state-of-the-art performance on two VLN tasks.Adequate ablation studies validate the effectiveness of the individual modules of our method.In general, the main contributions of this paper are as follows: (1) Two novel RVI techniques are designed for ISR learning that can empower agents with the essential thinking for VLN.(2) A novel ALG technique is proposed to motivate the agent to adaptively activate different action signals or scene memories based on different linguistic components.(3) Sufficient comparative and ablative studies on challenging VLN tasks demonstrate the superiority of our method.The experimental code will be publicly available after anonymous review.</p>
<p>Related Work</p>
<p>Scene Representation for VLN.Effective SRs are essential for the long-sequence decision-making and visioninstruction alignment of VLN.Early efforts (Dang et al. 2022;Tan et al. 2024) typically employ recurrent neural networks to model SR as a fixed-size feature vector, which may be inefficient in modeling sophisticated visual features and capturing the long-term feature dependence in historical trajectories.Due to the strong expression power of transformer (Hu et al. 2024), transformer-based models (Qiao et al. 2023;Wu et al. 2024;Cui et al. 2023;Wang et al. 2023c;Lin et al. 2022) have manifested their potential in VLN.Among them, architecture enhancement methods (Lin et al. 2022;Chen et al. 2021b;Hong et al. 2021) consider how to apply the powerful transformer structure to VLN under the reinforcement learning framework, facilitating more precise modeling of scenes.Trajectory optimization methods (Wang et al. 2023c;Qiao et al. 2023;Cui et al. 2023;Wu et al. 2024) treat VLN tasks as sequence modeling problems and train joint state-action models over entire trajectories under the behavior cloning framework.</p>
<p>Alternatively, some other methods (Wang et al. 2023b;An et al. 2023;Wang et al. 2023c) achieve SR by projecting encoded visual features into egocentric semantic maps or topological graphs, which exhaustively retain the visual contexts and scene geometries.Although these methods achieve promising results, their SRs contain redundant information.We argue that SR should adequately represent the high-level scene-understanding mindsets required for VLN, rather than providing agents with excessive and misleading scene details.Inspired by the trajectory optimization methods (Wu et al. 2024;Ehsani et al. 2024), we propose an ISR by modeling historical observations as compact neural grids.Unlike existing methods (Wang et al. 2023c;Wu et al. 2024;Chen et al. 2021b;Hong et al. 2021), we condense and refine the valuable historical information before feeding it into the cross-modal fusion module.In other words, the ISR is learned to emphasize the agent's insights into high-level visual signals and semantic scene layouts, which is distinct from existing SR modeling.</p>
<p>Linguistic Grounding for VLN.Fine-grained linguistic grounding is critical for instruction-following action prediction and VLN progress tracking.However, existing methods (Wang et al. 2023c;An et al. 2024;Georgakis et al. 2022;An et al. 2023) coarsely align all instruction tokens with the SR at the sentence level, which impairs the agent's insight into the navigation progress.Some other studies (Wu et al. 2024;Qiao et al. 2023) adopt auxiliary tasks to sequentially align historical observations with instructions during the pre-training phase.However, the positional and semantic alignments between historical observations and instruction tokens are still ambiguous.To mitigate these issues, alternative methods (Cui et al. 2023;Cheng et al. 2022) decouple navigation instructions into actions and landmarks and match them with entities in the panoramic images at a fine-grained level.However, given the diversity of scenes and the complexity of instructions, it is inadequate to bridge the vision-language gap using only navigational actions and entity landmarks.</p>
<p>To address the above issues, we propose to decouple a navigation instruction into different components, including landmarks, scenes, actions, and orientations.Then, an ALG technique is proposed to achieve dense alignment between the linguistic components and the ISR at the positional and semantic levels, respectively.The ALG technique allows VLN agents to evoke different episodic memories adaptively according to different linguistic components.</p>
<p>Preliminaries</p>
<p>Problem Definition.In this work, we address the VLN tasks in 3D indoor scenes, where the agents are required to reach specified remote regions or object instances.In particular, we focus on two practical settings: VLN in Continuous Environments (VLN-CE) (Krantz et al. 2020) and Objectgoal Navigation (ObjectNav) (Gervet et al. 2022)  } K k=1 of its current location, which both contain K single view images.The VLN agent also receives an instruction with L words for each episode, which are embedded as X = {x i } L i=1 .The ObjectNav agent can observe one single RGB image I rgb t and one single depth image I depth t .In each episode, the Ob-jectNav agent is given a target category c target specified by a semantic label (e.g., a toilet).To facilitate the learning of a unified VLN framework, ObjectNav's goal is converted to "Please navigate to [c target ] and stay within 1 m of it."by using a fixed instruction template.Unless otherwise stated, we default to introducing our method under the VLN setup.</p>
<p>ISR Initialization and Updating.At timestep t, the agent's observations specifically include the panoramic RGB-D images {R t , D t }, the pose (x t , y t , θ t ), and the previous navigation action a t−1 , as shown in Fig. 2. Following existing work (Wang et al. 2023a;An et al. 2024;Wang et al. 2023c), we first perform orientation embedding for each view of the panoramic image.Then, the pre-trained CLIP ResNet50 (Radford et al. 2021) and the ResNet18 pre-trained in PointNav (Wijmans et al. 2019b)  , respectively.Notably, the visual encoders stay frozen to make the training efficient.The agent's current pose is converted into a vector (x t , y t , sinθ t , cosθ t ) before encoding.Four different linear layers are used to project the visual embeddings, the pose vector, and the previous action into the same dimension.All the features are concatenated and further added a sinusoidal positional embedding of timestep t to obtain the current observation feature o t .</p>
<p>Our ISR summarizes the historical images until timestep t as neural grids M t = [m t ij ] h×w with h × w grids.Each grid is a d-dimensional feature vector m t ij ∈ R d whose position with respect to the center is designated [i−h/2, j −w/2].As each episode starts, the neural grids M 0 are initialized using their positions m
0 ij = w 0 m +M LP ([i−h/2, j−w/2])
, where w 0 m ∈ R d is a learnable embedding.At each timestep, the neural grids are updated given the new observation o t with a differentiable function.Given the effectiveness of transformers in sequential modeling and VLN (Chen et al. 2021a), a multi-layer transformer is employed to achieve interactions among neural grid-based situational memories.We first perform positional embedding for neural grids to enhance the geometry alignment between the neural grids and the observation.Then, all the neural grids and o t are concatenated as tokens which are fed to the transformer, as shown in Fig. 2.</p>
<p>Notably, unlike the voxels for 3D scene reconstruction, we introduce the concept of a "grid" to emphasize the relative positional encoding of ISR.In the following section, we expect agents to predict local semantic maps during RVI, which requires inferring the relative positional relations between high-level semantics.In addition, we expect the grids with different positions to be aligned with the corresponding instruction components during ALG.This is inspired by the fact that the hippocampus and cerebellum, which have different relative positions in the brain, are responsible for memory and movement, respectively.</p>
<p>Methodology Recursive Visual Imagination</p>
<p>To derive high-level scene priors from ISR, RVI motivates agents to focus on the regularity of visual transitions and semantic scene layouts while ignoring irrelevant visual contexts.As shown in Fig. 3, RVI specifically includes View Imagination (VI), Scene Layout Imagination (SLI), and Visual Semantic Prediction (VSP).</p>
<p>Given a query pose, VI motivates the agent to evoke the corresponding situational memory from ISR or learn the regularity of future visual transitions.At timestep t, we randomly sample a query pose {x t ′ , y t ′ , θ t ′ } and the corresponding RGB panoramic image R t ′ from a VLN trajectory, where t ′ ∈ [0, t + k].Then, a frozen pre-trained CLIP ResNet50 and a linear layer are utilized to encode R t ′ and the query pose as v t ′ and q t ′ , respectively.As shown in Fig. 3, q t ′ is fed into the multi-layer transformer along with M t−1 and o t to query visual features about pose {x t ′ , y t ′ , θ t ′ } from ISR. Notably, we only aim to extract potential features related to the query pose from the ISR, without expecting q t ′ to affect the ISR updating.Therefore, an attention masking operation is employed to prevent M t−1 and o t from paying attention to q t ′ .The output pose embedding is fed into an Multi-Layer Perception (MLP) to predict the visual feature v q t ′ .To enhance the agent's sensitivity to historical visual changes, we use a contrastive loss to clarify the correspondence between the poses and visual features by pushing v q t ′ and v t ′ closer to each other and moving v q t ′ away from visual features at other locations in the trajectory:
… … … … … … … …LCon = 1 T T t=0 −log exp(sim(v q t ′ , v t ′ )/τ ) t+k i=1 exp(sim(v q t ′ , vi)/τ ) , (1)
where τ is a softmax temperature scaling parameter and sim(•, •) corresponds to the cosine similarity.Notably, by setting the value of k &gt; 0, the agent is motivated to imagine visual features for the future k timesteps at specific locations.To make the agent further summarize the regularity of future visual transitions, we aim to learn the distribution of future frames conditional on the current frame, rather than deterministically rendering future visual features.In particular, we employ two MLPs p ϑ and q ϑ to approximate the learned prior distribution z t ′ ∼ p ϑ (z t ′ |v q t ′ ) and the posterior distribution ẑt ′ ∼ q ϑ (ẑ t ′ |v q t ′ , v t ′ ) that captures future uncertainty, respectively.We make the prior distribution to be closer to the posterior distribution by minimizing the KL divergence, which not only enables the agent to fantasize about the future but also makes the future variable more predictable.In summary, the loss function for visual imagination is as follows:
LV F = LCon + βKL[q ϑ (z t ′ |v q t ′ , v t ′ )||p ϑ (z t ′ |v q t ′ )],(2)
where β is a loss scale hyperparameter.When
0 &lt; t ′ ≤ t, β = 0, otherwise β = 0.5 (t &lt; t ′ ≤ t + k).
SLI is designed to enhance the agent's insights into the surrounding landmark semantics and the relative positional relationships among them.Technically, an MLP is used to predict egocentric local semantic maps {M t } T t=0 from ISR, where {M t } T t=0 is pre-generated from the MP3D dataset, as shown in Fig. 3. Please see the supplementary material for more details of M t ∈ R H×W .A Binary Cross-Entropy (BCE) loss is used to measure the SLI error:
LMap = 1 T T t=0 BCE(Linear(M t ), M t ).
(3)</p>
<p>To boost VI and SLI's focus on scene semantics, VSP is used as an auxiliary task to enhance the sensitivity of the observation encoding component to visual semantics.Technically, VSP is achieved to predict the existence of each object category and the ratio occupied by the objects in the current view (if they are present) based on the observation o t , as shown in Fig. 3.We obtain the ground-truth labels from the MP3D training scenes and use the BCE loss L Sem to measure the VSP errors.Please see the supplementary material for the data collection details for pre-training.</p>
<p>Adaptive Linguistic Grounding</p>
<p>Instruction Decoupling.Human beings can wisely focus on instruction-related landmarks in the scene and scenerelated orientations in the instructions when performing VLN tasks.To emulate such abilities, we propose to decouple the instruction into different components, which are independently and adaptively aligned with ISR's neural grids, producing more discriminative and clear vision-language matching.Technically, we follow the existing work (Wu et al. 2019) to parse the instructions grammatically and decouple the instructions into five semantic components: landmarks, scenes, actions, orientations, and others.Particularly, we generate the position labels L land , L scene , L action , L ori , and L other for the component's associated words by setting each component's word positions to 1 and the rest to 0, as shown in Fig. 4. Given that large language models (Achiam et al. 2023) can potentially solve this issue better, we report the related experimental results in the supplementary material.In addition, by dot-multiplying the cross-modal fused word tokens {x i } L i=0 with the position labels, we derive the textual features of the decoupled components {x i } 0&lt;i≤L .Notably, the decoupled textual features, as a result of crossattention, implicitly contain information about the global instruction and ISR while preserving the original textual features.That is, feature decoupling produces individual features while keeping the global context.</p>
<p>VLN Progress Tracking.Since VLN's decision-making is progressive, the agent needs to track the navigation progress and explicitly align the already executed instruction components, rather than the entire instruction, with the ISR.As shown in Fig. 4, an MLP is used to map the crossmodal fused tokens X = {x 1 , ..., xL } to instruction weights W t = [ω t 1 , ..., ω t L ], which assign higher attention to the already executed instruction components.The training target d t of progress tracking is defined as the normalized distance from the current viewpoint to the goal, i.e., the target will be 1 at the beginning and closer to 0 as the agent approaches the goal.We employ a mean squared loss L P ro to supervise the training of the progress tracking module.</p>
<p>Position and Semantic Alignments.Before performing the ALG, we need to specify which neural grids are aligned with which components in the instruction.To this end, we propose to treat the attention matrix of the last cross-modal attention layer as an affinity matrix to match the neural grids and instruction components (as shown in Fig. 4), since it is learned to adaptively measure the semantic similarity between the tokens (Pardyl et al. 2023).Such an idea has two benefits: (1) No additional matching algorithms are re- (2) Such a design facilitates the agent to learn neural grid's adaptive attention to different instruction components when the model parameters are updated.Specifically, we first perform row-wise max-pooling on the attention matrix to obtain each language token's most attentive neural grid { mt i } 0&lt;i≤L .Note that i ≤ L since multiple language tokens pay attention to the same neural grid.Fig. 4 shows an example of ISR actively and adaptively focusing on landmarks, scenes, i.e., positionally and semantically aligning { mt i } 0&lt;i≤L with the landmark and scene components in the instruction.Those tokens that do not actively pay attention to landmarks and scenes are forced to match other instruction components, i.e., actions, orientations, and others.For brevity, only the ALG technique for landmark and scene alignment shown in Fig. 4 is detailed below.</p>
<p>Position alignment aims to closely match the distribution of linguistically modulated ISR with the text distribution of navigation instructions.The ground-truth text distribution of landmarks and scenes is obtained by element-wise summing the position labels of the associated decoupled text components, i.e., L total = L land + L scene .In practice, we dotmultiply L total and W t to produce a ground-truth text distribution with navigational progress awareness, as shown in Fig. 4. The process of position label prediction is as follows:
Ltotal = Sof tmax(M LP (M ean([ mt 0 , ..., mt i ]))),(4
) where M ean(•) denotes averaging over the neural grids.We employ a BCE loss L P A to supervise the training of the position alignment.Semantic alignment aims to match semantically similar neural grids with instruction components and keep away the dissimilar ones from both through contrastive learning.The semantic alignment loss is defined as follows:
L SA = 1 |X + | xi ∈X + −log exp(α+ * (m ⊤ xi/τ )) l j=1 exp(α− * (m ⊤ xj /τ )) ,(5)
where X + = {x i } 0&lt;i≤L denotes the text features corresponding to the landmark and scene components, as shown in Fig. 4. l denotes the number of tokens in X + and m = M ean([ mt 0 , ..., mt i ]).τ is a temperature scaling parameter.α + and α − are the weights of positive term (landmarks and scenes) and negative term (actions, orientations, and others), respectively.Conversely, we can also utilize the ALG technique in practice to make agents actively and adaptively focus on action and orientation components in the instruction.</p>
<p>Pre-training and Fine-tuning for VLN</p>
<p>In the pre-training phase, we train the agent using a large number of pre-collected trajectories in the behavioral cloning framework (Hu et al. 2024).A cross-entropy loss with inflection weighting (Wijmans et al. 2019a) is employed for action prediction, which gives higher weights for actions different from the previous one:
LAction = 1 T T t=0 −(1 + γ1 a * t ̸ =a * t−1 log(p(a * t ))).(6)
The total loss L total in the pre-training phase is denoted as:
L total = LAction + β(LV F + LMap + LSem)+ λ(LP ro + LP A + LSA),(7)
where β and λ are weighting parameters.Furthermore, the Dagger technique (Ross, Gordon, and Bagnell 2011) is used to fine-tune the pre-trained models to address the distribution discrepancy between the offline training data and the target policy.Fine-tuning fundamentally differs from the pre-training phase that employs expert demonstration paths, as it involves novel data acquisition via exploration.Please see the supplementary materials for more details.</p>
<p>Experiments Experimental Settings and Implementation Details</p>
<p>Datasets.As stated in the problem definition, we evaluate our proposed VLN strategy on the R2R-CE and Habitat Ob-jectNav datasets:</p>
<p>(1) R2R-CE (Krantz et al. 2020) dataset comprises a total of 5,611 shortest path trajectories over 90 visually realistic scenes.To highlight our method's generalization to novel scenes, we report performance on the unseen validation (Val-Unseen) and test splits.Both splits contain episodes with novel paths and instructions from scenes that are unseen in training.An episode is successful if the stop decision is taken within 3 m of the goal position.</p>
<p>(2) ObjectNav experiments are performed on the MP3D dataset with the Habitat simulator.We use the standard split of 61 train / 11 val scenes with the Habitat ObjectNav dataset (Gervet et al. 2022), which consists of 21 goal categories.</p>
<p>Method</p>
<p>ObjectNav-MP3D (val) SR(%)↑ SPL(%)↑ DTS(m)↓ OVRL (Yadav et al. 2023) 28.6 7.4 -Ego 2 -MAP (Hong et al. 2023b)  All the goals are converted to instructions such as "Please navigate to [c target ] and stay within 1 m of it."by using a fixed instruction template.An episode is successful if the stop decision is taken within 1 m of the object goal.</p>
<p>We consider these two tasks instead of the others (Qi et al. 2020;Ku et al. 2020;Anderson et al. 2018) because they allow agents to take low-level actions for continuous movements and are thus more practical.R2R-CE and ObjectNav require more fine-grained decisions and rely more on efficient scene representation and instruction grounding.</p>
<p>Evaluation Metrics.There are several standard metrics (An et al. 2024) for VLN evaluation, including Success Rate (SR), Oracle SR (OSR), and SR penalized by Path Length (SPL).SR (%) gauges how often the predicted stop location is within a predefined distance from the true location.OSR (%) determines the frequency with which any point on the predicted path is within a certain distance of the goal.SPL (%) measures navigation effectiveness by combining the success rate with the length of the route.</p>
<p>Implementation Details.The number of layers and attention heads of the transformers in our VLN strategy are 4 and 8, respectively.If not additionally specified, the dimensions of ISR are sized h = w = 10 and d = 512.τ and k in VI are respectively set to 0.07 and 20.All egocentric semantic maps used in SLI have a scale of H = W = 32 with each pixel corresponding to 20 cm × 20 cm.The L in ALG is empirically set to 160 according to the length of the instructions in the R2R-CE dataset.The weights α + , α − , and τ in the semantic alignment of ALG are set to 1.0, 2.0, and 0.07, respectively.β and λ in Eq. 9 are set to 0.3 and 0.5.Following existing methods (Wang et al. 2023a;An et al. 2024), we employ a waypoint predictor for the VLN task to predict long-term navigation goals.For the ObjectNav task, we directly predict low-level navigation actions end-to-end.</p>
<p>For pre-training, we collect navigation trajectories based on the episodes in the training split, including visual observations, egocentric semantic maps, and semantically segmented views, please see the supplementary material for more details.The whole model is trained for 100 epochs on one NVIDIA GeForce RTX 3090 GPU using a learning rate of 1 × 10 −4 and batch size of 4. The optimizer is AdamW.For fine-tuning, our VLN policy is trained for more than 50 epochs on 4 NVIDIA GeForce RTX 3090 GPUs using a learning rate of 5 × 10 −5 and 6 threads.
Ablations Val Unseen LMap LCon LKL LP ro LP A LSA OSR ↑ SR ↑ SPL ↑ ✗ ✗ ✗ ✗ ✗ ✗ 58 49 43 ✓ ✗ ✗ ✗ ✗ ✗ 60 51 45 ✓ ✓ ✗ ✗ ✗ ✗ 62 52 45 ✓ ✓ ✓ ✗ ✗ ✗ 63 53 47 ✓ ✓ ✓ ✓ ✓ ✗ 64 55 48 ✓ ✓ ✓ ✗ ✓ ✓ 63 54 46 ✓ ✓ ✓ ✓ ✓ ✓ 67 58 50
Table 3: Ablation studies on the R2R-CE dataset.</p>
<p>Comparison with State-of-the-art Methods</p>
<p>We first conduct comparative studies between our VLN policy and the state-of-the-art methods on the R2R-CE dataset.</p>
<p>For adequate comparisons, the baselines are diverse in terms of SR.For example, CM 2 , GridMM, and ETPNav employ the explicit semantic grid map, visual feature field, and TSR as SRs, respectively.Ego 2 -Map uses a self-supervised SR learning scheme based on 2D-3D contrastive learning.However, these methods share the same drawback of using only cross-attention to ambiguously align SR with instruction features at the sentence level.GELA mitigates this problem and is similar to our ALG, but it only uses contrastive learning to align visual features with the object entities in the instructions.As shown in Tab. 1, our method achieves the best performance on both splits, reflecting the superiority of our ISR and ALG techniques.Notably, DREAMWALKER attempts to learn a world model for predicting future views to augment VLN, which is different from our visual imagination.However, DREAMWALKER requires constructing an additional TSR, which is difficult to scale to large-scale scenes.Our method overcomes this issue by using ISR to organize historical images and imagine spatio-temporal high-level semantics, thus significantly outperforms DREAMWALKER.</p>
<p>As expected, our method also achieves the best performance on the ObjectNav dataset, as shown in Tab. 2. Similarly, our method outperforms those methods that utilize semantic grid maps (HOZ e ++ and NaviFormer), visual feature fields (VLFM), and visual representations based on self-supervised contrastive learning (OVRL, Ego 2 -Map, and ECL).It is worth noting that T-Diff uses a trajectory diffusion technique to predict future trajectories, which is different from our idea of visual imagination.SG-Nav extracts common-sense knowledge from large language models to enhance ObjectNav, but relies on a TSR that are difficult to scale with scene size.Unlike the VLN methods in Tab. 1, which predict navigational subgoals across multiple time steps, ObjectNav requires the agent to make navigational decisions at each time step, and thus relies more heavily on fine-grained vision-language alignment.To this end, our method has excellent visual imagination and ALG abilities, which significantly improve the ObjectNav performance.</p>
<p>Ablation Studies</p>
<p>We conduct ablation studies on the individual components of our method to clarify their contributions.All ablations utilize L Action and L Sem to ensure basic action prediction and effective observation encoding.As shown in Tab. 3, all the RVI techniques (L M ap , L Con , and L KL ) can improve the VLN performance.In addition, the involvements of positional alignment L P A and semantic alignment L SA promote ALG, which further leads to substantial OSR, SR, and SPL boosts.Notably, L P A and L SA should be used in conjunction with L P ro as the navigation process is progressive.The absence of progress tracking L P ro will result in a significant decrease in performance.</p>
<p>Diagnostic Studies and Discussion</p>
<p>(1) Does the VLN progress tracking work ?Fig. 5 illustrates how the instruction weights change in the process tracking module as the VLN progresses.We find that the instruction weights in the progress tracking module can reflect which part of the instruction has been executed.In addition, the instruction weights also reflect the agent's attention to the scene and landmark components of the instruction.</p>
<p>(2) How much does the hyperparameters affect our method?Fig. 6 illustrates the sensitivity analysis results for two key hyperparameters, i.e., the range of visual imagination (k), and the dimensions of ISR (h and w).For k, we evaluated four cases with k = {10, 20, 30, 40}.For h and w, we evaluated the four cases h = w = {6, 8, 10, 12}.We find that our method performs best when k = 20 and w = h = 10.In addition, our method is insensitive to these hyperparameters and thus is robust.</p>
<p>Conclusion</p>
<p>This paper focuses on scene representation and instruction grounding problems in VLN tasks.For scene representation, we enable the agent's abilities to model the regularity of visual transitions and semantic scene layouts by learning an ISR, rather than retaining redundant geometric details.In other words, we advocate empowering VLN agents with two necessary abilities: (1) recalling the past and predicting the future and (2) imagining the current semantic layout of the surroundings.For linguistic grounding, we suggest adaptively aligning the ISR with different instruction components at the positional and semantic levels, rather than ambiguous vision-language matching.Sufficient comparative and ablation studies demonstrated our method's feasibility and superiority over existing methods.In the future, we will try to make efforts on zero-shot VLN based on multimodal large models to improve the generalization of VLN agents.Yin, H.;Xu, X.;Wu, Z.;Zhou, J.;and Lu, J. 2024b</p>
<p>Training Data Collection</p>
<p>The data collection process used for pre-training is shown in Fig. 7 and Fig. 8.The trajectories used for data collection come from the training splits of R2R-CE (Krantz et al. 2020) and MP3D-ObjectNav (Chaplot et al. 2020) datasets.In each episode, based on the MP3D scene data (Chang et al. 2017), the Habitat simulator (Ramakrishnan et al. 2021) renders RGB, depth, and semantically segmented images at each timestep.These visual perceptions are collected as a historical observation sequence together with the agent's actions and poses.For the data generation of SLI technique, we use the semantically segmented images, depth images, and camera parameters provided by the simulator to generate a ground-truth egocentric semantic map sequence {M t } T t=0 for each VLN episode, where M t ∈ R H×W .As shown in Fig. 8, each pixel in M t stores the index of the semantic category of the corresponding position in the scene, and the MP3D dataset contains a total of 41 semantics.An index of 0 means free, otherwise it means occupied by an obstacle.</p>
<p>The VSP task is designed to predict the existence of each object category and the ratio occupied by the objects in the views (if they are present) based on the current observation o t .We can obtain the corresponding ground-truth labels from the semantically segmented images.</p>
<p>Model Training Details</p>
<p>In the pre-training phase, we use behavioral cloning (Hu et al. 2024)  inflection weighting (Wijmans et al. 2019a) is employed for action prediction, which gives higher weights for actions different from the previous one:
L action pred = 1 T T t=0 −(1 + γ1 a * t ̸ =a * t−1 log(p(a * t ))).(8)
The total loss L total in the pre-training phase is denoted as:
L total = LAction + β(LV F + LMap + LSem)+ λ(LP ro + LP A + LSA),(9)
where β and λ are weighting parameters.In practice, we propose to employ L Action + β(L V F + L M ap + L Sem ) for the first stage of training to learn a high-quality scene representation with high-level scene priors.Then, the complete loss L total is used for the second stage of training, which adaptively aligns the learned scene representation with the instruction components at the positional and semantic levels.</p>
<p>The pre-training setting can make full use of the ability of transformers to extract the optimal policy from a large amount of offline data, but it also needs to address the distribution discrepancy between the offline training data and the target policy.Therefore, the Dagger technique (Ross, Gordon, and Bagnell 2011) is used to fine-tune the pre-trained models to enhance the generalization of VLN agents, following existing works (Chen et al. 2022b;Hong et al. 2022;Wang et al. 2023c).Fine-tuning fundamentally differs from the pre-training phase that employs expert demonstration paths, as it involves novel data acquisition via exploration.In particular, the model is trained with heuristic pseudo label a pse t , which is sampled from the distribution predicted by the agent:
L F T = 1 T T t=0
CrossEntropy(ã t , a pse t ).</p>
<p>For example, a predictor (Hong et al. 2022) is employed to generate several candidate waypoints in the VLN-CE setting.Then, the candidate waypoint nearest to the destination is used as the pseudo label a pse t to encourage the agent to learn a backtracking strategy.In the initial fine-tuning phase, the waypoint closest to the destination dominates the supervision.Meanwhile, the model's uncertain decision-making drives the agent to explore the environment and reduce the exposure bias.As the model grows stronger, it will increasingly trust its own decisions so that the latter stage of the fine-tuning will be mainly supervised by the model itself.</p>
<p>Performance Evaluation of ALG Variants</p>
<p>As shown in Fig. 4 in the paper, our proposed adaptive position and semantic alignments force ISR to actively focus  on the landmark and scene components in the instructions, which we call scene priority.Alternatively, we can also design an action-aware ALG variant to motivate ISR to actively pay attention to the action and orientation components, which we call action priority.The comparative results in Tab. 4 quantitatively evaluate the performance of two ALG variants.We find that the focus on scene and landmark components produces more efficient VLN agents under the R2R-CE setting.In other words, agents in the R2R-CE setup are more sensitive to landmark entities and scene references.</p>
<p>Instruction Decoupling based on a Large Language Model</p>
<p>Although performance gains have been achieved by using off-the-shelf tools (Schuster et al. 2015;Wu et al. 2019) to decouple navigation instructions, it will inevitably lead to incorrect component divisions due to semantic ambiguities.</p>
<p>In practice, we adjust a portion of incorrect component divisions by manually checking them.However, when more and more navigation instructions are employed to enhance the ALG, it is impractical to correct the semantic ambiguity manually.Fortunately, with the rise of large language models (Achiam et al. 2023), they have demonstrated language analysis and comprehension capabilities comparable to those of humans.Therefore, we prompted GPT-4 to divide navigation instructions into semantic components, including landmarks, scenes, actions, orientations, and others.An example of instruction parsing using GPT-4 is shown in Fig. 9, where the semantic component division is almost perfect.</p>
<p>In addition, we use different instruction parsing schemes to decouple the navigation instructions in the R2R-CE dataset and investigate their effects on the VLN performance, the results are shown in Tab. 5.The first row in Tab. 5 indicates that only off-the-shelf tools are used for instruction parsing without manual checks.The second line indicates the addition of a manual check.The third line indicates directly using the components decoupled by GPT-4.The experimental results show that GPT-4-based instruction decoupling leads to better VLN performance due to the pow- erful language analysis capability of large language models.When manual checking is missing, the decrease in VLN performance reflects the necessity of accurate instruction decoupling for positional and semantic alignments in the ALG.</p>
<p>More Visualization</p>
<p>Fig. 10 illustrates an example of R2R-CE with the navigation instruction "Exit the bedroom and turn left.Walk straight passing the gray couch and stop near the rug".The darker the base color of the words, the higher the corresponding weights and attention in Fig. 10.Eventually, the agent navigate to the vicinity of the rug by following the instruction.</p>
<p>…Figure 1 :
1
Figure 1: The VLN agent decouples an instruction into different components, including landmarks, scenes, actions, orientations, and others, which are adaptively aligned with high-level scene priors in the ISR.The pre-trained ISR can provide the necessary mindsets for VLN, including view imagination and scene layout imagination.</p>
<p>Figure 2 :
2
Figure2: An illustration of our VLN policy with RVI (Fig.3) and ALG (Fig.4).Our method treats SR learning as a sequence modeling problem and trains a joint state-action transformer over entire trajectories.</p>
<p>Figure 3 :
3
Figure 3: An illustration of RVI, including view imagination, scene layout imagination, and visual semantic prediction.</p>
<p>Figure 4 :
4
Figure 4: An illustration of ALG, including instruction decoupling, VLN progress tracking, and linguistic alignment.quired.(2)Such a design facilitates the agent to learn neural grid's adaptive attention to different instruction components when the model parameters are updated.Specifically, we first perform row-wise max-pooling on the attention matrix to obtain each language token's most attentive neural grid { mt i } 0&lt;i≤L .Note that i ≤ L since multiple language tokens pay attention to the same neural grid.Fig.4shows an example of ISR actively and adaptively focusing on landmarks, scenes, i.e., positionally and semantically aligning { mt i } 0&lt;i≤L with the landmark and scene components in the instruction.Those tokens that do not actively pay attention to landmarks and scenes are forced to match other instruction components, i.e., actions, orientations, and others.For brevity, only the ALG technique for landmark and scene alignment shown in Fig.4is detailed below.Position alignment aims to closely match the distribution of linguistically modulated ISR with the text distribution of navigation instructions.The ground-truth text distribution of landmarks and scenes is obtained by element-wise summing the position labels of the associated decoupled text components, i.e., L total = L land + L scene .In practice, we dotmultiply L total and W t to produce a ground-truth text distribution with navigational progress awareness, as shown in Fig.4.The process of position label prediction is as follows:</p>
<p>StepFigure 5 :
5
Figure 5: A visualization of how the instruction weights change with navigation progress.Different rows indicate weights at different time steps.A redder color indicates that the agent is more attentive to the corresponding words.</p>
<p>Figure 7 :
7
Figure 7: An example of scene and trajectory used for data collection for pre-training.</p>
<p>Figure 8 :
8
Figure 8: Examples of observation sequences collected along the trajectory in a navigation episode as shown in Fig. 7.Only one view per timestep is shown here.</p>
<p>User:</p>
<p>Figure 9: An illustration of semantic component division based on GPT-4.</p>
<p>Figure 10 :
10
Figure 10: (a)-(f) illustrate the navigation views and process tracking during VLN.We visualize the top-6 instruction weights during process tracking in red, with darker colors having higher weights.The blue arrows indicate the navigation directions for each step.</p>
<p>Table 1 :
1
Results on the R2R-CE dataset.
MethodVal Unseen OSR↑ SR↑ SPL↑ OSR↑ SR↑ SPL↑ Test UnseenCM 2 (Georgakis et al. 2022)4234 283931 24WS-MGMap (Chen et al. 2022a)4839 344535 28GELA (Cui et al. 2023)5948 415746 40GridMM (Wang et al. 2023c)6149 415646 39Ego 2 -Map (Hong et al. 2023a)-52 465647 41DREAMWALKER (Wang et al. 2023a)5949 445749 44ETPNav (An et al. 2024)6557 496355 48Zhang et.al. (Zhang and Kordjamshidi 2024)-58 49-56 48Ours6759 506457 50Those tokens that do not actively pay attention to actions andorientations are forced to align with other instruction com-ponents, i.e., landmarks, scenes, and others. Please see thesupplementary material for the performance of this variant.</p>
<p>Table 2 :
2
Results on the MP3D-ObjectNav dataset (val).
29.010.65.17</p>
<p>. SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zeroshot Object Navigation.arXiv preprint arXiv:2410.08189.Yokoyama, N.; Ha, S.; Batra, D.; Wang, J.; and Bucher, B. 2024.Vlfm: Vision-language frontier maps for zero-shot semantic navigation.In 2024 IEEE International Conference on Robotics and Automation (ICRA), 42-48.IEEE.Aware Object Goal Navigation via Simultaneous Exploration and Identification.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6672-6682.Zhang, S.; Song, X.; Yu, X.; Bai, Y.; Guo, X.; Li, W.; and Jiang, S. 2025.HOZ++: Versatile Hierarchical Object-to-Zone Graph for Object Navigation.IEEE Transactions on Pattern Analysis and Machine Intelligence.Zhang, S.; Yu, X.; Song, X.; Wang, X.; and Jiang, S. 2024.Imagine Before Go: Self-Supervised Generative Map for Object Goal Navigation.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16414-16425.Zhang, Y.; and Kordjamshidi, P. 2024.Narrowing the gap between vision and action in navigation.In Proceedings of the 32nd ACM International Conference on Multimedia, 856-865.
Yu, X.; Zhang, S.; Song, X.; Qin, X.; and Jiang, S. ???? Tra-jectory Diffusion for ObjectGoal Navigation. In The Thirty-eighth Annual Conference on Neural Information Process-ing Systems.Zhang, J.; Dai, L.; Meng, F.; Fan, Q.; Chen, X.; Xu, K.; andWang, H. 2023. 3D-</p>
<p>Table 4 :
4
to train VLN agents.The cross-entropy loss with VLN performance using different ALG variants.
DIA VariantsVal Unseen OSR ↑ SR ↑ SPL ↑Action Priority655547Scene Priority675850</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Bevbert: Multimodal map pre-training for language-guided navigation. D An, Y Qi, Y Li, Y Huang, L Wang, T Tan, J Shao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Etpnav: Evolving topological planning for vision-language navigation in continuous environments. D An, H Wang, W Wang, Z Wang, Y Huang, K He, L Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018Van Den Hengel, A</p>
<p>A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, arXiv:1709.06158Mat-terport3d: Learning from rgb-d data in indoor environments. 2017arXiv preprint</p>
<p>Object goal navigation using goal-oriented semantic exploration. D S Chaplot, D P Gandhi, A Gupta, R R Salakhutdinov, Advances in Neural Information Processing Systems. 202033</p>
<p>Think holistically, act down-to-earth: A semantic navigation strategy with continuous environmental representation and multi-step forward planning. B Chen, J Kang, P Zhong, Y Cui, S Lu, Y Liang, J Wang, 2023</p>
<p>Embodied Contrastive Learning with Geometric Consistency and Behavioral Awareness for Object Navigation. B Chen, J Kang, P Zhong, Y Liang, Y Sheng, J Wang, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Weakly-supervised multi-granularity map learning for vision-and-language navigation. P Chen, D Ji, K Lin, R Zeng, T Li, M Tan, C Gan, Advances in Neural Information Processing Systems. 2022a35</p>
<p>History aware multimodal transformer for vision-andlanguage navigation. Advances in neural information processing systems. S Chen, P.-L Guhur, C Schmid, I Laptev, 2021a34</p>
<p>History aware multimodal transformer for vision-andlanguage navigation. Advances in neural information processing systems. S Chen, P.-L Guhur, C Schmid, I Laptev, 2021b34</p>
<p>Think global, act local: Dual-scale graph transformer for vision-and-language navigation. S Chen, P.-L Guhur, M Tapaswi, C Schmid, I Laptev, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022b</p>
<p>Learning disentanglement with decoupled labels for vision-language navigation. W Cheng, X Dong, S Khan, J Shen, European Conference on Computer Vision. Springer2022</p>
<p>Grounded entity-landmark adaptive pre-training for vision-and-language navigation. Y Cui, L Xie, Y Zhang, M Zhang, Y Yan, E Yin, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Unbiased directed object attention graph for object navigation. R Dang, Z Shi, L Wang, Z He, C Liu, Q Chen, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022</p>
<p>SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World. K Ehsani, T Gupta, R Hendrix, J Salvador, L Weihs, K.-H Zeng, K P Singh, Y Kim, W Han, A Herrasti, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Crossmodal map learning for vision and language navigation. G Georgakis, K Schmeckpeper, K Wanchoo, S Dan, E Miltsakaki, D Roth, K Daniilidis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Navigating to objects in the real world. T Gervet, S Chintala, D Batra, J Malik, D S Chaplot, Science Robotics. 82022</p>
<p>Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation. Y Hong, Z Wang, Q Wu, S Gould, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Vln bert: A recurrent vision-and-language bert for navigation. Y Hong, Q Wu, Y Qi, C Rodriguez-Opazo, S Gould, Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition. the IEEE/CVF conference on Computer Vision and Pattern Recognition2021</p>
<p>Learning navigational visual representations with semantic map supervision. Y Hong, Y Zhou, R Zhang, F Dernoncourt, T Bui, S Gould, H Tan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023a</p>
<p>Learning navigational visual representations with semantic map supervision. Y Hong, Y Zhou, R Zhang, F Dernoncourt, T Bui, S Gould, H Tan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023b</p>
<p>On Transforming Reinforcement Learning With Transformers: The Development Trajectory. S Hu, L Shen, Y Zhang, Y Chen, D Tao, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>Beyond the nav-graph: Vision-and-language navigation in continuous environments. J Krantz, E Wijmans, A Majumdar, D Batra, S Lee, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer2020. August 23-28, 202016</p>
<p>Room-across-room: Multilingual vision-andlanguage navigation with dense spatiotemporal grounding. A Ku, P Anderson, R Patel, E Ie, J Baldridge, arXiv:2010.079542020arXiv preprint</p>
<p>. C Lin, Y Jiang, J Cai, L Qu, G Haffari, Z Yuan, </p>
<p>Multimodal transformer with variable-length memory for vision-and-language navigation. Springer, J O'keefe, N Burgess, European Conference on Computer Vision. 1996381Geometric determinants of the place fields of hippocampal neurons</p>
<p>Active visual exploration based on attention-map entropy. A Pardyl, G Rypeść, G Kurzejamski, B Zieliński, T Trzciński, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. the Thirty-Second International Joint Conference on Artificial Intelligence2023</p>
<p>Reverie: Remote embodied visual referring expression in real indoor environments. Y Qi, Q Wu, P Anderson, X Wang, W Y Wang, C Shen, Hengel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>. Y Qiao, Y Qi, Y Hong, Z Yu, P Wang, Q Wu, </p>
<p>Hop+: History-enhanced and order-aware pre-training for vision-and-language navigation. IEEE Transactions on Pattern Analysis and Machine Intelligence. 457</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, arXiv:2109.08238Habitat-matterport 3d dataset (hm3d): 1000 largescale 3d environments for embodied ai. 2021. 2021arXiv preprintInternational conference on machine learning</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statistics2011JMLR Workshop and Conference Proceedings</p>
<p>Generating semantically precise scene graphs from textual descriptions for improved image retrieval. S Schuster, R Krishna, A Chang, L Fei-Fei, C D Manning, Proceedings of the fourth workshop on vision and language. the fourth workshop on vision and language2015</p>
<p>The cerebellum: adaptive prediction for movement and cognition. A A Sokolov, R C Miall, R B Ivry, Trends in cognitive sciences. 2152017</p>
<p>Self-Supervised 3-D Semantic Representation Learning for Vision-and-Language Navigation. S Tan, K Sima, D Wang, M Ge, D Guo, H Liu, IEEE Transactions on Neural Networks and Learning Systems. Tolman, E. C. 1948. Cognitive maps in rats and men. 202455189</p>
<p>Differential effects of early hippocampal pathology on episodic and semantic memory. F Vargha-Khadem, D G Gadian, K E Watkins, A Connelly, W Van Paesschen, M Mishkin, Science. 27753241997</p>
<p>Dreamwalker: Mental planning for continuous visionlanguage navigation. H Wang, W Liang, L Van Gool, W Wang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023a</p>
<p>A dual semantic-aware recurrent globaladaptive network for vision-and-language navigation. L Wang, Z He, J Tang, R Dang, N Wang, C Liu, Q Chen, arXiv:2305.036022023barXiv preprint</p>
<p>Gridmm: Grid memory map for vision-and-language navigation. Z Wang, X Li, J Yang, Y Liu, S Jiang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023c</p>
<p>Embodied question answering in photorealistic environments with point cloud perception. E Wijmans, S Datta, O Maksymets, A Das, G Gkioxari, S Lee, I Essa, D Parikh, D Batra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019a</p>
<p>Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations. E Wijmans, A Kadian, A Morcos, S Lee, I Essa, D Parikh, M Savva, D Batra, H Wu, J Mao, Y Zhang, Y Jiang, L Li, W Sun, W.-Y Ma, arXiv:1911.00357Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019b. 2019arXiv preprintDd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames</p>
<p>Visionand-Language Navigation via Latent Semantic Alignment Learning. S Wu, X Fu, F Wu, Z.-J Zha, IEEE Transactions on Multimedia. 2024</p>
<p>Navi-Former: A Spatio-Temporal Context-Aware Transformer for Object Navigation. W Xie, H Jiang, Y Zhu, J Qian, J Xie, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Offline visual representation learning for embodied navigation. K Yadav, R Ramrakhya, A Majumdar, V.-P Berges, S Kuhar, D Batra, A Baevski, O Maksymets, Workshop on Reincarnating Reinforcement Learning at ICLR 2023. 2023</p>
<p>SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zeroshot Object Navigation. H Yin, X Xu, Z Wu, J Zhou, J Lu, arXiv:2410.081892024aarXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
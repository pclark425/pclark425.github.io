<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-419 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-419</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-419</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-264146084</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09383v1.pdf" target="_blank">Integrating Symbolic Reasoning into Neural Generative Models for Design Generation</a></p>
                <p><strong>Paper Abstract:</strong> Design generation requires tight integration of neural and symbolic reasoning, as good design must meet explicit user needs and honor implicit rules for aesthetics, utility, and convenience. Current automated design tools driven by neural networks produce appealing designs, but cannot satisfy user specifications and utility requirements. Symbolic reasoning tools, such as constraint programming, cannot perceive low-level visual information in images or capture subtle aspects such as aesthetics. We introduce the Spatial Reasoning Integrated Generator (SPRING) for design generation. SPRING embeds a neural and symbolic integrated spatial reasoning module inside the deep generative network. The spatial reasoning module decides the locations of objects to be generated in the form of bounding boxes, which are predicted by a recurrent neural network and filtered by symbolic constraint satisfaction. Embedding symbolic reasoning into neural generation guarantees that the output of SPRING satisfies user requirements. Furthermore, SPRING offers interpretability, allowing users to visualize and diagnose the generation process through the bounding boxes. SPRING is also adept at managing novel user specifications not encountered during its training, thanks to its proficiency in zero-shot constraint transfer. Quantitative evaluations and a human study reveal that SPRING outperforms baseline generative models, excelling in delivering high design quality and better meeting user specifications.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e419.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e419.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPRING</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial Reasoning Integrated Generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid neuro-symbolic design-generation system that embeds a symbolic constraint reasoner inside a neural generative pipeline to guarantee satisfaction of explicit spatial constraints while learning implicit spatial preferences from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SPRING (Spatial Reasoning Integrated Generator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end system for constrained design/image generation comprised of three modules: (1) a perception module (DETR object detector + ResNet18 scene encoder) that extracts existing objects and scene features; (2) a Spatial Reasoning Module (SRM) that generates bounding-box locations for new objects by iteratively refining each coordinate with a recurrent neural network (stacked GRUs) and applying a symbolic forward-checking constraint-satisfaction filter; (3) a Visual Element Generator (VEG) that fills each bounding box via a diffusion inpainting model (Stable Diffusion or GLIDE). The symbolic forward-checker enforces hard user-specified propositional spatial constraints at each refinement step; the GRU learns implicit placement preferences and provides probability distributions over 'left/right/stop' refinement tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Propositional logic constraint system + forward-checking symbolic constraint-satisfaction procedure. Constraints expressed in a custom propositional design language (predicates such as cleft/cright/cabove/cbelow, equality and size predicates, and value predicates). The symbolic component performs depth-first forward checking over the RNN's decision-token search tree to eliminate choices that lead to unsatisfiable constraint assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Recurrent neural network (stack of 3 GRUs with hidden size 500, dense + softmax output) performing iterative binary-halving refinement of each bounding-box variable (x, y, width, height) by emitting decision tokens ('left','right','stop'); perception via DETR object detector and ResNet18 encoder; diffusion-based generative models (Stable Diffusion or GLIDE) for inpainting object pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular, tightly-coupled hybrid in which the neural SRM proposes stochastic refinement decisions and the symbolic forward-checker intercepts each sampled decision to test feasibility (forward checking / lookahead DFS) and rejects/resamples decisions that necessarily lead to constraint violation; gradients are backpropagated through the neural parts (GRU and scene encoder) during training but the symbolic forward-checker is non-differentiable and applied at inference (and during training to block invalid sampled choices). The SRM is trained with teacher forcing (50%) and free-running (50%) phases; existing background object bounding boxes are injected as fixed constraints/inputs to the SRM so they influence the hidden state.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Guaranteed satisfaction of explicit spatial constraints at generation time despite stochastic neural proposals; interpretability via explicit decision-token traces and intermediate bounding-box layouts; zero-shot transfer to novel symbolic constraints (new constraint predicates can be added programmatically to the forward-checker after training); improved implicit spatial preference learning (e.g., placing toasters on counters) because the GRU learns from data while symbolic checks ensure feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Interior design image generation (adding objects to a background image under user-specified spatial constraints); evaluated via synthetic spatial-positioning scenarios (constraint accuracy, preference accuracy) and large-scale image generation metrics (Inception Score, FID) plus human study for specification satisfaction, aesthetic appeal, background preservation, and spatial naturalness.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Position accuracy: 1.0 (100% of tested positional constraints satisfied in evaluation set reported); Object accuracy: 0.77 (averaged over 20 constrained test images); Inception Score (IS): 3.59; Fréchet Inception Distance (FID): 160.36 (reported for SPRING[SD] in paper's Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Stable Diffusion baseline: Position accuracy ≈ 0.50 (50%), Object accuracy ≈ 0.63, IS: 3.58, FID: 162.73 (reported in paper's Table 3 for the Stable Diffusion baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Zero-shot constraint transfer: symbolic forward-checker can enforce new constraint types (example: an atop(...) constraint) not seen during training without retraining the neural SRM; SRM can handle novel combinations of constraints and backgrounds because the symbolic layer enforces feasibility while the GRU supplies aesthetic priors. Paper reports superior transfer/generalization relative to baselines (e.g., GAN+CVX cannot handle non-convex constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: iterative refinement decision tokens and intermediate bounding-box layouts are exposed, enabling tracing and diagnosis of which sub-decisions led to placements; symbolic constraints are explicit and human-readable, permitting programmatic editing and explanation of why a proposed decision was rejected (forward-checker can explain which constraint would be violated by a branch).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dependence on the VEG quality (diffusion model can occasionally inpaint wrong or malformed objects); limited precision due to per-mille coordinate representation; occasional odd object orientations due to limited dataset diversity and VEG inductive biases; symbolic forward-checker is non-differentiable and applied as a hard filter (may complicate some end-to-end training regimes); performance constrained by difficulty of learning locational preferences from limited data in cramped scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division-of-labor dual-process view: inspired by 'fast (System 1) / slow (System 2)' cognitive model — neural modules (GRU, diffusion) provide fast, learned, perceptual and aesthetic decisions while symbolic forward-checking provides slower, deliberate, logically precise planning and guarantees; the hybrid is justified as leveraging complementary strengths (learning/adaptation + guarantees/interpretability).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e419.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e419.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAN+CVX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAN with differentiable convex optimization layer (GAN + CVX)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline hybrid that models object locations as optimal solutions to learned convex quadratic programs (QPs) embedded as differentiable convex optimization layers inside a GAN-style pipeline to capture implicit placement preferences while respecting convex constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GAN + CVX (Generative Adversarial Network augmented with a differentiable convex optimization layer)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Baseline hybrid used in spatial-reasoning experiments: a GAN outputs parameters (or latent features) that feed into a differentiable convex optimization (quadratic program) layer; the QP's objective captures implicit spatial preferences and its convex constraints enforce positional constraints; QP solution yields coordinates for object placements and gradients are backpropagated through the convex layer during training using differentiable convex optimization techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Convex optimization model (quadratic program) representing constraints and an objective learned from data; the convex layer enforces constraints expressed in convex form.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Generative Adversarial Network (GAN) producing objective parameters / inputs to the convex layer; gradient-based learning (end-to-end backpropagation through the convex layer using differentiable optimization methods).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Differentiable embedding of a convex optimization solver as a neural network layer (as in 'differentiable convex optimization layers'); the GAN proposes objective/parameters and the convex solver produces feasible optimal coordinates; gradients from the supervised loss are backpropagated through the solver to update the GAN.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Guarantee of constraint satisfaction when constraints are convex and representable in the QP; ability to learn an objective that captures implicit preferences while retaining differentiable end-to-end training; limited capability to express and enforce only convex constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Synthetic spatial positioning scenarios and spatial preference learning; compared against SPRING on preference accuracy and constraint satisfaction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Can generalize learned preferences under convex constraint families but limited for non-convex constraints; paper reports GAN+CVX matches constraint satisfaction for convex constraints but underperforms SPRING on preference accuracy and cannot handle non-convex constraint types.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate: convex objective and constraints are explicit and interpretable, but the learned objective inside the QP may be less directly interpretable than symbolic logic clauses; the optimization solution is explainable in terms of convex feasibility/optimality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Cannot handle non-convex constraints or complex logical constraint combinations (disjunctions, arbitrary propositional formulas) because convex optimization layer requires convex representability; potential mode-collapse issues in GAN component; may fail to capture complex, non-convex commonsense spatial rules.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Optimization-as-layer framework: uses differentiable convex optimization to combine learned objectives (from neural nets) with declarative convex constraints, enabling end-to-end learning when constraints/objectives are convex.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e419.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e419.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiffConvexLayers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Convex Optimization Layers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that embed a convex optimization problem (e.g., QP) as a differentiable layer in a neural architecture so that solution outputs can be backpropagated through during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Differentiable convex optimization layers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Differentiable convex optimization layers (general technique)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Technique for integrating convex solvers into neural networks by providing differentiable mappings from inputs/parameters to optimization solutions, enabling gradient-based learning of upstream parameters that affect a constrained optimization outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Convex optimization problem (quadratic programs, convex cones) representing constraints and objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Gradient-based neural networks (any architecture) that emit solver parameters; automatic differentiation through the optimization solution map.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Embed solver as a differentiable layer (use KKT/implicit differentiation or specialized backprop through QP solvers); end-to-end training with gradients flowing through the optimization layer.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Ability to learn policies/objectives that respect convex constraints; enforce constraints during generation while still training with data; provides formal optimality properties of convex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Used generally in tasks requiring constrained outputs; cited in paper as an approach and used to motivate/compare methods for constrained location prediction (GAN+CVX baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Generalizes within convex constraint families; limited for non-convex logical constraints or combinatorial structure not expressible convexly.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability for constraints and solver outputs due to explicit optimization formulation; learned parameters reflect preferences encoded into the objective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inapplicable to non-convex constraints and complex propositional logic; may require careful problem formulation to admit convexity.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Optimization-as-differentiable-layer: uses convex duality/KKT differentiable mappings to allow gradients through solver outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e419.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e419.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DecDD-in-GAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding Decision Diagrams into Generative Adversarial Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that integrate symbolic decision-diagram structures (representing combinatorial constraints) into GANs to enforce combinatorial constraints during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embedding decision diagrams into generative adversarial networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Embedding decision diagrams into GANs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Related work family where symbolic decision diagrams (representing discrete combinatorial constraints) are embedded or used to filter outputs of GANs so that generated samples satisfy combinatorial or discrete constraints; mentioned as prior art for constrained generative modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Decision diagrams (explicit combinatorial constraint representations).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>GANs (neural generative networks) that are conditioned/filtered by decision-diagram constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Symbolic structure (decision diagram) is embedded into or used in conjunction with a neural generator (GAN) so generated outputs adhere to discrete constraints; integration can be achieved by constraining latent codes or filtering outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Ability to enforce discrete/combinatorial constraints in generative models; improved feasibility of generated structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Constrained generation tasks (e.g., constrained schedule/layout generation); cited in paper as related work embedding symbolic structures into generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Enforces combinatorial constraints at generation time; generalization depends on decision-diagram expressivity and GAN capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Decision diagrams provide explicit, inspectable constraint structure; helps debug constraint satisfaction of generated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires decision-diagram construction and scalability may be limited by decision-diagram size; integration specifics vary across implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Symbolic-structure conditioning of neural generators to guarantee combinatorial constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e419.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e419.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuro-Argumentative Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Argumentative Learning / Neuro-Argumentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods combining neural models with structured argumentative (logical) reasoning to handle conflicting information, produce explanations, and integrate data-driven and rule-driven inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-Argumentative Learning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Methods that use neural components to extract or score claims/evidence and integrate these with an argumentative symbolic reasoning layer that constructs and evaluates logical arguments, often producing explanations and resolving conflicts between pieces of evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Argumentation frameworks / symbolic argumentation systems (logical rules, attack/support relations), used to structure and evaluate arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks that provide scores, features, or candidate propositions for the argumentation engine.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular: neural subsystems propose or score facts/claims which are then composed and evaluated by symbolic argumentative reasoning engines; sometimes used to produce explainable outputs or dialectical explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved explainability and conflict-resolution capabilities compared to pure neural models; structured, dialectical explanations grounded in neural perception or scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned in related work as an example of neuro-symbolic integration for explainability and reasoning; applications include classification with explanations, dealing with conflicting information.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Potential for better generalization due to explicit reasoning layers; dependent on quality of argument construction and neural evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: produces argumentative explanations and structured logical reasoning artifacts that are inspectable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scalability and integration complexity; depends on quality of neural evidence and alignment between neural outputs and symbolic rule formats.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Argumentation theory combined with neural evidence scoring; emphasizes dialectical, structured explanations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e419.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e419.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuro-Symbolic Concept Learner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Symbolic Concept Learner (NS-CL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior neuro-symbolic system that disentangles perception (neural) from symbolic reasoning for tasks like visual question answering by learning visual concepts and using symbolic programs for logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-Symbolic Concept Learner (NS-CL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Architecture (from cited prior work) that uses neural perception to extract object-level concepts and a symbolic program executor (learned or induced) to perform compositional reasoning over these concepts; cited as example of disentangling vision and symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic program executor / logic programs for multi-step compositional reasoning (e.g., question-specific programs).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception modules (CNNs) to produce object attributes/concepts from images; neural module networks for executor interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline: perception produces symbolic concept representations which are consumed by a symbolic program interpreter to execute compositional queries; supervision can be weak (natural language paired with images).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Compositional generalization and interpretable multi-step reasoning over visual scenes; separation of perception and reasoning improves systematicity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Visual question answering and scene understanding; mentioned in related work as a neuro-symbolic example.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Reported compositional generalization benefits in original NS-CL work; cited here as background motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: outputs are symbolic programs and intermediate concept representations that are inspectable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires object-level supervision or reliable object detectors; scaling to unconstrained natural images and open vocabularies is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Separation of perception (neural) and symbolic programmatic reasoning to achieve compositionality and interpretability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Differentiable convex optimization layers <em>(Rating: 2)</em></li>
                <li>Embedding decision diagrams into generative adversarial networks <em>(Rating: 2)</em></li>
                <li>Constraint reasoning embedded structured prediction <em>(Rating: 2)</em></li>
                <li>The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision <em>(Rating: 2)</em></li>
                <li>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding <em>(Rating: 1)</em></li>
                <li>Neuro-argumentative learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-419",
    "paper_id": "paper-264146084",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "SPRING",
            "name_full": "Spatial Reasoning Integrated Generator",
            "brief_description": "A hybrid neuro-symbolic design-generation system that embeds a symbolic constraint reasoner inside a neural generative pipeline to guarantee satisfaction of explicit spatial constraints while learning implicit spatial preferences from data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SPRING (Spatial Reasoning Integrated Generator)",
            "system_description": "End-to-end system for constrained design/image generation comprised of three modules: (1) a perception module (DETR object detector + ResNet18 scene encoder) that extracts existing objects and scene features; (2) a Spatial Reasoning Module (SRM) that generates bounding-box locations for new objects by iteratively refining each coordinate with a recurrent neural network (stacked GRUs) and applying a symbolic forward-checking constraint-satisfaction filter; (3) a Visual Element Generator (VEG) that fills each bounding box via a diffusion inpainting model (Stable Diffusion or GLIDE). The symbolic forward-checker enforces hard user-specified propositional spatial constraints at each refinement step; the GRU learns implicit placement preferences and provides probability distributions over 'left/right/stop' refinement tokens.",
            "declarative_component": "Propositional logic constraint system + forward-checking symbolic constraint-satisfaction procedure. Constraints expressed in a custom propositional design language (predicates such as cleft/cright/cabove/cbelow, equality and size predicates, and value predicates). The symbolic component performs depth-first forward checking over the RNN's decision-token search tree to eliminate choices that lead to unsatisfiable constraint assignments.",
            "imperative_component": "Recurrent neural network (stack of 3 GRUs with hidden size 500, dense + softmax output) performing iterative binary-halving refinement of each bounding-box variable (x, y, width, height) by emitting decision tokens ('left','right','stop'); perception via DETR object detector and ResNet18 encoder; diffusion-based generative models (Stable Diffusion or GLIDE) for inpainting object pixels.",
            "integration_method": "Modular, tightly-coupled hybrid in which the neural SRM proposes stochastic refinement decisions and the symbolic forward-checker intercepts each sampled decision to test feasibility (forward checking / lookahead DFS) and rejects/resamples decisions that necessarily lead to constraint violation; gradients are backpropagated through the neural parts (GRU and scene encoder) during training but the symbolic forward-checker is non-differentiable and applied at inference (and during training to block invalid sampled choices). The SRM is trained with teacher forcing (50%) and free-running (50%) phases; existing background object bounding boxes are injected as fixed constraints/inputs to the SRM so they influence the hidden state.",
            "emergent_properties": "Guaranteed satisfaction of explicit spatial constraints at generation time despite stochastic neural proposals; interpretability via explicit decision-token traces and intermediate bounding-box layouts; zero-shot transfer to novel symbolic constraints (new constraint predicates can be added programmatically to the forward-checker after training); improved implicit spatial preference learning (e.g., placing toasters on counters) because the GRU learns from data while symbolic checks ensure feasibility.",
            "task_or_benchmark": "Interior design image generation (adding objects to a background image under user-specified spatial constraints); evaluated via synthetic spatial-positioning scenarios (constraint accuracy, preference accuracy) and large-scale image generation metrics (Inception Score, FID) plus human study for specification satisfaction, aesthetic appeal, background preservation, and spatial naturalness.",
            "hybrid_performance": "Position accuracy: 1.0 (100% of tested positional constraints satisfied in evaluation set reported); Object accuracy: 0.77 (averaged over 20 constrained test images); Inception Score (IS): 3.59; Fréchet Inception Distance (FID): 160.36 (reported for SPRING[SD] in paper's Table 3).",
            "declarative_only_performance": null,
            "imperative_only_performance": "Stable Diffusion baseline: Position accuracy ≈ 0.50 (50%), Object accuracy ≈ 0.63, IS: 3.58, FID: 162.73 (reported in paper's Table 3 for the Stable Diffusion baseline).",
            "has_comparative_results": true,
            "generalization_properties": "Zero-shot constraint transfer: symbolic forward-checker can enforce new constraint types (example: an atop(...) constraint) not seen during training without retraining the neural SRM; SRM can handle novel combinations of constraints and backgrounds because the symbolic layer enforces feasibility while the GRU supplies aesthetic priors. Paper reports superior transfer/generalization relative to baselines (e.g., GAN+CVX cannot handle non-convex constraints).",
            "interpretability_properties": "High: iterative refinement decision tokens and intermediate bounding-box layouts are exposed, enabling tracing and diagnosis of which sub-decisions led to placements; symbolic constraints are explicit and human-readable, permitting programmatic editing and explanation of why a proposed decision was rejected (forward-checker can explain which constraint would be violated by a branch).",
            "limitations_or_failures": "Dependence on the VEG quality (diffusion model can occasionally inpaint wrong or malformed objects); limited precision due to per-mille coordinate representation; occasional odd object orientations due to limited dataset diversity and VEG inductive biases; symbolic forward-checker is non-differentiable and applied as a hard filter (may complicate some end-to-end training regimes); performance constrained by difficulty of learning locational preferences from limited data in cramped scenes.",
            "theoretical_framework": "Division-of-labor dual-process view: inspired by 'fast (System 1) / slow (System 2)' cognitive model — neural modules (GRU, diffusion) provide fast, learned, perceptual and aesthetic decisions while symbolic forward-checking provides slower, deliberate, logically precise planning and guarantees; the hybrid is justified as leveraging complementary strengths (learning/adaptation + guarantees/interpretability).",
            "uuid": "e419.0"
        },
        {
            "name_short": "GAN+CVX",
            "name_full": "GAN with differentiable convex optimization layer (GAN + CVX)",
            "brief_description": "A baseline hybrid that models object locations as optimal solutions to learned convex quadratic programs (QPs) embedded as differentiable convex optimization layers inside a GAN-style pipeline to capture implicit placement preferences while respecting convex constraints.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GAN + CVX (Generative Adversarial Network augmented with a differentiable convex optimization layer)",
            "system_description": "Baseline hybrid used in spatial-reasoning experiments: a GAN outputs parameters (or latent features) that feed into a differentiable convex optimization (quadratic program) layer; the QP's objective captures implicit spatial preferences and its convex constraints enforce positional constraints; QP solution yields coordinates for object placements and gradients are backpropagated through the convex layer during training using differentiable convex optimization techniques.",
            "declarative_component": "Convex optimization model (quadratic program) representing constraints and an objective learned from data; the convex layer enforces constraints expressed in convex form.",
            "imperative_component": "Generative Adversarial Network (GAN) producing objective parameters / inputs to the convex layer; gradient-based learning (end-to-end backpropagation through the convex layer using differentiable optimization methods).",
            "integration_method": "Differentiable embedding of a convex optimization solver as a neural network layer (as in 'differentiable convex optimization layers'); the GAN proposes objective/parameters and the convex solver produces feasible optimal coordinates; gradients from the supervised loss are backpropagated through the solver to update the GAN.",
            "emergent_properties": "Guarantee of constraint satisfaction when constraints are convex and representable in the QP; ability to learn an objective that captures implicit preferences while retaining differentiable end-to-end training; limited capability to express and enforce only convex constraints.",
            "task_or_benchmark": "Synthetic spatial positioning scenarios and spatial preference learning; compared against SPRING on preference accuracy and constraint satisfaction metrics.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Can generalize learned preferences under convex constraint families but limited for non-convex constraints; paper reports GAN+CVX matches constraint satisfaction for convex constraints but underperforms SPRING on preference accuracy and cannot handle non-convex constraint types.",
            "interpretability_properties": "Moderate: convex objective and constraints are explicit and interpretable, but the learned objective inside the QP may be less directly interpretable than symbolic logic clauses; the optimization solution is explainable in terms of convex feasibility/optimality.",
            "limitations_or_failures": "Cannot handle non-convex constraints or complex logical constraint combinations (disjunctions, arbitrary propositional formulas) because convex optimization layer requires convex representability; potential mode-collapse issues in GAN component; may fail to capture complex, non-convex commonsense spatial rules.",
            "theoretical_framework": "Optimization-as-layer framework: uses differentiable convex optimization to combine learned objectives (from neural nets) with declarative convex constraints, enabling end-to-end learning when constraints/objectives are convex.",
            "uuid": "e419.1"
        },
        {
            "name_short": "DiffConvexLayers",
            "name_full": "Differentiable Convex Optimization Layers",
            "brief_description": "A class of methods that embed a convex optimization problem (e.g., QP) as a differentiable layer in a neural architecture so that solution outputs can be backpropagated through during training.",
            "citation_title": "Differentiable convex optimization layers",
            "mention_or_use": "mention",
            "system_name": "Differentiable convex optimization layers (general technique)",
            "system_description": "Technique for integrating convex solvers into neural networks by providing differentiable mappings from inputs/parameters to optimization solutions, enabling gradient-based learning of upstream parameters that affect a constrained optimization outcome.",
            "declarative_component": "Convex optimization problem (quadratic programs, convex cones) representing constraints and objectives.",
            "imperative_component": "Gradient-based neural networks (any architecture) that emit solver parameters; automatic differentiation through the optimization solution map.",
            "integration_method": "Embed solver as a differentiable layer (use KKT/implicit differentiation or specialized backprop through QP solvers); end-to-end training with gradients flowing through the optimization layer.",
            "emergent_properties": "Ability to learn policies/objectives that respect convex constraints; enforce constraints during generation while still training with data; provides formal optimality properties of convex problems.",
            "task_or_benchmark": "Used generally in tasks requiring constrained outputs; cited in paper as an approach and used to motivate/compare methods for constrained location prediction (GAN+CVX baseline).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Generalizes within convex constraint families; limited for non-convex logical constraints or combinatorial structure not expressible convexly.",
            "interpretability_properties": "High interpretability for constraints and solver outputs due to explicit optimization formulation; learned parameters reflect preferences encoded into the objective.",
            "limitations_or_failures": "Inapplicable to non-convex constraints and complex propositional logic; may require careful problem formulation to admit convexity.",
            "theoretical_framework": "Optimization-as-differentiable-layer: uses convex duality/KKT differentiable mappings to allow gradients through solver outputs.",
            "uuid": "e419.2"
        },
        {
            "name_short": "DecDD-in-GAN",
            "name_full": "Embedding Decision Diagrams into Generative Adversarial Networks",
            "brief_description": "Approaches that integrate symbolic decision-diagram structures (representing combinatorial constraints) into GANs to enforce combinatorial constraints during generation.",
            "citation_title": "Embedding decision diagrams into generative adversarial networks",
            "mention_or_use": "mention",
            "system_name": "Embedding decision diagrams into GANs",
            "system_description": "Related work family where symbolic decision diagrams (representing discrete combinatorial constraints) are embedded or used to filter outputs of GANs so that generated samples satisfy combinatorial or discrete constraints; mentioned as prior art for constrained generative modeling.",
            "declarative_component": "Decision diagrams (explicit combinatorial constraint representations).",
            "imperative_component": "GANs (neural generative networks) that are conditioned/filtered by decision-diagram constraints.",
            "integration_method": "Symbolic structure (decision diagram) is embedded into or used in conjunction with a neural generator (GAN) so generated outputs adhere to discrete constraints; integration can be achieved by constraining latent codes or filtering outputs.",
            "emergent_properties": "Ability to enforce discrete/combinatorial constraints in generative models; improved feasibility of generated structured outputs.",
            "task_or_benchmark": "Constrained generation tasks (e.g., constrained schedule/layout generation); cited in paper as related work embedding symbolic structures into generative models.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Enforces combinatorial constraints at generation time; generalization depends on decision-diagram expressivity and GAN capacity.",
            "interpretability_properties": "Decision diagrams provide explicit, inspectable constraint structure; helps debug constraint satisfaction of generated samples.",
            "limitations_or_failures": "Requires decision-diagram construction and scalability may be limited by decision-diagram size; integration specifics vary across implementations.",
            "theoretical_framework": "Symbolic-structure conditioning of neural generators to guarantee combinatorial constraints.",
            "uuid": "e419.3"
        },
        {
            "name_short": "Neuro-Argumentative Learning",
            "name_full": "Neuro-Argumentative Learning / Neuro-Argumentation",
            "brief_description": "A family of methods combining neural models with structured argumentative (logical) reasoning to handle conflicting information, produce explanations, and integrate data-driven and rule-driven inference.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Neuro-Argumentative Learning",
            "system_description": "Methods that use neural components to extract or score claims/evidence and integrate these with an argumentative symbolic reasoning layer that constructs and evaluates logical arguments, often producing explanations and resolving conflicts between pieces of evidence.",
            "declarative_component": "Argumentation frameworks / symbolic argumentation systems (logical rules, attack/support relations), used to structure and evaluate arguments.",
            "imperative_component": "Neural networks that provide scores, features, or candidate propositions for the argumentation engine.",
            "integration_method": "Modular: neural subsystems propose or score facts/claims which are then composed and evaluated by symbolic argumentative reasoning engines; sometimes used to produce explainable outputs or dialectical explanations.",
            "emergent_properties": "Improved explainability and conflict-resolution capabilities compared to pure neural models; structured, dialectical explanations grounded in neural perception or scoring.",
            "task_or_benchmark": "Mentioned in related work as an example of neuro-symbolic integration for explainability and reasoning; applications include classification with explanations, dealing with conflicting information.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Potential for better generalization due to explicit reasoning layers; dependent on quality of argument construction and neural evidence.",
            "interpretability_properties": "High: produces argumentative explanations and structured logical reasoning artifacts that are inspectable.",
            "limitations_or_failures": "Scalability and integration complexity; depends on quality of neural evidence and alignment between neural outputs and symbolic rule formats.",
            "theoretical_framework": "Argumentation theory combined with neural evidence scoring; emphasizes dialectical, structured explanations.",
            "uuid": "e419.4"
        },
        {
            "name_short": "Neuro-Symbolic Concept Learner",
            "name_full": "Neuro-Symbolic Concept Learner (NS-CL)",
            "brief_description": "A prior neuro-symbolic system that disentangles perception (neural) from symbolic reasoning for tasks like visual question answering by learning visual concepts and using symbolic programs for logical reasoning.",
            "citation_title": "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
            "mention_or_use": "mention",
            "system_name": "Neuro-Symbolic Concept Learner (NS-CL)",
            "system_description": "Architecture (from cited prior work) that uses neural perception to extract object-level concepts and a symbolic program executor (learned or induced) to perform compositional reasoning over these concepts; cited as example of disentangling vision and symbolic reasoning.",
            "declarative_component": "Symbolic program executor / logic programs for multi-step compositional reasoning (e.g., question-specific programs).",
            "imperative_component": "Neural perception modules (CNNs) to produce object attributes/concepts from images; neural module networks for executor interfaces.",
            "integration_method": "Pipeline: perception produces symbolic concept representations which are consumed by a symbolic program interpreter to execute compositional queries; supervision can be weak (natural language paired with images).",
            "emergent_properties": "Compositional generalization and interpretable multi-step reasoning over visual scenes; separation of perception and reasoning improves systematicity.",
            "task_or_benchmark": "Visual question answering and scene understanding; mentioned in related work as a neuro-symbolic example.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Reported compositional generalization benefits in original NS-CL work; cited here as background motivation.",
            "interpretability_properties": "High: outputs are symbolic programs and intermediate concept representations that are inspectable.",
            "limitations_or_failures": "Requires object-level supervision or reliable object detectors; scaling to unconstrained natural images and open vocabularies is challenging.",
            "theoretical_framework": "Separation of perception (neural) and symbolic programmatic reasoning to achieve compositionality and interpretability.",
            "uuid": "e419.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Differentiable convex optimization layers",
            "rating": 2,
            "sanitized_title": "differentiable_convex_optimization_layers"
        },
        {
            "paper_title": "Embedding decision diagrams into generative adversarial networks",
            "rating": 2,
            "sanitized_title": "embedding_decision_diagrams_into_generative_adversarial_networks"
        },
        {
            "paper_title": "Constraint reasoning embedded structured prediction",
            "rating": 2,
            "sanitized_title": "constraint_reasoning_embedded_structured_prediction"
        },
        {
            "paper_title": "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
            "rating": 2,
            "sanitized_title": "the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision"
        },
        {
            "paper_title": "Neural-symbolic vqa: Disentangling reasoning from vision and language understanding",
            "rating": 1,
            "sanitized_title": "neuralsymbolic_vqa_disentangling_reasoning_from_vision_and_language_understanding"
        },
        {
            "paper_title": "Neuro-argumentative learning",
            "rating": 1,
            "sanitized_title": "neuroargumentative_learning"
        }
    ],
    "cost": 0.0193095,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>INTEGRATING SYMBOLIC REASONING INTO NEURAL GENERATIVE MODELS FOR DESIGN GENERATION A PREPRINT</p>
<p>Maxwell J Jacobson jacobs57@purdue.edu 
Yexiang Xue yexiang@purdue.edu </p>
<p>Department of Computer Science
Purdue University
USA</p>
<p>Department of Computer Science
Purdue University
USA</p>
<p>INTEGRATING SYMBOLIC REASONING INTO NEURAL GENERATIVE MODELS FOR DESIGN GENERATION A PREPRINT</p>
<p>Design generation requires tight integration of neural and symbolic reasoning, as good design must meet explicit user needs and honor implicit rules for aesthetics, utility, and convenience. Current automated design tools driven by neural networks produce appealing designs, but cannot satisfy user specifications and utility requirements. Symbolic reasoning tools, such as constraint programming, cannot perceive low-level visual information in images or capture subtle aspects such as aesthetics. We introduce the Spatial Reasoning Integrated Generator (SPRING) for design generation. SPRING embeds a neural and symbolic integrated spatial reasoning module inside the deep generative network. The spatial reasoning module decides the locations of objects to be generated in the form of bounding boxes, which are predicted by a recurrent neural network and filtered by symbolic constraint satisfaction. Embedding symbolic reasoning into neural generation guarantees that the output of SPRING satisfies user requirements. Furthermore, SPRING offers interpretability, allowing users to visualize and diagnose the generation process through the bounding boxes. SPRING is also adept at managing novel user specifications not encountered during its training, thanks to its proficiency in zero-shot constraint transfer. Quantitative evaluations and a human study reveal that SPRING outperforms baseline generative models, excelling in delivering high design quality and better meeting user specifications.</p>
<p>Introduction</p>
<p>Neural and symbolic approaches are two fundamental drivers behind the progress of artificial intelligence (AI). Neural approaches have spearheaded major developments in learning from diverse and unstructured data, discovering hidden or fuzzy patterns, and producing effective predictive and generative models. Nevertheless, it is challenging for neural models to provide formal guarantees. Furthermore, predictions made by neural models may violate constraints, especially from rare and unseen inputs. Symbolic approaches have produced efficient and reliable algorithms that can provide formal guarantees, interpretability, and robustness. However, symbolic approaches often necessitate fully formulated problems prior to attempting solutions, leading to rigid models that are difficult to adapt to changing data distributions. Neither neural nor symbolic methods offer a complete solution on their own, as each has its own set of strengths and weaknesses. Consequently, many meaningful real-world tasks remain outside the reach of current artificial intelligences. A better solution would be the integration of neural and symbolic approaches. However, it is not trivial to bridge the two approaches due to multiple aspects of incompatibility. Recently, exciting work has emerged to unite neural and symbolic reasoning [55,68,97,6,101,73,66,13]. However, the complete success of neural-symbolic integration is far from achieved.</p>
<p>Design generation is a field that requires both neural and symbolic reasoning. Good design must meet industry standards and user specifications while capturing subtle aspects such as aesthetics and convenience. Symbolic approaches -for example, a constraint program -can be defined to create a plan that meets all standards and needs, given the full specifications of the existing design. Even so, constraint programs cannot understand designs in the form of pictures or videos. Nor can they provide compelling visualizations of the generated designs. Moreover, designs generated by constraint programming often satisfy the bare minimum of functionality without considering subtle aspects such as Input Input Constraint: Add a blue microwave right of the oven, and a green toaster left of the oven and below the sink. Figure 1: An interior design generated by our proposed SPRING model (middle) with a given background already containing an oven and a sink among other objects (left). The user specifications are at the bottom (provided to SPRING in the form of propositional logic; natural language text is used here to aid readability). SPRING creates a design satisfying the specifications. Text-to-image approaches like Stable Diffusion (right) often fail to meet these constraints.</p>
<p>SPRING Output (ours) Baseline (Stable Diffusion)</p>
<p>aesthetics and convenience. In fact, it is almost impossible to encode such subtle aspects into the objective function of a constraint program. Therefore, designs generated by constraint satisfaction solely are rarely welcomed by customers. This demonstrates a deep drawback in purely symbolic methods like constraint programming -they cannot engage with the nebulous world of low-level perception and reasoning. Recent advances in machine learning, in particular deep-generative models, have presented new opportunities to address these challenges. Text-to-image and graph-toimage models provide exciting possibilities for controllable content generation, but their control is not always precise enough to produce designs that meet complex specifications. For example, in Figure 1, a toaster must be added to the left of the oven and below the sink (already in the image). Additionally, a microwave needs to be added to the right of the oven. In the right panel, the Stable Diffusion model (one of the state-of-the-art neural generative models), taking the input of the initial kitchen configuration and the text specifications, simply alters the entire scene, producing results that look pleasing but do not fit the specification. This is emblematic of a deep issue with purely neural algorithms -thus far, they have failed to grasp the high-level reasoning that symbolic approaches can handle efficiently. Overall, AI-driven automatic design is an emerging field that requires the integration of neural and symbolic reasoning. However, such integration is beyond the reach of state-of-the-art models.</p>
<p>Our Approach. We introduce Spatial Reasoning Integrated Generator (SPRING) for design production. Given an initial indoor scene and user requirements described in propositional logic, the task is to generate a design that satisfies user specifications, looks pleasing and follows common sense. The essence of SPRING is the embedding of a neural and symbolic integrated spatial reasoning module within the deep generative network. The spatial reasoning module decides the locations of the objects to be generated in the form of bounding boxes, following an iterative refinement approach. The bounding boxes are predicted by a sequence-to-sequence neural model and are further filtered by symbolic constraint reasoning (forward checking). This integrated approach allows us to leverage the advantages of both neural and symbolic approaches. Here, symbolic constraint satisfaction deals with explicit specifications, such as user requirements, while neural networks handle aesthetics and common sense.</p>
<p>SPRING consists of three modules. The first perception module based on Detection Transformers (DETR) [19] extracts existing object positions from input images. It is followed by the spatial reasoning module, which uses neural and symbolic integrated approaches to generate the bounding boxes. When determining one coordinate of the bounding box (e.g., the x, y coordinates, width, or height), the recursive neural network in the spatial reasoning module iteratively halves the range of each coordinate until it is sufficiently small. During learning, the spatial reasoning module is trained to understand implicit spatial knowledge, such as potted plants usually being located on the floor, etc. Learned knowledge is reflected in the decisions it makes (that is, which half range of the coordinate falls into in every step). Training is completed by a teacher-forcing procedure that matches the bounding boxes predicted by the spatial reasoning module and those that contain the objects in the training images. During inference, explicit spatial constraints are enforced by a symbolic reasoning algorithm, which blocks decisions that necessarily lead to constraint  The neural and symbolic integrated spatial reasoning module decides the bounding boxes of each object to be generated. It iteratively halves each coordinate of every bounding box until it is sufficiently small. Symbolic reasoning is applied to the output of the neural net to ensure satisfaction of user constraints. These bounding boxes are filled by the visual element generator in the last step.</p>
<p>(Fourth row) Example pictures generated by SPRING demonstrate good quality designs satisfying user specifications in Figure 1.</p>
<p>violations. Finally, the bounding boxes are filled by a visual element generator, which is a diffusion model. The three modules of SPRING are illustrated in Figure 2.</p>
<p>How SPRING Advances the State-of-the-art:</p>
<p>(1) Guaranteed User Requirement Satisfaction via Embedding Symbolic Reasoning into Neural Generative Models: SPRING guarantees that the generated designs satisfy user specifications. This is beyond the capabilities of the stateof-the-art neural generative models.</p>
<p>(2) Interpretable Model: SPRING is more interpretable than alternate methods. At each step of iterative refinement, the user can trace the probabilities associated with each sub-decision made by the spatial reasoning module. This helps to debug potential dissatisfaction of implicit preferences.</p>
<p>(3) Zero-shot Transfer to Novel User Specifications: The spatial reasoning module is capable of handling novel constraints in a zero-shot manner. When novel user specifications not present in the training set are given at test time, the symbolic reasoning procedure in the spatial reasoning module still blocks the invalid output from the neural networks in the same way as handling familiar constraints, without the need for retraining or fine-tuning.</p>
<p>SPRING was evaluated in the domain of interior design generation, in which the AI must add elements to an interior space like a kitchen or a living room, taking into account existing objects and constraints given by the user. In this setting, it is vital that the algorithm produces images that satisfy the user's design specification, while also maintaining a high overall image quality. Accuracy metrics were collected to objectively measure how well SPRING was able to satisfy spatial constraints from the specification. SPRING was able to produce designs that satisfy complex user specifications, while text-based or scene graph-based baseline approaches could not. Regarding image quality, spatial naturalness and general aesthetics are both important factors in a good-looking design. Synthetic positioning datasets were created to evaluate SPRING's ability to learn implicit spatial constraints, such as toasters should not be put on the floor, TVs may hang on the wall, etc. In this situation, SPRING was able to outperform a baseline made up of a Generative Adversarial Network (GAN) further filtered by a differentiable convex programming layer. For general aesthetics, automated image quality metrics were collected in the form of FID and IS scores. The images produced by SPRING were on par in quality with the latest image generation tools. As additional support for these evaluations, a human study was conducted to assess SPRING for both specification satisfaction and image quality. The results corroborated the effectiveness of SPRING in generating high-quality images that meet user specifications while maintaining the overall aesthetics and spatial naturalness of the design. Furthermore, SPRING is capable of handling novel user constraints not presented in the training set in the zero-shot learning environment, which was demonstrated by its ability to satisfy complex novel constraints after training was complete.</p>
<p>How SPRING Advances Artificial Intelligence in General:</p>
<p>SPRING's contribution to the broader field of AI is underscored by its successful integration of neural and symbolic approaches. It exemplifies a powerful synergy where the learning and adaptability of neural models are enhanced by the precision and reliability of symbolic methods. This integration addresses a fundamental challenge in AI, offering a balanced solution that leverages the strengths of both methodologies while mitigating their weaknesses. For AI researchers and practitioners across various domains, SPRING provides a practical blueprint for developing more robust, adaptable, and intelligent systems capable of handling complex, real-world tasks with enhanced efficiency and reliability.</p>
<p>Problem Definition</p>
<p>Design generation is the problem of producing designs in the form of diagrams, environments, or images. The example chiefly explored here is interior design, in which the image is an interior in a home, such as a kitchen or living room, and the design is specified as a set of objects to add which have positional relationships to each other and objects already in the image. The problem can be defined as: Problem 1: (Design Production): Given: let B be a background image that contains initial objects W, and D be a design specification with new objects O and positional constraints C represented in the propositional design language defined below. C may reference objects in both O and W. T is the set of natural images. Find a scene image S, with B as a background, containing objects defined by O, such that all of C is satisfied and S is realistic; i.e., it is visually close to the images in T.</p>
<p>Propositional Design Language</p>
<p>Symbols and Constants. Our design language uses propositional logic. In this language, o 1 , o 2 , . . . , o N denote objects. These objects represent furniture or other objects that are either in the background image or need to be added.  top side of o 1 has a y-value equal to c. Table 1: Spatial relations and their truth conditions. Note that integer literals (shown here as c) are in units of perthousanths of the background image's width and height.</p>
<p>The constants used in the design language are integers and text strings. The integers are often used to denote the spatial distances between objects, while text strings define the properties and types of the objects.</p>
<p>Properties and Types. Property is a predicate that evaluates to true if and only if an object has a given property. For example, property(o 1 , "blue ′′ ) is true if and only if object o 1 has the property "blue ′′ (i.e., its color is blue). A special property is called type. This defines the type of object being reasoned over from a set of known types. For example, type(o 1 , "microwave ′′ ) evaluates to true if and only if o 1 has the type "microwave ′′ . Combining with the previous example, type(o 1 , "microwave ′′ ) ∧ property(o 1 , "blue ′′ ) means object o 1 is a blue microwave. Our language recognizes the following types: chair, couch, potted plant, bed, mirror, dining table, window, desk, toilet, door, TV, microwave, oven, toaster, sink, refrigerator, and blender.</p>
<p>Relations.</p>
<p>Relations are used to model spatial constraints between objects, providing a way to describe the relative position, size, and alignment of objects within a design. These relations evaluate to true if the spatial relationship is upheld between the objects included, and false otherwise. Our grammar includes various types of predicates to express these relationships (a complete description is available in Table 1). Complex Relationship. The complex spatial relationship among objects can be defined using logic operators (∧ for "and", ∨ for "or", ¬ for "not") to connect the set of predicates discussed above. For example, "a cozy brown leather chair is either completely left or completely right of the black and white striped couch." can be described as:
type(o 1 , "chair ′′ ) ∧ property(o 1 , "cozy ′′ ) ∧ property(o 1 , "brown ′′ )∧ type(o 2 , "couch ′′ ) ∧ property(o 2 , "black and white striped ′′ )∧ (cleft(o 1 , o 2 ) ∨ cright(o 1 , o 2 )).
This completes our definition of the propositional design language. While natural language is usually easier to use than structured languages like ours, it may suffer from ambiguities, which render it difficult to satisfy hard rules. Nevertheless, we intend to eventually extend our work to consider natural language as well. We leave such effort for future work.</p>
<p>Spatial Reasoning Integrated Generator</p>
<p>SPRING is motivated by the desire to smoothly integrate the handling of implicit preferences from data and explicit rules from the user to generate high quality design images. It does this by integrating a symbolic component to enforce the rules with a neural component for learning implicit preferences, perceiving the background, and drawing the image. Following this, SPRING is composed of a neural perception module, a hybrid spatial reasoning module, and a neural visual element generation module.</p>
<p>The perception module is designed to identify and locate objects present in the background image. It employs an object detector for predicting existing objects and their bounding boxes, and a scene encoder for extracting scene-level features.</p>
<p>The spatial reasoning module decides the spatial position of each object. This is also the module which, at its core, integrates symbolic and neural methods. In our setting, the spatial position is represented as a bounding box with parameters [x, y, width, height]. Here, x, y is the coordinate of the upper-left corner and width, height represent the width and height of the bounding box. We generate the bounding boxes of each object following an iterative refinement process, integrating neural network prediction and constraint reasoning. In particular, a recurrent neural network based on GRUs [23] iteratively halves the range of each coordinate. For example, the range of x starts at the width of the image. In one step, GRU decides whether we should halve the range of x to be the left side or the right side. Such steps repeat until the range of x is sufficiently small, and the middle point is chosen. During this process, symbolic reasoning is used to rule out invalid outputs which violate user specifications. The symbolic reasoning module is implemented as a forward-checking procedure. It forces the GRU to take a different decision if it detects the current decision leads to constraint violations. Note that the symbolic reasoning forward-checking algorithm fits seamlessly with the GRU, does not need to be trained, and can be edited programmatically without affecting the operation of the GRU. Due to this structure, the decisions made by the GRU take into account both implicit preferences and explicit rules. During learning, the spatial reasoning module is presented with a set of background images and the corresponding bounding boxes of various objects in the images and is trained to predict the boxes that match those in the training set. In this way, GRU learns to meet implicit preferences such as "TVs may be mounted on the wall, but kitchen tables are not".</p>
<p>The constraint reasoning tool embedded in the spatial reasoning module allows SPRING to be transferred to new constraints not presented in the training set in a zero-shot learning setting. This is because the symbolic reasoning module can still rule out infeasible locations even when such constraints never appear in the training set. In our proposed architecture, the GRU represents the neural aspect, capturing and refining implicit preferences, while the forward-checking module embodies the symbolic side, ensuring decisions adhere to explicit constraints. Combining them allows for a harmonious integration of learned preferences and rule-based specifications in spatial reasoningensuring that the placements are both intuitive and constrained.</p>
<p>In the third step, the visual element generation module takes as input the background, the prompt, and location of each object in the form of a layout, and outputs an image patch which contains the object and can be merged seamlessly into the background image.</p>
<p>Perception Module</p>
<p>The purpose of the perception module is to extract information about the existing background for the SRM -which is utilized both in iterative refinement for implicit preference and explicit rule satisfaction. The perception module includes an object detector for existing object prediction and a scene encoder for scene-level feature extraction. The object detector is implemented with a pre-trained DETR50 [19] object-detection model trained on the COCO dataset [62], but this can be jointly trained with the spatial reasoning module given a new dataset. The objects predicted are encoded into the SRM hidden state through a deterministic forward pass, but they can also be referenced in the propositional logic constraint language. A Resnet18 [47] architecture pre-trained on ImageNet [33] is used as the scene encoder, which is fine-tuned during training as a part of the SRM. The features it encodes are used to instantiate the hidden state of the SRM, which is updated by that module's variable encoders and the main GRU itself.</p>
<p>Spatial Reasoning Module (SRM)</p>
<p>Architecture &amp; Semantics of the SRM. The spatial position of an object is represented as a bounding box with parameters (x, y, width, height), where x, y are the coordinates of the upper-left corner and width, height are the dimensions of the bounding box. The set of these bounding boxes is conventionally known as a "layout" [106]. The spatial reasoning module is implemented using a Recurrent Neural Net (RNN) structure, further filtered by symbolic reasoning. The RNN, specifically implemented using a GRU, takes the input of the background image, the object identifiers (e.g., object 0, type "chair"), and outputs the four coordinates for each object to be generated sequentially. The background image is encoded by an encoder network as the initial hidden state of the RNN. Starting from this state, the SRM decides the bounding box of each object to be generated sequentially. Moreover, the locations of the bounding boxes are further filtered by a symbolic reasoning module to ensure constraint satisfaction.</p>
<p>The key idea for RNN to generate each coordinate is through iterative refinement. The basic unit of the RNN output is a decision token -a size 4 vector representing a score distribution of three possible decisions and one special start token. The input at each timestep is the previous timestep's decision as a one-hot vector. The network outputs at each step a softmax vector to assign the probability to each token. At the start of calculation, the selected variable is presumed to be within a given range (e.g. the first object's x value is between 0 and 100). The actions are to limit the position to be in the first half of the range (e.g. left: 0 ≤ x &lt; 50), or in the second half of the range (e.g. right:</p>
<p>50 &lt; x ≤ 100), or to terminate and select the middle value between the current minimum and maximum as the final location (e.g. stop: x = 50). This produces a sequence of decisions leading to the assignment to the corresponding variable. The sequence is embodied as a decision string composed of decision tokens. For example, the variable x between 0 and 100 may be assigned the decision string "left, right, stop", refining the value to 0 ≤ x &lt; 50 as we choose the left of the range, then 25 &lt; x &lt; 50 as we choose the right of the range, then x = 37 as we choose the center of the range. Our recurrent neural network is implemented in the form of GRUs. The GRU state vector (size 500) is also updated by summing it with the background image encoding and the object identifier encodings.</p>
<p>In this work, the primary network of the SRM is made up of 3 gated recurrent units followed by a dense layer, and a Softmax activated output layer. GRUs use a hidden state to preserve information across time, and manage that information using internal reset and update gates. This hidden state is initialized from a vector produced by the scene encoder within the perception module. The scene encoder is a pretrained ResNet18 backbone augmented with a dense layer. The purpose of this network is to extract features from the scene as a whole. The hidden state is also updated by a simple variable encoder, which produces a vector for each positional variable of each object to let the GRUs know that a new object or variable has started, and to encode the object class and variable type. Figure 2 shows the processes of generating two coordinates x 1 and y 1 using the proposed neural network module, including the encoding at the start of each variable.</p>
<p>-Attempt right.</p>
<p>-Check constraints for [25,50].
-25 &gt; 20 C 1 is not satisfiable. -Attempt left. -Check constraints for [0, 25]. -0 &lt; 20 C 1 satisfiable. -Go left.
-Attempt stop.</p>
<p>-Check constraints for [12,12].</p>
<p>-0 &lt; 20 C 1 satisfiable. stop.  Symbolic Reasoning in the SRM. The SRM leverages symbolic reasoning to filter RNN output to generate object locations satisfying user specifications. The SRM's GRU network outputs a decision string for each positional variable by sampling tokens from the probability distribution output from the iterative refinement process until a conclusive value is reached. At the beginning of a string, the start token is given as input and the variable encoder adds information to the GRU state vector. The decisions are sampled based on the probabilities output by the spatial reasoning module. For example, if the module outputs the scores [lef t : 75%, right : 5%, stop : 20%], the spatial reasoning module will select "left" 75% of the time, "right" 5% of the time, and will choose to stop 20% of the time and to choose the middle value of the current range.</p>
<p>Symbolic reasoning in the form of forward checking is used to explicitly enforce positional constraints from the users. It does this by ensuring that decisions which lead to constraints violation are ignored. For each decision in the iterative refinement, the algorithm checks that the current value range contains at least one value assignment which satisfies all constraints. If at least one constraint must be violated, the SRM rewinds the decision string one token and resamples without the option of the original choice. Our forward checking algorithm is implemented as a depth first search, exploring all future expansions of the decision strings. This forward checking process is conducted after each decision made by the GRU to see if constraint satisfaction is still possible. In our previous example, we may have selected the "left" token first, but then found that no value in the remaining range for that variable can satisfy all the constraints. We resample this token with new scores [lef t : 0%, right : 20%, stop : 80%]. The stop token is now the most likely to be chosen. The process is visualized in Figure 3.</p>
<p>During design generation, existing objects perceived in the background image may influence the decisions made by the SRM. Not only should the user be able to set positional constraints that reference existing objects, but these objects must be encoded in the neural component of the SRM as well. This is done by making no distinction between objects that already exist in the background -those predicted by the perception module -and objects that need to be generated in the scope of the SRM. Each of the existing objects is constrained to their already-known positional values, but they are still processed through the SRM to update the hidden state. As the constraints are set to a single value, every decision is forced down a particular path. These are processed through the SRM first before new objects.</p>
<p>Learning in the SRM. Aside from the explicit constraints given by the users, the locations predicted by the SRM should satisfy implicit preferences such as "toasters are rarely placed on the ground". The training of SRM uses a dataset containing background images with a bounding box for each object to be generated. Each object's location is defined by the upper left point, as well as the width and height of the bounding box. These variables are converted to decision strings as described in the previous sections. The training process is a supervised procedure -it trains the spatial reasoning module to generate bounding boxes (in the form of decision strings) which match those in the training dataset. This procedure teaches the neural network implicit spatial knowledge including the relative sizes of objects and their common spatial relationships (toasters go on the counter, televisions might be hung on the wall, etc).</p>
<p>The training of RNN is accomplished in-part through teacher forcing [96]. In teacher forcing, the ground truth decision token from the previous step drawn from the training set is used as the input to train the RNN to predict the decision of the current step. Teacher forcing can drastically speed up RNN training, but can also result in lower resilience to uncommon sequences. For this reason, a mixture of strategies -with and without teacher forcing -is used to train the model. 50% of the time the input of the RNN is set by teacher forcing and 50% set by the predictions of the RNN itself. We also randomize the order of the objects in the decision string during training.</p>
<p>The spatial reasoning module also needs to learn to encode the background image -extracting important features like the positions of the floor, ceiling, and walls. This is accomplished by the scene encoder in the perception module.</p>
<p>Gradients are back-propagated through the scene encoder network as a unified model.</p>
<p>Visual Element Generator (VEG)</p>
<p>The bounding boxes from the spatial reasoning module generate each object into the scene using the visual element generator. The result is a completed scene adhering to the design with all positional constraints guaranteed. The visual element generation module is sequentially called on each bounding box created by the spatial reasoning module. We leverage state-of-the-art neural generative models for our visual element generator, bypassing the need for extensive datasets and in-house model training. Our implementation is capable of using either the reduced and filtered GLIDE [72] model or the Stable Diffusion model [79]. In VEG, we give neural generative models a small margin of the surroundings of the object to be generated and ask them to inpaint the middle portion. One challenge of any diffusion model is the rare chance that it will generate the wrong object or no object at all. These inpainting failures are possible, though our method reduces them as each call is only responsible for a single object placed in a reasonable location. Notice that objects that are painted correctly are guaranteed to follow positional constraints.</p>
<p>Related Work</p>
<p>Neuro-Symbolic AI. Neuro-symbolic methods combine neural networks, which excel at pattern recognition and are adaptable to diverse tasks, with symbolic systems, which are adept at reasoning and explicit knowledge representation. This hybrid approach aims to leverage the strengths of both methodologies to create AI systems that can learn from data and reason with symbolic knowledge. This concept has produced numerous recent breakthroughs [68,97,6,101,73,66,43,42].</p>
<p>Much of the work into neuro-symbolic methods have focused on their much greater explainability than conventional neural methods, without sacrificing the predictive power of those same networks. This has taken many forms, including producing first-order logic explained predictions [46,24,9,25] and concept-based reasoning [67,66].</p>
<p>Work has also gone into using the formalisms in symbolic reasoning to make neural networks safer [100] and more fair [41,29,30]. This is broadly possible because symbolic algorithms often provide useful guarantees on their output that go beyond what purely neural methods can supply.</p>
<p>Still other approaches focus on the potential benefits to generalizability that symbolic approaches bring, allowing the system to deal with noise and nebulous concepts using neural approaches, and abstract reasoning using symbolic ones [86,66,39,51].</p>
<p>Methods of integration, desired outcomes, and the symbolic approaches used differ. For example, neuro-argumentative learning [89,107,10,28,4,31,85,7,26,27] integrates neural algorithms and symbolic argumentative reasoning, where the latter uses structured logical arguments to process conflicting information. By combining the two, it leverages the pattern recognition capabilities of neural networks to inform and refine the logical arguments, resulting in a system that can adapt to and often explain new data while maintaining structured reasoning capabilities. This question of integration is a major consideration for all parts of the field [13]. Our work focuses on the integration of neural networks with constraint reasoning, in the domain of design generation.</p>
<p>Fast &amp; Slow Thinking. Our work also draws inspiration from the fast and slow thinking model described in works from cognitive psychology. Psychophysiological studies have long supported the concept of dual-process thinking in humans [84,63] -in which more deliberate and more intuitive thinking are the result of separate fundamental processes. System 1 refers to a set of innate and learned automatic processes that are rapid, parallel, and unconscious. System 2 is responsible for abstract and hypothetical thinking and is slower, more deliberate, and more conscious [54,38]. These concepts hold exciting parallels for the world of AI, which are already being seriously explored [60,5]. Recent work has explored the idea that inspiration can be drawn from these neurocognitive concepts to improve adaptability, generalizability, common sense, and causal reasoning in AI [16].</p>
<p>Our SPRING leverages neural networks for S1 cognition while delegating S2 cognition and planning to constraint reasoning. In this regard, our work takes inspiration from concepts like fast and slow planning [40], with the notable change that our system does not utilize any sort of meta-cognition AI-subsystem [44] to regulate between fast and slow thinking modules explicitly.</p>
<p>Constraint</p>
<p>Reasoning &amp; Machine Learning. Constraint reasoning's integration with deep learning aims to boost reasoning and optimize performance. Historically, this has included incorporating "common-sense" reasoning into natural language models [48,90,74], case-based reasoning from expert systems [50], and relational reasoning between objects or terms [82,59,75,103,36].</p>
<p>Embedded constraint reasoning in machine learning is a promising idea [17,94,65,15,80], providing a structured way to incorporate domain knowledge and improve problem-solving capabilities. Convex optimization has been successfully employed as a neural net layer [18,1,3,2], and other constraint reasoning methods have been integrated into neural nets for various applications [12,34,99,8,57,21,69]. Often, this comes in the form of using data to produce constraint models. Constraint acquisition enables learning of constraint networks from examples [14,76,91].</p>
<p>Other research is focused on constraint models embedded into networks. For example, decision diagrams have been successfully integrated into GANs for constrained schedule generation [99], and similar approaches have excelled in text2SQL generation and similar tasks [52].</p>
<p>Automatic Design. Design domains have presented an exciting challenge for AI developers -recent progress has been made in architecture design [102], chip design [56], and biosystems [93]. Automatic interior design problems have been tackled in several ways previously [22] -including reinforcement learning [95] and conditioning [20]. Often, the data generated for this task is an abstract diagram of an interior space [58,70,71], but this approach is hard to visualize for a user. Other works focus on interior space images [61], offering better visual qualities but less structure within the data itself.</p>
<p>Text-to-Image &amp; Graph-to-Image Methods. Large diffusion models like DALL-E [78,77], GLIDE [72], and Stable Diffusion [79] can inpaint objects into scenes given a natural language prompt -made possible by large online datasets and the combination of transformer [92] and diffusion [83] models. However, they fail on complex instructions, such as constraining the number and spatial relationships of objects.</p>
<p>Another strong line of research encodes requirements into hidden vectors via graph or recurrent nets [32,98,37,64], and then conditions image generation on these vectors. This includes sg2im [53], which encodes positional relationships into scene graphs for image generation. These approaches struggle with complex constraints and those not present in training. Other works [104,35,45] adapt this approach for image-to-image problems, using scene graphs as editable latent representations. Also related are layout-to-image approaches [106,87] which generate an image from a background and user-supplied layout. Most recently, this has been achieved with ControlNets [105], which allow efficient tuning to new conditions for large diffusion models. Yet, these all require the positions of all objects to be strictly and completely defined by users (e.g., as sketches) and do not reason about positions or spatial relationships.</p>
<p>Experiments</p>
<p>Evaluating the performance of SPRING for interior design production involves examining three important aspects of the algorithm: the ability to consistently meet design requirements (i.e., constraints), the naturalness of the Spatial Reasoning Module's spatial predictions, and the realism of the final images produced by SPRING. It is important to emphasize that assessing the quality of generated images is a challenging problem in the field, and determining the naturalness of positions and dimensions, independent of aesthetic quality, is even more difficult. To tackle these challenges, the first aspect (constraint satisfaction) was evaluated using synthetic data scenarios for precise measurement, while the second and third aspects (quality of images, naturalness of positions and dimensions) were trained on real data. To address the known issues with automatic metrics commonly employed in image generation tasks, a human survey was also conducted.</p>
<p>Combining all of these experiments, several key conclusions can be drawn:</p>
<p>• SPRING is better than baselines at producing design scenes that fit the specification: This is evident from the design adherence experiments (higher position accuracy and object accuracy), as well as the scores in the human study.</p>
<p>• SPRING has comparable or superior image quality to baselines: This is evident from the image quality experiments (similar or better FID and IS scores), the human study, and the targeted spatial reasoning experiment (superior preference accuracy).</p>
<p>• SPRING can create complex scenes with zero-shot adapted constraints: This is demonstrated in the creation of the atop constraint and the images combining it with conjunctions and disjunctions (displayed in Figure 9). Figure 4: A bar-chart summary of evaluation metrics. Note the 3 main points: SPRING is better at specification satisfaction, SPRING creates images approximately as good-looking as state-of-the-art image generators (Stable Diffusion), and the human study supports both of the previous points. Background preservation and spatial naturalness scores are also given as specific aspects of specification satisfaction and aesthetics respectively. The pattern holds for these as well -similar naturalness, improved preservation.</p>
<p>Evaluating Image Quality</p>
<p>This experiment assesses the visual quality of images produced by SPRING using the leading automated image quality metrics.</p>
<p>Setup</p>
<p>Generating Specifications. To evaluate the visual quality of the produced images, 10,000 scenes were generated from SPRING and from baselines using random specifications generated via the following procedure. These specifications were unconstrained in position placement to focus on assessing the visual quality of images, not the feasibility of constraints in varied backgrounds. Possible object types were selected based on the room being generated -chair, couch, potted plant, dining table, and television for living rooms; microwave, oven, toaster, refrigerator, and potted plant for kitchens. A requested number of objects were selected for each scene, with replacement. Additionally, natural language prompts were generated programmatically for use with the Stable Diffusion baselines. These prompts listed all objects to be included. For example, "An oven, a refrigerator, and a toaster".</p>
<p>Baselines. SPRING's performance was compared with Stable Diffusion, and SG2IM, representing text-to-image and graph-to-image methods respectively. Another text-to-image baseline -GLIDE -was also checked on a smaller set of examples, but was found to produce lower quality images than Stable Diffusion. Rectangular masks, centered in the  </p>
<p>SPRING[SD] SPRING[GLIDE] Stable Diffusion GLIDE SG2IM</p>
<p>A refrigerator left of an oven and a microwave right and above the same oven.</p>
<p>A microwave, an oven, a toaster, and a sink. The sink is left of and at least partly above the oven, the microwave is right of and above the oven, and the toaster is below the microwave.</p>
<p>A blue microwave above a black oven. Figure 7: Comparisons between SPRING (VEG choice in brackets) and baselines on kitchen backgrounds from various web-sources. SPRING creates distinct objects and always follows positional constraints. Baseline methods produce compelling images, but rarely fulfill all constraints. They also greatly disrupt the background. Notice how the word "blue" in the first row affects the SPRING and baseline images. SPRING applies blueness to the microwave, while Stable Diffusion and GLIDE apply it to other parts of the image. (See appendix table 7 for logic definitions) image and featuring a 20-pixel margin around each edge, defined every mask in the Stable Diffusion baseline. This allowed Stable Diffusion to retain more context from the background.</p>
<p>During our evaluation, we found that the images generated by SG2IM were of low quality and displayed minimal variations when given the same scene graph. We maintained consistency with the parameters and code provided in the original SG2IM paper. After reaching out to the author via GitHub, we learned that the poor image quality was likely due to mode collapse. Nevertheless, we included these results in our evaluation for a comprehensive comparison.</p>
<p>Background images. Background images were generated by the algorithms themselves, increasing the difficulty of the task. SPRING handles this by using the VEG with the prompt "a clean, empty, living room." or "a clean, empty, kitchen.". Stable Diffusion used the same approach, generating a background before receiving the specification and then inpainting it to complete the task. As SG2IM inherently generates its own backgrounds, this aspect remains consistent with its usual operation.</p>
<p>Training. SPRING was trained on a subset of the COCO 2017 Detection dataset relating to interior design. It is difficult to find real-world datasets of background images (containing few objects) with bounding boxes of the objects to be generated. We instead use datasets of images containing objects and their corresponding bounding boxes (such as COCO). We remove these objects from the original images using the Telea method [88]. This lets us train SPRING to place bounding boxes in their most natural positions. To ensure that the dataset was focused on interior design, only images including one or more of the following categories were included: chair, couch, potted plant, bed, mirror, dining table, window, desk, toilet, door, TV, microwave, oven, toaster, sink, refrigerator, or blender. In order to train SPRING in a supervised manner, it was necessary to remove these objects from the images, leaving only the empty backgrounds and bounding box annotations. To accomplish this, various methods were tested with the goal of achieving visually pleasing results while also being efficient as numerous images needed to be processed. The Telea method was ultimately selected as it uses an image's gradient information to guide the inpainting process, resulting in visually plausible results.</p>
<p>SPRING was trained with a learning rate of 0.0001 and a batch size of 8 for 100 epochs. Negative Log Likelihood loss was utilized. Instead of training on pixel values, SPRING was trained on a relative system of per mille (parts per thousand). This allows the network to operate using the same parameters on multiple image sizes, but restricts precision slightly.</p>
<p>Metrics. The primary metrics we collect here are the Inception score (IS) [81] and Fréchet Inception Distance (FID) [49]. These are measures of the quality and diversity of images from a generative model based on the intermediate layers of a pre-trained classifier. For IS, higher scores indicate better quality images, while FID scores indicate better quality with lower scores. FID also requires comparison statistics to a control dataset -in our case, COCO 2017. Both metrics are known to be volatile [11], hence the large number of images produced for testing. Once compiled, all images are resized to 1000 × 1000 to make a fair comparison (IS and FID scores are effected by image size).</p>
<p>Figures. From the 10,000 scenes created, a handful of them were chosen to represent this group in figure 8. In addition to the large set of randomized, unconstrained, generated-background designs, a small set of exemplar images were produced with author-constructed specifications both using real images from across the web (see figure 5, 7) and from generated backgrounds (see figure 6). These could not feasibly be created at a scale where automated metrics like IS and FID would yield meaningful results, but they do allow an additional human-eye demonstration of image quality. Figure 8 consists of selected scenes from the 10,000 automatically generated designs, while Figures 5 and 6 display some of the designs crafted from author-constructed specifications, with the former utilizing real-world images and the latter featuring VEG-generated backgrounds. Quantitative results are presented in Table 3.</p>
<p>Results</p>
<p>From these images, we can conclude that SPRING produces lifelike images. We especially would like to draw our readers' attention to the images in Figure 2. We see that the microwave and the toaster were added in reasonable locations that one would expect. Also, their dimensions and perspective are quite natural. We conclude that this is because the spatial reasoning module was able to discern the usual locations of these appliances, and it works in harmony with the visual element generation module, which was able to understand the context (such as the tabletop and the closet in the specific direction) and put the appliances in the most natural orientations. Neural generative models such as Stable Diffusion also generate realistic scenes, but their results often violate user specifications (see later sections). The image quality from SG2IM was the worst due to mode collapsing. Note that the images generated by SPRING do have occasional imperfections -this suggests our future line of work. Most objects are placed realistically, but some are oriented oddly. We hypothesize that this issue is primarily due to the difficulty of learning locational preferences strongly from a limited dataset. The spatial reasoning module faces a complex task, working directly with an image feature vector to find suitable locations for every object, a challenge amplified in cramped scenes with strict constraints. Another contributing factor is the visual element generator, which has its own inductive biases. It often requires careful coaxing to generate furniture facing certain orientations, a likely consequence of its large-scale training on Internet data, where few images depict furniture from the back or facing away from the camera. Overall, though, the objects generated by SPRING are generally well-placed, integrating seamlessly into the given backgrounds. This implies that the SRM, despite the inherent difficulty of its task, performs well in its role while keeping constraints satisfied.</p>
<p>From the quantitative results, we can see that our assessment of image quality is validated at scale. Analyzing 10,000 generated scenes, SPRING's performance is comparable to Stable Diffusion and higher than SG2IM in both FID and IS metrics. This indicates that the integration of our constraint satisfaction capability does not undermine the overall aesthetic quality of the images, as corroborated by these state-of-the-art metrics.</p>
<p>Evaluating Design Adherence</p>
<p>This experiment evaluates how well SPRING adheres to the user specification provided. It tests for spatial constraints being upheld and the correct objects being placed into the image.</p>
<p>Setup</p>
<p>Generating Specifications. 20 interior designs were produced for testing. Specifications were generated using the same process as the "Evaluating Image Quality" experiment (section 5.1), except with constraints also being randomly generated. For ease of generation, only the constraints cleft, cright, cabove, and cbelow could be sampled -indicating a non-overlapping directional constraint in the specified direction. Each specification included 3 objects and 4 constraints, and was checked to ensure it was satisfiable. These constraints were also logged as scene graphs for use by SG2IM. The conversion from our language to scene graphs was done without ambiguity or information loss.</p>
<p>Baselines. The same baselines were tested as were introduced in the "Evaluating Image Quality" experiment (section 5.1) -including Stable Diffusion and SG2IM. Natural language constraints were used to inpaint images using Stable Diffusion, these were formed from programmatically constructed prompts derived from the same logic as SPRING. For SG2IM, which utilizes scene graphs as input, we converted these from SPRING's propositional logic without ambiguity.</p>
<p>Background images. Background images were generated using the same process as the 10,000-size set in the "Evaluating Image Quality" experiment (section 5.1).</p>
<p>Training. Training was done using the same process as the "Evaluating Image Quality" experiment (section 5.1).</p>
<p>Metrics. Object accuracy and position accuracy were measured as follows: Object accuracy. the author counted and classified the objects in each image. If all 3 objects were visible and identifiable, the image was given a 100% object accuracy. Two identifiable objects resulted in a 67% accuracy, and so on. This score was then averaged across all 20 images to measure the object accuracy of the method. Position accuracy. for each constraint clause (e.g. the oven is left of the toaster), the viewer checked whether the constraint was clearly true within the image. Clauses that referenced images which were not inpainted correctly (penalized in object accuracy) were not included in this count, only relationships between visible and recognizable objects. Position accuracy was also averaged across all 20 images to produce our final metrics.</p>
<p>Results</p>
<p>All metrics are presented in Table 3. Stable Diffusion had lower object accuracy than SPRING, and also showed much worse position accuracy compared to SPRING (50% for Stable Diffusion alone compared to 100% with SPRING). Identifying objects was not possible with SG2IM, hence its accuracy is 0. Notice that SPRING does not guarantee perfect object accuracy, but improves it by explicitly painting one object at a time in isolation from others. SPRING does guarantee perfect position accuracy via the forward checking procedure in the SRM. Several failing images of Stable Diffusion suggest the struggle of current neural generative models. For example, in the top row of Figure 7, Stable Diffusion was able to understand that some blue objects need to be added. However, it cannot carry out deep semantic reasoning to determine that the blue color is associated with the microwave. This same failing is visible in the right panel of Figure 1, where the descriptive texts "blue" and "green" are applied widely and arbitrarily, instead of being applied to their respective objects.</p>
<p>Evaluating Spatial Reasoning</p>
<p>Setup</p>
<p>This experiment assesses how well the generated object locations satisfy explicit user specifications and implicit preferences, which are hard to measure in real-world datasets. This is done by creating synthetic datasets which have both explicit user specifications and implicit preferences. To measure implicit preferences, the objects' positions are biased in ways unknown to the algorithms tested, but known to the authors. The goal is then to examine which algorithm can best capture this implicit bias (i.e., putting objects in locations following the inductive biases present in the training dataset). This section focuses on the spatial reasoning module in isolation, testing its ability to learn "good" positions while guaranteeing that all spatial constraints are satisfied.</p>
<p>Dataset &amp; Metrics. We define three synthetic scenarios with known object location distributions, known to the synthetic data generator but not the algorithms. No VEG or perception module is used. We evaluate two key metrics: constraint accuracy and preference accuracy. Constraint accuracy measures the percentage of generated locations that adhere to the explicitly provided constraints essential for the given scenario. On the other hand, preference accuracy quantifies the extent to which generated locations align with the learned implicit preferences. See B for scenario specifications, including the list of explicit constraints required and the implicit preferences to be checked.</p>
<p>Baseline. For qualitative comparisons on the spatial reasoning module, we compare with baselines that also produce bounding boxes for objects to be generated. Unfortunately, conditional neural generative models, such as Stable Diffusion, cannot be compared fairly because they do not provide coordinates for the generated objects. Hence, we compare SPRING planning with a Generative Adversarial Network (GAN) augmented with a differentiable convex optimization (CVX) layer. More specifically, this approach models the locations of the generated objects as optimal solutions to Quadratic Programs (QPs) subject to constraints. The objective function of the QP is designed to capture implicit location preferences and is learned via backpropagation (the work of [1] details an algorithm to backpropagate gradients over a convex optimization layer). To be specific, the objective function of the QP is updated during training to match the QP's optimal solution (predicted locations) with the actual locations in the dataset. Notice that the "GAN + CVX" approach can only deal with convex constraints and objective functions, which limits its applicability.</p>
<p>Training. Both methods are trained on each scenario until convergence, and each dataset is matched with a set of constraints during testing.</p>
<p>Results</p>
<p>As shown in Table 2, the SRM formulation SPRING uses outperformed or matched GAN + CVX in preference accuracy for every dataset. Both algorithms guarantee positional constraints, but GAN + CVX only does this with convex constraints. This implies that the SRM used by SPRING is very capable of learning implicit preferences from data while still maintaining complex spatial constraints.</p>
<p>Demonstrating Zero-shot Constraint Satisfaction</p>
<p>Our approach allows for re-programming of the spatial reasoning module for zero-shot constraint editing. New constraints can be implemented and used in filtering the GRU outputs of the SRM at any time, even after training. To demonstrate this capability, we construct a new constraint type after training SPRING. We define this constraint as atop(o 1 , o 2 ) -which constrains the first object's bounding box (o 1 ) to be on top of the first object's bounding box (o 2 ). More precisely, atop evaluates to true if and only if:</p>
<p>• the bottom of the first object's bounding box is between the top and the bottom of the second object's bounding box.</p>
<p>• the right side of the first object's bounding box is between the right and left sides of the second object's bounding box.</p>
<p>• the top of the first object's bounding box is above the top of the second object's bounding box. By combining this new constraint with the previous language set, very complex specifications can be produced. In Figure 9 in the main text, atop is used to define a complex specification utilizing the perception module to create a design which can be applied to any background including three tables. A flower and a fern are to be placed in the image. The flower is always one table to the right of the green fern (e.g. if the fern is on the middle table, the flower is  on the right table, and if the fern is on the left table, the flower is on the middle table). As demonstrated in that figure, specifications can be automatically applied to entire sets of backgrounds, and produce diverse and complex images through use of zero-shot constrains.</p>
<p>Human Study</p>
<p>This section presents a human study conducted to evaluate the performance of our SPRING method and Stable Diffusion inpainting for generating interior space images, such as kitchens, living rooms, and billiard rooms. We decided to conduct this study due to the difficulty of evaluating generated content for qualities like aesthetic appeal. Despite the existence of metrics like IS and FID used during the automated experiments, human studies remain the gold standard in generated image evaluation.</p>
<p>Participants were provided with a background image, a completed scene, and a bullet-pointed specification used to generate the image. Additionally, questions with images generated by SPRING displayed the intermediate layout.</p>
<p>Background images were generated using the methods described in 5.1. Constraints were handcrafted by the authors to fit the background. This was done with the aim of evaluating all approaches in good conditions -so that all algorithms can produce the highest quality images possible.</p>
<p>Evaluation Metrics</p>
<p>Participants were asked to rate each image on a 5-point Likert scale across four distinct metrics: specification satisfaction, aesthetic appeal, background preservation, and spatial naturalness.</p>
<p>• Specification satisfaction: Participants assessed how well the image adhered to the specifications, including object visibility, constraint satisfaction, and details such as color, texture, and material. The question read "On a scale of 1 to 5, how well does the image fit the prompt? This includes the objects being visible, the constraints being satisfied, and the details mentioned in the prompts (e.g. objects being the right color). Do not take image quality into account".</p>
<p>• Aesthetic appeal: Participants evaluated the overall quality of the image, regardless of its adherence to the prompt. The question read "On a scale of 1 to 5, rate the aesthetic quality of the image. This includes how realistic the image is, how good each object looks, and how good the scene as a whole looks. Do not take the prompt into account".</p>
<p>• Background preservation: Participants considered how well the original background was maintained without unnecessary alterations or damage. The question read "On a scale of 1 to 5, rate how well the background image was preserved. This includes unnecessary changes being made to the background not specified by the prompt, additional objects being added, details being unnecessarily removed, etc".</p>
<p>• Spatial naturalness: Participants assessed the proportions between objects, the proportions between an object and the background, and the natural placement of objects for their respective types. The question read "On a scale of 1 to 5, how reasonable are the locations and dimensions of the objects in the image".</p>
<p>Questionnaire Structure</p>
<p>The questionnaire included 3-4 sections, with versions 1 and 2 having three sections and version 3 having four sections. Versions were randomly assigned to each participant. Each section contained six examples generated by both SPRING and Stable Diffusion, using the same constraints and backgrounds. To ensure the validity of the responses, two "attention check / gold-standard" questions were incorporated into each version of the questionnaire. These questions have obvious answers for human beings. Responses that violated these attention checks were discarded. A total of 9 human subjects participated in the survey.</p>
<p>The specification satisfaction part is a multifaceted question involving some objectivity (are the positional constraints satisfied, are the right number of images displayed) and some subjectivity (do the objects look like the types they are supposed to be, do they have the right texture or material). For this reason, the specification satisfaction Likert score question is preceded by two additional questions where the subject is asked to check the boxes for the objects that are not visible and the constraints that are not satisfied. This means the subject is forced to recognize the objective parts of specification satisfaction before taking into account the subjective ones.</p>
<p>Initial Tutorial</p>
<p>Before the test could be completed, an initial tutorial was provided to the participants. A real image was presented, and the subjects were asked to fill out the questions in the same way as they would in the main study. After completing the example, the subject was given example answers so that they could check their understanding of the directions before they began the test.</p>
<p>Demographics</p>
<p>The study included participants with diverse backgrounds to ensure a comprehensive evaluation. All subjects were college graduates with degrees in science or technology fields. Most participants had some knowledge and familiarity with generative models or AI-generated content. All subjects are currently living in the United States of America, but have diverse countries of origin. All subjects understand the English language to an advanced degree, but for some, it is a second or third language. No subjects are known to have experience in interior design. Roughly half of the subjects are currently pursuing careers in academia, while the other half are pursuing careers in industry.</p>
<p>Results</p>
<p>The averaged results are presented in Table 4. Our findings indicate that SPRING nearly matches the aesthetic quality and spatial naturalness of Stable Diffusion, as anticipated. Furthermore, SPRING significantly outperforms Stable Diffusion in terms of specification satisfaction and background preservation. These results demonstrate the effectiveness of the SPRING method for generating high-quality interior space images that adhere closely to the provided specifications while maintaining the integrity of the original background. These results support those found in the previous experiments: better specification satisfaction with comparable or superior image quality.</p>
<p>Scenario SPRING ⋆ GAN+CVX Pref. Acc. ↑ Constr. Acc. ↑ Pref. Acc. ↑ Constr. Acc. ↑ "Basic"  Table 2: Results for evaluating "goodness" of location decisions by the SRM when tested on synthetic scenarios. Our SPRING is able to generate object locations better satisfying implicit preferences from data (measured by preference accuracy) compared to baselines, while both approaches 100% satisfy explicit user constraints. Note, SPRING can also handle non-convex constraints, while "GAN+CVX" cannot. More details of this experiment are in B. ⋆ indicates our method.</p>
<p>Method</p>
<p>Position  Table 3: Quantitative results from 10,000 generated scenes show our SPRING leads in position and object accuracy for user specification satisfaction, as well as Inception Score (IS) and Fréchet Inception Distance (FID) perception scores for visual image quality. FID statistics were calculated from the COCO 2017 validation set. Position accuracy represents how often the positional constraints are met. Object accuracy represents how often the correct number and type of objects are generated and identifiable.  Table 4: Results for the human survey with two methods: Stable Diffusion and SPRING (ours). Metrics collected include Likert scores (1-5) for specification satisfaction, aesthetic appeal, background preservation, and the naturalness of spatial qualities -e.g. proportions between objects, proportions between an object and the background. SPRING does much better at satisfying user specifications than Stable Diffusion, and is far less likely to damage the background due to its more controlled inpainting. SPRING receives slightly lower results for aesthetics and naturalness, perhaps representing a small trade-off between the amount of control provided by the algorithm and its ability to produce good-looking images.</p>
<p>Method</p>
<p>Conclusion</p>
<p>We have introduced SPRING for design production. Good designs must meet specific user requirements and comply with implicit guidelines on appearance and functionality. This is only feasible through the union of symbolic reasoning with neural algorithms. We achieve this by integrating data-driven neural generative models with symbolicallydriven constraint programming. The object positions suggested by the neural network are filtered through constraint reasoning-based forward checking to meet user specifications. Our SPRING produces high-quality interior design scenes that respect user-defined spatial requirements, satisfy common sense and look pleasing. Our approach also handles new constraint types in a zero-shot manner due to the forward checker's programmability. It is also more interpretable as decisions are made iteratively. The main limitation of our work is the dependence on the VEG model's quality and compatibility for object generation. Future work will focus on enhancing control between the SRM and VEG and adapting to other domains, such as 3D design.</p>
<p>A General Additional Experiment Details</p>
<p>A.1 Logical Language Translation</p>
<p>The logical instructions for generating the figures in this work are given in the following tables. Note the relation default, which combines a number of default constraints used on all objects to fit the limitations of the VEG module. For the figures generated using Stable Diffusion as the VEG, the objects are set to have a width and height between 256 and 512 pixels. Additionally, the bounding boxes for these objects are constrained such that they do not extend beyond the boundaries of the image. For the figures generated using GLIDE as VEG, the same default constraints are used, with the exception that the objects are bounded between 150 and 256 pixels. This does not include Figure  8, which has its objects described for each image in Table 8 -as that figure does not include positional constraints beyond the default.</p>
<p>Figure</p>
<p>Logic Language Natural Language  Replace the microwave with a retro toaster oven and add a potted potato plant fully below the toaster oven and at least partly above the oven.  type(0,"tv") ∧ type(1,"couch") ∧ type(2,"chair") ∧ property(0,"a flat screen tv") ∧ property(1,"a couch facing left") ∧ property(2,"a chair") ∧
default(0) ∧ default(1) ∧ default(2) ∧ left value(0,150) ∧ cright(1,0) ∧ cright(2,0) ∧ cabove(2,1)
Add a flat screen TV in the left 15% of the image. Also add a red chair and a couch right of the TV, where the chair is above the couch. # 0: a tv. type(1,"chair") ∧ type(2,"couch") ∧ type(3,"chair") ∧ property(1,"a red chair facing away") ∧ property(2,"a couch facing away") ∧ property(3,"a blue chair facing away"
) ∧ default(1) ∧ default(2) ∧ default(3) ∧ cbelow(1,0) ∧ yeq(2,1) ∧ yeq(3,1) ∧ cleft(1,2) ∧ cleft(2,3)
Add a blue chair, a couch, and a red chair, left to right, all under the TV. # 0: a sink, 1: an oven. type(2,"microwave") ∧ type(3,"toaster") ∧ property(2,"a blue microwave") ∧ property(3,"a green toaster") ∧ default(2) ∧ default(3) ∧ cright(2,1) ∧ cleft(3,1) ∧ cbelow(3,0)</p>
<p>Add a blue microwave right of the oven, and a green toaster left of the oven and below the sink.  type(0, "chair") ∧ type(1, "couch") ∧ type(2, "coffee table") ∧ property(0, "a cozy brown leather chair") ∧ property(1, "a black and white stripe couch") ∧ property(2, "a stout coffee table") ∧
default(0) ∧ default(1) ∧ default(2) ∧ cleft(0, 1) ∧ xeq(0, 1) ∧ below(2, 0) ∧ right(2, 0) ∧ wider(1, 0) ∧ wider(2, 0) ∧ taller(1, 2)
Objects:</p>
<p>A cozy brown leather chair, a black and white stripe couch, a stout coffee table in front of the couch.</p>
<p>Constraints:</p>
<p>The chair is completely left of the couch and horizontally aligned with it. The coffee table is below the chair and to its right. The couch is wider than the chair, as is the coffee table.</p>
<p>The couch is taller than the coffee table.   type(0, "microwave") ∧ type(1, "toaster") ∧ property(0, "a brown microwave") ∧ property(1, "a wood-panel pop-up toaster") ∧ default(0) ∧ default(1) ∧ cleft(0, 1) ∧ below(1, 0)</p>
<p>Objects:</p>
<p>A brown microwave, a wood-panel pop-up toaster.</p>
<p>Constraints:</p>
<p>The microwave is completely to the left of the toaster and the toaster is below the microwave. type(0, "microwave") ∧ type(1, "oven") ∧ property(0, "a blue microwave") ∧ property(1, "a white oven") ∧ default(0) ∧ default(1) ∧ cleft(1, 0) ∧ cbelow(1, 0) ∧ right value(0, 500)</p>
<p>Objects:</p>
<p>A blue microwave, a white oven.</p>
<p>Constraints:</p>
<p>The microwave is in the right 50% of the image. The oven is completely left and below the microwave.  type(0,"microwave") ∧ type(1,"oven") ∧ property(0,"a blue microwave") ∧ property(1,"a black oven") ∧ default(0) ∧ default(1) ∧ cabove(0, 1)</p>
<p>A blue microwave above a black oven. Fig. 7 type(0,"microwave") ∧ type(1,"oven") ∧ type(2,"refrigerator") ∧ default(0) ∧ default(1) ∧ default(2) ∧ cleft(2,1) ∧ cright(0,1) ∧ cabove(0,1)</p>
<p>A refrigerator left of an oven and a microwave right and above the same oven. type(0, "microwave") ∧ type(1, "oven") ∧ type(2, "toaster") ∧ type(3, "sink") ∧ default(0) ∧ default(1) ∧ default(2) ∧ default(3) ∧ right(0, 1) ∧ above(0, 1) ∧ left(3, 1) ∧ above(3, 1) ∧ below(2, 0)</p>
<p>A microwave, an oven, a toaster, and a sink. The sink is left of and at least partly above the oven, the microwave is right of and above the oven, and the toaster is below the microwave.  , table  couch, chair, table  chair, table  chair, chair, table  oven, refrigerator, microwave  microwave, oven, refrigerator  oven, plant,  couch, couch, table  couch, chair   Table 8: Objects generated for Figure 8, listed from top left to bottom right. These images do not contain positional constraints, and show that the SPRING's spatial reasoning module can provide good locations for objects even without human planning and closely tailored specifications. Figure 10: A closer look at some of the images generated for Figures 1 and 2. Note that the background is minimally disturbed. Also note that, while the microwave can be anywhere right of the oven, it is placed in a reasonable placeon the counter or built into the cabinets.</p>
<p>B Evaluating Spatial Reasoning Synthetic Scenarios</p>
<p>In this test, we compare the performance of the spatial reasoning module alone to a generative adversarial network equipped with convex optimization as an embedded layer. We will evaluate each model's ability to meet both explicit constraints from the user and implicit preferences derived from the training data. The test scenarios will involve generating synthetic data for each object type, with the data distribution designed to express a strong preference for specific values. A range of common values for each preference will be established as acceptable, with values outside this range considered as not meeting the preference. The use of synthetic data is necessary as it allows us to know the acceptable range for each preference, which is not possible with real data. Both the spatial reasoning module and the GAN + CVX baseline will be trained on this preference-laden data, and then evaluated on their ability to satisfy constraints and preferences. The preference accuracies will be reported as the percentage of clauses that are satisfied.</p>
<p>The test includes three scenarios, each with specific data generation functions and acceptable preference ranges. The first scenario, "basic", involves objects with a higher likelihood of being generated on opposite halves of the image. The second scenario, "tight", has objects with specific width and height ratios with stricter acceptable ranges. The third scenario, "complex", has multiple preferences, such as object 1 being at least 1.5 times taller than its width, and object 2 having a specific y-coordinate value dependent on that of object 1. These scenarios demonstrate that the SRM's preference learning is effective and outperforms other methods such as GAN + CVX.</p>
<p>Two random functions were used to compose the synthetic datasets. rnd(j, k) randomly generates an integer between j and k from a normal distribution with mean j + k−j 2 and standard deviation k−j 12 . Selected values are rounded, and constrained between j and k. For example, if object 1's x value is drawn from rnd(1, 500), then the mean of x will be 250.5 and the standard deviation will be 41.583. uni(j, k) does the same with a uniform distribution between j and k.</p>
<p>The following sections include preference ranges (under "checked preferences") for each scenario. The preference accuracy is measured as a percentage of correctly satisfied clauses across 256 generated examples. The object associated with each variable is indicated in the subscript (e.g. 1 ≤ x o1 ≤ 500 asserts that the x value of object 1 must be between 0 and 500 to satisfy the preference). Constraints are also given (under "constraints") in our constraint language. The constraint accuracy is measured as a percentage of correctly satisfied clauses across the same 256 generated examples. Constraints:</p>
<p>B.1 Basic Scenario</p>
<p>• above(o1, o2, 300) -Object 1 is above object 2 by at least 300 per mille.</p>
<p>Checked preferences: Constraints:</p>
<p>• left(o1, o2, 180) -Object 1 is left of object 2 by at least 180 per mille.</p>
<p>• left(o2, o3, 180) -Object 2 is left of object 3 by at least 180 per mille.</p>
<p>Checked preferences:</p>
<p>• 220 ≤ wo1 ≤ 256</p>
<p>• 120 ≤ wo2 ≤ 150</p>
<p>• 120 ≤ wo3 ≤ 150</p>
<p>• 120 ≤ ho1 ≤ 150</p>
<p>• 120 ≤ ho2 ≤ 150</p>
<p>• 220 ≤ ho3 ≤ 256</p>
<p>B.3 Complex Scenario</p>
<p>Object x y width (w) height (h)</p>
<p>Figure 2 :
2Rollout of the SPRING system. (First row) SPRING consists of the perception module, the spatial reasoning module, and the visual element generation module. (Second &amp; third rows)</p>
<p>left(o 1 , o 2 , c) left side of o 1 is at least c units left of left side of o 2 . cleft(o 1 , o 2 , c) right side of o 1 is at least c units left of left side of o 2 . left value(o 1 , c) left side of o 1 has an x-value less than c. right(o 1 , o 2 , c) left side of o 1 is at least c units right of left side of o 2 . cright(o 1 , o 2 ) left side of o 1 is at least c units right of right side of o 2 . right value(o 1 , c) left side of o 1 has an x-value greater than c. narrower(o 1 , o 2 , c) o 1 is at least c units narrower than object o 2 . narrower value(o 1 , c) o 1 has a width less than c. shorter(o 1 , o 2 , c) o 1 is at least c units shorter than object o 2 . shorter value(o 1 , c) o 1 has a height less than c. taller(o 1 , o 2 , c) o 1 is at least c units taller than object o 2 . taller value(o 1 , c) o 1 has a height greater than c. wider(o 1 , o 2 , c) o 1 is at least c units wider than object o 2 . wider value(o 1 , c) o 1 has a width greater than c. heq(o 1 , o 2 ) o 1 and object o 2 have the same height. heq value(o 1 , c) o 1 has a height equal to c. weq(o 1 , o 2 ) o 1 and object o 2 have the same width. weq value(o 1 , c) o 1 has a width equal to c. xeq(o 1 , o 2 ) left side of o 1 is in line with left side of o 2 . xeq value(o 1 , c) left side of o 1 has an x-value equal to c. yeq(o 1 , o 2 ) top side of o 1 is in line with top side of o 2 . yeq value(o 1 , c)</p>
<p>Figure 3 :
3Example of neuro-symbolic reasoning in the form of forward checking in the SRM. In each step, a decision is sampled based on the probability distribution predicted by the GRU. A forward-checking procedure checks if the sampled decision leads to constraint violations. If it does, new decisions are sampled from the remaining actions until options remain which violate no constraints. This process repeats, generating each decision until all spatial variables are fully decided.</p>
<p>Figure 5 :
5Example visualizations from SPRING. Note the diverse locations possible given each background and specification. Backgrounds are from various web-sources. Constraints are described in text for clarity. They were provided to SPRING in propositional logic (See appendix table 5 for logic definitions).</p>
<p>Figure 6 :
6More visualizations from SPRING with comparisons to Stable Diffusion. Backgrounds are generated as detailed in section 5.1. Constraints are described in text for clarity. They were provided to SPRING in propositional logic and Stable Diffusion as text (See appendix table 6 for logic definitions).</p>
<p>Figure 8 :
8Images produced by SPRING with generated backgrounds and no positional constraints -only objects are specified. SPRING's SRM still finds sensible locations for objects, even in the absence of complex constraints. Backgrounds are generated as detailed in section 5.1. The list of objects generated for each image here can be found in A.1.</p>
<p>Figure 9 :
9SPRING's zero-shot transfer learning performance on a new constraint atop never present in the training set. atop constrains an object to be sitting on top of another. This specification applies to all backgrounds which already have three tables: 0, 1, and 2 from left to right. The flower is always one table to the right of the green fern, showing good performance of SPRING.</p>
<p>Fig. 5 type
5(0,"oven") ∧ property(0,"a modern looking oven") ∧ default(0) ∧ right value(0, 400) Add a modern looking oven in the middle or right side.</p>
<p>Fig. 5 #
50: a microwave, 1: an oven. type(2,"toaster") ∧ type(3,"plant") ∧ property(2,"a retro toaster oven") ∧ property(3,"a potted potato plant") ∧ default(2) ∧ default(3) ∧ xeq(2,0) ∧ yeq(2,0) ∧ wider(2,0) ∧ taller(2,0) ∧ cbelow(3,0) above(3,1)    </p>
<p>Fig. 5 #
50: a chair. type(1,"chair") ∧ type(2,"table") ∧ property(1,"a dark colored leather chair") ∧ property(2,"a wooden table") ∧ default(1) ∧ default(2) ∧ cleft(1,0,300) ∧ below(2,1) ∧ below(2,0) ∧ left(2,0) ∧ right(2,1,100) Add a dark colored leather chair fully left of the white chair, and a wooden table between and below them.</p>
<p>Fig. 5
5Fig. 5</p>
<p>Fig. 5
5Fig. 5</p>
<p>Fig. 5
5Fig. 5</p>
<p>Figure
Figure Logic Language Natural Language</p>
<p>Fig
Fig. 6</p>
<p>Fig. 6
6Fig. 6</p>
<p>Fig. 6
6Fig. 6</p>
<p>Figure Logic Language Natural Language</p>
<p>Fig. 7
7Fig. 7</p>
<p>RelationTruth condition above(o1  , o 2 , c) top side of o 1 is at least c units above top side of o 2 . cabove(o 1 , o 2 ) bottom side of o 1 is at least c units above top side of o 2 . above value(o 1 , c) top side of o 1 has a y-value less than c. below(o 1 , o 2 , c) top side of o 1 is at least c units below top side of o 2 . cbelow(o 1 , o 2 ) top side of o 1 is at least c units below bottom side of o 2 . below value(o 1 , c) top side of o 1 has a y-value greater than c.</p>
<p>1 .
1Spatial relationships with constant offsets: This group of predicates defines spatial relationships betweentwo objects with a constant offset. Predicates like above, below, right, and left describe relative positions 
between objects in the x and y dimensions. For example, above(o 1 , o 2 , k) is true if and only if object o 1 is 
above object o 2 by at least k vertical units. More precisely, the top side of o 1 is at least k vertical units above 
the top side of o 2 . In this paper, we define one vertical unit as one-thousandth (per-mille) of the image's 
height, and similarly, one horizontal unit as one-thousandth of the image's width. Sometimes the distance k is 
omitted. In this case, above(o 1 , o 2 ) is true if and only if o 1 is above o 2 . Additionally, the grammar provides 
cabove, cbelow, cright, and cleft predicates, which represent complete spatial constraints where the 
entire bounding box of one object is constrained in one direction away from its counterpart, with no overlap. 
For example, cabove(o 1 , o 2 , k) holds if and only if object o 1 is completely above object o 2 with a constant 
vertical distance of at least k. In other words, the bottom side of o 1 is at least c vertical units above the top 
side of o 2 . </p>
<ol>
<li>
<p>Size comparisons between objects: shorter, taller, narrower, and wider predicates define the size 
constraints between objects. For example, shorter(o 1 , o 2 , c) evaluates to true if and only if object o 1 is at 
least c vertical units shorter than object o 2 . Again, the argument c can be omitted. </p>
</li>
<li>
<p>Equality constraints: These predicates establish equal attribute relationships between objects. For example, 
xeq(o 1 , o 2 ) holds if and only if objects o 1 and o 2 share the same x-position (meaning they are vertically 
aligned). Similarly, yeq, weq, and heq predicates ensure that two objects share the same y-position, width, 
or height, respectively. </p>
</li>
<li>
<p>Constraints with constant values: This group of predicates sets specific constraints on an object's attribute 
with a constant value instead of in reference to another object. For example, above value(o, k) holds if 
and only if the y-position of the top-left corner of object o is above (less than) k. Other predicates like 
below value, right value, left value, shorter value, taller value, narrower value, 
wider value, xeq value, yeq value, weq value, and heq value constrain an object based on given 
values. </p>
</li>
</ol>
<p>x 1 = 12.<right> 
<left> </p>
<p><stop> </p>
<p>X 2 </p>
<p>X 1 </p>
<p>... </p>
<p>[20, 50] </p>
<p>[0, 100] </p>
<p>... </p>
<p><right> 
<left> </p>
<p><stop> </p>
<p>X 2 </p>
<p>X 1 </p>
<p>... </p>
<p>[20, 50] </p>
<p>[0, 50] </p>
<p>... </p>
<p><right> 
<left> </p>
<p><stop> </p>
<p>X 2 </p>
<p>X 1 </p>
<p>... </p>
<p>[20, 50] </p>
<p>[0, 25] </p>
<p>... </p>
<p>C1: X1 &lt; X2. 
C2: X2 &lt; 51. 
C3: X2 &gt; 19. </p>
<p>Constraints </p>
<p>-Attempt left. 
-Check constraints for [0, 50]. 
-0 &lt; 20 
C 1 satisfiable. 
-Go left. </p>
<p>decision scoring 
range table </p>
<p>Accuracy ↑ Object Accuracy ↑ IS ↑ FID ↓Stable Diffusion 
0.5 
0.63 
3.58 162.73 
SG2IM 
0.0 
0.0 
2.52 
NaN 
SPRING[SD] ⋆ 
1.0 
0.77 
3.59 160.36 </p>
<p>Table 5 :
5The logic formulations used for figures 5 with accompanying descriptive text. '#' comments represent objects that were detected by the perception module and incorporated. Note the relation default: this sets width and height between 256 and 512 pixels to accommodate Stable Diffusion.</p>
<p>Objects:A big kitchen table surrounded by chairs, made of wood, a big flatscreen television mounted on a wall.Constraints:The table is completely below the TV and the TV is completely to the left of the table. The table is wider than 40% of the image and taller than 30% of the image. The TV is wider than 30% of the image.table") ∧ 
type(1, "television") ∧ 
property(0, 
"a big wooden kitchen table surrounded by chairs") ∧ 
property(1, 
"a big flatscreen television mounted on a wall") ∧ 
default(0) ∧ default(1) ∧ 
cbelow(0, 1) ∧ 
cleft(1, 0) ∧ 
wider value(0, 400) ∧ 
taller value(0, 300) ∧ 
wider value(1, 300) </p>
<p>Table 6 :
6The logic formulations used for figures 6 with accompanying descriptive text. Note the relation default: this sets width and height between 256 and 512 pixels to accommodate Stable Diffusion.</p>
<p>Table 7 :
7The logic formulations used for figures 7 with accompanying descriptive text. Note the relation default: this sets width and height between 256 and 512 pixels to accommodate Stable Diffusion.microwave, potted plant 
microwave, plant 
microwave, oven 
toaster, plant 
toaster, plant 
couch, table 
chair, table 
plant, chair 
couch, table 
couch, tv 
chair, plant, table 
chair, table 
couch, chair, plant 
couch, chair, table, 
chair, table 
microwave, refrigerator 
microwave, refrigerator, oven 
refrigerator, microwave 
oven, toaster, toaster 
microwave, refrigerator, toaster 
toaster, oven, refrigerator 
plant, tv, table 
plant, couch 
chair, table 
couch, tv, plant 
refrigerator, refrigerator, oven 
sink, toaster, microwave 
sink, toaster, oven 
microwave, microwave, oven 
microwave, toaster 
microwave, refrigerator, refrigerator 
plant, couch</p>
<p>• 1 ≤ xo1 ≤ 500 • 500 ≤ xo2 ≤ 1000 B.2 Tight ScenarioObject 
x 
y 
width (w) 
height (h) </p>
<p>1 
rnd(1,1000) 
rnd(300,700) 
rnd(220,256) 
rnd(120,150) 
2 
rnd(1,1000) 
rnd(300,700) 
rnd(120,150) 
rnd(120,150) 
3 
rnd(1,1000) 
rnd(300,700) 
rnd(120,150) 
rnd(220,256) </p>
<p>AcknowledgementsThis research was supported by NSF grants IIS-1850243, CCF-1918327.
Differentiable convex optimization layers. Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, J Zico Kolter, Advances in Neural Information Processing Systems. Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J. Zico Kolter. Differen- tiable convex optimization layers. In Advances in Neural Information Processing Systems, pages 9558-9570, 2019.</p>
<p>Learning convex optimization models. Akshay Agrawal, Shane Barratt, Stephen Boyd, IEEE/CAA Journal of Automatica Sinica. 88Akshay Agrawal, Shane Barratt, and Stephen Boyd. Learning convex optimization models. IEEE/CAA Journal of Automatica Sinica, 8(8):1355-1364, 2021.</p>
<p>Learning convex optimization control policies. Akshay Agrawal, Shane Barratt, Stephen Boyd, Bartolomeo Stellato, PMLRProceedings of the 2nd Conference on Learning for Dynamics and Control. Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A. Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilingerthe 2nd Conference on Learning for Dynamics and Control120Akshay Agrawal, Shane Barratt, Stephen Boyd, and Bartolomeo Stellato. Learning convex optimization control policies. In Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A. Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilinger, editors, Proceedings of the 2nd Conference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning Research, pages 361-373. PMLR, 10-11 Jun 2020.</p>
<p>Dax: Deep argumentative explanation for neural networks. E Albini, Lertvittayakumjorn, F Rago, Toni, abs/2012.05766CoRRE Albini, P Lertvittayakumjorn, A Rago, and F Toni. Dax: Deep argumentative explanation for neural networks. CoRR, abs/2012.05766, 2020.</p>
<p>Thinking fast and slow with deep learning and tree search. Thomas Anthony, Zheng Tian, David Barber, Advances in Neural Information Processing Systems. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.</p>
<p>Conversational multihop reasoning with neural commonsense knowledge and symbolic logic rules. Forough Arabshahi, Jenifer Lee, Antoine Bosselut, Yejin Choi, Tom M Mitchell, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021EMNLP 2021Forough Arabshahi, Jenifer Lee, Antoine Bosselut, Yejin Choi, and Tom M. Mitchell. Conversational multi- hop reasoning with neural commonsense knowledge and symbolic logic rules. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021), 2021.</p>
<p>Sparx: Sparse argumentative explanations for neural networks. H Ayoobi, F Potyka, Toni, abs/2301.09559CoRRH Ayoobi, N Potyka, and F Toni. Sparx: Sparse argumentative explanations for neural networks. CoRR, abs/2301.09559, 2023.</p>
<p>CLR-DRNets: Curriculum Learning with Restarts to Solve Visual Combinatorial Games. Yiwei Bai, Di Chen, Carla P Gomes, 27th International Conference on Principles and Practice of Constraint Programming. Laurent D. Michel21014, Dagstuhl, Germany, 2021. Schloss Dagstuhl -Leibniz-Zentrum für InformatikYiwei Bai, Di Chen, and Carla P. Gomes. CLR-DRNets: Curriculum Learning with Restarts to Solve Visual Combinatorial Games. In Laurent D. Michel, editor, 27th International Conference on Principles and Practice of Constraint Programming (CP 2021), volume 210 of Leibniz International Proceedings in Informatics (LIPIcs), pages 17:1-17:14, Dagstuhl, Germany, 2021. Schloss Dagstuhl -Leibniz-Zentrum für Informatik.</p>
<p>Entropy-based logic explanations of neural networks. P Barbiero, Ciravegna, P Giannini, M Lió, S Gori, Melacci, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence36P Barbiero, G Ciravegna, F Giannini, P Lió, M Gori, and S Melacci. Entropy-based logic explanations of neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6046- 6054, 2022.</p>
<p>Handbook of Formal Argumentation. P Baroni, M Gabbay, L Giacomin, Van Der Torre, College PublicationsP Baroni, D Gabbay, M Giacomin, and L van der Torre, editors. Handbook of Formal Argumentation. College Publications, 2018.</p>
<p>A note on the inception score. Shane Barratt, Rishi Sharma, arXiv:1801.01973arXiv preprintShane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.</p>
<p>Neuron constraints to model complex real-world problems. Andrea Bartolini, Michele Lombardi, Michela Milano, Luca Benini, Principles and Practice of Constraint Programming-CP 2011: 17th International Conference. Perugia, ItalySpringer17CPAndrea Bartolini, Michele Lombardi, Michela Milano, and Luca Benini. Neuron constraints to model com- plex real-world problems. In Principles and Practice of Constraint Programming-CP 2011: 17th International Conference, CP 2011, Perugia, Italy, September 12-16, 2011. Proceedings 17, pages 115-129. Springer, 2011.</p>
<p>R Tarek, Artur S Besold, Sebastian Garcez, Howard Bader, Pedro Bowman, Pascal Domingos, Kai-Uwe Hitzler, Luís C Kühnberger, Daniel Lamb, Priscila Machado Vieira Lowd, Leo Lima, De Penning, Gadi Pinkas, Hoifung Poon, and Gerson Zaverucha. Neural-Symbolic Learning and Reasoning: A Survey and Interpretation. AmsterdamIOS Press2022Tarek R. Besold, Artur S. d'Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-Uwe Kühnberger, Luís C. Lamb, Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, and Gerson Zaverucha. Neural-Symbolic Learning and Reasoning: A Survey and Interpretation. IOS Press, Amsterdam, 2022.</p>
<p>Constraint acquisition. Christian Bessiere, Frédéric Koriche, Nadjib Lazaar, Barry O&apos; Sullivan, Combining Constraint Solving with Mining and Learning. 244Artificial IntelligenceChristian Bessiere, Frédéric Koriche, Nadjib Lazaar, and Barry O'Sullivan. Constraint acquisition. Artificial Intelligence, 244:315-342, 2017. Combining Constraint Solving with Mining and Learning.</p>
<p>The Inductive Constraint Programming Loop. Christian Bessière, Tias Luc De Raedt, Lars Guns, Mirco Kotthoff, Siegfried Nanni, Nijssen, O&apos; Barry, Anastasia Sullivan, Dino Paparrizou, Helmut Pedreschi, Simonis, IEEE Intelligent Systems. Christian Bessière, Luc de Raedt, Tias Guns, Lars Kotthoff, Mirco Nanni, Siegfried Nijssen, Barry O'Sullivan, Anastasia Paparrizou, Dino Pedreschi, and Helmut Simonis. The Inductive Constraint Programming Loop. IEEE Intelligent Systems, 2018.</p>
<p>Thinking fast and slow in ai. Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jon Lenchner, Nick Linck, Andrea Loreggia, Keerthiram Murugesan, Nicholas Mattei, Francesca Rossi, Biplav Srivastava, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jon Lenchner, Nick Linck, Andrea Loreggia, Keerthiram Murugesan, Nicholas Mattei, Francesca Rossi, and Biplav Srivastava. Thinking fast and slow in ai. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 15042-15046, 2021.</p>
<p>Improving deep learning models via constraint-based domain knowledge: a brief survey. Andrea Borghesi, Federico Baldo, Michela Milano, arXiv:2005.10691arXiv preprintAndrea Borghesi, Federico Baldo, and Michela Milano. Improving deep learning models via constraint-based domain knowledge: a brief survey. arXiv preprint arXiv:2005.10691, 2020.</p>
<p>Convex optimization. Stephen Boyd, Lieven Vandenberghe, Cambridge university pressStephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.</p>
<p>End-to-end object detection with transformers. CoRR, abs. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR, abs/2005.12872, 2020.</p>
<p>Building-gan: Graph-conditioned architectural volumetric design generation. Kai-Hung Chang, Chin-Yi Cheng, Jieliang Luo, Shingo Murata, Mehdi Nourbakhsh, Yoshito Tsuji, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Kai-Hung Chang, Chin-Yi Cheng, Jieliang Luo, Shingo Murata, Mehdi Nourbakhsh, and Yoshito Tsuji. Building-gan: Graph-conditioned architectural volumetric design generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11956-11965, October 2021.</p>
<p>Automating crystal-structure phase mapping by combining deep learning with constraint reasoning. Di Chen, Yiwei Bai, Sebastian Ament, Wenting Zhao, Dan Guevarra, Lan Zhou, Bart Selman, John M Bruce Van Dover, Carla P Gregoire, Gomes, Nature Machine Intelligence. 39Di Chen, Yiwei Bai, Sebastian Ament, Wenting Zhao, Dan Guevarra, Lan Zhou, Bart Selman, R Bruce van Dover, John M Gregoire, and Carla P Gomes. Automating crystal-structure phase mapping by combining deep learning with constraint reasoning. Nature Machine Intelligence, 3(9):812-822, 2021.</p>
<p>Application of ai technology in interior design. Zixuan Chen, Xiang Wang, E3S Web of Conferences. 179page 02105. E3S Web of ConferencesZixuan Chen and Xiang Wang. Application of ai technology in interior design. In E3S Web of Conferences, volume 179, page 02105. E3S Web of Conferences, 2020.</p>
<p>On the properties of neural machine translation: Encoder-decoder approaches. Kyunghyun Cho, Dzmitry Bart Van Merrienboer, Yoshua Bahdanau, Bengio, SSST@EMNLP. Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. In SSST@EMNLP, 2014.</p>
<p>Logic explained networks. G Ciravegna, Barbiero, M Giannini, P Gori, M Lió, S Maggini, Melacci, Artificial Intelligence. 314103822G Ciravegna, P Barbiero, F Giannini, M Gori, P Lió, M Maggini, and S Melacci. Logic explained networks. Artificial Intelligence, 314:103822, 2023.</p>
<p>A constraint-based approach to learning and explanation. G Ciravegna, Giannini, M Melacci, M Maggini, Gori, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34G Ciravegna, F Giannini, S Melacci, M Maggini, and M Gori. A constraint-based approach to learning and explanation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3658-3665, 2020.</p>
<p>Extracting dialogical explanations for review aggregations with argumentative dialogical agents. O Cocarascu, F Rago, Toni, Proc. AAMAS 2019. AAMAS 2019IFAAMSO Cocarascu, A Rago, and F Toni. Extracting dialogical explanations for review aggregations with argumenta- tive dialogical agents. In Proc. AAMAS 2019, pages 1261-1269. IFAAMS, 2019.</p>
<p>Data-empowered argumentation for dialectically explainable predictions. O Cocarascu, Stylianou, F Cyras, Toni, Proc. ECAI 2020. ECAI 2020IOS Press325O Cocarascu, A Stylianou, K Cyras, and F Toni. Data-empowered argumentation for dialectically explainable predictions. In Proc. ECAI 2020, FAIA 325, pages 2449-2456. IOS Press, 2020.</p>
<p>Argumentation for machine learning: A survey. O Cocarascu, Toni, Proc. COMMA 2016, FAIA 287. COMMA 2016, FAIA 287IOS PressO Cocarascu and F Toni. Argumentation for machine learning: A survey. In Proc. COMMA 2016, FAIA 287, pages 219-230. IOS Press, 2016.</p>
<p>Making existing clusterings fairer: Algorithms, complexity results and insights. I Davidson, Ravi, The Thirty-Fourth AAAI Conference on Artificial Intelligence. New York, NY, USAAAAI Press2020I Davidson and SS Ravi. Making existing clusterings fairer: Algorithms, complexity results and insights. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 3733-3740, New York, NY, USA, 2020. AAAI Press.</p>
<p>Towards auditing unsupervised learning algorithms and human processes for fairness. I Davidson, Ravi, arXiv:2209.11762arXiv preprintI Davidson and SS Ravi. Towards auditing unsupervised learning algorithms and human processes for fairness. arXiv preprint arXiv:2209.11762, 2022.</p>
<p>Argflow: A toolkit for deep argumentative explanations for neural networks. A Dejl, He, H Mangal, Mohsin, Surdu, P Albini, Lertvittayakumjorn, F Rago, Toni, Proc. AAMAS 2021. AAMAS 2021ACMA Dejl, P He, P Mangal, H Mohsin, B Surdu, E Albini, P Lertvittayakumjorn, A Rago, and F Toni. Argflow: A toolkit for deep argumentative explanations for neural networks. In Proc. AAMAS 2021, pages 1761-1763. ACM, 2021.</p>
<p>Generative scene graph networks. Fei Deng, Zhuo Zhi, Donghun Lee, Sungjin Ahn, International Conference on Learning Representations. Fei Deng, Zhuo Zhi, Donghun Lee, and Sungjin Ahn. Generative scene graph networks. In International Conference on Learning Representations, 2021.</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.</p>
<p>Teaching the old dog new tricks: Supervised learning with constraints. Fabrizio Detassis, Michele Lombardi, Michela Milano, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Fabrizio Detassis, Michele Lombardi, and Michela Milano. Teaching the old dog new tricks: Supervised learning with constraints. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 3742-3749, 2021.</p>
<p>Semantic image manipulation using scene graphs. Helisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, D Gregory, Federico Hager, Christian Tombari, Rupprecht, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionHelisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, Gregory D Hager, Federico Tombari, and Christian Rupprecht. Semantic image manipulation using scene graphs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5213-5222, 2020.</p>
<p>Relational reinforcement learning. Sašo Džeroski, Hendrik Luc De Raedt, Blockeel, Inductive Logic Programming: 8th International Conference, ILP-98. Madison, Wisconsin, USASpringer8Sašo Džeroski, Luc De Raedt, and Hendrik Blockeel. Relational reinforcement learning. In Inductive Logic Programming: 8th International Conference, ILP-98 Madison, Wisconsin, USA, July 22-24, 1998 Proceedings 8, pages 11-22. Springer, 1998.</p>
<p>Genesis: Generative scene inference and sampling with object-centric latent representations. Martin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, Ingmar Posner, International Conference on Learning Representations. Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. In International Conference on Learning Representations, 2020.</p>
<p>In two minds: dual-process accounts of reasoning. Jonathan St, B T Evans, Trends in cognitive sciences. 710Jonathan St BT Evans. In two minds: dual-process accounts of reasoning. Trends in cognitive sciences, 7(10):454-459, 2003.</p>
<p>Learning explanatory rules from noisy data. Richard Evans, Edward Grefenstette, 11172Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. JAIR, 1:11172, 2018.</p>
<p>Fast and slow planning. Francesco Fabiano, Vishal Pallagani, Marianna Bergamaschi Ganapini, Lior Horesh, Andrea Loreggia, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Francesco Fabiano, Vishal Pallagani, Marianna Bergamaschi Ganapini, Lior Horesh, Andrea Loreggia, Keerthi- ram Murugesan, Francesca Rossi, and Biplav Srivastava. Fast and slow planning, 2023.</p>
<p>Improving fairness generalization through a sample-robust optimization method. Julien Ferry, Ulrich Aivodji, Sebastien Gambs, Marie-Jose Huguet, Mohamed Siala, Machine Learning. Julien Ferry, Ulrich Aivodji, Sebastien Gambs, Marie-Jose Huguet, and Mohamed Siala. Improving fairness generalization through a sample-robust optimization method. Machine Learning, Jul 2022.</p>
<p>Improving fairness generalization through a sample-robust optimization method. Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, Mohamed Siala, Machine Learning. 112Julien Ferry, Ulrich Aïvodji, Sébastien Gambs, Marie-José Huguet, and Mohamed Siala. Improving fairness generalization through a sample-robust optimization method. Machine Learning, 112(6):2131-2192, 2023.</p>
<p>Flexible and inherently comprehensible knowledge representation for dataefficient learning and trustworthy human-machine teaming in manufacturing environments. Vedran Galetic, Alistair Nottle, Proceedings of the Neuro-symbolic AI for Agent and Multi-Agent Systems [NeSyMAS] Workshop, part of the 22nd International Conference on Autonomous Agents and Multiagent Systems. the Neuro-symbolic AI for Agent and Multi-Agent Systems [NeSyMAS] Workshop, part of the 22nd International Conference on Autonomous Agents and Multiagent SystemsLondonVedran Galetic and Alistair Nottle. Flexible and inherently comprehensible knowledge representation for data- efficient learning and trustworthy human-machine teaming in manufacturing environments. In Proceedings of the Neuro-symbolic AI for Agent and Multi-Agent Systems [NeSyMAS] Workshop, part of the 22nd International Conference on Autonomous Agents and Multiagent Systems, London, 2023. Presented on May 30, 2023.</p>
<p>Thinking fast and slow in ai: The role of metacognition. M Bergamaschi Ganapini, Murray Campbell, Francesco Fabiano, Lior Horesh, Jon Lenchner, Andrea Loreggia, Nicholas Mattei, Francesca Rossi, Biplav Srivastava, Kristen Brent Venable, Machine Learning, Optimization, and Data Science. Giuseppe Nicosia, Varun Ojha, Emanuele La Malfa, Gabriele La Malfa, Panos Pardalos, Giuseppe Di Fatta, Giovanni Giuffrida, and Renato UmetonChamSpringer Nature SwitzerlandM. Bergamaschi Ganapini, Murray Campbell, Francesco Fabiano, Lior Horesh, Jon Lenchner, Andrea Loreg- gia, Nicholas Mattei, Francesca Rossi, Biplav Srivastava, and Kristen Brent Venable. Thinking fast and slow in ai: The role of metacognition. In Giuseppe Nicosia, Varun Ojha, Emanuele La Malfa, Gabriele La Malfa, Panos Pardalos, Giuseppe Di Fatta, Giovanni Giuffrida, and Renato Umeton, editors, Machine Learning, Optimization, and Data Science, pages 502-509, Cham, 2023. Springer Nature Switzerland.</p>
<p>Unconditional scene graph generation. Sarthak Garg, Helisa Dhamo, Azade Farshad, Sabrina Musatian, Nassir Navab, Federico Tombari, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionSarthak Garg, Helisa Dhamo, Azade Farshad, Sabrina Musatian, Nassir Navab, and Federico Tombari. Un- conditional scene graph generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16362-16371, 2021.</p>
<p>Logic explained networks. Marco Gori, 17th International Workshop on Neural-Symbolic Learning and Reasoning (NESY 2023). Siena, Italy2023Marco Gori. Logic explained networks. In 17th International Workshop on Neural-Symbolic Learning and Reasoning (NESY 2023), Certosa di Pontignano, Siena, Italy, 2023.</p>
<p>Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR '16. 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR '16IEEEKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR '16, pages 770-778. IEEE, June 2016.</p>
<p>A hybrid neural network model for commonsense reasoning. Pengcheng He, Xiaodong Liu, Weizhu Chen, Jianfeng Gao, Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing. the First Workshop on Commonsense Inference in Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsPengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network model for com- monsense reasoning. In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 13-21, Hong Kong, China, November 2019. Association for Computational Linguistics.</p>
<p>Gans trained by a two time-scale update rule converge to a local nash equilibrium. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, Advances in Neural Information Processing Systems. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.</p>
<p>Kwang Hyuk Im and Sang Chan Park. Case-based reasoning and neural network based expert system for personalization. Expert Systems with Applications. 321Kwang Hyuk Im and Sang Chan Park. Case-based reasoning and neural network based expert system for personalization. Expert Systems with Applications, 32(1):77-85, 2007.</p>
<p>Conceptors: an easy introduction. ArXiv, abs/1406.2671. Herbert Jaeger, Herbert Jaeger. Conceptors: an easy introduction. ArXiv, abs/1406.2671, 2014.</p>
<p>Constraint reasoning embedded structured prediction. Nan Jiang, Maosen Zhang, Willem-Jan Van Hoeve, Yexiang Xue, Journal of Machine Learning Research. 23345Nan Jiang, Maosen Zhang, Willem-Jan van Hoeve, and Yexiang Xue. Constraint reasoning embedded structured prediction. Journal of Machine Learning Research, 23(345):1-40, 2022.</p>
<p>Image generation from scene graphs. Justin Johnson, Agrim Gupta, Li Fei-Fei, CVPR. Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In CVPR, 2018.</p>
<p>Thinking, fast and slow. Farrar, Straus and Giroux. Daniel Kahneman, New YorkDaniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, New York, 2011.</p>
<p>The third ai summer: Aaai robert s. engelmore memorial lecture. AI Magazine. Henry Kautz, 43Henry Kautz. The third ai summer: Aaai robert s. engelmore memorial lecture. AI Magazine, 43(1):105-125, 2022.</p>
<p>Accelerating chip design with machine learning. Brucek Khailany, Haoxing Ren, Steve Dai, Saad Godil, Ben Keller, Robert Kirby, Alicia Klinefelter, Rangharajan Venkatesan, Yanqing Zhang, Bryan Catanzaro, IEEE Micro. 406Brucek Khailany, Haoxing Ren, Steve Dai, Saad Godil, Ben Keller, Robert Kirby, Alicia Klinefelter, Ranghara- jan Venkatesan, Yanqing Zhang, Bryan Catanzaro, et al. Accelerating chip design with machine learning. IEEE Micro, 40(6):23-32, 2020.</p>
<p>Learning combinatorial optimization algorithms over graphs. Advances in neural information processing systems. Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, Le Song, 30Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in neural information processing systems, 30, 2017.</p>
<p>A deep learning approach for interior designing of apartment building architecture using u2 net. Gaurav Kumar, Ashish Kumar, Rahul , 2022 International Conference on Sustainable Computing and Data Communication Systems (ICSCDS). Gaurav Kumar, Ashish Kumar, and Rahul. A deep learning approach for interior designing of apartment building architecture using u2 net. In 2022 International Conference on Sustainable Computing and Data Communication Systems (ICSCDS), pages 1465-1471, 2022.</p>
<p>Graph neural networks meet neural-symbolic computing: A survey and perspective. C Luís, Artur D&apos;avila Lamb, Marco Garcez, Marcelo O R Gori, Pedro H C Prates, Moshe Y Avelar, Vardi, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. Christian Bessierethe Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-207Survey trackLuís C. Lamb, Artur d'Avila Garcez, Marco Gori, Marcelo O.R. Prates, Pedro H.C. Avelar, and Moshe Y. Vardi. Graph neural networks meet neural-symbolic computing: A survey and perspective. In Christian Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 4877-4884. International Joint Conferences on Artificial Intelligence Organization, 7 2020. Survey track.</p>
<p>A path towards autonomous machine intelligence version 0.9. Yann Lecun, Open Review. 22022Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62, 2022.</p>
<p>Visual-attention gan for interior sketch colourisation. Xinrong Li, Hong Li, Chiyu Wang, Xun Hu, Wei Zhang, IET Image Processing. 154Xinrong Li, Hong Li, Chiyu Wang, Xun Hu, and Wei Zhang. Visual-attention gan for interior sketch colourisa- tion. IET Image Processing, 15(4):997-1007, 2021.</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.</p>
<p>The different role of working memory in open-ended versus closed-ended creative problem solving: A dual-process theory account. Yunn-Wen Wei-Lun Lin, Lien, Creativity Research Journal. 251Wei-Lun Lin and Yunn-Wen Lien. The different role of working memory in open-ended versus closed-ended creative problem solving: A dual-process theory account. Creativity Research Journal, 25(1):85-96, 2013.</p>
<p>Learning to compose visual relations. Nan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, Antonio Torralba, Advances in Neural Information Processing Systems. 34Nan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and Antonio Torralba. Learning to compose visual relations. Advances in Neural Information Processing Systems, 34:23166-23178, 2021.</p>
<p>Boosting combinatorial problem modeling with machine learning. Michele Lombardi, Michela Milano, arXiv:1807.05517arXiv preprintMichele Lombardi and Michela Milano. Boosting combinatorial problem modeling with machine learning. arXiv preprint arXiv:1807.05517, 2018.</p>
<p>The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, Jiajun Wu, International Conference on Learning Representations. Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The neuro-symbolic con- cept learner: Interpreting scenes, words, and sentences from natural supervision. In International Conference on Learning Representations, 2019.</p>
<p>Glancenets: Interpretable, leak-proof concept-based models. E Marconato, S Passerini, Teso, Advances in Neural Information Processing Systems. E Marconato, A Passerini, and S Teso. Glancenets: Interpretable, leak-proof concept-based models. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Detect, understand, act: A neurosymbolic hierarchical reinforcement learning framework. Ludovico Mitchener, David Tuckey, Matthew Crosby, Alessandra Russo, Machine Learning. 111Ludovico Mitchener, David Tuckey, Matthew Crosby, and Alessandra Russo. Detect, understand, act: A neuro- symbolic hierarchical reinforcement learning framework. Machine Learning, 111(4):1523-1549, 2022.</p>
<p>Decision focused learning for prediction + optimisation problems. Maxime Mulamba, Emilio Gamba, Tias Guns, Proceedings of AAAI, Constraint Programming and Machine Learning Bridge Program. AAAI, Constraint Programming and Machine Learning Bridge ProgramMaxime Mulamba, Emilio Gamba, and Tias Guns. Decision focused learning for prediction + optimisation problems. In Proceedings of AAAI, Constraint Programming and Machine Learning Bridge Program, 2023.</p>
<p>House-gan: Relational generative adversarial networks for graph-constrained house layout generation. Nelson Nauata, Kai-Hung Chang, Chin-Yi Cheng, Greg Mori, Yasutaka Furukawa, European Conference on Computer Vision. SpringerNelson Nauata, Kai-Hung Chang, Chin-Yi Cheng, Greg Mori, and Yasutaka Furukawa. House-gan: Relational generative adversarial networks for graph-constrained house layout generation. In European Conference on Computer Vision, pages 162-177. Springer, 2020.</p>
<p>House-gan++: Generative adversarial layout refinement network towards intelligent computational agent for professional architects. Nelson Nauata, Sepidehsadat Hosseini, Kai-Hung Chang, Hang Chu, Chin-Yi Cheng, Yasutaka Furukawa, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionNelson Nauata, Sepidehsadat Hosseini, Kai-Hung Chang, Hang Chu, Chin-Yi Cheng, and Yasutaka Fu- rukawa. House-gan++: Generative adversarial layout refinement network towards intelligent computational agent for professional architects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13632-13641, 2021.</p>
<p>Glide: Towards photorealistic image generation and editing with text-guided diffusion models. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, Mark Chen, Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2021.</p>
<p>Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Maxwell Nye, Michael Tessler, Josh Tenenbaum, M Brenden, Lake, Advances in Neural Information Processing Systems. M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman VaughanCurran Associates, Inc34Maxwell Nye, Michael Tessler, Josh Tenenbaum, and Brenden M Lake. Improving coherence and consis- tency in neural sequence models with dual-system, neuro-symbolic reasoning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 25192-25204. Curran Associates, Inc., 2021.</p>
<p>Towards neural network-based reasoning. Baolin Peng, Zhengdong Lu, Hang Li, Kam-Fai Wong, abs/1508.05508ArXiv. Baolin Peng, Zhengdong Lu, Hang Li, and Kam-Fai Wong. Towards neural network-based reasoning. ArXiv, abs/1508.05508, 2015.</p>
<p>Relational reasoning using neural networks: A survey. Hima Anil Audumbar Pise, Ian Vadapalli, Sanders, International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems. 29Supp02Anil Audumbar Pise, Hima Vadapalli, and Ian Sanders. Relational reasoning using neural networks: A survey. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 29(Supp02):237-258, 2021.</p>
<p>Classifier-based constraint acquisition. Eugene C Steven D Prestwich, Freuder, O&apos; Barry, David Sullivan, Browne, Annals of Mathematics and Artificial Intelligence. 89Steven D Prestwich, Eugene C Freuder, Barry O'Sullivan, and David Browne. Classifier-based constraint acquisition. Annals of Mathematics and Artificial Intelligence, 89:655-674, 2021.</p>
<p>Hierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.06125arXiv preprintAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, PMLRProceedings of the 38th International Conference on Machine Learning. Marina Meila and Tong Zhangthe 38th International Conference on Machine Learning139Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8821-8831. PMLR, 18-24 Jul 2021.</p>
<p>High-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.</p>
<p>Using constraint-based operators to solve the vehicle routing problem with time windows. Louis-Martin Rousseau, Michel Gendreau, Gilles Pesant, Journal of heuristics. 8Louis-Martin Rousseau, Michel Gendreau, and Gilles Pesant. Using constraint-based operators to solve the vehicle routing problem with time windows. Journal of heuristics, 8:43-58, 2002.</p>
<p>Improved techniques for training gans. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, Advances in neural information processing systems. 29Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.</p>
<p>A simple neural network module for relational reasoning. Adam Santoro, David Raposo, G David, Mateusz Barrett, Razvan Malinowski, Peter Pascanu, Timothy Battaglia, Lillicrap, Advances in Neural Information Processing Systems. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.</p>
<p>Deep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, PMLRProceedings of the 32nd International Conference on Machine Learning. Francis Bach and David Bleithe 32nd International Conference on Machine LearningLille, France37Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256-2265, Lille, France, 07-09 Jul 2015. PMLR.</p>
<p>The shifting sands of creative thinking: Connections to dual-process theory. Paul T Sowden, Andrew Pringle, Liane Gabora, Thinking &amp; Reasoning. 211Paul T. Sowden, Andrew Pringle, and Liane Gabora. The shifting sands of creative thinking: Connections to dual-process theory. Thinking &amp; Reasoning, 21(1):40-60, 2015.</p>
<p>Neural qbafs: Explaining neural networks under lrp-based argumentation frameworks. P Sukpanichnant, P Rago, F Lertvittayakumjorn, Toni, Proc. AIxIA 2021. AIxIA 2021Springer13196P Sukpanichnant, A Rago, P Lertvittayakumjorn, and F Toni. Neural qbafs: Explaining neural networks under lrp-based argumentation frameworks. In Proc. AIxIA 2021, LNCS 13196, pages 429-444. Springer, 2021.</p>
<p>Neuro-symbolic program search for autonomous driving decision module design. Jiankai Sun, Hao Sun, Tian Han, Bolei Zhou, PMLRProceedings of the 2020 Conference on Robot Learning. Jens Kober, Fabio Ramos, and Claire Tomlinthe 2020 Conference on Robot Learning155Jiankai Sun, Hao Sun, Tian Han, and Bolei Zhou. Neuro-symbolic program search for autonomous driving decision module design. In Jens Kober, Fabio Ramos, and Claire Tomlin, editors, Proceedings of the 2020 Conference on Robot Learning, volume 155 of Proceedings of Machine Learning Research, pages 21-30. PMLR, 16-18 Nov 2021.</p>
<p>Object-centric image generation from layouts. Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, Devon Hjelm, Shikhar Sharma, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 2647-2655, 2021.</p>
<p>An image inpainting technique based on the fast marching method. Alexandru Telea, Journal of graphics tools. 91Alexandru Telea. An image inpainting technique based on the fast marching method. Journal of graphics tools, 9(1):23-34, 2004.</p>
<p>A roadmap for neuro-argumentative learning. Francesca Toni, 17th International Workshop on Neural-Symbolic Learning and Reasoning (NESY 2023). Siena, Italy2023Short paperFrancesca Toni. A roadmap for neuro-argumentative learning. In 17th International Workshop on Neural-Symbolic Learning and Reasoning (NESY 2023), Certosa di Pontignano, Siena, Italy, 2023. Short paper.</p>
<p>A simple method for commonsense reasoning. H Trieu, Quoc V Trinh, Le, Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning, 2018.</p>
<p>Learning constraint models from data. C Dimosthenis, Tias Tsouros, Kostas Guns, Stergiou, Proceedings of AAAI, Constraint Programming and Machine Learning Bridge Program. AAAI, Constraint Programming and Machine Learning Bridge ProgramDimosthenis C. Tsouros, Tias Guns, and Kostas Stergiou. Learning constraint models from data. In Proceedings of AAAI, Constraint Programming and Machine Learning Bridge Program, 2023.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Biosystems design by machine learning. Michael Jeffrey Volk, Ismini Lourentzou, Shekhar Mishra, Tung Lam, Chengxiang Vo, Huimin Zhai, Zhao, ACS synthetic biology. 97Michael Jeffrey Volk, Ismini Lourentzou, Shekhar Mishra, Lam Tung Vo, Chengxiang Zhai, and Huimin Zhao. Biosystems design by machine learning. ACS synthetic biology, 9(7):1514-1533, 2020.</p>
<p>Learning mdps from features: Predict-then-optimize for sequential decision making by reinforcement learning. Kai Wang, Sanket Shah, Haipeng Chen, Andrew Perrault, Finale Doshi-Velez, Milind Tambe, Advances in Neural Information Processing Systems. M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman VaughanCurran Associates, Inc34Kai Wang, Sanket Shah, Haipeng Chen, Andrew Perrault, Finale Doshi-Velez, and Milind Tambe. Learn- ing mdps from features: Predict-then-optimize for sequential decision making by reinforcement learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 8795-8806. Curran Associates, Inc., 2021.</p>
<p>Rlayout: Interior design system based on reinforcement learning. Ningqi Wang, Chang Niu, Zizhong Li, 12th International Symposium on Computational Intelligence and Design (ISCID). 1Ningqi Wang, Chang Niu, and Zizhong Li. Rlayout: Interior design system based on reinforcement learning. In 2019 12th International Symposium on Computational Intelligence and Design (ISCID), volume 1, pages 117-120, 2019.</p>
<p>A learning algorithm for continually running fully recurrent neural networks. Ronald J Williams, David Zipser, Neural Computation. 12Ronald J. Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural Computation, 1(2):270-280, 1989.</p>
<p>Spring: Gpt-4 out-performs rl algorithms by studying papers and reasoning. Yue Wu, Shrimai Prabhumoye, Yonatan So Yeon Min, Ruslan Bisk, Amos Salakhutdinov, Tom Azaria, Yuanzhi Mitchell, Li, Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms by studying papers and reasoning, 2023.</p>
<p>Kun Xu, Haoyu Liang, Jun Zhu, Hang Su, Bo Zhang, arXiv:1807.03877Deep structured generative models. arXiv preprintKun Xu, Haoyu Liang, Jun Zhu, Hang Su, and Bo Zhang. Deep structured generative models. arXiv preprint arXiv:1807.03877, 2018.</p>
<p>Embedding decision diagrams into generative adversarial networks. Yexiang Xue, Willem-Jan Van Hoeve, International Conference on Integration of Constraint Programming. SpringerYexiang Xue and Willem-Jan van Hoeve. Embedding decision diagrams into generative adversarial networks. In International Conference on Integration of Constraint Programming, Artificial Intelligence, and Operations Research, pages 616-632. Springer, 2019.</p>
<p>Safe reinforcement learning via probabilistic logic shields. Wen-Chi Yang, Giuseppe Marra, Gavin Rens, Luc De Raedt, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23). IJCAI. the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23). IJCAIWen-Chi Yang, Giuseppe Marra, Gavin Rens, and Luc De Raedt. Safe reinforcement learning via probabilistic logic shields. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23). IJCAI, 2023.</p>
<p>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.</p>
<p>Deep Learning Architect: Classification for Architectural Design Through the Eye of Artificial Intelligence. Yuji Yoshimura, Bill Cai, Zhoutong Wang, Carlo Ratti, Springer International PublishingChamYuji Yoshimura, Bill Cai, Zhoutong Wang, and Carlo Ratti. Deep Learning Architect: Classification for Architectural Design Through the Eye of Artificial Intelligence, pages 249-265. Springer International Pub- lishing, Cham, 2019.</p>
<p>. Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, arXiv:1806.01830Relational deep reinforcement learning. arXiv preprintVinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830, 2018.</p>
<p>Learning visual commonsense for robust scene graph generation. Alireza Zareian, Zhecan Wang, Haoxuan You, Shih-Fu Chang, European Conference on Computer Vision. SpringerAlireza Zareian, Zhecan Wang, Haoxuan You, and Shih-Fu Chang. Learning visual commonsense for robust scene graph generation. In European Conference on Computer Vision, pages 642-657. Springer, 2020.</p>
<p>Adding conditional control to text-to-image diffusion models. Lvmin Zhang, Anyi Rao, Maneesh Agrawala, Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion mod- els, 2023.</p>
<p>Image generation from layout. Bo Zhao, Lili Meng, Weidong Yin, Leonid Sigal, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionBo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8584-8593, 2019.</p>
<p>Argumentative xai: A survey. Antonio Kristijonasčyras, Emanuele Rago, Pietro Albini, Francesca Baroni, Toni, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. Zhi-Hua Zhouthe Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-2182021Survey TrackKristijonasČyras, Antonio Rago, Emanuele Albini, Pietro Baroni, and Francesca Toni. Argumentative xai: A survey. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4392-4399. International Joint Conferences on Artificial Intelligence Organiza- tion, 8 2021. Survey Track.</p>
<p>128) rnd(w o2 × 1.5,200) 3 rnd(1,900) rnd(1,144) rnd(64, 128) rnd(w o3 × 2,w o3 × 2) 4 rnd(1,1050) rnd(400,665) rnd(64, 128) rnd(w o4 − 10,w o4 + 10) Constraints: • left(o1, o2, 400) -Object 1 is left of object 2 by at least 400 per mille. • above(o2, o4, 200) -Object 2 is above object 4 by at least 200 per mille. • right(o4, o1, 250) -Object 4 is right of object 1 by at least 250 per mille. ,1050) rnd(375,565) rnd(64, 128) rnd(w o1 × 1.5,200) 2 rnd(1,1050) rnd(y o1 − 10,y o1 + 10) rnd. 64right value(o1, 500) -Object 1 is in the right 50% of the imagernd(1,1050) rnd(375,565) rnd(64, 128) rnd(w o1 × 1.5,200) 2 rnd(1,1050) rnd(y o1 − 10,y o1 + 10) rnd(64, 128) rnd(w o2 × 1.5,200) 3 rnd(1,900) rnd(1,144) rnd(64, 128) rnd(w o3 × 2,w o3 × 2) 4 rnd(1,1050) rnd(400,665) rnd(64, 128) rnd(w o4 − 10,w o4 + 10) Constraints: • left(o1, o2, 400) -Object 1 is left of object 2 by at least 400 per mille. • above(o2, o4, 200) -Object 2 is above object 4 by at least 200 per mille. • right(o4, o1, 250) -Object 4 is right of object 1 by at least 250 per mille. • right value(o1, 500) -Object 1 is in the right 50% of the image.</p>            </div>
        </div>

    </div>
</body>
</html>
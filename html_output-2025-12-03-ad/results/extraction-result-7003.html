<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7003 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7003</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7003</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-210942740</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2001.11003v1.pdf" target="_blank">Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are directly connected. In contrast, local node encoding considers the relations between neighbor nodes capturing the graph structure, but it can fail to capture long-range relations. In this work, we gather both encoding strategies, proposing novel neural models that encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-to-text datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7003.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7003.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TripleLinear</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized triple sequence (edge/triple list)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential serialization of KGs where subject-relation-object triples are flattened into a token sequence and fed to a sequence model (Transformer baseline in this paper). This representation treats triples as a sequence and does not explicitly preserve multi-hop graph topology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized triple sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each RDF/knowledge-graph triple (subject, relation, object) is converted into a textual token sequence (e.g. subject || relation || object) and triples are concatenated to form the model input; no explicit graph adjacency structure is encoded beyond the triple ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; token-based; lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Edge-list / triple-list linearization (concatenate triples in some dataset-dependent order); no canonical ordering reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AGENDA (and used as baseline for WebNLG comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation / KG verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer baseline (linearized triples as input)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard Transformer encoder-decoder where the encoder input is the linearized triple sequence (baseline in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AGENDA baseline BLEU = 14.11 ±0.28 (Table 2). Other baselines reported in WebNLG comparisons (see paper) but linearized-baseline exact WebNLG number not presented in main table.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simple to use with standard seq2seq models; serves as a baseline. Does not exploit graph topology which the authors argue limits node representation quality for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy with respect to graph topology and multi-hop structure; ignores explicit adjacency and long-range structural relations, which can harm generation of coherent multi-sentence outputs and long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Underperforms compared to graph-based encodings that explicitly model local (GNN/GAT) and/or global (self-attention) node interactions; the authors' combined global+local encoders (CGE) outperform the linearized-triple Transformer baseline on AGENDA (BLEU 17.81 vs 14.11).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7003.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7003.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TokenNodeGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-level entity node graph (entity→token nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph transformation used in this paper that expands each multi-token entity into one node per token and converts each entity-level triple into fully connected token-token edges between head and tail tokens, with token positional embeddings retained to preserve intra-entity order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Token-level entity node graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each entity (possibly multi-word) is split into tokens and each token becomes a separate node in the graph; for every original triple (eh, r, et), all tokens of eh are connected to all tokens of et with edges labeled with r. Node initial embeddings are token embeddings plus positional embeddings to recover order inside multi-token entities.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based graph (graph-structured; aims to be more expressive than entity-level graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph transformation: entity→tokens; triple expansion into token-token edges (complete bipartite connections between token sets of linked entities); tokens carry positional embeddings to preserve sequence order.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AGENDA, WebNLG (used as input-graph representation after transformation; dataset statistics reported after this transformation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (KG verbalization, multi-sentence generation on AGENDA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CGE / PGE family (combined global+local GAT-based encoders) and also used as input for baseline calculations</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cascaded Graph Encoder (CGE) and Parallel Graph Encoder (PGE) built on Graph Attention Networks (GAT) variants; CGE cascades global attention then local GAT; PGE runs them in parallel and concatenates outputs. Transformer decoder is used for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, CHRF++, human Fluency/Adequacy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AGENDA: CGE (using this token-node representation) BLEU = 17.81 ±0.15, METEOR = 21.75 ±0.55, CHRF++ = 46.76 ±0.12 (Table 2). WebNLG: CGE variants trained on transformed graphs achieve top reported numbers (e.g., CGE-LG BLEU = 63.10 ±0.13).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Increases representational power and flexibility: token-level nodes allow learning finer-grained alignments between source tokens and generated text; shown to improve automatic metrics and human judgments versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Expands graph size (more nodes and edges) and increases computational cost; breaks natural sequential order of multi-word entities unless positional embeddings are added (authors add positional embeddings to mitigate this); can increase memory and runtime due to dense token-token edges for each triple.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Stronger than entity-level linearizations and triple-only serializations because it allows token-to-token attentions and richer alignments; the paper reports improved performance over transformer baseline and other graph encoders that operate at entity level.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7003.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7003.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LeviGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi transformation / relations-as-nodes (Relations as Nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph conversion where relation-edges are turned into first-class relation nodes connected to subject and object; this makes relations part of the node vocabulary and allows the decoder to attend/generate relation tokens directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi graph (relations-as-nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each original relation edge is converted into a new relation node; the relation node is connected to the subject and object token nodes via two binary edges (subject→relation-node and relation-node→object), effectively turning edges into nodes so relations are represented as tokens in the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured; token/node-based; approximately lossless (preserves relation information explicitly)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph transformation: edge → relation node; connect relation node to subject and object tokens with binary relations. Relation nodes are then treated as part of the shared vocabulary available to decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (explicitly evaluated with this variant: CGE-LG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation / KG verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CGE-LG (Cascaded Graph Encoder with Levi graph input)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cascaded global-local GAT encoder applied to Levi-transformed graphs; relation nodes are included in input node embeddings and share the model vocabulary with tokens, decoder is Transformer-based.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG (seen categories): CGE-LG BLEU = 63.10 ±0.13, METEOR = 44.11 ±0.09, CHRF++ = 76.33 ±0.10 (Table 3). CGE-LG reported as best-performing variant on WebNLG in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allows relations to be generated/attended as vocabulary items (makes relation information directly usable during decoding) and can reduce parameters depending on setup; shown to achieve best WebNLG results with fewer parameters in CGE-LG.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Changes graph structure and increases number of nodes; may increase model graph size but can reduce certain parameterizations; not necessarily canonical — ordering of nodes still not specified; potential mismatch to models that expect edge-labeled graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms relation-as-parameter encoding (CGE-RP) in this paper for the WebNLG seen-categories setting (CGE-LG is the best reported variant), and has the advantage of making relation tokens part of the decoding vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7003.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7003.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RelationsParam</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relations as Parameters (relation-type parameter encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoding strategy where relation types are encoded as learned parameter matrices/weights (per relation) in the local GAT aggregator instead of becoming explicit nodes; the model uses relation-specific linear transforms to incorporate typed edges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relations-as-parameters (relation-weight encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Relation labels r are represented as learned parameter matrices W_r used to transform neighbor node embeddings during local aggregation (h_N(v) = sum_{(u,r) in N(v)} alpha_{vu} W_r h_u), so relations are encoded through model parameters rather than additional nodes or tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>relational weight encoding; graph-structured; lossless with respect to typed relations (keeps relation types), not sequential</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Use per-relation parameter matrices (or regularized factorized basis for many relations) inside a GAT-style local encoder to incorporate relation type into attention and message passing.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (CGE-RP variant reported), AGENDA (local relation encoding used in main models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation / KG verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CGE-RP (Cascaded Graph Encoder with Relations as Parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cascaded global-local GAT encoder where local GAT uses learned W_r matrices per relation (with optional basis-function decomposition to regularize large relation vocabularies); Transformer decoder for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG (seen categories): CGE-RP BLEU = 62.30 ±0.27, METEOR = 43.51 ±0.18, CHRF++ = 75.49 ±0.34 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Encodes typed relations compactly and lets the model learn relation-specific transformations; with basis decomposition can avoid parameter explosion when many relation types exist.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>When number of relation types is large, per-relation parameters can lead to parameter explosion; paper uses basis-function decomposition regularization to mitigate this. Relations-as-parameters are not directly generatable tokens in the decoder (unlike Levi graph), limiting direct relation lexicalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Slightly underperforms Levi-as-node variant on WebNLG in this paper (CGE-RP BLEU 62.30 vs CGE-LG 63.10) but both outperform prior state-of-the-art; relations-as-parameters can be more parameter-intensive without decomposition, while Levi graphs enable relations to be part of vocabulary for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7003.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7003.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BPENodes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Byte-Pair Encoding (BPE) subword node tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A subword tokenization applied to entity and target words so that some graph nodes correspond to subword units rather than whole words; used to reduce vocabulary size and handle rare words across models and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Byte-Pair Encoding (BPE) node tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Entities and targets are segmented with BPE so graph node labels can be subword units; consequently some nodes correspond to subword pieces and both input node vocabulary and output share the same subword vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based; subword sequential encoding for node labels</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Statistical BPE segmentation applied to entity words and target text prior to creating token nodes; nodes may correspond to subword units.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AGENDA, WebNLG (used across models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CGE / PGE / Transformer baseline (all models share BPE vocabulary between node and target tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models use shared subword vocabulary; some nodes represent subword tokens so embedding and decoding operate on shared BPE units.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not reported as a standalone metric; used as preprocessing step in all competitive models (paper notes that sharing the vocabulary was important—ablation shows removing shared vocabulary reduces BLEU: CGE variant without shared vocab BLEU = 15.52 vs CGE 17.25 in ablation Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Helps handle rare words and reduces vocabulary size; sharing the BPE vocabulary between node inputs and target tokens improved performance (ablation indicates vocabulary sharing is critical).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Introduces subword nodes which interact with graph structure (increasing number of nodes in token-level graph); may complicate alignment between original multi-token entities and nodes (paper mitigates with positional embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Standard preprocessing beneficial versus word-level tokenization; the paper explicitly ablates vocabulary sharing and finds it important for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural amr: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Graph Attention Networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7003",
    "paper_id": "paper-210942740",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "TripleLinear",
            "name_full": "Linearized triple sequence (edge/triple list)",
            "brief_description": "A sequential serialization of KGs where subject-relation-object triples are flattened into a token sequence and fed to a sequence model (Transformer baseline in this paper). This representation treats triples as a sequence and does not explicitly preserve multi-hop graph topology.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Linearized triple sequence",
            "representation_description": "Each RDF/knowledge-graph triple (subject, relation, object) is converted into a textual token sequence (e.g. subject || relation || object) and triples are concatenated to form the model input; no explicit graph adjacency structure is encoded beyond the triple ordering.",
            "representation_type": "sequential; token-based; lossy",
            "encoding_method": "Edge-list / triple-list linearization (concatenate triples in some dataset-dependent order); no canonical ordering reported in this paper.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AGENDA (and used as baseline for WebNLG comparisons)",
            "task_name": "graph-to-text generation / KG verbalization",
            "model_name": "Transformer baseline (linearized triples as input)",
            "model_description": "Standard Transformer encoder-decoder where the encoder input is the linearized triple sequence (baseline in experiments).",
            "performance_metric": "BLEU, METEOR, CHRF++",
            "performance_value": "AGENDA baseline BLEU = 14.11 ±0.28 (Table 2). Other baselines reported in WebNLG comparisons (see paper) but linearized-baseline exact WebNLG number not presented in main table.",
            "impact_on_training": "Simple to use with standard seq2seq models; serves as a baseline. Does not exploit graph topology which the authors argue limits node representation quality for generation.",
            "limitations": "Lossy with respect to graph topology and multi-hop structure; ignores explicit adjacency and long-range structural relations, which can harm generation of coherent multi-sentence outputs and long-range dependencies.",
            "comparison_with_other": "Underperforms compared to graph-based encodings that explicitly model local (GNN/GAT) and/or global (self-attention) node interactions; the authors' combined global+local encoders (CGE) outperform the linearized-triple Transformer baseline on AGENDA (BLEU 17.81 vs 14.11).",
            "uuid": "e7003.0",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "TokenNodeGraph",
            "name_full": "Token-level entity node graph (entity→token nodes)",
            "brief_description": "A graph transformation used in this paper that expands each multi-token entity into one node per token and converts each entity-level triple into fully connected token-token edges between head and tail tokens, with token positional embeddings retained to preserve intra-entity order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Token-level entity node graph",
            "representation_description": "Each entity (possibly multi-word) is split into tokens and each token becomes a separate node in the graph; for every original triple (eh, r, et), all tokens of eh are connected to all tokens of et with edges labeled with r. Node initial embeddings are token embeddings plus positional embeddings to recover order inside multi-token entities.",
            "representation_type": "token-based graph (graph-structured; aims to be more expressive than entity-level graphs)",
            "encoding_method": "Graph transformation: entity→tokens; triple expansion into token-token edges (complete bipartite connections between token sets of linked entities); tokens carry positional embeddings to preserve sequence order.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AGENDA, WebNLG (used as input-graph representation after transformation; dataset statistics reported after this transformation)",
            "task_name": "graph-to-text generation (KG verbalization, multi-sentence generation on AGENDA)",
            "model_name": "CGE / PGE family (combined global+local GAT-based encoders) and also used as input for baseline calculations",
            "model_description": "Cascaded Graph Encoder (CGE) and Parallel Graph Encoder (PGE) built on Graph Attention Networks (GAT) variants; CGE cascades global attention then local GAT; PGE runs them in parallel and concatenates outputs. Transformer decoder is used for generation.",
            "performance_metric": "BLEU, METEOR, CHRF++, human Fluency/Adequacy",
            "performance_value": "AGENDA: CGE (using this token-node representation) BLEU = 17.81 ±0.15, METEOR = 21.75 ±0.55, CHRF++ = 46.76 ±0.12 (Table 2). WebNLG: CGE variants trained on transformed graphs achieve top reported numbers (e.g., CGE-LG BLEU = 63.10 ±0.13).",
            "impact_on_training": "Increases representational power and flexibility: token-level nodes allow learning finer-grained alignments between source tokens and generated text; shown to improve automatic metrics and human judgments versus baselines.",
            "limitations": "Expands graph size (more nodes and edges) and increases computational cost; breaks natural sequential order of multi-word entities unless positional embeddings are added (authors add positional embeddings to mitigate this); can increase memory and runtime due to dense token-token edges for each triple.",
            "comparison_with_other": "Stronger than entity-level linearizations and triple-only serializations because it allows token-to-token attentions and richer alignments; the paper reports improved performance over transformer baseline and other graph encoders that operate at entity level.",
            "uuid": "e7003.1",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "LeviGraph",
            "name_full": "Levi transformation / relations-as-nodes (Relations as Nodes)",
            "brief_description": "A graph conversion where relation-edges are turned into first-class relation nodes connected to subject and object; this makes relations part of the node vocabulary and allows the decoder to attend/generate relation tokens directly.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Levi graph (relations-as-nodes)",
            "representation_description": "Each original relation edge is converted into a new relation node; the relation node is connected to the subject and object token nodes via two binary edges (subject→relation-node and relation-node→object), effectively turning edges into nodes so relations are represented as tokens in the graph.",
            "representation_type": "graph-structured; token/node-based; approximately lossless (preserves relation information explicitly)",
            "encoding_method": "Graph transformation: edge → relation node; connect relation node to subject and object tokens with binary relations. Relation nodes are then treated as part of the shared vocabulary available to decoder.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (explicitly evaluated with this variant: CGE-LG)",
            "task_name": "graph-to-text generation / KG verbalization",
            "model_name": "CGE-LG (Cascaded Graph Encoder with Levi graph input)",
            "model_description": "Cascaded global-local GAT encoder applied to Levi-transformed graphs; relation nodes are included in input node embeddings and share the model vocabulary with tokens, decoder is Transformer-based.",
            "performance_metric": "BLEU, METEOR, CHRF++",
            "performance_value": "WebNLG (seen categories): CGE-LG BLEU = 63.10 ±0.13, METEOR = 44.11 ±0.09, CHRF++ = 76.33 ±0.10 (Table 3). CGE-LG reported as best-performing variant on WebNLG in paper.",
            "impact_on_training": "Allows relations to be generated/attended as vocabulary items (makes relation information directly usable during decoding) and can reduce parameters depending on setup; shown to achieve best WebNLG results with fewer parameters in CGE-LG.",
            "limitations": "Changes graph structure and increases number of nodes; may increase model graph size but can reduce certain parameterizations; not necessarily canonical — ordering of nodes still not specified; potential mismatch to models that expect edge-labeled graphs.",
            "comparison_with_other": "Outperforms relation-as-parameter encoding (CGE-RP) in this paper for the WebNLG seen-categories setting (CGE-LG is the best reported variant), and has the advantage of making relation tokens part of the decoding vocabulary.",
            "uuid": "e7003.2",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "RelationsParam",
            "name_full": "Relations as Parameters (relation-type parameter encoding)",
            "brief_description": "An encoding strategy where relation types are encoded as learned parameter matrices/weights (per relation) in the local GAT aggregator instead of becoming explicit nodes; the model uses relation-specific linear transforms to incorporate typed edges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Relations-as-parameters (relation-weight encoding)",
            "representation_description": "Relation labels r are represented as learned parameter matrices W_r used to transform neighbor node embeddings during local aggregation (h_N(v) = sum_{(u,r) in N(v)} alpha_{vu} W_r h_u), so relations are encoded through model parameters rather than additional nodes or tokens.",
            "representation_type": "relational weight encoding; graph-structured; lossless with respect to typed relations (keeps relation types), not sequential",
            "encoding_method": "Use per-relation parameter matrices (or regularized factorized basis for many relations) inside a GAT-style local encoder to incorporate relation type into attention and message passing.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (CGE-RP variant reported), AGENDA (local relation encoding used in main models)",
            "task_name": "graph-to-text generation / KG verbalization",
            "model_name": "CGE-RP (Cascaded Graph Encoder with Relations as Parameters)",
            "model_description": "Cascaded global-local GAT encoder where local GAT uses learned W_r matrices per relation (with optional basis-function decomposition to regularize large relation vocabularies); Transformer decoder for generation.",
            "performance_metric": "BLEU, METEOR, CHRF++",
            "performance_value": "WebNLG (seen categories): CGE-RP BLEU = 62.30 ±0.27, METEOR = 43.51 ±0.18, CHRF++ = 75.49 ±0.34 (Table 3).",
            "impact_on_training": "Encodes typed relations compactly and lets the model learn relation-specific transformations; with basis decomposition can avoid parameter explosion when many relation types exist.",
            "limitations": "When number of relation types is large, per-relation parameters can lead to parameter explosion; paper uses basis-function decomposition regularization to mitigate this. Relations-as-parameters are not directly generatable tokens in the decoder (unlike Levi graph), limiting direct relation lexicalization.",
            "comparison_with_other": "Slightly underperforms Levi-as-node variant on WebNLG in this paper (CGE-RP BLEU 62.30 vs CGE-LG 63.10) but both outperform prior state-of-the-art; relations-as-parameters can be more parameter-intensive without decomposition, while Levi graphs enable relations to be part of vocabulary for decoding.",
            "uuid": "e7003.3",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "BPENodes",
            "name_full": "Byte-Pair Encoding (BPE) subword node tokenization",
            "brief_description": "A subword tokenization applied to entity and target words so that some graph nodes correspond to subword units rather than whole words; used to reduce vocabulary size and handle rare words across models and datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Byte-Pair Encoding (BPE) node tokenization",
            "representation_description": "Entities and targets are segmented with BPE so graph node labels can be subword units; consequently some nodes correspond to subword pieces and both input node vocabulary and output share the same subword vocabulary.",
            "representation_type": "token-based; subword sequential encoding for node labels",
            "encoding_method": "Statistical BPE segmentation applied to entity words and target text prior to creating token nodes; nodes may correspond to subword units.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AGENDA, WebNLG (used across models)",
            "task_name": "graph-to-text generation",
            "model_name": "CGE / PGE / Transformer baseline (all models share BPE vocabulary between node and target tokens)",
            "model_description": "Models use shared subword vocabulary; some nodes represent subword tokens so embedding and decoding operate on shared BPE units.",
            "performance_metric": "BLEU, METEOR, CHRF++",
            "performance_value": "Not reported as a standalone metric; used as preprocessing step in all competitive models (paper notes that sharing the vocabulary was important—ablation shows removing shared vocabulary reduces BLEU: CGE variant without shared vocab BLEU = 15.52 vs CGE 17.25 in ablation Table 4).",
            "impact_on_training": "Helps handle rare words and reduces vocabulary size; sharing the BPE vocabulary between node inputs and target tokens improved performance (ablation indicates vocabulary sharing is critical).",
            "limitations": "Introduces subword nodes which interact with graph structure (increasing number of nodes in token-level graph); may complicate alignment between original multi-token entities and nodes (paper mitigates with positional embeddings).",
            "comparison_with_other": "Standard preprocessing beneficial versus word-level tokenization; the paper explicitly ablates vocabulary sharing and finds it important for best performance.",
            "uuid": "e7003.4",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural amr: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "Graph Attention Networks",
            "rating": 1,
            "sanitized_title": "graph_attention_networks"
        }
    ],
    "cost": 0.0142935,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs
29 Jan 2020</p>
<p>Leonardo F R Ribeiro ribeiro@aiphes.tu-darmstadt.de 
Research Training Group AIPHES and UKP Lab
Technische Universität Darmstadt ‡ School of Engineering
Westlake University</p>
<p>Yue Zhang yue.zhang@wias.org.cn 
Claire Gardent claire.gardent@loria.fr 
CNRS/LORIA
NancyFrance</p>
<p>Iryna Gurevych 
Research Training Group AIPHES and UKP Lab
Technische Universität Darmstadt ‡ School of Engineering
Westlake University</p>
<p>Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs
29 Jan 20203177C4DEAE117A60EC4DFC69E0703322arXiv:2001.11003v1[cs.CL]
Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations.Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are connected.In contrast, local node encoding considers the relations between directly connected nodes capturing the graph structure, but it can fail to capture long-range relations.In this work, we gather the best of both encoding strategies, proposing novel models that encode an input graph combining both global and local node contexts.Our approaches are able to learn better contextualized node embeddings for text generation.In our experiments, we demonstrate that our models lead to significant improvements in KGto-text generation, achieving BLEU scores of 17.81 on AGENDA dataset, and 63.10 on the WebNLG dataset for seen categories, outperforming the state of the art by 3.51 and 2.51 points, respectively 1 .</p>
<p>Introduction</p>
<p>Graph-to-text generation refers to the task of generating natural language text from input graph structures, which can be semantic representations (Konstas et al., 2017), sub-graphs from knowledge graphs (KG) (Koncel-Kedziorski et al., 2019) or other forms of structured data (Konstas and Lapata, 2013).While many recent works (Song et al., 2018;Damonte and Cohen, 2019;Ribeiro et al., 2019;Guo et al., 2019) focus on generating sentence-level outputs, a more challenging and interesting scenario emerges when the goal is to generate bigger multi-sentence text, such as a document or paragraph.In this context, the input graphs are much more diverse, representing 1 Code is available at https://github.com/UKPLab/kg2textknowledge from different domains and in different ways.The task is thus more demanding since it can be necessary to select relevant parts of graph for generating a concise text, and to handle document planning issues such as order, coherence and discourse markers (Gardent et al., 2017).</p>
<p>A key issue in neural graph-to-text generation is how to encode graphs.The basic idea is to incrementally calculate node representations by aggregating context information.To this end, two main approaches have been proposed: (i) models based on local node aggregation, usually based on Graph Neural Networks (GNN) (Ribeiro et al., 2019;Guo et al., 2019) and (ii) models that leverage global node aggregation.Systems based on the global encoding strategy are typically based on Transformer architectures (Zhu et al., 2019;Cai and Lam, 2020), using self-attention to compute a node representation based on all nodes in the graph.This approach enjoys the advantage of large context range, but neglects the graph topology by effectively treating every node as being connected to all the others in the graph.In contrast, models based on local aggregation learn the representation of each node based on its adjacent nodes as defined in the input.This method effectively exploits the graph structure.However, encoding relations between distant nodes can be challenging by requiring more graph encoding layers, which can also propagate noise (Li et al., 2018).</p>
<p>For example, Figure 1a presents a KG, for which a corresponding text is shown in Figure 1b.The nodes GNN and DistMulti have relations with the nodes node embeddings and link prediction, respectively.Both relations are important for GNN and DistMulti during the text generation phase, but are in different connected components.As shown in Figure 1c, a global encoder can learn a node representation for DistMulti which captures information from indirectly connected entities such as node embeddings.Encoding such dependencies is important for KG verbalisation as KGs are known to be highly incomplete, often missing links between entities (Schlichtkrull et al., 2018).In addition, the global encoding can capture long-range complex dependencies between entities, supporting document planning.In contrast, the local strategy refines the node representation with richer neighborhood information, as nodes that share the same neighborhood exhibit a strong homophily: two entities belonging to the same topic in a KG are much more likely to be connected than at random.Consequently, the local context enriches the node representation with topic-related information from KG triples.For example, in Figure 1a, GAT reaches node embeddings through the GNN.This transitive relation can be captured by a local encoder, as shown in Figure 1d.Capturing this form of relationship also can support text generation at the sentence level.</p>
<p>In this paper, we investigate novel graph-totext architectures that combine both global and lo-cal node aggregations, gathering the benefits from both strategies.In particular, we propose a unified graph-to-text framework based on Graph Attention Networks (GAT, Veličković et al., 2018).As part of this framework, we empirically compare two main architectures: a cascaded architecture that performs global node aggregation before performing local node aggregation, and a parallel architecture that performs global and local aggregation simultaneously, before concatenating the representations.While the cascaded architecture allows the local encoder to leverage global encoding features, the parallel architecture allows more independent features to compliment each other.To further consider fine-grained integration, we additionally consider layer-wise integration of global and local encoders.</p>
<p>Extensive experiments show that our approaches consistently outperform recent models on two benchmarks for text generation from KGs, giving the best reported results so far.Compared with parallel structures, cascaded structures give better performance with smaller numbers of parameters.To the best of our knowledge, we are the first to consider integrating global and local context aggregation in graph-to-text generation, and the first to propose a unified GAT structure for integrating global and local aggregation.</p>
<p>Related Work</p>
<p>Early efforts for graph-to-text generation employ statistical methods (Flanigan et al., 2016;Pourdamghani et al., 2016;Song et al., 2017).Recently, several neural graph-to-text models have exhibited success by levering different encoder mechanisms based on GNN and Transformer architectures, learning effective latent graph representations.</p>
<p>AMR-to-Text Generation Recent neural models have been applied to sentence-level generation from Abstract Meaning Representation (AMR) graphs.Konstas et al. (2017) provide the first neural approach for this task, by linearising the input graph as a sequence of nodes and edges.Song et al. (2018) propose the graph recurrent network (GRN) to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model based on GGNNs (Li et al., 2016).However, both approaches only employ local node aggregation strategies.Damonte and Cohen (2019) and Ribeiro et al. (2019) develop models employing GNNs and LSTMs, in order to learn complementary node contexts.Recent methods (Zhu et al., 2019;Cai and Lam, 2020) 2019) introduce a systematic comparison between pipeline and neural end-to-end approaches for text generation from RDF graphs.Nevertheless, those approaches consider the triples as separated structures, not explicitly considering the graph topology.To explicitly encode the graph structure, Marcheggiani and Perez Beltrachini (2018) propose an encoder based on graph convolutional networks (GCN) and show superior performance compared to LSTMs.Our work is related to Koncel-Kedziorski et al. (2019) who propose a transformer-based approach that only focuses on the relations between directly connected nodes.However, our models focus on both global and local node relations, capturing complementary graph contexts.</p>
<p>Graph-to-Text Model</p>
<p>In this section, we describe (i) the general concept of GNNs; (ii) the proposed local and global graph encoders; (iii) the graph transformation adopted to create a relational graph from the input; and (iv) the various combined global and local graph architectures.</p>
<p>Graph Neural Networks (GNN)</p>
<p>Formally, let G = (V, E, R) denote a multirelational graph2 with nodes u, v ∈ V and labelled edges (u, r, v) ∈ E, where r ∈ R represents the relation between u and v. GNNs work by iteratively learning a representation vector h v of a node v ∈ V based on both its context node neighbors and edge features, through an information propagation scheme.More formally, the l-th layer aggregates the representations of v's context nodes:
h (l) N (v) = AGGR (l) h (l−1) u , r vu : u ∈ N (v) ,
where AGGR (l) (.) is an aggregation function, shared by all nodes on the l-th layer.r vu represents the relation between v and u.N (v) is a set of context nodes for v.In most GNNs, the context nodes are those adjacent to v.</p>
<p>The aggregated context representation is used to update the representation of v:
h (l) v = COMBINE (l) h (l−1) v , h(l)
N (v) .</p>
<p>After l iterations, a node's representation encodes the structural information within its lhop neighborhood.The choices of AGGR (l) (.) and COMBINE (l) (.) differ by the specific GNN model.An example of AGGR (l) (.) is the sum of the representations of N (v).An example of COMBINE (l) (.) is a concatenation after the feature transformation.</p>
<p>Global Graph Encoder</p>
<p>A global graph encoder aggregates a global context for updating each node, by treating the graph as fully connected (see Figure 1c).We use the attention mechanism as the message passing scheme, extending the self-attention network structure of Transformer (Vaswani et al., 2017) to a GAT structure.In particular, we compute a layer of the global convolution for a node v ∈ V, which takes the input feature representations h v as input, adopting AGGR (l) (.) as:
h N (v) = u∈V α vu W g h u ,(1)
where W g ∈ R dv×dz is a model parameter.The attention weight α vu is calculated as:
α vu = exp(e vu ) k∈V exp(e vk ) ,(2)
where,
e vu = W q h v W k h u /d z (3)
is the attention function which measures the global importance of node u's features to node v. W q , W k ∈ R dv×dz are model parameters and d z is a scaling factor.</p>
<p>Multi-head Attention.To capture distinct relations between nodes, K different global convolutions are calculated and concatenated:
hN (v) = K k=1 h (k) N (v) .(4)
Finally, we define COMBINE (l) (.) employing layer normalization (LayerNorm) and a fully connected feed-forward network (FFN), in a similar way as the transformer architecture:
ĥv = hN (v) + h v , hv = LayerNorm( ĥv ) , h global v = FFN( hv ) + ĥv .
(5) This strategy creates an artificial complete graph with O(n 2 ) edges.Note that the global encoder do not consider the edge relations between nodes.In particular, if the labelled edges were considered, the self-attention space complexity would increases to Θ(|R| n 2 ).</p>
<p>Local Graph Encoder</p>
<p>The representation h global v captures macro relationships from v to all other nodes in the graph.However, this representation lacks both structural information regarding the local neighborhood of v and the graph topology.Also, it does not capture typed relations between nodes (see Equations 1 and 3).In order to capture those crucial graph information and impose a strong relational inductive bias, we build a local graph encoder by employing a modified version of GAT augmented with relational weights.In particular, we compute a layer of the local convolution for a node v ∈ V, adopting AGGR (l) (.) as:
h N (v) = (u,r) ∈ Ñ (v) α vu W r h u ,(6)
where
W r ∈ R dv×dz encodes the relation r ∈ R between v and u. Ñ (v) = N (v) ∪ v and N (v) is a set of nodes adjacent to v.
The attention coefficient α vu is computed as:
α vu = exp(e vu ) (k,r) ∈ Ñ (v) exp(e vk ) ,(7)
where,
e vu = σ a [W r h v W r h u ] (8)
is the attention function which calculates the relative importance of adjacent nodes, considering typed relations.σ is an activation function, denotes concatenation and a is a model parameter.</p>
<p>We employ multi-head attentions to learn local relations in different perspectives, as in Equation 4, generating ȟN (v) .Finally, we define COMBINE (l) (.) as:
h local v = RNN(h v , ȟN (v) ) ,(9)
where we employ as RNN a Gated Recurrent Unit (GRU) (Cho et al., 2014).GRU facilitates information propagation between local layers.This choice is motivated by recent works (Xu et al., 2018;Dehmamy et al., 2019) that theoretically demonstrate that sharing information between layers helps the structural signals propagate.In a similar direction, AMR-to-text generation models employ LSTMs (Song et al., 2017) and dense connections (Guo et al., 2019) between GNN layers.</p>
<p>Graph Preparation</p>
<p>We represent a KG as a multi-relational graph G e = (V e , E e , R) with entity nodes e ∈ V e and labeled edges (e h , r, e t ) ∈ E e , where r ∈ R denotes the relation existing from the entity e h to e t . 3nlike other current approaches (Koncel-Kedziorski et al., 2019;Moryossef et al., 2019), we represent an entity as a set of nodes.Formally, we transform each G e into a new graph G = (V, E, R), where each token of an entity e ∈ V e becomes a node v ∈ V. We convert each edge (e h , r, e t ) ∈ E e into a set of edges (with the same relation r) and connect every token of e h to every token of e t .That is, an edge (u, r, v) will belong to E if and only if there exists an edge (e h , r, e t ) ∈ E e such that u ∈ e h and v ∈ e t .We represent each node v ∈ V with an embedding h 0 v ∈ R dv , generated from its corresponding token.</p>
<p>The new graph G increases the representational power of the model because it allows learning node embeddings at a token level, instead of entity level.This is particularly important for text generation as it permits the model to be more flexible, capturing richer relationships between entity tokens.This also allows the model to learn relations and attention functions between source and target tokens.However, it has the side effect of removing the natural sequential order of multi-word expressions such as entities.To preserve this information, we employ position embeddings (Vaswani et al., 2017), i.e., h 0 v becomes the sum of the corresponding token embedding and the positional embedding for v.</p>
<p>Combining Global and Local Encodings</p>
<p>Our goal is to implement a graph encoder capable of encoding global and local aspects of the input graph.We hypothesize that the two sources of information are complementary and a combination of both enriches node representations for text generation.In order to test this hypothesis, we investigate four possible combination architectures.Figure 2 presents our proposed encoders.</p>
<p>Parallel Graph Encoding.In this setup, we compose global and local graph encoders in a fully parallel structure (Figure 2a).Note that each graph encoder can have different numbers of layers and attention heads.h 0 v is the initial input for the first layer of both encoders.The final node representation is the concatenation of the local and global node representations:
h v = [ h global v h local v ] .
Cascaded Graph Encoding.We cascade local and global graph encoders as shown in Figure 2b, by first computing a global-contextual node embedding, and then refining it with the local context.h 0 v is the initial input for the global encoder and h global v is the initial input for the local encoder.</p>
<p>Layer-wise Parallel and Cascaded Graph Encoding.To allow fine-grained interaction between the two types of contextual information, we also combine the encoders in a layer-wise fashion.In particular, for each graph layer, we employ both the local and global encoders in a parallel structure as shown in Figure 2c.We also experiment cascading the graph encoders layer-wise (Figure 2d).</p>
<p>Decoder</p>
<p>Our decoder follows the same core architecture of the Transformer decoder.Each time step t is updated by interleaving multiple rounds of multihead attention over the output of the encoder (node embeddings h v ) and attention over previouslygenerated tokens (token embeddings).An additional challenge in our setup is to generate multisentence outputs.In order to encourage the model to generate longer texts, we employ a length penalty (Wu et al., 2016) to refine the pure maxprobability beam search.</p>
<p>Data and Preprocessing</p>
<p>We attest the effectiveness of our models on two datasets: AGENDA (Koncel-Kedziorski et al., 2019) and WebNLG (Gardent et al., 2017).Table 1 shows the statistics for both datasets.</p>
<p>AGENDA.In this dataset, KGs are paired with scientific abstracts extracted from proceedings of 12 top AI conferences.Each instance consists of the paper title, a KG and the paper abstract.Entities correspond to scientific terms which are often multi-word expressions (co-referential entities are merged).We treat each token in the title as a node, creating a unique graph with title and KG tokens as nodes.As shown in Table 1, the average output length is considerably large, as the target output are multi-sentence abstracts.</p>
<p>WebNLG.In this dataset, each instance contains a graph extracted from DBPedia.The target text consists of one or more sentences that verbalise the graph.We evaluate the models on the test set with seen categories.Note that this dataset has a considerable number of edge relations (see Table 1).In order to avoid parameter explosion, we use regularization based on basis function decomposition to define the model relation weights (Schlichtkrull et al., 2018).Also, as an alternative, we employ the Levi Transformation to create nodes from relational edges between entities (Beck et al., 2018).That is, we create a new relation node for each edge relation between two nodes.The new relation node is connected to the subject and object token entities by two binary relations, respectively.</p>
<p>Experiments and Discussion</p>
<p>The models are trained for 30 epochs with early stopping based on the development BLEU score.We use Adam optimization with initial learning rate of 0.5.The vocabulary is shared between the node and target tokens.In order to mitigate the effects of random seeds, for the test sets, we report the averages for 4 training runs along with their standard deviation.Hyperparameters are tuned on the development set of both datasets.Following previous work (Castro Ferreira et al., 2019), we employ byte pair encoding (BPE) to split entity words into smaller more frequent pieces.So some nodes in the graph can be sub-words.We also obtain sub-words on the target side.We call our models PGE-LW (layer-wise parallel encoder), CGE-LW (layer-wise cascaded encoder), and PGE (fully parallel encoder) and CGE (fully cascaded encoder).We use a standard version of the Transformer as baseline and a linearized version of the triples of the KG is used as input.Following previous works, we evaluate the results in terms of BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and sentence-level CHRF++ (Popović, 2015) scores.To better attest the quality of the generated texts, we also perform a human evaluation.</p>
<p>Results on AGENDA</p>
<p>Results on WebNLG</p>
<p>We compare the performance of our best model (CGE) with six state-of-the-art results of graphto-text models reported for this dataset (Gardent et al., 2017;Trisedya et al., 2018;Marcheggiani and Perez Beltrachini, 2018;Castro Ferreira et al 2018), an approach that encodes both intra-triple and inter-triple relationships, by 4.5 BLEU points.Interestingly, their intra-triple and inter-triple mechanisms capture relationships within a triple and among triples, approaches closely related with our local and global encodings.However, they rely on encoding sequence of relations and entities based on traversal graph algorithms, whereas we explicitly exploit the graph structure, throughout the local neighborhood aggregation.</p>
<p>Relations as Nodes.CGE-LG uses Levi graphs as inputs and achieves the best performance, even thought it uses less parameters.One advantage of this approach is that it allows the model to handle new relations, as they are treated as nodes.Moreover, the relations become part of the shared vocabulary, making this information directly usable during the decoding process.We outperform an approach based on GNNs (Marcheggiani and Perez Beltrachini, 2018)</p>
<p>Ablation Study</p>
<p>In Table 4, we report an ablation study on the impact of each module used in CGE model on the development set of AGENDA dataset.</p>
<p>Global Graph Encoder.We start by an ablation on the global encoder.After removing the global attention coefficients, the performance of the model drops by 1.77 BLEU and 1.72 CHRF++ scores.Results also show that using FFN in the global COMBINE(.)function is important to the model but less effective than the global attentions.However, when we remove FNN, the number of parameters drops considerably (around 19%) from 66.9 to 54.3 million.Finally, without the entire global encoder, the result drops substantially by 2.29 BLEU points.This indicates that enriching node embeddings with a global context allows learning more expressive graph representations.</p>
<p>Local Graph Encoder.We first remove the local graph attention and the BLEU score drops to 16.44, showing that the neighborhood attention improves the performance.After removing the relation types, encoded as model weights, the performance drops 0.48 BLEU points.However, the number of parameters is reduced around 10 million.This indicates that we can have a more efficient model, in terms of the number of parameters, with a slight drop in performance.Removing the GRU used on the COMBINE(.)function drops the performance considerably.The worse performance occurs if we remove the entire local encoder, with a BLEU score of 14.43, essentially making the encoder similar to the baseline.Finally, we note that the vocabulary sharing is critical to improve the performance, and the length penalty is beneficial as we generate multi-sentence outputs.</p>
<p>Comparing Encoding Strategies</p>
<p>The overall performance on both datasets suggests the superiority of combining global and local node representations.However, to have a better understanding of the positive and negative aspects of each proposed model, we introduce a systematic comparison between the encoding strategies.</p>
<p>Figure 3a shows the impact of graph diameter in the four encoding methods.The models perform on par for graphs with smaller diameters.Models based on layer-wise aggregations (PGE-LW and CGE-LW) have better performance when handling larger graph diameters.However, their overall per- formance is worse compared to the fully independent models because only 2% of the graphs on the AGENDA dev set have a diameter larger than or equal to 5.This indicates that the layer-wise encoders can better capture long-distance node dependencies.Moreover, the margin between PGE-LW and CGE-LW increases as the diameters increase, suggesting that PGE-LW can be a good option to encode graphs with larger diameter.Figure 3b shows the models' performance with respect to the number of triples.CGE achieves better results when the number of triples is large (≥ 9).On the other hand, the PGE has relatively worse when handling more information, that is, KGs with more triples.</p>
<p>Impact of the Graph Structure and Output Length</p>
<p>We investigate the performance of our best model (CGE) concerning different data properties.</p>
<p>Number of Triples.</p>
<p>In Table 5, we perform an inspection on the effect of the number of triples on the models' performance, measured using CHRF++ scores 4 for the WebNLG dev set.</p>
<p>In general, our model obtains better scores over almost all partitions, showing that capturing explicitly structural information is beneficial for text generation.The performance decreases as the number of triples increase.However, when handling datapoints with more triples (7), Adapt and our model achieve higher performance.We hypothesize that this happens because the models receive a considerable amount of input data, giving   more context to the text generation process, even though the graph structure being more complex.</p>
<p>Number of Nodes.</p>
<p>Figure 4a shows the effect of the graph size, measured in number of nodes, on the performance.Note that the score increases as the graph size increases.This trend is particularly interesting and contrasting to AMR-to-text generation, in which the models' general performance decreases as the graph size increases (Cai and Lam, 2020).In AMR benchmarks, the graph size is correlated with the sentence size, and longer sentences are more challenging to generate than the smaller ones.On the other hand, AGENDA contains similar abstract lengths5 and when the input is a bigger graph, the model has more information to be leveraged during the generation.We also investigate the performance with respect to the number of local graph layers.The performances with 1 and 4 layers are similar, while the best performance, regardless of the number of nodes, is achieved with 3 layers.Graph Diameter.Figure 4b shows the impact of the graph diameter on the performance, when employing only global or local encoding modules or both, for the AGENDA dev set.Similarly to the graph size, the score increases as the diameter increases.As the global encoder is not aware of the graph structure, this module has the worst scores, even though it enables direct node communication over long distance.In contrast, the local encoder can propagate precise node information throughout the graph structure for k-hop distances, making the relative performance better.We also observe that the performance gap between the global and local encoders increases when the diameter is 1.In this case, the graph has many connected components; that is, the triples do not share entities.It reveals that computing node representation based on adjacent nodes, rather than based on the entire set of entities, leads to better performance.Table 5 shows the performances for our best model and others with respect to the graph diameter for WebNLG dev set.In contrast to AGENDA, the score decreases as the diameter increases.This behavior highlights a crucial difference between the two datasets.Whereas in the WebNLG the graph size is correlated with the output size, this is not the case for AGENDA.For WebNLG, higher diameters pose additional challenges to the models as they need to generate larger outputs.</p>
<p>Output Length.One interesting phenomenon to analyze is the length distribution (in number of words) of the generated outputs.We expect that our models generate texts with similar output lengths as the reference texts.However, as shown in Figure 4c, the reference texts usually are bigger than the texts generated by all models.The texts generated by CGE-no-pl, a CGE model without length penalty, are consistently longer than the baseline.Also, note that we increase the length of the texts when we employ the length penalty (see Section 3.6).However, there is still a gap between the reference and the generated text lengths.We leave further investigation of this aspect for future work.</p>
<p>Effect of the Number of Nodes on the Output Length.Figure 5 shows the effect of the size of a graph, defined as the number of nodes, on the quality (measured in CHRF++ scores) and length of the generated text (in number of words) in the AGENDA dev set.We bin both the graph size and the output length in 4 classes.Our model consistently outperforms the baseline, in some cases by a large margin.When handling smaller graphs (with ≤ 35 nodes), both models have difficulties generating good summaries.However, for these smaller graphs, our model achieves a score 12.2% better when generating texts with length ≤ 75.Interestingly, when generating longer summaries (length &gt;140) from smaller graphs, our model outperforms the baseline by an impressive 21.7%, indicating that our model is more effective in capturing semantic signals from graphs with scarce information in order to generate better text.Our approach also performs better when the graph size is large (number of nodes &gt; 55) but the generation output is small (≤ 75), beating the baseline by 9 points.</p>
<p>Human Evaluation</p>
<p>To further assess the quality of the generated text, we conduct a human evaluation on the WebNLG test set with seen categories.Following previous works (Gardent et al., 2017;Castro Ferreira et al., 2019), we assess two quality criteria: (i) Fluency (i.e., does the text flow in a natural, easy to read manner?) and (ii) Adequacy (i.e., does the text clearly express the data?).We divide the datapoints into seven different sets by the number of triples.For each set, we randomly select 20 texts generated by Adapt, CGE-LG and their corresponding human reference text (420 texts in total).Since the number of datapoints for each set is not balanced (see Table 5), this sampling strategy assures us to have the same amount of samples for the different triple sets.Moreover, having human references may serve as an indicator of the sanity of the human evaluation experiment.We recruited human workers from Mechanical Turk to rate the text outputs on a 1-5 Likert scale.For each text, we collect scores from 4 workers and average them.Table 6 shows the results.We first note a similar trend as in the automatic evaluation, with CGE-LG outperforming Adapt on both fluency and adequacy.In sets with the number of triples smaller than 5, CGE-LG was the highest rated system in fluency.Similarly to the automatic evaluation, both systems are better in generating text from graphs with smaller diameters.Also note that bigger diameters pose difficulties to the models, which achieve their worst performance for diameters ≥ 3.</p>
<p>Conclusion</p>
<p>We introduced an unified graph attention network structure for investigating graph-to-text architectures that combined global and local graph representations in order to improve text generation.An extensive evaluation of our models demonstrated that global and local contexts are empirically complementary, and a combination can achieve stateof-the-art results on KG-to-text generation.In addition, cascaded architectures give better results compared with parallel architectures.To our knowledge, we are the first to consider both local and global aggregation in a graph attention network.</p>
<p>…</p>
<p>Figure 1: A graphical representation (a) of a scientific text (b).(c) A global encoder directly captures longer dependencies between any pair of nodes (blue and red arrows), but fails in capturing the graph structure.(d) A local encoder explicitly accesses information from the adjacent nodes (blue arrows) and implicitly captures distant information (dashed red arrows).</p>
<p>Figure 3 :
3
Figure 3: (a) Comparison between different encoder architectures with respect to (a) graph diameter and (b) number of triples, for dev set of AGENDA dataset.</p>
<p>Figure 4 :
4
Figure 4: CHRF++ scores for AGENDA dev set, with respect to (a) the number of nodes, and (b) the graph diameter.(c) Distribution of output length of the gold references and models' output for the AGENDA dev set.</p>
<p>Figure 5 :
5
Figure 5: Relation between the number of nodes and the length of the generated text, in number of words.</p>
<p>Table 1 :
1
Data statistics.Nodes and edges values are calculated after the graph transformation.Averages are computed per instance.
a)Contextualised Embeddingsb)Contextualised Embeddingsc)Contextualised Embeddingsd)Contextualised EmbeddingsN x Global Node Encoder LayerLocal Node Encoder Layer M xN x M xLocal Node Encoder Layer Node Embeddings Global Node Encoder LayerN x Global Node EncoderLocal Node EncoderN xLocal Node Encoder Global Node EncoderNode EmbeddingsNode EmbeddingsNode EmbeddingsNode EmbeddingsFigure 2: Overview of the proposed encoder architectures. First, architectures with complete separated parallel(a) and cascaded (b) global and local node encoders. c) Global and local node representations are concatenatedlayer-wise. d) Both node representations are cascaded layer-wise.train dev test relations avg entities avg nodes avg edges avg lengthAGENDA 38,720 1,000 1,000712.444.368.6140.3WebNLG 18,102 871 9713734.034.9101.024.2</p>
<p>Table 2 :
2
Results on AGENDA test set.#L and #H are the numbers of layers and the attention heads in each layer, respectively.When more than one, the values are for the global and local encoders, respectively.#P stands for the number of parameters in millions (node embeddings included).
Model#L #HBLEUMETEORCHRF++#PKoncel-Kedziorski et al. (2019)6814.30 ±1.01 18.80 ±0.28--Baseline6814.11 ±0.28 19.35 ±0.52 41.95 ±0.39 54.4PGE-LW68, 4 17.40 ±0.08 22.06 ±0.09 46.19 ±0.16 67.7CGE-LW68, 8 17.44 ±0.10 22.02 ±0.13 46.24 ±0.14 76.4PGE6, 3 8, 8 17.17 ±0.38 21.70 ±0.25 45.75 ±0.43 67.4CGE6, 3 8, 8 17.81 ±0.15 21.75 ±0.55 46.76 ±0.12 66.9</p>
<p>Table 2
2presents the results. We report the num-ber of layers and attention heads employed bythe models. For a fair comparison, we use thesame number of global layers and attention headsamong different models. Our approaches substan-tially outperform the transformer baseline. CGE,our best model, outperforms Koncel-Kedziorskiet al. (2019), a graph transformer model thatonly allows information exchange between adja-cent nodes, by a large margin, achieving a BLEUscore of 17.81, 24.5% higher. Those results indi-cate that combining the local node context, lever-aging the graph topology, and the global node con-text, capturing macro-level node relations, leads tobetter node embeddings for text generation. Themodels based on layer-wise encoding have similarresults with CGE-LW achieving the best METEORscore. PGE has the worse performance amongthe proposed models. Even though CGE has thesmallest number of parameters, it achieves the bet-ter performance in terms of BLEU and CHRF++scores.</p>
<p>Table 3 :
3
., Results on WebNLG test set with seen categories.2019).Three systems are the best competitors in the challenge for seen categories: UPF-FORGe, Melbourne and Adapt.UPF-FORGe follows a rule-based approach, whereas Melbourne and Adapt employ encoder-decoder models with linearized triple sets.Table3presents the results.
ModelBLEUMETEOR CHRF++#PUPF-FORGe40.8840.00--Melbourne54.5241.0070.72-Adapt60.5944.0076.01-Marcheggiani and Perez (2018) 55.9039.00-4.9Trisedya et al. (2018)58.6040.60--Castro et al. (2019)57.2041.00--CGE-RP62.30 ±0.27 43.51 ±0.18 75.49 ±0.34 13.9CGE-LG63.10 ±0.13 44.11 ±0.09 76.33 ±0.10 12.8
Relations as Parameters.CGE-RP encodes relations as model parameters and achieves a BLEU score of 62.30, 8.9% better than the best model of Castro Ferreira et al. (2019), who employ an endto-end architecture based on GRUs.CGE-RP also outperform Trisedya et al. (</p>
<p>Table 4 :
4
Ablation study for modules used in the encoder and decoder of the CGE model.
ModelBLEU CHRF++ #PCGE17.2545.6166.9Global Encoder-Global Attention 15.4843.8963.8-FFN16.4844.8554.3-Global Encoder14.9643.1848.0Local Encoder-Graph Attention16.4445.5466.9-Weight Relations 16.7745.5156.7-GRU16.1944.5465.4-Local Encoder14.4342.4354.4-Shared Vocab.15.5244.0286.7Decoder-Length Penalty16.5044.6166.9
by a large margin of 7.2 BLEU points, showing our graph encoding strategies lead to a better text generation.We also outperform Adapt, a strong competitor that employs subword encodings, by 2.51 BLEU points.</p>
<p>Table 6 :
6
All 3.96 C 4.44 C 4.12 B 4.54 B 4.24 A 4.63 A 1-2 3.94 C 4.59 B 4.18 B 4.72 A 4.30 A 4.69 A 3-4 3.79 C 4.45 B 3.96 B 4.50 AB 4.14 A 4.66 A 5-7 4.08 B 4.35 B 4.18 B 4.45 B 4.28 A 4.59 A 3.98 C 4.50 B 4.16 B 4.61 A 4.28 A 4.66 A ≥ 3 3.91 C 4.33 B 4.03 B 4.43 B 4.17A 4.60 A Fluency (F) and Adequacy (A) obtained in the human evaluation.#T refers to the number of input triples and #D to graph diameters.The ranking was determined by pair-wise Mann-Whitney tests with p &lt; 0.05, and the difference between systems which have a letter in common is not statistically significant.</p>
<h1>TAdaptCGE-LGReferenceFAFAFA#DAdaptCGE-LGReferenceFAFAFA1-2</h1>
<p>In this paper, multi-relational graphs refer to directed graphs with labelled edges.
R contains relations both in canonical direction (e.g. used-for) and in inverse direction (e.g. used-for-inv).
As shown on Figure4c, 83% of the gold abstracts have more than 100 words.
AcknowledgmentsThis work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1.
Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Graph transformer for graph-to-sequence learning. Deng Cai, Wai Lam, Proceedings of The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)2020</p>
<p>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Thiago Castro Ferreira, Chris Van Der Lee, Emiel Van Miltenburg, Emiel Krahmer, 10.18653/v1/D19-1052Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 10.3115/v1/D14-1179Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, Qatar2014Association for Computational Linguistics</p>
<p>Structural neural encoders for AMR-to-text generation. Marco Damonte, Shay B Cohen, 10.18653/v1/N19-1366Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, Minnesota20191Association for Computational Linguistics</p>
<p>Understanding the representation power of graph neural networks in learning graph topology. Nima Dehmamy, Albert-Laszlo Barabasi, Rose Yu, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Meteor universal: Language specific translation evaluation for any target language. Michael Denkowski, Alon Lavie, 10.3115/v1/W14-3348Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationBaltimore, Maryland, USAAssociation for Computational Linguistics2014</p>
<p>Generation from abstract meaning representation using tree transducers. Jeffrey Flanigan, Chris Dyer, Noah A Smith, Jaime Carbonell, 10.18653/v1/N16-1087Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational Linguistics2016</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, Spain2017Association for Computational Linguistics</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Yan Zhang, Zhiyang Teng, Wei Lu, 10.1162/tacl_a_00269Transactions of the Association for Computational Linguistics. 72019</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics2019</p>
<p>Neural amr: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Inducing document plans for concept-to-text generation. Ioannis Konstas, Mirella Lapata, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USA2013Association for Computational Linguistics</p>
<p>Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. Q Li, Z Han, X.-M Wu, The Thirty-Second AAAI Conference on Artificial Intelligence. AAAI2018</p>
<p>Gated graph sequence neural networks. Yujia Li, Richard Zemel, Marc Brockschmidt, Daniel Tarlow, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)San Juan, Puerto Rico2016</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez Beltrachini, Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational Linguistics2018</p>
<p>Step-by-step: Separating planning from realization in neural data-to-text generation. Amit Moryossef, Yoav Goldberg, Ido Dagan, 10.18653/v1/N19-1236Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, Minnesota20191Association for Computational Linguistics</p>
<p>Bleu: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02Stroudsburg, PA, USAAssociation for Computational Linguistics2002</p>
<p>chrF: character n-gram fscore for automatic MT evaluation. Maja Popović, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, Portugal2015Association for Computational Linguistics</p>
<p>Generating English from abstract meaning representations. 10.18653/v1/W16-6603Proceedings of the 9th International Natural Language Generation conference. Nima Pourdamghani, Kevin Knight, Ulf Hermjakob, the 9th International Natural Language Generation conferenceEdinburgh, UKAssociation for Computational Linguistics2016</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Modeling relational data with graph convolutional networks. Sejr Michael, Thomas N Schlichtkrull, Peter Kipf, Rianne Bloem, Van Den, Ivan Berg, Max Titov, Welling, 10.1007/978-3-319-93417-4_38The Semantic Web -15th International Conference, ESWC 2018. Heraklion, Crete, Greece2018. June 3-7, 2018</p>
<p>AMRto-text generation with synchronous node replacement grammar. Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P17-2002Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20172Short Papers)</p>
<p>A graph-to-sequence model for AMR-to-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia20181Association for Computational Linguistics</p>
<p>GTR-LSTM: A triple encoder for sentence generation from RDF data. Jianzhong Bayu Distiawan Trisedya, Rui Qi, Wei Zhang, Wang, 10.18653/v1/P18-1151Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30. I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc2017</p>
<p>Graph Attention Networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, International Conference on Learning Representations. Vancouver, Canada2018</p>
<p>Google's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, CoRR, abs/1609.08144Oriol Vinyals. 2016Greg Corrado, Macduff Hughes, and Jeffrey Dean</p>
<p>Representation learning on graphs with jumping knowledge networks. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken Ichi Kawarabayashi, Stefanie Jegelka, ICML. 2018</p>
<p>Modeling graph structure in transformer for better AMR-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, 10.18653/v1/D19-1548Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>
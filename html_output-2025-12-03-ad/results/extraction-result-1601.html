<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1601 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1601</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1601</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-9ea792e78a07a30d9b2c80669e074e05dc42b7ee</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9ea792e78a07a30d9b2c80669e074e05dc42b7ee" target="_blank">Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The results suggest that combining intra-episode and across-training exploration bonuses with learning progress creates a promising method for automated curriculum generation, which may substantially increase the ability to train more capable, generally intelligent agents.</p>
                <p><strong>Paper Abstract:</strong> An important challenge in reinforcement learning is training agents that can solve a wide variety of tasks. If tasks depend on each other (e.g. needing to learn to walk before learning to run), curriculum learning can speed up learning by focusing on the next best task to learn. We explore curriculum learning in a complex, visual domain with many hard exploration challenges: Minecraft. We find that learning progress (defined as a change in success probability of a task) is a reliable measure of learnability for automatically constructing an effective curriculum. We introduce a learning-progress based curriculum and test it on a complex reinforcement learning problem (called"Simon Says") where an agent is instructed to obtain a desired goal item. Many of the required skills depend on each other. Experiments demonstrate that: (1) a within-episode exploration bonus for obtaining new items improves performance, (2) dynamically adjusting this bonus across training such that it only applies to items the agent cannot reliably obtain yet further increases performance, (3) the learning-progress based curriculum elegantly follows the learning curve of the agent, and (4) when the learning-progress based curriculum is combined with the dynamic exploration bonus it learns much more efficiently and obtains far higher performance than uniform baselines. These results suggest that combining intra-episode and across-training exploration bonuses with learning progress creates a promising method for automated curriculum generation, which may substantially increase our ability to train more capable, generally intelligent agents.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1601.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1601.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uniform baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform task sampling without exploration bonus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline training that samples goals uniformly from the 107-item 'Simon Says' set and uses only the goal-conditioned +1 reward on success (no exploration bonus or curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with IMPALA-style convnet + LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy and value share an enlarged IMPALA convnet (conv modules with 64,128,128 filters), followed by FC512, summed feature embeddings, LSTM, and FC512 heads; trained with PPO + GAE. Visual input 64x64 RGB; features include inventory, health, items in hands; goal provided as one-hot.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Minecraft (MineRL) 'Simon Says'</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3D visual Minecraft environment based on MineRL. Agent observes 64x64 RGB frames and structured game-state features (inventory, health, food, etc.). Each episode consists of sequential goal tasks: a one-hot goal id specifies which of 107 items to obtain within up to 1500 steps; after success/failure a new goal is selected without resetting the world. Actions include movement, attack, use, craft/equip/place helper actions, and camera control; frame-skip=4.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>procedural item-collection tasks (Minecraft tech-tree)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Collecting 'dirt', 'sapling', crafting stone tools, obtaining 'iron ingot', 'redstone' items, and deep items like 'diamond'.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Hierarchical tech-tree: many items require prerequisite items and multi-step sequences (mining → crafting → combining), forming compositional and prerequisite dependencies across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>random / uniform</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>107 tasks ranging from surface (single-step) items to deep tech-tree multi-step items requiring mining and multi-step crafting (e.g., diamond).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Discovered 17/107 items (defined as success probability > 5%) at convergence; learning plateaus early (Figure 1 baseline red).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Uniform sampling without exploration or curriculum performs poorly on the hard, compositional Simon Says set: only 17 items discovered and learning plateaus, demonstrating need for exploration incentives or curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1601.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1601.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static exploration bonus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Within-episode exploration bonus (static) a.k.a. Curiosity-Search style</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intra-episode exploration bonus that rewards the first few collections of each distinct item within an episode (reward decays multiplicatively per repeated collection), added to the main goal reward with a fixed coefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with IMPALA-style convnet + LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same policy architecture as baseline (enlarged IMPALA convnet + FC + LSTM), trained with PPO + GAE; exploration bonus added as auxiliary reward signal.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Minecraft (MineRL) 'Simon Says'</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>As above: 64x64 RGB + structured features; exploration bonus rewards item collections regardless of current goal, with per-item reward 0.5^N for N-th collection and constraints preventing exploit (only rewarded if N > maximum inventory count earlier in episode).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>procedural item-collection tasks (within-episode exploratory behaviors)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Encourages collecting diverse new items in an episode (e.g., gather surface items, mine stone, craft stone tools).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Encourages within-episode breadth exploration which can expose prerequisite items enabling downstream compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>static exploration bonus</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Exploration bonus applied to all 107 items across training (no across-training removal). Coefficient tuned (0.05 for uniform sampling experiments). Incentivizes intra-episode diversity similar to Curiosity Search.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>not ordered (uniform sampling of tasks) with an added within-episode intrinsic reward for novel item collections</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same 107 items; bonus affects exploration of both simple and complex items but can distract from hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Uniform sampling + static exploration bonus: discovered 43/107 items (success prob > 5%) at convergence (Figure 1 dotted green).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Compared against uniform no-bonus baseline: 43 vs 17 discovered items (improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A within-episode exploration bonus substantially increases the number of discovered items (17 → 43), but when static (applied to all items) it can distract the agent away from learning hard, deep-tech tasks because easy items remain lucrative to collect each episode.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1601.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1601.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic exploration bonus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum-based dynamic exploration bonus (across-training diversity pressure)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An across-training mechanism that rewards item collections only for items whose conditional success probability is below 0.1, maintained via EMA; items are removed from the exploration-reward set once reliably obtained and re-added if performance drops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with IMPALA-style convnet + LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture; exploration bonus included but dynamically gated per-item based on EMA-tracked conditional success probability (< 0.1 inclusion threshold). Exploration bonus coefficient raised to 0.5 when dynamic gating used.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Minecraft (MineRL) 'Simon Says'</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same as above. EMA used to track per-item conditional success probabilities; exploration set contains items with success probability < 0.1; items can be removed/added across training.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>procedural item-collection tasks with across-episode exploration incentives</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Promotes learning of hard-to-obtain items (iron, redstone, gold, diamond branches) by rewarding only items not yet reliably obtained.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Targets items low in success probability that often act as prerequisites for deeper tasks, thus encouraging acquisition of prerequisite items across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>dynamic exploration bonus (across-training gating)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Across-training curriculum that removes items from intra-episode exploration reward once EMA-tracked conditional success probability > 0.1; thereby shifts exploration pressure toward items the agent cannot reliably obtain and reduces distraction from easy tasks. EMA timescale used for success estimates (per A.4: LP EMA timescale = 1250 optimization steps).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>adaptive by current performance (success-probability threshold gating of exploration bonus)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Applied over the same 107-item range; preferentially focuses exploration on low-success (harder) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Uniform sampling + dynamic exploration bonus: discovered ~70/107 items (Figure 1 dashed yellow).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Compared to uniform + static exploration (43) and uniform no-bonus (17).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynamically removing already-learned items from the exploration bonus prevents the exploration reward from distracting the agent and substantially increases discovered items (43 → ~70), acting as an across-training diversity pressure similar to novelty-search/count-based intrinsic motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1601.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1601.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unidirectional LP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unidirectional learning-progress curriculum (increase-only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Curriculum that samples tasks based on positive learning progress (increases in success probability) measured as difference between fast and slow EMA success estimates, reweighted toward low success probabilities; only tracks increasing progress (rectified).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with IMPALA-style convnet + LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture; curriculum selects goals based on per-task unidirectional learning progress computed from p_fast and p_slow EMAs (EMA timescale reported: 1250 optimization steps for LP in A.5), with reweighting function f(p) (p_theta=0.1) and sampling that focuses 90% weight on top ~20% by reweighted LP.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Minecraft (MineRL) 'Simon Says'</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>As above. Curriculum influences which goal/task is sampled each rollout based on inferred learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>procedural item-collection tasks (goal-conditioned association learning + unconditional discovery aided by exploration bonus)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Same 107 items; curriculum accelerates learning associations between one-hot goal label and item collection for items showing positive progress.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Exploits prerequisite structure by intensifying sampling of tasks currently exhibiting positive gains, which can increase availability of prerequisites for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>unidirectional learning-progress curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Compute p_fast and p_slow via EMAs; LP = max(0, f(p_fast)-f(p_slow)) where f reweights toward low p (p_theta=0.1). Z-score LPs, sigmoid centered on 90%-quantile to focus sampling, then normalize; 90% of sampling weight placed on roughly top 20% tasks. Only uses increases in success probability (does not respond to decreases).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>adaptive by recent increases in per-task success probability (learning progress), reweighted toward low success tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same 107 items, from surface to deep-tech items.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Unidirectional LP + dynamic exploration bonus: discovered 79/107 items (success prob > 5%)—almost as many as bidirectional but training shows cycles of forgetting (Figure 1 light solid blue).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Compared to bidirectional LP (82), dynamic exploration alone (~70), static exploration (43), uniform (17).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared direct to bidirectional LP: unidirectional discovers 79 vs bidirectional 82 but exhibits catastrophic-forgetting cycles due to not resampling tasks whose performance declines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Unidirectional LP is effective at discovering many tasks (79) but unstable: it fails to respond to performance drops and thus undergoes cycles of catastrophic forgetting and rediscovery; much of the skill remains latent enabling rapid recovery when resampled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1601.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1601.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bidirectional LP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional learning-progress curriculum (increase+decrease)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Curriculum that samples tasks by absolute learning progress (both increasing and decreasing success probability) measured via difference between fast and slow EMAs, reweighted toward low success probabilities and focused sampling on the top LP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with IMPALA-style convnet + LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture; curriculum uses absolute difference |f(p_fast)-f(p_slow)| with reweighting f(p) (p_theta=0.1), EMA timescales, z-scoring and sigmoid-based sampling (focus 90% weight on top ~20%). Tracks both increases and decreases in success prob to resample tasks at risk of forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Minecraft (MineRL) 'Simon Says'</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>As above. Curriculum adaptively chooses which goal to present based on reweighted, bidirectional learning progress estimates per task.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>procedural item-collection tasks with hierarchical prerequisites</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Obtaining items across the tech tree including surface items, stone/coal/iron branches, redstone/gold/diamond items; agent solved many dependent items requiring prerequisite acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Explicit prerequisite hierarchy: items deeper in tech tree require earlier items; curriculum focuses sampling on frontier of learnable tasks and resamples tasks whose performance declines to prevent forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>bidirectional learning-progress curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Compute fast and slow EMA success probabilities per task; apply reweighting f(p) that magnifies low p (p_theta=0.1); LP = absolute difference between reweighted fast and slow estimates. Convert LP to sampling weight by z-scoring and applying a sigmoid centered near the 90%-quantile; concentrate ~90% weight on top ~20% LP tasks. Bidirectional LP increases sampling for tasks showing either upward or downward changes, enabling remediation of forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>adaptive by magnitude of recent change in task success probability (learning progress), reweighted to prioritize low-success tasks and both gains and declines</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>107 tasks from trivial surface items to complex deep-tech items; curriculum tracks learning frontier as agent acquires prerequisites enabling deeper tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Bidirectional LP + dynamic exploration bonus: discovered 82/107 items (success prob > 5%) at convergence (best treatment; Figure 1 dark dotted blue). Agent obtained 82 items with >5% success, 4 items with non-zero <5%, 27 items zero success.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Compared to uniform + dynamic exploration (~70), uniform + static (~43), uniform no-bonus (17).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared to unidirectional LP (79 but unstable), dynamic exploration alone (~70), static (~43), uniform (~17); bidirectional LP yields highest discovered-item count and mitigates catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bidirectional LP best balances accelerating discovery of new tasks and preventing catastrophic forgetting by sampling tasks whose success probabilities change in either direction; reweighting toward low success probabilities and concentrating sampling on top LP tasks steers training to the frontier of currently learnable tasks and yields the highest number of discovered items (82/107). The method is computationally efficient compared to naive i.i.d. resampling of all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime. <em>(Rating: 2)</em></li>
                <li>Deep curiosity search: Intra-life exploration improves performance on challenging deep reinforcement learning problems. <em>(Rating: 2)</em></li>
                <li>Abandoning objectives: Evolution through the search for novelty alone. <em>(Rating: 2)</em></li>
                <li>Unifying count-based exploration and intrinsic motivation. <em>(Rating: 2)</em></li>
                <li>Automated curriculum learning for neural networks. <em>(Rating: 2)</em></li>
                <li>Teacher-student curriculum learning. <em>(Rating: 1)</em></li>
                <li>Automated goal generation for reinforcement learning agents. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1601",
    "paper_id": "paper-9ea792e78a07a30d9b2c80669e074e05dc42b7ee",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "Uniform baseline",
            "name_full": "Uniform task sampling without exploration bonus",
            "brief_description": "Baseline training that samples goals uniformly from the 107-item 'Simon Says' set and uses only the goal-conditioned +1 reward on success (no exploration bonus or curriculum).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with IMPALA-style convnet + LSTM",
            "agent_description": "Policy and value share an enlarged IMPALA convnet (conv modules with 64,128,128 filters), followed by FC512, summed feature embeddings, LSTM, and FC512 heads; trained with PPO + GAE. Visual input 64x64 RGB; features include inventory, health, items in hands; goal provided as one-hot.",
            "agent_size": null,
            "environment_name": "Minecraft (MineRL) 'Simon Says'",
            "environment_description": "3D visual Minecraft environment based on MineRL. Agent observes 64x64 RGB frames and structured game-state features (inventory, health, food, etc.). Each episode consists of sequential goal tasks: a one-hot goal id specifies which of 107 items to obtain within up to 1500 steps; after success/failure a new goal is selected without resetting the world. Actions include movement, attack, use, craft/equip/place helper actions, and camera control; frame-skip=4.",
            "procedure_type": "procedural item-collection tasks (Minecraft tech-tree)",
            "procedure_examples": "Collecting 'dirt', 'sapling', crafting stone tools, obtaining 'iron ingot', 'redstone' items, and deep items like 'diamond'.",
            "compositional_structure": "Hierarchical tech-tree: many items require prerequisite items and multi-step sequences (mining → crafting → combining), forming compositional and prerequisite dependencies across tasks.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": "random / uniform",
            "task_complexity_range": "107 tasks ranging from surface (single-step) items to deep tech-tree multi-step items requiring mining and multi-step crafting (e.g., diamond).",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "Discovered 17/107 items (defined as success probability &gt; 5%) at convergence; learning plateaus early (Figure 1 baseline red).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "Uniform sampling without exploration or curriculum performs poorly on the hard, compositional Simon Says set: only 17 items discovered and learning plateaus, demonstrating need for exploration incentives or curricula.",
            "uuid": "e1601.0",
            "source_info": {
                "paper_title": "Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Static exploration bonus",
            "name_full": "Within-episode exploration bonus (static) a.k.a. Curiosity-Search style",
            "brief_description": "An intra-episode exploration bonus that rewards the first few collections of each distinct item within an episode (reward decays multiplicatively per repeated collection), added to the main goal reward with a fixed coefficient.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with IMPALA-style convnet + LSTM",
            "agent_description": "Same policy architecture as baseline (enlarged IMPALA convnet + FC + LSTM), trained with PPO + GAE; exploration bonus added as auxiliary reward signal.",
            "agent_size": null,
            "environment_name": "Minecraft (MineRL) 'Simon Says'",
            "environment_description": "As above: 64x64 RGB + structured features; exploration bonus rewards item collections regardless of current goal, with per-item reward 0.5^N for N-th collection and constraints preventing exploit (only rewarded if N &gt; maximum inventory count earlier in episode).",
            "procedure_type": "procedural item-collection tasks (within-episode exploratory behaviors)",
            "procedure_examples": "Encourages collecting diverse new items in an episode (e.g., gather surface items, mine stone, craft stone tools).",
            "compositional_structure": "Encourages within-episode breadth exploration which can expose prerequisite items enabling downstream compositional tasks.",
            "uses_curriculum": false,
            "curriculum_name": "static exploration bonus",
            "curriculum_description": "Exploration bonus applied to all 107 items across training (no across-training removal). Coefficient tuned (0.05 for uniform sampling experiments). Incentivizes intra-episode diversity similar to Curiosity Search.",
            "curriculum_ordering_principle": "not ordered (uniform sampling of tasks) with an added within-episode intrinsic reward for novel item collections",
            "task_complexity_range": "Same 107 items; bonus affects exploration of both simple and complex items but can distract from hard tasks.",
            "performance_with_curriculum": "Uniform sampling + static exploration bonus: discovered 43/107 items (success prob &gt; 5%) at convergence (Figure 1 dotted green).",
            "performance_without_curriculum": "Compared against uniform no-bonus baseline: 43 vs 17 discovered items (improvement).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "A within-episode exploration bonus substantially increases the number of discovered items (17 → 43), but when static (applied to all items) it can distract the agent away from learning hard, deep-tech tasks because easy items remain lucrative to collect each episode.",
            "uuid": "e1601.1",
            "source_info": {
                "paper_title": "Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Dynamic exploration bonus",
            "name_full": "Curriculum-based dynamic exploration bonus (across-training diversity pressure)",
            "brief_description": "An across-training mechanism that rewards item collections only for items whose conditional success probability is below 0.1, maintained via EMA; items are removed from the exploration-reward set once reliably obtained and re-added if performance drops.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with IMPALA-style convnet + LSTM",
            "agent_description": "Same architecture; exploration bonus included but dynamically gated per-item based on EMA-tracked conditional success probability (&lt; 0.1 inclusion threshold). Exploration bonus coefficient raised to 0.5 when dynamic gating used.",
            "agent_size": null,
            "environment_name": "Minecraft (MineRL) 'Simon Says'",
            "environment_description": "Same as above. EMA used to track per-item conditional success probabilities; exploration set contains items with success probability &lt; 0.1; items can be removed/added across training.",
            "procedure_type": "procedural item-collection tasks with across-episode exploration incentives",
            "procedure_examples": "Promotes learning of hard-to-obtain items (iron, redstone, gold, diamond branches) by rewarding only items not yet reliably obtained.",
            "compositional_structure": "Targets items low in success probability that often act as prerequisites for deeper tasks, thus encouraging acquisition of prerequisite items across episodes.",
            "uses_curriculum": true,
            "curriculum_name": "dynamic exploration bonus (across-training gating)",
            "curriculum_description": "Across-training curriculum that removes items from intra-episode exploration reward once EMA-tracked conditional success probability &gt; 0.1; thereby shifts exploration pressure toward items the agent cannot reliably obtain and reduces distraction from easy tasks. EMA timescale used for success estimates (per A.4: LP EMA timescale = 1250 optimization steps).",
            "curriculum_ordering_principle": "adaptive by current performance (success-probability threshold gating of exploration bonus)",
            "task_complexity_range": "Applied over the same 107-item range; preferentially focuses exploration on low-success (harder) tasks.",
            "performance_with_curriculum": "Uniform sampling + dynamic exploration bonus: discovered ~70/107 items (Figure 1 dashed yellow).",
            "performance_without_curriculum": "Compared to uniform + static exploration (43) and uniform no-bonus (17).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "Dynamically removing already-learned items from the exploration bonus prevents the exploration reward from distracting the agent and substantially increases discovered items (43 → ~70), acting as an across-training diversity pressure similar to novelty-search/count-based intrinsic motivation.",
            "uuid": "e1601.2",
            "source_info": {
                "paper_title": "Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Unidirectional LP",
            "name_full": "Unidirectional learning-progress curriculum (increase-only)",
            "brief_description": "Curriculum that samples tasks based on positive learning progress (increases in success probability) measured as difference between fast and slow EMA success estimates, reweighted toward low success probabilities; only tracks increasing progress (rectified).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with IMPALA-style convnet + LSTM",
            "agent_description": "Same architecture; curriculum selects goals based on per-task unidirectional learning progress computed from p_fast and p_slow EMAs (EMA timescale reported: 1250 optimization steps for LP in A.5), with reweighting function f(p) (p_theta=0.1) and sampling that focuses 90% weight on top ~20% by reweighted LP.",
            "agent_size": null,
            "environment_name": "Minecraft (MineRL) 'Simon Says'",
            "environment_description": "As above. Curriculum influences which goal/task is sampled each rollout based on inferred learning progress.",
            "procedure_type": "procedural item-collection tasks (goal-conditioned association learning + unconditional discovery aided by exploration bonus)",
            "procedure_examples": "Same 107 items; curriculum accelerates learning associations between one-hot goal label and item collection for items showing positive progress.",
            "compositional_structure": "Exploits prerequisite structure by intensifying sampling of tasks currently exhibiting positive gains, which can increase availability of prerequisites for downstream tasks.",
            "uses_curriculum": true,
            "curriculum_name": "unidirectional learning-progress curriculum",
            "curriculum_description": "Compute p_fast and p_slow via EMAs; LP = max(0, f(p_fast)-f(p_slow)) where f reweights toward low p (p_theta=0.1). Z-score LPs, sigmoid centered on 90%-quantile to focus sampling, then normalize; 90% of sampling weight placed on roughly top 20% tasks. Only uses increases in success probability (does not respond to decreases).",
            "curriculum_ordering_principle": "adaptive by recent increases in per-task success probability (learning progress), reweighted toward low success tasks",
            "task_complexity_range": "Same 107 items, from surface to deep-tech items.",
            "performance_with_curriculum": "Unidirectional LP + dynamic exploration bonus: discovered 79/107 items (success prob &gt; 5%)—almost as many as bidirectional but training shows cycles of forgetting (Figure 1 light solid blue).",
            "performance_without_curriculum": "Compared to bidirectional LP (82), dynamic exploration alone (~70), static exploration (43), uniform (17).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared direct to bidirectional LP: unidirectional discovers 79 vs bidirectional 82 but exhibits catastrophic-forgetting cycles due to not resampling tasks whose performance declines.",
            "transfer_generalization": null,
            "key_findings": "Unidirectional LP is effective at discovering many tasks (79) but unstable: it fails to respond to performance drops and thus undergoes cycles of catastrophic forgetting and rediscovery; much of the skill remains latent enabling rapid recovery when resampled.",
            "uuid": "e1601.3",
            "source_info": {
                "paper_title": "Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Bidirectional LP",
            "name_full": "Bidirectional learning-progress curriculum (increase+decrease)",
            "brief_description": "Curriculum that samples tasks by absolute learning progress (both increasing and decreasing success probability) measured via difference between fast and slow EMAs, reweighted toward low success probabilities and focused sampling on the top LP tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with IMPALA-style convnet + LSTM",
            "agent_description": "Same architecture; curriculum uses absolute difference |f(p_fast)-f(p_slow)| with reweighting f(p) (p_theta=0.1), EMA timescales, z-scoring and sigmoid-based sampling (focus 90% weight on top ~20%). Tracks both increases and decreases in success prob to resample tasks at risk of forgetting.",
            "agent_size": null,
            "environment_name": "Minecraft (MineRL) 'Simon Says'",
            "environment_description": "As above. Curriculum adaptively chooses which goal to present based on reweighted, bidirectional learning progress estimates per task.",
            "procedure_type": "procedural item-collection tasks with hierarchical prerequisites",
            "procedure_examples": "Obtaining items across the tech tree including surface items, stone/coal/iron branches, redstone/gold/diamond items; agent solved many dependent items requiring prerequisite acquisition.",
            "compositional_structure": "Explicit prerequisite hierarchy: items deeper in tech tree require earlier items; curriculum focuses sampling on frontier of learnable tasks and resamples tasks whose performance declines to prevent forgetting.",
            "uses_curriculum": true,
            "curriculum_name": "bidirectional learning-progress curriculum",
            "curriculum_description": "Compute fast and slow EMA success probabilities per task; apply reweighting f(p) that magnifies low p (p_theta=0.1); LP = absolute difference between reweighted fast and slow estimates. Convert LP to sampling weight by z-scoring and applying a sigmoid centered near the 90%-quantile; concentrate ~90% weight on top ~20% LP tasks. Bidirectional LP increases sampling for tasks showing either upward or downward changes, enabling remediation of forgetting.",
            "curriculum_ordering_principle": "adaptive by magnitude of recent change in task success probability (learning progress), reweighted to prioritize low-success tasks and both gains and declines",
            "task_complexity_range": "107 tasks from trivial surface items to complex deep-tech items; curriculum tracks learning frontier as agent acquires prerequisites enabling deeper tasks.",
            "performance_with_curriculum": "Bidirectional LP + dynamic exploration bonus: discovered 82/107 items (success prob &gt; 5%) at convergence (best treatment; Figure 1 dark dotted blue). Agent obtained 82 items with &gt;5% success, 4 items with non-zero &lt;5%, 27 items zero success.",
            "performance_without_curriculum": "Compared to uniform + dynamic exploration (~70), uniform + static (~43), uniform no-bonus (17).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared to unidirectional LP (79 but unstable), dynamic exploration alone (~70), static (~43), uniform (~17); bidirectional LP yields highest discovered-item count and mitigates catastrophic forgetting.",
            "transfer_generalization": null,
            "key_findings": "Bidirectional LP best balances accelerating discovery of new tasks and preventing catastrophic forgetting by sampling tasks whose success probabilities change in either direction; reweighting toward low success probabilities and concentrating sampling on top LP tasks steers training to the frontier of currently learnable tasks and yields the highest number of discovered items (82/107). The method is computationally efficient compared to naive i.i.d. resampling of all tasks.",
            "uuid": "e1601.4",
            "source_info": {
                "paper_title": "Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime.",
            "rating": 2
        },
        {
            "paper_title": "Deep curiosity search: Intra-life exploration improves performance on challenging deep reinforcement learning problems.",
            "rating": 2
        },
        {
            "paper_title": "Abandoning objectives: Evolution through the search for novelty alone.",
            "rating": 2
        },
        {
            "paper_title": "Unifying count-based exploration and intrinsic motivation.",
            "rating": 2
        },
        {
            "paper_title": "Automated curriculum learning for neural networks.",
            "rating": 2
        },
        {
            "paper_title": "Teacher-student curriculum learning.",
            "rating": 1
        },
        {
            "paper_title": "Automated goal generation for reinforcement learning agents.",
            "rating": 1
        }
    ],
    "cost": 0.013957,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft</h1>
<p>Ingmar Kanitscheider, ${ }^{* \text {, }}$ Joost Huizinga, ${ }_{\text {i }}$ David Farhi, William Hebgen Guss, Brandon Houghton, Raul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, Oleg Klimov, Jeff Clune ${ }^{\dagger}$</p>
<p>OpenAI</p>
<h4>Abstract</h4>
<p>An important challenge in reinforcement learning is training agents that can solve a wide variety of tasks. If tasks depend on each other (e.g. needing to learn to walk before learning to run), curriculum learning can speed up learning by focusing on the next best task to learn. We explore curriculum learning in a complex, visual domain with many hard exploration challenges: Minecraft. We find that learning progress (defined as a change in success probability of a task) is a reliable measure of learnability for automatically constructing an effective curriculum. We introduce a learning-progress based curriculum and test it on a complex reinforcement learning problem (called "Simon Says") where an agent is instructed to obtain a desired goal item. Many of the required skills depend on each other. Experiments demonstrate that: (1) a within-episode exploration bonus for obtaining new items improves performance, (2) dynamically adjusting this bonus across training such that it only applies to items the agent cannot reliably obtain yet further increases performance, (3) the learning-progress based curriculum elegantly follows the learning curve of the agent, and (4) when the learning-progress based curriculum is combined with the dynamic exploration bonus it learns much more efficiently and obtains far higher performance than uniform baselines. These results suggest that combining intra-episode and across-training exploration bonuses with learning progress creates a promising method for automated curriculum generation, which may substantially increase our ability to train more capable, generally intelligent agents. ${ }^{\ddagger}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>An important challenge in reinforcement learning (RL) is to train agents that can solve a wide variety of tasks. Tasks often vary in difficulty and depend on each other, such that learning easier tasks first may help with learning more difficult tasks later. Just as human children crawl before they can walk, and walk before they can run, interesting use cases for RL agents have difficulty hierarchies we would like to exploit: Learning first how to write syntactically correct code might help an agent to later learn how to fix semantic errors in a program. Learning first how to solve high school physics problems likely helps learn college-level physics. Such dependencies particularly exist in RL, where the problem of exploration may prevent a policy from experiencing any useful gradient on a hard task before it has mastered a prerequisite task. If the goal is to pass a set of unit tests in a programming domain, an agent will not get any useful feedback if it always fails all unit tests because it is unable to write syntactically correct code. Side-stepping exploration by collecting human demonstrations can be an option, but in many domains of interest it might be difficult or impossible to collect enough high-quality data to reach expert-human-level performance. Furthermore, if the model relies purely on imitating humans its maximum performance will be limited by the best demonstrations in our data set, and even the combination of all the best demonstrations that humanity has to offer probably will not move the model far beyond human performance.</p>
<p>In the following we assume that we're given a collection of RL tasks, each of which consists of a reward function (that defines the goal of the task) and an environment distribution. A naive solution to learning all tasks would be to attempt to learn all tasks simultaneously by uniformly sampling tasks irrespective of the dependencies between them. This solution ensures that the agent experiences a useful gradient as long as some of the tasks are learnable given the agent’s current skill level. However, if the fraction of learnable tasks at any moment is very small, it is computationally very inefficient, as the agent spends a lot of time on tasks where it does not actually learn anything.</p>
<p>A more efficient alternative is to build a curriculum that narrows the distribution of tasks being trained on to those that are currently learnable. Which tasks are learnable at a given point in time, or in what order tasks are most easily learnable, is typically not known in advance. As such, the present paper explores methods for how we can infer learnability on the fly and build an automatic curriculum over tasks. In particular, our goal was to, given a set of tasks of varying difficulty, learn as many tasks as fast and compute-efficient as possible.</p>
<p>This project focused specifically on tasks with identical environment distributions, but different reward functions. Having tasks with different goals but the same environment distribution is a natural setting for the powerful models we wish to create, as the real world presents a diverse but fixed environment distribution in which we would want the model to perform many different tasks. The same is true for our current generation of models, which generally have to learn many different tasks (e.g. write different programs, summarize different books, answer different questions, generate different images) in a universal environment such as vision or language.</p>
<p>A curriculum only helps if mastering one task makes another task easier to learn. However, in a goal-conditioned setup, even when tasks are learned in the "correct" order, meaning that easier tasks are learned before the harder tasks that depend on them, it can be difficult to learn the harder tasks. One problem is that the agent does not know the relationship between different tasks, meaning that if the agent is given the goal for a task that it hasn’t learned yet, it does not know which of its previously learned behaviors might help it achieve that goal. Another problem is that, even if the agent did know exactly which of the previously learned tasks is related to the task it is currently trying to solve, the behavior on that previously learned task may not generalize to the current task (meaning that executing the behavior learned on the previous task does not result in any zero-shot performance on the current task), because the tasks are too different. For example, even if the agent has learned to write syntactically correct code and is executing that behavior frequently, it may never write a program that passes a particular unit-test when that unit-test is selected as the current task and thus be unable to learn to write a program for it. We find that adding a goal-independent exploration bonus that rewards all tasks the agent has not yet learned helps the agent learn new tasks.</p>
<p>We have developed and evaluated these curriculum learning methods in a Minecraft environment based on the MineRL platform[1]. Minecraft is a well-thought-out visual world with rudimentary physics, which has the potential to allow our agents to learn many useful skills such as visual processing, spatial awareness, inferring causality and conducting experiments. Most relevant to</p>
<p>curriculum learning in particular, Minecraft features a large tech-tree with many dependencies, making it relatively straightforward to define tasks of varying difficulty that depend on each other to varying degrees. In our experiment, in each task the agent is asked to obtain one out of 107 Minecraft items on command ("Simon Says"), some of which are deep into the tech tree (Figure 4).</p>
<p>Our key results are:</p>
<ul>
<li>Uniform sampling does not learn many tasks, and learning flatlines at a low level (Figure 1, red).</li>
<li>An exploration bonus (as an auxiliary reward) to perform tasks unrelated to the current goal (a.k.a. curiosity search [2, 3]) substantially improves performance (Figure 1, green).</li>
<li>Adding an additional across-training diversity pressure (similar to novelty search [4] and intrinsic motivation [5]) by removing the exploration bonus dynamically for items the agent can already reliably obtain further improves performance (Figure 1, yellow).</li>
<li>Adding a learning progress curriculum increases performance even more (Figure 1, dark dotted blue). A video of a successful agent obtaining challenging items high up in the tech tree can be viewed at https://youtu.be/MFDudOvn3oc.</li>
<li>With the learning progress curriculum, the sampling of tasks elegantly follows the learning curve of the agent, focusing learning on the frontier of the agent's skill set as that skill set expands (Figure 3, bidirectional learning-progress curriculum).</li>
<li>Looking for any learning change (including performance drops) (Figure 1, dark dotted blue) prevents the catastrophic forgetting of previously learned tasks that otherwise occurs when you only measure learning improvements (Figure 1, light solid blue), hurting overall performance.
<img alt="img-0.jpeg" src="img-0.jpeg" /></li>
</ul>
<p>Figure 1: Number of discovered "Simon Says" target items (out of a total of 107 items) as a function of optimization steps for each of the training schemes we explore. Our goal was to maximize the number of items the agent is able to obtain and not to focus on obtaining high success probabilities; we therefore classify items with success probability larger than $5 \%$ as discovered ${ }^{3}$. The bidirectional learning progress curriculum discovers the largest number of items, followed by uniform sampling with dynamic exploration bonus, uniform sampling with fixed exploration bonus and uniform sampling without any exploration bonus. The unidirectional learning progress curriculum at times discovers almost as many items as the bidirectional learning progress curriculum, but undergoes cycles where it forgets and rediscovers a large fraction of discovered items. We found inter-run variation to be low, and each run was expensive (21d on 32 GPUs), so we only plot one run per treatment.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Conditional success probabilities of individual "Simon Says" target items under each treatment. Items are ordered according to the order in which the bidirectional learning-progress curriculum learns to collect the items. We only show items that are discovered by at least one of the treatments. The bidirectional learning progress curriculum discovers a super set of items discovered by other treatments.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Probability of sampling individual "Simon Says" target items under each treatment. Items are ordered as in Figure 2. The learning progress curricula accurately track, and thus sample, items whose success probability changes (for bidirectional-learning progress) or increases (for unidirectional-learning progress) the most.</p>
<h2>2 Methods</h2>
<h3>2.1 Learning progress curriculum</h3>
<p>First, we identify the conditions where we expect curriculum learning to work much better than uniform sampling: assume we are given a long list of mutually dependent and progressively more difficult RL tasks $T_1, \ldots, T_N$. We further assume that a policy that has mastered $T_i$ can learn $T_{i+1}$, but not $T_{i+2}$. An agent can therefore learn all tasks if they are presented in this order. However, learning all tasks under uniform sampling is much harder, because only every $1/N$-th rollout is sampled from a learnable task. At a minimum, if it takes at least $H$ samples per batch from task $T$ to learn it,</p>
<p><sup>3</sup>We found that the 5% threshold clearly shows the differences between the different treatments within 50,000 optimizers steps, but we expect that similar results could be obtained for higher success-probability thresholds given sufficient additional training.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Set of 107 "Simon Says" Minecraft items that the agent is tasked to obtain. The left-most column contains items the agent can obtain at the surface without mining. The remaining items are grouped by the mining resource ("stone", "coal", "iron", "lapis", "redstone", "gold", "diamond") that is required to craft them. From left to right, item categories are roughly ordered by how difficult they are to obtain. Difficulty is mainly determined by how deep they are in the Minecraft tech tree and by how rare they are to find. The highlighted items are prerequisite items for downstream items, and thus represent essential stepping stones in the curriculum. The color code indicates the final success probability of our best treatment (the bidirectional learning progress curriculum, presented below). The agent obtains 82 items with success probability &gt; 0.05, 4 items have non-zero success probability below 0.05, and 27 items have zero success probability. In comparison, a naive baseline (uniform sampling only) obtains only 17 items with success probability &gt; 0.05.
we will need to collect $N$ times more samples (i.e. train with an $N$-times larger batch size) when performing uniform sampling just to ensure that we get $H$ samples from task $T$ per batch. In addition, the non-learnable tasks may add additional noise to each batch without providing any signal, meaning an even larger than $N$-times larger batch size may be needed. The optimal curriculum is therefore much more compute efficient than uniform sampling.
A key requirement of designing an automatic curriculum is to infer which new task may be learnable given the current skill a priori. A common approach is to use success probability as a proxy for learnability. Typically, one defines a lower success probability threshold below which tasks are considered too hard and an upper success probability threshold above which tasks are considered too easy and one would predominantly sample tasks between these two thresholds [6, 7, 8]. However, this approach has several shortcomings: First, if the initial state of the environment is randomized, the agent may have intermediate success probability on a task solely because the task is too easy from some initial states and too hard (or even impossible) from others, thus preventing the agent from improving any further. In Minecraft, for example, it is only possible to collect jungle planks when starting in a jungle biome. If the agent only starts in the jungle $20 \%$ of the time, its success probability for jungle planks would be capped at $20 \%$, even if it perfectly learned the task. Second, success probability thresholds that correlate well with learnability might in general depend on the task, the initial state of the environment and the learning stage of the agent. Finally, stochastic environments (i.e. environments with a stochastic transition function) can have a similar disconnect between success probability and learnability as environments with random initial states: An easy-to-learn task may be capped at an intermediate success probability because of an uncontrollable stochastic event that prevents the agent from succeeding reliably. Selecting tasks with intermediate success probability might therefore select tasks where the agent cannot learn anything new.
Instead, we infer learnability of tasks by measuring learning progress, i.e. the recent change in success probability for each task. Given such a measure, the curriculum predominantly samples tasks with large learning progress. We explore sampling based on a bidirectional learning progress measure</p>
<p>(that tracks both increases and decreases in success probability) and a unidirectional measure (that only tracks increases in success probability). The advantage of sampling based on the bidirectional measure is that it not only samples novel tasks when they start showing learning progress, but also samples tasks that are being forgotten.</p>
<h1>2.2 Learning progress inference</h1>
<p>When designing a process to infer learning progress from empirical measurements of successes and failures on a given task it is important to consider the parameters that influence the accuracy of a learning progress estimator. In particular, as learning progress is measured in terms of how the success probability changes over time, it is important to pick the appropriate time scale $\Delta t$ over which the before/after change in success probability is measured. Consider the following example of a task whose true success probability (black) increases over time (Figure 5, left).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Inference of learning progress on an example fictional problem from the slope of the measured success probability curve.</p>
<p>We would like to infer the true learning progress at $t=0$ (vertical dotted black line), which we define as the slope of the success probability curve (solid red line), from the success probabilities measured at time snapshots $t$ and $t-\Delta t$. However, we only have access to a history (up to $t$ ) of noisy measurements of success probability (jagged grey line; the measurements here are sampled from a binomial distribution with the success probability $p$ equal to the value of the true probability - the black line - at time $t$ and $n$, the number of samples, set to 200): If we choose $\Delta t$ too small, we end up measuring the slope of the noise (dashed yellow line) instead of the slope of the underlying trend (with different noise the slope of yellow could have actually been sharply negative!); the variance of our estimator is too high. If we choose $\Delta t$ too large we do not fully capture recent changes in the slope (light blue); our estimator is biased (in the statistical sense: the error is non-zero even with infinite data). An intermediate $\Delta t$ gives us a good trade-off between bias and variance (dashed dark blue line). Indeed, resampling from the binomial many times (each sample being noisy owing to the small sample size) shows that the expected square error is minimal for intermediate values of $\Delta t$ (Figure 5, middle, blue) and we can show analytically that this is in general true for any curved success probability curve (Figure 5, middle, orange and Appendix A.1).
Besides picking the right time scale, we can improve the reliability of our learning progress estimator by calculating it based on the success probability measurements of all previous time steps, rather than just two snapshots in time. We can implement this by replacing the snapshots with two exponential mean average (EMA) processes with different time scales. In particular, the first EMA represents a "fast success probability" $p_{\text {fast }}$ estimate for each task (Figure 5, right, red). We then smooth $p_{\text {fast }}$ with a second, identical EMA to obtain a "slow success probability" estimate $p_{\text {slow }}$ (green). From there, we define learning progress as the difference between the fast and slow success probability estimates (Figure 5, right, yellow). This learning progress estimate increases after the success probability curves upward (meaning the agent is going from no learning progress to increasing learning progress), flattens after the success probability reaches a linear upward trend (meaning learning progress is happening at a steady pace), and finally goes down to zero again after the measured success probability converges (meaning no further learning progress is happening because the agent has learned everything there is to learn on this task). Note that learning progress does not perfectly follow the derivative of the success probability, but is delayed because it is calculated from two EMA processes which themselves are delayed estimates of the success probability. In practice, we are able to correct for this temporal delay, as explained in detail below. Based on this definition of</p>
<p>learning progress, we further define bidirectional and unidirectional learning progress as the absolute and rectified difference, respectively, between fast and slow success probability. If $p_{\text {fast }}$ is larger than $p_{\text {slow }}$, as in Figure 5, right, the two measures are identical.</p>
<p>We can improve the curriculum further by putting additional focus on tasks that have not only high learning progress, but also a low success probability. The rationale is as follows: Our goal is to learn as many tasks as possible, as opposed to learning a few tasks very well. We thus care much more about an improvement from $0.01 \%$ to $5 \%$ in a new task than we do about moving a task we already perform well on from $95 \%$ to $99.99 \%$ reliability (this is the same reason why we chose a $5 \%$ threshold for the purpose of evaluation). Since low success probability tasks are more likely to be novel, putting particular focus on them helps to expand the set of learned tasks. In addition, low success probability tasks are also harder to learn because the agent observes fewer successes. Sampling hard tasks more often increases the number of successes and therefore helps to mitigate the exploration problem. While our focus on low-probability tasks may seem reminiscent of the fixed success-probability-threshold methods discussed above, it avoids the false positives when selecting tasks solely based on success thresholds because it excludes tasks without learning progress.</p>
<p>We implement this focus by applying a reweighting function to $p_{\text {fast }}$ and $p_{\text {slow }}$ before computing learning progress. The reweighting function we use magnifies learning progress in tasks with low success probability at the expense of learning progress in tasks with high success probability. However, tasks without learning progress are still mapped to zero learning progress, no matter the success probability (see Appendix A. 2 for details). In the fictional example above, reweighted learning progress is given by the blue curve in Figure 5, right. The figure illustrates that this reweighting can also compensate for the temporal delay that we observed with the regular learning progress estimate because success probabilities will be lowest right when the agent starts to learn a particular task. Reweighting is applied in the same way for bidirectional and unidirectional learning progress.
As a last step, we use a sampling function that focuses $90 \%$ of sampling weight on roughly $20 \%$ of tasks with the largest reweighted learning progress. This sampling function was designed to strike a balance between focusing only on the tasks with the highest learning probability, which can cause catastrophic forgetting of all other tasks, and uniformly sampling all tasks, which would waste a lot of computation on tasks where no learning progress is observed (see Appendix A.3).
In conclusion, by taking the difference between a fast and slow estimate of the measured success probability we obtain an accurate, but delayed, estimate of learning progress (Figure 5, yellow). By reweighing this estimate towards tasks with low success probability we compensate for the delay and put the majority of focus on exactly those tasks that are currently learnable, but have not been learned yet (Figure 5, blue). Taking $90 \%$ of our samples from the tasks that score within the top $20 \%$ of this reweighed learning-progress metric thus ensures that our curriculum always pushes on the frontier of the currently learnable tasks that have not been learned (well) yet.</p>
<h1>2.3 Curricula over goal-conditioned tasks</h1>
<p>A key requirement of curriculum learning (at least using current RL methods) is that the agent achieves a small success probability (within available/reasonable compute) on a new task after mastering a previous task. However, if tasks differ by goals that are given as observations to the agent, the agent may not know how to interpret the new goal observation and may just display random behavior instead of the behavior of a prerequisite task. In addition, obtaining useful feedback in a goal-conditioned setup is much harder than learning unconditional tasks because the agent only experiences a positive reward for completing the new task in episodes where the goal matches the task. The success probability of the new task is therefore suppressed by the background probability of sampling the corresponding goal, which makes the task difficult to learn.
We can encourage the discovery of new goals by combining the goal-conditioned main reward with a goal-independent exploration bonus for all of the curriculum tasks, even if they are unrelated to the current goal. This exploration bonus helps the agent to explore the environment when given an unknown goal, thus increasing the chances of success. In our Minecraft experiment, where a new goal corresponds to collecting a new item, for each item in the set of potential goal items, we provide a reward the first few times the agent collects that item in an episode, regardless of the currently selected goal. Specifically, the agent receives a reward of $0.5^{N}$ for the $N$-th collection of the same item, i.e. the reward decays with a factor of 0.5 for each subsequent collection. In addition, the agent</p>
<p>only receives a reward for a subsequent collection of item $X$ if $N$ is larger than the number of items $X$ in its inventory at any previous time during the episode (this constraint prevents the agent from just racking up reward by dropping and picking up items). The exploration bonus therefore incentivizes the agent to constantly collect or craft new items that it hasn't held previously during the episode. This idea of encouraging an agent to do as many new, different things within an episode is similar to previous work in Curiosity Search [2, 3].
The exploration bonus is combined with the main reward by simply summing the two rewards, meaning they have to be carefully balanced: the agent may favor the exploration bonus over the main reward if the exploration bonus is too high or not gain any benefits from the exploration bonus when it is too low. In addition, this balance will have to change as the curriculum advances; at the start of training it is fine for the agent to get an exploration bonus for all of the easy tasks, but as the curriculum moves towards harder tasks it becomes increasingly likely that the agent will spend most of the limited time it has per episode collecting reward by obtaining the exploration bonus for all easy items rather than attempting to complete the hard task selected by the curriculum. In our experiments, this balance is maintained across training by a second curriculum that successively removes already-learned goals from the exploration bonus. We call this curriculum-based exploration bonus the "dynamic exploration bonus".
In our Minecraft experiment, we implement the dynamic exploration bonus in the following way: Using exponential mean averaging (EMA), we keep track of the individual success probabilities in the main, goal-conditioned, task. Only items for which the agent has a success probability smaller than 0.1 are included in the set of items rewarded by the exploration bonus (called "exploration set"). Thus, as the agent improves its performance over training, items are successively removed from the exploration set, but are added back if the performance on the corresponding task drops again below 0.1. The dynamic exploration reward can be seen as an implementation of across-training diversity pressure similar to previous work in Novelty Search [4] and Intrinsic Motivation [5].</p>
<h1>2.4 "Simon Says" task</h1>
<p>We study curriculum learning on a set of goal-conditioned Minecraft tasks, in which the agent is tasked to collect one out of a set of 107 items from the Minecraft tech tree (Figure 4) ${ }^{4}$. Some items (such as "dirt") can be very easily obtained, while other items (such as "diamond") are rare and also require first obtaining many other resources and crafting required tools. The agent observes the target item as a one-hot encoding. It has 5 minutes ( 1500 time steps) to complete the task and obtains a reward of +1 upon success. After each success or failure a new task is selected without resetting the world or respawning the agent. Tasks may be sampled uniformly or from a distribution that is determined by a curriculum.
The maximum episode length is 30 minutes ( 9000 time steps), but the episode is terminated prematurely if the agent has failed two consecutive tasks or dies of other causes, such as drowning, falling off a cliff or falling in lava. After an episode ends, a new episode is started in a new environment, but the agent has a $50 \%$ chance to retain the inventory from the end of the previous episode. In preliminary experiments, we found that this "inventory inheritance" leads to slightly faster learning, as the agent does not always have to gather all the necessary prerequisite items from scratch when trying to learn how to obtain difficult items deep in the tech tree. Because each run was computationally expensive ( 21 d on 32 GPUs) we only plot one run per treatment, but we found inter-run variation to be low.</p>
<h2>3 Results: Evaluation on Minecraft "Simon Says"</h2>
<h3>3.1 Uniform sampling without exploration bonus</h3>
<p>In the standard Simon Says task, the agent only experiences a positive reward for obtaining an item if the item corresponds to the goal of the task. This makes all but the easiest tasks difficult to learn, because the success probability of the task is suppressed by the probability of sampling the corresponding goal and because there is no other exploration bonus. As expected, the results with this</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>method are poor: the agent only learns to obtain 17 out of 107 tasks (Figure 1, red line). Note that we say that the agent has "learned" a task if it achieves a success probability of at least $5 \%$. Worse, the plot shows that learning plateaued early and does not improve over time. The agent only discovers a subset of items that can be obtained on the surface, such as "dirt", "sapling" and a number of wooden tools (Figure 2, 1st from left).</p>
<h1>3.2 Uniform sampling with fixed exploration bonus</h1>
<p>To support exploration of harder tasks, we add the exploration bonus over all items in the Simon Says 107 set. The exploration bonus is added to the main Simon Says reward with a coefficient that was tuned to each condition. For uniform sampling without curriculum we found in preliminary experiments that a coefficient of 0.05 performs best.
Adding the exploration bonus increases the number of explored items at convergence from 17 to 43 (see Figure 1, dotted green line). The agent discovers almost all surface items, learns to mine underground, learns to craft stone tools, and even discovers how to create a few iron tools such as "iron ingot", "heavy weighted pressure plate", "tripwire hook" and "iron shovel" (Figure 2, 2nd from left).</p>
<h3>3.3 Uniform sampling with dynamic exploration bonus</h3>
<p>However, while the exploration bonus helps the agent in solving a larger number of tasks, it actually can make it hard to learn to collect harder items that are deeper in the tech tree. The reason is that the exploration bonus equally rewards items that the agent already knows how to obtain (but has not yet obtained this episode) and items that the agent still needs to learn how to get. When given a hard-to-obtain goal that the agent has not learned yet, it may be more rewarding to collect easy-to-obtain and previously learned items in order to collect their exploration bonus, therefore "distracting" the agent from the task it is currently supposed to complete. One straightforward solution to this problem is to provide an exploration bonus only for those items that the agent does not yet reliably know how to obtain. This allows us to include the exploration bonus only when it is useful, namely for learning how to obtain new items, without distracting the agent.
The "dynamic exploration bonus" implements exactly this idea by removing items whose goalconditioned success probability in the main Simon-Says task is larger than 0.1 .
As we only give an exploration bonus for hard items that the agent rarely gets, we can increase the coefficient of the exploration bonus without increasing the extent to which the agent gets distracted by easy items. In preliminary experiments we found that a coefficient of 0.5 performed best.
The dynamic exploration bonus further increases the number of explored items at convergence to about 70 (see Figure 1, dashed yellow line). From within the target set, the agent discovers all surface items, all stone items, most iron items and most redstone items (Figure 2, 3rd from left).
Conceptually, the dynamic exploration bonus and the conditional Simon Says reward interleave in the following way: At first, the exploration bonus incentivizes the agent to learn how to obtain an item unconditionally, i.e. irrespective of the current Simon Says task. As the unconditional success probability increases, so does the conditional success probability, that is, the success probability when the new item corresponds to the goal. Once the conditional success probability surpasses 0.1 , the item is removed from the exploration set and is only rewarded through the main Simon Says reward (i.e. only when given the task where the item corresponds to the goal). The agent can then solidify the association between the observed Simon Says task label and the task.</p>
<h3>3.4 Learning progress curriculum</h3>
<p>In this treatment we sample Simon Says tasks using the learning progress curriculum instead of uniform sampling. As in the previous section, we remove easy items from the exploration set using the dynamic exploration bonus and we again set the overall coefficient of the exploration bonus to 0.5. The learning progress curriculum improves task learning by two mechanisms: First, by focusing on Simon Says tasks with the largest learning progress it supports learning the conditional association between task label and task. Second, the curriculum interacts with the exploration bonus to facilitate the unconditional discovery of novel items. If learning progress is made on an item such that the</p>
<p>curriculum frequently samples it, the agent is likely to obtain it more often and, if that item is a prerequisite for any dependent items, the agent is now more frequently in a position where it actually has this prerequisite item and is thus able to collect the dependent items.</p>
<p>We tested both the bidirectional and unidirectional versions of the learning progress curriculum (described above). With the bidirectional learning progress curriculum, the agent discovers 82 items (see Figure 1, dotted blue line). The agent discovers all surface items, all stone items, most iron items, most redstone items, half of the golden items and a few diamond items (Figure 2, 2nd from right). With the unidirectional learning progress curriculum, the agent discovers 79 items (see Figure 1, solid light blue line and Figure 2, 1st from right), which is almost as many as the bidirectional learning progress treatment, but training in the unidirectional treatment is unstable because of many cycles of forgetting and rediscovery (see next section). Both curricula accurately sample items for which success probability changes or increases the most (compare Figure 2, 1st and 2nd from right with Figure 3, 1st and 2nd from right).</p>
<p>With both the bidirectional and unidirectional learning-progress curricula, the interaction between the dynamic exploration bonus and the conditional Simon Says reward is similar to the interaction between the dynamic exploration bonus and uniform sampling. However, with the learning progress curricula the learning of the conditional association (performing the task when asked) is more focused and accelerated, because the learning progress curriculum detects the increase in conditional success probability and consequently focuses on that task, which means there will be many more rollouts where the task is also the current goal and thus many more positive examples from which the agent can learn the association.</p>
<h1>3.5 Mitigating catastrophic forgetting by tracking bidirectional learning progress</h1>
<p>A curious phenomenon of the unidirectional learning progress curriculum is that it goes through cycles where the number of discovered items drop sharply, before it again recovers. These cycles are caused by catastrophic forgetting owing to correlations in learning progress between groups of items. As an example, let us consider a case where, after having discovered underground items, the agent improves its success probability for a surface item such as "sapling". The unidirectional learning progress curriculum samples "sapling" more often which causes the agent to spend more time at the surface, which in turn causes the agent to also improve its success probability for other surface items (they are easier because the agent is already on the surface), creating a positive feedback loop. Meanwhile, success probabilities for underground items decrease because the agent spends less time underground and thus forgets (catastrophically) how to perform such tasks. The bidirectional learning progress curriculum would immediately increase its sampling of underground items in order to prevent those tasks from being forgotten, which would prevent catastrophic forgetting and thus cycles from appearing. In contrast, the unidirectional learning progress curriculum does not increase the sampling of underground items when their success probabilities are decreasing. As a consequence, it enters a period where it exclusively focuses on surface items and generally these periods last long enough for the agent to almost completely forget how to obtain underground items when asked. Since only 24 out of the 107 potential goals are surface items, this causes a large drop in the number of discovered items. However, after about 1000-2000 optimizers steps, the curriculum notices positive learning progress on the underground items again, allowing it to rediscover underground items, and the number of discovered items recovers (Figure 1, solid light blue line). Interestingly, much of the ability to perform these skills is still latent in the network, as its performance recovery is swift.</p>
<p>Neural networks in general and RL agents in particular are subject to catastrophic forgetting if the data distribution changes during learning. The easiest method to mitigate catastrophic forgetting is to sample the data i.i.d. (i.e. uniform sampling of discrete tasks), whereas a curriculum might cause the agent to forget early tasks.</p>
<p>The success of the bidirectional learning progress curriculum shows that monitoring previous tasks and resampling them if their performance drops can be a powerful method to mitigate catastrophic forgetting. As shown in Figure 3, 2nd from right, only sporadic resampling of old tasks is sufficient, which is much more compute efficient and has better scaling properties than iid sampling of all previously learned tasks.</p>
<h1>4 Related work</h1>
<p>Automated curriculum learning There is an extensive literature on training neural networks with a curriculum [9], see [10] for an overview. More recently, automated curricula have been studied extensively in the context of RL in [11, 12, 13, 14, 6, 7, 8, 15, 16, 17, 18, 19]. Generally, tasks are selected based on success probability or reward thresholds [6, 7, 8, 16] or regret [17, 18]. Static-threshold-based methods present an intuitive starting point for a curriculum, but have the downside that they are difficult or even impossible to tune, as discussed previously (Sec. 3.4). Regret-based methods calculate per-task regret by subtracting the average return over several rollouts on that task from the maximum (known) return on that task, and then preferrably select tasks where regret is high, with the theory being that there is still a lot to learn on tasks where regret is high [17, 18]. In the presence of environmental stochasticity, this scheme may select more stochastic, less learnable tasks at the expense of less stochastic, more learnable tasks, because the maximum return may have been obtained under particularly lucky conditions such that the average return will always be much lower, despite there being nothing left to learn. Learning-progress-based curricula, like the method in this paper, have the potential to address these issues, as discussed next.
Learning progress was first proposed as a curiosity signal that incentivizes agents to explore novel states in stochastic environments [20, 21, 22]. Later, in [23, 13, 14] learning progress was used as a measure to select tasks in a curriculum. The novel contributions of our work are to systematically study how learning progress can be measured reliably, to apply learning progress curricula on hard problems that require RL at scale, and to show how learning progress curricula over goals can be combined with a dynamic, goal-independent exploration bonus.
Curiosity Search The static exploration bonus we examined incentivizes the agent to obtain items that it has not obtained in the current episode, and is thus a method for encouraging within-episode exploration. Within episode exploration has previously been explored in the Curiosity Search work by [2, 3], who demonstrated that it effectively encourages agents to explore their environment, though they also demonstrated the downside of having to explore the entire environment in every episode when trying to perform deep exploration.
Intrinsic motivation The dynamic exploration bonus, on the other hand, changes over training and encourages the agent to obtain different items, not within a single episode, but across episodes. Exploration across episodes has been extensively studied in the form of count-based exploration (e.g. [5]), where the algorithm tracks the number of times each state has been visited over training and provides a bonus reward to the agent for visiting each state that is inverse proportional to the number of times that state has been visited. [24] adapted count-based exploration to work in state-spaces that are too large to keep individual counts, and they demonstrated some success in deeply exploring sparse reward environments. However, later work [25, 26] hypothesized that unconditional countbased exploration methods can suffer from detachment, where the agent consumes all the intrinsic reward towards one or more states and forgets how to return to those states, and derailment, where the exploratory mechanism of the algorithm can prevent the agent from returning to a previously visited state. Our bidirectional learning-progress curriculum avoids detachment by immediately increasing the sampling rate of any goal where success probabilities are declining, thus reducing the probability that the agent forgets how to visit a previous "state", as well as by reintroducing items back into the exploration bonus reward set if their success probability ever drops too low, thus ensuring that the agent can always recover. The learning-progress curriculum does not address derailment as explicitly, but the underlying dynamic exploration reward does in effect reduce derailment by removing learned items from the exploration bonus; while the agent is in a state where it does not have the necessary prerequisites to obtain any of the items still in exploration bonus reward set, it is incentivized to first obtain the necessary prerequisites (without taking exploratory actions), before focusing on exploratory actions again.</p>
<h2>5 Discussion, Conclusion, and Future Work</h2>
<p>We have introduced a learning-progress curriculum with a dynamic exploration bonus that adapts to the current skill level of the agent. Experiments were conducted on the Minecraft "Simon Says" tasks. We first showed that uniform sampling with no exploration bonus performs poorly, obtaining only 17 tasks and hitting a ceiling at that level where additional training produces no new learning. We then showed that combining the main goal-dependent reward with a static goal-independent</p>
<p>exploration bonus increases the number of learned tasks from 17 to 43 . Dynamically adjusting the exploration bonus to only include tasks with low success probability further increases the number of learned tasks to 70 . Sampling tasks using the bidirectional learning progress curriculum instead of uniform sampling further increases the number of solved tasks to 82 . Moreover, the sampling of tasks elegantly follows the learning curve of the agent, focusing learning on the frontier of the agent's skill set as that skill set expands. In addition, the bidirectional learning progress curriculum, which tracks both tasks with improvements and deterioration in performance, effectively mitigates catastrophic forgetting (which we see in the unidirectional learning-progress curriculum) by resampling tasks that are at risk of being forgotten.</p>
<p>There are various ways in which the learning progress curriculum could be expanded in future work. First, while the current method was designed under the assumption that explicit task labels are available, it could be made more general by developing methods that can dynamically group tasks into clusters over which learning progress is averaged. Second, if the number of tasks becomes too large it becomes computationally expensive to faithfully estimate learning progress for each task. A promising future direction would be to estimate learning progress over large tasks spaces using function approximation or, relatedly, generate environments that are expected to feature high learning progress via a neural network environment generator.</p>
<h1>Acknowledgments</h1>
<p>We thank Ilge Akkaya, Bob McGrew, Reiichiro Nakano, Matthias Plappert and John Schulman for discussions, support and feedback on this manuscript.</p>
<h2>References</h2>
<p>[1] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019.
[2] Christopher Stanton and Jeff Clune. Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime. PloS one, 11(9):e0162235, 2016.
[3] Christopher Stanton and Jeff Clune. Deep curiosity search: Intra-life exploration improves performance on challenging deep reinforcement learning problems. arXiv preprint arXiv:1806.00553, 2018.
[4] Joel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for novelty alone. Evolutionary computation, 19(2):189-223, 2011.
[5] Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309-1331, 2008.
[6] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019.
[7] Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth Stanley. Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. In International Conference on Machine Learning, pages 9940-9951. PMLR, 2020.
[8] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.
[9] Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71-99, 1993.
[10] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41-48, 2009.
[11] Sainbayar Sukhbaatar, Emily Denton, Arthur Szlam, and Rob Fergus. Learning goal embeddings via self-play for hierarchical reinforcement learning. arXiv preprint arXiv:1811.09083, 2018.
[12] Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. In International conference on machine learning, pages 1515-1528. PMLR, 2018.
[13] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learning. IEEE transactions on neural networks and learning systems, 31(9):3732-3740, 2019.</p>
<p>[14] Rémy Portelas, Cédric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. In Conference on Robot Learning, pages 835-853. PMLR, 2020.
[15] Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic curriculum learning through value disagreement. Advances in Neural Information Processing Systems, 33, 2020.
[16] Andres Campero, Roberta Raileanu, Heinrich Küttler, Joshua B Tenenbaum, Tim Rocktäschel, and Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv preprint arXiv:2006.12122, 2020.
[17] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. arXiv preprint arXiv:2012.02096, 2020.
[18] Izzeddin Gur, Natasha Jaques, Kevin Malta, Manoj Tiwari, Honglak Lee, and Aleksandra Faust. Adversarial environment generation for learning to navigate the web. arXiv preprint arXiv:2103.01991, 2021.
[19] OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben D'Sa, Arthur Petron, Henrique Ponde de Oliveira Pinto, et al. Asymmetric self-play for automatic goal discovery in robotic manipulation. arXiv preprint arXiv:2101.04882, 2021.
[20] Jürgen Schmidhuber. Curious model-building control systems. In Proc. international joint conference on neural networks, pages 1458-1463, 1991.
[21] Pierre-Yves Oudeyer, Frederic Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286, 2007.
[22] Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, and Daniel Yamins. Active world model learning with progress curiosity. In International Conference on Machine Learning, pages 5306-5315. PMLR, 2020.
[23] Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In international conference on machine learning, pages 13111320. PMLR, 2017.
[24] Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. arXiv preprint arXiv:1606.01868, 2016.
[25] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.
[26] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. First return, then explore. Nature, 590(7847):580-586, Feb 2021.
[27] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning, pages 1407-1416. PMLR, 2018.
[28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[29] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.</p>
<h1>A Appendix</h1>
<h2>A. 1 Optimal $\Delta t$ for estimating learning progress:</h2>
<p>Let us model success probability as an i.i.d stochastic process $x(t)$ with mean $\mu(t)$ and variance $\sigma^{2}(t)$. Learning progress is given by the derivative of the mean, $\mu^{\prime}(t)$. We want to estimate learning progress $\mu^{\prime}(t)$ by averaging $n$ samples from $x(t)$ and from $x(t-\Delta t)$ and calculating the difference quotient: The expected square error of this estimator $\hat{\mu^{\prime}}(t)=\frac{\frac{1}{n} \sum_{i} x_{i}(t)-\frac{1}{n} \sum_{i} x_{i}(t-\Delta t)}{\Delta t}$ is given by:</p>
<p>$$
\operatorname{err}^{2}=\frac{2 \bar{\sigma}^{2}}{n \Delta t^{2}}+\frac{1}{4} \mu^{\prime \prime}(t)^{2} \Delta t^{2}+O\left(\Delta t^{3}\right)
$$</p>
<p>where $\bar{\sigma}^{2}=\frac{1}{2}\left(\sigma^{2}(t)+\sigma^{2}(t-\Delta t)\right)$ is the average variance and $\mu^{\prime \prime}(t)$ is the second derivative (curvature) of the mean. The notation $O\left(\Delta t^{3}\right)$ means that we assume $\Delta t$ to be small and that we</p>
<p>neglect terms of third order and higher. The curve (1) corresponds to the orange curve in Figure 5, middle.</p>
<p>The optimal $\Delta t$ that minimizes the square error in (1) is given by $(\Delta t)_{\text {opt }}=\left(\frac{8 \bar{\sigma}^{2}}{n \mu^{\prime \prime}(t)^{2}}\right)^{\frac{1}{4}}$. If we increase the number of measurements $n$, our estimate of success probability becomes more accurate and we can afford a smaller $\Delta t$. If the curvature $\mu^{\prime \prime}(t)$ increases (i.e. the success probability curve has high curvature), we need to choose a smaller $\Delta t$ to keep the bias of our estimator in check. If the average variance $\bar{\sigma}^{2}$ gets larger, we need to increase $\Delta t$ to keep the variance of our estimator in check. The optimum exists if and only if $\mu^{\prime \prime}(t)$ is non-zero.</p>
<h1>Proof of (1):</h1>
<p>Since we know the means and variances of the samples of $x(t)$ and $x(t-\Delta t)$ we can also calculate the mean and variance of the estimator $\hat{\mu^{\prime}}(t)=\frac{2}{n} \sum_{i} x_{i}(t)-\frac{1}{n} \sum_{i} x_{i}(t-\Delta t)$, because the latter is just defined by a linear combination of $x_{i}(t)$ and $x_{i}(t-\Delta t)$.
For the mean we find:</p>
<p>$$
\left\langle\hat{\mu}^{\prime}(t)\right\rangle=\frac{\frac{1}{n} \sum_{i}\left\langle x_{i}(t)\right\rangle-\frac{1}{n} \sum_{i}\left\langle x_{i}(t-\Delta t)\right\rangle}{\Delta t}=\frac{\mu(t)-\mu(t-\Delta t)}{\Delta t}
$$</p>
<p>This means that our estimator is an unbiased estimator of the finite difference quotient, but not of the derivative $\mu^{\prime}(t)$ (which we obtain in the limit $\Delta t \rightarrow 0$ ). We can calculate the relationship between the two by Taylor-expanding to second order in $\Delta t$ :</p>
<p>$$
\mu(t-\Delta t)=\mu(t)-\mu^{\prime}(t) \Delta t+\frac{1}{2} \mu^{\prime \prime}(t) \Delta t^{2}+O\left(\Delta t^{3}\right)
$$</p>
<p>The $O\left(\Delta t^{3}\right)$-notation means that we are neglecting 3rd and high-order terms in $\Delta t$, because we assume $\Delta t$ to be small.
Plugging this expression in our expression for the mean yields:</p>
<p>$$
\left\langle\hat{\mu^{\prime}}(t)\right\rangle=\mu^{\prime}(t)-\frac{1}{2} \mu^{\prime \prime}(t) \Delta t+O\left(\Delta t^{2}\right)
$$</p>
<p>This expression means that the bias of our estimator is determined (to first order) by the second derivative of the mean. Note also that the neglected terms are now of 2nd order and higher, because we have divided by $\Delta t$.
For the variance we find:</p>
<p>$$
\operatorname{Var} \hat{\mu^{\prime}}(t)=\frac{\frac{1}{n^{2}} \sum_{i} \operatorname{Var} x_{i}(t)+\frac{1}{n^{2}} \sum_{i} \operatorname{Var} x_{i}(t-\Delta t)}{\Delta t^{2}}=\frac{\frac{1}{n} \sigma(t)+\frac{1}{n} \sigma(t-\Delta t)}{\Delta t^{2}}
$$</p>
<p>To arrive at this expression we have made use of several facts and assumptions: Since all samples from $x(t)$ and $x(t-\Delta t)$ are uncorrelated we can simply add the variances. Second, if one multiplies/divides a random variable by a constant, its variance gets multiplied/divided by the square of the same constant.
Using our expression for the average variance, $\bar{\sigma}^{2}=\frac{1}{2}\left(\sigma^{2}(t)+\sigma^{2}(t-\Delta t)\right)$, the variance is given by:</p>
<p>$$
\operatorname{Var} \hat{\mu^{\prime}}(t)=\frac{2 \bar{\sigma}^{2}}{n \Delta t^{2}}
$$</p>
<p>The expected square error of our estimator is given by the sum of the square of the bias and the variance:</p>
<p>$$
\operatorname{err}^{2}=\left(\left\langle\hat{\mu^{\prime}}(t)\right\rangle-\mu^{\prime}(t)\right)^{2}+\operatorname{Var} \hat{\mu^{\prime}}(t)=\frac{2 \bar{\sigma}^{2}}{n \Delta t^{2}}+\frac{1}{4} \mu^{\prime \prime}(t)^{2} \Delta t^{2}+O\left(\Delta t^{3}\right)
$$</p>
<p>which completes our proof.</p>
<h1>A. 2 Reweighting of learning progress towards small success probabilities</h1>
<p>For each task we use EMAs with different time scales to obtain a "fast" and a "slow" measure of success probability, $p_{\text {fast }}$ and $p_{\text {slow }}$. Our bidirectional learning progress measure is given by $L P=\left|f\left(p_{\text {fast }}\right)-f\left(p_{\text {slow }}\right)\right|$, where we use the reweighting function</p>
<p>$$
f(p)=\frac{\left(1-p_{\theta}\right) p}{p+p_{\theta}(1-2 p)}
$$</p>
<p>with parameter $p_{\theta}=0.1$.
The reweighting function magnifies differences in small probabilities, an effect that is illustrated in Figure 6: A probability difference between $p=0.1$ and $p=0.2$ (dotted red lines) leads to a much larger difference in reweighted probabilities than a probability difference between $p=0.6$ and $p=0.7$ (dotted blue lines).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Reweighting of probabilities before calculating learning progress</p>
<h2>A. 3 Conversion of reweighted learning progress measure to sampling weight</h2>
<p>When converting reweighted learning progress to sampling weight (i.e. unnormalized sampling probability) we choose a sampling function that focuses on roughly $10 \%$ of tasks with the largest reweighted learning progress, but also prevents overfitting to just one or two tasks. The algorithm is as follows:</p>
<ul>
<li>
<p>Z-score the reweighted learning progress (subtract mean and divide by standard deviation)</p>
</li>
<li>
<p>Apply a sigmoid to the result. The sigmoid is centered on $90 \%$ quantile of the normal distribution (Figure 7). The saturation of the sigmoid for large learning progress prevents sampling from just focusing on one or two tasks.</p>
</li>
<li>Normalize resulting weights to sampling probabilities.</li>
</ul>
<p>If the reweighted LP measures were Gaussian-distributed, the above algorithm would focus on sampling the top $10 \%$ of tasks. In practice the LP measures often deviate from Gaussianity and we see the curriculum sometimes focus on a larger or smaller percentage of tasks.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Sigmoid sampling function applied to z-scored reweighted learning progress</p>
<h1>A. 4 Experiment details</h1>
<h2>A.4.1 Environment</h2>
<p>The Minecraft environment the agents interacts with is based on MineRL [1]. At each time step, the agent observes a $64 \times 64$ RGB video frame of the player's point-of-view and a set of features about the game-state: player inventory, health, food, saturation, experience point and the item in the main hand and off hand. In addition, the agent observes the id of the current goal item and the items in the exploration set. The main actions of the agent are "attack", "back", "forward", "jump", "left", "right", "sneak", "sprint" and "use" as well as control of the camera. We also provided the agent with special actions to craft, equip and place specific items without using the GUI interface (items that could not be crafted, equipped or placed in a given game state were masked out using an action mask). We used a "frame skip" of 4, i.e. every action by the policy, except "place", "craft", "equip" and camera actions, was repeated for 4 consecutive time steps in the MineRL environment before the policy could sample a new action. Given that the Minecraft environment runs at 20 frames per second, this allows the agent to choose a different action every 200 milliseconds, close to the typical human reaction time of 250 milliseconds. Finally, we added a "channel attack" action that had the effect of repeating the "attack" action for a fixed number of policy time steps (the precise number depended on which tool the agent was holding in its main hand at the time the "channel attack" action was taken, with</p>
<p>better tools taking fewer steps, because better tools require fewer consecutive attack actions to break a block), which made it easier for the agent to learn how to mine resources.</p>
<h1>A.4.2 Policy and Optimization details</h1>
<p>The policy and value function networks shared the same architecture and weights. Visual observations were processed using an enlarged version of the IMPALA convnet [27]: the number of filters of the 3 modules were 64,128 and 128 , respectively, instead of 16,32 and 32 as in the original IMPALA convnet. The convnet was followed by a fully connected layer of size 512. Feature observations were embedded linearly (inventory observations) or using one-hot encoding (all other feature observations) in a 512-dimensional embedding space. We then summed all visual and feature embeddings and processed it through an LSTM layer and another fully connected layer of size 512. The network output was given by a linear, factorial action head for the policy and a linear layer for the value function. Optimization was performed using Proximal Policy Optimization [28] and General Advantage Estimation [29].</p>
<h2>A. 5 Optimization hyperparameters</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Buffer size</th>
<th style="text-align: center;">209,728</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Mini-batch size</td>
<td style="text-align: center;">$3,264 \times 10$ time steps</td>
</tr>
<tr>
<td style="text-align: center;">Learning rate</td>
<td style="text-align: center;">$3 \cdot 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">PPO clipping parameter</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">Entropy coefficient</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: center;">GAE parameter $\lambda$</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: center;">BPTT truncation length (student)</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">max episode length</td>
<td style="text-align: center;">9000</td>
</tr>
<tr>
<td style="text-align: center;">EMA time scale of learning progress curriculum</td>
<td style="text-align: center;">1250 optimization steps</td>
</tr>
<tr>
<td style="text-align: center;">1 optimization step</td>
<td style="text-align: center;">$\sim 17180$ policy frames</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ In addition to the 107 items displayed in Figure 4, the target set also contains 6 additional items that we later realized were impossible for the agent to obtain.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
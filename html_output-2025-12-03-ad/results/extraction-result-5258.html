<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5258 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5258</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5258</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-265871676</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.00149v3.pdf" target="_blank">One for All: Towards Training One Graph Model for All Classification Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Designing a single model to address multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in solving different tasks within the language domain. However, a unified model for various graph tasks remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. We propose \textbf{One for All (OFA)}, the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-attributed graphs to unify different graph data by describing nodes and edges with natural language and uses language models to encode the diverse and possibly cross-domain text attributes to feature vectors in the same embedding space. Furthermore, OFA introduces the concept of nodes-of-interest to standardize different tasks with a single task representation. For in-context learning on graphs, OFA introduces a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning. We train the OFA model using graph data from multiple domains (including citation networks, molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs well across different tasks, making it the first general-purpose across-domains classification model on graphs.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5258.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5258.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAGs / OFA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-Attributed Graphs (TAGs) / One-for-All (OFA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OFA converts any graph node/edge attributes into standardized natural-language sentences (Text-Attributed Graphs) which are encoded by an LLM into a shared embedding space, then a single GNN processes the prompted graph (including NOI prompt node and class nodes) to solve node/link/graph classification in supervised, few-shot and zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text-Attributed Graphs (TAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Every node/edge is serialized into a short, structured natural-language string with a fixed protocol: node texts begin with "Feature node." followed by one-or-more feature entries of the form "<feature description>: <feature content>; ..."; edges begin with "Feature edge.". Additionally, task information is encoded as "Prompt node.<task description>."; class nodes are textual labels/descriptions. All texts s_vi and s_eij are embedded via an LLM to produce x_i = LLM(s_vi), x_ij = LLM(s_eij). A prompt graph (NOI prompt node connected to NOI nodes and class nodes, optionally support NOI prompt nodes for few-shot) is appended; the combined prompted graph G_m is processed by a relation-aware GNN (R-GCN style with edge features and attention over layers) and class-node embeddings are used for binary/multi-class prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks, knowledge graphs, molecular graphs, web-link graphs (a general protocol for any graph where nodes/edges are describable by text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Unifies heterogeneous feature spaces into a single LLM-derived embedding space (co-embedding of node/edge attributes and task descriptions); interpretable (human-readable text features); domain-aware (LLM embeddings separate domains into subspaces); expressive when combined with GNN (retains explicit graph structure via GNN message passing rather than relying purely on text); supports in-context learning via prompt graph (enables few-shot/zero-shot); depends on quality/size of LLM encoder; requires designing descriptive text templates, so compactness and exact faithfulness depend on the textual generation rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, link prediction, graph classification evaluated in supervised, few-shot (N-way K-shot) and zero-shot settings across nine datasets (Cora, PubMed, ogbn-arxiv, Wiki-CS, FB15K237, WN18RR, ChEMBL, PCBA, HIV).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy, AUC, APR used. Representative results (reported in paper): OFA joint models achieve comparable or better supervised performance than GCN/GAT/GIN baselines across datasets; example few-shot/zero-shot: OGbN-arxiv 5-way 5-shot OFA-joint-lr ~61.45±2.56% (accuracy), FB15K237 20-way 5-shot OFA-joint-lr ~82.56±1.58% (accuracy), HIV 2-way 5-shot AUC ~63.58±1.81. (Multiple dataset-specific metrics and standard deviations are reported in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared against classical GNNs using raw features (GCN, GAT, GIN) and several LLM/GNN hybrids, OFA's TAG encoding + prompting enables a single GNN to be trained across domains and tasks; joint TAG training often outperforms independently trained models, and larger/more capable LLM encoders tend to yield better joint performance. Ablation vs a pooling-based alternative ("-Class node") shows OFA's prompt-node + class-node design retains performance when datasets are trained jointly while the pooling approach degrades.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires manual design of textual feature templates and reliance on chosen LLM encoder; current OFA handles classification only (not regression); cross-domain training data is still limited relative to LLMs; textualization risks omitting some fine-grained numerical or structural details unless explicitly encoded; LLM quality affects final performance and convergence; careful sampling/resampling required to prevent small datasets from being neglected in joint training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5258.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5258.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphText (Graph reasoning in text space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that describe graph entities and relations in natural language and feed the textual serialization into LLMs for graph reasoning / downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtext: Graph reasoning in text space.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Natural-language serialization of graph structure and attributes</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph nodes and edges (and sometimes relations) are described in natural language and concatenated into text prompts which are given to LLMs. The representation emphasizes textual encoding of entities and (optionally) relations; structural relationships are expressed via textual tokens and sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs; knowledge graphs and text-attributed graphs (as applied in literature cited).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages LLM semantic understanding of textual descriptions; simple and human-interpretable; may be weak at preserving precise graph structural inductive biases because structural topology is represented implicitly in text rather than via explicit graph message passing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning tasks where LLMs generate answers from textual graph descriptions (paper cites general use in node/link/graph tasks in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper mentions LLM-based methods can perform zero/few-shot tasks; no unified numerical metrics provided in this paper for GraphText specifically (paper treats these works as baselines/related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper notes text-only approaches (including GraphText) can lose important structural features; OFA contrasts by using LLM text encodings for attributes but preserving explicit structure via a GNN, leading to stronger performance on structure-sensitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Implicit structure encoding via text may fail to capture graph topology precisely; susceptible to length/formatting limits of text prompts; potential information loss for structure-heavy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5258.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5258.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM / Natural language is all a graph needs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural language is all a graph needs (InstructGLM-style methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that convert graph information into natural language prompts and ask LLMs to perform graph tasks directly from those textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Natural language is all a graph needs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-to-text instructions / descriptive serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode nodes/edges/relations and sometimes local subgraph context as natural-language sentences or instructions which are provided as input to an LLM; the LLM produces answers (class labels, link existence, etc.) from the text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Various graphs; often applied to small graphs and reasoning over knowledge or molecular descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Taps LLM world knowledge and in-context learning; offers high interpretability; may not preserve fine-grained structural signals unless explicitly described; susceptible to verbosity and LLM hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node/link/graph-level tasks via LLM generation; zero/few-shot evaluation commonly reported in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper does not report numerical results for InstructGLM here; mentions such LLM-only graph description methods can do zero/few-shot but may miss structural details.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper contrasts these LLM-text-only approaches with OFA: text-only approaches risk losing structural information, while OFA combines LLM textual encoding of attributes with a GNN to preserve structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Loss of explicit topology, reliance on LLM reasoning, and potential unreliability in mathematical/quantitative prediction or structure-heavy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5258.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5258.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLGraph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that describes graph structure and entities in natural language and feeds the text to LLMs for graph tasks (referenced as a related approach in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NLGraph</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text serialization of graph entities and relations</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent nodes/edges/relations as natural-language descriptions; use LLMs to perform reasoning on the serialized text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs, including knowledge graphs and text-attributed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Human-readable and leverages LLM semantics; potentially loses explicit structural inductive biases if structural detail is not carefully encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning tasks and question-answering over graph content (as cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper groups NLGraph with other LLM-text approaches and notes their weakness on structure-sensitive tasks compared to hybrid LLM+GNN designs like OFA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Implicit representation of topology in text; possible information loss and dependence on prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5258.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5258.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4Graph (Can large language models understand graph structured data? An empirical evaluation and benchmarking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study/approach that feeds textual descriptions of graph data to general-purpose LLMs (e.g., GPT-4) to evaluate their ability to understand and reason about graph-structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text serialization / descriptive prompts of graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph nodes/edges/relations and subgraphs into natural language descriptions to create prompts for LLMs, then evaluate the LLM's reasoning/output.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (benchmarking across multiple graph types).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Exposes LLM's ability to interpret textual graph encodings; interpretable evaluation pipeline; limited for complex topology unless carefully described.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Empirical benchmarking of LLMs on graph understanding tasks (QA, link inference, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This paper references GPT4Graph as related work; no direct numerical reproduction here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper cites GPT4Graph alongside other LLM-text approaches and emphasizes structural information loss risk; OFA is presented as a hybrid that avoids that pitfall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmarking shows LLMs may struggle when structural nuances are not captured textually; scaling to large graphs is challenging due to prompt size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5258.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5258.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMMOL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMMOL (Can large language models empower molecular property prediction?)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that use SMILES or other serial chemical representations as text inputs to LLMs for molecular property prediction and zero/few-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models (llms) empower molecular property prediction?.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES / sequence-based textual encoding of molecules</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent molecules as SMILES strings or other linear textual notations and feed them into LLMs to predict properties, potentially supplementing with natural-language descriptions of atoms/bonds.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Molecular graphs (serialized as SMILES or textual descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact, fits token-based LLM input; captures chemical connectivity implicitly via SMILES ordering; may not preserve all local subgraph symmetries; benefits from LLM pretraining on chemical text or sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Molecular property prediction, few-shot and zero-shot molecular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper cites LMMOL as related work and notes these methods can perform zero/few-shot learning; no direct numerical comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>The paper notes SMILES/sequence-only representations can be effective but are an implicit structural encoding and may miss details that graph-based GNNs + TAGs preserve; OFA uses explicit atom/bond textual templates plus GNN structure to mitigate losses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Implicit representation of graph topology can be ambiguous (SMILES has canonicalization issues); LLMs may not reason about structural motifs as effectively as structure-aware GNN decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5258.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5258.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIMLET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIMLET (A unified graph-text model for instruction-based molecule zero-shot learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph–text hybrid for molecular zero-shot learning that combines textual inputs and graph reasoning for molecule property tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Instructional graph-text encoding (molecule descriptions + SMILES/text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use text descriptions or SMILES as inputs into an LLM together with instructions; combine textual embedding with graph-aware components to predict molecular properties in zero/few-shot setups.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Molecular graphs (graph-text hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Designed for zero-shot molecular property prediction; leverages instruction tuning; may rely on LLM pretraining and instruction design quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Molecular property prediction (graph-level tasks), evaluated in few-shot/zero-shot settings (e.g., ChEMBL/PCBA/HIV datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited in paper as a baseline for graph-level few-shot tasks; reported GIMLET performance (in literature) but the OFA paper does not reprint GIMLET numerical figures in full—used for comparison on AUC metrics for molecular datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>OFA compares favorably in few-shot/zero-shot molecular evaluations in some settings; OFA emphasizes explicit GNN structure plus TAGs, while GIMLET focuses on instruction-based LLM solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Instruction-based LLM methods may underperform when explicit graph topology is crucial; reliance on instruction quality and LLM capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5258.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5258.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT (Graph instruction tuning for large language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM inputs with trainable structural vectors and an alignment module, concatenated with textual descriptions to improve LLM structure reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text + trainable structural vectors with alignment</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent graph entities as text but also include trainable vector tokens encoding structure; an alignment module maps graph structure to the LLM input space, concatenating structural vectors with textual input to help LLMs reason about topology.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (knowledge graphs, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Hybrid: keeps human-readable text while augmenting with learned structural tokens to mitigate purely text-based loss of topology; requires training alignment and additional vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning tasks targeted at improving LLM structural understanding (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper references GraphGPT; no numerical reproduction in OFA paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper positions GraphGPT as an orthogonal approach to OFA: GraphGPT augments LLM inputs with learned structure tokens, while OFA encodes attributes via LLM and preserves structure with an explicit GNN; both aim to address LLMs' structural limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires training additional alignment modules and learned vectors; may increase parameter count and complexity; still relies on effective alignment to LLM token space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5258.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5258.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explanations-as-Features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explanations as features: LLM-based features for text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that uses LLM-generated explanations or textual features as input features for GNNs on text-attributed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explanations as features: Llmbased features for text-attributed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM-derived textual explanation features</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use LLMs to produce textual explanations or derived text features for nodes/edges and convert these into embeddings which serve as node/edge features for downstream GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (graphs where nodes/edges have text metadata).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Produces semantically rich features leveraging LLM explanations; interpretable and may aid downstream GNNs; quality tied to LLM explanation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node/link/graph classification on text-attributed graphs (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced as related work in OFA; no numeric results reproduced in OFA paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Related to OFA's use of LLM embeddings for attributes; OFA differs by systematically templating attributes into TAGs and introducing prompting and prompt-graph design for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on explanation quality and potential verbosity; might introduce redundant or noisy signals if explanations are inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5258.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5258.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prodigy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prodigy (Enabling in-context learning over graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts classification tasks into link-prediction problems on prompt graphs to enable in-context learning on graphs; used as a baseline in OFA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prodigy: Enabling in-context learning over graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Prompt graph conversion to link-prediction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transforms classification tasks into link-prediction on an augmented prompt graph where virtual/ prompt nodes encode task context; uses prompt graphs and graph encoders to perform in-context learning and few-shot classification.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs; prompt-augmented subgraphs for node/link/graph-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Prompt-graph based, enabling in-context/few-shot learning without retraining the whole model for each task; relies on learned prompt vectors and graph augmentation to present support examples and task description.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Few-shot node and link classification tasks; used as a baseline for OFA's few-shot/zero-shot evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used in comparisons: OFA reports comparable or better few-shot performance than Prodigy on several benchmarks (e.g., on ogbn-arxiv and FB15K237 tasks in paper tables). Specific example: on ogbn-arxiv 5-way 5-shot, Prodigy ~61.09±5.85% vs OFA-joint-lr ~61.45±2.56% (accuracy) as reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Prodigy uses prompt graphs with trainable prompt vectors; OFA differs by (1) converting raw attributes into human-readable TAGs encoded by an LLM and (2) using a single GNN trained across multiple domains with NOI prompt nodes and class nodes enabling zero-shot predictions without retraining prompts. OFA often matches or exceeds Prodigy in experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prodigy's prompt vectors often need re-training per downstream shot/setting; may require pretraining on large corpora (e.g., MAG240M or Wiki) for transfer; less explicit use of LLM-encoded semantic attribute embeddings compared to OFA's TAG pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One for All: Towards Training One Graph Model for All Classification Tasks', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graphtext: Graph reasoning in text space. <em>(Rating: 2)</em></li>
                <li>Natural language is all a graph needs <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking <em>(Rating: 2)</em></li>
                <li>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models. <em>(Rating: 2)</em></li>
                <li>Explanations as features: Llmbased features for text-attributed graphs. <em>(Rating: 2)</em></li>
                <li>Prodigy: Enabling in-context learning over graphs. <em>(Rating: 2)</em></li>
                <li>Can large language models (llms) empower molecular property prediction?. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5258",
    "paper_id": "paper-265871676",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "TAGs / OFA",
            "name_full": "Text-Attributed Graphs (TAGs) / One-for-All (OFA)",
            "brief_description": "OFA converts any graph node/edge attributes into standardized natural-language sentences (Text-Attributed Graphs) which are encoded by an LLM into a shared embedding space, then a single GNN processes the prompted graph (including NOI prompt node and class nodes) to solve node/link/graph classification in supervised, few-shot and zero-shot settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Text-Attributed Graphs (TAGs)",
            "representation_description": "Every node/edge is serialized into a short, structured natural-language string with a fixed protocol: node texts begin with \"Feature node.\" followed by one-or-more feature entries of the form \"&lt;feature description&gt;: &lt;feature content&gt;; ...\"; edges begin with \"Feature edge.\". Additionally, task information is encoded as \"Prompt node.&lt;task description&gt;.\"; class nodes are textual labels/descriptions. All texts s_vi and s_eij are embedded via an LLM to produce x_i = LLM(s_vi), x_ij = LLM(s_eij). A prompt graph (NOI prompt node connected to NOI nodes and class nodes, optionally support NOI prompt nodes for few-shot) is appended; the combined prompted graph G_m is processed by a relation-aware GNN (R-GCN style with edge features and attention over layers) and class-node embeddings are used for binary/multi-class prediction.",
            "graph_type": "Citation networks, knowledge graphs, molecular graphs, web-link graphs (a general protocol for any graph where nodes/edges are describable by text)",
            "representation_properties": "Unifies heterogeneous feature spaces into a single LLM-derived embedding space (co-embedding of node/edge attributes and task descriptions); interpretable (human-readable text features); domain-aware (LLM embeddings separate domains into subspaces); expressive when combined with GNN (retains explicit graph structure via GNN message passing rather than relying purely on text); supports in-context learning via prompt graph (enables few-shot/zero-shot); depends on quality/size of LLM encoder; requires designing descriptive text templates, so compactness and exact faithfulness depend on the textual generation rules.",
            "evaluation_task": "Node classification, link prediction, graph classification evaluated in supervised, few-shot (N-way K-shot) and zero-shot settings across nine datasets (Cora, PubMed, ogbn-arxiv, Wiki-CS, FB15K237, WN18RR, ChEMBL, PCBA, HIV).",
            "performance_metrics": "Accuracy, AUC, APR used. Representative results (reported in paper): OFA joint models achieve comparable or better supervised performance than GCN/GAT/GIN baselines across datasets; example few-shot/zero-shot: OGbN-arxiv 5-way 5-shot OFA-joint-lr ~61.45±2.56% (accuracy), FB15K237 20-way 5-shot OFA-joint-lr ~82.56±1.58% (accuracy), HIV 2-way 5-shot AUC ~63.58±1.81. (Multiple dataset-specific metrics and standard deviations are reported in paper tables.)",
            "comparison_to_other_representations": "Compared against classical GNNs using raw features (GCN, GAT, GIN) and several LLM/GNN hybrids, OFA's TAG encoding + prompting enables a single GNN to be trained across domains and tasks; joint TAG training often outperforms independently trained models, and larger/more capable LLM encoders tend to yield better joint performance. Ablation vs a pooling-based alternative (\"-Class node\") shows OFA's prompt-node + class-node design retains performance when datasets are trained jointly while the pooling approach degrades.",
            "limitations_or_challenges": "Requires manual design of textual feature templates and reliance on chosen LLM encoder; current OFA handles classification only (not regression); cross-domain training data is still limited relative to LLMs; textualization risks omitting some fine-grained numerical or structural details unless explicitly encoded; LLM quality affects final performance and convergence; careful sampling/resampling required to prevent small datasets from being neglected in joint training.",
            "uuid": "e5258.0",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GraphText",
            "name_full": "GraphText (Graph reasoning in text space)",
            "brief_description": "A family of methods that describe graph entities and relations in natural language and feed the textual serialization into LLMs for graph reasoning / downstream tasks.",
            "citation_title": "Graphtext: Graph reasoning in text space.",
            "mention_or_use": "mention",
            "representation_name": "Natural-language serialization of graph structure and attributes",
            "representation_description": "Graph nodes and edges (and sometimes relations) are described in natural language and concatenated into text prompts which are given to LLMs. The representation emphasizes textual encoding of entities and (optionally) relations; structural relationships are expressed via textual tokens and sentences.",
            "graph_type": "General graphs; knowledge graphs and text-attributed graphs (as applied in literature cited).",
            "representation_properties": "Leverages LLM semantic understanding of textual descriptions; simple and human-interpretable; may be weak at preserving precise graph structural inductive biases because structural topology is represented implicitly in text rather than via explicit graph message passing.",
            "evaluation_task": "Graph reasoning tasks where LLMs generate answers from textual graph descriptions (paper cites general use in node/link/graph tasks in related work).",
            "performance_metrics": "Paper mentions LLM-based methods can perform zero/few-shot tasks; no unified numerical metrics provided in this paper for GraphText specifically (paper treats these works as baselines/related work).",
            "comparison_to_other_representations": "Paper notes text-only approaches (including GraphText) can lose important structural features; OFA contrasts by using LLM text encodings for attributes but preserving explicit structure via a GNN, leading to stronger performance on structure-sensitive tasks.",
            "limitations_or_challenges": "Implicit structure encoding via text may fail to capture graph topology precisely; susceptible to length/formatting limits of text prompts; potential information loss for structure-heavy tasks.",
            "uuid": "e5258.1",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "InstructGLM / Natural language is all a graph needs",
            "name_full": "Natural language is all a graph needs (InstructGLM-style methods)",
            "brief_description": "Methods that convert graph information into natural language prompts and ask LLMs to perform graph tasks directly from those textual descriptions.",
            "citation_title": "Natural language is all a graph needs",
            "mention_or_use": "mention",
            "representation_name": "Graph-to-text instructions / descriptive serialization",
            "representation_description": "Encode nodes/edges/relations and sometimes local subgraph context as natural-language sentences or instructions which are provided as input to an LLM; the LLM produces answers (class labels, link existence, etc.) from the text.",
            "graph_type": "Various graphs; often applied to small graphs and reasoning over knowledge or molecular descriptions.",
            "representation_properties": "Taps LLM world knowledge and in-context learning; offers high interpretability; may not preserve fine-grained structural signals unless explicitly described; susceptible to verbosity and LLM hallucinations.",
            "evaluation_task": "Node/link/graph-level tasks via LLM generation; zero/few-shot evaluation commonly reported in related work.",
            "performance_metrics": "Paper does not report numerical results for InstructGLM here; mentions such LLM-only graph description methods can do zero/few-shot but may miss structural details.",
            "comparison_to_other_representations": "Paper contrasts these LLM-text-only approaches with OFA: text-only approaches risk losing structural information, while OFA combines LLM textual encoding of attributes with a GNN to preserve structure.",
            "limitations_or_challenges": "Loss of explicit topology, reliance on LLM reasoning, and potential unreliability in mathematical/quantitative prediction or structure-heavy tasks.",
            "uuid": "e5258.2",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "NLGraph",
            "name_full": "NLGraph",
            "brief_description": "A method that describes graph structure and entities in natural language and feeds the text to LLMs for graph tasks (referenced as a related approach in the paper).",
            "citation_title": "NLGraph",
            "mention_or_use": "mention",
            "representation_name": "Text serialization of graph entities and relations",
            "representation_description": "Represent nodes/edges/relations as natural-language descriptions; use LLMs to perform reasoning on the serialized text.",
            "graph_type": "General graphs, including knowledge graphs and text-attributed graphs.",
            "representation_properties": "Human-readable and leverages LLM semantics; potentially loses explicit structural inductive biases if structural detail is not carefully encoded.",
            "evaluation_task": "Graph reasoning tasks and question-answering over graph content (as cited in related work).",
            "performance_metrics": "Not reported in this paper (cited as related work).",
            "comparison_to_other_representations": "Paper groups NLGraph with other LLM-text approaches and notes their weakness on structure-sensitive tasks compared to hybrid LLM+GNN designs like OFA.",
            "limitations_or_challenges": "Implicit representation of topology in text; possible information loss and dependence on prompt design.",
            "uuid": "e5258.3",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT4Graph",
            "name_full": "GPT4Graph (Can large language models understand graph structured data? An empirical evaluation and benchmarking)",
            "brief_description": "Study/approach that feeds textual descriptions of graph data to general-purpose LLMs (e.g., GPT-4) to evaluate their ability to understand and reason about graph-structured data.",
            "citation_title": "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking",
            "mention_or_use": "mention",
            "representation_name": "Text serialization / descriptive prompts of graphs",
            "representation_description": "Convert graph nodes/edges/relations and subgraphs into natural language descriptions to create prompts for LLMs, then evaluate the LLM's reasoning/output.",
            "graph_type": "General graphs (benchmarking across multiple graph types).",
            "representation_properties": "Exposes LLM's ability to interpret textual graph encodings; interpretable evaluation pipeline; limited for complex topology unless carefully described.",
            "evaluation_task": "Empirical benchmarking of LLMs on graph understanding tasks (QA, link inference, etc.).",
            "performance_metrics": "This paper references GPT4Graph as related work; no direct numerical reproduction here.",
            "comparison_to_other_representations": "Paper cites GPT4Graph alongside other LLM-text approaches and emphasizes structural information loss risk; OFA is presented as a hybrid that avoids that pitfall.",
            "limitations_or_challenges": "Benchmarking shows LLMs may struggle when structural nuances are not captured textually; scaling to large graphs is challenging due to prompt size.",
            "uuid": "e5258.4",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LMMOL",
            "name_full": "LMMOL (Can large language models empower molecular property prediction?)",
            "brief_description": "Approaches that use SMILES or other serial chemical representations as text inputs to LLMs for molecular property prediction and zero/few-shot tasks.",
            "citation_title": "Can large language models (llms) empower molecular property prediction?.",
            "mention_or_use": "mention",
            "representation_name": "SMILES / sequence-based textual encoding of molecules",
            "representation_description": "Represent molecules as SMILES strings or other linear textual notations and feed them into LLMs to predict properties, potentially supplementing with natural-language descriptions of atoms/bonds.",
            "graph_type": "Molecular graphs (serialized as SMILES or textual descriptions).",
            "representation_properties": "Compact, fits token-based LLM input; captures chemical connectivity implicitly via SMILES ordering; may not preserve all local subgraph symmetries; benefits from LLM pretraining on chemical text or sequences.",
            "evaluation_task": "Molecular property prediction, few-shot and zero-shot molecular tasks.",
            "performance_metrics": "Paper cites LMMOL as related work and notes these methods can perform zero/few-shot learning; no direct numerical comparisons provided here.",
            "comparison_to_other_representations": "The paper notes SMILES/sequence-only representations can be effective but are an implicit structural encoding and may miss details that graph-based GNNs + TAGs preserve; OFA uses explicit atom/bond textual templates plus GNN structure to mitigate losses.",
            "limitations_or_challenges": "Implicit representation of graph topology can be ambiguous (SMILES has canonicalization issues); LLMs may not reason about structural motifs as effectively as structure-aware GNN decoders.",
            "uuid": "e5258.5",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GIMLET",
            "name_full": "GIMLET (A unified graph-text model for instruction-based molecule zero-shot learning)",
            "brief_description": "A graph–text hybrid for molecular zero-shot learning that combines textual inputs and graph reasoning for molecule property tasks.",
            "citation_title": "Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning.",
            "mention_or_use": "use",
            "representation_name": "Instructional graph-text encoding (molecule descriptions + SMILES/text)",
            "representation_description": "Use text descriptions or SMILES as inputs into an LLM together with instructions; combine textual embedding with graph-aware components to predict molecular properties in zero/few-shot setups.",
            "graph_type": "Molecular graphs (graph-text hybrid).",
            "representation_properties": "Designed for zero-shot molecular property prediction; leverages instruction tuning; may rely on LLM pretraining and instruction design quality.",
            "evaluation_task": "Molecular property prediction (graph-level tasks), evaluated in few-shot/zero-shot settings (e.g., ChEMBL/PCBA/HIV datasets).",
            "performance_metrics": "Cited in paper as a baseline for graph-level few-shot tasks; reported GIMLET performance (in literature) but the OFA paper does not reprint GIMLET numerical figures in full—used for comparison on AUC metrics for molecular datasets.",
            "comparison_to_other_representations": "OFA compares favorably in few-shot/zero-shot molecular evaluations in some settings; OFA emphasizes explicit GNN structure plus TAGs, while GIMLET focuses on instruction-based LLM solutions.",
            "limitations_or_challenges": "Instruction-based LLM methods may underperform when explicit graph topology is crucial; reliance on instruction quality and LLM capacity.",
            "uuid": "e5258.6",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT (Graph instruction tuning for large language models)",
            "brief_description": "A method that augments LLM inputs with trainable structural vectors and an alignment module, concatenated with textual descriptions to improve LLM structure reasoning.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models.",
            "mention_or_use": "mention",
            "representation_name": "Text + trainable structural vectors with alignment",
            "representation_description": "Represent graph entities as text but also include trainable vector tokens encoding structure; an alignment module maps graph structure to the LLM input space, concatenating structural vectors with textual input to help LLMs reason about topology.",
            "graph_type": "General graphs (knowledge graphs, etc.).",
            "representation_properties": "Hybrid: keeps human-readable text while augmenting with learned structural tokens to mitigate purely text-based loss of topology; requires training alignment and additional vectors.",
            "evaluation_task": "Graph reasoning tasks targeted at improving LLM structural understanding (cited as related work).",
            "performance_metrics": "Paper references GraphGPT; no numerical reproduction in OFA paper.",
            "comparison_to_other_representations": "Paper positions GraphGPT as an orthogonal approach to OFA: GraphGPT augments LLM inputs with learned structure tokens, while OFA encodes attributes via LLM and preserves structure with an explicit GNN; both aim to address LLMs' structural limitations.",
            "limitations_or_challenges": "Requires training additional alignment modules and learned vectors; may increase parameter count and complexity; still relies on effective alignment to LLM token space.",
            "uuid": "e5258.7",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Explanations-as-Features",
            "name_full": "Explanations as features: LLM-based features for text-attributed graphs",
            "brief_description": "Approach that uses LLM-generated explanations or textual features as input features for GNNs on text-attributed graphs.",
            "citation_title": "Explanations as features: Llmbased features for text-attributed graphs.",
            "mention_or_use": "mention",
            "representation_name": "LLM-derived textual explanation features",
            "representation_description": "Use LLMs to produce textual explanations or derived text features for nodes/edges and convert these into embeddings which serve as node/edge features for downstream GNNs.",
            "graph_type": "Text-attributed graphs (graphs where nodes/edges have text metadata).",
            "representation_properties": "Produces semantically rich features leveraging LLM explanations; interpretable and may aid downstream GNNs; quality tied to LLM explanation fidelity.",
            "evaluation_task": "Node/link/graph classification on text-attributed graphs (cited as related work).",
            "performance_metrics": "Referenced as related work in OFA; no numeric results reproduced in OFA paper.",
            "comparison_to_other_representations": "Related to OFA's use of LLM embeddings for attributes; OFA differs by systematically templating attributes into TAGs and introducing prompting and prompt-graph design for in-context learning.",
            "limitations_or_challenges": "Depends on explanation quality and potential verbosity; might introduce redundant or noisy signals if explanations are inconsistent.",
            "uuid": "e5258.8",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Prodigy",
            "name_full": "Prodigy (Enabling in-context learning over graphs)",
            "brief_description": "A method that converts classification tasks into link-prediction problems on prompt graphs to enable in-context learning on graphs; used as a baseline in OFA experiments.",
            "citation_title": "Prodigy: Enabling in-context learning over graphs.",
            "mention_or_use": "use",
            "representation_name": "Prompt graph conversion to link-prediction",
            "representation_description": "Transforms classification tasks into link-prediction on an augmented prompt graph where virtual/ prompt nodes encode task context; uses prompt graphs and graph encoders to perform in-context learning and few-shot classification.",
            "graph_type": "General graphs; prompt-augmented subgraphs for node/link/graph-level tasks.",
            "representation_properties": "Prompt-graph based, enabling in-context/few-shot learning without retraining the whole model for each task; relies on learned prompt vectors and graph augmentation to present support examples and task description.",
            "evaluation_task": "Few-shot node and link classification tasks; used as a baseline for OFA's few-shot/zero-shot evaluations.",
            "performance_metrics": "Used in comparisons: OFA reports comparable or better few-shot performance than Prodigy on several benchmarks (e.g., on ogbn-arxiv and FB15K237 tasks in paper tables). Specific example: on ogbn-arxiv 5-way 5-shot, Prodigy ~61.09±5.85% vs OFA-joint-lr ~61.45±2.56% (accuracy) as reported in paper tables.",
            "comparison_to_other_representations": "Prodigy uses prompt graphs with trainable prompt vectors; OFA differs by (1) converting raw attributes into human-readable TAGs encoded by an LLM and (2) using a single GNN trained across multiple domains with NOI prompt nodes and class nodes enabling zero-shot predictions without retraining prompts. OFA often matches or exceeds Prodigy in experiments reported.",
            "limitations_or_challenges": "Prodigy's prompt vectors often need re-training per downstream shot/setting; may require pretraining on large corpora (e.g., MAG240M or Wiki) for transfer; less explicit use of LLM-encoded semantic attribute embeddings compared to OFA's TAG pipeline.",
            "uuid": "e5258.9",
            "source_info": {
                "paper_title": "One for All: Towards Training One Graph Model for All Classification Tasks",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graphtext: Graph reasoning in text space.",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Natural language is all a graph needs",
            "rating": 2,
            "sanitized_title": "natural_language_is_all_a_graph_needs"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning.",
            "rating": 2,
            "sanitized_title": "gimlet_a_unified_graphtext_model_for_instructionbased_molecule_zeroshot_learning"
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models.",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "Explanations as features: Llmbased features for text-attributed graphs.",
            "rating": 2,
            "sanitized_title": "explanations_as_features_llmbased_features_for_textattributed_graphs"
        },
        {
            "paper_title": "Prodigy: Enabling in-context learning over graphs.",
            "rating": 2,
            "sanitized_title": "prodigy_enabling_incontext_learning_over_graphs"
        },
        {
            "paper_title": "Can large language models (llms) empower molecular property prediction?.",
            "rating": 1,
            "sanitized_title": "can_large_language_models_llms_empower_molecular_property_prediction"
        }
    ],
    "cost": 0.01989275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>12 Jul 2024</p>
<p>Hao Liu liuhao@wustl.edu 
Washington University in St. Louis</p>
<p>Jiarui Feng feng.jiarui@wustl.edu 
Washington University in St. Louis</p>
<p>Lecheng Kong 
Washington University in St. Louis</p>
<p>Ningyue Liang fliang@wustl.edu 
Washington University in St. Louis</p>
<p>Dacheng Tao dacheng.tao@ntu.edu.sg 
Nanyang Technological University</p>
<p>Yixin Chen 
Washington University in St. Louis</p>
<p>Muhan Zhang 
Peking University</p>
<p>12 Jul 2024BEC819264DD99F1331E11649E25A960DarXiv:2310.00149v3[cs.LG]
Designing a single model to address multiple tasks has been a long-standing objective in artificial intelligence.Recently, large language models have demonstrated exceptional capability in solving different tasks within the language domain.However, a unified model for various graph tasks remains underexplored, primarily due to the challenges unique to the graph learning domain.First, graph data from different areas carry distinct attributes and follow different distributions.Such discrepancy makes it hard to represent graphs in a single representation space.Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies.Finally, an appropriate graph prompting paradigm for in-context learning is unclear.We propose One for All (OFA), the first general framework that can use a single graph model to address the above challenges.Specifically, OFA proposes text-attributed graphs to unify different graph data by describing nodes and edges with natural language and uses language models to encode the diverse and possibly cross-domain text attributes to feature vectors in the same embedding space.Furthermore, OFA introduces the concept of nodes-of-interest to standardize different tasks with a single task representation.For in-context learning on graphs, OFA introduces a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning.We train the OFA model using graph data from multiple domains (including citation networks, molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its ability in supervised, fewshot, and zero-shot learning scenarios.OFA performs well across different tasks, making it the first general-purpose across-domains classification model on graphs.</p>
<p>INTRODUCTION</p>
<p>Recently, large language models (LLMs) have received tremendous attention due to their power and versatility in solving natural language tasks like text generation, machine translation, and questionanswering.LLMs' in-context learning ability and universality allow the model to directly perform various cross-domain downstream tasks by providing related context or prompt to the model, therefore avoiding any fine-tuning on model parameters (Brown et al., 2020;Zhang et al., 2023b;Lu et al., 2021;Bommasani et al., 2021).</p>
<p>Despite the great success of foundation models on language, developing a foundation model for graph structure data is less explored.Particularly, several challenges unique to graph data prevent the direct transfer of foundation model design from the language domain to the graph domain.First, although the natures of language tasks differ, they are still uniformly represented in humaninterpretable texts.An LLM can encode them into the same text embedding space and train on different source tasks together.However, graph datasets from different sources are usually completely different in feature representation.Concretely, widely used graph datasets include citation Cross-domain texts in graphs and task descriptions can be co-embedded in the same space by an LLM.OFA's graph prompting paradigm converts the input with embedded features to prompted graphs with a unified task representation, which allows adaptive downstream prediction.</p>
<p>networks (Yang et al., 2016;Hu et al., 2020), e-commerce networks (Shchur et al., 2018), knowledge graphs (Dettmers et al., 2018;Toutanova &amp; Chen, 2015), and molecular graphs (Dwivedi et al., 2020).Their raw forms contain attributes generated from isolated processes.For example, node features in molecular graphs are usually vectors whose entries are indices of nominal features of atoms.</p>
<p>In contrast, node features in e-commerce networks could be Bag-of-Word vectors of item descriptions.These features are so different in dimension, scale, and semantic meanings that it is almost impossible to directly learn the representation of these data using the same model.Second, different downstream tasks in the graph domain attend to different parts of the graph and require taskspecific knowledge and methodologies.Specifically, graph-related tasks can be roughly divided into three classes: node-level, link-level, and graph-level.Even though Graph Neural Networks (GNNs) achieved great success in all three task classes, the rationale for the success behind each task class is different.For node-level tasks, proper smoothing of the node features leads to good performance (Defferrard et al., 2016;Chien et al., 2021;He et al., 2021).However, for link-level and graph-level tasks, encoding the local structure is vital to the success, encouraging a line of work that develops more expressive GNNs (Xu et al., 2018;Zhang &amp; Li, 2021;Zhang &amp; Chen, 2018).</p>
<p>Generally, a powerful model for node-level tasks may not work on link-level or graph-level tasks.Consequently, current models are incompetent and infeasible to learn different tasks jointly.Third, the design of the in-context learning or prompt is straightforward in natural language, where we can simply add a description of the task or a few examples to the input.However, there is no existing solution to add such context information to the graph generically.How to design a unified way to perform cross-domain and in-context learning on the graph tasks is ambiguous.</p>
<p>To address these challenges, we propose One-for-All (OFA), a general solution for building and training a foundation GNN model with in-context learning ability across different domains.OFA has three main unique features: (1) OFA uses text-attributed graphs (TAGs) to integrate graph datasets from different domains into one large TAG dataset and leverages the power of LLMs to learn from all domains jointly.We collect nine graph datasets commonly used in the community varying in size, domains, and task types (see Table 1 for the full list).Then, we describe all nodes and edges in the graphs using human-readable texts and embed the texts from different domains into the same embedding space with a single LLM.</p>
<p>(2) OFA proposes the nodes-of-interest (NOI) subgraph and the NOI prompt node, which not only unify different types of graph tasks but also improve the ability of the foundation model to learn the structural information in the graph.(3) OFA introduces a carefully designed and widely applicable graph prompting paradigm (GPP) that inserts a prompt graph into the original input graph in a task-specific way.The nodes in the prompt graph contain all related information about the downstream task (described by texts and encoded by the same LLM encoder as the input graph).Then, the modified graph becomes the actual input to the foundation graph model.Thus, the model is adaptive to perform different tasks according to the prompt graphs.Figure 1 illustrates the pipeline of OFA.After training, the users can describe any graph with natural texts and apply the OFA pipeline to predict possibly unseen classes.We evaluate the proposed OFA on all collected TAG datasets under supervised, few-shot, and zeroshot scenarios.We demonstrate that a single OFA model can perform well on cross-domain and cross-task scenarios.In particular, we show that the OFA achieves great results on zero-shot learning, which is impossible for most of the existing graph models.</p>
<p>PRELIMINARIES</p>
<p>Text-attributed graphs (TAGs).We define a TAG as a graph where each node and each edge in the graph is associated with a text sentence.We denote a TAG as
G = (V, E, R), where V = {v 1 , . . . , v |V| } is the set of nodes, R = {r 1 , . . . , r |R| } is the set of relations, E = {e 1 , . . . , e |E| } is the set of edges. An edge e ij = (v i , r, v j ) ∈ E consists of a source node v i ∈ V, a relation r ∈ R,
and a target node v j ∈ V. Let s vi denote the text sentence associated with node v i and s eij denote the text sentence associated with edge e ij ∈ E. Finally, let N k (v) denote the set of all neighbor nodes within k hops of node v.Note that some concurrent works also introduce the concept of TAGs (Chen et al., 2023;He et al., 2023).However, they only focus on graphs whose raw node features are already texts.On the contrary, we extend this concept and regard all graphs as TAGs since any nodes and edges are describable by texts.</p>
<p>Learning scenarios.In this work, we focus on the classification problem.Denote a dataset as
D = {(d i , y i )} D 1
, where D is the number of data in the dataset, d i is a data sample, y i ∈ Y is the label of d i and Y is the set of all data labels.To train a classifier, we split the dataset into the train, validation, and test sets, denoted as D train , D val , and D test respectively.Their label sets are Y train , Y val , and Y test .We focus on three learning scenarios.In supervised learning, the model will be trained on D train and be evaluated on D val to determine the best model.Finally, D test is used to evaluate the model's performance.All labels in the validation and test data are seen during training, that is, Y train = Y test .The second learning scenario is few-shot learning.The training and evaluation procedure of few-shot learning is similar to supervised learning.However, in fewshot learning, we have Y train Y test = ∅.Few-shot learning typically deals with N -way K-shot tasks, where we use
N • K data {(d i , y i )} N •K 1
as support samples, such that each distinct class is provided with K labeled data and N is the total number of distinct classes.Next, given these support samples, the model needs to classify data in the query set Q = {d i } n 1 into these N classes.The third learning scenario is zero-shot learning, which can be viewed as a special case of few-shot learning.In zero-shot learning, for any N -way K-shot task, we have K = 0 as there are no support samples.</p>
<p>In-context learning in language.In-context learning mainly refers to the ability of the model to learn tasks given only a few examples in the form of demonstration (Dong et al., 2023).For the language model, this is mainly achieved by the prompting mechanism.The pretrained LLM model takes the demonstration as input, which is provided by the prompt text C.Then, the answer to the task is given by generating the rest of the sentence conditioned on C (Brown et al., 2020).</p>
<p>ONE-FOR-ALL: TOWARDS FOUNDATION MODEL ON GRAPH</p>
<p>The proposed OFA is a general graph learning framework that uses one model to simultaneously solve classification tasks varying in formats and backgrounds, similar to LLMs that can answer substantially different questions using the same model weight.Figure 1 illustrates the pipeline of OFA.OFA can be divided into three parts.First, graphs from different domains are integrated into text-attributed graphs with the same format, allowing a single LLM to embed all TAGs into the same space.In the second part, OFA unifies different task types in the graph domain by introducing the Nodes-of-Interest (NOI) subgraph and NOI prompt node, where a graph model can attend to taskrelevant information automatically.Finally, OFA proposes the Graph Prompting Paradigm (GPP) that organically injects task information into the graph data, enabling in-context learning.</p>
<p>3.1 UNIFYING GRAPH DATA FROM DIFFERENT DOMAINS WITH TAGS One critical challenge in building a foundation model for graphs is that cross-domain graph data are usually generated by entirely different procedures and have node/edge attributes embedded in different spaces.This makes graph models trained on one domain almost impossible to generalize to another domain.However, despite the distinct attributes across datasets, almost all can be described by human-interpretable language.For example, in molecular graphs where nodes represent atoms, we can use plain text to describe the node with atomic features, including element names, chirality, etc.The key advantage is that by using text to describe nodes and edges, we can apply an LLM to encode different graph attributes into the same space.Consequently, we introduce the concept of TAGs to integrate graph data from different domains systematically.</p>
<p>Specifically, we design a standardized format for text feature generation of any nodes and edges in graph data.The text feature format for nodes is shown below:</p>
<p>Text feature of nodes: Feature node.<feature description>: <feature content>; <feature description>: <feature content>; ... Example: Feature node.Atom: Carbon, Atomic number 6, helix chirality, is not in a ring, ... Example: Feature node.Paper title and abstract: Attention is all you need.The dominant sequence transduction models are ... Given a TAG G, the text feature s vi always starts with the text Feature node. to indicate that this node is an input node with features from the original graph as opposed to prompt nodes, which will be introduced in Section 3.3.Next, the text describes the type of a feature, followed by the content of the feature.If there are multiple features for a node, they are joined by semicolons.The construction of the text feature s eij for edge e ij is similar, except the start of the text is Feature edge.</p>
<p>Text feature of edges: Feature edge.<feature description>: <feature content>; <feature description>: <feature content>; ... Example: Feature edge.Chemical Bond: ionic bonding, is conjugated, ... Example: Feature edge.Citation from one paper to another.</p>
<p>Following the protocol, we meticulously collected nine graph datasets widely recognized as benchmarks in numerous downstream tasks.This collection encompasses graph data from various domains, including citation networks, molecular graphs, knowledge graphs, and more.Additionally, it covers nearly all classification tasks employed in the research community, i.e., node classification, link prediction, and graph classification.We provide the detailed summarization of all the collected datasets in OFA in Table 1 and detailed collection and processing protocol in Appendix B.1.</p>
<p>As mentioned above, we can apply an LLM encoder to encode all text features into a fixed-length vector as the final input feature of all nodes/edges.Namely, for node v i and edge e ij , their vector representations are defined as x i = LLM(s vi ) and x ij = LLM(s eij ).Because the LLM-encoded input features contain domain information, the subsequent pipeline can capture and leverage this information.Generally, any kind of LLM can be used as the encoder, and a stronger LLM potentially yields better overall performance.In OFA, we evaluate and compare the performance of different LLMs (further discussion in Section 5).We also provide a visualization of all generated OFA datasets in Appendix B.2.</p>
<p>UNIFYING DIFFERENT GRAPH TASKS WITH NODES-OF-INTEREST</p>
<p>Downstream classification tasks in the graph domain can be divided into different categories like:</p>
<p>(1) node-level tasks, where the task is to classify a node in the graph; (2) link-level tasks, where the task is to reason about the connection between a node pair; (3) graph-level tasks, where the task is to make prediction on the whole graph.However, tasks at different levels need to be handled by distinct procedures and methods, which makes the construction of a foundation model for graphs difficult.In contrast, different downstream tasks in language share the same autoregressive generation nature, which makes the knowledge learned from the next-token prediction task used in LLMs uniformly beneficial to various downstream tasks.Then the question arises: Can we unify different graph tasks into a single task to facilitate the training and knowledge transferring in the graph domain?</p>
<p>In OFA, we propose Nodes-of-Interest (NOI) subgraph and NOI prompt node to achieve the goal.The term NOI refers to the set of target nodes in a task, illustrated by the blue nodes in Figure 2, and is represented as T .NOI is not limited to the listed levels of tasks, and its size depends on the prediction target.An NOI subgraph is defined as the subgraph around the NOI.Denote
S h (v) = {V h v , E h v , R h v }
as the h-hop ego-subgraphs around v, consisting of h-hop neighbor nodes of v and all interconnecting edges.A NOI subgraph G h (T ) combines ego-subgraphs of all nodes in NOI,
G h (T ) = v∈T S h (v) = { v∈T V h v , v∈T E h v , v∈T R h v }.(1)
For node-level tasks on node v, NOI is the node itself, so T = {v} and G h (T ) = S h (v).For linklevel tasks on node pair (v i , v j ), we have
T = {v i , v j } and G h ({v i , v j }) = S h (v i ) S h (v j ).
For graph-level tasks, NOI contains all nodes in the graph, and the NOI subgraph is
G h (V) = (V, E, R).
Then, we define the NOI prompt node to unify the processing and readout procedures in different task types.The NOI prompt node is associated with a task prompt text: The text is encoded by the same LLM as other text in G.The NOI prompt node connects to all nodes in NOI, as illustrated by double concentric circles in Figure 2. Through message passing, the NOI prompt node summarizes information in the NOI and the task description.We can then attach class nodes to the NOI prompt node for downstream tasks, which we will explain further in Section 3.3.While concurrent works also utilize subgraphs to unify different types of tasks (Sun et al., 2023;Liu et al., 2023c), these approaches mainly leverage the subgraph concept to transform tasks into a graph-level task, without a NOI prompt node design.In contrast, with the NOI prompt node, we do not require any explicit pooling mechanism, distinguishing our method from previous ones.The combination of NOI subgraph and NOI prompt nodes in our design achieves a unified readout and treatment for all node-level, link-level, and graph-level tasks.Further, the NOI prompt node connected to NOI can be viewed as a labeling trick, which uplifts the expressive power of the original graph model to better learn structural information around the NOI (Zhang et al., 2021).Moreover, the task prompt text on the NOI prompt node allows the graph model to adjust the readout parameters according to the specific task, which is not feasible in existing works.</p>
<p>GRAPH PROMPTING PARADIGM FOR GRAPH IN-CONTEXT LEARNING</p>
<p>One of the most fascinating properties of LLMs is their ability of in-context learning through prompting, which allows the model to perform various downstream tasks in different learning scenarios without fine-tuning.For example, in a few-shot scenario where the goal is to predict the category of a paper based on its title and abstract, we can provide LLMs with k papers from each category as context and instruct the model to generate predictions based on the provided context.However, research on performing in-context learning for graphs remains relatively uncharted.</p>
<p>We recognize that the core principle of in-context learning involves manipulating the input data to align it with downstream tasks.Hence, we propose the Graph Prompting Paradigm (GPP) to manipulate the input graph so that the graph model can acquire task-relevant information from the input itself.Such a paradigm endows the graph model with in-context learning ability for both seen and unseen classes, enabling zero-shot learning.Concretely, a prompt graph, denoted as P = (V p , E p , R p ) has two types of nodes.The first node type is the NOI prompt node, which we have introduced in section 3.2.Suppose we are querying a target NOI subgraph G q h (T q ) = (V h q , E h q , R h q ), and the NOI prompt node is p q .GPP adds edges between the NOI prompt node and every node in NOI, as illustrated by the dotted line in Figure 2. We denote them by  {(t, r t2p , p q ), (p q , r p2t , t)|t ∈ T q }.Note that r t2p and r p2t are the relation types for edges from NOI to the NOI prompt node and the reverse edges, respectively.The second node type in the prompt graph is called the class node.Each class node holds text information related to a specific class.Denote the class node for class i by c i .We add edges between every class node and the NOI prompt node as illustrated by the gray lines in Figure 2, denoted as:
E q cross = NOI Graph (a) Node-level task (b) Link-level task (c) Graph-E query = {(p q , r q2c , c i ), (c i , r c2q , p q )|i ∈ [N ]},
where N is the number of classes.r q2c and r c2q specify the edge relation type from the NOI prompt node to a class node and the reverse.Overall, the prompt graph P = (V p , E p , R p ) is given by:
V p = {p q } {c i |i ∈ [N ]}, E p = E query E q cross , R p = {r t2p , r p2t , r q2c , r c2q }.(2)
Then, the prompted graph fed to the subsequent graph model is the combination of the input graph and prompt graph, denoted as
G m = (V h q V p , E h q E p , R h q R p ).
We use a graph learning model to process the prompted graph and use the embeddings of the class nodes to make binary classification.Specifically, let h ci be the vector representation of class node c i from the graph learning model.We predict the likelihood of the NOI belonging to class i by
P [NOI belongs to class i] = σ(MLP(h ci )),(3)
where MLP is a Multi-layer Perceptron whose 1-dimensional output represents the classification score of h ci .Note that because the NOI prompt node and class nodes connect to the NOI and contain task text description, the fixed-length vector h ci contains information about both the input graph and task, making the prediction task-dependent.While existing graph learning methods need to use different pooling mechanisms for different tasks, this formulation turns different levels of tasks into the same binary classification tasks on class nodes so all tasks can be trained together.For multi-class problems, we compare the prediction scores of different classes to make the decision.
l = argmax i (MLP(h ci )|i ∈ [N ]) ,(4)
l is the predicted class of the NOI.Apart from the generality on task levels, because the graph model is oblivious to the class node order and can inductively infer the class based on the class text description on the class node, the users can attach arbitrary unseen class nodes with proper text descriptions to the NOI prompt node.In such cases, the model can predict the unseen class nodes based on its experience with seen class nodes whose text description is semantically close to the unseen ones, facilitating zero-shot learning.</p>
<p>The GPP can also prompt few-shot problems, where support NOI subgraphs of unseen classes are provided to help classify the query NOI subgraph better.The support NOI subgraphs are denoted by G i,k h (T i k ) for the i-th class and k-th support sample and the NOI T i k belong to class i.As for the query NOI prompt node, we connect each support NOI subgraph to its corresponding support NOI prompt node p i,k by
E s cross = i∈[N ],k∈[K] E i,k cross = i∈[N ],k∈[K] {(t, r t2p , p i,k ), (p i,k , r p2t , t)|t ∈ T i k }.(5)
Then, to augment classification, we connect the support NOI prompt node to the class node that its NOI belongs to, as illustrated by the few-shot section in Figure 2.That is,
E supp = {(p i,k , r s2c , c i )|i ∈ [N ], k ∈ [K]}.
Because the relation type r s2c differs from r q2c between the query NOI prompt node and class nodes, the model can differentiate information from query and support NOI.The overall components of the few-shot prompt graph P are
V p = {p q } {p i,k |i ∈ [N ],k ∈ [K]} {c i |i ∈ [N ]}, E p = E q cross E query E s cross E supp , R p = {r t2p , r p2t , r q2c , r c2q , r s2c }.(6)
The prompted graph can be constructed in a similar way as discussed above.Like in the aforementioned scenarios, the output embeddings of class nodes are used to make binary classifications.OFA utilizes few-shot support examples by connecting the support NOI prompt nodes to the corresponding class nodes, and the model synthesizes both exemplary information and the task semantic information on the class nodes for more accurate predictions.Note that the few-shot class node representations are still consistent with that in zero-shot scenarios, so they can also be trained together.</p>
<p>To summarize, NOI represents the set of nodes related to the task, and the extracted NOI subgraph includes the neighborhood information of NOI.Then, the NOI prompt node summarizes information in the NOI by a graph learning model because all NOI nodes connect to the NOI prompt node.The NOI prompt node is later connected to a set of class nodes with text descriptions.After graph model processing, class node representations contain class information, task information, and NOI information, which can be used to make predictions independently, just like a prompted input to LLM contains the input, target, and task description.The implementation details and training procedure of the model can be found in Appendix C.</p>
<p>RELATED WORKS</p>
<p>The success of the LLM and prompt learning has enlightened many recent works that try to incorporate similar ideas to graphs.The first line of research tries to design prompt learning in the graph domain.Both VNT (Tan et al., 2023) and GraphPrompt (Liu et al., 2023c;Yu et al., 2023) introduce trainable prompt vectors to extract related information for different downstream tasks.HG-PROMPT (Yu et al., 2024a) further extends GraphPrompt to heterogeneous graphs.Prodigy (Huang et al., 2023) converts classification tasks to link prediction problems on prompt graphs, facilitating in-context graph learning.All-in-one (Sun et al., 2023) proposes to learn prompt graphs from downstream tasks.Our work also uses prompting to unify all classification tasks.More importantly, unlike existing work that still needs to train separate GNNs in different domains, we leverage language models to unify different tasks further and use one GNN to solve all tasks.The second line of research combines language models with GNNs.Some works directly apply LLMs to solve graph problems by describing the graph using natural language and feeding text description to LLMs, including InstructGLM (Ye et al., 2023), GraphText (Zhao et al., 2023b), NLGraph (Wang et al., 2023) and GPT4Graph (Guo et al., 2023).However, these methods also describe graph connections by texts, losing important structural features, while our method explicitly utilizes the information through GNNs.Very recently, GraphGPT (Tang et al., 2023) introduced a new method to encode the graph structure information with trainable vectors and an alignment module.More related works can be found in Appendix A.</p>
<p>EXPERIMENTS</p>
<p>The experiment section assesses OFA's potential to serve as a graph foundation model by answering the following questions: Q1: How does replacing the raw node/edge features with text features from LLM affect GNN performance?Q2: Using text as features for all graphs, is a single OFA GNN versatile to tasks in all domains?Q3: What is the effect of different LLMs?Q4: Is the proposed graph prompting paradigm effective in in-context learning?By answering these questions, we validate the approach of using TAGs and OFA's ability to solve various tasks, demonstrating the strong potential of using OFA as a unified graph foundation model.More experiment and training details can be found in Appendix D.</p>
<p>CROSS-DOMAIN SUPERVISED LEARNING</p>
<p>To answer Q1-Q3, we conduct experiments on the supervised learning scenario using all collected OFA datasets with different LLMs.Specifically, we select four popular LLMs for evaluation, including sentence transformer (Reimers &amp; Gurevych, 2019), e5-large-v2 (Wang et al., 2022a), Llama2-7b, and Llama2-13b (Touvron et al., 2023).Then, we evaluate OFA in two different settings.The first setting trains and tests the model on each dataset independently with text embedding generated by the sentence transformer, denoted as OFA-ind-st.The second setting trains a single model using all datasets jointly.We denote the joint model utilized different LLMs as OFA-st, OFA-e5, OFA-llama2-7b, and OFA-llama2-13b respectively.For baseline methods, we use GCN (Kipf &amp; Welling, 2017), GAT (Veličković et al., 2018), and GIN (Xu et al., 2018) for fair comparison.The results can be found in Table 2 and Table 3.</p>
<p>From the results, we have the following observations: (1) Both the independent and joint training achieve comparable or better results on all datasets compared to baseline methods.</p>
<p>(2) OFA successfully enabled a single graph model to be effective on all graph datasets across different domains as the joint version with all different LLMs performs well on all datasets.Further, we can see that the joint version OFA-st achieves better results on most of the datasets compared to OFA-ind-st.This may indicate that by leveraging the text feature and GPP, the knowledge learned from one domain/dataset can be useful for the learning of other domains/datasets.(3) The comparison of different LLMs is interesting, generally speaking, a larger LLM can achieve better and more stable performance in joint training.We also observe a faster convergence for larger LLM (Llama2-13b).However, the margin is less significant.Meanwhile, different LLMs seem specialized for different domains.For example, Llama2 achieves better performance in citation networks but e5-large-v2 achieves great results in molecular datasets.To further investigate the mechanism behind the OFA, we take the output embedding of NOI prompt nodes from OFA-joint-st for each dataset and project it to two-dimensional space.As shown in Figure 3, node embeddings from different domains are separated.This demonstrates that the OFA model can represent data from different domains in different sub-spaces to process it.In Appendix E, we conduct additional ablation studies to verify the effectiveness of the proposed GPP.</p>
<p>FEW-SHOT AND ZERO-SHOT LEARNING</p>
<p>To answer Q4, we design few-shot and zero-shot scenarios for all levels of tasks.We consider both transductive and transfer situations.In the transductive setting, the task is to classify unseen classes on the same training graph.In the transfer setting, both the test graph and test classes are unseen.For simplicity, all experiments are conducted using OFA datasets generated from the sentence transformer.We train one few-shot model on various tasks varying N -way and k-shot, where N ≥ 2 and k ≥ 0. We include ogbn-arxiv, FB15K237, and Chemble as training sets, then evaluate two transductive settings: ogbn-arxiv and FB15K237, and four transfer settings: Cora, WN18RR, HIV, and PCBA.We present node/link/graph level results in For node-level tasks, we compare with meta-learning models (Ding et al., 2020;Wang et al., 2022c;b) and graph contrastive learning model (Tan et al., 2022).For graph-level tasks, LLMbased models Galactica-1.3B(Taylor et al., 2022) and GIMLET (Zhao et al., 2023a) are considered.Prodigy Huang et al. (2023) follows a different setting, where the model is trained on the MAG240M or Wiki datasets (Hu et al., 2021) and transferred to the corresponding tasks.OFA exhibits comparable or better performance than most existing works on few-shot tasks.Especially in the transfer setting of node tasks, where all baselines are trained and evaluated on the Cora dataset, OFA still shows comparable performance without any prior knowledge about the test dataset, illustrating its capability to generalize.Furthermore, our proposed GPP endows OFA with the ability to address zero-shot scenarios-a task generally impossible for most existing baseline models.OFA utilizes one single model to address all low-resource tasks across domains, demonstrating the ability of in-context learning.</p>
<p>A RELATED WORKS (EXTENDED)</p>
<p>Large Language Model and Prompt Learning.Since the ground-breaking advances in Large Language Models, including GPT (Brown et al., 2020;Wei et al., 2022) and LLaMa (Touvron et al., 2023), tremendous attention has been drawn to developing and using them.One particular approach is prompt learning.By prompting the LLMs according to a carefully designed paradigm, users can uncover LLM's surprising capability in many difficult tasks.Chain-of-thoughts, by providing stepby-step reasoning examples, greatly improve LLMs' reasoning power.Considerable efforts were also made to use prior semantic knowledge learned by LLMs to perform zero-shot and few-shot tasks (Reynolds &amp; McDonell, 2021;Liu et al., 2023b;Chowdhery et al., 2022).The key advantage of prompting LLMs is that no further fine-tuning is required for the model to adapt to a new class of tasks.Innovated by this practical advantage, several works adapted the prompting idea to the GNN domain.VNT (Tan et al., 2023) offers a new method for few-shot node classification (FSNC) by integrating trainable virtual node embeddings into the original graph, which serves as a form of prompting.For each new FSNC task, the pre-trained graph transformer model can be frozen and only the embedding of virtual nodes needs to be trained.GraphPrompt (Liu et al., 2023c;Yu et al., 2023) also introduces trainable prompt vectors to extract related information for the downstream tasks.Meanwhile, it further utilizes the subgraph and link prediction to design a novel unsupervised pertaining method, which unifies the pertaining and downstream tasks.2023) proposes to learn prompt graphs from data.Our work also uses prompting to unify all classification tasks.However, our works differ in two parts.First, unlike existing work that still needs to train separate GNNs in different domains, we leverage language models to unify different tasks further and use one GNN to solve all tasks.Second, most of the existing works cannot perform zero-shot learning as their prompt module needs to be re-trained for different downstream tasks.instead, OFA directly describes all tasks in a unified way, which allows the model to perform zero-shot learning without further training.</p>
<p>Graph Neural Networks.Recently, extensive efforts have been spent on using GNNs to learn relational data.Earlier GNN variants, including GCN (Kipf &amp; Welling, 2017), GAT (Veličković et al., 2018), and GraphSage (Hamilton et al., 2017), achieved great success in solving graph learning problems.Later works unify the GNNs into the Message Passing Neural Network (Gilmer et al., 2017) and show the expressivity upper-bound of such framework (Xu et al., 2018;Morris et al., 2019), which demonstrates the inherent difference between learning on graphs and learning on other data formats such as images and languages where expressivity is not an issue.Following such observation, subsequent works, such as SEAL (Zhang &amp; Chen, 2018) and ID-GNN (You et al., 2021), propose subgraph GNNs that apply GNNs to subgraphs and aggregate subgraph representations to enhance the expressivity.Moreover, unlike MPNN, which fits the full graph into the GPU memory, subgraph GNNs process the data by subgraphs, solving the scalability problem of MPNN.Innovated by subgraph GNNs, our work and concurrent works (Huang et al., 2023;Sun et al., 2023) also add prompts to subgraphs of interest, which allows our models to perform cross-level tasks.</p>
<p>The applications of GNN have also become prevalent.Much research focuses on the molecular property prediction domain and has achieved promising performance (Zhang et al., 2023a;Feng et al., 2023;Kong et al., 2023;Feng et al., 2022).GNNs have also become one of the primary tools in citation network analysis and mining (Kipf &amp; Welling, 2017;Chien et al., 2022).Many efforts have also been made to adapt GNNs to the Knowledge Graph mining domain and achieved great success due to GNN's efficiency and inductive learning capabilities (Zhu et al., 2021;Kong et al., 2022).While the promising results of these applications demonstrate the potential of GNNs, a critical problem is that they are tailored to the specified application.On the contrary, our work, by three carefully designed components, unifies all tasks into one GNN framework that allows largescale training and inference across different domains.The proposed framework can be a foundation model in the graph learning domain.</p>
<p>Language Models and Graph Neural Networks.The surge of foundational Large Language Models inspires several directions combining language models with GNNs.One of them directly applies LLMs to solve graph problems.They propose graph descriptive languages to represent the structural information as prompt texts and feed them to LLMs, like InstructGLM (Ye et al., 2023), Graph-Text (Zhao et al., 2023b), NLGraph (Wang et al., 2023), and GPT4Graph (Guo et al., 2023).Some works for molecular graph classification propose to only use SMILE molecule sequences to describe the molecular graph in input to the LLM, including LMMOL (Qian et al., 2023) and GIMLET (Zhao et al., 2023a).The exceptional power of LLMs enables these methods to perform difficult tasks such as zero/few-shot learning.However, such graph representation is implicit, and the LLM might not correctly capture the structural information but only summarize texts corresponding to the nodes that appear in the prompt.Such approaches will fall short in applications where graph structures are important.</p>
<p>Table 7: A comparison between OFA and related methods.</p>
<p>In-context Few-shot Zero-shot Cross-tasks Cross-domain GNN
LMMOL ✓ ✓ ✓ GIMLET ✓ ✓ ✓ InstructGLM ✓ ✓ ✓ ✓ ✓ GraphText ✓ ✓ ✓ ✓ ✓ NLGraph ✓ ✓ ✓ ✓ ✓ GPT4GRAPH ✓ ✓ ✓ ✓ ✓ ExpAsFeat ✓ ✓ VNT ✓ ✓ ✓ LLMForGraph ✓ ✓ ✓ ✓ PRODIGY ✓ ✓ ✓ GraphPrompt ✓ ✓ ✓ ✓ All-in-One ✓ ✓ ✓ ✓ OFA ✓ ✓ ✓ ✓ ✓ ✓
Very recently, GraphGPT (Tang et al., 2023) introduced a new method to encode the graph structure information with trainable vectors and an alignment module.It concatenates the trainable vectors along with textual input to LLms to improve the structure reasoning ability of LLMs.Another direction uses LLMs to encode the corresponding texts of nodes, such as paper abstracts, and apply GNN to the graph with encoded text embeddings (He et al., 2023;Chen et al., 2023).The high-quality text representation from LLMs allows these models to achieve better performance.Our approach adopts the same idea of using text embedding to represent graph entities (nodes and edges).However, we are the first work that drives the language model to full power by systematically unifying graph entities in different domains with the same language protocol and representing them in the same embedding space.This consequently grants cross-domain functionality that other GNN frameworks lack.Table 7 summarizes the characteristics of different models.We can see that OFA is the most versatile model among existing works.</p>
<p>B MORE ON OFA DATASET B.1 DATASET COLLECTION AND CONSTRUCTION</p>
<p>In this section, we discuss the detailed processing procedure for each dataset collected in OFA.</p>
<p>B.1.1 CORA</p>
<p>Cora is a citation network that contains papers and their citation relationship in the computer science domain.The raw text data of the Cora dataset was collected from the GitHub repository provided in Chen et al. (2023).Each node in Cora represents a research paper from the computer science domain.The raw text feature of a node is the title and abstract of the respective paper.Every edge in the Cora dataset indicates the citation relationship between papers.Each node's label corresponds to the category of the paper.Tasks that can be executed on Cora include predicting the paper's category (node-level) or identifying missing citation links within the graph (link-level).Using the proposed processing protocol, we reformat all text features in Cora.Particularly, for each node category, we further use gpt-3.5-turbo(ChatGPT) to generate a description as additional information.In table 8, we show a processed example on Cora dataset.</p>
<p>B.1.2 PUBMED</p>
<p>Cora is a citation network that contains papers and their citation relationship in the biomedical domain.The raw text data of the PubMed dataset was collected from the GitHub repository provided in Chen et al. (2023).All nodes and edges are similar to Cora dataset and we use the exact same processing procedure as Cora dataset.Because its original literature categories are diabetes, experimental/diabetes, type 1/diabetes, and type 2, which are overly simple and very difficult for our BERT-based LLM to distinguish, we asked ChatGPT to generate a detailed description of each category.Cora is a citation network that contains papers and their citation relationship collected from Arxiv platform.The raw text data of the ogbn-arxiv was collected using the same protocol as the GitHub repository provided in Prodigy (Huang et al., 2023).For ogbn-arxiv, we only evaluate the model on the node classification task.All nodes and edges are similar to Cora dataset and we use the exact same processing procedure as Cora dataset except that the description for each category is directly obtained from the Prodigy.</p>
<p>B.1.4 WIKI-CS</p>
<p>Wiki-CS is an Internet link network with each node representing a Wikipedia page and each edge representing the reference link.The raw text of the Wiki-CS dataset was collected from the official website (Mernyei &amp; Cangea, 2020).The raw text feature of a node is the name and content of an entry in Wikipedia.Each node's label corresponds to the category of the entry.We evaluate Wiki-CS on node classification tasks.In table 9, we show a processed example on the Wiki-CS dataset using the processing protocol proposed in OFA.</p>
<p>B.1.5 FB15K237</p>
<p>FB15K237 is a knowledge graph that contains knowledge base relation triples and textual mentions of Freebase entity pairs.The raw text data of nodes in FB15K237 was collected from GitHub repository2 .The raw text feature of a node is the name of the relation entity and its description.The raw text feature of an edge is the type of relation between two entities.We provide an example of processed data example in Table 10.</p>
<p>B.1.6 WN18RR</p>
<p>WN18RR is a knowledge graph, which is a subset of WordNet that consists of 18 relations and 40943 entities.The raw text data of nodes in WN18RR was collected from GitHub repository 2 .The raw text feature of nodes and edges are the same as FB15K237 and we follow the same process protocol to process the WN18RR dataset.2018) is a subset of the BioChem BioAssay dataset consisting of 128 labels on the biological activities of small molecules.</p>
<p>(3) MOLHIV dataset Wu et al. (2018) contains over 40,000 compounds labeled for their ability to inhibit HIV replication.The raw data of these datasets are represented in SMILE string format.We use RDKit to construct molecule objects and graph structures from the SMILE string and use natural language to describe the atoms (nodes) and bonds (edges) as shown in Table 11.For the class nodes text, we use the assay/molecule properties description in Zhao et al. (2023a).</p>
<p>B.2 VISUALIZATION OF OFA DATASET</p>
<p>In this section, we provide a visualization of all generated OFA datasets using the sentence transformer.Concretely, For each generated dataset, we randomly select 400 node embeddings and project it to 2 dimensions using TSNE.Note for molecular datasets, we include all node embeddings.The result is shown in Figure 4. We can see that the generated embeddings from different domains are successfully separated by the LLM as the embedding from molecular datasets, knowledge graphs, wiki pages, and citation networks are well separated in the visualization.Moreover, the LLM can even separate the citation network from different research domains.embeddings from Arxiv and Cora, which mainly contain papers from computer science are close to each other and far from the embeddings of Pubmed, which focus on biology.This result reveals one of the key rationales behind the success of the OFA.That is, by encoding the datasets from different domains into different sub-spaces, OFA allows one GNN to learn the information from different domains separately without impacting each other.</p>
<p>C IMPLEMENTATION OF OFA</p>
<p>The last section described how OFA combines LLMs, text-attributed graphs, and the graphprompting paradigm to facilitate cross-domain and in-context learning.In this section, we illustrate the training and evaluation procedure of OFA.</p>
<p>Training an OFA pipeline for different tasks is straightforward with the unified task representation introduced in Section 3 because all the inputs are standardized to the prompted text attributed graph G m regardless of the task type.The task information is carried out in the prompt nodes and edges.</p>
<p>Recall that OFA first embeds all texts in graphs to vector representations using LLM:
x ij = LLM (s eij ), ∀e ij ∈ E m , x i = LLM (s vi ), ∀v i ∈ V m .(7)
Then, a GNN processes G m along with the embedded texts through multiple message-passing layers and gets the final node embedding for each node in G m .Formally,
h l+1 vi = W self h l vi + r∈Rm vj ∈N r 1 (vi) 1 |N r 1 (v i )| W r (ReLU(h l vj + x ij )),(8)
where h 0 vi = x i , W self and W r are trainable transformation matrix for self-loop and relations, N r 1 (v i ) is the direct neighbors that connects to v i with relation type r.Our GNN model is a natural extension of R-GCN (Schlichtkrull et al., 2018), incorporating edge features.Such an implementation helps differentiate nodes in the input graph and the prompt graph.It also effectively extracts useful information from the input graph and encodes it to the class node in the prompt graph for prediction.Usually, the last layer output can be used for prediction.However, due to the different natures of tasks and the well-known over-smoothing effect, a GNN with a fixed number of layers will not perform well on all tasks.Hence, we design a simple attention layer to summarize all layer outputs from h 1 to h L .Eventually, the node representation is
h vi = H • Sof tmax((W k H) ′ • W q x i ) ′ , H = [h 1 vi ... h l vi ... h L vi ],(9)
where W k and W q are key and query trainable weights.Note that because the text features x i also include the domain information, by querying on the feature, a properly trained attention layer can choose the most important layers to aggregate outputs based on the domain.
p i = σ(MLP(h ci )), (10)
where σ is the Sigmoid function.If it's a multi-class problem, OFA collects all classes' probabilities as
l p = argmax ((p i |i ∈ [N ])) ,(11)
and l p is the final prediction from the model.The binary training loss for a single G m can be expressed as:
L Gm = − 1 N N i=1 y i • log(p i ) + (1 − y i ) • (1 − log(p i )), (12)
where N is the number of candidate classes in G m .</p>
<p>D EXPERIMENTAL SETTINGS</p>
<p>In this section, we provide detailed settings for all experiments shown in the paper.The code of OFA can be found at the Github link https://github.com/LechengKong/OneForAll.</p>
<p>D.1 SUPERVISED LEARNING EXPERIMENTS</p>
<p>We set the number of layers for GNN of independent training (OFA-ind-st) and joint training to be 6 and 7 respectively, as the joint model might require a larger embedding space.The dropout rate is set to 0.15.We set the hidden size of the GNN as 768 and project the initial embedding generated by different LLMs to 768 before the GNN.We train OFA-ind-st for 100 epochs and 50 for OFA-st and OFA-e5.For OFA-llama2-7b and OFA-llama2-13b, we only train 40 epochs due to the limitation of computing resources.For OFA-ind-st, we evaluate the model after each epoch and take the model with the best validation performance for testing.For all versions of joint training, since there is no easy way to define the best validation performance, we directly use the final model for testing.For Cora-node, Pubmed-node, and ogbn-arxiv datasets, a different split is used.Specifically, for Cora-node and Pubmed-node, the split is obtained from Chen et al. (2023), where they split data into 10 folds and we use the first fold as our split.For ogbn-arxiv, in each experiment, we will randomly split data with a train/val/test ratio of 0.8/0.1/0.1.For Cora-node, Pubmed-node, ogbnarxiv, WN18RR, and FB15K237, we rerun the baseline models 10 times and report the average performance.For other datasets, we report results from existing works.</p>
<p>D.2 FEW-SHOT AND ZERO-SHOT LEARNING EXPERIMENTS</p>
<p>For OFA joint training low resource (OFA-joint-lr) experiments, we set the number of layers to be 5, the learning rate as 0.0001, and the number of epochs to be 30.We split the label of ogbn-arxiv dataset with ratio [20, 10, 10] for train/validation/test, and use the [142,47,48] as the split ratio for FB15K237 dataset.During training, we use the training set from Arxiv (node-level), the training set from FB15K237 (link-level), and the whole Chemble dataset (graph-level) to train a single model for all downstream tasks.We construct diverse N -way k-shot tasks from the training sets.For ogbnarxiv dataset, N varies from 3 to 5, k varies from 0 to 5; for FB15K237 dataset, N varies from 3 to 10, k varies from 0 to 5; for ChEMBL dataset, all tasks are 2-way, and k varies from 0 to 10.</p>
<p>In few-shot graph construction, the text feature of class nodes differs significantly from that in supervised and zero-shot scenarios.Specifically, we omit category information and utilize a uniform text feature across all class nodes.This uniformity shifts the focus from learning class-specific information to enhancing the model's ability to compare the query node with support nodes.Such comparisons are crucial for determining matches, which proves particularly effective in few-shot scenarios where the model must classify unseen classes.</p>
<p>During the test, we involved six datasets: for the transductive setting, we evaluate on the test set from ogbn-arxiv dataset and test set from FB15K237 dataset; for the transfer setting, we evaluate on Cora (node-level), WN18RR (link-level), MOLPCBA and MOLHIV (graph-level).Note that for the transductive setting, we keep the same train/validation/test labels for OFA and all the baselines to ensure a fair comparison.For baseline results on the Cora dataset, we report the performance provided by COLA (Liu et al., 2023a), which is the average performance of 20 random label splits.</p>
<p>For baseline results of Prodigy, we follow the provided code to pre-train separate models for different shot numbers, that is, we pre-train 5-shot/3-shot/1-shot models for node-level tasks and pre-train 5-shot/3-shot/1-shot models for link-level tasks, then evaluate different shot settings using the corresponding pre-trained model.For the baselines of graph-level tasks, we report the results from GIMLET (Zhao et al., 2023a).</p>
<p>E MORE EXPERIMENTAL RESULTS</p>
<p>This section includes more experimental results that provide a detailed evaluation of OFA's incontext learning ability and justify the OFA design by ablation study.</p>
<p>E.1 ABLATION STUDY</p>
<p>While using LLM to unify graph and task representation is a unique design in OFA, there are alternatives to our prompting paradigm GPP to make classifications.One alternative is to discard graph prompting completely but keep track of the NOI for each NOI subgraph.After being processed by the graph model, the embeddings of the NOI are summarized by average pooling to form the final NOI representation, the LLM encoded tasks representation is concatenated to the NOI for binary classification.We denote this as "-Class node".We perform a case study comparing the two methods on hiv, ogbn-arxiv, Cora-node, and Cora-link datasets when these datasets are trained jointly using the same model and separately.The results are presented in Table 12.</p>
<p>We observe that, if the datasets are trained separately, all methods achieve similar performance, since in end-to-end training for one dataset the prompting design is essentially a pooling mechanism.However, it is striking that the "-Class node" approach's performance significantly drops when the datasets are trained together, while the OFA prompting approach maintains the original performance.</p>
<p>Intuitively, when datasets from different domains are trained together, the model needs to learn which domain a particular data is from and which tasks are being performed to make appropriate predictions.However, without the NOI prompt node that carries task text descriptions, the "-Class node" approaches can confound tasks in different domains.</p>
<p>E.2 FEW-SHOT AND ZERO-SHOT RESULTS</p>
<p>To explore the in-context learning ability of OFA, we train a single model for all few-shot and zeroshot low-resource tasks denoted as OFA-joint-lr.Here, we provide more comprehensive experiment results spanning more ways and shots.We also implement three experiments that train separately on node-, link-, and graph-level tasks denoted as OFA-ind-lr.For node-level tasks, we train the model using the ogbn-arxiv dataset.For link-level tasks, we train the model on the FB15K237 dataset.For graph-level tasks, we train the model on Chemble.</p>
<p>Results for the ogbn-arxiv and Cora can be found in Table 13 and Table 16, respectively.We can see that for the Cora dataset, OFA-joint-lr achieve better performance than OFA-ind-lr in all setting.This may indicate that the knowledge learned from other task levels like link-or graph-level can help the generalization of the model on Cora.For ogbn-arxiv dataset, the results for OFA-joint-lr and OFA-ind-lr are similar.</p>
<p>The results for FB15K237 and WN18RR datasets can be found in Table 14 and Table 15, respectively.For link-level task, the OFA-ind-lr have better results than OFA-joint-lr.This may indicate that the knowledge learned from graph or node level is not helpful for link-level tasks.</p>
<p>Finally, the results for HIV and PCBA datasets can be found in Table 17 and Table 18, respectively.We can notice that joint training can benefit the prediction of HIV in most cases and the zero-shot scenario of PCBA, but the performance of few-shot tasks of PCBA dropped significantly.One reason might be the different number of tasks used for training: the graph-related tasks involved in individual training are much more than those in joint training.</p>
<p>F LIMITATIONS AND FUTURE WORKS</p>
<p>While OFA aims to provide a solution for the general graph foundation model, it is not yet able to perform regression tasks, because regression targets can be unbounded in values.Hence, if the range of all target values in a zero-shot regression task falls out of the range that OFA is trained on, it is very  difficult for OFA to predict the correct target value in that range, which is why we focus on general classification in this work.A potential approach is to specify a target range in the NOI prompt node task description, and let the model predict regression value based on the specified range.However, reasoning about math concepts is difficult even for most advanced LLMs, and certainly unreliable for our current adopted LLMs.Hence, we leave such an approach to future work.</p>
<p>While OFA is already trained in several different domains and the performance is on par or even outperforms some GNN and LLM approaches, the training data for the graph foundation model is still scarce compared to that of LLMs.Also, LLMs explore training techniques beyond supervised training, including auto-regressive training, and contrastive learning, which largely improve LLM's ability to model data and generalize to unseen tasks.Other unsupervised training techniques are possible, like the one proposed in GraphPrompt (Liu et al., 2023c).We believe these training techniques can further enhance the performance of OFA and regard this as an important direction to explore in the future.</p>
<p>Figure 1 :
1
Figure1: The pipeline of OFA.An input to the model contains a text-attributed graph and a task description.Cross-domain texts in graphs and task descriptions can be co-embedded in the same space by an LLM.OFA's graph prompting paradigm converts the input with embedded features to prompted graphs with a unified task representation, which allows adaptive downstream prediction.</p>
<p>Figure 2 :
2
Figure 2: In-context learning design in OFA</p>
<p>Figure 3 :
3
Figure 3: Output embedding space of NOI prompt nodes on all datasets for OFA-joint-st.</p>
<p>B. 1
1
.7 MOLECULAR DATASETS We adopted three molecular datasets: (1) ChEMBL dataset Gaulton et al. (2012) is a widely used molecule property prediction dataset.It contains 1,310 prediction target labels of molecules from biological assays for drug discovery.(2) MOLPCBA dataset Wu et al. (</p>
<p>Figure 4 :
4
Figure 4: Embedded node features from all OFA datasets (sentence transformer).</p>
<p>Because datasets vary in sample size, smaller sets might be neglected during training.Hence, we sample from each dataset by a multiple of the dataset size to form the training set and control the weight of each dataset.Data are resampled after every epoch.The multipliers are Cora-link: 1.5, Cora-node 2, Pubmed-link: 0.5, Pubmed-node: 2.5, ogbn-arxiv: 0.7, WN18RR: 0.6, FB15K237: 0.4, WikiCS: 2, ChEMBL: 1, PCBA: 2, HIV: 4. For independent and joint training, we repeat experiments 10 and 3 times, respectively, and report the mean results and standard deviation.The final training set contains all training sets of the collected datasets, and the test sets are the datasets' original test sets.</p>
<p>Table 1 :
1
Detailed summarization of all collected datasets in OFA.
DatasetDomainTask# Graphs Avg. #Nodes Avg. #Edges # ClassesCoraCitationNode/Link12,70810,5567PubMedCitationNode/Link119,71744,3383ogbn-arxivCitationNode1169,3431,166,24340Wiki-CSWeb linkNode111,701216,12310FB15K237 KnowledgeLink114,541310,116237WN18RRKnowledgeLink140,94393,00311PCBAMoleculeGraph437,92926.028.1128HIVMoleculeGraph41,12725.527.52ChEMBLMoleculeGraph365,06525.955.91048</p>
<p>Text feature of the NOI prompt node: Prompt node.<task description>.Example: Prompt node.Graph classification on molecule properties.Example: Prompt node.Node classification on the literature category of the paper.</p>
<p>Table 2 :
2
Results on supervised learning (first).22±0.48 73.21±0.7398.69±0.1077.80±2.60 77.48±0.1777.75±0.7474.45±3.55OFA-llama2-13b 94.53±0.5174.76±1.2298.59±0.1078.25±0.7177.51±0.1777.65±0.2276.71±1.19
CoraCora 1PubMed PubMed 1 ogbn-arxiv 1 Wiki-CSHIVTask typeLinkNodeLinkNodeNodeNodeGraphMetricAUC ↑Acc ↑AUC ↑Acc ↑Acc ↑Acc ↑AUC ↑GCN90.40±0.20 78.86±1.48 91.10±0.50 74.49±0.99 74.09±0.17 79.07±0.10 75.49±1.63GAT93.70±0.10 82.76±0.79 91.20±0.10 75.24±0.44 74.07±0.10 79.63±0.10 74.45±1.53OFA-ind-st91.87±1.03 75.61±0.87 98.50±0.06 73.87±0.88 75.79±0.11 77.72±0.65 73.42±1.14OFA-st94.04±0.49 75.90±1.26 98.21±0.02 75.54±0.05 75.54±0.11 78.34±0.35 78.02±0.17OFA-e592.83±0.38 72.20±3.24 98.45±0.05 77.91±1.44 75.88±0.17 73.02±1.06 78.29±1.48OFA-llama2-7b 94.</p>
<p>Table 3 :
3
Results on supervised learning (second).
coralink coranode pubmedlinkpubmednode arxiv WN18RRFB15K237 wikicschempcba chemhivWN18RR FB15K237PCBATask typeLinkLinkGraphMetricAcc ↑Acc ↑APR ↑GCN67.40±2.40 74.20±1.10 20.20±0.24GIN57.30±3.40 70.70±1.80 22.66±0.28OFA-ind-st97.22±0.18 95.77±0.01 22.73±0.32OFA-st96.91±0.11 95.54±0.06 24.83±0.10OFA-e597.84±0.35 95.27±0.28 25.19±0.33OFA-llama2-7b 98.08±0.16 95.56±0.05 21.35±0.94OFA-llama2-13b 98.14±0.25 95.69±0.07 21.54±1.25</p>
<p>Table 4 :
4
Table 4 / 5 / 6, respectively.The model is denoted by OFA-joint-lr (low-resource).Few-shot and Zero-shot results (Acc) on ogbn-arxiv and Cora (Node-level).</p>
<h1>Wayogbn-arxiv-5-way (Transductive)Cora-2-way (Transfer)Task5-shot3-shot1-shot0-shot5-shot1-shot0-shotGPN50.53±3.07 48.32±3.80 38.58±1.61-63.83±2.86 56.09±2.08-TENT60.83±7.45 56.03±8.90 45.62±10.70-58.97±2.40 54.33±2.10-GLITTER56.00±4.40 57.44±4.90 47.12±2.73----TLP-BGRL 50.13±8.78 46.21±7.92 35.81±8.58-81.31±1.89 59.16±2.48-TLP-SURGL 77.89±6.46 74.19±7.55 61.75±10.07-92.49±1.02 81.52±2.09-Prodigy61.09±5.85 58.64±5.84 48.23±6.18----OFA-joint-lr 61.45±2.56 59.78±2.51 50.20±4.27 46.19±3.83 76.10±4.41 67.44±4.47 56.92±3.09</h1>
<p>Table 5 :
5
Few-shot and Zero-shot results (Acc) on FB15K237 and WN18RR (Link-level).
Table 6: Few-shot and Zero-shot results (AUC) onHIV and PCBA (Graph-level &amp; Transfer setting).# WayFB15K237-20-way (Transductive)WN18RR-5-way (Transfer)# Way TaskHIV-2-way 5-shot 0-shotPCBA-2-way 5-shot 0-shotTask5-shot0-shot5-shot0-shotGalactica-1.3B-33.85-52.02Prodigy74.92±6.03---GIMLET-66.24-62.11OFA-joint-lr 82.56±1.58 70.20±2.40 46.32 ±4.18 30.96±5.46OFA-joint-lr 63.58±1.81 35.67±4.46 51.53±9.94 60.62±5.456 CONCLUSIONS, LIMITATIONS AND FUTURE RESEARCH
In this work, we propose OFA, the first solution towards building the foundation GNN model for learning on graphs.By showing great results on supervised, few-shot, and zero-shot scenarios, OFA reveals great potential as the future foundation model on the graph.Currently, OFA falls short of learning regression tasks and the cross-domain datasets are limited, we leave this to future work.More discussion can be found in Appendix F.</p>
<p>Table 8 :
8
A processed example on Cora/Pubmed/ogbn-arxiv dataset.Node type Text feature Input graph node feature node.literature category and description: <title>.<abstract>
Input graph edge feature edge. co-citation (Cora/Pubmed)Input graph edge feature edge. citation (OGBN-ARXIV)Node classification taskClass nodeprompt node. literature category and description: <category name>.<description>Prompt nodeprompt node. node classification of literature category.Link prediction taskClass nodeprompt node. two papers (do not have/have) co-citationPrompt nodeprompt node. link prediction on the papers that are cited togetherTable 9: A processed example on Wiki-CS dataset.Node typeText featureNode classification taskClass nodeprompt node. wikipedia entry category: <category name>Prompt nodeprompt node. node classification of wikipedia entry category.B.1.3 OGBN-ARXIV
Input graph node feature node.wikipedia entry name: <entry name>.Entry content: <entry content> Input graph edge feature edge.wikipedia page link.</p>
<p>Table 10 :
10
A processed example on FB15K237 dataset.Node type Text feature Input graph node feature node.entity and entity description: <entity name>.<entity descrip-tion> Input graph edge feature edge.relation between two entities: <relation name>
Link prediction taskClass nodeprompt node. relation between two entities: &lt; relation name &gt;Prompt nodeprompt node. relation type prediction between the connected entities.</p>
<p>Table 11 :
11
A processed example on Molecule datasets.Node typeText featureInput graph node feature node.atom.<element name>, <atom chirality>, degree of <atom degree>, formal charge of <formal charge>, num of hydrogen is <number of hydrogen>, num of radical electron is <number of radical electrons>, hybridization is <hybridization>, (is/is not) aromatic, (is/is not) in ring.
Input graph edge feature edge. chemical bond. <bond type> bond, bond stereo is <bondstereo>, (is/is not) conjugatedNode classification taskClass nodeprompt node.
molecule property description.<molecule property description.e.g. the molecule is effective to the following assay:...> Prompt node prompt node.Graph classification on molecule properties.</p>
<p>Lastly, we gather the output node embedding for each class node {h ci |∀i ∈ [N ]} and use an MLP prediction head to perform binary classification on every class node.Even for multi-class problems, each class node connects to the NOI prompt node and hence can integrate the information of other connected class nodes through message-passing.While each class node is processed separately by the MLP, such binary classification is still conditioned on other candidate classes and not individualized.Moreover, since the format of G m is consistent in different learning scenarios (supervised/few-shot/zero-shot) and task types (node/link/graph-level), such a procedure can work without any modification across these situations.Formally, the likelihood of class i is:</p>
<p>Table 12 :
12
Ablation study on different prompting design.
JointSeparateFull-Class nodeFull-Class nodeogbn-arxiv 75.23±0.0375.19±0.1075.06±0.0875.39±0.09HIV75.92±0.4571.60±0.5475.81±0.3675.43±0.50Cora-node 75.48±0.2971.39±0.0775.72±0.4976.12±0.87Cora-link92.27±0.8489.51±0.4693.16±0.9592.80±0.62</p>
<p>Table 13 :
13
Few-shot results (Acc) on ogbn-arxiv (Node-level).</p>
<h1>Way5-way3-wayTask5-shot3-shot1-shot0-shot5-shot3-shot1-shot0-shotGPN50.53±3.07 48.32±3.80 38.58±1.61-62.25±4.94 58.52±3.00 48.45±5.60-TENT60.83±7.45 56.03±8.90 45.62±10.70-74.20±9.93 70.48±11.50 59.38±13.55-GLITTER56.00±4.40 57.44±4.90 47.12±2.73-62.13±10.85 60.93±12.12 59.20±5.48-BGRL50.13±8.78 46.21±7.92 35.81±8.58-62.93±11.74 58.37±11.34 46.30±10.83-SURGL77.89±6.46 74.19±7.55 61.75±10.07-86.27±7.54 83.75±8.86 73.46±12.68-Prodigy61.09±5.85 58.64±5.84 48.23±6.18-73.64±6.93 71.43±7.28 61.59±8.53-OFA-joint-lr 61.45±2.56 59.78±2.51 50.20±4.27 46.19±3.83 73.22±2.65 72.24±3.81 60.60±3.71 58.87±3.37OFA-ind-lr 59.92±1.32 58.68±6.40 52.80±3.94 46.56±0.82 72.18±3.33 71.80±1.59 60.47±2.65 64.13±0.98</h1>
<p>Table 14 :
14
Few-shot results (Acc) on FB15K237 (Link-level).</p>
<h1>Way20-way10-wayTask5-shot3-shot1-shot0-shot5-shot3-shot1-shot0-shotProdigy74.92±6.03 70.32±6.30 55.49±6.88 5.11±3.07 84.30±7.80 79.61±8.28 66.10±9.89 10.02±6.46OFA-joint-lr 82.56±1.58 81.33±2.54 75.39±2.86 70.20±2.40 90.44±1.95 89.68±1.38 86.38±1.60 70.84±2.31OFA-ind-lr 87.43±1.56 86.42±2.45 83.87±2.88 72.24 ±3.45 93.44±1.57 92.44±0.84 89.16±1.11 82.08±0.81Table 15: Few-shot results (Acc) on WN18RR (Link-level).# Way10-way5-wayTask5-shot3-shot1-shot0-shot5-shot3-shot1-shot0-shotOFA-joint-lr 31.42±1.74 28.46±1.83 26.24±1.96 19.98±3.26 46.32±4.18 44.20±4.58 33.86±3.41 30.96±5.46OFA-ind-lr 32.64±1.56 30.56±1.02 25.82±1.07 20.40±2.86 48.32±3.19 45.04±2.39 34.40±1.47 38.24±1.76</h1>
<p>Table 16 :
16
Few-shot results (Acc)on Cora (Node-level).-lr 48.76±2.65 34.04±4.1028.72±9.9076.10±4.1167.44±4.4756.92±3.09OFA-ind-lr 42.28±2.3531.28±2.6323.68±1.6772.20±3.8262.22±1.1751.85±4.35</p>
<h1>Way5-way2-wayTask5-shot1-shot0-shot5-shot1-shot0-shotOFA-joint</h1>
<p>Table 17 :
17
Results (AUC) on HIV and 2-way tasks.-lr 61.23±1.9063.58±1.8163.99±1.2159.39±2.1035.67±4.46OFA-ind-lr 54.36±4.9057.56±3.6659.30±3.0457.17±1.8251.16±5.89
Task10-shot5-shot3-shot1-shot0-shotOFA-joint</p>
<p>Table 18 :
18
Results (AUC) on PCBA and 2-way tasks -lr 51.64±9.9051.53±9.9451.58±9.5951.72±8.5760.62±5.45OFA-ind-lr 54.58±2.9054.80 ±3.75 54.67±4.3554.92±4.38 52.71±5.57
Task10-shot5-shot3-shot1-shot0-shotOFA-joint
The split we use differs from original one, see Apeendix D for details.
https://github.com/villmow/datasets_knowledge_embedding/tree/master
ACKNOWLEDGEMENT Hao Liu, Jiarui Feng, Lecheng Kong, and Yixin Chen are supported by NSF grant CBE-2225809.Muhan Zhang is supported by the National Key R&amp;D Program of China (2022ZD0160303) and National Natural Science Foundation of China (62276003).
On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, S Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri S Castellon, Annie S Chatterji, Kathleen A Chen, Jared Creel, Dora Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren E Gale, Karan Gillespie, Noah D Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas F Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, O Khani, Pang Wei Khattab, Mark S Koh, Ranjay Krass, Rohith Krishna, Ananya Kuditipudi, Faisal Kumar, Mina Ladhak, Tony Lee, Jure Lee, Isabelle Leskovec, Levent, Lisa Xiang, Xuechen Li, Tengyu Li, Ali Ma, Christopher D Malik, Suvir Manning, Eric Mirchandani, Zanele Mitchell, Suraj Munyikwa, Avanika Nair, Deepak Narayan, Benjamin Narayanan, Allen Newman, Juan Carlos Nie, Niebles, J F Hamed Nilforoshan, Giray Nyarko, Laurel J Ogut, Isabel Orr, Armin W Papadimitriou ; Rohan Taori, Florian Thomas, Rose E Tramèr, William Wang, Bohan Wang, Jiajun Wu, Yuhuai Wu, Sang Wu, Michihiro Michael Xie, Jiaxuan Yasunaga, You, A Matei, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Kaitlyn Zheng, Percy Zhou, Liang, ArXiv, abs/2108.07258Joon Sung Park. Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, H Yusuf, Camilo Roohani, Jack Ruiz, Ryan, Dorsa Christopher R'e, Shiori Sadigh, Keshav Sagawa, Andy Santhanam, Krishna Parasuram Shih, Alex Srinivasan, Tamkin, 2021</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang, 2023</p>
<p>Adaptive universal generalized pagerank graph neural network. Eli Chien, Jianhao Peng, Pan Li, Olgica Milenkovic, International Conference on Learning Representations. 2021</p>
<p>Node feature extraction by self-supervised multi-scale neighborhood prediction. Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic, Inderjit S Dhillon, International Conference on Learning Representations. 2022</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in Neural Information Processing Systems. 2016</p>
<p>Convolutional 2d knowledge graph embeddings. Tim Dettmers, Minervini Pasquale, Stenetorp Pontus, Sebastian Riedel, Proceedings of the 32th AAAI Conference on Artificial Intelligence. the 32th AAAI Conference on Artificial IntelligenceFebruary 2018</p>
<p>Graph prototypical networks for few-shot learning on attributed networks. Kaize Ding, Jianling Wang, Jundong Li, Kai Shu, Chenghao Liu, Huan Liu, Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>A survey on in-context learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui, 2023</p>
<p>Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, arXiv:2003.00982Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. 2020arXiv preprint</p>
<p>How powerful are k-hop message passing graph neural networks. Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, Muhan Zhang, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Extending the design space of graph neural networks by rethinking folklore weisfeiler-lehman. Jiarui Feng, Lecheng Kong, Hao Liu, Dacheng Tao, Fuhai Li, Muhan Zhang, Yixin Chen, </p>
<p>Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Chembl: a large-scale bioactivity database for drug discovery. Anna Gaulton, Louisa J Bellis, Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun Mcglinchey, David Michalovich, Bissan Al-Lazikani, Nucleic acids research. 40D12012</p>
<p>Neural message passing for quantum chemistry. Justin Gilmer, Patrick F Samuel S Schoenholz, Oriol Riley, George E Vinyals, Dahl, International conference on machine learning. PMLR2017</p>
<p>Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking, 2023. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han, Advances in Neural Information Processing Systems. 2017</p>
<p>Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Mingguo He, Zhewei Wei, Zengfeng Huang, Hongteng Xu, Advances in Neural Information Processing Systems. A Beygelzimer, Y Dauphin, P Liang, J Wortman Vaughan, 2021</p>
<p>Explanations as features: Llmbased features for text-attributed graphs. Xiaoxin He, Xavier Bresson, Thomas Laurent, Bryan Hooi, 2023</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, arXiv:2005.006872020arXiv preprint</p>
<p>Ogb-lsc: A large-scale challenge for machine learning on graphs. Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, Jure Leskovec, arXiv:2103.094302021arXiv preprint</p>
<p>Prodigy: Enabling in-context learning over graphs. Qian Huang, Hongyu Ren, Peng Chen, Gregor Kržmanc, Daniel Zeng, Percy Liang, Jure Leskovec, arXiv:2305.126002023arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, International Conference on Learning Representations. 2017</p>
<p>Geodesic graph neural network for efficient graph representation learning. Lecheng Kong, Yixin Chen, Muhan Zhang, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Mag-gnn: Reinforcement learning boosted graph neural network. Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, Muhan Zhang, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Graph contrastive learning meets graph meta learning: A unified method for few-shot node tasks. Hao Liu, Jiarui Feng, Lecheng Kong, Dacheng Tao, Yixin Chen, Muhan Zhang, arXiv:2309.103762023aarXiv preprint</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023b</p>
<p>Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023c</p>
<p>Pretrained transformers as universal computation engines. Kevin Lu, Aditya Grover, P Abbeel, Igor Mordatch, ArXiv, abs/2103.052472021</p>
<p>Wiki-cs: A wikipedia-based benchmark for graph neural networks. Péter Mernyei, Cȃtȃlina Cangea, arXiv:2007.029012020arXiv preprint</p>
<p>Weisfeiler and leman go neural: Higher-order graph neural networks. Christopher Morris, Martin Ritzert, Matthias Fey, Jan William L Hamilton, Gaurav Eric Lenssen, Martin Rattan, Grohe, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2019</p>
<p>Can large language models empower molecular property prediction?. Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu, 2023</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics201911</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. Laria Reynolds, Kyle Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. 2021</p>
<p>Modeling relational data with graph convolutional networks. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den, Ivan Berg, Max Titov, Welling, 10.1007/978-3-319-93417-4_38The Semantic Web: 15th International Conference. Heraklion, Crete, Greece; Berlin, HeidelbergSpringer-Verlag2018. June 3-7, 2018. 2018</p>
<p>Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan Günnemann, arXiv:1811.05868Pitfalls of graph neural network evaluation. 2018arXiv preprint</p>
<p>All in one: Multi-task prompting for graph neural networks. Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan, 10.1145/3580305.3599256Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '23. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '23New York, NY, USA2023Association for Computing Machinery. ISBN 9798400701030</p>
<p>Transductive linear probing: A novel framework for few-shot node classification. Zhen Tan, Song Wang, Kaize Ding, Jundong Li, Huan Liu, arXiv:2212.056062022arXiv preprint</p>
<p>Virtual node tuning for few-shot node classification. Zhen Tan, Ruocheng Guo, Kaize Ding, Huan Liu, 10.1145/3580305.3599541Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '23. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, 2023</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Observed versus latent features for knowledge base and text inference. Kristina Toutanova, Danqi Chen, Workshop on Continuous Vector Space Models and their Compositionality. 2015</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, International Conference on Learning Representations. 2018</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, 2023</p>
<p>Text embeddings by weakly-supervised contrastive pre-training. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv:2212.035332022aarXiv preprint</p>
<p>Graph few-shot learning with task-specific structures. Song Wang, Chen Chen, Jundong Li, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Task-adaptive few-shot node classification. Song Wang, Kaize Ding, Chuxu Zhang, Chen Chen, Jundong Li, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022c</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, S Aneesh, Karl Pappu, Vijay Leswing, Pande, Chemical science. 922018</p>
<p>How powerful are graph neural networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, International Conference on Learning Representations. 2018</p>
<p>Revisiting semi-supervised learning with graph embeddings. Zhilin Yang, William Cohen, Ruslan Salakhudinov, International conference on machine learning. PMLR2016</p>
<p>Natural language is all a graph needs. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, 2023</p>
<p>Identity-aware graph neural networks. Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, Jure Leskovec, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021</p>
<p>Generalized graph prompt: Toward a unification of pre-training and downstream tasks on graphs. Xingtong Yu, Zhenghao Liu, Yuan Fang, Zemin Liu, Sihong Chen, Xinming Zhang, 2023</p>
<p>Hgprompt: Bridging homogeneous and heterogeneous graphs for few-shot prompt learning. Xingtong Yu, Yuan Fang, Zemin Liu, Xinming Zhang, 2024a</p>
<p>Multigprompt for multi-task pretraining and prompting on graphs. Xingtong Yu, Chang Zhou, Yuan Fang, Xinming Zhang, 2024b</p>
<p>A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests. Bohang Zhang, Guhao Feng, Yiheng Du, Di He, Liwei Wang, 2023a</p>
<p>Link prediction based on graph neural networks. Muhan Zhang, Yixin Chen, Advances in Neural Information Processing Systems. 2018</p>
<p>Nested graph neural networks. Muhan Zhang, Pan Li, Advances in Neural Information Processing Systems. 202134</p>
<p>Labeling trick: A theory of using graph neural networks for multi-node representation learning. Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, Long Jin, Advances in Neural Information Processing Systems. A Beygelzimer, Y Dauphin, P Liang, J Wortman Vaughan, 2021</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, Qi Liu, 2023a</p>
<p>Graphtext: Graph reasoning in text space. Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, 2023b</p>
<p>Neural bellman-ford networks: A general graph neural network framework for link prediction. Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, Jian Tang, Advances in Neural Information Processing Systems. 202134</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4336 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4336</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4336</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-276961662</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.09894v2.pdf" target="_blank">What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The scientific literature's exponential growth makes it increasingly challenging to navigate and synthesize knowledge across disciplines. Large language models (LLMs) are powerful tools for understanding scientific text, but they fail to capture detailed relationships across large bodies of work. Unstructured approaches, like retrieval augmented generation, can sift through such corpora to recall relevant facts; however, when millions of facts influence the answer, unstructured approaches become cost prohibitive. Structured representations offer a natural complement -- enabling systematic analysis across the whole corpus. Recent work enhances LLMs with unstructured or semistructured representations of scientific concepts; to complement this, we try extracting structured representations using LLMs. By combining LLMs' semantic understanding with a schema of scientific concepts, we prototype a system that answers precise questions about the literature as a whole. Our schema applies across scientific fields and we extract concepts from it using only 20 manually annotated abstracts. To demonstrate the system, we extract concepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and evolutionary biology. The resulting database highlights emerging trends and, by visualizing the knowledge graph, offers new ways to explore the ever-growing landscape of scientific knowledge. Demo: abby101/surveyor-0 on HF Spaces. Code: https://github.com/chiral-carbon/kg-for-science.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4336.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4336.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surveyor (schema + Llama-3 pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs (domain-agnostic schema + Llama-3 70B extraction pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-agnostic, schema-driven pipeline that uses an LLM (Llama-3 70B Instruct) with few-shot prompting to extract categorized scientific concepts from titles/abstracts, store them in a SQL database, and analyze co-occurrence/temporal patterns via knowledge graphs and SQL queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Schema-guided LLM extraction + KG/SQL analysis (authors' pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The method defines a coarse, cross-domain schema of nine concept categories (models, tasks, datasets, fields, modalities, methods, objects, properties, instruments). It processes paper text sentence-by-sentence using few-shot prompting (demonstrations drawn from 3 demonstration papers / 9 few-shot examples) and an optimized prompt to elicit concept tags in either human-readable or JSON output. Extraction is implemented with Llama-3 70B Instruct, with prompt engineering tuning (varying number/selection of few-shot examples, prompt ordering/structure, input granularity sentence vs paragraph, output format). Extracted tagged concepts are written into a SQL database (papers and predictions tables) and analyzed via aggregate SQL queries; co-occurrence graphs are built by treating concepts as nodes and paper co-occurrence as edges and visualized with a force-directed layout. The pipeline was applied to titles+abstracts and supports downstream queries for modality distributions, temporal trends, and method/instrument adoption patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Llama-3 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astrophysics, Fluid Dynamics, Evolutionary Biology (demonstrated cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>29,980 papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Aggregate statistical patterns and relationships: co-occurrence relationships, modality distributions, temporal trends and adoption patterns (statistical correlations / aggregate counts). The system is not reported to extract symbolic mathematical laws or closed-form equations directly.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured semantic tags (category-labelled concepts) delivered as human-readable tagged text or JSON; stored as SQL tables; visualized as co-occurrence knowledge graphs (nodes/edges) enabling numeric aggregate queries.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Development set of 20 manually annotated papers (3 used as few-shot examples, 17 as dev set) with exact-match metrics computed (precision/recall/F1 measured against manual annotations). Prompt engineering evaluated on this held-out development set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On development set: human-readable output: precision 44% ± 12%, recall 31% ± 11% (exact-match); average processing time ≈ 2.8 s per sentence. JSON output: precision similar, recall ≈ 40% ± 12%; processing time ≈ 4 s per sentence. Additional comparative statistic: RAKE average extracted keyword count ≈ 69.23 vs authors' method ≈ 32.12 (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared qualitatively and quantitatively to RAKE (rake-nltk). RAKE extracted more (avg ≈ 69.23 keywords) but produced flatter, noisier lists; the authors' method extracted fewer concepts (avg ≈ 32.12) but organized them into semantic categories (Table 2/3). No numeric precision/recall reported for RAKE in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Moderate extraction precision and recall (noisy extractions, concept hallucination); ambiguity in category assignment across domains (e.g., named entity vs generic concept); difficulty disambiguating named entities vs generic terms ('Melbourne wind tunnel' vs 'wind-tunnel data'); limited to titles/abstracts (not full text) in experiments; does not extract explicit symbolic/analytical laws (only statistical/structural relationships); scalability costs (processing time per sentence) and remaining noise in extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4336.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4336.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that augments LLMs with retrieval from external corpora to ground generation in retrieved documents, used widely for knowledge-intensive Q&A and synthesis but producing unstructured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combine a retrieval component over a document corpus with an LLM that conditions on retrieved passages to produce answers; used to provide wider context and reduce hallucination. In the paper it is discussed as an unstructured approach that can recall facts across corpora but becomes cost-prohibitive when millions of facts influence answers and is limited for systematic quantitative analyses across large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Various LLMs (unspecified in this paper / general RAG approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / cross-domain (knowledge-intensive tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not intended to extract formal quantitative laws; primarily used for fact retrieval and unstructured synthesis (answers, summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Unstructured natural-language responses conditioned on retrieved documents (not structured symbolic laws).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Paper contrasts RAG conceptually with structured knowledge approaches, arguing RAG is less suited for systematic quantitative analysis at scale; no numeric head-to-head provided in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Cost-prohibitive at extreme scale (when millions of facts influence an answer); limited ability to support systematic, queryable quantitative analyses across corpora; hallucinations in LLM outputs persist without structured representation.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4336.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4336.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMuse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMuse (Gu & Krenn) - LLM + RAKE hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semistructured approach that combines LLM-based concept extraction with the RAKE statistical keyword extraction algorithm to produce keyword/phrase-level concepts from scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciMuse (LLM + RAKE hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines statistical keyphrase extraction (RAKE) with LLM-based processing to extract concepts as keyword phrases; it emphasizes semistructured keyword/phrase extraction rather than role-labelling of concepts. The authors cite SciMuse as a prior semistructured approach.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature (applied by authors of SciMuse to scientific text)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Keyword/phrase extraction and semantic concept identification (not explicit quantitative laws).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Flat keyword/phrase lists (semistructured), not category-labelled tags.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Cited as semistructured prior work; the current paper argues SciMuse-like approaches treat concepts uniformly and do not label functional roles, limiting quantitative analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Produces keyword phrases without distinguishing functional roles (methods vs datasets vs instruments), limiting downstream quantitative analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4336.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4336.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM + vector-KG (astronomy ext.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-driven concept extraction + vector-similarity knowledge graphs (astronomy extension)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach (cited in related work) that uses LLMs to extract concepts and then groups them into knowledge graphs using vector-based semantic similarity for astronomy-specific corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM extraction + vector-similarity knowledge graph construction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use LLMs to extract concepts from texts and then cluster/group concepts via embedding/vector-similarity methods to build knowledge graphs; edges/groupings based on embedding proximity rather than role-based semantic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy (as cited extension), generalizable</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Co-occurrence/semantic similarity patterns and clusters (statistical/semantic relationships), not explicit symbolic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Vector-clustered knowledge graphs (nodes are concept phrases; grouping by embedding similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Paper contrasts role-labelled schema approach with embedding-based grouping approaches; embedding-based methods rely on semantic similarity rather than functional role labels.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Treats all concepts uniformly (no functional role distinctions), which limits ability to answer certain quantitative/role-based queries.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4336.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4336.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain-specific LM + KG embeddings (biomed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-specific language models integrated with knowledge graph embeddings (biomedicine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach combining biomedical language models with knowledge graph embedding techniques to improve performance on knowledge base completion and related tasks, at the cost of field-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Domain-specific LM + knowledge graph embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Integrate domain-specific LMs with knowledge graph embeddings to complete or enrich biomedical knowledge bases; reported to improve performance on domain tasks but requires field-specific finetuning and domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Domain-specific LMs (unspecified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Knowledge-base completion and structured relationship inference (statistical/relational patterns), not necessarily closed-form quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Knowledge graph triples / embedded graph representations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reported improved performance in cited work (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Cited as higher-performing than some baselines but requiring finetuning; no numeric comparisons included here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires domain-specific finetuning and domain expertise; less generalizable across fields without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4336.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4336.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generative-agent style system applied to scientific literature to answer research questions by combining retrieval with LLM generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PaperQA (RAG-based generative agent)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses retrieval over scientific documents combined with LLM generation to answer questions about papers or scientific topics; cited as a related LLM-based system for engaging with scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific literature (general)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Unstructured synthesis and retrieval of facts; not reported to extract formal quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Unstructured text answers, retrieval-augmented summaries</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Subject to hallucination; unstructured outputs limit systematic quantitative analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4336.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4336.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited preprint that explores systems aimed at automated, open-ended scientific discovery, integrating many AI components to propose and test scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI Scientist (automated discovery pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Ambitious research direction combining generative models, experiment planning, and automated hypothesis generation/testing to enable end-to-end automated scientific discovery; cited as related work on AI-driven discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific discovery (cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Potentially discovers hypotheses or relationships; specifics not detailed in this paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>General challenges for fully automated discovery (validation, experimental testing, safety); details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4336.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4336.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predicting research trends (Krenn & Zeilinger)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predicting research trends with semantic and neural networks with an application in quantum physics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that combines semantic and neural network methods to predict research trends; cited as an example of using semantic / neural methods to forecast scientific directions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Semantic + neural-network trend prediction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Applies semantic analysis and neural network models to bibliographic/semantic data to predict research trends; used as related literature showing neural methods for extracting patterns in science.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics / scientometrics (example application in quantum physics)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Trend prediction / statistical forecasting of research topics (statistical patterns), not explicit symbolic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Statistical / predictive outputs (time-series / trend forecasts)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not discussed in detail in this paper; cited as related approach.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders <em>(Rating: 2)</em></li>
                <li>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>Predicting research trends with semantic and neural networks with an application in quantum physics <em>(Rating: 2)</em></li>
                <li>Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study <em>(Rating: 1)</em></li>
                <li>Automatic keyword extraction from individual documents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4336",
    "paper_id": "paper-276961662",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "Surveyor (schema + Llama-3 pipeline)",
            "name_full": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs (domain-agnostic schema + Llama-3 70B extraction pipeline)",
            "brief_description": "A domain-agnostic, schema-driven pipeline that uses an LLM (Llama-3 70B Instruct) with few-shot prompting to extract categorized scientific concepts from titles/abstracts, store them in a SQL database, and analyze co-occurrence/temporal patterns via knowledge graphs and SQL queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Schema-guided LLM extraction + KG/SQL analysis (authors' pipeline)",
            "method_description": "The method defines a coarse, cross-domain schema of nine concept categories (models, tasks, datasets, fields, modalities, methods, objects, properties, instruments). It processes paper text sentence-by-sentence using few-shot prompting (demonstrations drawn from 3 demonstration papers / 9 few-shot examples) and an optimized prompt to elicit concept tags in either human-readable or JSON output. Extraction is implemented with Llama-3 70B Instruct, with prompt engineering tuning (varying number/selection of few-shot examples, prompt ordering/structure, input granularity sentence vs paragraph, output format). Extracted tagged concepts are written into a SQL database (papers and predictions tables) and analyzed via aggregate SQL queries; co-occurrence graphs are built by treating concepts as nodes and paper co-occurrence as edges and visualized with a force-directed layout. The pipeline was applied to titles+abstracts and supports downstream queries for modality distributions, temporal trends, and method/instrument adoption patterns.",
            "llm_model_used": "Llama-3 70B Instruct",
            "scientific_domain": "Astrophysics, Fluid Dynamics, Evolutionary Biology (demonstrated cross-domain)",
            "number_of_papers": "29,980 papers",
            "type_of_quantitative_law": "Aggregate statistical patterns and relationships: co-occurrence relationships, modality distributions, temporal trends and adoption patterns (statistical correlations / aggregate counts). The system is not reported to extract symbolic mathematical laws or closed-form equations directly.",
            "extraction_output_format": "Structured semantic tags (category-labelled concepts) delivered as human-readable tagged text or JSON; stored as SQL tables; visualized as co-occurrence knowledge graphs (nodes/edges) enabling numeric aggregate queries.",
            "validation_method": "Development set of 20 manually annotated papers (3 used as few-shot examples, 17 as dev set) with exact-match metrics computed (precision/recall/F1 measured against manual annotations). Prompt engineering evaluated on this held-out development set.",
            "performance_metrics": "On development set: human-readable output: precision 44% ± 12%, recall 31% ± 11% (exact-match); average processing time ≈ 2.8 s per sentence. JSON output: precision similar, recall ≈ 40% ± 12%; processing time ≈ 4 s per sentence. Additional comparative statistic: RAKE average extracted keyword count ≈ 69.23 vs authors' method ≈ 32.12 (see Table 2).",
            "baseline_comparison": "Compared qualitatively and quantitatively to RAKE (rake-nltk). RAKE extracted more (avg ≈ 69.23 keywords) but produced flatter, noisier lists; the authors' method extracted fewer concepts (avg ≈ 32.12) but organized them into semantic categories (Table 2/3). No numeric precision/recall reported for RAKE in this paper.",
            "challenges_limitations": "Moderate extraction precision and recall (noisy extractions, concept hallucination); ambiguity in category assignment across domains (e.g., named entity vs generic concept); difficulty disambiguating named entities vs generic terms ('Melbourne wind tunnel' vs 'wind-tunnel data'); limited to titles/abstracts (not full text) in experiments; does not extract explicit symbolic/analytical laws (only statistical/structural relationships); scalability costs (processing time per sentence) and remaining noise in extractions.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4336.0",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "An approach that augments LLMs with retrieval from external corpora to ground generation in retrieved documents, used widely for knowledge-intensive Q&A and synthesis but producing unstructured outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Retrieval-Augmented Generation (RAG)",
            "method_description": "Combine a retrieval component over a document corpus with an LLM that conditions on retrieved passages to produce answers; used to provide wider context and reduce hallucination. In the paper it is discussed as an unstructured approach that can recall facts across corpora but becomes cost-prohibitive when millions of facts influence answers and is limited for systematic quantitative analyses across large corpora.",
            "llm_model_used": "Various LLMs (unspecified in this paper / general RAG approaches)",
            "scientific_domain": "General / cross-domain (knowledge-intensive tasks)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Not intended to extract formal quantitative laws; primarily used for fact retrieval and unstructured synthesis (answers, summaries).",
            "extraction_output_format": "Unstructured natural-language responses conditioned on retrieved documents (not structured symbolic laws).",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": "Paper contrasts RAG conceptually with structured knowledge approaches, arguing RAG is less suited for systematic quantitative analysis at scale; no numeric head-to-head provided in this work.",
            "challenges_limitations": "Cost-prohibitive at extreme scale (when millions of facts influence an answer); limited ability to support systematic, queryable quantitative analyses across corpora; hallucinations in LLM outputs persist without structured representation.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4336.1",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SciMuse",
            "name_full": "SciMuse (Gu & Krenn) - LLM + RAKE hybrid",
            "brief_description": "A semistructured approach that combines LLM-based concept extraction with the RAKE statistical keyword extraction algorithm to produce keyword/phrase-level concepts from scientific text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "SciMuse (LLM + RAKE hybrid)",
            "method_description": "Combines statistical keyphrase extraction (RAKE) with LLM-based processing to extract concepts as keyword phrases; it emphasizes semistructured keyword/phrase extraction rather than role-labelling of concepts. The authors cite SciMuse as a prior semistructured approach.",
            "llm_model_used": null,
            "scientific_domain": "General scientific literature (applied by authors of SciMuse to scientific text)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Keyword/phrase extraction and semantic concept identification (not explicit quantitative laws).",
            "extraction_output_format": "Flat keyword/phrase lists (semistructured), not category-labelled tags.",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": "Cited as semistructured prior work; the current paper argues SciMuse-like approaches treat concepts uniformly and do not label functional roles, limiting quantitative analysis.",
            "challenges_limitations": "Produces keyword phrases without distinguishing functional roles (methods vs datasets vs instruments), limiting downstream quantitative analyses.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4336.2",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM + vector-KG (astronomy ext.)",
            "name_full": "LLM-driven concept extraction + vector-similarity knowledge graphs (astronomy extension)",
            "brief_description": "An approach (cited in related work) that uses LLMs to extract concepts and then groups them into knowledge graphs using vector-based semantic similarity for astronomy-specific corpora.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "LLM extraction + vector-similarity knowledge graph construction",
            "method_description": "Use LLMs to extract concepts from texts and then cluster/group concepts via embedding/vector-similarity methods to build knowledge graphs; edges/groupings based on embedding proximity rather than role-based semantic labels.",
            "llm_model_used": null,
            "scientific_domain": "Astronomy (as cited extension), generalizable",
            "number_of_papers": null,
            "type_of_quantitative_law": "Co-occurrence/semantic similarity patterns and clusters (statistical/semantic relationships), not explicit symbolic laws.",
            "extraction_output_format": "Vector-clustered knowledge graphs (nodes are concept phrases; grouping by embedding similarity).",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": "Paper contrasts role-labelled schema approach with embedding-based grouping approaches; embedding-based methods rely on semantic similarity rather than functional role labels.",
            "challenges_limitations": "Treats all concepts uniformly (no functional role distinctions), which limits ability to answer certain quantitative/role-based queries.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4336.3",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Domain-specific LM + KG embeddings (biomed)",
            "name_full": "Domain-specific language models integrated with knowledge graph embeddings (biomedicine)",
            "brief_description": "An approach combining biomedical language models with knowledge graph embedding techniques to improve performance on knowledge base completion and related tasks, at the cost of field-specific fine-tuning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Domain-specific LM + knowledge graph embeddings",
            "method_description": "Integrate domain-specific LMs with knowledge graph embeddings to complete or enrich biomedical knowledge bases; reported to improve performance on domain tasks but requires field-specific finetuning and domain data.",
            "llm_model_used": "Domain-specific LMs (unspecified in this paper)",
            "scientific_domain": "Biomedicine",
            "number_of_papers": null,
            "type_of_quantitative_law": "Knowledge-base completion and structured relationship inference (statistical/relational patterns), not necessarily closed-form quantitative laws.",
            "extraction_output_format": "Knowledge graph triples / embedded graph representations",
            "validation_method": "Reported improved performance in cited work (details not provided in this paper).",
            "performance_metrics": null,
            "baseline_comparison": "Cited as higher-performing than some baselines but requiring finetuning; no numeric comparisons included here.",
            "challenges_limitations": "Requires domain-specific finetuning and domain expertise; less generalizable across fields without retraining.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4336.4",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "PaperQA",
            "name_full": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "brief_description": "A retrieval-augmented generative-agent style system applied to scientific literature to answer research questions by combining retrieval with LLM generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "PaperQA (RAG-based generative agent)",
            "method_description": "Uses retrieval over scientific documents combined with LLM generation to answer questions about papers or scientific topics; cited as a related LLM-based system for engaging with scientific literature.",
            "llm_model_used": null,
            "scientific_domain": "Scientific literature (general)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Unstructured synthesis and retrieval of facts; not reported to extract formal quantitative laws.",
            "extraction_output_format": "Unstructured text answers, retrieval-augmented summaries",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Subject to hallucination; unstructured outputs limit systematic quantitative analysis.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4336.5",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "The AI Scientist",
            "name_full": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "brief_description": "A cited preprint that explores systems aimed at automated, open-ended scientific discovery, integrating many AI components to propose and test scientific hypotheses.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "AI Scientist (automated discovery pipeline)",
            "method_description": "Ambitious research direction combining generative models, experiment planning, and automated hypothesis generation/testing to enable end-to-end automated scientific discovery; cited as related work on AI-driven discovery.",
            "llm_model_used": null,
            "scientific_domain": "General scientific discovery (cross-domain)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Potentially discovers hypotheses or relationships; specifics not detailed in this paper's text.",
            "extraction_output_format": "Not specified in this paper",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "General challenges for fully automated discovery (validation, experimental testing, safety); details not provided here.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4336.6",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Predicting research trends (Krenn & Zeilinger)",
            "name_full": "Predicting research trends with semantic and neural networks with an application in quantum physics",
            "brief_description": "A prior work that combines semantic and neural network methods to predict research trends; cited as an example of using semantic / neural methods to forecast scientific directions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Semantic + neural-network trend prediction",
            "method_description": "Applies semantic analysis and neural network models to bibliographic/semantic data to predict research trends; used as related literature showing neural methods for extracting patterns in science.",
            "llm_model_used": null,
            "scientific_domain": "Physics / scientometrics (example application in quantum physics)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Trend prediction / statistical forecasting of research topics (statistical patterns), not explicit symbolic laws.",
            "extraction_output_format": "Statistical / predictive outputs (time-series / trend forecasts)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Not discussed in detail in this paper; cited as related approach.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4336.7",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
            "rating": 2,
            "sanitized_title": "interesting_scientific_idea_generation_using_knowledge_graphs_and_llms_evaluations_with_100_research_group_leaders"
        },
        {
            "paper_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
            "rating": 2,
            "sanitized_title": "predicting_research_trends_with_semantic_and_neural_networks_with_an_application_in_quantum_physics"
        },
        {
            "paper_title": "Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study",
            "rating": 1,
            "sanitized_title": "scientific_language_models_for_biomedical_knowledge_base_completion_an_empirical_study"
        },
        {
            "paper_title": "Automatic keyword extraction from individual documents",
            "rating": 1,
            "sanitized_title": "automatic_keyword_extraction_from_individual_documents"
        }
    ],
    "cost": 0.014699199999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs
29 May 2025</p>
<p>Abhipsha Das abhipsha.das@nyu.edu 
Polymathic AI</p>
<p>Flatiron Institute</p>
<p>Nicholas Lourie 
Polymathic AI</p>
<p>New York University</p>
<p>Siavash Golkar 
Polymathic AI</p>
<p>New York University</p>
<p>Mariel Pettee 
Polymathic AI</p>
<p>Lawrence Berkeley National Laboratory</p>
<p>What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs
29 May 202553D0D06EF33E949974E5F358664A809BarXiv:2503.09894v2[cs.CL]
The scientific literature's exponential growth makes it increasingly challenging to navigate and synthesize knowledge across disciplines.Large language models (LLMs) are powerful tools for understanding scientific text, but they fail to capture detailed relationships across large bodies of work.Unstructured approaches, like retrieval augmented generation, can sift through such corpora to recall relevant facts; however, when millions of facts influence the answer, unstructured approaches become cost prohibitive.Structured representations offer a natural complement-enabling systematic analysis across the whole corpus.Recent work enhances LLMs with unstructured or semistructured representations of scientific concepts; to complement this, we try extracting structured representations using LLMs.By combining LLMs' semantic understanding with a schema of scientific concepts, we prototype a system that answers precise questions about the literature as a whole.Our schema applies across scientific fields and we extract concepts from it using only 20 manually annotated abstracts.To demonstrate the system, we extract concepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and evolutionary biology.The resulting database highlights emerging trends and, by visualizing the knowledge graph, offers new ways to explore the ever-growing landscape of scientific knowledge.Demo: abby101/surveyor-0 on HF Spaces.</p>
<p>Introduction</p>
<p>Consider a researcher seeking to build a multimodal foundation model for astrophysics.They might begin by asking: What are the most important data modalities to support-the most common § Video: YouTube Link ones in the field?How would such a researcher go about answering these questions today?</p>
<p>One might hope that LLMs could easily answer such a question, but while they have created unprecedented opportunities for accelerating scientific discovery, they struggle to aggregate reliable statistics and generate systematic analyses across the breadth of scientific literature.Manually reviewing papers or consulting domain experts are not scalable approaches when there are thousands of papers to investigate.</p>
<p>Most current approaches rely on unstructured methods like retrieval augmented generation (RAG) [9,11,3].While these methods excel at broad information retrieval and synthesis, they make it difficult to analyze specific patterns in research across large bodies of literature.The limitations become particularly evident when researchers try to understand how research in a field evolves.They need to track new instruments, identify research problems that require methodological innovation, and understand how theoretical models get validated across disciplines.Unstructured representations struggle to systematically capture these relationships.</p>
<p>Some efforts explore semistructured representations such as: keyphrase extractions based on statistical patterns [5] and LLM-based concept extraction combined with constructing vector-similaritybased knowledge graphs [15].While valuable, these methods typically treat all concepts uniformly without distinguishing between their functional roles in scientific work, or rely on semantic similarity of concepts using embedding models.This limits the utility of extracted knowledge when a researcher needs a quantitative analysis.</p>
<p>To address this challenge, we introduce a novel approach using LLMs that extracts categorized concepts from scientific papers using a general schema covering key research entities like objects, datasets, methods, and modalities.Our system combines structured knowledge representation with an interactive query interface to enable researchers to analyze methodological patterns, track research evolution, and understand relationships between different aspects of scientific work at scale.We visualize the extracted structured information using knowledge graphs that provide key insights into concept co-occurrence in scientific research.The main contributions of this work are: 1) a generic schema for categorizing scientific concepts across different fields, 2) a scalable LLM-based extraction pipeline to mine concepts from papers, 3) an interactive system for querying scientific information, 4) informative knowledge graphs built from the extracted concepts to represent scientific fields.</p>
<p>Method</p>
<p>Our system consists of three key components: a schema defining different kinds of scientific concepts, a pipeline for prompting LLMs to extract these concepts, and a database to store this structured knowledge for efficient queries and analysis.</p>
<p>Schema</p>
<p>Through an iterative discussion process of example selection, comparing manual annotations between the authors, and examining scientific papers across different domains, we developed an annotation schema capturing nine fundamental categories of scientific concepts: models, tasks, datasets, fields, modalities, methods, objects, properties, and instruments.In designing the schema, we aimed for categories that apply across scientific disciplines.This schema intentionally uses broad category definitions that are immediately understandable to scientists without requiring study of specialized class or type of data/observations with similar or the same structure method:</p>
<p>approach, technique or procedure to complete a task object:</p>
<p>entity that can be studied property:</p>
<p>quantitative or qualitative descriptor, or an inherent attribute of an entity, data, modality or method instrument: device or system used for making measurements Table 1: Definitions for a schema of scientific concepts.</p>
<p>and intricate taxonomies.We opted for coarser categories to avoid the ambiguity and complexity that arises when making subtle distinctions between closely related concepts, even though this means some concepts could be described by multiple tags.</p>
<p>Rather than implementing complex disambiguation tests, our simple tagging schema allows us to maintain scalability; however, there is always a trade-off between coverage and precision.</p>
<p>Original Sentence</p>
<p>We present an analysis of a new Australia Telescope Compact Array (ATCA) radiocontinuum observation of supernova remnant (SNR) G1.9+0.3, which at an age of 181±25 years is the youngest known in the Galaxy.</p>
<p>Tagged Sentence</p>
<p>We present an analysis of a new <dataset> <instrument>Australia Telescope Compact Array (ATCA)</instrument> <modality>radio-continuum</modality> observation</dataset> of <object>supernova remnant (SNR) G1.9+0.3 </object>, which at an <property>age</property> of 181±25 years is the youngest known in the <object>Galaxy</object>.</p>
<p>We implemented our extraction pipeline using the open-source Llama-3 70B Instruct model [4], employing few-shot learning to guide concept extraction.For our prompt optimization experiments, we manually annotated 20 papers, using 3 demonstration papers for few-shot examples and the remaining 17 as a development set to iteratively refine the prompts.Above is an example of the manual annotation process sentence-by-sentence.</p>
<p>The pipeline processes the language content sentence-by-sentence using manually annotated examples to demonstrate the target structure (see Fig. 2).</p>
<p>Pipeline</p>
<p>To optimize extraction reliability, we conducted systematic prompt engineering experiments on the manually annotated set, varying: (i) number and selection of few-shot examples, (ii) structure and ordering of the prompt, (iii) granularity of input text (sentence vs. paragraph), (iv) format of extracted concepts (JSON vs. human-readable).</p>
<p>To guide this iteration, we used a comprehensive set of metrics calculated on our development set, including: precision, recall and F-1 scores, for exact matches.In addition, we also considered the processing time and efficiency of the different approaches.During the annotation process, we found that even simple and broad concepts, such as a modality, encounter ambiguities when applied across scientific fields.As a result, it is likely that no method can achieve complete agreement with the development set annotations.Rather than as an absolute benchmark, we used the development set as a directional signal-a way to see if a given change improved the extraction process.Ultimately, our optimized prompt configuration consisting of instruction, schema and  from each of the 3 demonstration papers) shows promising results.The final results on our development set were: precision of 44% ± 12% and recall of 31% ± 11% in human-readable response format, with processing times averaging around 2.8 seconds per sentence.When using JSON output format, we observed similar precision levels with slightly higher recall rates of 40% ± 12% and processing times of around 4 seconds per sentence.While some noise persisted in the extracted data, the results were sufficient to explore using such structured knowledge from the scientific literature in order to discover relationships and systematically analyze complex statistical questions.</p>
<p>Database</p>
<p>The extracted concepts and their relationships are stored in a SQL database for scientific analysis queries which enables fast computation of aggregate statistics.The SQL database contains 2 tables: papers and predictions (see Fig. 1).The papers table contains metadata, raw text, author and category information about the papers, and the predictions table contains the information about the tagged concepts, the tag type and the papers they come from.Storing both the extracted information and relevant metadata about where the extractions came from facilitates analyses such as the evolution of methodological approaches over time or the adoption patterns of new experimental techniques across fields.</p>
<p>Demo</p>
<p>Visualization</p>
<p>To explore the relationships between scientific concepts, we built a dynamic visualization using force-directed graph layouts.In this representation, nodes represent individual scientific concepts V (e.g., specific methods, instruments, or objects of study), while edges represent co-occurrences within the same paper E in a graph G = (V, E).A physics-based spring layout algorithm determines the spatial arrangement, with frequently cooccurring concepts drawn closer together.</p>
<p>Our visualization system supports interactive exploration through: 1. Tag-type filtering to focus on specific concept categories (e.g., only methods or instruments), 2. Node highlighting to emphasize specific concepts and their immediate connections, 3. Depth-based exploration to reveal n-hop neighborhoods around concepts of interest, 4. Dynamic force-directed layout updates to reflect filtered subgraphs.</p>
<p>This graph-based approach enables both targeted investigation of specific concept relationships and broader analysis of methodological patterns across domains.For example, researchers can identify clusters of related experimental techniques, trace the adoption of methods across different subfields, or visualize isolated clusters of objects to understand how they are studied.</p>
<p>Query Interface</p>
<p>The query interface supports SQL queries for scientific concept exploration, with predefined queries demonstrating use cases like modality distribution analysis and temporal trends.The interface allows researchers to ask increasingly sophisticated questions by leveraging the structured database.For example, a researcher building an astrophysics foundation model could analyze most-used modalities, examine their current coverage, track usage trends over 5 years, and estimate coverage gains from adding new modalities.This approach helps scientists make data-driven research decisions that would be difficult to achieve through other means.</p>
<p>Results</p>
<p>Dataset.We collected the titles and abstracts from 30,000 articles from arXiv comprising 10,000 papers each from astrophysics, fluid dynamics and evolutionary biology, in order test across a breadth of scientific disciplines.Titles and abstracts (i.e.article metadata) were used instead of the full text in order to optimize processing efficiency while maintaining representativeness, since the titles and abstracts of papers are likely to be more information dense than the papers' bodies.After setting aside 20 astrophysics papers for prompt development, we used the optimized prompts refined through this development process to extract concepts from the remaining 9,980 astrophysics papers (from the original 10,000), as well as all 10,000 papers from each of the other two fields (fluid dynamics and evolutionary biology).Our final extractions and subsequent knowledge graph visualizations include results for these 29,980 papers.</p>
<p>Graph-based Exploration</p>
<p>Fig. 3 demonstrates the interconnected nature of scientific concepts through co-occurrence knowledge graphs from our analyzed domains.†</p>
<p>Demonstrating Example Queries</p>
<p>We examine several key questions to demonstrate the interface's capability for both exploratory research and targeted investigation in scientific discovery.</p>
<p>Related Work</p>
<p>Large language models (LLMs) [2,7,13,16,4] have recently demonstrated remarkable capabilities in language tasks, particularly through advances in prompting strategies [2,18].These advances have inspired building LLM-based pipelines to engage with complex scientific discovery tasks [8,1,10].Despite these advancements, the problem of hallucinations in LLMs persists [6].</p>
<p>To overcome this, a popular approach is to augment LLMs with unstructured, external knowledge [11,9], but while RAG excels in retrieving broad context information at scale, it is limited in providing precise information and avenues for systematic analysis, which can be efficiently realized through structured knowledge representations.Some prior work applies semistructured knowledge representations to the scientific literature, such as Gu and Krenn [5]'s SciMuse system which combines LLMs with the RAKE algorithm to extract concepts as keyword phrases.[15] extended this to astronomy, using LLMs to extract concepts from scientific texts and construct knowledge graphs, grouping the concepts with a vectorbased semantic similarity.In biomedicine, [12] integrated domain-specific language models with knowledge graph embeddings, showing improved performance but requiring field-specific finetuning.</p>
<p>Our work differs from these approaches by providing a domain-agnostic framework that combines LLM-powered semantic understanding with queryable structured knowledge curation, and enhanced by graph visualizations.Unlike previous methods, our approach introduces generalizable categorization schemes that enable cross-domain concept extraction, relationship mapping, and question answering.</p>
<p>Discussion</p>
<p>Our tool enables quantitative analysis of research methodologies across scientific domains by organizing concepts into distinct categories like methods, instruments, and data modalities, allowing researchers to systematically investigate patterns that would be difficult to discover through traditional literature review or citation analysis.The combination of SQL queries and graph visualization proves especially valuable for exploring methodological connections.For instance, answering how similar mathematical models get applied across different fields is hard with existing approaches but readily solvable by our system.</p>
<p>While the system shows promise, its current extraction precision leaves room for improvement.At times, the LLM can struggle to distinguish specific named entities (like "Melbourne wind tunnel") from generic concepts (like "wind-tunnel data"), introducing noise into the extracted relationships.Future work could address these limitations through improved prompting strategies, post-training of models by gathering insights from domain experts, and extracting more sophisticated relationships between concepts beyond co-occurrence.</p>
<p>Conclusion</p>
<p>This work demonstrates how combining LLMs with structured knowledge representation can enable systematic analysis of scientific literature.Our four key contributions are: a domain-agnostic schema for categorizing scientific concepts, a scalable LLM-based extraction pipeline, a queryable interactive system, and informative knowledge graphs built from the extracted concepts.The results show that even with modest extraction accuracy, our approach can reveal valuable insights about cross-disciplinary connections and research evolution that would be difficult to discover through traditional literature review, opening up new possibilities for navigating scientific research.support provided by the Simons Foundation and Schmidt Sciences, LLC.</p>
<p>A Comparison of Keyword Extraction Methods</p>
<p>A.1 Methodology</p>
<p>In this appendix we compare two approaches to scientific concept extraction: Rapid Automatic Keyword Extraction (RAKE) [14] and our method.RAKE is an unsupervised statistical method based on word frequency and co-occurrence, while our schema-based approach leverages Llama-3 70B for semantic understanding.We randomly sampled 300 scientific papers (100 each from astrophysics, fluid dynamics, and evolutionary biology) from our dataset and applied RAKE extractions on the title and abstract text, performed using the rake-nltk library [17].The extracted keywords were then compared to those generated by our method.Our approach consistently extracted fewer concepts than RAKE but organized them into semantic categories that reveal their functional roles within the scientific discourse.In table 3, we see that across all domains, object was the predominant concept type, followed by property.</p>
<p>A.2 Quantitative Comparison</p>
<p>A.3 Qualitative Analysis</p>
<p>A.3.1 Astrophysics</p>
<p>A Detailed Analysis of a Magnetic Island Observed by WISPR on Parker Solar Probe</p>
<p>We present the identification and physical analysis of a possible magnetic island feature seen in white-light images observed by the Wide-field Imager for Solar Probe (WISPR) on board the Parker Solar Probe (Parker).The island is imaged by WISPR during Parker's second solar encounter on 2019 April 06, when Parker was 38 solar radii from the Sun center.We report that the average velocity and acceleration of the feature are approximately 334 km s and -0.64 m s-2.The kinematics of the island feature, coupled with its direction of propagation, indicate that the island is likely entrained in the slow solar wind.The island is elliptical in shape with a density deficit in its center, suggesting the presence of a magnetic guide field.We argue that this feature is consistent with the formation of this island via reconnection in the current sheet of the streamer.The feature's aspect ratio (calculated as the ratio of its minor to major axis) evolves from an elliptical to a more circular shape that approximately doubles during its propagation through WISPR's field of view.The island is not distinct in other white-light observations from the Solar and Heliospheric Observatory (SOHO) and the Solar Terrestrial Relations Observatory (STEREO) coronagraphs, suggesting that this is a comparatively faint heliospheric feature and that viewing perspective and WISPR's enhanced sensitivity are key to observing the magnetic island.</p>
<p>RAKE Keywords</p>
<p>[(score, "keyword") * count] (17.67, "possible magnetic island feature seen"), (13.71, "solar terrestrial relations observatory"), (13.33, "comparatively faint heliospheric feature"), (9.00, "2019 april 06"), (8.71, "slow solar wind"), (8.71, "second solar encounter"), (8.71, "38 solar radii"), (8.50, "light images observed"), (8.50, "approximately 334 km"), (8.33, "magnetic guide field"), (8.00, "island via reconnection"), (6.96, "parker solar probe"), (6.00, "heliospheric observatory"), (5.33, "magnetic island"), (5.21, "solar probe"), (4.50, "light observations"), (4.50, "approximately doubles"), (4.33, "island feature"), (4.00, "viewing perspective"), (4.00, "physical analysis"), (4.00, "major axis"), (4.00, "likely entrained"), (4.00, "field imager"), (4.00, "enhanced sensitivity"), (4.00, "density deficit"), (4.00, "current sheet"), (4.00, "average velocity"), (3.75, "parker ()"), (3.50, "sun center"), (3.50, "circular shape"), (3.50, "aspect ratio"), (2.71, "solar"), (2.33, "feature") * 3, (2.00, "island") * 4, (2.00, "field"), (1.75, "parker") * 2, (1.50, "shape"), (1.50, "ratio"), (1.50, "center"), (1.00, "wispr") * 4, (1.00, "wide"), (1.00, "white") * 2, (1.00, "view"), (1.00, "suggesting") * 2, (1.00, "streamer"), (1.00, "stereo"), (1.00, "soho"), (1.00, "report"), (1.00, "propagation") * 2, (1.00, "present"), (1.00, "presence"), (1.00, "observing"), (1.00, "minor"), (1.00, "kinematics"), (1.00, "key"), (1.00, "indicate"), (1.00, "imaged"), (1.00, "identification"), (1.00, "formation"), (1.00, "evolves"), (1.00, "elliptical") * 2, (1.00, "distinct"), (1.00, "direction"), (1.00, "coupled"), (1.00, "coronagraphs"), (1.00, "consistent"), (1.00, "calculated"), (1.00, "board"), (1.00, "argue"), (1.00, "acceleration"), (1.00, "64"), (1.00, "2"), (1.00, "0")</p>
<p>Concepts (by type):</p>
<p>• object: Parker Solar Probe, magnetic island feature, white-light images, island, Parker, Sun, feature, island feature, slow solar wind, streamer, WISPR's field of view, magnetic island, heliospheric feature</p>
<p>• instrument: WISPR, Wide-field Imager for Solar Probe (WISPR), Parker Solar Probe (Parker), Solar and Heliospheric Observatory (SOHO), Solar Terrestrial Relations Observatory (STEREO)</p>
<p>• property: solar radii, velocity, acceleration, density deficit, shape, aspect ratio, faint</p>
<p>The RAKE extraction produces a flat list of keywords with associated scores.In contrast, our approach organizes concepts by semantic role, such as instruments (WISPR, SOHO), objects (Sun, slow solar wind), and their properties (solar radii, acceleration).</p>
<p>A.3.2 Fluid Dynamics</p>
<p>Approximation of sea surface velocity field by fitting surrogate two-dimensional flow to scattered measurements</p>
<p>In this paper, a rapid approximation method is introduced to estimate the sea surface velocity field based on scattered measurements.The method uses a simplified two-dimensional flow model as a surrogate model, which mimics the real submesoscale flow.The proposed approach treats the interpolation of the flow velocities as an optimization problem, aiming to fit the flow model to the scattered measurements.To ensure consistency between the simulated velocity field and the measured values, the boundary conditions in the numerical simulations are adjusted during the optimization process.Additionally, the relevance of quantity and quality of the scattered measurements is assessed, emphasizing the importance of the measurement locations within the domain as well as explaining how these measurements contribute to the accuracy and reliability of the sea surface velocity field approximation.The proposed methodology has been successfully tested in both synthetic and real-world scenarios, leveraging measurements obtained from Global Positioning System (GPS) drifters and high-frequency (HF) radar systems.The adaptability of this approach for different domains, measurement types, and conditions implies that it is suitable for real-world submesoscale scenarios where only an approximation of the sea surface velocity field is sufficient.</p>
<p>RAKE Keywords</p>
<p>[(score, "keyword") * count] (22.83, "sea surface velocity field based"), (20.83, "sea surface velocity field approximation"), (17.83, "sea surface velocity field"), (11.50, "simulated velocity field"), (9.00, "global positioning system"), (8.50, "rapid approximation method"), (8.50, "measurement locations within"), (8.20, "leveraging measurements obtained"), (8.00, "world submesoscale scenarios"), (7.83, "dimensional flow model"), (7.50, "proposed approach treats"), (7.17, "real submesoscale flow"), (5.00, "world scenarios"), (4.83, "flow model"), (4.50, "proposed methodology"), (4.50, "method uses"), (4.50, "measurement types"), (4.50, "flow velocities"), (4.33, "surrogate model"), (4.20, "scattered measurements") * 3, (4.20, "measurements contribute"), (4.00, "successfully tested"), (4.00, "simplified two"), (4.00, "radar systems"), (4.00, "optimization process"), (4.00, "optimization problem"), (4.00, "numerical simulations"), (4.00, "measured values"), (4.00, "ensure consistency"), (4.00, "different domains"), (4.00, "conditions implies"), (4.00, "boundary conditions"), (3.00, "approximation"), (2.00, "approach"), (1.67, "real") * 2, (1.00, "well"), (1.00, "synthetic"), (1.00, "suitable"), (1.00, "sufficient"), (1.00, "reliability"), (1.00, "relevance"), (1.00, "quantity"), (1.00, "quality"), (1.00, "paper"), (1.00, "mimics"), (1.00, "introduced"), (1.00, "interpolation"), (1.00, "importance"), (1.00, "high"), (1.00, "hf"), (1.00, "gps"), (1.00, "frequency"), (1.00, "fit"), (1.00, "explaining"), (1.00, "estimate"), (1.00, "emphasizing"), (1.00, "drifters"), (1.00, "domain"), (1.00, "assessed"), (1.00, "aiming"), (1.00, "adjusted"), (1.00, "additionally"), (1.00, "adaptability"), (1.00, "accuracy") Concepts (by type): Our method's extraction effectively distinguishes between physical objects of study (porous media), models (two-dimensional flow model), methodological approaches (rapid approximation method), and instruments (Global Positioning System (GPS) drifters).</p>
<p>A.3.3 Evolutionary Biology</p>
<p>Complexity-stability relationships in disordered dynamical systems</p>
<p>Robert May famously used random matrix theory to predict that large, complex systems cannot admit stable fixed points.However, this general conclusion is not always supported by empirical observation: from cells to biomes, biological systems are large, complex and, often, stable.In this paper, we revisit May's argument in light of recent developments in both ecology and random matrix theory.Using a non-linear generalization of the competitive Lotka-Volterra model, we show that there are, in fact, two kinds of complexity-stability relationships in disordered dynamical systems: if self-interactions grow faster with density than cross-interactions, complexity is destabilizing; but if cross-interactions grow faster than self-interactions, complexity is stabilizing.</p>
<p>RAKE Keywords</p>
<p>[(score, "keyword") * count] (40.50, "robert may famously used random matrix theory"), (40.00, "complex systems cannot admit stable fixed points"), (15.00, "random matrix theory"), (10.00, "disordered dynamical systems"), (8.00, "interactions grow faster") * 2, (6.50, "revisit may"), (6.00, "biological systems"), (4.00, "volterra model"), (4.00, "two kinds"), (4.00, "stable"), (4.00, "stability relationships"), (4.00, "recent developments"), (4.00, "linear generalization"), (4.00, "general conclusion"), (4.00, "empirical observation"), (4.00, "complex"), (4.00, "competitive lotka"), (4.00, "always supported"), (2.00, "interactions") * 2, (1.00, "using"), (1.00, "stabilizing"), (1.00, "show"), (1.00, "self") * 2, (1.00, "predict"), (1.00, "paper"), (1.00, "often"), (1.00, "non"), (1.00, "light"), (1.00, "large") * 2, (1.00, "however"), (1.00, "fact"), (1.00, "ecology"), (1.00, "destabilizing"), (1.00, "density"), (1.00, "cross") * 2, (1.00, "complexity") * 3, (1.00, "cells"), (1.00, "biomes"), (1.00, "argument")</p>
<p>Concepts (by type):</p>
<p>• object: disordered dynamical systems, systems, cells, biomes, biological systems</p>
<p>• property: complexity-stability relationships, large, complex, stable, complexity-stability, density</p>
<p>• method: random matrix theory</p>
<p>• field: ecology</p>
<p>• model: non-linear generalization of the competitive Lotka-Volterra model Our method's extraction captures biological entities (cells, biomes, systems) and their properties, while also identifying the specific models and methods used.</p>
<p>A.4 Insights From The Comparison</p>
<p>RAKE is efficient, language-independent, and quantitatively ranks keyword importance, but it lacks semantic depth, extracts fragmented phrases, and does not distinguish between concept types.Our method, while computationally more demanding and occasionally prone to concept hallucination, provides structured semantic categorization, generates coherent concepts, and captures domainspecific nuances more effectively.</p>
<p>Both methods introduce some noise, though RAKE produces significantly more.The structured semantic representation in our approach offers a more meaningful and organized summary compared to RAKE's flat keyword list, making it more useful for domain experts.</p>
<p>"Figure 1 :
1
Figure 1: Illustration of the structured concept extraction pipeline: i) the corpus used, ii) running optimized prompt on full corpus, iii) storing model's outputs and corpus metadata in SQL database.</p>
<p>9 few-shot examples (3 sentences with annotated extractions Illustration: Prefix + Prompt The following schema is provided to tag the title and abstract of a given scientific paper as shown in the examples: $SCHEMA Sentence: This magnetic field strength implies a minimum total energy of the synchrotron radiation of E min ≈ 1.8×10 48 ergs.Extractions: property: magnetic field strength, energy object: synchrotron radiation ... (Total 9 few-shot examples) ... Sentence: We present HATNet observations of XO-5b, confirming its planetary nature based on evidence beyond that described in the announcement of Burke et al Ground Truth Tags: dataset: HATNet observations instrument: HATNet object: XO-5b Predicted Tags: dataset: HATNet observations object: XO-5b, planetary nature</p>
<p>Figure 2 :
2
Figure 2: Expanded prompt illustration with schema and few-shot examples, along with the sentence to predict tags for.</p>
<p>(a) Analysis of galaxy images: various galactic entities, measurement objects, clouds, instruments, and properties.(b) COVID-19 cluster analysis: geographic distribution of pandemic research and immune health terms indicating research focus areas.(c) Fluid dynamics clusters centered around "flow," branching into related flow and turbulence-based physical phenomena.(d) Astrophysical objects and phenomena spanning multiple scales, from individual stars and binaries to galactic structures and clusters.</p>
<p>Figure 3 :
3
Figure 3: Co-occurrence graphs: astrophysics (a, d), epidemiology (b), fluid dynamics (c).</p>
<p>Table 2
2
reveals significant differences in the average count of extracted concepts by each method across domains for the subset of 300 papers.
DomainRAKE Ours Ratio(avg)(avg) (avg)Astrophysics69.533.82.05Fluid Dynamics70.332.62.15Evolutionary Biology67.729.72.28Overall69.232.12.16</p>
<p>Table 2 :
2
Average number of extracted concepts by domain
DomainOur method: concept typesAstrophysicsobject (56.1%), property (24.7%),instrument (5.4%), method (3.6%),modality (3.5%), model (3.1%), task(1.5%), field (1.0%), dataset (0.6%)Fluid Dynamicsobject (44.7%), property (31.0%),method (10.0%), model (5.3%), modal-ity (3.3%), field (1.9%), task (1.9%),instrument (1.2%), dataset (0.5%)Evolutionary Biology object (51.2%), property (25.7%),model (9.2%), method (5.4%), task(3.3%), field (2.5%), modality (1.7%),dataset (0.4%), instrument (0.2%)</p>
<p>Table 3 :
3
Distribution of concept types by domain</p>
<p>†  We use the d3-force library; see documentation for more information on the spring-layout implementation.
AcknowledgmentsThe computations in this work were, in part, run at facilities supported by the Scientific Computing Core at the Flatiron Institute, a division of the Simons Foundation.Polymathic AI acknowledgeshttps://github.com/chiral-carbon/kgfor-science.§
arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4. Preprint. </p>
<p>others. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, arXiv:2005.14165Language Models are Few-Shot Learners. 202012Preprint</p>
<p>Retrieval-Augmented Generation for Large Language Models: A Survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, arXiv:2312.109972024Preprint</p>
<p>Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, arXiv:2407.21783The Llama 3 Herd of Models. 2024Preprint</p>
<p>Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders. Xuemei Gu, Mario Krenn, arXiv:2405.170442024Preprint</p>
<p>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu, 10.1145/3703155ACM Transactions on Information Systems. 2024</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162023Preprint</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. Mario Krenn, Anton Zeilinger, 10.1073/pnas.1914370116Proceedings of the National Academy of Sciences. 11742020</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, arXiv:2005.114012021Preprint</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024Preprint</p>
<p>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.075592023Preprint</p>
<p>Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, arXiv:2106.097002021Preprint</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Anadkat, arXiv:2303.08774Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff BelgumPreprintand 262 others. 2024. GPT-4</p>
<p>Automatic keyword extraction from individual documents. Stuart J Rose, Dave W Engel, Nick Cramer, Wendy Cowley, 2010</p>
<p>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery. Zechang Sun, Yuan-Sen, Yaobo Ting, Nan Liang, Song Duan, Zheng Huang, Cai, arXiv:2406.013912024Preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, arXiv:2307.09288Wenyin Fu, and 49 others. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint</p>
<p>rake-nltk: Python implementation of the rapid automatic keyword extraction algorithm using nltk. B V Vishwas, 2017</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
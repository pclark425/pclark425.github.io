<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1306 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1306</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1306</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-221140175</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2008.06686v1.pdf" target="_blank">Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics</a></p>
                <p><strong>Paper Abstract:</strong> Zero-shot sim-to-real transfer of tasks with complex dynamics is a highly challenging and unsolved problem. A number of solutions have been proposed in recent years, but we have found that many works do not present a thorough evaluation in the real world, or underplay the significant engineering effort and task-specific fine tuning that is required to achieve the published results. In this paper, we dive deeper into the sim-to-real transfer challenge, investigate why this is such a difficult problem, and present objective evaluations of a number of transfer methods across a range of real-world tasks. Surprisingly, we found that a method which simply injects random forces into the simulation performs just as well as more complex methods, such as those which randomise the simulator's dynamics parameters, or adapt a policy online using recurrent network architectures.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1306.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1306.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo (Multi-Joint dynamics with Contact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics engine used to simulate rigid-body dynamics, contacts and jointed robots; it was the primary simulator used to train all RL policies in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mujoco: A physics engine for model-based control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A commercial/high-performance rigid-body physics engine for model-based control that simulates multibody dynamics, contacts, joint constraints, friction and allows application of external/generalised forces. In this work MuJoCo is used as the backend physics engine for all simulated training environments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics / rigid-body dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high fidelity for rigid-body dynamics and contacts (suitable for control and contact-rich robotic tasks); not intended for soft-body, fluid, thermal or molecular-scale physics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>models mass matrices, joint dynamics, contact and friction, external/generalised forces; supports configurable observation noise and time delays; discrete time-stepping; can simulate actuator/controller dynamics (PID gains, damping, armature, friction). Does not model thermal, fluid, or biochemical processes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>TD3-trained control policies (Conservative, Adaptive) and additional frameworks (UPOSI universal policy + OSI, EPI probing + task policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement-learning agents implemented as fully-connected neural networks (policy: 5 layers × 512 units; Q-networks similar), trained with TD3; PPO used for probing policy in EPI. Adaptive variants include LSTM branches; UPOSI includes an OSI regressor network.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Dynamics-aware robotic control tasks (zero-shot sim-to-real): reaching, pushing, and sliding — i.e., learning control policies robust to dynamics discrepancies between sim and real.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robotic experiments (Sawyer 7-DOF arm, real puck and plate setups)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative summary: Policies trained in MuJoCo using Random Force Injection (RFI) transferred most consistently to the real world; Domain Randomisation (DR) could match performance but required significantly more task-specific tuning; UPOSI/EPI variants transferred worse in many cases. No numeric success rates are provided in the main text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper argues that, for zero-shot sim-to-real where limited task-specific tuning is allowed, detailed randomisation of many simulator parameters is often unnecessary; simpler stochasticisation (RFI) applied inside MuJoCo can suffice for transfer, implying not all fine-grained simulator features are required.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No Randomisation (NR) policies failed to transfer reliably. Domain Randomisation could produce destructive simulator behaviours for some parameter combinations and required days of tuning; UPOSI and some adaptive approaches performed poorly in transfer, suggesting inaccuracies in inferred dynamics hurt transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1306.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1306.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robosuite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robosuite (robotics simulation toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotics simulation toolkit/wrapper used together with MuJoCo to construct the specific robot environments (Sawyer arm, objects, tasks) used for training the RL agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Robosuite (MuJoCo backend)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A high-level robotics environment/toolkit that provides robot models, task wrappers and interfaces on top of a physics engine (MuJoCo here) to ease building manipulation tasks and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / mechanics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>inherits MuJoCo's fidelity (medium-to-high for rigid-body/contact dynamics); Robosuite itself is a task/environment wrapper rather than a distinct physics fidelity layer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>provides URDF-based kinematics, task scaffolding, sensors and observation interfaces; relies on MuJoCo for physics (contacts, friction, forces); allows configuration of controller gains, observation noise and time delays.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>TD3/PPO RL policies trained in Robosuite environments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same RL agents and architectures as used with MuJoCo; Robosuite provides the environment and robot models used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Same robotic control/dynamics adaptation tasks (reaching, pushing, sliding) used to evaluate sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robotic experiments</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Same qualitative transfer outcomes as reported for MuJoCo-based training (RFI most consistent, DR comparable when extensively tuned, NR/UPOSI/EPI often worse).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors emphasise that using realistic kinematics (URDF) and measured geometry is part of the baseline, but many detailed dynamics parameters are hard to measure and often guessed; they find simpler injection of stochastic forces often suffices.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Issues arise when DR settings produce unstable/destructive simulator behaviours; Robosuite inherits such failure modes from the physics engine configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1306.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1306.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulation tools comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited prior work that compares accuracy and properties of several popular physics engines (Bullet, Havok, MuJoCo, ODE, PhysX); mentioned by the authors as background on simulator accuracy comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Bullet, Havok, MuJoCo, ODE, PhysX (collection compared in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A set of general-purpose physics engines used in robotics and graphics; the cited paper conducts an empirical comparison of their accuracy for model-based robotics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics / simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>varies by engine; the cited comparison characterises relative fidelity/accuracy across engines for rigid-body/contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Comparative study (in the cited work) of contact, collision, constraint handling and numerical integration behaviours across engines; the present paper only cites that comparison and does not re-run it.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The cited work compares multiple physics engines; the current paper references it as prior art but does not itself perform a simulator-to-simulator fidelity comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The present paper notes that while simulator comparisons exist (cited), prior works often do not evaluate sim-to-real transfer thoroughly; this paper focuses on sim-to-real dynamics transfer evaluation instead.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1306.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1306.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Force Injection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulation randomisation regime that injects random generalized forces into the simulator's dynamics each timestep instead of randomising large numbers of simulator parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo (regime applied within the MuJoCo environment)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>MuJoCo environments augmented by sampling and applying white-noise generalized forces to degrees of freedom at every simulation timestep to emulate unmodelled/external disturbances.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics / control / sim-to-real dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity with stochastic forcing to cover unmodelled dynamics; purposefully sacrifices explicit parameter diversity for stochastic disturbance coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>injects zero-mean white-noise generalized forces with a uniform sampling range per dimension (7–13 dimensions depending on task); retains MuJoCo's rigid-body/contact physics but does not explicitly randomise masses, friction, actuator gains, etc.; observation noise and time delays can be added (RFI+ variant).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>TD3-trained policies (Conservative and Adaptive variants); also tested with UPOSI and EPI frameworks under same randomisation regimes</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement-learning neural-network policies trained with TD3 (and PPO for EPI probing), using RFI to create training ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn robust control policies for reaching, pushing and sliding that generalise to real-world dynamics discrepancies (zero-shot sim-to-real).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robotic platform (Sawyer arm; objects with April tag pose extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: RFI produced the best and most consistent zero-shot sim-to-real transfer across tasks in this study, often outperforming or matching Domain Randomisation while being far easier and faster to tune. RFI+ (RFI plus observation noise) sometimes produced more stable but less successful behaviours in some tasks. No numeric success rates are provided in the main text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>When compared to No Randomisation (NR), Domain Randomisation (DR) and RFI+ variants, RFI achieved overall superior and more consistent real-world transfer under the paper's limited task-specific tuning constraints; DR required substantially more tuning to match RFI.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors conclude that detailed randomisation of many simulator parameters is not strictly necessary for zero-shot dynamics transfer in their tasks; injecting stochastic forces (a simpler, lower-dimensional modification) can be sufficient, reducing engineering effort.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>RFI sometimes led to aggressive policies (e.g., in sliding task causing object to fall) — RFI+ produced more conservative but less successful behaviour in reaching the goal. DR could produce destructive simulator behaviours if poorly tuned; NR failed to transfer reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Solving rubik's cube with a robot hand <em>(Rating: 2)</em></li>
                <li>Preparing for the unknown: Learning a universal policy with online system identification <em>(Rating: 2)</em></li>
                <li>Environment probing interaction policies <em>(Rating: 2)</em></li>
                <li>Modelling generalized forces with reinforcement learning for sim-to-real transfer <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1306",
    "paper_id": "paper-221140175",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo (Multi-Joint dynamics with Contact)",
            "brief_description": "A physics engine used to simulate rigid-body dynamics, contacts and jointed robots; it was the primary simulator used to train all RL policies in this paper's experiments.",
            "citation_title": "Mujoco: A physics engine for model-based control",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo",
            "simulator_description": "A commercial/high-performance rigid-body physics engine for model-based control that simulates multibody dynamics, contacts, joint constraints, friction and allows application of external/generalised forces. In this work MuJoCo is used as the backend physics engine for all simulated training environments.",
            "scientific_domain": "mechanics / robotics / rigid-body dynamics",
            "fidelity_level": "medium-to-high fidelity for rigid-body dynamics and contacts (suitable for control and contact-rich robotic tasks); not intended for soft-body, fluid, thermal or molecular-scale physics.",
            "fidelity_characteristics": "models mass matrices, joint dynamics, contact and friction, external/generalised forces; supports configurable observation noise and time delays; discrete time-stepping; can simulate actuator/controller dynamics (PID gains, damping, armature, friction). Does not model thermal, fluid, or biochemical processes.",
            "model_or_agent_name": "TD3-trained control policies (Conservative, Adaptive) and additional frameworks (UPOSI universal policy + OSI, EPI probing + task policy)",
            "model_description": "Reinforcement-learning agents implemented as fully-connected neural networks (policy: 5 layers × 512 units; Q-networks similar), trained with TD3; PPO used for probing policy in EPI. Adaptive variants include LSTM branches; UPOSI includes an OSI regressor network.",
            "reasoning_task": "Dynamics-aware robotic control tasks (zero-shot sim-to-real): reaching, pushing, and sliding — i.e., learning control policies robust to dynamics discrepancies between sim and real.",
            "training_performance": null,
            "transfer_target": "real-world robotic experiments (Sawyer 7-DOF arm, real puck and plate setups)",
            "transfer_performance": "Qualitative summary: Policies trained in MuJoCo using Random Force Injection (RFI) transferred most consistently to the real world; Domain Randomisation (DR) could match performance but required significantly more task-specific tuning; UPOSI/EPI variants transferred worse in many cases. No numeric success rates are provided in the main text excerpt.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "The paper argues that, for zero-shot sim-to-real where limited task-specific tuning is allowed, detailed randomisation of many simulator parameters is often unnecessary; simpler stochasticisation (RFI) applied inside MuJoCo can suffice for transfer, implying not all fine-grained simulator features are required.",
            "failure_cases": "No Randomisation (NR) policies failed to transfer reliably. Domain Randomisation could produce destructive simulator behaviours for some parameter combinations and required days of tuning; UPOSI and some adaptive approaches performed poorly in transfer, suggesting inaccuracies in inferred dynamics hurt transfer.",
            "uuid": "e1306.0",
            "source_info": {
                "paper_title": "Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "Robosuite",
            "name_full": "Robosuite (robotics simulation toolkit)",
            "brief_description": "A robotics simulation toolkit/wrapper used together with MuJoCo to construct the specific robot environments (Sawyer arm, objects, tasks) used for training the RL agents.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Robosuite (MuJoCo backend)",
            "simulator_description": "A high-level robotics environment/toolkit that provides robot models, task wrappers and interfaces on top of a physics engine (MuJoCo here) to ease building manipulation tasks and experiments.",
            "scientific_domain": "robotics / mechanics",
            "fidelity_level": "inherits MuJoCo's fidelity (medium-to-high for rigid-body/contact dynamics); Robosuite itself is a task/environment wrapper rather than a distinct physics fidelity layer.",
            "fidelity_characteristics": "provides URDF-based kinematics, task scaffolding, sensors and observation interfaces; relies on MuJoCo for physics (contacts, friction, forces); allows configuration of controller gains, observation noise and time delays.",
            "model_or_agent_name": "TD3/PPO RL policies trained in Robosuite environments",
            "model_description": "Same RL agents and architectures as used with MuJoCo; Robosuite provides the environment and robot models used during training.",
            "reasoning_task": "Same robotic control/dynamics adaptation tasks (reaching, pushing, sliding) used to evaluate sim-to-real transfer.",
            "training_performance": null,
            "transfer_target": "real-world robotic experiments",
            "transfer_performance": "Same qualitative transfer outcomes as reported for MuJoCo-based training (RFI most consistent, DR comparable when extensively tuned, NR/UPOSI/EPI often worse).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors emphasise that using realistic kinematics (URDF) and measured geometry is part of the baseline, but many detailed dynamics parameters are hard to measure and often guessed; they find simpler injection of stochastic forces often suffices.",
            "failure_cases": "Issues arise when DR settings produce unstable/destructive simulator behaviours; Robosuite inherits such failure modes from the physics engine configuration.",
            "uuid": "e1306.1",
            "source_info": {
                "paper_title": "Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "Simulation tools comparison",
            "name_full": "Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx",
            "brief_description": "A cited prior work that compares accuracy and properties of several popular physics engines (Bullet, Havok, MuJoCo, ODE, PhysX); mentioned by the authors as background on simulator accuracy comparisons.",
            "citation_title": "Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx",
            "mention_or_use": "mention",
            "simulator_name": "Bullet, Havok, MuJoCo, ODE, PhysX (collection compared in cited work)",
            "simulator_description": "A set of general-purpose physics engines used in robotics and graphics; the cited paper conducts an empirical comparison of their accuracy for model-based robotics tasks.",
            "scientific_domain": "mechanics / robotics / simulation",
            "fidelity_level": "varies by engine; the cited comparison characterises relative fidelity/accuracy across engines for rigid-body/contact dynamics.",
            "fidelity_characteristics": "Comparative study (in the cited work) of contact, collision, constraint handling and numerical integration behaviours across engines; the present paper only cites that comparison and does not re-run it.",
            "model_or_agent_name": "",
            "model_description": "",
            "reasoning_task": "",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "The cited work compares multiple physics engines; the current paper references it as prior art but does not itself perform a simulator-to-simulator fidelity comparison.",
            "minimal_fidelity_discussion": "The present paper notes that while simulator comparisons exist (cited), prior works often do not evaluate sim-to-real transfer thoroughly; this paper focuses on sim-to-real dynamics transfer evaluation instead.",
            "failure_cases": null,
            "uuid": "e1306.2",
            "source_info": {
                "paper_title": "Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "RFI",
            "name_full": "Random Force Injection",
            "brief_description": "A simulation randomisation regime that injects random generalized forces into the simulator's dynamics each timestep instead of randomising large numbers of simulator parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo (regime applied within the MuJoCo environment)",
            "simulator_description": "MuJoCo environments augmented by sampling and applying white-noise generalized forces to degrees of freedom at every simulation timestep to emulate unmodelled/external disturbances.",
            "scientific_domain": "mechanics / robotics / control / sim-to-real dynamics",
            "fidelity_level": "medium-fidelity with stochastic forcing to cover unmodelled dynamics; purposefully sacrifices explicit parameter diversity for stochastic disturbance coverage.",
            "fidelity_characteristics": "injects zero-mean white-noise generalized forces with a uniform sampling range per dimension (7–13 dimensions depending on task); retains MuJoCo's rigid-body/contact physics but does not explicitly randomise masses, friction, actuator gains, etc.; observation noise and time delays can be added (RFI+ variant).",
            "model_or_agent_name": "TD3-trained policies (Conservative and Adaptive variants); also tested with UPOSI and EPI frameworks under same randomisation regimes",
            "model_description": "Reinforcement-learning neural-network policies trained with TD3 (and PPO for EPI probing), using RFI to create training ensemble.",
            "reasoning_task": "Learn robust control policies for reaching, pushing and sliding that generalise to real-world dynamics discrepancies (zero-shot sim-to-real).",
            "training_performance": null,
            "transfer_target": "real-world robotic platform (Sawyer arm; objects with April tag pose extraction)",
            "transfer_performance": "Qualitative: RFI produced the best and most consistent zero-shot sim-to-real transfer across tasks in this study, often outperforming or matching Domain Randomisation while being far easier and faster to tune. RFI+ (RFI plus observation noise) sometimes produced more stable but less successful behaviours in some tasks. No numeric success rates are provided in the main text excerpt.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "When compared to No Randomisation (NR), Domain Randomisation (DR) and RFI+ variants, RFI achieved overall superior and more consistent real-world transfer under the paper's limited task-specific tuning constraints; DR required substantially more tuning to match RFI.",
            "minimal_fidelity_discussion": "Authors conclude that detailed randomisation of many simulator parameters is not strictly necessary for zero-shot dynamics transfer in their tasks; injecting stochastic forces (a simpler, lower-dimensional modification) can be sufficient, reducing engineering effort.",
            "failure_cases": "RFI sometimes led to aggressive policies (e.g., in sliding task causing object to fall) — RFI+ produced more conservative but less successful behaviour in reaching the goal. DR could produce destructive simulator behaviours if poorly tuned; NR failed to transfer reliably.",
            "uuid": "e1306.3",
            "source_info": {
                "paper_title": "Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics",
                "publication_date_yy_mm": "2020-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx",
            "rating": 2,
            "sanitized_title": "simulation_tools_for_modelbased_robotics_comparison_of_bullet_havok_mujoco_ode_and_physx"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Solving rubik's cube with a robot hand",
            "rating": 2,
            "sanitized_title": "solving_rubiks_cube_with_a_robot_hand"
        },
        {
            "paper_title": "Preparing for the unknown: Learning a universal policy with online system identification",
            "rating": 2,
            "sanitized_title": "preparing_for_the_unknown_learning_a_universal_policy_with_online_system_identification"
        },
        {
            "paper_title": "Environment probing interaction policies",
            "rating": 2,
            "sanitized_title": "environment_probing_interaction_policies"
        },
        {
            "paper_title": "Modelling generalized forces with reinforcement learning for sim-to-real transfer",
            "rating": 2,
            "sanitized_title": "modelling_generalized_forces_with_reinforcement_learning_for_simtoreal_transfer"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 1,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        }
    ],
    "cost": 0.014747249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Crossing the Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics</p>
<p>Eugene Valassakis 
Zihan Ding 
Edward Johns 
Crossing the Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics</p>
<p>Zero-shot sim-to-real transfer of tasks with complex dynamics is a highly challenging and unsolved problem.</p>
<p>I. INTRODUCTION</p>
<p>Zero-shot sim-to-real transfer aims to deploy a control policy trained in simulation to the real-world, without further training on task-specific real-world data. Much progress has been made in this field when learning vision-based policies with simple underlying dynamics, typically centred around the concept of domain randomisation, a technique that consists of injecting noise into different aspects of the simulation during training [16], [29]. However, transferring policies where the dynamics is complex is much more challenging, even when removing the need for vision transfer through the use of fiducial markers.</p>
<p>In our research, we originally set out to experimentally evaluate the field of zero-shot sim-to-real transfer for dynamics, aiming to determine the best methods for practical use. This revealed a somewhat surprising story that is not immediately obvious from reading previous published works. We found that works which have apparently shown great success, do not highlight enough the tedious engineering and task-specific tuning efforts required to achieve the published results, often simply stating the final simulator configurations used, which are not necessarily applicable to different tasks. Meanwhile, others have bypassed the real problem of interest by validating their methods only in simplified simulated environments, presenting results on sim-to-sim transfer rather than sim-to-real transfer. These combined shortcomings create a gap in the understanding between those with first-hand experience implementing these methods, and those reading such works and taking the results at face value.</p>
<p>In this paper, we aim to help the community cross that gap by providing a much more thorough examination of the problem than is typically seen in other works. We (i) shed some light on the fundamental aspects of sim-to-real transfer for a Reinforcement Learning (RL) policy, (ii) highlight the engineering process necessary to set up simulators for such a task, and (iii) provide thorough benchmarking and evaluation of recent methods, across three different tasks (see Figure 1), whilst only allowing for limited task-specific finetuning. In doing so, we also highlight some methods that we believe have been under-considered in the current literature, as well as provide real-world results for methods where published results exist only for sim-to-sim transfer. Through our experiments, we found that many of the more complex recent methods do not scale well to real-world tasks without significant task-specific tuning, which defeats the purpose of zero-shot transfer. We also found that some simple methods performed just as well, if not better, than more complex alternatives, whilst being significantly easier to implement and interpret. Most notably, we found that simply injecting random forces during simulation is at least as effective as the more typical approach of randomising the full range of simulation parameters. A video, supplementary material, and code, are available on our website * .</p>
<p>II. RELATED WORK</p>
<p>Sim-to-real transfer can be seen as a special case of domain adaptation (DA), the study of training models in a source domain to deploy them in a target domain. For sim-to-real with RL in particular, methods typically address the visual [16], [29] or dynamics [1], [3], [4], [24], [34], [35] adaptation problems, and either attempt zeroshot transfer [1], [16], or require real-world test-time data through meta-learning [5], residual learning [18], real-world optimisation, [33] or mixed training between simulation and real-world [6], [8]. Our work focuses on zero-shot sim-toreal transfer for dynamics, and we review in more detail closely related works below. Although existing works have compared the accuracy of different physics simulators, in sim-to-sim [11] or sim-to-real settings [9], to the best of our knowledge ours is the first to perform an extensive evaluation of sim-to-real transfer methods for dynamics.</p>
<p>A. Dynamics Domain Randomisation</p>
<p>Domain randomisation for dynamics sim-to-real transfer is a popular and widely adopted method, which involves randomising simulation parameters, such as physical parameters (e.g. mass and friction), control parameters (e.g. actuator gains), time delays, and observation noise. Although this idea has been around for a long time [15], it has recently been re-popularised with the advent of Deep Learning and Reinforcement learning, where it has shown successes in tasks such as object pushing [24], object pivoting [4], legged locomotion [14], and tactile sensing [10]. Perhaps one of the biggest success stories of this approach is OpenAI's work on dexterous manipulation [1], [3]. Nonetheless, we believe these works underplay the amount of engineering effort required to achieve successful transfer, such as taskspecific tuning of simulation parameters, which is not a scalable solution for zero-shot transfer.</p>
<p>As such, a number of works have proposed methods to automatically determine the distribution from which to sample these parameters [1], [8], [21]- [23], [25], most of which either require real-world data collection or have thus far only been evaluated on sim-to-sim transfer. Our work is orthogonal to these methods, as our findings suggest that there might be simpler parameter sampling spaces that lead to just as strong policy transfer, and any of the methods we benchmark could be augmented by such an automated process.</p>
<p>B. System identification</p>
<p>System identification methods typically consist of incorporating carefully identified properties and behaviours of the real environment into the simulators themselves [2], [7], [8], [17], [19], [20], [28]. Our work focusses on the zero-shot regime, and Yu et al. [34], demonstrate that system identification can be used online, via training universal policies that can generalise across environments with different dynamics, and predicting the underlying dynamics parameters. This was extended in [35] by conditioning policies on a latent representations of the dynamics. However, both these methods were only evaluated in sim-to-sim settings, and in this work we put them to the test in realistic sim-to-real scenarios.</p>
<p>III. MODELLING THE REALITY GAP</p>
<p>A. Reinforcement Learning</p>
<p>A robotics control task can be modelled as Partially Observable Markov Decision Process (POMDP), M PO = {A, S, R, P, γ, O, P O }, with A, S, O the action, state and observation spaces, R the reward function, P and P O the transition and emission probability distributions and γ the discount factor. At time t, given an action a t , the state of the system s t transitions to s t+1 ∼ P(s t+1 |s t , a t ), and emits an observation o t ∼ P O (o t |s t ) and a reward r t = R(a t , s t ). To solve the task, an RL algorithm finds a parametric policy π θ (a t |o t ) that maximises the expected return,
J(θ) = E θ ∞ t=1 γ t r t . B. Sim-to-Real
Under this POMDP model, the real world may differ from the simulator in the transition probability P, and emission probability P O . Dealing with differences in P is referred to as dynamics sim-to-real transfer, and differences with P O as visual sim-to-real transfer when the observations are images. The policy π θ (a t |o t ) typically involves inferring the underlying state s t (implicitly or explicitly) resulting in an effective policy µ(a t |s t ). An important distinction between the dynamics and visual problems is that the policy µ should be adaptive to P and invariant to P O .</p>
<p>Domain randomisation is arguably the most popular way to deal with the sim-to-real transfer problem to date. It consists of sampling a set of simulation parameters ξ ∼ p(ξ), creating an ensemble of training POMDPs which differ in P and/or P O . Its aim is to find a policy π θ * that can generalise across all these environments, formally θ * = arg max θ E ξ J ξ (θ), where J ξ is the objective for each individual environment. In practice, a new set of parameters ξ is drawn from p(ξ) at the start of each training episode. The underlying hypothesis is that the real-world POMDP falls within the distribution of simulation POMDPs, allowing the simulation policy to transfer via interpolation.</p>
<p>In our work, we are mostly interested in the dynamics transfer problem, so we minimise the impact of discrepancies in P O by using proprioceptive information and pose extractors to make low-dimensional observations for our policies (see Section V-A). Nonetheless, effects such as observation noise, calibration errors and observation time delays are impossible to eliminate entirely. As such, although not further emphasising the distinction, we randomise both dynamics and observation parameters where appropriate.</p>
<p>IV. METHODS</p>
<p>In this section, we describe the framework we used to set up the simulator and implement the different sim-to-real transfer methods, which is illustrated in Fig. 2. We also describe the individual methods in detail, and distinguish between methods for randomising simulation parameters (IV-A), and methods for training simulation policies using these parameter distributions (IV-B).</p>
<p>A. Environment Randomisation Regimes</p>
<p>When creating a simulated environment for a real-world task, this environment is dependent upon a set of dynamics parameters. Choosing a distribution to sample these parameters from is a key challenge, and significant trial-and-error is typically required. A detailed breakdown of all simulator parameter values, distributions and ranges we used in our experiments can be found in our supplementary material, and here we describe the process we used in order to obtain</p>
<p>Set up the simulation environments</p>
<p>Set the robot's kinematics to the URDF specification Set the noise parameters to 0</p>
<p>Set up baseline parameters</p>
<p>Measure the object's dimensions Guess the object's physical parameters (e.g friction)</p>
<p>Adjust randomisation distributions</p>
<p>Initial Guess</p>
<p>Pre-training adjustment  those. In our implementations, we first find a set of baseline parameters which result in reasonable simulator behaviour (blue blocks in Fig. 2):</p>
<p>• Kinematics parameters of the robot arm are set to the values described in the robot's URDF file. • For other objects in the scene (the puck, the table, see Fig. 1), we set their geometric properties with physical measurements. Their physical parameters, such as friction, are notoriously hard to measure, and in our implementations we use educated guesses by looking up typical values for different materials, and visually inspecting for realistic behaviour on the simulator. • To optimise the dynamics parameters of the arm, such as controller gains, damping ratios and joint friction values, we use differential evolution [27], with a cost function that minimises the difference between some arbitrary control signal and the simulated robot's response (see left plot on Fig. 3). • Basic noise parameters, such as observation noise and time delays, are all set to 0. We now describe four randomisation regimes which further build upon these baseline values, each defining a different distribution of training environments for our policies.</p>
<p>No Randomisation (NR). Policies are trained using the baseline parameter values, with no randomisation.</p>
<p>Domain Randomisation (DR). Domain randomisation [3], [24] consists of sampling sets of simulator parameters in order to create new environments at each episode of training. These include physical parameters such as masses and friction coefficients, actuation parameters such as controller gains, observation parameters such as time delays or measurement noise, and unmodelled effect parameters such as action noise. A more detailed description can be found in Section V-A, with a full list in our supplementary material. The baseline practice for zero-shot sim-to-real domain randomisation consists of following a tedious manual calibration and trial-and-error process in order to set the sampling distributions of these parameters such that the task can be solved. It is not always clear from previous works how this manual adjustment process is conducted, and in our methodology we decomposed the process into three key stages (red blocks on Fig. 2):</p>
<p>1) Initial guess. We use a uniform distribution for parameters that are sampled from a fixed range, a loguniform distribution when sampling a multiplicative factor to the baseline value, and an equiprobable categorical distribution for discrete-value parameters. The initial range of these distributions is set using educated guesses, which relies on prior knowledge, domain expertise, and visual inspection of simulation behaviour. 2) Pre-training adjustment. We create a hard-coded control policy for each task and execute it several times in the real world. We then execute the same policy in simulation the same number of times, using the parameter distributions defined in 1) to randomise the simulation. We plot the resulting trajectories, tracking the end-effector position (x, y, z), as well as the position and orientation of objects involved in the task. One of these can be seen in the right diagram of Fig. 3, which illustrates the trajectory of the end-effector y position in the reaching task. Visually inspecting these plots, we adjust the ranges of the parameter distributions using trial-and-error until the trajectories obtained have similar variation to the real world trajectories. 3) Post-training adjustment. After training the task policies, we assess their performance both in simulation and the real world. If needed, we adjust the distributions, retrain the policies, and re-evaluate their performance. For a practical application, this would be repeated until the performance is satisfactory, relative to some real-world performance threshold for instance. In our experiments, we allowed for up to two post-training adjustments, in order to evaluate performance with minimal task-specific tuning. However, note that the policies themselves are never trained with real-world data. In following this procedure, several challenges arise. First, specific parameter combinations may result in destructive simulator behaviours that greatly hurt policy learning. Second, the effect of varying one parameter is dependent on the values of all the remaining parameters which makes iterative adjustment challenging. Third, the dimensionality of the parameter space is vast, ranging between 31 and 67 depending on the task, and several parameters have esoteric interpretations, specifically dependent on modelling details of the simulator used. In summary, adjusting these parameters requires heavy engineering efforts, significant domain-specific knowledge of the particular simulator and task, and typically post-training fine-tuning.</p>
<p>Random Force Injection (RFI) Rather than randomising dynamics parameters, an alternative is to simply inject random noise into an environment with a fixed set of dynamics parameters. To the best of our knowledge, RFI has only briefly been considered in literature as a baseline to realworld data system identification methods [17], or as an unmodelled effect in addition to the full DR procedure [1]. In this work, we examine RFI on its own merit, and show that this much simpler method can be just as effective as DR in crossing the reality gap.</p>
<p>A challenge in adding noise to simulator dynamics is the need to keep the visited simulation states stable, consistent, and physically realistic. This can be addressed by adding noise in the form of random generalised forces to the degrees of freedom of the simulator [17]. Formally, RFI augments the generalised equations of motion of the simulated rigid body system with random generalised forces f r ∼ P at each timestep, such that:
M(q)v + c(q, v) = 0 τ + K k J T k (q)f k + f r ,(1)
with q, v the generalised positions and velocities, K the number of external forces f k , J T k the Jacobian mapping from external forces to generalised coordinates, τ a torques vector, 0 a vector of zeros, M the mass matrix, and c a bias forces vector [17], [30].</p>
<p>These forces inject white noise with a mean of 0, so the only parameters to set is the range of the uniform force sampling distribution for each dimension. Those can be adjusted using the same initial guess, pre-training adjustment and post-training adjustment process described in the previous section. This is now a much simpler task than adjusting DR parameters, as (i) the dimensionality of the problem is much smaller, with only 7 to 13 dimensions to adjust depending on the task, and (ii) the range of random uniform noise is conceptually simple with predictable effects, and therefore easier to tune than more complex dynamics parameters. Overall, this results in significantly reducing the amount of time and expertise required to optimise these parameters for a particular task. Intuitively, RFI aims to diversify the states observed in simulation such that this encompasses the real-world distribution of states, even though there is no explicit adaptation to the underlying dynamics itself as is possible with DR. However, our experiments showed that despite this, in practice RFI is at least as effective as DR, under the constraint of limited task-specific tuning.</p>
<p>RFI with Observation Noise (RFI+). Conceptually, as RFI only injects noise into the dynamics of the simulator, it only accounts for the reality gap in the transition distribution P of the POMDP underlying the task. As mentioned earlier, however, in real-world tasks it is impossible to completely eliminate the reality gap in the emission probability P O . In order to take into account these differences, we formulate the RFI+ training regime, which augments RFI by adding noise affecting the observation space of the policy, but not the underlying state of the world. This includes white noise in the observations, but also observation time delays. The sampling distributions for the parameters here are the same as their counterparts in DR and RFI. Overall, this adds another 2 to 7 parameters that need to be tuned, but maintains the desirable properties of RFI mentioned before.</p>
<p>B. Transfer Policies</p>
<p>From previous works, we have identified four main principles underlying zero-shot domain adaptation methods for dynamics [1], [3], [24], [34], [35]. In this section we briefly describe these methods and our particular design decisions, with further implementation details in our supplementary material.</p>
<p>Conservative Policy. A conservative policy receives a single environment observation input at time t in order to compute the appropriate control action. Conceptually, this policy cannot infer information about the environment dynamics to help adjust its behaviour, as it does not have access to the effect of its actions on the state. As such, its optimal behaviour should be to take small, conservative actions, such that if a large unexpected state transition occurs, the policy can still recover.</p>
<p>Adaptive Policy. An adaptive policy has access to a sequence of observations and actions up to the current timestep t. Such a policy aims to create an internal representation of the current environment dynamics online, using the observed behaviour, and adapt to these dynamics in real time. In order to implement the adaptive policy, we simply augment the core architecture with an LSTM layer branch as in [24].</p>
<p>Universal Policy with Online System Identification (UPOSI). With the UPOSI framework, Yu et al. [34] suggest to explicitly represent the dynamics of an environment as the set of simulator parameters that define it, and regress those values with an online system identification (OSI) module from a history of state-action inputs. Subsequently, these predicted parameters are input to a universal policy (UP), alongside with the environment state, to enable online adaptation to dynamics.</p>
<p>In training UPOSI, we follow the procedure described in the original paper [34]. We note that as opposed to [34], we predict a much more diverse set of parameters, characterising dynamics of both the robot and external objects. In order to facilitate this prediction, our OSI network input is made by stacking 5 vectors that include (i) the task-specific policy input states defined in Section V-A, (ii) the policy actions, (iii) the robot's joint angles, (iv) the robot's joint velocities, and (v) the low-level joint velocity control derived from the actions.</p>
<p>Environment Probing Interaction Policies (EPI). Zhou et al. [35] proposed a framework similar to UPOSI, but with an alternative way of representing the environment dynamics. In their work, a probing policy interacts with the environment for a small number of timesteps, and the resulting stateaction trajectory is passed in a dynamics embedding module that condenses it into a latent representation z. This value z does not have a clear physical interpretation, but is trained in a way to distill the dynamics of the environment. In the zero-shot setting, the universal task policy picks up the task execution where the probing policy left off, and uses z in its input to gain information about the environment's dynamics.</p>
<p>V. EXPERIMENTS A. Tasks</p>
<p>In order to evaluate our implementations, we tested each on three different tasks of varying difficulties, Reaching, Pushing, and Sliding, which we now describe. Further experimental details can be found in the supplementary material.</p>
<p>All our simulation environments are created using the MuJoCo [30] physics engine and the Robosuite [12] toolkit. All our experiments use a 7-DOF Sawyer robot arm with a main control loop at 10 Hz. The pushing and reaching task policies output end-effector Cartesian target velocities, converted to joint velocities through inverse kinematics, and the sliding task policies directly output target joint velocities. Object poses are extracted in the real world using April tags [32]. For pushing, in order to get accurate measurements throughout the workspace, two cameras in different positions were used. All of our policy inputs are expressed in the endeffector frame unless otherwise specified. All our tasks are finite-horizon, and do not terminate if the goal is reached before the end of the episode. For the sliding task, an episode may terminate early if the object falls off the sliding plate.</p>
<p>Reaching. In the reaching task, a target within an allowed region is specified on the table, and the robot has 6 s to reach a goal 2.5 cm above that target (see Fig. 1). Three goals (easy, intermediate, hard) are specified to test this task, as is shown in Fig. 4.</p>
<p>The policy inputs consist of the vector between the current position of the end-effector and the goal, concatenated with the 3D Cartesian velocity of the end-effector. In the real world, these are obtained using the proprioceptive sensors of the robot. The actions output by the policy form endeffector Cartesian velocities. The orientation of the endeffector constantly points vertically downwards. For domain randomisation, we randomise the control frequency, the PID feedback frequency, the PID gains, the observation noise variance, the action noise range, the robot link masses, and the joint damping, friction and armature values, giving a total of 56 parameters.  Pushing. The pushing task consists of pushing over 8 s a cuboid puck on the table, from a starting location to some specified goal within an allowed region (see Fig. 1). Again, at test time three goals are specified, as seen in Fig. 4.</p>
<p>The plane of motion is defined by the table top, and the policy inputs consist of a concatenation of the in-plane components of (i) the vector between the end-effector and the puck, (ii) the vector between the puck and the goal, (iii) the end-effector and puck Cartesian velocities, and (iv) a sinecosine encoding of the angle of rotation of the object around the surface normal of the table. The policy actions consist of in-plane end-effector Cartesian velocity commands, and the orientation of the end-effector is kept constant.</p>
<p>It is interesting to note that for pushing, the reality gap in observations is much stronger than for reaching, as pose information from fiducial markers (used for the puck) is much less accurate than the robot's proprioceptive sensors. Additionally to the parameters considered for reaching, domain randomisation here also includes time delays in the observations, object observation noise, and object physical properties such as mass, size and friction coefficients, for a total of 67 parameters.</p>
<p>Sliding. For the sliding task, we attach the end-effector of the robot to a rectangular cardboard plate, and designate a starting area and a specific goal position as seen in Fig. 1 and Fig. 4. The purpose of the robot is to tilt the sliding plate in a way to make the object slide to the goal over 6 s. As opposed to reaching and pushing, an episode may terminate early if the puck falls off the plate, and single goal location is used over both training and testing to facilitate learning.</p>
<p>In this task, we operate directly in joint-velocity space, and we only allow the last two joints of the robot to move. The policy inputs consist of the object Cartesian position and velocity in the goal frame, the sine-cosine encoding of the rotation angle about the sliding plane normal, and the joint positions and velocities of the last two joints of the robot. The policy actions consist of joint velocity targets for the last two joints of the robot. The same parameters as with the pushing task are randomised, except that only the two relevant joints are considered.</p>
<p>B. Network Architectures and RL Methods</p>
<p>All our methods were fully trained in simulation using RL, with no further fine-tuning in the real world. For pushing Rates are averaged over all tested goals. The best performing method in the real world for each environment is in bold. The '-' indicates that the policy was too dangerous to run full experiments.  For pushing, the reward was augmented with the negative distance between the end-effector and the object, and finally for sliding, it was augmented with a negative reward if the object falls off the sliding platform. In order to make our benchmarking as fair as possible, we used a common core neural network architecture and the same reinforcement learning techniques for all methods. We used twin delayed deep deterministic policy gradients (TD3) [13] with a fixed set of training hyperparameters as our main algorithm, and proximal policy optimisation (PPO) [26] to train the probing policy in EPI. The network is fully-connected with 5 layers for the policy network and 4 layers for the Q-network, each with 512 units per layer and ReLU activations. The final layers of the policy networks have Tanh activations for normalising the action outputs. For UPOSI, the UP network follows our core architecture, and the OSI network has 4 fully connected hidden layers of [512, 256, 128, 64] units, along with dropout layers. Deeper networks for the UP and longer sequences of motion state inputs to the OSI were considered, but neither seemed to improve results. For EPI, the dynamics embedding network has 3 hidden layers of 512 nodes and ReLU activations, and a Tanh activation for the output layer. Training z also requires an extra prediction network, which in our case has 4 hidden layers with ReLU activations. We set z to be 10dimensional, and allowed the probing policy to execute 10 timesteps before the task-policy takes over.</p>
<p>C. Experimental Setup</p>
<p>For each of our tasks, as illustrated in Fig. 2, we trained both of the Conservative and Adaptive policies with each of NR, DR, RFI and RFI+, as well as the UPOSI and EPI algorithms using the same environment randomisations as DR. In order to evaluate each method, we compare their performance in both simulation and reality, since some methods may find it difficult to learn even in simulation without extensive hyperparameter tuning. Therefore, we set up in simulation precisely the same experimental conditions (starting state, goal position), and deployed the same trained policies. In the real world, for each task and goal position considered, 7 trajectories were gathered, giving a total 469 real world trajectories. In simulation, tests for each method were carried out with environments randomised the same way as during the training of that method, and 50 trajectories were gathered for each task and goal giving a total of 3500 trajectories, from which the average was calculated.</p>
<p>An episode was considered a success if the endeffector/object were within some threshold distance of the specified goal location over the final 0.5 s of execution. For the reaching task this threshold was 1 cm, for the pushing task it was 3 cm, and for the sliding task it was 2.3 cm. For the sliding task, we further recorded how often the object fell from the plate. Table I shows a summary of our results, and a more detailed breakdown is in the supplementary material. Fig. 5 shows an illustration of the real-world trajectories for the reaching task. Examples of some trained policies are shown in our video.</p>
<p>D. Results</p>
<p>From Table I it becomes apparent that RFI performed best across the board, and with most consistency. This is significant considering that DR took several days to tune, and we used post-training adjustment in an attempt to raise its performance (see Section IV), while the RFI distribution range was tuned within a few hours, and with no further taskspecific post-training processing required. Nonetheless, when considering the full breakdown of results in our supplementary material, RFI did not always achieve the highest returns, and there was no strong overall winner. The trajectories plotted in Fig. 5 also show similarities in performances across all methods, except for NR and UPOSI which performed significantly worse. We can, however, conclude that RFI overall performed no worse than other methods, whilst being significantly easier to implement, and significantly quicker to set simulation parameters for.</p>
<p>We further note that RFI generally performed better than   RFI+, which is surprising given that RFI+ conceptually accounts more thoroughly for the sim-to-real discrepancies, and we conjecture that RFI+ could benefit from further tuning of the parameter ranges. Nonetheless, an interesting observation is that for the sliding task, RFI+ resulted in a more stable policy, rarely falling off the platform but also unable to reach the goal, while RFI resulted in a much more aggressive policy, often falling off the platform but sometimes managing to successfully complete the task. Overall, this highlights a trend that seems to form: the more parameters that are considered for environment randomisation, the more tuning of those parameters that needs to be done for the performance to be satisfactory, while simpler alternatives still seem to work just as well.</p>
<p>Finally, an important general trend is that our adaptive policies performed worse than our conservative policies. As such, they seem to have failed in capturing and exploiting environment dynamics information, which is contrary to what some previous works suggest [1]. A possible explanation for this is that, due to the short episode length, the adaptive policy does not have enough time to extract meaningful information about each environment before the environment resets. It is also possible that further careful tuning of our network and training hyperparameters might have allowed us to observe these properties. At the very least, we can conclude that adaptive policies are not always preferable to conservative policies as other works seem to suggest, because they appear to require stronger engineering efforts and fine tuning to lift their performance above a conservative policy.  </p>
<p>E. Investigating System Identification</p>
<p>To better understand the UPOSI and EPI results, we tested their universal policy and system identification components in isolation. For testing the universal policies, we show in Table II the effect of using white noise as input to the UP, rather than the predicted parameters. In order to test the OSI component of UPOSI, we show in Table III the average % error in the predictions of OSI across our simulated tasks, relative to the range of the OSI predictions. We can not do this with EPI, so instead we take the following approach: we take three very different sets of parameters for the reaching task, and for each roll out 50 trajectories. We then show in Fig 6 a two dimensional t-SNE [31] graph of the latent variables z obtained for each of the three parameter sets.</p>
<p>As we can see from Table II, with the exception of UPOSI on the pushing task, both universal policies are utilising the system identification module, since this performs significantly better than a policy conditioned on white noise. However, Table III and Fig 6 show that the system identification modules are overall not very accurate. In the case of EPI for instance, we see that although the three parameter sets tested are quite different, the latent space does not differentiate well between them. This suggests that inferring dynamics information from state-action sequences is a difficult task, and in fact ill-posed, as different parameter combinations may result in the same observed behaviour.</p>
<p>VI. CONCLUSIONS</p>
<p>In this study, we have thoroughly examined the fundamental aspects and engineering processes involved in zero-shot sim-to-real transfer for dynamics. We compared, across three tasks, several methods which previously either have not been given significant exposure, or have been evaluated only on simplified environments. Amongst these, we have not found a clear winner, although it appears that all methods relying on inferring environment dynamics online trail behind. Overall, we believe that an important, and somewhat surprising, takeaway from our results is that, for zero-shot transfer where the amount of task-specific fine-tuning is limited, the standard practice of domain randomisation performed no better than simple random force injection. This is significant, because random force injection is much more practical to set up than full dynamics parameter randomisation, and dramatically lowers the engineering efforts and expertise required to achieve sim-to-real transfer.</p>
<p>1Fig. 1 :
1The Robot Learning Lab at Imperial College London eugene.valassakis15@imperial.ac.Illustration of our experiment tasks in simulation and the real world.</p>
<p>Fig. 2 :
2Block diagram of our methodology. On the left are depicted the steps followed to set up the simulator training environments, and on the right are the different combinations of randomisation regimes and policy training methods tested.</p>
<p>Fig. 3 :
3Illustration of the reality gap (left) and the effect of randomising simulator parameters (right). For a constant velocity command, several real world trajectories and simulation trajectories are illustrated. Bold lines represent the mean. The y-axis corresponds to the y-component of endeffector velocity. The "real" and "command" curves are the same and repeated in all plots, for comparison.</p>
<p>Fig. 4 :
4Start and goal positions for each task in the real world. Easy goals are well centred in the area of possible goals, hard goals are the furthest to the initial position, and intermediate goals are in the middle. For reaching, the end-effector begins directly above the easy goal.</p>
<p>, we randomised at each episode the starting positions of the end-effector and the goal within pre-specified regions, and for pushing we placed the puck next to the end-effector. For sliding, we randomised only the starting position of the puck within a specified region. In all tasks, the reward had a positive component for getting within a certain threshold of the goal, a negative reward component if the joint limits of the robot are hit, and a negative reward component consisting of the negative distance between the end-effector/puck and the goal. For reaching, the reward was augmented with a negative component for the end-effector hitting the table.</p>
<p>Fig
. 6: t-SNE plot of EPI latent vectors z obtained for three distinct sets of environment dynamics.</p>
<p>TABLE I :
IMain results for all experiments.</p>
<p>Fig. 5: Horizontal trajectories of the end-effector over 5 trials on the reaching task. The axes distances are in meters.0.10 </p>
<p>0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>Concervative Policy: 
NR </p>
<p>0.10 
0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>Concervative Policy: 
DR </p>
<p>0.10 
0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>Concervative Policy: 
RFI </p>
<p>0.10 
0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>Concervative Policy: 
RFI+ </p>
<p>Easy Goal: Solid-Real, Dotted-Sim 
Intermediate Goal: Solid-Real, Dotted-Sim 
Hard Goal: Solid-Real, Dotted-Sim </p>
<p>0.10 
0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>Adaptive Policy: 
NR </p>
<p>0.10 
0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>Adaptive Policy: 
DR </p>
<p>0.10 
0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>Adaptive Policy: 
RFI </p>
<p>0.10 
0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>Adaptive Policy: 
RFI+ </p>
<p>0.10 
0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>UPOSI </p>
<p>0.10 
0.05 0.00 
0.05 
0.10 </p>
<p>0.100 
0.075 
0.050 
0.025 
0.000 
0.025 
0.050 
0.075 
0.100 </p>
<p>EPI </p>
<p>TABLE II :
IIComparing learned and noise inputs to the UP. Success rate across all goals for each task. When the success rate of both the learned and noise inputs are 0.0, the average return is shown in parenthesis.Reaching 
Pushing 
Sliding 
Learned 
Noise 
Learned 
Noise 
Learned 
Noise </p>
<p>TABLE III :
IIIError in the OSI prediction for each task.Reaching 
Pushing 
Sliding 
10.9 % 
11.0 % 
12.7 % </p>
<p>Solving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R Ribas, arXiv:1910.07113arXiv preprintI. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al. Solving rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.</p>
<p>Tunenet: Oneshot residual tuning for system identification and sim-to-real robot task transfer. A Allevato, E S Short, M Pryor, A Thomaz, Conference on Robot Learning. A. Allevato, E. S. Short, M. Pryor, and A. Thomaz. Tunenet: One- shot residual tuning for system identification and sim-to-real robot task transfer. In Conference on Robot Learning, pages 445-455, 2020.</p>
<p>Learning dexterous in-hand manipulation. O M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mc-Grew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, The International Journal of Robotics Research. 391O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. Mc- Grew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.</p>
<p>R Antonova, S Cruciani, C Smith, D Kragic, arXiv:1703.00472Reinforcement learning for pivoting task. arXiv preprintR. Antonova, S. Cruciani, C. Smith, and D. Kragic. Reinforcement learning for pivoting task. arXiv preprint arXiv:1703.00472, 2017.</p>
<p>Meta reinforcement learning for sim-to-real domain adaptation. K Arndt, M Hazara, A Ghadirzadeh, V Kyrki, arXiv:1909.12906arXiv preprintK. Arndt, M. Hazara, A. Ghadirzadeh, and V. Kyrki. Meta rein- forcement learning for sim-to-real domain adaptation. arXiv preprint arXiv:1909.12906, 2019.</p>
<p>Learning to play table tennis from scratch using muscular robots. D Büchler, S Guist, R Calandra, V Berenz, B Schölkopf, J Peters, arXiv:2006.05935arXiv preprintD. Büchler, S. Guist, R. Calandra, V. Berenz, B. Schölkopf, and J. Peters. Learning to play table tennis from scratch using muscular robots. arXiv preprint arXiv:2006.05935, 2020.</p>
<p>Sim2real2sim: Bridging the gap between simulation and real-world in flexible object manipulation. CoRR, abs. P Chang, T Padir, P. Chang and T. Padir. Sim2real2sim: Bridging the gap between simulation and real-world in flexible object manipulation. CoRR, abs/2002.02538, 2020.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEEY. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox. Closing the sim-to-real loop: Adapting simula- tion randomization with real world experience. In 2019 International Conference on Robotics and Automation (ICRA), pages 8973-8979. IEEE, 2019.</p>
<p>Benchmarking simulated robotic manipulation through a real world dataset. J Collins, J Mcvicar, D Wedlock, R Brown, D Howard, J Leitner, IEEE Robotics and Automation Letters. 51J. Collins, J. McVicar, D. Wedlock, R. Brown, D. Howard, and J. Leitner. Benchmarking simulated robotic manipulation through a real world dataset. IEEE Robotics and Automation Letters, 5(1):250- 257, 2019.</p>
<p>Sim-to-real transfer for optical tactile sensing. Z Ding, N Lepora, E Johns, IEEE International Conference on Robotics and Automation (ICRA). 2020Z. Ding, N. Lepora, and E. Johns. Sim-to-real transfer for optical tactile sensing. In IEEE International Conference on Robotics and Automation (ICRA), 2020.</p>
<p>Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx. T Erez, Y Tassa, E Todorov, 2015 IEEE international conference on robotics and automation (ICRA). IEEET. Erez, Y. Tassa, and E. Todorov. Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx. In 2015 IEEE international conference on robotics and automation (ICRA), pages 4397-4404. IEEE, 2015.</p>
<p>Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. L Fan, Y Zhu, J Zhu, Z Liu, O Zeng, A Gupta, J Creus-Costa, S Savarese, L Fei-Fei, Conference on Robot Learning. L. Fan, Y. Zhu, J. Zhu, Z. Liu, O. Zeng, A. Gupta, J. Creus-Costa, S. Savarese, and L. Fei-Fei. Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. In Conference on Robot Learning, 2018.</p>
<p>Addressing function approximation error in actor-critic methods. S Fujimoto, H Van Hoof, D Meger, arXiv:1802.09477arXiv preprintS. Fujimoto, H. Van Hoof, and D. Meger. Addressing function approxi- mation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 4265872J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics, 4(26):eaau5872, 2019.</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, European Conference on Artificial Life. SpringerN. Jakobi, P. Husbands, and I. Harvey. Noise and the reality gap: The use of simulation in evolutionary robotics. In European Conference on Artificial Life, pages 704-720. Springer, 1995.</p>
<p>Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task. S James, A J Davison, E Johns, CoRLS. James, A. J. Davison, and E. Johns. Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task. In CoRL, 2017.</p>
<p>Modelling generalized forces with reinforcement learning for sim-to-real transfer. R Jeong, J Kay, F Romano, T Lampe, T Rothorl, A Abdolmaleki, T Erez, Y Tassa, F Nori, arXiv:1910.09471arXiv preprintR. Jeong, J. Kay, F. Romano, T. Lampe, T. Rothorl, A. Abdol- maleki, T. Erez, Y. Tassa, and F. Nori. Modelling generalized forces with reinforcement learning for sim-to-real transfer. arXiv preprint arXiv:1910.09471, 2019.</p>
<p>Residual reinforcement learning for robot control. T Johannink, S Bahl, A Nair, J Luo, A Kumar, M Loskyll, J A Ojea, E Solowjow, S Levine, 2019 International Conference on Robotics and Automation (ICRA). IEEET. Johannink, S. Bahl, A. Nair, J. Luo, A. Kumar, M. Loskyll, J. A. Ojea, E. Solowjow, and S. Levine. Residual reinforcement learning for robot control. In 2019 International Conference on Robotics and Automation (ICRA), pages 6023-6029. IEEE, 2019.</p>
<p>Sim2real transfer for reinforcement learning without dynamics randomization. M Kaspar, J D M Osorio, J Bock, arXiv:2002.11635arXiv preprintM. Kaspar, J. D. M. Osorio, and J. Bock. Sim2real transfer for rein- forcement learning without dynamics randomization. arXiv preprint arXiv:2002.11635, 2020.</p>
<p>Learning active task-oriented exploration policies for bridging the sim-to-real gap. J Liang, S Saxena, O Kroemer, arXiv:2006.01952arXiv preprintJ. Liang, S. Saxena, and O. Kroemer. Learning active task-oriented exploration policies for bridging the sim-to-real gap. arXiv preprint arXiv:2006.01952, 2020.</p>
<p>B Mehta, M Diaz, F Golemo, C J Pal, L Paull, arXiv:1904.04762Active domain randomization. arXiv preprintB. Mehta, M. Diaz, F. Golemo, C. J. Pal, and L. Paull. Active domain randomization. arXiv preprint arXiv:1904.04762, 2019.</p>
<p>Learning domain randomization distributions for transfer of locomotion policies. M Mozifian, J C G Higuera, D Meger, G Dudek, arXiv:1906.00410arXiv preprintM. Mozifian, J. C. G. Higuera, D. Meger, and G. Dudek. Learning domain randomization distributions for transfer of locomotion policies. arXiv preprint arXiv:1906.00410, 2019.</p>
<p>Bayesian domain randomization for sim-to-real transfer. F Muratore, C Eilers, M Gienger, J Peters, arXiv:2003.02471arXiv preprintF. Muratore, C. Eilers, M. Gienger, and J. Peters. Bayesian domain ran- domization for sim-to-real transfer. arXiv preprint arXiv:2003.02471, 2020.</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). IEEEX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to- real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA), pages 1-8. IEEE, 2018.</p>
<p>F Ramos, R C Possas, D Fox, arXiv:1906.01728Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators. arXiv preprintF. Ramos, R. C. Possas, and D. Fox. Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators. arXiv preprint arXiv:1906.01728, 2019.</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Prox- imal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. R Storn, K Price, Journal of global optimization. 114R. Storn and K. Price. Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. Journal of global optimization, 11(4):341-359, 1997.</p>
<p>J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.10332Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprintJ. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bo- hez, and V. Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEEJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on, pages 23-30. IEEE, 2017.</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, IEEE/RSJ International Conference on Intelligent Robots and Systems. E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033, 2012.</p>
<p>Visualizing data using t-SNE. L Van Der Maaten, G Hinton, Journal of Machine Learning Research. 9L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579-2605, 2008.</p>
<p>AprilTag 2: Efficient and robust fiducial detection. J Wang, E Olson, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEJ. Wang and E. Olson. AprilTag 2: Efficient and robust fiducial detection. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4193-4198. IEEE, oct 2016.</p>
<p>W Yu, C K Liu, G Turk, arXiv:1810.05751Policy transfer with strategy optimization. arXiv preprintW. Yu, C. K. Liu, and G. Turk. Policy transfer with strategy optimization. arXiv preprint arXiv:1810.05751, 2018.</p>
<p>Preparing for the unknown: Learning a universal policy with online system identification. W Yu, J Tan, C K Liu, G Turk, Robotics: Science and Systems XIII, Massachusetts Institute of Technology. N. M. Amato, S. S. Srinivasa, N. Ayanian, and S. KuindersmaCambridge, Massachusetts, USAW. Yu, J. Tan, C. K. Liu, and G. Turk. Preparing for the unknown: Learning a universal policy with online system identification. In N. M. Amato, S. S. Srinivasa, N. Ayanian, and S. Kuindersma, editors, Robotics: Science and Systems XIII, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA, July 12-16, 2017, 2017.</p>
<p>W Zhou, L Pinto, A Gupta, arXiv:1907.11740Environment probing interaction policies. arXiv preprintW. Zhou, L. Pinto, and A. Gupta. Environment probing interaction policies. arXiv preprint arXiv:1907.11740, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
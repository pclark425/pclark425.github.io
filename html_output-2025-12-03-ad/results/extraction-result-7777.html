<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7777 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7777</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7777</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-277467991</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.00255v2.pdf" target="_blank">SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers</a></p>
                <p><strong>Paper Abstract:</strong> This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models. The best-performing LLM using \ModelName~achieves only 39% execution accuracy, highlighting the benchmark's difficulty. Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We make available our benchmark and code at https://github.com/xyzCS/SciReplicate-Bench and project homepage at https://xyzcs.github.io/scireplicate.github.io/.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7777.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7777.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RG-Acc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning Graph Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel metric introduced in this paper that quantifies how well an LLM's extracted reasoning steps and their data-flow relationships match a reference reasoning graph derived from code comments and structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (model versions listed in paper: GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini-low/med/high)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / natural language processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>algorithm comprehension / representation of reasoning steps (used for algorithm reproduction)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reasoning Graph Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Construct a DAG where nodes correspond to discrete reasoning-step comments aligned with code snippets and edges represent data-flow (a variable used in one snippet last defined/modified in another). Compare generated graph G_g to reference G_r by node matching (mapping comments via GPT-4o) and edge matching (BFS between matched nodes), then compute a weighted score using node and edge significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>S_r (reasoning graph accuracy score)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>S_r = sum_{matched nodes} s_n + sum_{matched edges} s_e; node significance s_n is computed from complexity measures (number of variable definitions/usages, function calls, arithmetic ops, lines of code) normalized across the reference graph; edge significance s_e = product of connected node significances normalized. Reported on a 0–1 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Validated on a subset: 3 PhD students independently assessed reasoning graph accuracy for 20 tasks generated by two models (GPT-4o-mini and O3-mini-medium). Mean agreement reported and Pearson correlation with automated metric computed (r = 0.7518, p < 0.005).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average RG accuracy ≈ 0.716 without agent assistance; Paper Agent yields +0.009, Code Agent +0.036, combined +0.037 (averages reported). Node matching via GPT-4o repeated three times (temp=0, top-p=1) and averaged to reduce variability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Automated RG-Acc strongly correlates with human judgments (Pearson r = 0.7518, p < 0.005) on the evaluated subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Node matching depends on GPT-4o and can introduce randomness; authors mitigated via deterministic sampling and averaging but note residual variability. Metric relies on insertion of non-overlapping, non-nested comment snippets by the generator, which may be brittle in practice. Significance weighting requires accurate counting of code features and assumes those capture semantic importance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7777.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7777.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExecAcc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An execution-based correctness metric: generated code is integrated into the repository and run on verification test cases; code is counted correct only if outputs match the reference for all test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (model versions listed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / natural language processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>executable implementation of algorithm (used to validate algorithmic claims)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Execution Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Insert the generated function/method into the target repository, run the repository-specific verification suite in an isolated Python environment, and mark the task as correct if all automated test cases match the reference outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Execution accuracy (fraction of tasks where generated code passes the verification suite)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion (0–1) of tasks for which generated implementation produces identical outputs to the reference across the provided test cases (typically 10 test cases per task when available).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciReplicate-Bench (100 tasks from 36 papers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average execution accuracy across evaluated LLMs = 0.235; best model (Claude-Sonnet-3.7 with Sci-Reproducer) reached 0.390. Ablation: Sci-Reproducer improved average execution accuracy by +0.181 relative to NoAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Some tasks have fewer than 10 test cases; execution correctness can be sensitive to hardware/runtime differences (authors instruct local runs and fix randomness via seeds and deterministic replacements); passing binary tests may not capture logical differences in implementations that produce similar outputs for test inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7777.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7777.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code similarity metric combining n-gram BLEU with syntactic (AST) and semantic (data-flow graph) information to better evaluate generated code versus reference implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (evaluated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / natural language processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>code similarity evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute a composite score that augments classical BLEU with AST-based syntactic equivalence and DFG-based semantic equivalence between generated and reference code.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>CodeBLEU score</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Composite score (reported on 0–1 scale in this paper); the specific CodeBLEU weighting follows prior CodeBLEU implementations (BLEU + AST + DFG components).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average CodeBLEU reported ≈ 0.320 across models; Sci-Reproducer yields average +0.057 improvement in CodeBLEU compared to agent-less generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Code similarity can be a poor proxy for functional correctness (high CodeBLEU may not guarantee correct behavior, and low CodeBLEU may still correspond to functionally correct alternative implementations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7777.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7777.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DepRecall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependency/API Recall</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recall metrics measuring how many required intra-file dependencies, cross-file dependencies, and external APIs from the reference implementation are identified and used in the generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (evaluated with/without Sci-Reproducer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / software engineering for ML</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dependency identification evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Recall of Intra-file / Cross-file Dependencies and External APIs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Annotators provide ground-truth lists of internal variable/function/class dependencies and external API usages for each task; compute recall as fraction of these reference dependency items correctly identified/used by the generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recall per category (intra-file, cross-file, API)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Recall = (# of reference dependency/API items present in generated code) / (total # of reference dependency/API items); reported on 0–1 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>With Sci-Reproducer, average recall gains relative to no-agent baseline were +0.441 (intra-file), +0.239 (cross-file), and +0.100 (API). Example: Claude-Sonnet-3.7 achieved intra-file 0.776, cross-file 0.636, API 0.626 when using Sci-Reproducer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Accurate dependency recall requires comprehensive, correct annotations; some dependencies are implicit or context-dependent and may be hard to detect automatically. Recall does not capture correctness of usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7777.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7777.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark introduced in this paper consisting of 100 algorithm reproduction tasks derived from 36 NLP papers (2024), with reference implementations, reasoning graph annotations, dependency annotations, and verification suites.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (benchmark used to evaluate many LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / natural language processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark for algorithm reproduction and code-based validation of algorithmic claims</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provides structured inputs (function signatures, LaTeX algorithm description, literature context, repository context), ground-truth implementations, annotated reasoning graphs, dependency annotations, and an isolated test environment with verification scripts (typically 10 test cases per task) to evaluate LLMs' ability to reproduce algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Supports Execution Accuracy, Reasoning Graph Accuracy, CodeBLEU, Dependency/API Recall</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>N/A (platform providing tasks and ground truth used by the listed metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciReplicate-Bench (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Annotation process: human annotators refactored repositories, created reasoning-comment mappings, documented dependencies, and produced verification suites; annotating each paper ≈12 hours on average. Human evaluation: 3 PhD students assessed RG-Acc on 20 tasks for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Benchmark contains 100 tasks from 36 papers; baseline aggregate results reported (ExecAcc avg 0.235, CodeBLEU 0.320, RG-Acc avg 0.716 without agents).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Some algorithm LaTeX descriptions miss implementation details; a few tasks have fewer than 10 test cases; reliance on refactored repositories and annotation quality; attempts to reduce data leakage by selecting 2024 papers and refactoring to ensure deterministic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7777.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7777.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sci-Reproducer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sci-Reproducer (dual-agent framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-agent system introduced in this paper: a Paper Agent that retrieves and summarizes literature-context information (RAG-like, uses ReAct strategy) and a Code Agent that searches repository code, web resources, and uses a compiler for iterative testing/debugging to produce executable implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Agent wrapper over host LLMs (paper evaluates Sci-Reproducer operating with multiple LLMs listed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP / reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>agentic framework for algorithm reproduction (action-based evaluation pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Sci-Reproducer (framework-level ablations and action-invocation analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate contributions of Paper Agent and Code Agent via ablation experiments (NoAgent, NoPaperAgent, NoCodeAgent) and track tool/action usage statistics; measure downstream effects on Execution Accuracy, CodeBLEU, RG-Acc, and dependency recall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Improvement deltas on Execution Accuracy, CodeBLEU, RG-Acc, and recall metrics; tool invocation counts</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported as absolute or relative improvements, e.g., average ExecAcc increase +0.181 with Sci-Reproducer vs NoAgent; per-model tool usage counts.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Sci-Reproducer reduces syntax errors (from ≈80% to ≈24–29%), increases ExecAcc on average by +0.181; Code Agent contributes more to reducing syntax errors and recall gains than Paper Agent; reasoning LLMs use tools less and exhibit smaller gains due to 'overthinking'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Effectiveness depends on LLM's propensity to use tools (reasoning LLMs underutilize tools); framework cannot dynamically access runtime information during generation except via explicit Compiler action; agent performance sensitive to prompt design and action definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7777.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7777.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human RG Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation of Reasoning Graph Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human validation experiment where expert annotators rated the correctness of generated reasoning graphs to validate the automated RG-Acc metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human expert rating validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human rating of reasoning graphs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Three PhD students independently assessed RG-Acc for 20 tasks produced by two LLMs under Sci-Reproducer; results compared to automated GPT-4o-based RG-Acc scoring to measure alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human mean scores per task and Pearson correlation with automated RG-Acc</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Human ratings (scale not explicitly quantified in text) aggregated and correlated with automated scores; Pearson r reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Subset of SciReplicate-Bench (20 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three PhD student annotators, 20 tasks, independent assessments; reported mean and standard deviation and Pearson correlation r = 0.7518 (p < 0.005) with automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Strong positive alignment between human judgments and automated RG-Acc (r = 0.7518, p < 0.005).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small sample (20 tasks) and small number of human raters; human rating scale and calibration details not exhaustively described; further large-scale human validation recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7777.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7777.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verification Suite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verification Suite and Test Environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Per-task verification suites provided in the benchmark: isolated Python environments and test scripts (typically 10 cases per task) used to automatically verify functional correctness of generated implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automated test harness for functional validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Verification Suite Execution</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Annotators created deterministic Python execution environments, fixed randomness, and implemented task-specific comparison scripts; generated code is executed in these environments to compare outputs against references.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used to compute Execution Accuracy (binary pass/fail per task and per test case comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Task-level correctness determined by matching outputs for provided test cases; typical suite size = 10 test cases where possible (some tasks fewer).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Verification suites used across benchmark; authors ensured deterministic behavior by fixing seeds and removing nondeterministic constructs; some tasks had fewer than 10 test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Test-case coverage may be limited; some tasks inherently stochastic or hardware-sensitive; authors recommend local runs and fixed seeds to mitigate variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PaperBench <em>(Rating: 2)</em></li>
                <li>Paper2CodeBench <em>(Rating: 2)</em></li>
                <li>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark <em>(Rating: 2)</em></li>
                <li>MLGym <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7777",
    "paper_id": "paper-277467991",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "RG-Acc",
            "name_full": "Reasoning Graph Accuracy",
            "brief_description": "A novel metric introduced in this paper that quantifies how well an LLM's extracted reasoning steps and their data-flow relationships match a reference reasoning graph derived from code comments and structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini variants)",
            "model_size": "various (model versions listed in paper: GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini-low/med/high)",
            "scientific_domain": "computer science / natural language processing",
            "theory_type": "algorithm comprehension / representation of reasoning steps (used for algorithm reproduction)",
            "evaluation_method_name": "Reasoning Graph Accuracy",
            "evaluation_method_description": "Construct a DAG where nodes correspond to discrete reasoning-step comments aligned with code snippets and edges represent data-flow (a variable used in one snippet last defined/modified in another). Compare generated graph G_g to reference G_r by node matching (mapping comments via GPT-4o) and edge matching (BFS between matched nodes), then compute a weighted score using node and edge significance.",
            "evaluation_metric": "S_r (reasoning graph accuracy score)",
            "metric_definition": "S_r = sum_{matched nodes} s_n + sum_{matched edges} s_e; node significance s_n is computed from complexity measures (number of variable definitions/usages, function calls, arithmetic ops, lines of code) normalized across the reference graph; edge significance s_e = product of connected node significances normalized. Reported on a 0–1 scale.",
            "dataset_or_benchmark": "SciReplicate-Bench",
            "human_evaluation_details": "Validated on a subset: 3 PhD students independently assessed reasoning graph accuracy for 20 tasks generated by two models (GPT-4o-mini and O3-mini-medium). Mean agreement reported and Pearson correlation with automated metric computed (r = 0.7518, p &lt; 0.005).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Average RG accuracy ≈ 0.716 without agent assistance; Paper Agent yields +0.009, Code Agent +0.036, combined +0.037 (averages reported). Node matching via GPT-4o repeated three times (temp=0, top-p=1) and averaged to reduce variability.",
            "comparison_to_human_generated": true,
            "comparison_results": "Automated RG-Acc strongly correlates with human judgments (Pearson r = 0.7518, p &lt; 0.005) on the evaluated subset.",
            "limitations_noted": "Node matching depends on GPT-4o and can introduce randomness; authors mitigated via deterministic sampling and averaging but note residual variability. Metric relies on insertion of non-overlapping, non-nested comment snippets by the generator, which may be brittle in practice. Significance weighting requires accurate counting of code features and assumes those capture semantic importance.",
            "uuid": "e7777.0",
            "source_info": {
                "paper_title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ExecAcc",
            "name_full": "Execution Accuracy",
            "brief_description": "An execution-based correctness metric: generated code is integrated into the repository and run on verification test cases; code is counted correct only if outputs match the reference for all test cases.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini variants)",
            "model_size": "various (model versions listed in paper)",
            "scientific_domain": "computer science / natural language processing",
            "theory_type": "executable implementation of algorithm (used to validate algorithmic claims)",
            "evaluation_method_name": "Execution Accuracy",
            "evaluation_method_description": "Insert the generated function/method into the target repository, run the repository-specific verification suite in an isolated Python environment, and mark the task as correct if all automated test cases match the reference outputs.",
            "evaluation_metric": "Execution accuracy (fraction of tasks where generated code passes the verification suite)",
            "metric_definition": "Proportion (0–1) of tasks for which generated implementation produces identical outputs to the reference across the provided test cases (typically 10 test cases per task when available).",
            "dataset_or_benchmark": "SciReplicate-Bench (100 tasks from 36 papers)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": true,
            "reported_results": "Average execution accuracy across evaluated LLMs = 0.235; best model (Claude-Sonnet-3.7 with Sci-Reproducer) reached 0.390. Ablation: Sci-Reproducer improved average execution accuracy by +0.181 relative to NoAgent.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Some tasks have fewer than 10 test cases; execution correctness can be sensitive to hardware/runtime differences (authors instruct local runs and fix randomness via seeds and deterministic replacements); passing binary tests may not capture logical differences in implementations that produce similar outputs for test inputs.",
            "uuid": "e7777.1",
            "source_info": {
                "paper_title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CodeBLEU",
            "name_full": "CodeBLEU",
            "brief_description": "A code similarity metric combining n-gram BLEU with syntactic (AST) and semantic (data-flow graph) information to better evaluate generated code versus reference implementations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (evaluated in paper)",
            "model_size": "various",
            "scientific_domain": "computer science / natural language processing",
            "theory_type": "code similarity evaluation",
            "evaluation_method_name": "CodeBLEU",
            "evaluation_method_description": "Compute a composite score that augments classical BLEU with AST-based syntactic equivalence and DFG-based semantic equivalence between generated and reference code.",
            "evaluation_metric": "CodeBLEU score",
            "metric_definition": "Composite score (reported on 0–1 scale in this paper); the specific CodeBLEU weighting follows prior CodeBLEU implementations (BLEU + AST + DFG components).",
            "dataset_or_benchmark": "SciReplicate-Bench",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Average CodeBLEU reported ≈ 0.320 across models; Sci-Reproducer yields average +0.057 improvement in CodeBLEU compared to agent-less generation.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Code similarity can be a poor proxy for functional correctness (high CodeBLEU may not guarantee correct behavior, and low CodeBLEU may still correspond to functionally correct alternative implementations).",
            "uuid": "e7777.2",
            "source_info": {
                "paper_title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DepRecall",
            "name_full": "Dependency/API Recall",
            "brief_description": "Recall metrics measuring how many required intra-file dependencies, cross-file dependencies, and external APIs from the reference implementation are identified and used in the generated code.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (evaluated with/without Sci-Reproducer)",
            "model_size": "various",
            "scientific_domain": "computer science / software engineering for ML",
            "theory_type": "dependency identification evaluation",
            "evaluation_method_name": "Recall of Intra-file / Cross-file Dependencies and External APIs",
            "evaluation_method_description": "Annotators provide ground-truth lists of internal variable/function/class dependencies and external API usages for each task; compute recall as fraction of these reference dependency items correctly identified/used by the generated code.",
            "evaluation_metric": "Recall per category (intra-file, cross-file, API)",
            "metric_definition": "Recall = (# of reference dependency/API items present in generated code) / (total # of reference dependency/API items); reported on 0–1 scale.",
            "dataset_or_benchmark": "SciReplicate-Bench",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "With Sci-Reproducer, average recall gains relative to no-agent baseline were +0.441 (intra-file), +0.239 (cross-file), and +0.100 (API). Example: Claude-Sonnet-3.7 achieved intra-file 0.776, cross-file 0.636, API 0.626 when using Sci-Reproducer.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Accurate dependency recall requires comprehensive, correct annotations; some dependencies are implicit or context-dependent and may be hard to detect automatically. Recall does not capture correctness of usage.",
            "uuid": "e7777.3",
            "source_info": {
                "paper_title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SciReplicate-Bench",
            "name_full": "SciReplicate-Bench",
            "brief_description": "A benchmark introduced in this paper consisting of 100 algorithm reproduction tasks derived from 36 NLP papers (2024), with reference implementations, reasoning graph annotations, dependency annotations, and verification suites.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (benchmark used to evaluate many LLMs)",
            "model_size": null,
            "scientific_domain": "computer science / natural language processing",
            "theory_type": "benchmark for algorithm reproduction and code-based validation of algorithmic claims",
            "evaluation_method_name": "SciReplicate-Bench",
            "evaluation_method_description": "Provides structured inputs (function signatures, LaTeX algorithm description, literature context, repository context), ground-truth implementations, annotated reasoning graphs, dependency annotations, and an isolated test environment with verification scripts (typically 10 test cases per task) to evaluate LLMs' ability to reproduce algorithms.",
            "evaluation_metric": "Supports Execution Accuracy, Reasoning Graph Accuracy, CodeBLEU, Dependency/API Recall",
            "metric_definition": "N/A (platform providing tasks and ground truth used by the listed metrics)",
            "dataset_or_benchmark": "SciReplicate-Bench (this work)",
            "human_evaluation_details": "Annotation process: human annotators refactored repositories, created reasoning-comment mappings, documented dependencies, and produced verification suites; annotating each paper ≈12 hours on average. Human evaluation: 3 PhD students assessed RG-Acc on 20 tasks for validation.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": true,
            "reported_results": "Benchmark contains 100 tasks from 36 papers; baseline aggregate results reported (ExecAcc avg 0.235, CodeBLEU 0.320, RG-Acc avg 0.716 without agents).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Some algorithm LaTeX descriptions miss implementation details; a few tasks have fewer than 10 test cases; reliance on refactored repositories and annotation quality; attempts to reduce data leakage by selecting 2024 papers and refactoring to ensure deterministic behavior.",
            "uuid": "e7777.4",
            "source_info": {
                "paper_title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Sci-Reproducer",
            "name_full": "Sci-Reproducer (dual-agent framework)",
            "brief_description": "A dual-agent system introduced in this paper: a Paper Agent that retrieves and summarizes literature-context information (RAG-like, uses ReAct strategy) and a Code Agent that searches repository code, web resources, and uses a compiler for iterative testing/debugging to produce executable implementations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Agent wrapper over host LLMs (paper evaluates Sci-Reproducer operating with multiple LLMs listed)",
            "model_size": null,
            "scientific_domain": "computer science / NLP / reproducibility",
            "theory_type": "agentic framework for algorithm reproduction (action-based evaluation pipeline)",
            "evaluation_method_name": "Sci-Reproducer (framework-level ablations and action-invocation analysis)",
            "evaluation_method_description": "Evaluate contributions of Paper Agent and Code Agent via ablation experiments (NoAgent, NoPaperAgent, NoCodeAgent) and track tool/action usage statistics; measure downstream effects on Execution Accuracy, CodeBLEU, RG-Acc, and dependency recall.",
            "evaluation_metric": "Improvement deltas on Execution Accuracy, CodeBLEU, RG-Acc, and recall metrics; tool invocation counts",
            "metric_definition": "Reported as absolute or relative improvements, e.g., average ExecAcc increase +0.181 with Sci-Reproducer vs NoAgent; per-model tool usage counts.",
            "dataset_or_benchmark": "SciReplicate-Bench",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Sci-Reproducer reduces syntax errors (from ≈80% to ≈24–29%), increases ExecAcc on average by +0.181; Code Agent contributes more to reducing syntax errors and recall gains than Paper Agent; reasoning LLMs use tools less and exhibit smaller gains due to 'overthinking'.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Effectiveness depends on LLM's propensity to use tools (reasoning LLMs underutilize tools); framework cannot dynamically access runtime information during generation except via explicit Compiler action; agent performance sensitive to prompt design and action definitions.",
            "uuid": "e7777.5",
            "source_info": {
                "paper_title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Human RG Eval",
            "name_full": "Human evaluation of Reasoning Graph Accuracy",
            "brief_description": "A human validation experiment where expert annotators rated the correctness of generated reasoning graphs to validate the automated RG-Acc metric.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science / NLP",
            "theory_type": "human expert rating validation",
            "evaluation_method_name": "Human rating of reasoning graphs",
            "evaluation_method_description": "Three PhD students independently assessed RG-Acc for 20 tasks produced by two LLMs under Sci-Reproducer; results compared to automated GPT-4o-based RG-Acc scoring to measure alignment.",
            "evaluation_metric": "Human mean scores per task and Pearson correlation with automated RG-Acc",
            "metric_definition": "Human ratings (scale not explicitly quantified in text) aggregated and correlated with automated scores; Pearson r reported.",
            "dataset_or_benchmark": "Subset of SciReplicate-Bench (20 tasks)",
            "human_evaluation_details": "Three PhD student annotators, 20 tasks, independent assessments; reported mean and standard deviation and Pearson correlation r = 0.7518 (p &lt; 0.005) with automated evaluation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Strong positive alignment between human judgments and automated RG-Acc (r = 0.7518, p &lt; 0.005).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Small sample (20 tasks) and small number of human raters; human rating scale and calibration details not exhaustively described; further large-scale human validation recommended.",
            "uuid": "e7777.6",
            "source_info": {
                "paper_title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Verification Suite",
            "name_full": "Verification Suite and Test Environment",
            "brief_description": "Per-task verification suites provided in the benchmark: isolated Python environments and test scripts (typically 10 cases per task) used to automatically verify functional correctness of generated implementations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science / software engineering",
            "theory_type": "automated test harness for functional validation",
            "evaluation_method_name": "Verification Suite Execution",
            "evaluation_method_description": "Annotators created deterministic Python execution environments, fixed randomness, and implemented task-specific comparison scripts; generated code is executed in these environments to compare outputs against references.",
            "evaluation_metric": "Used to compute Execution Accuracy (binary pass/fail per task and per test case comparisons)",
            "metric_definition": "Task-level correctness determined by matching outputs for provided test cases; typical suite size = 10 test cases where possible (some tasks fewer).",
            "dataset_or_benchmark": "SciReplicate-Bench",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": true,
            "reported_results": "Verification suites used across benchmark; authors ensured deterministic behavior by fixing seeds and removing nondeterministic constructs; some tasks had fewer than 10 test cases.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Test-case coverage may be limited; some tasks inherently stochastic or hardware-sensitive; authors recommend local runs and fixed seeds to mitigate variability.",
            "uuid": "e7777.7",
            "source_info": {
                "paper_title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PaperBench",
            "rating": 2,
            "sanitized_title": "paperbench"
        },
        {
            "paper_title": "Paper2CodeBench",
            "rating": 2,
            "sanitized_title": "paper2codebench"
        },
        {
            "paper_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark",
            "rating": 2,
            "sanitized_title": "corebench_fostering_the_credibility_of_published_research_through_a_computational_reproducibility_agent_benchmark"
        },
        {
            "paper_title": "MLGym",
            "rating": 1
        }
    ],
    "cost": 0.01963775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers
7 Aug 2025</p>
<p>Yanzheng Xiang yanzheng.xiang@kcl.ac.uk 
King's College London</p>
<p>Hanqi Yan hanqi.yan@kcl.ac.uk 
King's College London</p>
<p>Shuyin Ouyang shuyin.ouyang@kcl.ac.uk 
King's College London</p>
<p>Lin Gui lin.1.gui@kcl.ac.uk 
King's College London</p>
<p>Yulan He yulan.he@kcl.ac.uk 
King's College London</p>
<p>The Alan Turing Institute</p>
<p>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers
7 Aug 202531A2EB758D35837D1F67695B1219E75CarXiv:2504.00255v2[cs.CL]
This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers.The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs.To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases.Building on SciReplicate-Bench, we propose Sci-Reproducer, a dualagent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions.To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure.For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics.In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models.The best-performing LLM using Sci-Reproducer achieves only 39% execution accuracy, highlighting the benchmark's difficulty.Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction.We make available our benchmark and code at GitHub and project homepage at Homepage.</p>
<p>Introduction</p>
<p>The evolution of Large Language Models (LLMs) has ushered in a transformative era in scientific discovery, positioning them as powerful tools for streamlining research (Gridach et al., 2025;Buehler, 2024;Lu et al., 2024), from idea generation to verification and publication writing.For instance, Si et al. (2025); Gu &amp; Krenn (2025) demonstrated how LLMs can be prompted to generate novel research ideas, while Yuan et al. (2022); Du et al. (2024) explored their use in producing literature reviews for idea evaluation.Additionally, LLMs are increasingly integrated into tools like Semantic Scholar1 , Research Rabbit2 , and Undermind Research Assistant3 , enhancing literature discovery, citation analysis, and knowledge synthesis.These advancements, both in research methodologies and practical applications, suggest that LLMs have the potential to assist across multiple stages of scientific discovery.</p>
<p>Among the aforementioned advancements in research acceleration, the ability of LLMs to correctly generate code for validating real-world scientific ideas is particularly noteworthy.Computational validation is crucial across many fields, yet researchers often face barriers due to limited coding expertise or inaccessible implementations.By converting scientific Table 1: Comparisons of different machine learning software engineering benchmarks.</p>
<p>To address this gap, we developed SciReplicate-Bench, the first benchmark specifically designed to evaluate LLMs' capabilities in code generation for reproducing research findings from academic papers.It consists of 100 code reproduction tasks derived from 36 papers published in leading conferences in 2024.This recent publication window was deliberately chosen to minimize the risk of data leakage.An overview of the task is illustrated in Figure 1, with a concrete example provided in Figure A2 in Appendix E. The task consists of two main steps: 1. Algorithm understanding.LLMs must extract essential information from the paper, such as workflow details, algorithm descriptions, and hyperparameter values.2. Code implementation.LLMs then implement a function or method within a provided repository, using both the extracted information and the LaTeX representation of the algorithm from the paper.We introduce Sci-Reproducer, a dual-agent system that combines a Paper Agent and a Code Agent to handle these two steps collaboratively and implement code for the target algorithm.</p>
<p>To rigorously assess LLM performance on this benchmark, we evaluate two dimensions corresponding to the aforementioned two steps: algorithm comprehension correctness and code correctness.To evaluate algorithm comprehension, we introduce a reasoning graph to represent the reasoning logic behind algorithm reproduction.Each node in the graph represents a code comment, which reflects a single reasoning step and is aligned with a specific segment of code.Edges between nodes are defined based on data flow relationships across different code segments.We compute the similarity between the generated reasoning graph and a reference graph to derive the reasoning graph accuracy.To evaluate code correctness, we employ established metrics including execution accuracy (Rajkumar et al., 2022;Xiang et al., 2023), CodeBLEU (Ren et al., 2020), and recall of intra/cross-file dependencies and APIs.</p>
<p>Our work makes the following contributions:</p>
<p>Benchmarks: SciReplicate-Bench, a benchmark of 100 algorithm reproduction tasks from recent NLP publications.</p>
<p>Metric:</p>
<p>We propose a novel reasoning graph accuracy metric for evaluating algorithmic comprehension.Approach: Sci-Reproducer, a dual-agent framework combining paper understanding and code implementation.Insights: Comprehensive evaluation across state-of-the-art LLMs reveals four key findings: (i) the task remains highly challenging, with execution accuracy below 40% for all models; (ii) reasoning models exhibit "overthinking" behavior (Cuadron et al., 2025;Sui et al., 2025), over-relying on internal reasoning rather than utilizing available tools for information extraction; (iii) while LLMs demonstrate strong algorithmic comprehension, they struggle with practical implementation; and (iv) algorithm reproduction failures often stem from incomplete paper descriptions, which our Sci-Reproducer effectively addresses.</p>
<p>Related Work</p>
<p>Our work lies at the intersection of AI for automating scientific discovery and LLM-based code generation.</p>
<p>AI for Automating Scientific Discovery</p>
<p>The application of LLMs to accelerate scientific research has emerged as a rapidly growing field with diverse approaches.Several studies have demonstrated the potential for comprehensive research automation through end-to-end AI systems.Schmidgall et al. (2025); Lu et al. (2024) developed frameworks that integrate idea generation, experimental validation, and manuscript composition, with some AI-authored papers successfully passing workshop review processes (Yamada et al., 2025).Complementary research has focused on the creative aspects of scientific inquiry, with Wang et al. (2023); Ghafarollahi &amp; Buehler (2024); O'Neill et al. (2025) investigating LLMs' capacity for generating novel research hypotheses.Notably, recent evaluations by Gu et al. (2024); Kumar et al. (2024); Liu et al. (2025); Si et al. (2024) suggest that AI-generated research concepts may occasionally exceed human-generated ideas in terms of novelty and originality.Within computational disciplines where implementation validation is essential, LLMs have shown promise in algorithm design and code development tasks.Our proposed SciReplicate-Bench addresses a previously underexplored area: the automated reproduction of algorithms directly from academic publications.This represents a unique challenge at the intersection of scientific literature comprehension and executable code synthesis.While recent parallel efforts such as PaperBench (Starace et al., 2025) and Paper2CodeBench (Seo et al., 2025) have tackled related problems by exploring full codebase reconstruction, these evaluation approaches rely substantially on manual assessment criteria and LLM-based correctness judgments, introducing potential inconsistencies and reliability concerns.Our approach prioritizes objective evaluation through execution accuracy, providing more rigorous validation than non-executable assessment methodologies (Wang et al., 2022;Chen et al., 2021).</p>
<p>The broader ecosystem of computational reproducibility research includes specialized frameworks such as MLGym (Nathani et al., 2025) for baseline improvement, and evaluation benchmarks developed by Siegel et al. (2024);Ren et al. (2023) that assess LLMs' ability to reproduce published experimental results using existing codebases.</p>
<p>LLMs for Code Generation</p>
<p>Code generation has emerged as a prominent application of LLMs, with benchmarks ranging from basic programming tasks (Chen et al., 2021;Jain et al., 2024;Austin et al., 2021;Hendrycks et al., 2021;Liu et al., 2022) to realistic software engineering challenges like SWE-bench (Jimenez et al., 2023), which uses actual repository pull requests.However, these benchmarks primarily target general software engineering rather than scientific algorithm reproduction.</p>
<p>Recent efforts have developed machine learning-specific benchmarks (Liu et al., 2023;Huang et al., 2023;Chan et al., 2024), but these typically involve implementing algorithms proposed by the models themselves or solving relatively straightforward tasks.They lack the depth of algorithmic understanding and rigorous paper analysis required for reproducing algorithms from peer-reviewed publications.</p>
<p>Despite advances in tool-augmented code generation (Schick et al., 2023;Zhang et al., 2024a;a;2023b), no existing system specifically addresses the unique challenge of translating academic papers into executable code.Our Sci-Reproducer framework demonstrates the ability to comprehend academic publications and convert abstract algorithm descriptions into functional implementations.</p>
<p>SciReplicate-Bench</p>
<p>Overview SciReplicate-Bench is designed to evaluate LLMs' ability to reproduce algorithms from academic papers, consisting of 100 tasks curated from 36 recent NLP publications with their corresponding open-source implementations.The task categories are detailed in Figure A1 in Appendix B. The benchmark focuses on repository-level code generation, where each task is centered around implementing a specific function or class method.As illustrated in Figure A3 in Appendix E, each task comprises nine components, which can be categorized into three groups corresponding to code generation, evaluation, and analysis, respectively.</p>
<p>For code generation, the following components are provided as inputs to LLMs:</p>
<p>Function signature: the definition of the target function, including detailed descriptions of its input and output variables.</p>
<p>Algorithm Description: The LaTeX code description of the target algorithm, typically located within a subsection or paragraph of the target paper.</p>
<p>Literature context: the original paper along with its cited references, providing broader conceptual context.</p>
<p>Repository context: all source files and code in the repository that inform or support the target implementation.</p>
<p>For evaluation, the following components are provided for code execution and metrics calculation:</p>
<p>Reference implementation: ground-truth code serving as the reference for CodeBLEU evaluation.</p>
<p>Reasoning graph annotations: structured representations of the algorithmic logic and implementation flow, enabling assessment of reasoning graph accuracy.Dependency annotations: comprehensive documentation of internal dependencies, crossfile relationships, and external API usage for computing recall metrics across all dependency categories.</p>
<p>Test environment: isolated Python execution environment containing validation cases and automated verification scripts for assessing implementation correctness.</p>
<p>To enable further analysis of the underlying causes of LLM failures, the benchmark includes: Missing/Mismatch Information: the LaTeX description of the algorithm may omit certain implementation details, which could either appear elsewhere in the paper or be entirely absent.We also annotate mismatches between the paper description and the reference implementation.</p>
<p>Task Definition Based on SciReplicate-Bench, an LLM is given the algorithm description, function signature, literature context, and repository context as input.The LLM is asked to output a function that implements the target algorithm.</p>
<p>Benchmark Construction</p>
<p>The benchmark construction process comprises four key steps: paper selection, Python environment setup, documentation, and verification suite preparation.To mitigate the risk of data leakage, we selected papers published in 2024 that provide publicly available code repositories.During the annotation process, each repository was refactored to isolate the core algorithm into a standalone function, and all sources of randomness were removed to ensure reproducibility and prevent leakage.On average, annotating each paper requires approximately 12 hours.Details of the annotation process are provided in Appendix A.</p>
<p>Evaluation Metrics</p>
<p>Evaluating Algorithm Comprehension</p>
<p>We propose the reasoning graph accuracy metric to evaluate how well LLMs understand the logic and implementation of algorithms.During code generation, LLMs are prompted to insert specially formatted, non-overlapping, non-nested comments that mark reasoning steps derived from the algorithm's LaTeX code (The prompt can be found in Figure A5).</p>
<p>We then construct a reasoning graph G = {N, E} (illustrated in Figure A3), modeled as a Directed Acyclic Graph (DAG).Each node n i = ⟨w i , c i ⟩, n i ∈ N represents a reasoning step with a comment w i and corresponding code snippet c i .An edge e i = ⟨n i , n j ⟩, e i ∈ E is added if a variable used in c j is defined or last modified in c i .To compute the reasoning graph accuracy, we compare the generated graph G g with the reference graph G r via node and edge matching:</p>
<p>Node matching: comments from G r and G g are passed to GPT-4o, which maps each reference node to one or more nodes in the generated graph.A node in G r is considered matched if it has at least one corresponding node in G g .The prompt template used for this process is available in Figure A4.</p>
<p>Edge matching: for each reference edge e r = ⟨n i r , n j r ⟩, if both endpoint nodes have corresponding matches in G g , we apply Breadth-First Search(BFS) to verify whether a corresponding edge exists in G g .</p>
<p>The reasoning graph accuracy S r is computed as:
S r = n i ∈N m ∑ n i s n i + e j ∈E m ∑ e j s e j . (1)
where N m and E m denote the sets of matched nodes and edges, respectively, and s n i and s e j represent their corresponding significance scores.Node significance is determined by the complexity of its corresponding code segment, measured by the number of variable definitions and usages, function calls, arithmetic operations, and lines of code, then normalized across the reference graph.Edge significance is calculated as the product of the significance scores of its connected nodes, followed by normalization.</p>
<p>Evaluating Code Generation</p>
<p>For assessing coding ability, we use the following evaluation metrics:</p>
<p>• Execution accuracy (Xiang et al., 2023;Zhang et al., 2024b;Long et al., 2022): we integrate the generated code into the repository and execute it to obtain results.If all test cases match the reference results, we consider the code correct.</p>
<p>• CodeBLEU (Ren et al., 2020): this metric evaluates how similar the generated code is to reference code by using the traditional BLEU metric (Papineni et al., 2002) while incorporating syntactic information through abstract syntax trees (AST) and semantic understanding via data-flow graphs (DFG).</p>
<p>• Recall (Li et al., 2024): we calculate recall scores specifically for intra-file dependencies, cross-file dependencies, and external APIs.</p>
<p>SearchSection Section ID</p>
<p>The entire content of a section based on the section label.</p>
<p>SearchLiterature Paper ID, query The answer to the query searched from the literature (identified by Paper ID).</p>
<p>Code Agent SearchCode Name</p>
<p>The definition of a specific code element in repository.</p>
<p>SearchFile Name</p>
<p>The content of a certain file in repository.</p>
<p>SearchWeb Query</p>
<p>The information obtained from the website.</p>
<p>Compiler code</p>
<p>The feedback from the compiler after executing the code.</p>
<p>Sci-Reproducer</p>
<p>To address this task, we introduce Sci-Reproducer4 , a dual-agent framework designed for scientific paper algorithm replication.As illustrated in Figure 1, Sci-Reproducer comprises a Paper Agent and a Code Agent that collaboratively work to replicate algorithms described in a given paper.The predefined actions employed by the agents are summarized in Table 2, with implementation details provided in Appendix C.</p>
<p>Paper Agent</p>
<p>Based on the provided algorithm description, the Paper Agent systematically retrieves contextual information from the literature context to support algorithmic understanding.Due to the input length limitations of LLMs, it is infeasible to input entire paper along with their associated literature.Consequently, the Paper Agent must selectively extract pertinent information, following a strategy akin to Retrieval Augmented Generation (RAG) (Wang et al., 2024b;Sarthi et al., 2024).The Paper Agent incrementally builds an understanding of the target algorithm by executing predefined actions to query the literature context.To facilitate this process, we adopt ReAct (Yao et al., 2022) as the agent strategy, which enables seamless integration of action execution with intermediate reasoning steps.</p>
<p>After the Paper Agent concludes that all necessary information has been collected, it generates a comprehensive literature report comprising key findings that fill in the missing  components of the target algorithm's LaTeX source.An example of the literature report is shown in Figure A8.This report subsequently serves as a crucial input for the Code Agent.</p>
<p>The prompt used to guide the Paper Agent is provided in Figure A6.</p>
<p>Code Agent</p>
<p>Informed by the algorithm description, literature report, and code context, the Code Agent searches the repository to locate essential dependencies required for implementation.It can also browse websites for additional information and use a compiler to test and iteratively debug the code, ensuring proper execution by identifying and fixing syntax errors.The prompt for the Code Agent is provided in Figure A7.</p>
<p>Experiments</p>
<p>We evaluate Sci-Reproducer on the SciReplicate-Bench benchmark using 7 advanced LLMs, including five non-reasoning LLMs: GPT-4o-mini (4o mini, 2024), GPT-4o (GPT-4o, 2024), Claude-Sonnet-3.7 (Claude-Sonnet-3.7,2025), Gemini-2.0-Flash(Gemini-2.0-Flash,2024), and Deepseek-V3 (DeepSeek-AI et al., 2024), and different versions of the reasoning models O3-mini (o3 mini, 2024), i.e., three different levels of reasoning intensity.For the reasoning graph accuracy metric, node matching is performed using GPT-4o, which may introduce some randomness.To reduce this variability, we set the temperature to 0 and top-p to 1, ensuring more deterministic generation.The calculation is repeated three times, and we report the average score as the final result.</p>
<p>Results on SciReplicate-Bench</p>
<p>LLMs face challenges with actual implementation</p>
<p>Although LLMs are capable of understanding algorithms, their performance in code generation remains suboptimal.Despite using Sci-Reproducer, the average execution accuracy remains low at 0.235, with a CodeBLEU score of 0.320.</p>
<p>Accurate dependency and API identification is crucial for code implementation</p>
<p>Effectively recognizing and leveraging dependencies from the source repository and external APIs is essential for accurate code implementation.The integration of Code Agent led to substantial gains in recall with average increases of 0.441, 0.239, and 0.100, respectively, compared to cases without the agent.With Sci-Reproducer, Claude-Sonnet-3.7 attains the highest execution accuracy of 0.390, with the highest recall for intra/cross file dependency and API usage, at 0.776, 0.636, and 0.626 respectively.</p>
<p>Overthinking leads to limited improvement in reasoning LLMs</p>
<p>Reasoning LLMs exhibit more modest performance gains when using Sci-Reproducer.While they achieve an average execution accuracy improvement of 0.13, non-reasoning models demonstrate substantially larger gains of 0.212.This pattern extends to recall metrics, where reasoning LLMs show improvements of 0.243, 0.061, and 0.041 respectively, compared to non-reasoning LLMs' more pronounced gains of 0.560, 0.345, and 0.135.We attribute this performance gap to the "overthinking" phenomenon (Cuadron et al., 2025;Sui et al., 2025), where excessive internal reasoning impedes effective action execution, a limitation that we examine in detail in the following subsection.</p>
<p>Tool Usage Analysis</p>
<p>Figure 2 presents the number of times each LLM invokes actions on the full dataset with Sci-Reproducer.We observe the following: • For code-related actions, reasoning LLMs demonstrate limited tool usage, employing "SearchFile", "SearchCodeItem", and "SearchWeb" only 25.0, 3.3, and 0.0 times on average, respectively.Non-reasoning LLMs use these same actions far more extensively, with averages of 210.4,68.2, and 16.8 times respectively.This disparity reveals a fundamental behavioral difference: reasoning models favor internal deliberation over external information gathering.Conversely, reasoning LLMs invoke "Compiler" more frequently, indicating they require more debugging iterations due to inadequate contextual information gathering.This over-reliance on internal reasoning undermines performance: advanced models like o3-mini-high and o3-mini-low achieve execution accuracy comparable to GPT-4o-mini, negating their theoretical computational advantages.• Paper-related actions exhibit a similar pattern.Reasoning LLMs use "SearchPaper", "SearchSection", and "SearchLiterature" an average of 56.3, 70.0, and 20.0 times respectively, while non-reasoning LLMs demonstrate substantially higher usage at 244.8, 188.4,and 58.0 times respectively.Additionally, we observe a clear preference for target paper extraction over external literature consultation.Actions targeting the primary paper ("SearchPaper" and "SearchSection") are invoked 174.1 and 144 times on average, significantly more than "SearchLiterature" which accesses related works only 43.8 times.</p>
<p>Error Analysis</p>
<p>Syntax Errors</p>
<p>Table A4 shows the syntax error rates for each model across different configurations.Without the Code Agent, syntax errors occurred at rates of 80.3% ("NoAgent") and 83.3% ("Paper Agent").After implementing the Code Agent, these error rates dropped significantly to 29.4% ("Code Agent") and 24.9% ("Sci-Reproducer").The remaining syntax errors mainly result from incorrectly using repository dependencies.This occurs because our approach, unlike human developers, cannot dynamically access runtime information through a compiler during the code generation process.</p>
<p>Logic Errors</p>
<p>Another issue stems from differences in implementation logic, which can be broadly categorized into: (1) discrepancy in algorithm implementation that result in differing outputs, and</p>
<p>(2) missing or mismatch information in the algorithm descriptions in the paper compared to the actual code.</p>
<p>Implementation discrepancy An algorithm may have multiple valid implementation approaches.For example, the cross-entropy loss function can be implemented by directly  invoking the PyTorch API "torch.nn.CrossEntropy" or by manually coding it from scratch.Such implementation choices may introduce subtle differences that lead to variations in the final output of the function.</p>
<p>Missing/Mismatched information in algorithm description Algorithmic descriptions in research papers often lack concrete implementation details, and in certain cases, the provided code may exhibit minor discrepancies compared to the descriptions in the paper.</p>
<p>We manually compared the implementation code of all tasks in the dataset with their descriptions in the papers to identify missing or mismatch information.We then provided this information as additional input and apply Sci-Reproducer framework on three LLMs.The Results is shown in Table 4, regarding to Execution Acc, the performance for GPT-4o-mini, Deepseek-V3 and O3-mini-low improved 0.050, 0.250 and 0.040 respectively.The missing information can be divided into four categories:</p>
<p>• Hyperparameters and configurations: descriptions of target algorithms in papers often omit specific hyperparameter settings, such as the batch size.• Numerical stability techniques: standard techniques for ensuring numerical stability, such as handling division by zero.• Implementation logic: common implementation practices and model design choices, such as data splitting protocols.• Coding strategy: practical programming techniques that enhance implementation efficiency and reliability, such as early stopping criteria.</p>
<p>More examples for each category can be found in Table A5 in Appendix E. As for mismatched information, it occurs far less frequently compared to missing information, and its categories largely overlap with those mentioned above.</p>
<p>To mitigate the widespread issues of missing and mismatched information, the first category can generally be addressed by referencing the original research paper and related literature, or by inspecting the code repository for explicit configurations.However, addressing the other three categories requires familiarity with general machine learning coding conventions, thus necessitating that the LLMs identify and utilize implementation patterns from comparable algorithms to enhance code quality.Future research may improve performance by incorporating implementation insights from similar algorithms through techniques such as in-context learning (Zhou et al., 2023;Xiang et al., 2024), and by leveraging real-time compiler feedback to infer precise variable values.</p>
<p>Conclusion</p>
<p>We evaluate LLMs' ability to replicate algorithms described in recent NLP papers.To support this, we introduce SciReplicate-Bench, a benchmark with rich annotations, and Sci-Reproducer, a dual-agent framework for bridging algorithm understanding and code generation.We assess performance using reasoning graph accuracy and standard implementation metrics.Results show the task is highly challenging, with failures largely caused by missing or inconsistent algorithm descriptions.</p>
<ol>
<li>Detailed annotation: for each aligned function, annotators documented input/output variables, intra-and cross-file dependencies, and external API usage.Additionally, they inserted explanatory comments mapping code segments to algorithm components.Based on these annotations and variable dependencies, we can construct a reasoning graph representing the implementation logic.During the annotation process, LLMs were employed to assist with algorithm-function alignment and the generation of variable descriptions and code comments.All outputs were subsequently reviewed and corrected by human annotators to ensure accuracy.</li>
</ol>
<p>The final selected papers are listed in Table A1.</p>
<p>Step 4: verification suite preparation Finally, annotators created verification suites with 10 test cases per task, drawn from the original datasets used in each repository for the majority of papers.For a small number of repositories, fewer than 10 test cases could be constructed.For instance, algorithms that analyze LLM parameters may have only a single test case.Given the inherent randomness in many NLP implementations and potential machine-related variability, we addressed reproducibility from two angles:</p>
<p>• Eliminating code randomness: annotators fixed random seeds and replaced nondeterministic operations (e.g., unordered sets) with deterministic equivalents to ensure consistent outputs across runs.• Controlling hardware variability: users were instructed to run both reference and generated code locally to eliminate discrepancies caused by hardware differences.</p>
<p>Lastly, annotators implemented task-specific comparison scripts to evaluate output correctness, accounting for variations in return types across tasks.</p>
<p>B Details of the Task Categories</p>
<p>C Details of the Actions</p>
<p>In this section, we provide implement details for all actions defined in the Sci-Reproducer.</p>
<p>SearchPaper We obtain the LaTeX source code of the target academic paper from arXiv 7and apply regular expression-based parsing to extract the content corresponding to each section.Subsequently, we iteratively feed the content of each subsection, along with the query generated by the large language model, into GPT-4o-mini.The model extracts relevant information and returns it as an observation to the paper agent.</p>
<p>SearchSection Following the same approach as SearchPaper, the tool begins by parsing the LaTeX source code of the target algorithm.Upon receiving a section ID from the Paper Agent, it retrieves and returns the content of the corresponding section.</p>
<p>SearchLiterature Given a paper ID and a query, the tool attempts to download the corresponding LaTeX source code from arXiv.If the LaTeX source code is unavailable, it returns no information.Otherwise, it extracts content relevant to the query from the paper, following the same procedure as the SearchPaper action.</p>
<p>SearchCode For each Python file in the code repository, we utilize the Python AST8 package to parse the file and extract all defined classes, functions, and global variables.Unlike embedding-based code search methods (Zhang et al., 2024c;2023c), the Code Agent in our framework directly provides the name of a code item.The tool then returns the corresponding definition if it exists; otherwise, it returns an empty response.</p>
<p>SearchFile When the Code Agent provides a file name, the tool returns the full content of the corresponding file.</p>
<p>SearchWeb When the Code Agent issues a query, we use the Google Search API 9 to retrieve relevant information from websites.These results are then processed by GPT-4omini, which filters the content and extracts the information most relevant to the query for return.</p>
<p>Compiler Once the Code Agent completes code generation, it invokes the compiler to execute the code.The generated function or method is inserted into the original Python file, and the corresponding Python environment is used to run the code.The output from the compiler is then returned as the feedback.</p>
<p>D Human Evaluation of Reasoning Graph Accuracy</p>
<p>To validate the reliability of our LLM-based evaluation metrics, we conducted a human evaluation study on a subset of our benchmark.Three PhD students in computer science independently assessed the reasoning graph accuracy for 20 tasks generated by GPT-4o-mini and O3-mini-medium using Sci-Reproducer.</p>
<p>As shown in Table A2, human evaluations demonstrate strong alignment with our automated LLM-based assessments.The mean scores show consistent agreement between human annotators and GPT-4o evaluations, with standard deviations indicating reasonable inter-annotator consistency.</p>
<p>Furthermore, we computed the Pearson correlation coefficient between human and LLMbased evaluations across all assessed instances.As presented in Table A3, the correlation (r = 0.7518, p ¡ 0.005) indicates a strong positive relationship between human judgments and automated assessments, supporting the validity of our LLM-based evaluation approach.The code snippets corresponding to each comment must not overlap, and nesting is not allowed.5. Single Function: Implement the code within a single function (or method), without breaking it into multiple functions.6. Import Package: Import all packages within the function (or method) to ensure the code is self -contained.7. Format, the comment for each snippet should be in the following format: # ---------------------------------------------------------------------------# Snippet x: Comment here # ---------------------------------------------------------------------------# # -------------------------------------------------------------------# Snippet 3: The transformation is a simple element-wise operation combined with # the scaling_factor, illustrating a simplified version of the # alignment concept from the LaTeX, which might involve more complex # contrastive or attentional calculations.# -------------------------------------------------------------------# [Begin Snippet 3] updated_segment = torch.add(segment_main,torch.mul(segment_aux,scaling_factor)) transformed_tokens[i, start_idx:end_idx, :] = updated_segment # [End Snippet 3] # ---------------------------------------------------------------------------# Snippet 4: The final transformed_tokens are now partially aligned with the auxiliary # reference, reflecting the notion of augmenting token-level outputs # (Equation references in the LaTeX snippet would correspond to eq. (2 -3) or # similar definitions of reference alignment).# ---------------------------------------------------------------------------# [Begin Snippet 4] return transformed_tokens # [End Snippet 4] ``Ỳ our answer:</p>
<p>Figure 1 :
1
Figure 1: Overview of the task and the proposed Sci-Reproducer framework.The task involves algorithm understanding and code implementation, handled by a Paper Agent and a Code Agent operating in separate contexts with specialized actions.</p>
<p>Figure 2 :
2
Figure 2: A grouped bar chart illustrating the frequency of tool usage by different models.The x-axis represents various actions, while the y-axis indicates the total number of times each tool was used on this dataset.</p>
<p>Model</p>
<p>(Sci-Reproducer) Exe Acc(↑) CodeBLEU(↑) RG Acc(↑) Recall Intra-File(↑) Cross-File(↑) API(↑)</p>
<p>Figure A1 :
A1
Figure A1: The categories of the tasks within SciReplicate-Bench.The benchmark encompasses five main task categories in the NLP domain: representation and embedding methods, loss functions and optimization objectives, information extraction and aggregation, model architecture components, and inference and search algorithms.The distribution of each task category is illustrated in Figure A1.</p>
<p>start_idx, end_idx) in enumerate(transformation_indices): # -----------------------------------------------------------------------# Snippet 1: We first verify if the current batch item is valid by checking the # batch_mask, analogous to referencing an in-context example (\mathbf{S}) # that has a corresponding reference (\mathbf{H_r}) in the LaTeX snippet.# -----------------------------------------------------------------------#[Begin Snippet 1] if batch_mask[i]: # [End Snippet 1]# -------------------------------------------------------------------# Snippet 2: Here, we apply a basic shift to the token_representation by # incorporating a slice from the auxiliary_representation, akin to # combining (\mathbf{H}) with a portion of (\mathbf{H_r}) for # enhanced alignment.# -------------------------------------------------------------------# [Begin Snippet 2] segment_main = transformed_tokens[i, start_idx:end_idx, :] segment_aux = auxiliary_representation[i, start_idx:end_idx, :] # [End Snippet 2]</p>
<p>Figure A5 :
A5
Figure A5: The prompt for code generation.</p>
<p>Table 2 :
2
The pre-defined actions for the Paper Agent and the Code Agent.</p>
<p>Table 3 :
3
Performance evaluation on the SciReplicate-Bench benchmark.Models with notation indicate reasoning LLMs."Exe Acc" represents execution accuracy while "RG Acc" indicates reasoning graph accuracy.</p>
<p>Table 3 displays
3
Sci-Reproducer's evaluation results and contributions of Code/Paper Agent.The "No Agent"directly prompts the LLM to generate code based solely on the algorithm description and function signature."NoPaper Agent" allows the LLM to use Code Agent actions, but restricts access to Paper Agent actions."NoCode Agent" grants access to Paper Agent actions but blocks Code Agent capabilities.The results offer key insights, discussed in the following.executionaccuracy without using the agent to examine literature and repository contexts.With enhancement of Sci-Reproducer, these LLMs show notable improvements, with an average increase of 0.181 in execution ACC and 0.057 in CodeBLEU, although even the best-performing model, Claude-Sonnet-3.7,only achieved 0.390 execution accuracy.This highlights the exceptional challenge presented by our SciReplicate-Bench.
LLMs struggles on SciReplicate-Bench Most LLMs perform poorly, achieving less than0.1
LLMs can comprehend algorithm logicLLMs demonstrate strong algorithmic comprehension capabilities, as evidenced by reasoning graph accuracy scores averaging 0.716 even without agent assistance.The addition of individual agents provides modest but consistent improvements: the Paper Agent increases understanding by 0.009 on average, while the Code Agent contributes a larger gain of 0.036.Combined agent deployment yields a cumulative improvement of 0.037.These enhancements stem from complementary mechanisms: the Paper Agent strengthens theoretical comprehension by gathering relevant contextual information from academic literature, while the Code Agent facilitates practical understanding through extraction of pertinent code patterns and dependency structures from repositories.</p>
<p>Table 4 :
4
Experimental Results when missing/mismatched information is regard as external input in the prompt.
GPT-4o-mini0.2200.3160.8090.5880.4850.409Deepseek-V30.4700.3780.8340.6820.4240.609o3-mini-low0.2200.2920.8500.2590.0910.460</p>
<p>Please complete the target function (or method) and provide the pure code output without any additional text.Include comments in the code following these specific guidelines:Code Comments: 1. Focus on Reasoning: Comments should explain the reasoning behind the code generation process, as derived from the LaTeX description.2. No Implementation Details: Avoid including any code-specific implementation details in the comments.3. Mapping to LaTeX: Each comment must indicate which functionality described in the LaTeX code is implemented in the subsequent Python code snippet.4. No overlap:
TitleConference1. From Zero to Hero: Cold-Start Anomaly Detection (Reiss et al., 2024)Findings of ACL 20242. Addressing Order Sensitivity of In-Context Demonstration Examples in Causal LanguageFindings of ACL 2024Models (Xiang et al., 2024)3. Breaking the Ceiling of the LLM Community by Treating Token Generation as a ClassificationFindings of EMNLP 2024for Ensembling (Yu et al., 2024)4. Simple but Effective Compound Geometric Operations for Temporal Knowledge GraphCompletion (Ying et al., 2024)
https://www.semanticscholar.org
https://www.researchrabbit.ai/
https://www.undermind.ai/
A video demonstration showcasing Sci-Reproducer's capabilities is provided at https://youtu. be/qcSIMgyehjE
https://paperswithcode.com/api/v1/docs/
https://docs.github.com/en/rest?apiVersion=2022-11-28
https://arxiv.org/
https://docs.python.org/3/library/ast.html
https://developers.google.com/custom-search/v1/
AcknowledgementsThis work was supported in part by the UK Engineering and Physical Sciences Research Council (EPSRC) through a Turing AI Fellowship (grant no.EP/V020579/1, EP/V020579/2).We thank the authors of the selected papers for making their code openly available.Code Repository Code Context Website Actions Search web Code Interpreter Search Code Literature ReportAppendixA Details of the Annotation ProcessStep 1: paper selection We curated NLP papers from leading conferences in 2024, including ACL, EMNLP, ICLR, Neurips, and COLING.Using a web crawler, we collected accepted paper titles and employed the PapersWithCode API 5 to identify those with open-source implementations.For each identified paper, we retrieved corresponding GitHub repository links and metadata (e.g., stars, issues, release dates) via the GitHub REST API 6 .To filter candidates, we applied the following criteria:• Removed survey/exploratory papers while retaining method-focused research.• Applied a cutoff date of January 1, 2024 to avoid data leakage.• Excluded repositories with fewer than 5 stars to ensure basic quality assurance.Subsequently, researchers manually reviewed each candidate paper and its repository.We discarded papers with excessive computational demands, poorly structured code, ambiguous documentation, missing preprocessing steps, or reported reproduction issues.Step2: python environment setup For papers passing the initial screening, annotators followed the README to set up the environment and replicate experiments.Common issues included dependency conflicts, data loading failures, and incomplete or buggy code.Annotators attempted to resolve these problems; repositories with irrecoverable errors were excluded.Step3: annotation Annotation consists of two steps:1. Algorithm-Function alignment: most papers contain multiple algorithmic components, often organized as subsections.Annotators segmented these into distinct units and mapped each to its corresponding implementation.Code was refactored to encapsulate each algorithm in a standalone function or method.Papers with implementations too fragmented for restructuring were excluded.Published as a conference paper at COLM 2025 While this preliminary validation demonstrates the effectiveness of our automated metrics, comprehensive human evaluation across the full benchmark remains an important direction for future work.E Figures and TablesTarget Algorithm 4.1 Sentence ExtractionWe propose to model sentence extraction as extractive summarization.For this purpose, we concatenate all the sentences in the document into an input sequence, which is then fed to BertSum(Liu and Lapata, 2019)to obtain the score for each sentence.The details ofthe process can be found in Appendix a.  Step2: Calculate positive similarity score: cos(query, key) measures how close the model token is to the reference token.Step3: Calculate negative similarity score: cos(query, query.clone().detach())measures how close the token is to itself (detached) which serves as the negative reference.Step4: Final contrastive loss for these tokens: -log(positive / (positive + negative)) This aligns with the InfoNCE or contrastive objective described in the paper's Eq. (Information Augmentation).Step5: Accumulate the loss and update count and average the total contrastive loss by the number of token-level comparisons.The loss is averaged across tokens.Mismatch details: None⑨ Verification Suite 1. Test Cases: 10 test cases chosen from benchmark adopted in the original code repository.Output Compare Script:A Python program designed to verify the accuracy of the generated code's output.Implementation logic Data splitting, application of dropout, formatting of input sequences, and handling special or edge cases in the input data.Code EnvironmentCoding strategyCaching for performance enhancement, retry mechanisms to handle failures, early stopping criteria, and strategies for memory optimization.TableA5: Some examples for different missing information categories.[Task Overview] Reproduce Python code corresponding to a LaTeX-based methodology from a scientific paper.However, due to the paper's length, it cannot be fully ingested by a large language model at once.Therefore, the solution requires two main steps: 1. Information Retrieval (Your Current Task): Extract relevant details, insights, and supporting information from the academic paper's LaTeX description and related literature.2. Code Reproduction (Subsequent Task): Implement the Python code based on the information gathered and the provided LaTeX.[Your Specific Focus] You are tasked exclusively with Step 1: Information Retrieval.You must gather and organize all necessary details that will later be used to implement the Python code.[Input] 1. List of sections: The paper includes the following sections (titles are provided for reference): -w'x' (string): The title of the referenced section.Example:-Latex: "The full derivation of our loss function can be found in method Section .",Action: SearchSection["method"] * SearchLiterature[key, query] Description: If the target section cites another paper (\cite{label}) and you determine that some information needs to be retrieved from that paper, return SearchLiterature[label, query], where query is the specific information you need to look for in the referenced paper.Parameters:-'key' (string): The citation key of the referenced paper.In LaTeX, when citing a paper, we use \cite{x}, where x represents the citation key.-'query' (string): The specific information to search for in the referenced paper.Example: I -Latex: "We adopt the metric proposed in \cite{wang2025}".Action: SearchLiterature["wang2025", "The proposed metric in the paper"] -Latex: "The algorithm is based on the work of \cite{smith2018}".Action: SearchLiterature["smith2018", "The algorithm details in the paper"] -Latex: "The dataset is based on the study by \cite{jones2020}".Action: SearchLiterature["jones2020", "The dataset details in the paper"] [Instruction] In order to complete code reproduction, it is first necessary to understand the algorithm described in the LaTeX description.The tools "SearchPaper", "SearchSection" and "SearchLiterature" should be used to retrieve relevant information from the paper to help you understand the methodology proposed in the latex description.For example: 1.If the LaTeX Description lacks the definition of a variable, use "SearchPaper" tool to find its definition.2. If the LaTeX Description references other sections of the paper, use "SearchSection" tool to retrieve those sections and supplement the missing details. 3.If the LaTeX Description cites methods from other papers, use "SearchLiterature" tool to extract relevant information from the referenced papers.[Action] 1. Apply a tool defined above to gather external information.2. If you have gathered all the necessary information, fully understood the LaTeX code, and are prepared to proceed to the Code Reproduction stage, the appropriate action is "Finish"[Observation] 1.If the action is apply predefined tool, then the observation should be the return response of the tool.You are a code assistant tasked with reproducing a Python function corresponding to a algorithm in the methods part of a scientific paper.The local coding environment includes a GPU and supports CUDA.I will provide the following information:1. Repository structure: The organization of files within the code repository.This is a repository-level code generation task, so you should explore the repo thoroughly to extract useful code.2. Target function: The definition of the python function you need to implement.3. LaTeX description: The LaTeX code for the corresponding algorithm in the paper, describing the algorithm implemented by the target function.4. The extracted information: The information extracted from the target paper, and relevant literature that can provide you more details when implement the target function.5. Tools: Tools that can be adopted to gather external information during the generation process.The information is extracted from the paper and relevant literature by a paper search agent, which consists of a series of information points.When you implement the target function, you should refer to the extracted information to understand the target algorithm.When information from "Relevant Literature" conflicts with the target paper, always prioritize the information from the target paper.The extracted information is as follows: In order to complete this task, it is necessary to use tools to search the code repository for context that can help implement the target function.For example: 1. Use "SearchFile" to retrieve the content of a Python file from the repository.2. Use "SearchCodeItem" to find details about a specific code item within the repository.3. Use "SearchWeb" to retrieve information from the website.To effectively tackle the code reproduction task, follow a structured process that alternates between Thought, Action, and Observation steps:[Thought] 1. Analyze the current situation.2. Identify missing information from code.As it is a repo-level code generation task, you need to explore the relvant functions, classes, in the code repository.3. Plan the next steps to gather the required information.[Action] 1. Apply a tool defined above to gather external information.2. If you are ready to generate the code, then the action should be "GenerateCode".[Observation] 1.If the action is apply predefined tool, then the observation should be the return response of the tool.2. If the action is "GenerateCode", then the observation is the result returned by the interpreter after executing the generated code.
GPT 4o mini. </p>
<p>Adaptive contrastive search: Uncertainty-guided decoding for open-ended text generation. Esteban Garces Arias, Julian Rodemann, Meimingwei Li, Christian Heumann, M Aßenmacher, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, ArXiv, abs/2108.077322021</p>
<p>Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning. Markus J Buehler, 10.1088/2632-2153/ad7228Machine Learning: Science and Technology. 2024</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal A Maksin, Lilian Patwardhan, Aleksander Weng, Mkadry, ArXiv, abs/2410.070952024</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo Bavarian, Clemens Winter, Phil Tillet, Felipe Petroski Such, David W Cummings, Matthias Plappert, Fotios Chantzis, ArXiv, abs/2107.03374Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew CarrEvaluating large language models trained on code</p>
<p>Lifelong knowledge editing for llms with retrieval-augmented continuous prompt learning. Qizhou Chen, Taolin Zhang, Dongyang Li, Longtao Huang, Hui Xue, Chengyu Wang, Xiaofeng He, Conference on Empirical Methods in Natural Language Processing. 2024a</p>
<p>Routerdc: Querybased router by dual contrastive learning for assembling large language models. Shuhao Chen, Weisen Jiang, Baijiong Lin, James T Kwok, Yu Zhang, ArXiv, abs/240919886. 2024b</p>
<p>Learning to maximize mutual information for chain-of-thought distillation. Xin Chen, Hanxian Huang, Yanjun Gao, Yi Wang, Jishen Zhao, Ke Ding, Annual Meeting of the Association for Computational Linguistics. 2024c</p>
<p>When is tree search useful for llm planning? it depends on the discriminator. Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, Huan Sun, Annual Meeting of the Association for Computational Linguistics. 2024d</p>
<p>Reasoning paths optimization: Learning to reason and explore from diverse paths. Ken Yew, Guizhen Chia, Weiwen Chen, Anh Tuan Xu, Soujanya Luu, Li Poria, Bing, ArXiv, abs/2410.108582024</p>
<p>Nearest neighbor normalization improves multimodal retrieval. Neil Chowdhury, Franklin Wang, Sumedh Shenoy, Douwe Kiela, Sarah Schwettmann, Tristan Thrush, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis , Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E Gonzalez, ArXiv, abs/2502.082352025</p>
<p>Deepseek-Ai , Aixin Liu, Bei Feng, Bing Xue, Bing-Li Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J L Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Jun-Mei Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R J Chen, R L Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S S Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shao-Ping Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, T Shuting Pan, Tao Wang, Tian Yun, Tianyu Pei, W L Sun, Wangding Xiao, Wanjia Zeng, Wei Zhao, Wen An, Wenfeng Liu, Wenjun Liang, Wen-Xuan Gao, Wentao Yu, X Q Zhang, Xiangyu Li, Xianzu Jin, Xiaoling Wang, Xiaodong Bi, Xiaohan Liu, Xi-Cheng Wang, Xiaokang Shen, Xiaokang Chen, Xiaosha Zhang, Xiaotao Chen, Xiaowen Nie, Xiaoxiang Sun, Xin Wang, Xin Cheng, Xin Liu, Xingchao Xie, Xingkai Liu, Xinnan Yu, Xinxia Song, Xinyi Shan, Xinyu Zhou, Xinyuan Yang, Xuecheng Li, Xuheng Su, Y K Lin, Y Q Li, Y X Wang, Y X Wei, Yang Zhu, Yanhong Zhang, Yanping Xu, Yao Huang, Yao Li, Yaofeng Zhao, Yao Sun, Yaohui Li, Yi Wang, Yi Yu, Yichao Zheng, Yifan Zhang, Yi Shi, Ying Xiong, Ying He, Yishi Tang, Yisong Piao, Yixuan Wang, Yi-Bing Tan, Yiyuan Ma, Yongqiang Liu, Yu Guo, Yuan Wu, Yuchen Ou, Yuduan Zhu, Yue Wang, Yuheng Gong, Yujia Zou, Yukun He, Yunfan Zha, Yunxiang Xiong, Yuting Ma, Yu-Wei Yan, Yu Luo, Yuxuan Mei You, Yuyang Liu, Z F Zhou, Zehui Wu, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhen Xu, Zhen Huang, Zhenda Zhang, Xie, Zhewen Zhen Guo Zhang, Zhibin Hao, Zhicheng Gou, Zhigang Ma, Zhihong Yan, Zhipeng Shao, Zhiyu Xu, Zhongyu Wu, Zhuoshu Zhang, Zihui Li, Zijia Gu, Zijun Zhu, Zi-An Liu, Ziwei Li, Xie, ArXiv, abs/2412.19437Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report. Ziyang Song2024</p>
<p>Document-level claim extraction and decontextualisation for fact-checking. Zhenyun Deng, M Schlichtkrull, Andreas Vlachos, Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>LLMs assist NLP researchers: Critique paper (meta-)reviewing. Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Ranran Haoran, Vipul Zhang, Yinghui Gupta, Tao Li, Fei Li, Qin Wang, Tianlin Liu, Pengzhi Liu, Congying Gao, Chen Xia, Cheng Xing, Zhaowei Jiayang, Ying Wang, Raj Sanjay Su, Ruohao Shah, Jing Guo, Haoran Gu, Kangda Li, Zihao Wei, Lu Wang, Surangika Cheng, Meng Ranathunga, Jie Fang, Fei Fu, Ruihong Liu, Eduardo Huang, Yixin Blanco, Rui Cao, Philip S Zhang, Wenpeng Yu, Yin, 10.18653/v1/2024.emnlp-main.292Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, ArXiv, abs/2409.055562024</p>
<p>GPT-4o. </p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack, 2025</p>
<p>Xuemei Gu and Mario Krenn. Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders. Tianyang Gu, Jingjin Wang, Zhihao Zhang, Haohong Li, ArXiv, abs/2412.141412024. 2025Llms can realize combinatorial creativity: generating creative ideas via llms for scientific research</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Xiaodong Song, Jacob Steinhardt, ArXiv, abs/2105.099382021</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, International Conference on Machine Learning. 2023</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, ArXiv, abs/2310.067702024. 2023Ofir Press, and Karthik NarasimhanSwe-bench: Can language models resolve real-world github issues?</p>
<p>Exploring concept depth: How large language models acquire knowledge at different layers?. Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang, ArXiv, abs/2404.070662024</p>
<p>Masklid: Code-switching language identification through iterative masking. Franccois Amir Hossein Kargaran, Hinrich Yvon, Schutze, ArXiv, abs/2406.062632024</p>
<p>Towards robust and generalized parameterefficient fine-tuning for noisy label learning. Yeachan Kim, Junho Kim, Sangkeun Lee, ArXiv, abs/2411.008732024</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, ArXiv, abs/2409.061852024</p>
<p>Style-specific neurons for steering llms in text style transfer. Wen Lai, Viktor Hangya, Alexander Fraser, ArXiv, abs/2410.005932024</p>
<p>Deveval: A manually-annotated code generation benchmark aligned with real-world code repositories. Jia Li, Ge Li, Yunfei Zhao, Yongming Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang, Yuqi Zhu, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, Yongbin Li, ArXiv, abs/2405Conference on Empirical Methods in Natural Language Processing. Hongfu Liu, Yuxi Xie, Ye Wang, Michael Shieh, 19856. 2024. 2024aAdvancing adversarial suffix transfer learning on aligned large language models</p>
<p>Beyond single-event extraction: Towards efficient document-level multi-event argument extraction. Wanlong Liu, Li Zhou, Dingyi Zeng, Yichen Xiao, Shaohuan Cheng, Chen Zhang, Grandee Lee, Malu Zhang, Wenyu Chen, ArXiv, abs/2405.018842024b</p>
<p>Haotian Ye, and Hinrich Sch ütze. Transmi: A framework to create strong baselines from multilingual pretrained language models for transliterated data. Yihong Liu, Chunlan Ma, ArXiv, abs/2405.099132024c</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou ; Yuliang, Xiangru Liu, Zefan Tang, Junjie Cai, Yichi Lu, Yanjun Zhang, Zexuan Shao, Helan Deng, Zengxian Hu, Kaikai Yang, Ruijun An, Shuzheng Huang, Sheng Si, Haozhe Chen, Zheng Zhao, Liang Li, Yiming Chen, Yan Zong, Tianyu Wang, Zhiwei Liu, Baobao Jiang, Yujia Chang, Wangchunshu Qin, Yilun Zhou, Zhao, ArXiv, abs/2503.21248Arman Cohan, and Mark B. Gerstein. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code. 2025. 2023</p>
<p>Code generation from flowcharts with texts: A benchmark dataset and an approach. Zejie Liu, Xiaoyu Hu, Deyu Zhou, Lin Li, Xu Zhang, Yanzheng Xiang, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Nl2sql generation with noise labels based on multi-task learning. Lingli Long, Yongjin Zhu, Jun Shao, Zheng Kong, Jian Li, Yanzheng Xiang, Xu Zhang, Journal of Physics: Conference Series. 22942022</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N Foerster, Jeff Clune, David Ha, ArXiv, abs/2408.062922024</p>
<p>Less is ken: a universal and simple non-parametric pruning algorithm for large language models. Michele Mastromattei, Fabio Massimo Zanzotto ; Deepak, Lovish Nathani, Nicholas Madaan, Niko Roberts, Ajay Lay Bashlykov, Vincent Menon, Amar Moens, Despoina Budhiraja, Vladislav Magka, Gaurav Vorotilov, Dieuwke Chaurasia, Ricardo Silveira Hupkes, Tatiana Cabral, Jakob Shavrina, Yoram Foerster, William Bachrach, Wang Yang, Roberta Raileanu, ArXiv, abs/2502.144992024. 2025Mlgym: A new framework and benchmark for advancing ai research agents. o3 mini</p>
<p>Sparks of science: Hypothesis generation using structured paper data. O' Charles, Tirthankar Neill, Roberta Ghosal, Mike Raileanu, Thang Walmsley, Kevin Bui, Ioana Schawinski, Ciuca, ArXiv, abs/2504.129762025</p>
<p>Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson, Michael E Moseley, Curtis P Langlotz, Akshay S Chaudhari, Jean-Benoit Delbrouck, ArXiv, abs/2405.03595Generative radiology report evaluation and error notation. Green2024</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Annual Meeting of the Association for Computational Linguistics. 2002</p>
<p>Enhancing knowledge distillation of large language models through efficient multi-modal distribution alignment. Tianyu Peng, Jiajun Zhang, International Conference on Computational Linguistics. 2024</p>
<p>Neuromax: Enhancing neural topic modeling via maximizing mutual information and group topic regularization. Duy-Tung Pham, Thien , Trang Nguyen Vu, Tung Nguyen, Linh Ngo Van, Duc , Anh Nguyen, Thien Huu Nguyen, ArXiv, abs/2409.197492024</p>
<p>Evaluating the text-to-sql capabilities of large language models. Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau, ArXiv, abs/2204.004982022</p>
<p>From zero to hero: Cold-start anomaly detection. Tal Reiss, George Kour, Naama Zwerdling, Ateret Anaby-Tavor, Yedid Hoshen, ArXiv, abs/2405.203412024</p>
<p>Superbench: A super-resolution benchmark dataset for scientific machine learning. N Pu Ren, Shashank Benjamin Erichson, Omer Subramanian, Zarija San, Michael W Lukic, Mahoney, ArXiv, abs/2306.140702023</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, M Tang, Ambrosio Zhou, Shuai Blanco, Ma, ArXiv, abs/2009.102972020</p>
<p>Raptor: Recursive abstractive processing for tree-organized retrieval. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D Manning, ArXiv, abs/2401.180592024</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, ArXiv, abs/2302.047612023</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, ArXiv, abs/2501.04227Agent laboratory: Using llm agents as research assistants. 2025</p>
<p>Paper2code: Automating code generation from scientific papers in machine learning. Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang, ArXiv, abs/2504.171922025</p>
<p>Ircan: Mitigating knowledge conflicts in llm generation via identifying and reweighting contextaware neurons. Dan Shi, Renren Jin, Tianhao Shen, Weilong Dong, Xinwei Wu, Deyi Xiong, ArXiv, abs/2406.184062024</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, ArXiv, abs/2409.041092024</p>
<p>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, Trans. Mach. Learn. Res. 20242024</p>
<p>Unsupervised homography estimation on multimodal image pair via alternating optimization. Sanghyeob Song, Jaihyun Lew, Hyemi Jang, Sungroh Yoon ; Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, ArXiv, abs/2504.01848Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan2024. 2025Paperbench: Evaluating ai's ability to replicate ai research</p>
<p>Stop overthinking: A survey on efficient reasoning for large language models. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, Xia Hu, 2025</p>
<p>Language-specific neurons: The key to multilingual capabilities in large language models. Liyan Tang, Philippe Laban, Greg Durrett, ; Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, Ji-Rong Wen, ArXiv, abs/2402.16438Conference on Empirical Methods in Natural Language Processing. 2024a. 2024bMinicheck: Efficient fact-checking of llms on grounding documents</p>
<p>Unifying dualspace embedding for entity alignment via contrastive learning. Hung Quoc To, Minh Huynh Nguyen, D Q Nghi, Bui ; Cunda, Weihua Wang, Qiuyu Wang, Feilong Liang, Guanglai Bao, Gao, ArXiv, abs/2412.05028Annual Meeting of the Association for Computational Linguistics. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 2023. 2024a. 2023Annual Meeting of the Association for Computational Linguistics</p>
<p>Garlic: Llm-guided dynamic progress control with hierarchical weighted graph for long document qa. Xinyu Wang, Yanzheng Xiang, Lin Gui, Yulan He ; Yaoke, Yun Wang, Wenqiao Zhu, Yueting Zhang, Yunfei Zhuang, Siliang Li, Tang, ArXiv, abs/2410.04790Conference on Empirical Methods in Natural Language Processing. 2024b. 2024cBridging local details and global context in text-attributed graphs</p>
<p>Execution-based evaluation for open-domain code generation. Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>G³r: A graph-guided generate-and-rerank framework for complex and cross-domain text-to-sql generation. Yanzheng Xiang, Qian-Wen Zhang, Xu Zhang, Zejie Liu, Yunbo Cao, Deyu Zhou, Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Bill Yuchen Lin, and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He ; Zhangchen, Fengqing Xu, Luyao Jiang, Jinyuan Niu, Jia, ArXiv, abs/2402.089832024. 2024Addressing order sensitivity of incontext demonstration examples in causal language models</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Nicolaus Foerster, Jeff Clune, David Ha, ArXiv, abs/2504.080662025</p>
<p>Simple but effective compound geometric operations for temporal knowledge graph completion. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao ; Rui Ying, Mengting Hu, Jianfeng Wu, Yalan Xie, Xiaoyi Liu, Zhunheng Wang, Ming Jiang, Hang Gao, Linlin Zhang, Renhong Cheng, ArXiv, abs/2210.03629Annual Meeting of the Association for Computational Linguistics. 2022. 2024React: Synergizing reasoning and acting in language models</p>
<p>Breaking the ceiling of the llm community by treating token generation as a classification for ensembling. Yao-Ching Yu, Chun-Chih Kuo, Ziqi Ye, Yu-Cheng Chang, Yueh-Se Li, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>Neuron-level knowledge attribution in large language models. Zeping Yu, Sophia Ananiadou, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Can we automate scientific reviewing?. Weizhe Yuan, Pengfei Liu, Graham Neubig, 10.1613/jair.1.12862J. Artif. Int. Res. 1076-975775December 2022</p>
<p>Gliner: Generalist model for named entity recognition using bidirectional transformer. Urchade Zaratiana, Nadi Tomeh, Pierre Holat, Thierry Charnois, ArXiv, abs/2311.085262023</p>
<p>End-to-end beam retrieval for multi-hop question answering. Jiahao Zhang, H Zhang, Dongmei Zhang, Yong Liu, Sheng Huang, North American Chapter. the Association for Computational Linguistics2023a</p>
<p>Toolcoder: Teach code generation models to use api search tools. Kechi Zhang, Ge Li, Jia Li, Zhuo Li, Zhi Jin, ArXiv, abs/2305.040322023b</p>
<p>Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin, Annual Meeting of the Association for Computational Linguistics. 2024a</p>
<p>I2r: Intra and intermodal representation learning for code search. Xu Zhang, Yanzheng Xiang, Zejie Liu, Xiaoyu Hu, Deyu Zhou, Intell. Data Anal. 282023c</p>
<p>Hfd: Hierarchical feature decoupling for sql generation from text. Xu Zhang, Xiaoyu Hu, Zejie Liu, Yanzheng Xiang, Deyu Zhou, Intell. Data Anal. 282024b</p>
<p>Secon: Maintaining semantic consistency in data augmentation for code search. Xu Zhang, Zexu Lin, Xiaoyu Hu, Jianlei Wang, Wenpeng Lu, Deyu Zhou, ACM Transactions on Information Systems. 2024c</p>
<p>Ratescore: A metric for radiology report generation. W Zhao, C Wu, X Zhang, Y Zhang, Y Wang, W Xie, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>The mystery of in-context learning: A comprehensive survey on interpretation and analysis. Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>When is Tree Search Useful for LLM Planning?. Chen , 2024d2024It Depends on the Discriminator</p>
<p>Functional Overlap Reranking for Neural Code Generation. To, Findings of ACL 2024. 2023</p>
<p>Beyond Single-Event Extraction: Towards Efficient Document-Level Multi-Event Argument Extraction. Liu, Findings of ACL 2024. 2024b</p>
<p>Unifying Dual-Space Embedding for Entity Alignment via Contrastive Learning. Wang , 2024a2025</p>
<p>Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concepts at Different Layers?. Jin , 20242025</p>
<p>TRANSMI: A Framework to Create Strong Baselines from Multilingual Pretrained Language Models for Transliterated Data. Liu, 2024c2025</p>
<p>Enhancing Knowledge Distillation of Large Language Models through Efficient Multi-Modal Distribution Alignment. 2024Peng &amp; Zhang2025</p>
<p>Document-level Claim Extraction and Decontextualisation for Fact-Checking. Deng, 20242024</p>
<p>IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons. Shi, 20242024</p>
<p>RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models. Chen , 2024b2024</p>
<p>Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization. Song, 20242024</p>
<p>Sarthi, ICLR 2024RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. 2024</p>
<p>Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models. Findings of ACL 2024. Mastromattei &amp; Zanzotto2024</p>
<p>Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation. Arias, Findings of EMNLP 2024. 2024</p>
<p>Efficient Fact-Checking of LLMs on Grounding Documents. ; Minicheck, Tang, 2024a2024</p>
<p>Nearest Neighbor Normalization Improves Multimodal Retrieval. Chowdhury, 20242024</p>
<p>Neuron-Level Knowledge Attribution in Large Language Models. Yu &amp; Ananiadou20232024</p>
<p>Zhao, Metric for Radiology Report Generation. 20242024</p>
<p>Ostmeier, GREEN: Generative Radiology Report Evaluation and Error Notation. 2024Findings of EMNLP 2024</p>
<p>Lai, Specific Neurons for Steering LLMs in Text Style Transfer. 20242024</p>
<p>Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models. Tang, 2024b2024</p>
<p>Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths. Chia , Findings of EMNLP 2024. 2024</p>
<p>Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning. Chen , 2024a2024</p>
<p>Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization. ; Neuromax, Pham, Findings of EMNLP 2024. 2024</p>
<p>Bridging Local Details and Global Context in Text-Attributed Graphs. Wang , 2024c2024</p>
<p>Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models. Liu, 2024a2024</p>
<p>SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding. Xu, 20242024</p>
<p>MaskLID: Code-Switching Language Identification through Iterative Masking. Kargaran, 20242024</p>
<p>Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning. Kim, 20242024</p>
<p>Learning to Maximize Mutual Information for Chain-of-Thought Distillation. Chen , Findings of ACL 2024. 2024c</p>
<p>Zaratiana Gliner, Generalist Model for Named Entity Recognition using Bidirectional Transformer. 20232024</p>
<p>End-to-End Beam Retrieval for Multi-Hop Question Answering. Zhang, 2023a2024</p>
<p>Table A1: List of papers used in our Sci-Replicate benchmark. </p>            </div>
        </div>

    </div>
</body>
</html>
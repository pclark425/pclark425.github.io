<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3674 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3674</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3674</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-260887765</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.07120v2.pdf" target="_blank">Position: Key Claims in LLM Research Have a Long Tail of Footnotes</a></p>
                <p><strong>Paper Abstract:</strong> Much of the recent discourse within the ML community has been centered around Large Language Models (LLMs), their functionality and potential -- yet not only do we not have a working definition of LLMs, but much of this discourse relies on claims and assumptions that are worth re-examining. We contribute a definition of LLMs, critically examine five common claims regarding their properties (including 'emergent properties'), and conclude with suggestions for future research directions and their framing.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3674.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3674.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A Large Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specialized large language model trained primarily on scientific literature and metadata, intended to generate and synthesize scientific text and knowledge for tasks within the scientific domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A Large Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A large language model trained on a corpus of scientific texts (papers, reference materials, etc.) designed to produce encyclopedic outputs and to be used flexibly for a range of science-focused downstream tasks (e.g., summaries, explanations, citation-style generation). The paper mentions it as an example of an LLM trained on scientific literature intended for knowledge work in science.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this position paper beyond 'scientific literature' — the cited original work reports a large corpus of scientific papers, reference data, and related metadata (exact size and sources are not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not detailed in this paper; implied use via natural-language prompts or task-specific inputs typical for LLMs (e.g., prompts requesting summaries, explanations, or citations).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Pretraining on large-scale scientific corpora (unsupervised token prediction) with transfer to downstream tasks; specific synthesis methods are not described in this position paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Generative outputs such as encyclopedic passages, summaries, and science-domain text; format depends on prompted task (natural-language text).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not described in detail in this position paper; the paper cites Galactica as an example but does not report its evaluation procedures here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>This position paper only cites Galactica as an exemplar of LLMs trained on scientific literature; no new experimental results from Galactica are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mentioned indirectly via broader concerns: risks of training-data contamination, hallucination, lack of transparency (especially for models used as scientific baselines), and reproducibility problems when models or their data are not fully disclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No comparisons reported in this paper; referenced only as an example of domain-trained LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Position: Key Claims in LLM Research Have a Long Tail of Footnotes', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3674.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3674.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciBERT: A pretrained language model for scientific text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-derived pretrained language model trained on a large corpus of scientific publications to provide domain-specific representations for downstream scientific NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciBERT: A pretrained language model for scientific text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SciBERT</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A domain-adapted pretrained transformer (masked language model) trained on scientific text corpora to produce embeddings and be fine-tuned for scientific-domain NLP tasks; cited as an LLM that qualifies under the authors' LLM definition because it is pretrained on large-scale domain data and used for transfer learning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Described in this paper only as 'scientific literature' — original SciBERT work used a large corpus of papers from biomedical and computer science domains (not enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Typical usage is via fine-tuning or prompting for downstream tasks; the position paper does not detail query formats for SciBERT specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Standard masked-language-model pretraining on scientific corpora followed by transfer learning (fine-tuning) for tasks — no specialized distillation-for-theory step is described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Learned contextual representations (embeddings) and task-specific outputs after fine-tuning (classification, extraction), rather than high-level theory statements; used as a building block for domain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not given here; SciBERT is referenced as an example of domain-specific LLM pretraining rather than evaluated in this position paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited to illustrate that domain-trained LLMs (trained on scientific literature) exist and are expected to be used flexibly across tasks; no quantitative results from SciBERT are presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>General concerns in the paper apply: domain coverage limits, potential dataset biases, and issues of reproducibility and transparency when using domain-specific LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Position: Key Claims in LLM Research Have a Long Tail of Footnotes', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3674.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3674.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextbooksAreAllYouNeed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Textbooks are all you need</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A work cited that explores training language models on structured, curated textbook-style corpora as a way to concentrate high-quality domain knowledge for model learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Textbooks are all you need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Textbook-trained LLM approach</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An approach (as per the cited paper) that trains or fine-tunes LLMs primarily on textbook/curated instructional corpora to concentrate domain knowledge for downstream reasoning and knowledge synthesis; mentioned here as an example of efforts emphasizing data quality over sheer scale.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Curated textbook-style corpora (structured educational materials) rather than broad web-scale crawls — exact composition and size are not described in this position paper.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not detailed here; typical LLM usage via natural-language prompts or fine-tuning for domain tasks is implied.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Pretraining/fine-tuning on high-quality textbook corpora to encode structured domain knowledge; specific synthesis algorithms are not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Domain-focused generative outputs or improved downstream task performance; not specified in this position paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not provided in this paper; referenced only to illustrate emphasis on data quality.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as part of an argument that higher-quality training data can produce smaller models that perform well; no concrete metrics from the cited work are reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Position paper highlights general issues: whether such training produces generalizable emergent behaviors, potential coverage gaps, and the need for transparent data reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Position: Key Claims in LLM Research Have a Long Tail of Footnotes', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3674.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3674.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROOTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The ROOTS Search Tool: Data Transparency for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search/indexing tool effort designed to provide transparency for LLM training data by enabling search over known training corpora to link model outputs to potential sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ROOTS Search Tool: Data Transparency for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ROOTS Search Tool</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A tool that builds search indices over LLM training corpora so researchers can query potential provenance for model outputs and investigate whether particular content was present in training data; mentioned as a current effort to link outputs to training evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Indexes of known training corpora or plausible subsets thereof (the position paper references the tool but does not enumerate the indexed corpora or their sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Queries are formulated as search queries (likely natural-language or string queries) to find matching or similar passages in training corpora; the paper references ROOTS as enabling provenance queries.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not a distillation system per se — rather, a retrieval/indexing provenance tool to support analysis of whether model outputs could have been learned/memorized from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Search results linking candidate training documents and passages to an LLM output (provenance evidence), enabling traceability and contamination analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not described here; the position paper lists ROOTS among relevant efforts for assessing memorization and linking outputs to training data but does not report evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as a relevant tool for improving transparency and for studies aiming to establish links between model behavior and training data; no performance numbers are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Implied limitations include incomplete coverage of private or undisclosed training data, difficulty of exact-match detection versus semantic similarity, and the broader challenge of definitively proving absence of evidence in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not applicable / not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Position: Key Claims in LLM Research Have a Long Tail of Footnotes', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3674.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3674.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DataPortraits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data Portraits: Recording Foundation Model Training Data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study/tool that documents and provides metadata/portraits for foundation-model training datasets to improve transparency about what models are trained on.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data Portraits: Recording Foundation Model Training Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Data Portraits</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An approach to document, index, and provide searchable metadata about foundation-model training corpora to help researchers assess provenance, contamination risks, and dataset composition; cited as part of efforts to link LLM behavior to training data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Metadata and derived indices about large training corpora (not enumerated in this position paper); aims to cover sizeable multi-source corpora used for LLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not explicitly described here; intended queries would include provenance and content-origin queries to assess whether specific materials could appear in training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not a distillation LLM method — a dataset documentation/indexing approach to facilitate downstream analysis and validation of LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Dataset metadata, provenance indices, and search results linking candidate outputs to sources; used as evidence in contamination and memorization analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not provided in this paper; referenced as part of ongoing transparency work without evaluations described here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as a relevant transparency effort to help validate claims about emergent properties and memorization; no results are presented in this position paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Position paper highlights general limitations: inability to cover closed/private training data, challenges of semantic-level matches vs exact matches, and the broader difficulty of proving absence of training evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Position: Key Claims in LLM Research Have a Long Tail of Footnotes', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3674.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3674.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OLMO / DOLMa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OLMO model and DOLMa dataset (Accelerating the science of language models / An open corpus of three trillion tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent efforts cited that aim to increase transparency and openness in LLM research by releasing models (OLMO) and large open pretraining corpora (DOLMa) to support reproducible study and benchmarking of LLM behavior on scientific questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accelerating the science of language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>OLMO / DOLMa openness effort</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>OLMO is described as a more transparent language model and DOLMa as a very large open-pretraining dataset (trillions of tokens) intended to enable reproducible pretraining and more thorough study of LLM behavior including links between training data and model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>DOLMa is described in the cited works as an open corpus of approximately three trillion tokens for language model pretraining (this position paper references these works but does not provide their detailed composition).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not described here; their purpose is to provide open resources enabling a variety of research queries and controlled studies rather than a single fixed query protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Standard large-scale pretraining on the open corpus; the position paper cites these resources as steps toward enabling rigorous study rather than describing new distillation algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Not specified in this paper — outputs would be standard LLM generative or representation outputs depending on downstream tasks researchers run using the open resources.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not given here; the position paper cites these releases as enabling reproducible evaluation work but does not report evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as examples of improved openness in the field that could enable better linkage between behavior and training data; the position paper does not present experimental outcomes from these resources.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>General concerns from the paper apply: even with open corpora, careful methodology is required to avoid confounds (e.g., overlap between evaluation and pretraining data), and large-scale resource requirements still limit reproducibility for many groups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Position: Key Claims in LLM Research Have a Long Tail of Footnotes', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A Large Language Model for Science <em>(Rating: 2)</em></li>
                <li>SciBERT: A pretrained language model for scientific text <em>(Rating: 2)</em></li>
                <li>The ROOTS Search Tool: Data Transparency for LLMs <em>(Rating: 2)</em></li>
                <li>Data Portraits: Recording Foundation Model Training Data <em>(Rating: 2)</em></li>
                <li>Textbooks are all you need <em>(Rating: 1)</em></li>
                <li>Accelerating the science of language models <em>(Rating: 1)</em></li>
                <li>An open corpus of three trillion tokens for language model pretraining research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3674",
    "paper_id": "paper-260887765",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "Galactica",
            "name_full": "Galactica: A Large Language Model for Science",
            "brief_description": "A domain-specialized large language model trained primarily on scientific literature and metadata, intended to generate and synthesize scientific text and knowledge for tasks within the scientific domain.",
            "citation_title": "Galactica: A Large Language Model for Science",
            "mention_or_use": "mention",
            "system_or_method_name": "Galactica",
            "system_or_method_description": "A large language model trained on a corpus of scientific texts (papers, reference materials, etc.) designed to produce encyclopedic outputs and to be used flexibly for a range of science-focused downstream tasks (e.g., summaries, explanations, citation-style generation). The paper mentions it as an example of an LLM trained on scientific literature intended for knowledge work in science.",
            "input_corpus_description": "Not specified in this position paper beyond 'scientific literature' — the cited original work reports a large corpus of scientific papers, reference data, and related metadata (exact size and sources are not detailed here).",
            "topic_or_query_specification": "Not detailed in this paper; implied use via natural-language prompts or task-specific inputs typical for LLMs (e.g., prompts requesting summaries, explanations, or citations).",
            "distillation_method": "Pretraining on large-scale scientific corpora (unsupervised token prediction) with transfer to downstream tasks; specific synthesis methods are not described in this position paper.",
            "output_type_and_format": "Generative outputs such as encyclopedic passages, summaries, and science-domain text; format depends on prompted task (natural-language text).",
            "evaluation_or_validation_method": "Not described in detail in this position paper; the paper cites Galactica as an example but does not report its evaluation procedures here.",
            "results_summary": "This position paper only cites Galactica as an exemplar of LLMs trained on scientific literature; no new experimental results from Galactica are reported here.",
            "limitations_or_challenges": "Mentioned indirectly via broader concerns: risks of training-data contamination, hallucination, lack of transparency (especially for models used as scientific baselines), and reproducibility problems when models or their data are not fully disclosed.",
            "comparison_to_baselines_or_humans": "No comparisons reported in this paper; referenced only as an example of domain-trained LLMs.",
            "uuid": "e3674.0",
            "source_info": {
                "paper_title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "SciBERT",
            "name_full": "SciBERT: A pretrained language model for scientific text",
            "brief_description": "A BERT-derived pretrained language model trained on a large corpus of scientific publications to provide domain-specific representations for downstream scientific NLP tasks.",
            "citation_title": "SciBERT: A pretrained language model for scientific text",
            "mention_or_use": "mention",
            "system_or_method_name": "SciBERT",
            "system_or_method_description": "A domain-adapted pretrained transformer (masked language model) trained on scientific text corpora to produce embeddings and be fine-tuned for scientific-domain NLP tasks; cited as an LLM that qualifies under the authors' LLM definition because it is pretrained on large-scale domain data and used for transfer learning.",
            "input_corpus_description": "Described in this paper only as 'scientific literature' — original SciBERT work used a large corpus of papers from biomedical and computer science domains (not enumerated here).",
            "topic_or_query_specification": "Typical usage is via fine-tuning or prompting for downstream tasks; the position paper does not detail query formats for SciBERT specifically.",
            "distillation_method": "Standard masked-language-model pretraining on scientific corpora followed by transfer learning (fine-tuning) for tasks — no specialized distillation-for-theory step is described in this paper.",
            "output_type_and_format": "Learned contextual representations (embeddings) and task-specific outputs after fine-tuning (classification, extraction), rather than high-level theory statements; used as a building block for domain tasks.",
            "evaluation_or_validation_method": "Not given here; SciBERT is referenced as an example of domain-specific LLM pretraining rather than evaluated in this position paper.",
            "results_summary": "Cited to illustrate that domain-trained LLMs (trained on scientific literature) exist and are expected to be used flexibly across tasks; no quantitative results from SciBERT are presented in this paper.",
            "limitations_or_challenges": "General concerns in the paper apply: domain coverage limits, potential dataset biases, and issues of reproducibility and transparency when using domain-specific LLMs.",
            "comparison_to_baselines_or_humans": "Not reported in this paper.",
            "uuid": "e3674.1",
            "source_info": {
                "paper_title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "TextbooksAreAllYouNeed",
            "name_full": "Textbooks are all you need",
            "brief_description": "A work cited that explores training language models on structured, curated textbook-style corpora as a way to concentrate high-quality domain knowledge for model learning.",
            "citation_title": "Textbooks are all you need",
            "mention_or_use": "mention",
            "system_or_method_name": "Textbook-trained LLM approach",
            "system_or_method_description": "An approach (as per the cited paper) that trains or fine-tunes LLMs primarily on textbook/curated instructional corpora to concentrate domain knowledge for downstream reasoning and knowledge synthesis; mentioned here as an example of efforts emphasizing data quality over sheer scale.",
            "input_corpus_description": "Curated textbook-style corpora (structured educational materials) rather than broad web-scale crawls — exact composition and size are not described in this position paper.",
            "topic_or_query_specification": "Not detailed here; typical LLM usage via natural-language prompts or fine-tuning for domain tasks is implied.",
            "distillation_method": "Pretraining/fine-tuning on high-quality textbook corpora to encode structured domain knowledge; specific synthesis algorithms are not discussed in this paper.",
            "output_type_and_format": "Domain-focused generative outputs or improved downstream task performance; not specified in this position paper.",
            "evaluation_or_validation_method": "Not provided in this paper; referenced only to illustrate emphasis on data quality.",
            "results_summary": "Cited as part of an argument that higher-quality training data can produce smaller models that perform well; no concrete metrics from the cited work are reproduced here.",
            "limitations_or_challenges": "Position paper highlights general issues: whether such training produces generalizable emergent behaviors, potential coverage gaps, and the need for transparent data reporting.",
            "comparison_to_baselines_or_humans": "Not reported in this paper.",
            "uuid": "e3674.2",
            "source_info": {
                "paper_title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ROOTS",
            "name_full": "The ROOTS Search Tool: Data Transparency for LLMs",
            "brief_description": "A search/indexing tool effort designed to provide transparency for LLM training data by enabling search over known training corpora to link model outputs to potential sources.",
            "citation_title": "The ROOTS Search Tool: Data Transparency for LLMs",
            "mention_or_use": "mention",
            "system_or_method_name": "ROOTS Search Tool",
            "system_or_method_description": "A tool that builds search indices over LLM training corpora so researchers can query potential provenance for model outputs and investigate whether particular content was present in training data; mentioned as a current effort to link outputs to training evidence.",
            "input_corpus_description": "Indexes of known training corpora or plausible subsets thereof (the position paper references the tool but does not enumerate the indexed corpora or their sizes).",
            "topic_or_query_specification": "Queries are formulated as search queries (likely natural-language or string queries) to find matching or similar passages in training corpora; the paper references ROOTS as enabling provenance queries.",
            "distillation_method": "Not a distillation system per se — rather, a retrieval/indexing provenance tool to support analysis of whether model outputs could have been learned/memorized from training data.",
            "output_type_and_format": "Search results linking candidate training documents and passages to an LLM output (provenance evidence), enabling traceability and contamination analysis.",
            "evaluation_or_validation_method": "Not described here; the position paper lists ROOTS among relevant efforts for assessing memorization and linking outputs to training data but does not report evaluations.",
            "results_summary": "Mentioned as a relevant tool for improving transparency and for studies aiming to establish links between model behavior and training data; no performance numbers are provided in this paper.",
            "limitations_or_challenges": "Implied limitations include incomplete coverage of private or undisclosed training data, difficulty of exact-match detection versus semantic similarity, and the broader challenge of definitively proving absence of evidence in training data.",
            "comparison_to_baselines_or_humans": "Not applicable / not reported in this paper.",
            "uuid": "e3674.3",
            "source_info": {
                "paper_title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "DataPortraits",
            "name_full": "Data Portraits: Recording Foundation Model Training Data",
            "brief_description": "A study/tool that documents and provides metadata/portraits for foundation-model training datasets to improve transparency about what models are trained on.",
            "citation_title": "Data Portraits: Recording Foundation Model Training Data",
            "mention_or_use": "mention",
            "system_or_method_name": "Data Portraits",
            "system_or_method_description": "An approach to document, index, and provide searchable metadata about foundation-model training corpora to help researchers assess provenance, contamination risks, and dataset composition; cited as part of efforts to link LLM behavior to training data.",
            "input_corpus_description": "Metadata and derived indices about large training corpora (not enumerated in this position paper); aims to cover sizeable multi-source corpora used for LLM pretraining.",
            "topic_or_query_specification": "Not explicitly described here; intended queries would include provenance and content-origin queries to assess whether specific materials could appear in training sets.",
            "distillation_method": "Not a distillation LLM method — a dataset documentation/indexing approach to facilitate downstream analysis and validation of LLM outputs.",
            "output_type_and_format": "Dataset metadata, provenance indices, and search results linking candidate outputs to sources; used as evidence in contamination and memorization analyses.",
            "evaluation_or_validation_method": "Not provided in this paper; referenced as part of ongoing transparency work without evaluations described here.",
            "results_summary": "Cited as a relevant transparency effort to help validate claims about emergent properties and memorization; no results are presented in this position paper.",
            "limitations_or_challenges": "Position paper highlights general limitations: inability to cover closed/private training data, challenges of semantic-level matches vs exact matches, and the broader difficulty of proving absence of training evidence.",
            "comparison_to_baselines_or_humans": "Not reported in this paper.",
            "uuid": "e3674.4",
            "source_info": {
                "paper_title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "OLMO / DOLMa",
            "name_full": "OLMO model and DOLMa dataset (Accelerating the science of language models / An open corpus of three trillion tokens)",
            "brief_description": "Recent efforts cited that aim to increase transparency and openness in LLM research by releasing models (OLMO) and large open pretraining corpora (DOLMa) to support reproducible study and benchmarking of LLM behavior on scientific questions.",
            "citation_title": "Accelerating the science of language models",
            "mention_or_use": "mention",
            "system_or_method_name": "OLMO / DOLMa openness effort",
            "system_or_method_description": "OLMO is described as a more transparent language model and DOLMa as a very large open-pretraining dataset (trillions of tokens) intended to enable reproducible pretraining and more thorough study of LLM behavior including links between training data and model outputs.",
            "input_corpus_description": "DOLMa is described in the cited works as an open corpus of approximately three trillion tokens for language model pretraining (this position paper references these works but does not provide their detailed composition).",
            "topic_or_query_specification": "Not described here; their purpose is to provide open resources enabling a variety of research queries and controlled studies rather than a single fixed query protocol.",
            "distillation_method": "Standard large-scale pretraining on the open corpus; the position paper cites these resources as steps toward enabling rigorous study rather than describing new distillation algorithms.",
            "output_type_and_format": "Not specified in this paper — outputs would be standard LLM generative or representation outputs depending on downstream tasks researchers run using the open resources.",
            "evaluation_or_validation_method": "Not given here; the position paper cites these releases as enabling reproducible evaluation work but does not report evaluations.",
            "results_summary": "Mentioned as examples of improved openness in the field that could enable better linkage between behavior and training data; the position paper does not present experimental outcomes from these resources.",
            "limitations_or_challenges": "General concerns from the paper apply: even with open corpora, careful methodology is required to avoid confounds (e.g., overlap between evaluation and pretraining data), and large-scale resource requirements still limit reproducibility for many groups.",
            "comparison_to_baselines_or_humans": "Not reported in this paper.",
            "uuid": "e3674.5",
            "source_info": {
                "paper_title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A Large Language Model for Science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "SciBERT: A pretrained language model for scientific text",
            "rating": 2,
            "sanitized_title": "scibert_a_pretrained_language_model_for_scientific_text"
        },
        {
            "paper_title": "The ROOTS Search Tool: Data Transparency for LLMs",
            "rating": 2,
            "sanitized_title": "the_roots_search_tool_data_transparency_for_llms"
        },
        {
            "paper_title": "Data Portraits: Recording Foundation Model Training Data",
            "rating": 2,
            "sanitized_title": "data_portraits_recording_foundation_model_training_data"
        },
        {
            "paper_title": "Textbooks are all you need",
            "rating": 1,
            "sanitized_title": "textbooks_are_all_you_need"
        },
        {
            "paper_title": "Accelerating the science of language models",
            "rating": 1,
            "sanitized_title": "accelerating_the_science_of_language_models"
        },
        {
            "paper_title": "An open corpus of three trillion tokens for language model pretraining research",
            "rating": 1,
            "sanitized_title": "an_open_corpus_of_three_trillion_tokens_for_language_model_pretraining_research"
        }
    ],
    "cost": 0.0168325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Position: Key Claims in LLM Research Have a Long Tail of Footnotes
1 Jun 2024</p>
<p>Anna Rogers 
IT University of Copenhagen</p>
<p>Alexandra Sasha Luccioni 
Canada</p>
<p>Position: Key Claims in LLM Research Have a Long Tail of Footnotes
1 Jun 2024E292E763D308BC99B9B09EEF789BE790arXiv:2308.07120v2[cs.CL]
Much of the recent discourse within the ML community has been centered around Large Language Models (LLMs), their functionality and potential -yet not only do we not have a working definition of LLMs, but much of this discourse relies on claims and assumptions that are worth reexamining.We contribute a definition of LLMs, critically examine five common claims regarding their properties (including 'emergent properties'), and conclude with suggestions for future research directions and their framing.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have become ubiquitous in the Machine Learning (ML) research landscape, and they already impact the lives of thousands of people in contexts ranging from health (Graber-Stiehl, 2023;Harrer, 2023) to education (Kasneci et al., 2023).Yet despite the many research articles on LLMs, their very definition remains unclear, and much of this work is based on claims that are often stated, but remain debatable in terms of their framing, theoretical grounding, or empirical evidence.When we, as researchers, repeat such claims uncritically, we contribute to the narratives shaped by business interests (McKelvey et al., 2023), and may mislead the public and ourselves.</p>
<p>This position paper argues that LLM research should be more precise with its key terms and claims.To that end, we propose a definition for the term "LLM" ( §2), and we critically examine five common claims about LLM functionality, drawing heavily on both empirical studies and socio-technical critiques of LLMs ( §3).We then consider the impact that these claims have on ML research ( §4).We conclude with concrete proposals for maintaining rigor and diversity in ML research and practice ( §5).</p>
<p>What Counts as an Large Language Model?</p>
<p>The common technical definition of "language models" is "models that assign probabilities to upcoming words, or sequences of words in general" (Jurafsky &amp; Martin, 2024, p.32).Yet the term "large language models" is currently used in quite a different way, both within the research community (e.g. in the call for papers of academic conferences such as EMNLP ( 2023)), in news articles (Roose, 2023), and even by legislators at U.S. Senate hearings (Zakrzewski, 2023)).Despite its ubiquity, this definition is far from clear.For instance, how many parameters should a neural network have to qualify as an LLM?And do only Transformer-type architectures qualify?What about multimodal models, such as text-to-image or image-to-text?</p>
<p>Given the many subfields and contexts in which this term is already used, it is not feasible to impose a single definition and expect everybody to adhere to it.But even if there is no agreement, both academic and general public discussions would be more productive if the authors of research papers spelled out or cited their own working definitions.What follows is our own attempt, which we hope could be useful to others (either for using our version, or as a base to be modified by other researchers to reflect their own understanding of this term).It relies upon three definitional criteria:</p>
<p>(1) LLMs model text 1 and can be used to generate 2 it based on input context, i.e. by selecting the tokens that are the most likely given the partial context provided as input (either masked 3 or as a prompt).The text can be in any modality -characters, pixels, audio, etc. (2) LLMs receive large-scale pretraining, where 'large-scale' refers to the pre-training data rather than the number of parameters.The exact threshold for LLMqualifying volume of data is necessarily arbitrary, and for an English corpus we propose setting it to 1B tokens 4 (inspired by Chelba et al. (2013)).</p>
<p>(3) LLMs are used for transfer learning, on the assumption that they encode information that can be leveraged in other tasks.Currently the most common transfer learning methods with LLMs are fine-tuning, as in BERT (Devlin et al., 2019), and prompting, as in GPT-3 (Brown et al., 2020), but there are many other methods (Pan &amp; Yang, 2010;Ramponi &amp; Plank, 2020;Alyafeai et al., 2020;Zhuang et al., 2021).</p>
<p>According to the above criteria, BERT (Devlin et al., 2019) and its derivatives do qualify as LLMs, as do models from the GPT series (Radford et al., 2018).So do n-gram language models, given that they are derived from a sufficiently large corpus, such as Google Books (Lin et al., 2012).Earlier word-level representations such as word2vec (Mikolov et al., 2013) are ruled out on the first criterion, when they are viewed by themselves, but their training is conceptually very similar to a masked language model, and they can also use large volumes of text (over 100B tokens for the original word2vec).Modern LLMs can also be used standalone, or for creating representations used in other systems (e.g.BERT's [CLS] token representation (Devlin et al., 2019) fed into classifiers).On our criteria, such representations would not be LLMs, but they are derived from LLMs.</p>
<p>Our first criterion does not rule out multimodal models like GPT-4 (OpenAI, 2023)), as long as they output text -even if they also accept or output images or other modalities.The "text" is typically human-written text in a natural language, but it could also be synthetic data (e.g.text created with templates from knowledge base data).The training data of modern LLMs typically also includes text that is not natural language data: code, ascii art, midi music, math notation, chess transcripts etc.While such data can be learned via token prediction, it is not the focus of our definition. 4The 1B threshold could need adjustment for different languages and tokenization schemes, if empirical evidence justifies it.We propose to consider raw source data, without any augmentation or considering multiple training runs.We are assuming that these data points would be mostly unique, since it is common practice to deduplicate training data.</p>
<p>When it comes to multimodal LLMs, the amount of textual information in multimodal data can still be approximated via token count (e.g. in transcripts).The other information would be extralinguistic, and a unit for that is yet to be developed -but compute or parameter counts do not do it justice either.Ideally we would be able to semantically chunk the multimodal content at least as crudely as tokens chunk text, and relate it to the text that it grounds.E.g. we should be able to distinguish between a voiceover over blank screen, a dialogue captured with a fixed camera, or the same dialogue captured from various angles, based on who is speaking.</p>
<p>Our second criterion allows for the inclusion of models such as tinyBERT (Jiao et al., 2020): it has only 4.4M parameters, but its training dataset (via model distillation) is the same as that of the full BERT model, which contains 3.3B tokens.Our choice of linking the "large" part of LLMs to the volume of data rather model size also helps to deal with another edge case: the models that were reduced in size (e.g. via distillation, such as Sanh et al. (2019)) for the sake of computational efficiency, but maintain comparable performance to the original models.</p>
<p>Our third criterion applies to the "general-purpose" LLMs that are purported to be domain-independent.But the core criterion is transfer learning, which may also take place within a specific domain.For example, SciBERT (Beltagy et al., 2019) or Galactica (Taylor et al., 2022) are still LLMs, even though they were primarily trained on scientific literature, because they are expected to be used flexibly for a range of tasks within that domain.</p>
<p>At present, in most cases the three criteria listed above correspond to Transformer-based models that are used to generate text.But having a more concrete definition helps to ground the scientific discourse, and provide a point of reference for updating it in the future.</p>
<p>As we learn more about both LLMs and neural architectures, our proposed threshold for "large" may change, e.g. if there is empirical evidence or theoretical guarantees that a certain volume of natural texts provides sufficient signal for a defined set of linguistic "skills" for a given model architecture.Hopefuly in the future we would also have more proofs, theoretical rationales, or empirical evidence, based on which the "large" part could be further qualified by numerous factors relevant to the performance of the final model: the diversity in the textual data (domains, languages, registers etc.), benchmark contamination, acceptable levels of data augmentation or explicit instruction in the form of annotated data, ratio and role of non-linguistic data, duplicate and near-duplicate data points, allowed number of model runs over the training data, etc.</p>
<p>LLMs vs "foundation" and "frontier models".LLMs are also sometimes referred to as "foundation models", a term proposed by Bommasani et al (2021) to refer to "any model that is trained on broad data (generally using selfsupervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks".This partly corresponds to our criteria (2) and (3), but makes no attempt to quantify their scale.It is also intentionally broader, so as to include e.g.models for computer vision or protein data.The term "LLM" is more specific, and useful in studies modeling language data.Another recently proposed term is "frontier models", defined as "highly capable foundation models that could exhibit suf-ficiently dangerous capabilities" (Anderljung et al., 2023).This relies on the above "foundation model" term, and only adds the criterion of "sufficiently dangerous capabilities", which the authors acknowledge to be vague.Various kinds of risks from LLMs are beyond the scope of this work, but see §3.5 for a relevant discussion of "emergent properties".</p>
<p>Fact-checking LLM Functionality</p>
<p>We discuss five common claims about LLMs: that LLMs are robust ( §3.1), that they systematically achieve state-of-theart results ( §3.2), that their performance is predominantly due to their scale ( §3.3), that they are "general-purpose technologies ( §3.4) and that they exhibit emergent properties ( §3.5).We are not saying that all these claims are completely false -but they all have many caveats, which are mentioned much less frequently.By collecting existing evidence and counter-arguments, we aim to highlight some of the gaps and inconsistencies in our current knowledge and to help orient future work so as to address these gaps.</p>
<p>Claim: LLMs are Robust</p>
<p>Early symbolic AI approaches are often described as "brittle" because of their strict dependence on pre-formulated knowledge and lack of robustness outside of the distribution they were trained on.Lenat &amp; Feigenbaum (1981, p.1175) described this as "a plateau of competence, but the edges of that plateau are steep descents into complete incompetence".With the advent of LLMs, the issue of robustness is seen to be much less prominent.For instance, Bommasani et al. (2021, p.109) state: "pretraining on unlabeled data is an effective, general-purpose way to improve accuracy on [out-of-distribution] test distributions".LLMs are often presented as multi-task learners that are robust without explicit supervision, even outside the distribution they were trained on (Radford et al., 2019;Hendrycks et al., 2019;2020).Indeed, we have overcome the problem of the steep descents into complete incompetence: unfamiliar inputs no longer completely break the system.But deep-learning-based ML systems are still fundamentally brittle, only in a different way: according to Chollet (2019, p.3), they are "unable to make sense of situations that deviate slightly from their training data or the assumptions of their creators".Chollet goes even further, stating in an interview that there are no fixes for this issue (Heaven et al., 2019).Impressive as the latest LLMs are, they still make errors even in simple tasks like adding numeric literals (Chang &amp; Bergen, 2023).Moreover, they are just as vulnerable to adversarial attacks (Zou et al., 2023) as earlier models (Wallace et al., 2019).</p>
<p>One could argue that "robust" does not mean "perfect" -but in that case, what does it mean?For an ML engineer, it is something like "sufficiently useful in practice".From that point of view, two situations are possible: the model will be deployed in the conditions either (a) guaranteed to be similar to its training distribution in all aspects that matter for its performance, or (b) expected to diverge from that.Our current LLM-based solutions may be sufficiently robust for in-distribution scenarios, but few would argue the same for out-of-distribution cases.Case (b) covers many, if not most, LLM application areas: text classifiers will continually encounter new topics and domains, language usage will evolve, the correct answers to factual questions will change, discourse strategies for interaction with AIenabled chatbots will shift, people will adapt what they post online and to the privacy and surveillance concerns, etc.And failures of ML systems may have real-world consequences for those who diverge the most from its core distribution: e.g.people may be denied asylum due to errors of machine translation systems, something that we have already seen happen (Nalbandian, 2022).</p>
<p>One well-studied cause of brittleness in the LLMs of the BERT generation was shortcut learning (McCoy et al., 2019;Rogers et al., 2020;Branco et al., 2021;Choudhury et al., 2022, inter alia) -models picking up undesirable spurious correlations from the training data, which are very likely to exist in all the larger datasets used by the data-hungry deep learning systems (Gardner et al., 2021).This problem is still there for the latest LLMs: when they fail on counterfactual tasks or adversarial perturbations, this suggests that their successes are due not to learning the general principles behind a certain operation, but some narrow heuristic that does not transfer to new contexts.(Wu et al., 2023).</p>
<p>In the few-shot evaluation paradigm, we also now have a new robustness problem.The art of 'prompt engineering' (Liu et al., 2023c) arose out of the prompt sensitivity phenomenon: slight variations in the phrasing of the prompt that would not make much difference to a human can lead to very different LLM output (Lu et al., 2021;Zhao et al., 2021).In a recent evaluation of 30 LLMs, Liang et al. (2022, p.12) conclude that "all models show significant sensitivity to the formatting of prompt, the particular choice of in-context examples, and the number of in-context examples across all scenarios and for all metrics".The reports of sensitivity to exact wording keep coming for the latest models, including GPT-4 (Lee et al., 2023;Gan &amp; Mori, 2023).</p>
<p>Claim: (Few-shot) LLMs Are State-of-the-Art</p>
<p>LLM-based approaches have become the default in the current research literature, and are largely perceived to be the current SOTA (state-of-the-art) across NLP benchmarks.For example, Gillioz et al. (2020, p.179) state: "Models like GPT and BERT relying on this Transformer architecture have fully outperformed the previous state-of-the-art networks.It surpassed the earlier approaches by such a wide margin that all the recent cutting edge models seem to rely on these Transformer-based architectures."</p>
<p>The above was written in the days of fine-tuned Transformerbased LLMs like BERT.At this point, such a statement needs to be considered in the context of the distinction between few-shot performance (ostensibly out-of-domain performance achieved by a model that was not specifically trained on a given task), vs performance of a model finetuned for a given task.For example, both BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) were presented with evaluation on question answering, among other tasks, but the former was fine-tuned, while the latter evaluated in a few-shot way.</p>
<p>Generally speaking, an ML model that has been trained on some domain data can be reasonably expected to perform better in that domain than a comparable model that hasn't received such training.This means that we now have two different notions of SOTA, where the few-shot setting could reasonably be expected to yield worse performance vs the same model if it was fine-tuned, but requires less data and training.Still, the current research papers introducing LLMs often include only few-or zero-shot evaluations, which creates the impression that this is the only evaluation that matters.For example, OPT (Zhang et al., 2022) was evaluated on 16 tasks concurrently without fine-tuning, establishing new accuracy in several of them; the same goes for models such as PaLM (Chowdhery et al., 2022), LLaMa (Touvron et al., 2023a), and many others.</p>
<p>We broadly agree that pre-trained models based on Transformer architecture are likely to be the current SOTA when they are provided additional in-domain supervision.But most of the current LLM evaluation discourse shifted to few-or zero-shot evaluation, and in that context the SOTA claim may not hold.Hence, many of the current results may not actually represent the current SOTA.</p>
<p>When we consider direct comparisons between few-shot LLMs and supervised systems, not based on the same LLMs, the winner depends on the specific case, but the few-shot LLM is not at all guaranteed to win -especially in the "true few-shot" setting, where prompts are not selected based on extra held-out data (Perez et al., 2021).They may also be at disadvantage in the niche domains or tasks like sequence labeling that are less straightforward to formulate as a text generation task.Consider that few-shot GPT-3 (Brown et al., 2020) is on the SuperGLUE (Wang et al., 2019) leaderboard with the average score of 71.8, compared to a score of 84.6 achieved by a fine-tuned RoBERTa model (Liu et al., 2019).As another example, recent work on NER (Wang et al., 2023) and relation extraction (Wan et al., 2023) explicitly formulated their problem as few-shot learning generally trailing behind supervised approaches in their tasks of interest, and their contribution -as overcoming that (in both cases, with the help of a supervised method in the pipeline).OpenAI (2023) claimed that GPT-4 outperforms unspecified fine-tuned models on 6 out of 7 verbal reasoning tasks, but provided no detail on model or benchmark selection, making it impossible to reproduce or verify these results.</p>
<p>One more consideration for the "(few-shot) LLMs are SOTA" statement is that it implies a direct competition with other methods, with which a meaningful comparison is possible.But what are we comparing -the model architectures or training data?ML as a scientific field focuses on the former, but most LLM leaderboards present apple-to-orange comparisons.</p>
<p>Finally, for both few-shot and fine-tuned evaluation of LLMs, most of the reported results should be taken with a grain of salt because of test data contamination.For example, GPT-4 received a lot of press coverage due to the claim of achieving a score that falls in the top 10% of test takers on a simulated bar exam (Katz et al., 2023) -but that result was soon questioned on grounds of improper evaluation and possible data contamination (Martínez, 2023).</p>
<p>In the GPT-3 report, OpenAI itself documented how hard it is to avoid benchmark contamination (Brown et al., 2020).By now, multiple studies presented evidence of the presence of common NLP benchmarks in multiple datasets used for training LLMs (Dodge et al., 2021;Magar &amp; Schwartz, 2022;Blevins &amp; Zettlemoyer, 2022), which can inflate LLM performance in certain tasks and datasets.The LM Contamination Index5 , a collaborative effort to document benchmark contamination, currently has 375 entries for various benchmarks and models across different tasks.Furthermore, a recent study has documented the effect where GPTs score higher on the "old" benchmarks than on the new ones (Liu et al., 2023a), which strongly suggests that the previously reported evaluation results may be inflated.</p>
<p>Claim: (LLM) Scale Is All You Need</p>
<p>Scaling has played a central role in the success of LLMsstarting with the 'scaling laws' paper for causal language models (Kaplan et al., 2020), which found that their performance improves with scaling the model size, data size, and also the amount of compute used for training.This analysis was subsequently expanded to other benchmarks, modalities and downstream tasks (Ghorbani et al., 2021;Alabdulmohsin et al., 2022;Hernandez et al., 2021;Hoffmann et al., 2022).It is often mentioned as a key factor 6 in LLM performance; for instance, Huang et al. (2023b, p.1) state: "scaling has enabled Large Language Models (LLMs) to achieve state-of-the-art performance on a range of Natural Language Processing (NLP) tasks".The focus on scaling is in line with the "bitter lesson" of Sutton (2019), which states that we should stop working on methods based on the human knowledge of the target problem, and embrace "search and learning", because "the only thing that matters in the long run is the leveraging of computation".Indeed, the scaling hypothesis seems to be supported by the fact that LLMs have been growing in size for several years: e.g.BERT-base in 2018 had 340M parameters (Devlin et al., 2019), and in 2022 PaLM had 540B parameters in 2022 (Chowdhery et al., 2022).And there is evidence that, even with the same architecture and training data, larger models tend to perform better, even with adversarial evaluation (Bhargava et al., 2021;Ray Choudhury et al., 2022;Wang et al., 2022b).</p>
<p>However, it is important to keep in mind that many of the best-known LLMs scaled both the number of parameters and their training data concurrently (besides any differences in architecture and training set-up)7 .While this seems to support the scaling laws hypothesis as formulated by Kaplan et al. ( 2020)-we do not know which of these components is most responsible for the improvement, and most LLMs are not directly comparable by more than one of these criteria.Furthermore, simply increasing the size of the training dataset has a complex relation with its quality.When we train LLMs on cleaner, more diverse text data, we do not merely provide more data, but more knowledge (by better covering the kinds of information that may be required for performing the model's task).Then, the improved performance likely results from the fact that we are supplying more knowledge, and not just more scale/computation, and it is bounded by availability of such knowledge.Unlike in chess and Go, for LLMs the new knowledge has to come from manually created sources, which do not cover all scenarios that may arise in practice, and hence computation can only take us as far as the data goes. 8In practice, we break the central tenet of the "bitter lesson" all the time: when we identify an area where a model under-performs, a common solution is to try to collect, label, synthesize, or prestigious kind of work (Sambasivan et al., 2021).even create more data for that problem, which is tantamount to injecting manually-curated knowledge.</p>
<p>This would be in line with the fact that the developers of high-performing LLMs now spend more effort on improving the quality of the training data.E.g. both PaLM reports (Chowdhery et al., 2022;Anil et al., 2023) devote considerable effort to data cleaning, with PaLM 2 explicitly attributing higher performance on English benchmarks to higher quality data (Anil et al., 2023, p.9); the paper accompanying the Chinchilla model also makes a similar "quality over quantity" point with regards to data (Hoffmann et al., 2022).The Llama 3 announcement9 stresses a heavy investment in pre-training data.The key result of Phi model (Gunasekar et al., 2023) is also explicitly presented as a smaller high-performing model, made possible by training on higher-quality data.</p>
<p>Furthermore, the last few years have seen an increased skepticism around the scaling hypothesis, starting with 'efficient scaling' proposals for Transformer models, which showed that smaller, more efficient models can outperform bigger ones in certain settings (Tay et al., 2021).This was further explored in practice via the Inverse Scaling Prize, showing that there are tasks, such as logical reasoning and pattern matching, where the performance does not seem to improve with model size (McKenzie et al., 2022).</p>
<p>Finally, let us consider the fact that there are highperforming open-access models such as LLaMa series (Touvron et al., 2023a;b), which perform very well despite being much smaller than GPT-3.The success of techniques such as knowledge distillation (Pan et al., 2020;Sanh et al., 2019) and sparsity (Srinivasan et al., 2023) also strongly suggests at least that the model size by itself is not the 'secret sauce'.And, of course, it can be a deal-breaker for deploying models in production, irrespective of performance gains.</p>
<p>Claim: LLMs Are General-Purpose Technologies</p>
<p>According to Eloundou et al. (2023, p.3), Generative Pretrained Transformers (GPTs) are general-purpose technologies (GPTs).This framing can be found both in the media (Kuttan, 2023;McKendrick, 2023) and preprints (Tamkin et al., 2021;Liu et al., 2023b).10GPT (General-Purpose Technology) is a term used by economists and historians to refer to technologies that are both era-defining and pervasive over time; however, what qualifies as a GPT and what does not has been hard to delineate, since technologies are often nested within other systems of recursive technologies and systems (Knell &amp; Vannuccini, 2022).A widely-accepted definition by economists Lipsey and Carlaw proposes 4 criteria for a technology to be considered general-purpose: (1) it is a single, recognisable generic technology, (2) it comes to be widely used across the economy, (3) it has many different uses and (4) it creates many spillover effects (Lipsey et al., 2005).According to these criteria, a total of 24 technologies such as the wheel, the printing press and electricity are considered GPTs.</p>
<p>At this point, it is hard to assert the general-purpose status of LLMs by the above criteria.They are not a single, generic technology (as discussed in §2).They are currently not widely used in different domains (Bekar et al., 2018;Prytkova, 2021), and remain auxiliary tools even in those domains where they are most used (Bianchini et al., 2020).In terms of it usage, LLMs are not widely used in the vast majority of economic activities, and they are reliant upon specific commodities such as large amounts of GPUs and highly specialized labor, which are only available in a small number of industries and by a handful of organizations (Bresnahan, 2019).A further constraint is the fact that LLMs, like all deep learning-based technology, can be expected to work best in-distribution -and it is unclear to what extent issues with robustness ( §3.1) will limit its broader utility in the vast number of economic sectors associated with the smaller communities and languages.</p>
<p>As for the final criterion, regarding the potential spillover effects of these technologies, it is really too early to tell what these may be (Bresnahan, 2019;Crafts, 2021;Natale &amp; Ballatore, 2020).The overnight popularity of ChatGPT, which is often seen as proof of the widespread usage of LLMs in broader society, can mainly be attributed to the creation of a simple user interface on top of models that had been previously available via APIs (Eloundou et al., 2023), rather than technological novelty.Until we can meaningfully assess the adoption and application of LLMs, we should refrain from putting them in the same conceptual category as the printing press, and focus on defining what they can and cannot be used for (and under what conditions).</p>
<p>Claim: LLMs Exhibit "Emergent Properties"</p>
<p>LLMs are often discussed in terms of their "emergent properties".11Among such properties, various researchers have included few-shot learning (Bommasani et al., 2021), "aug-mented prompting" techniques such as "chain of thought" (Wei et al., 2022b), predicting intermediate computation results (Nye et al., 2021) or whether the answer is correct (Kadavath et al., 2022), and, on the input side -instruction following (Ouyang et al., 2022;Wei et al., 2021).</p>
<p>In line with the confusion about the term "LLM" ( §2), the term "emergent properties" seems to be used in at least 4 distinct ways in current LLM research: Definition 3.1.A property that a model exhibits despite not being explicitly trained for it.E.g.Bommasani et al. (2021, p.5) refers to few-shot performance of GPT-3 (Brown et al., 2020) as "an emergent property that was neither specifically trained for nor anticipated to arise".Definition 3.2.(Opposite to Definition 3.1): a property that the model learned from the pre-training data.E.g.Deshpande et al. (2023, p.8) discuss emergence as evidence of "the advantages of pre-training".Definition 3.3.A property "is emergent if it is not present in smaller models but is present in larger models."(Wei et al., 2022a, p.2).</p>
<p>Definition 3.4.A version of Definition 3.3, where what makes emergent properties "intriguing" is "their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales" (Schaeffer et al., 2023, p.1) Definition 3.2 seems describe the expected outcome of successful training.In this sense, "emergent properties" could be referred to simply as "learned properties".Definition 3.3 is questionable if the "emergent" behavior can be achieved in smaller models12 .Even more importantly, we still have the contamination problem: if both the bigger and smaller models were trained on data similar to the test data, the bigger one would still be reasonably expected to perform better just because it has more capacity to learn it.In that case, Definition 3.3 follows from the Definition 3.2.</p>
<p>With respect to Definition 3.4, Schaeffer et al. ( 2023) make a convincing case that such sharp increases in performance may be an artifact of the chosen evaluation metric13 rather than a fundamental property of scaling the model.</p>
<p>An even bigger issue with Definition 3.4 is that we simply do not have enough data points to say that the increase in performance is sharp: e.g. if we had intermediate model sizes between the commonly-used 13B, 70B, and 150+B, we would likely see a smooth transition.Wei (2023) acknowledges that, but argues that the "emergence" phenomenon is still interesting if there are large differences in predictability: for some problems, performance of large models can easily be extrapolated from performance of models 1000x less in size, whereas for others, even it cannot be extrapolated even from 2x less size.But the cited predictability at 1,000x less compute refers to the GPT-4 report (OpenAI, 2023), where the developers knew the target evaluation in advance, and specifically optimized for "predictable scaling".This is in contrast with the unpredictability at 2x less compute for unplanned BIG-Bench evaluation by Wei et al. (2022a).</p>
<p>So, we are left with Definition 3.1, which can be interpreted in two ways: Definition 3.5.A property is emergent if the model was not exposed to training data for that property.</p>
<p>Definition 3.6.A property is emergent even if the model was explicitly trained for it -as long as the developers were unaware of it.</p>
<p>Per Definition 3.6, it would appear that we are training LLMs as a very expensive method to discover what data exists on the Web.For example, the fact that ChatGPT can generate chess moves that are plausible-looking (but often illegal) 14 is to be expected, given the vast amount of publicly-available chess transcripts on the Web.Per Definition 3.5, we can prove that some property is emergent only by showing that there was no evidence that could have been the basis for the model outputs in the training data.For commercial models with undisclosed data such as ChatGPT, this is out of the question.But we would go further and argue that the emergent properties according to Definition 3.5 are only a hypothesis (if not wishful thinking) even for the "open" LLMs, because so far we are lacking detailed studies (or even a methodology) to consider the exact relation between the amount and kinds of evidence in the training text data for a particular model output.Hence, to the best of our knowledge, there is no evidence for the existence of "emergent properties" per Definition 3.5.Until we have such evidence, it seems strange for the ML community to conclude that the best explanation for high performance is not-learned-from-data emergent propertiesespecially in the face of evidence to the contrary. 1514 https://reddit.com/r/AnarchyChess/comments/10ydnbb/i_placed_stockfish_white_against_chatgpt_black/ 15 E.g.Liang et al. (2022, p.12), evaluate 30 LLMs and conclude that "regurgitation (of copyrighted materials) risk clearly correlates with model accuracy".Liu et al. (2023a) report that ChatGPT and GPT-4 perform better on older compared to newly released What about benchmarks that are newly created and tested on for the first time in a given study -aren't they, by definition, uncontaminated?We argue not: in the absence of a methodology to even compare test and training data beyond trivial exact matches, "new" tests may be so similar to something observed that they do not really count as "new".This has been a known problem even with benchmark datasets, the size of which is very small compared to LLM training data 16 .The LLM training data may also include something that is not even public 17 on the internet, and thus not easily searchable to confirm the contamination.Perhaps the most striking example of how unreliable the "new" test examples are is the "sparks of intelligence" study (Bubeck et al., 2023).Using the methodology of newly constructed test cases, checked against public web data, and their perturbations, Bubeck et al. (2023) notably concluded that GPT-4 possesses "a very advanced theory of mind".At least two studies have since come to the opposite conclusion (Sap et al., 2023;Shapira et al., 2023).</p>
<p>The above discussion focused on the way the term "emergence" is currently used in LLM research.In philosophy of science, there are many nuanced discussions of emergence to which we cannot do justice here, but broadly it can be characterized as a phenomenon in complex systems that is dependent on its constituent parts, but is also distinct from them.The leading example in Stanford Encyclopedia of Philosophy is a tornado: it consists of dust and debris, but it "its features and behaviors appear to differ in kind from those of its most basic constituents"(O'Connor &amp; Wong, 2020).Emergent phenomena are described by a different scientific field, on a different level than the constituent phenomena: it is possible to understand the behavior of tornadoes without understanding particle physics.Does this notion of emergence apply in the context of ML?</p>
<p>Our take is that such a notion of emergence would hold for LLMs: the information they encode is not explainable by the model weights, at least at the current state of interpretability research.However, this also applies to most deep learning models (e.g. a simple MLP for sentiment classification), and the latest LLMs are not something special.benchmarks, and McCoy et al. (2023) show that their performance depends on probabilities of output word sequences in web texts.Lu et al. (2023) show that "emergent abilities" of 18 LLMs can be ascribed mostly to in-context learning.For in-context learning itself, the results of Chan et al. (2022) suggest that it happens only in Transformers trained on sequences, structurally similar to the sequences in which in-context learning would be tested.</p>
<p>16 E.g.Lewis et al. (2021) found that about 30% of test samples in 3 popular QA benchmarks have near-duplicates in train data. 17In particular, the OpenAI models could have been trained not only on Web data, but also on the data of thousands of researchers who over the past year submitted their trickiest test cases to GPT-3 API (the policy for the API data to be opted out of training by default only changed in Spring 2023).</p>
<p>How Does This Change the Theory and</p>
<p>Practice of ML?</p>
<p>The success of LLMs brought our field an increase in funding, real-world applications, and attention.But the perception of their robustness ( §3.1) and SOTA status ( §3.2), the idea that their success is purely due to scale ( §3.3), that they are a general-purpose technology ( §3.4), and their illdefined "emergent properties" ( §3.5) also contribute to the following trends:</p>
<p>• Homogeneity of approaches.If LLMs are so great, why would we pursue anything else?It is understandable that most current NLP research is focused on LLMs, but this also means that as researchers we have most of our eggs in one basket, and become less likely to develop alternatives or see the gaps in our knowledge.• De-democratization.Knight (2023) estimates that training GPT-4 cost more than $100 million; although the true number is unknown, the increase in computational requirements of LLMs is clear (Thompson et al., 2022).This means that graduate students, independent researchers, and even most academic labs struggle to either reproduce existing results or train new LLMs that would require large amounts of compute.• Industry influence.Recent research into the affiliations of researchers in ML (Abdalla &amp; Abdalla, 2021;Ahmed &amp; Wahed, 2020;Birhane et al., 2022;Whittaker, 2021) has shown both a steep increase in industry presence in conference publications over the past years (e.g. a 180% growth for NLP (Abdalla et al., 2023)), and an influence over the topics of research being pursued by the community, such as robustness and similar challenges, instead of more theory-oriented topics.Given the above point, this trend makes sense, since only industry labs with extensive funding can afford to train and deploy LLMs.But it has implications for the diversity of ideas and approaches in the field.• (Further) decreased reproducibility.Reproducibility was an issue in ML even before LLMs (Crane, 2018;Cohen et al., 2018;Bouthillier et al., 2019), and it has not gotten much better (Belz et al., 2021;2022;Belz, 2022).LLMs pose additional issues both in reproducing their training and fine-tuning (Sellam et al., 2021;McCoy et al., 2020) and inference results (Hagmann et al., 2023), not to mention the lack of control over API-only models that can change over time (Chen et al., 2023;Rogers, 2023).If the tested model is deprecated, or changes in any way (e.g. by extra training or finetuning, changes in its associated parameters, filtering mechanisms or system prompts), the results reported in the study will no longer be reproducible.They might not even hold at the time of publication -and we will have no idea why.</p>
<p>The ideas that LLMs are robust ( §3.1) and exhibit emergent properties ( §3.5) further contribute to the impression of irrelevance of any theory of the linguistic, social, cognitive, or any other phenomena that LLMs are supposed to model.This is unsatisfactory if the goal is scientific research, but even from a purely engineering perspective this stance is dangerous, since it entails that we either cannot or do not need to provide specifications for cases where a given model is safe or unsafe to use.As a result, LLMs may be deployed in society at large, without compelling evidence of their performance in all the target scenarios, or among the demographic groups that are likely underrepresented in the training data (Bender et al., 2021), or across the lessresourced languages (Zhu et al., 2023;Bang et al., 2023;Lai et al., 2023;Huang et al., 2023a;Ziems et al., 2023).</p>
<p>Ways Forward</p>
<p>We have argued that there are many deep knowledge gaps in LLM research, and the field is changing in ways that make these gaps less likely to be addressed.We now conclude with some concrete recommendations for future work.</p>
<p>Maintaining diversity of research approaches.We advocate not for stopping all work on LLMs, but for maintaining a healthy diversity of approaches and tasks.Among other things, this means efforts by the conferences to ensure fair reviews for the "niche" submissions. 18Putting all of our eggs in the proverbial LLM basket runs the risk of missing out on new research directions and exciting opportunities to make significant connections with other fields, such as linguistics and cognitive science.</p>
<p>Defining terminology.We discussed the lack of clarity on the very term "large language model" ( §2), as well as "emergent properties" ( §3.5Not using "closed" models as baselines.Since the launch of GPT-4 in March 2023, numerous studies have used it both as a benchmark to compare different methods and models (e.g.Liu et al., 2023a;Sun et al., 2023) as well as an object of scientific study in itself (e.g.Bubeck et al., 2023;Wei et al., 2022a;Zhang et al., 2023).We believe that this is problematic for several reasons:</p>
<p>• It may produce inflated performance reports due to undisclosed training data, which could be contaminated with benchmark data.This could then create the false impression that the "closed" model is intrinsically so much better that there is no competing with it.• It normalizes methodologically dubious apples-tooranges comparisons to black box models.• The reproducibility of evaluations is significantly reduced (see §4). • Where the "closed" models are provided commercially, academic researchers essentially perform free work to help the company improve their product -and they even pay 19 for that privilege, often with funding from public grants that could be spent on improving the public resources.Moreover, if the general perception is that only this kind of work constitutes "frontier" LLM research, the labs without the financial means to pay for the API access are at severe disadvantage, and the field gets homogenized even further.</p>
<p>While we recognize that some analyses of proprietary, "closed" models like GPT-4 and PaLM can be useful (e.g.audits, red-teaming), relying on these models as baselines, and especially expecting others to do so, is both unfair and unreliable.We recommend using open-source or at least open-access 20 models like FLAN-T5 (Chung et al., 2022), BLOOM (Scao et al., 2022), LLaMa (Touvron et al., 2023a;b) and FALCON (Almazrouei et al., 2023).</p>
<p>Further rigorous studies of LLM functionality.There are numerous knowledge gaps about LLM fuctionality.What exactly makes a given type of model (e.g. a Transformerbased LLM) SOTA on a given task, given that there are no confounds such as differences in training data and model capacity?What kinds of brittleness do LLMs exhibit, and how to mitigate that?Do they really have any emergent properties (and how are these defined)?How can we ensure specific kinds of robustness?Large-scale benchmarks (e.g.Srivastava et al., 2023) do not address these questions, 19 According to one US academic researcher we spoke to, the monthly spending on OpenAI API in their lab is capped at $10K a month, and otherwise would sometimes even exceed that sum. 20We do not advocate for using only open-source models, because the models available now can be "open" in different relevant ways (code, weights, license, training data), and these aspects matter more/less for different research questions and real-world applications (Solaiman, 2023).A model like Mistral (Jiang et al., 2023) is more transparent than GPT-4 in terms of its architecture, but not the training data.A model like BLOOM (Le Scao et al., 2023) is more transparent in terms of training data and can be an excellent object of research, but it is not strictly open-source, e.g. because of its RAIL license.Our position is that in research, we should aim to use the most transparent options available, and thankfully there seems to be a trend towards more openness in this sense (e.g. the recent OLMO model (Groeneveld et al., 2024) and its training dataset DOLMa (Soldaini et al., 2024)).</p>
<p>since they are constructed on the basis of data availability rather than definitions of specific phenomena (Raji et al., 2021).These questions are not answered by the current large-scale evaluation endeavors (e.g.Liang et al., 2022).While they are very valuable, there are too many confounding variables in the published models, and more extensive error analysis and exploration is needed to disentangle them.Access to both models and their training data is especially important for this purpose, since it can help establish links between LLM behavior and their training data (Piktus et al., 2023) and understand their biases (Gururangan et al., 2022;Johnson et al., 2022;Abid et al., 2021).</p>
<p>Developing better evaluation methodology.There has been progress towards multi-dimensional evaluation that takes into account more than performance (Ethayarajh &amp; Jurafsky, 2020;Liang et al., 2022;Chung et al., 2023) and reproducibility (Dodge et al., 2019;Ulmer et al., 2022;Magnusson et al., 2023), but much more remains to be done.There is also a dire need for structural incentives to encourage reproducibility efforts and publication of negative results.And most importantly, we need a lot more work on the validity of the underlying constructs (Raji et al., 2021;Schlangen, 2021).Ideally, LLM evaluation would target specific types of language processing rather than broadly defined tasks (Ribeiro et al., 2020;Rogers et al., 2023a).It would carefully explore the decision boundaries and specific kinds of brittleness (Kaushik et al., 2019;Gardner et al., 2020), and control for potential confounds so as to focus purely on model architecture.As discussed in §3.5, we also need new methodology for systematically linking the model outputs to potential evidence in the training data at LLM scalerelevant current efforts include the work on memorization (Thakkar et al., 2021;Carlini et al., 2022;Chang et al., 2023, inter alia) and providing search indices for LLM training data (Piktus et al., 2023;Marone &amp; Van Durme, 2023).All this is in parallel to the general problems with evaluating open-ended generation, including the dependence on generation strategies (Meister &amp; Cotterell, 2021), and issues with popular metrics such as perplexity (Wang et al., 2022a).</p>
<p>Conclusion</p>
<p>As researchers, we cannot help but observe the nearuniversal focus on LLMs in recent years and their impact on our field.This paper discusses several knowledge gaps and common claims that should be taken with a grain of salt, as well as the ways in which LLMs changed the research landscape.We argue for more rigor in definitions, experimental studies, and evaluation methodology, as well as higher standards for transparency and reproducibility.We hope to open the door for more discussion of the contribution of LLMs to ML research, and how we can better leverage their strengths while understanding their limitations.
 We understand 'text' as 'a unit of language in use'(Halliday  &amp; Hasan,<br />
), the product of using that language. We see this as a more accurate description, because LLMs model the corpora they are trained on, rather than language in general(Veres, 2022).2 Most current LLMs produce text in some way, and so our definition focuses on these. To extend it to energy-based models or discriminative models like ELECTRA(Clark et al., 2020), we could say 'can be used to generate or score text'.3 While BERT-style(Devlin et al., 2019) encoders could be used to generate text, it is admittedly a tortuous way of doing so, compared to autoregressive models. However, in both cases we fundamentally solve a classification problem over the model's vocabulary, and autoregressive models could be viewed as a special case of masked language models.
https://hitz-zentroa.github.io/lmcontamination/
  6  To be fair, the focus on scaling does not entail that other factors are completely irrelevant, and we are not saying that the entire ML community believes that "scale is all you need". But it is also fair to say that scaling has received a lot more attention than other factors, in particular data, which has long been considered as a less
E.g. BERT was trained on roughly 3.3 billion tokens, and PaLM's training data had 780 billion, representing a growth of 1500 times in terms of model size and 260 times in terms of dataset size. This came with a big improvement in performance: BERT achieved an accuracy of 70.1% on the RTE dataset, PaLM achieves an accuracy of 95.7%, with possible data contamination.
This is not to say that such a system cannot be useful in some cases, or that it cannot exhibit some generalization within a space sufficiently covered by data: the linguistic fluency of the current LLMs is a testament to the possibility of learning "closed" systems such as syntax.
https://ai.meta.com/blog/meta-llama-3/
The preprints byEloundou et al. (2023);Liu et al. (2023b) were co-authored by researchers affiliated with OpenAI.
Some researchers discuss "emergent abilities" rather than "properties". Both of these terms could also use better definitions. Our interpretation is that they are used interchangeably in LLM research, and the chief difference is the anthropomorphizing framing for "ability". The underlying construct in case ofWei et al. (2022a) seems to be NLP "tasks", on the assumptions that these tasks have construct validity, and specific evaluation datasets provide valid measurements of performance on these tasks. The task/benchmark confusion is exacerbated by the fact that in BIG-Bench(Srivastava et al., 2023) the constituent datasets are sometimes named as if they were tasks (e.g. "IPA transliterate").
See e.g.Schick &amp; Schütze (2021);Gao et al. (2021). One could object that these studies provide the smaller models a lot of extra help (reformatting the inputs, selecting extra models etc.) This is true, but the commonly reported few-shot performance is also not really few-shot, and reliably choosing good prompts becomes harder with larger models (presumably due to more complex decision boundaries)(Perez et al., 2021).
Wei (2023) andAnderljung et al. (2023, p.38)  argue that this is not important, because "non-smooth" metrics used for real tasks are the ones we care about. But then the question remains whether this is something special about LLMs, or just a property of the metric. Simple models also have such "emergent properties", as shown bySchaeffer et al. (2023) for a shallow nonlinear autoencoder.
E.g. papers on "niche" topics can have priority in reviewer assignments(Rogers et al., 2023b).
AcknowledgementsWe would like to thank all anonymous reviewers of this paper.Their insightful comments were invaluable for sharpening the discussion.Rob van der Goot, Christian Hardmeier, Yacine Jernite, Margaret Mitchell, Dennis Ulmer read the early versions of this paper and provided feedback.We also thank Ryan Cotterell, Ishita Dasgupta, Laura Gwilliams, Julia Haas, Anna Ivanova, Tal Linzen, Ben Lipkin, Asad Sayeed for their insights and discussion.This work was partly supported by DFF Inge Lehmann grant to Anna Rogers (3160-00022B).Impact StatementThis work is a position paper, expressing the personal views of its authors, and supported by evidence and arguments we cite rather than new experimental work.Our goal is not to criticize a specific research direction or technology as a whole, but to raise the awareness about the issues with the lack of clarity with key terms and claims in our field.We recognize that we come from a position of privilege, holding positions at institutions located in the Global North.Our opinions do not reflect the lived experiences of visible minorities or those who come from less privileged institutions and geographical regions.We have endeavored to have our manuscript read by colleagues from other domains of expertise and other institutions, but we recognize that it does not represent the experiences and perspectives of all members of the ML community.
The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. M Abdalla, M Abdalla, Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. the 2021 AAAI/ACM Conference on AI, Ethics, and Society2021</p>
<p>M Abdalla, J P Wahle, T Ruas, A Névéol, F Ducel, S M Mohammad, K Fort, arXiv:2305.02797The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research. 2023arXiv preprint</p>
<p>Persistent anti-Muslim bias in Large Language Models. A Abid, M Farooqi, J Zou, Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. the 2021 AAAI/ACM Conference on AI, Ethics, and Society2021</p>
<p>The de-democratization of AI: Deep learning and the compute divide in artificial intelligence research. N Ahmed, M Wahed, arXiv:2010.155812020arXiv preprint</p>
<p>Revisiting neural scaling laws in language and vision. I M Alabdulmohsin, B Neyshabur, X Zhai, Advances in Neural Information Processing Systems. 202235</p>
<p>Falcon-40B: an open large language model with stateof-the-art performance. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, G Penedo, arxiv2023</p>
<p>Z Alyafeai, M S Alshaibani, I Ahmad, arXiv:2007.04239A Survey on Transfer Learning in Natural Language Processing. May 2020cs, stat</p>
<p>M Anderljung, J Barnhart, A Korinek, J Leung, C O'keefe, J Whittlestone, S Avin, M Brundage, J Bullock, D Cass-Beggs, B Chang, T Collins, T Fist, G Hadfield, A Hayes, L Ho, S Hooker, E Horvitz, N Kolt, J Schuett, Y Shavit, D Siddarth, R Trager, K Wolf, Frontier AI Regulation: Managing Emerging Risks to Public Safety. November 2023</p>
<p>. R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, E Chu, J H Clark, L E Shafey, Y Huang, K Meier-Hellstern, G Mishra, E Moreira, M Omernick, K Robinson, S Ruder, Y Tay, K Xiao, Y Xu, Y Zhang, G H Abrego, J Ahn, J Austin, P Barham, J Botha, J Bradbury, S Brahma, K Brooks, M Catasta, Y Cheng, C Cherry, C A Choquette-Choo, A Chowdhery, C Crepy, S Dave, M Dehghani, S Dev, J Devlin, M Díaz, N Du, E Dyer, V Feinberg, F Feng, V Fienber, M Freitag, X Garcia, S Gehrmann, L Gonzalez, G Gur-Ari, S Hand, H Hashemi, L Hou, J Howland, A Hu, J Hui, J Hurwitz, M Isard, A Ittycheriah, M Jagielski, W Jia, K Kenealy, M Krikun, S Kudugunta, C Lan, K Lee, B Lee, E Li, M Li, W Li, Y Li, J Li, H Lim, H Lin, Z Liu, F Liu, M Maggioni, A Mahendru, J Maynez, V Misra, M Moussalem, Z Nado, J Nham, E Ni, A Nystrom, A Parrish, M Pellat, M Polacek, A Polozov, R Pope, S Qiao, E Reif, B Richter, P Riley, A C Ros, A Roy, B Saeta, R Samuel, R Shelby, A Slone, D Smilkov, D R So, D Sohn, S Tokumine, D Valter, V Vasudevan, K Vodrahalli, X Wang, P Wang, Z Wang, T Wang, J Wieting, Y Wu, K Xu, Y Xu, L Xue, P Yin, J Yu, Q Zhang, S Zheng, C Zheng, W Zhou, D Zhou, S Petrov, Y Wu, 2. 2023Technical Report</p>
<p>. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning. 2023Interactivity</p>
<p>General purpose technologies in theory, application and controversy: a review. C Bekar, K Carlaw, R Lipsey, Journal of Evolutionary Economics. 282018</p>
<p>SciBERT: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, arXiv:1903.106762019arXiv preprint</p>
<p>A metrological perspective on reproducibility in NLP. A Belz, Computational Linguistics. 4842022</p>
<p>A Belz, S Agarwal, A Shimorina, E Reiter, arXiv:2103.07929A systematic review of reproducibility research in Natural Language Processing. 2021arXiv preprint</p>
<p>A Belz, M Popović, S Mille, arXiv:2204.05961Quantified reproducibility assessment of nlp results. 2022arXiv preprint</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021</p>
<p>Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics. P Bhargava, A Drozd, A Rogers, Proceedings of the Second Workshop on Insights from Negative Results in NLP. the Second Workshop on Insights from Negative Results in NLPDominican RepublicAssociation for Computational Linguisticsnov 2021Online and Punta Cana</p>
<p>S Bianchini, M Müller, P Pelletier, arXiv:2009.01575Deep learning in science. 2020arXiv preprint</p>
<p>The values encoded in machine learning research. A Birhane, P Kalluri, D Card, W Agnew, R Dotan, M Bao, 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022</p>
<p>Language contamination helps explains the cross-lingual capabilities of english pretrained models. T Blevins, L Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Unreproducible research is reproducible. X Bouthillier, C Laurent, P Vincent, PMLR, 09-15Proceedings of the 36th International Conference on Machine Learning. K Chaudhuri, R Salakhutdinov, the 36th International Conference on Machine LearningJun 201997</p>
<p>Shortcutted commonsense: Data spuriousness in deep learning of commonsense reasoning. R Branco, A Branco, J Rodrigues, J Silva, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Artificial intelligence technologies and aggregate growth prospects. Prospects for Economic Growth in the United States. T Bresnahan, 2019</p>
<p>Language Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. NeurIPS 2020. June 202033</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023arXiv preprint</p>
<p>Quantifying memorization across neural language models. N Carlini, D Ippolito, M Jagielski, K Lee, F Tramer, C Zhang, arXiv:2202.076462022arXiv preprint</p>
<p>Data Distributional Properties Drive Emergent In-Context Learning in Transformers. S Chan, A Santoro, A Lampinen, J Wang, A Singh, P Richemond, J Mcclelland, F Hill, Advances in Neural Information Processing Systems. December 202235</p>
<p>K K Chang, M Cramer, S Soni, D Bamman, Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4. 2023</p>
<p>Language model behavior: A comprehensive survey. T A Chang, B K Bergen, arXiv:2303.115042023arXiv preprint</p>
<p>One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling. C Chelba, T Mikolov, M Schuster, Q Ge, T Brants, P Koehn, T Robinson, 2013GoogleTechnical report</p>
<p>Position: Key Claims in LLM Research Have a Long Tail of Footnotes Chollet. L Chen, M Zaharia, J Zou, arXiv:2307.09009arXiv:1911.01547F. On the Measure of Intelligence. 2023. November 2019arXiv preprintHow is ChatGPT's behavior changing over time?</p>
<p>S R Choudhury, A Rogers, I Machine Augenstein, Reading, arXiv:2209.07430Fast and Slow: When Do Models" Understand" Language?. 2022arXiv preprint</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, A Webson, S S Gu, Z Dai, M Suzgun, X Chen, A Chowdhery, S Narang, G Mishra, A Yu, V Zhao, Y Huang, A Dai, H Yu, S Petrov, E H Chi, J Dean, J Devlin, A Roberts, D Zhou, Q V Le, Wei , J. Scaling Instruction-Finetuned Language Models. 2022</p>
<p>. J.-W Chung, J Liu, Z Wu, Y Xia, M Ml Chowdhury, Leaderboard, 2023</p>
<p>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. K Clark, M.-T Luong, Q V Le, C D Manning, International Conference on Learning Representations. 2020</p>
<p>Three dimensions of reproducibility in natural language processing. K B Cohen, J Xia, P Zweigenbaum, T Callahan, O Hargraves, F Goss, N Ide, A Névéol, C Grouin, L Hunter, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)2018</p>
<p>Artificial intelligence as a general-purpose technology: an historical perspective. N Crafts, Oxford Review of Economic Policy. 3732021</p>
<p>Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results. M Crane, 10.1162/tacl_a_00018Transactions of the Association for Computational Linguistics. 62018</p>
<p>Honey, I shrunk the language: Language model behavior at reduced scale. V Deshpande, D Pechi, S Thatte, V Lialin, A Rumshisky, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 20191</p>
<p>Show Your Work: Improved Reporting of Experimental Results. J Dodge, S Gururangan, D Card, R Schwartz, N A Smith, 10.18653/v1/D19-1224Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. J Dodge, M Sap, A Marasović, W Agnew, G Ilharco, D Groeneveld, M Mitchell, M Gardner, 2021</p>
<p>T Eloundou, S Manning, P Mishkin, D Rock, arXiv:2303.10130GPTs are GPTs: An early look at the labor market impact potential of large language models. 2023arXiv preprint</p>
<p>Emnlp 2023 call for main conference papers theme track: Large language models and the future of nlp. EMNLP. 2023</p>
<p>Utility is in the eye of the user: A critique of nlp leaderboards. K Ethayarajh, D Jurafsky, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)OnlineNovember 2020Association for Computational Linguistics</p>
<p>Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks. C Gan, T Mori, June 2023</p>
<p>Making pre-trained language models better few-shot learners. T Gao, A Fisch, D Chen, doi: 10.18653Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 2021</p>
<p>M Gardner, Y Artzi, V Basmova, J Berant, B Bogin, S Chen, P Dasigi, D Dua, Y Elazar, A Gottumukkala, N Gupta, H Hajishirzi, G Ilharco, D Khashabi, K Lin, J Liu, N F Liu, P Mulcaire, Q Ning, S Singh, N A Smith, S Subramanian, R Tsarfaty, E Wallace, A Zhang, B Zhou, Evaluating NLP Models via Contrast Sets. April 2020</p>
<p>M Gardner, W Merrill, J Dodge, M E Peters, A Ross, S Singh, N A Smith, arXiv:2104.08646Competency problems: On finding and removing artifacts in language data. 2021arXiv preprint</p>
<p>Scaling laws for neural machine translation. B Ghorbani, O Firat, M Freitag, A Bapna, M Krikun, X Garcia, C Chelba, C Cherry, arXiv:2109.077402021arXiv preprint</p>
<p>Overview of the Transformer-based Models for NLP Tasks. A Gillioz, J Casas, E Mugellini, O Abou Khaled, 2020 15th Conference on Computer Science and Information Systems (FedCSIS). IEEE2020</p>
<p>Is the world ready for ChatGPT therapists?. I Graber-Stiehl, Nature. 61779592023</p>
<p>D Groeneveld, I Beltagy, P Walsh, A Bhagia, R Kinney, O Tafjord, A H Jha, H Ivison, I Magnusson, Y Wang, arXiv:2402.00838Accelerating the science of language models. 2024arXiv preprint</p>
<p>Textbooks are all you need. S Gunasekar, Y Zhang, J Aneja, C C T Mendes, A D Giorno, S Gopi, M Javaheripi, P Kauffmann, G De Rosa, O Saarikivi, A Salim, S Shah, H S Behl, X Wang, S Bubeck, R Eldan, A T Kalai, Y T Lee, Y Li, 2023</p>
<p>S Gururangan, D Card, S K Dreier, E K Gade, L Z Wang, Z Wang, L Zettlemoyer, N A Smith, arXiv:2201.10474Whose language counts as high quality? measuring language ideologies in text data selection. 2022arXiv preprint</p>
<p>Towards inferential reproducibility of machine learning research. M Hagmann, P Meier, S Riezler, 2023</p>
<p>M Halliday, R Hasan, 10.4324/9781315836010Cohesion in English. 2013Routledge, London, 0 edition</p>
<p>Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine. S Harrer, 2023EBioMedicine90</p>
<p>Why deep-learning AIs are so easy to fool. D Heaven, Nature. 57477772019</p>
<p>Using pre-training can improve model robustness and uncertainty. D Hendrycks, K Lee, M Mazeika, International Conference on Machine Learning. PMLR2019</p>
<p>Pretrained transformers improve out-ofdistribution robustness. D Hendrycks, X Liu, E Wallace, A Dziedzic, R Krishnan, D Song, arXiv:2004.061002020arXiv preprint</p>
<p>. D Hernandez, J Kaplan, T Henighan, S Mccandlish, arXiv:2102.012932021Scaling Laws for Transfer. arXiv preprint</p>
<p>. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D De Las Casas, L A Hendricks, J Welbl, A Clark, T Hennigan, E Noland, K Millican, G Van Den Driessche, B Damoc, A Guy, S Osindero, K Simonyan, E Elsen, J W Rae, O Vinyals, Sifre , L. Training Compute-Optimal Large Language Models. 2022</p>
<p>Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting. H Huang, T Tang, D Zhang, W X Zhao, T Song, Y Xia, F Wei, 2023a</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023b</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>TinyBERT: Distilling BERT for natural language understanding. X Jiao, Y Yin, L Shang, X Jiang, X Chen, L Li, F Wang, Q Liu, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>R L Johnson, G Pistilli, N Menédez-González, L D D Duran, E Panai, J Kalpokiene, D J Bertulfo, arXiv:2203.07785The Ghost in the Machine has an American accent: value conflict in GPT-3. 2022arXiv preprint</p>
<p>D Jurafsky, J H Martin, Speech and Language Processing. February 20243rd Ed. Draft</p>
<p>. S Kadavath, T Conerly, A Askell, T Henighan, D Drain, E Perez, N Schiefer, Z Hatfield-Dodds, N Dassarma, E Tran-Johnson, S Johnston, S El-Showk, A Jones, N Elhage, T Hume, A Chen, Y Bai, S Bowman, S Fort, D Ganguli, D Hernandez, J Jacobson, J Kernion, S Kravec, L Lovitt, K Ndousse, C Olsson, S Ringer, D Amodei, T Brown, J Clark, N Joseph, B Mann, S Mccandlish, C Olah, Kaplan, J. Language Models (Mostly) Know What They Know. nov 2022</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>ChatGPT for good? on opportunities and challenges of large language models for education. E Kasneci, K Seßler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, Learning and Individual Differences. 1031022742023</p>
<p>GPT-4 Passes the Bar Exam. D M Katz, M J Bommarito, S Gao, P Arredondo, March 2023</p>
<p>Learning The Difference That Makes A Difference With Counterfactually-Augmented Data. D Kaushik, E Hovy, Z Lipton, International Conference on Learning Representations. September 2019</p>
<p>Tools and concepts for understanding disruptive technological change after schumpeter. M Knell, S Vannuccini, Jena Economic Research Papers. 2022Technical report</p>
<p>OpenAI's CEO Says the Age of Giant AI Models Is Already Over. W Knight, 2023</p>
<p>How Enterprises Can Become Ready To Work With LLMs. J Kuttan, Forbes. 2023</p>
<p>Ethics and deep learning. T Lacroix, S J Prince, 2023</p>
<p>V D Lai, N T Ngo, A P B Veyseh, H Man, F Dernoncourt, T Bui, T H Nguyen, ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning. 2023</p>
<p>Le Scao, T Fan, A Akiki, C Pavlick, E Ilić, S Hesslow, D Castagné, R Luccioni, A S Yvon, F Gallé, M , A 176b-parameter open-access multilingual language model. 2023</p>
<p>Limits, and Risks of GPT-4 as an AI Chatbot for Medicine. P Lee, S Bubeck, J Petro, Benefits, 10.1056/NEJMsr2214184New England Journal of Medicine. 0028-479338813March 2023</p>
<p>On the thresholds of knowledge. D Lenat, E A Feigenbaum, 1981MIT PressCambridge, MA</p>
<p>Question and answer test-train overlap in open-domain question answering datasets. P Lewis, P Stenetorp, S Riedel, doi: 10.18653Proceedings of the 16th Conference of the European Chapter. P Merlo, J Tiedemann, R Tsarfaty, the 16th Conference of the European ChapterAssociation for Computational LinguisticsApril 2021</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>Syntactic annotations for the google books ngram corpus. Y Lin, J.-B Michel, E A Lieberman, J Orwant, W Brockman, S Petrov, Proceedings of the ACL 2012 system demonstrations. the ACL 2012 system demonstrations2012</p>
<p>Economic transformations: general purpose technologies and longterm economic growth. R G Lipsey, K I Carlaw, C T Bekar, 2005OupOxford</p>
<p>H Liu, R Ning, Z Teng, J Liu, Q Zhou, Y Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of ChatGPT and GPT-4. 2023aarXiv preprint</p>
<p>J Liu, X Xu, Y Li, Y Tan, arXiv:2308.05201generate" the future of work through AI: Empirical evidence from online labor markets. 2023barXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. 5592023c</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A Robustly Optimized BERT Pretraining Approach. jul 2019</p>
<p>Are emergent abilities in large language models just in-context learning?. S Lu, I Bigoulaeva, R Sachdeva, H T Madabushi, I Gurevych, 2023</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Y Lu, M Bartolo, A Moore, S Riedel, P Stenetorp, arXiv:2104.087862021arXiv preprint</p>
<p>Data contamination: From memorization to exploitation. I Magar, R Schwartz, arXiv:2203.082422022arXiv preprint</p>
<p>Reproducibility in NLP: What have we learned from the checklist?. I Magnusson, N A Smith, J Dodge, 10.18653/v1/2023.findings-acl.809Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>M Marone, B Van Durme, Data Portraits: Recording Foundation Model Training Data. March 2023</p>
<p>Re-Evaluating GPT-4's Bar Exam Performance. E Martínez, May 2023</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in Natural Language Inference. R T Mccoy, E Pavlick, T Linzen, arXiv:1902.010072019arXiv preprint</p>
<p>BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. R T Mccoy, J Min, T Linzen, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPAssociation for Computational Linguistics2020</p>
<p>Embers of autoregression: Understanding large language models through the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M Hardy, T L Griffiths, 2023</p>
<p>News coverage of artificial intelligence reflects business and government hype -not critical voices. The Conversation. F Mckelvey, G Dandurand, J Roberge, April 2023</p>
<p>Why GPT Should Stand For 'General Purpose Technology' For All. J Mckendrick, Forbes. 2023</p>
<p>I Mckenzie, A Lyzhov, A Parrish, A Prabhu, A Mueller, N Kim, S Bowman, E Perez, The Inverse Scaling Prize. 2022</p>
<p>C Meister, R Cotterell, arXiv:2106.00085Language Model Evaluation Beyond Perplexity. 2021arXiv preprint</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, Proceedings of International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)2013</p>
<p>An eye for an 'i:'a critical assessment of artificial intelligence tools in migration and asylum management. L Nalbandian, Comparative Migration Studies. 1012022</p>
<p>Imagining the thinking machine: Technological myths and the rise of artificial intelligence. S Natale, A Ballatore, Convergence. 2612020</p>
<p>Show Your Work: Scratchpads for Intermediate Computation with Language Models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, C Sutton, A Odena, oct 2021</p>
<p>Emergent Properties. Stanford Encyclopedia of Philosophy. T O'connor, H Y Wong, August 2020</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, mar 2022</p>
<p>H Pan, C Wang, M Qiu, Y Zhang, Y Li, J Huang, Meta-Kd, arXiv:2012.01266A meta knowledge distillation framework for language model compression across domains. 2020arXiv preprint</p>
<p>A Survey on Transfer Learning. S J Pan, Q Yang, 10.1109/TKDE.2009.191IEEE Transactions on Knowledge and Data Engineering. 1558-21912210October 2010</p>
<p>True Few-Shot Learning with Language Models. E Perez, D Kiela, K Cho, M Ranzato, A Beygelzimer, Y Dauphin, P Liang, Vaughan, Advances in Neural Information Processing Systems. J W , Curran Associates, Inc202134</p>
<p>A Piktus, C Akiki, P Villegas, H Laurençon, G Dupont, A S Luccioni, Y Jernite, A Rogers, The ROOTS Search Tool: Data Transparency for LLMs. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. 32023System Demonstrations</p>
<p>ICT's Wide Web: a System-Level Analysis of ICT's Industrial Diffusion with Algorithmic Links. E Prytkova, SSRN 3772429. 2021</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, OpenAI blog. 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>AI and the Everything in the Whole Wide World Benchmark. I D Raji, E Denton, E M Bender, A Hanna, A Paullada, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 Pre-Proceedings (NeurIPS Datasets and Benchmarks 2021). the Neural Information Processing Systems Track on Datasets and Benchmarks 1 Pre-Proceedings (NeurIPS Datasets and Benchmarks 2021)August 2021</p>
<p>Neural Unsupervised Domain Adaptation in NLP-A Survey. A Ramponi, B Plank, 10.18653/v1/2020.coling-main.603Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, SpainDecember 2020</p>
<p>Machine Reading, Fast and Slow: When Do Models "Understand" Language?. Ray Choudhury, S Rogers, A Augenstein, I , Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaOctober 2022International Committee on Computational Linguistics</p>
<p>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList. M T Ribeiro, T Wu, C Guestrin, S Singh, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Closed AI Models Make Bad Baselines. Hacking semantics. A Rogers, April 2023</p>
<p>Getting closer to AI complete question answering: A set of prerequisite real tasks. A Rogers, O Kovaleva, M Downey, A Rumshisky, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension. A Rogers, M Gardner, I Augenstein, 10.1145/3560260ACM Computing Surveys. 0360-0300551045February 2023a</p>
<p>Program chairs' report on peer review at ACL 2023. A Rogers, M Karpinska, J Boyd-Graber, N Okazaki, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers), pp. xl-lxxv. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 2023b1</p>
<p>How does ChatGPT really work?. K Roose, 2023</p>
<p>Everyone wants to do the model work, not the data work": Data Cascades in High-Stakes AI. N Sambasivan, S Kapania, H Highfill, D Akrong, P Paritosh, L Aroyo, 10.1145/3411764.3445518Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21. the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21New York, NY, USAAssociation for Computing MachineryMay 2021</p>
<p>V Sanh, L Debut, J Chaumond, T Wolf, Distilbert, arXiv:1910.01108a distilled version of BERT: smaller, faster, cheaper and lighter. 2019arXiv preprint</p>
<p>M Sap, R Lebras, D Fried, Y Choi, Neural Theoryof-Mind? On the Limits of Social Intelligence in Large LMs. 2023</p>
<p>T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, M Gallé, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Are Emergent Abilities of Large Language Models a Mirage?. R Schaeffer, B Miranda, S Koyejo, arXiv:2304.150042023arXiv preprint</p>
<p>It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners. T Schick, H Schütze, 10.18653/v1/2021.naacl-main.185Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterAssociation for Computational LinguisticsJune 2021</p>
<p>Targeting the Benchmark: On Methodology in Current Natural Language Processing Research. D Schlangen, 10.18653/v1/2021.acl-short.85Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20212</p>
<p>T Sellam, S Yadlowsky, J Wei, N Saphra, A D'amour, T Linzen, J Bastings, I Turc, J Eisenstein, D Das, I Tenney, E Pavlick, arXiv:2106.16163The MultiBERTs: BERT Reproductions for Robustness Analysis. jun 2021</p>
<p>N Shapira, M Levy, S H Alavi, X Zhou, Y Choi, Y Goldberg, M Sap, V Shwartz, Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. May 2023</p>
<p>The gradient of generative AIrelease: Methods and considerations. I Solaiman, Proceedings of the 2023 ACM conference on fairness, accountability, and transparency. the 2023 ACM conference on fairness, accountability, and transparency2023</p>
<p>L Soldaini, R Kinney, A Bhagia, D Schwenk, D Atkinson, R Authur, B Bogin, K Chandu, J Dumas, Y Elazar, arXiv:2402.00159An open corpus of three trillion tokens for language model pretraining research. 2024arXiv preprint</p>
<p>V Srinivasan, D Gandhi, U Thakker, R Training Prabhakar, Large, arXiv:2304.05511Language Models Efficiently with Sparsity and Dataflow. 2023arXiv preprint</p>
<p>. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, A Kluska, A Lewkowycz, A Agarwal, A Power, A Ray, A Warstadt, A W Kocurek, A Safaya, A Tazarv, A Xiang, A Parrish, A Nie, A Hussain, A Askell, A Dsouza, A Slone, A Rahane, A S Iyer, A Andreassen, A Madotto, A Santilli, A Stuhlmüller, A Dai, A La, A Lampinen, A Zou, A Jiang, A Chen, A Vuong, A Gupta, A Gottardi, A Norelli, A Venkatesh, A Gholamidavoodi, A Tabassum, A Menezes, A Kirubarajan, A Mullokandov, A Sabharwal, A Herrick, A Efrat, A Erdem, A Karakaş, B R Roberts, B S Loe, B Zoph, B Bojanowski, B Özyurt, B Hedayatnia, B Neyshabur, B Inden, B Stein, B Ekmekci, B Y Lin, B Howald, B Orinion, C Diao, C Dour, C Stinson, C Argueta, C F Ramírez, C Singh, C Rathkopf, C Meng, C Baral, C Wu, C Callison-Burch, C Waites, C Voigt, C D Manning, C Potts, C Ramirez, C E Rivera, C Siro, C Raffel, C Ashcraft, C Garbacea, D Sileo, D Garrette, D Hendrycks, D Kilman, D Roth, D Freeman, D Khashabi, D Levy, D M González, D Perszyk, D Hernandez, D Chen, D Ippolito, D Gilboa, D Dohan, D Drakard, D Jurgens, D Datta, D Ganguli, D Emelin, D Kleyko, D Yuret, D Chen, D Tam, D Hupkes, D Misra, D Buzan, D C Mollo, D Yang, D.-H Lee, D Schrader, E Shutova, E D Cubuk, E Segal, E Hagerman, E Barnes, E Donoway, E Pavlick, E Rodola, E Lam, E Chu, E Tang, E Erdem, E Chang, E A Chi, E Dyer, E Jerzak, E Kim, E E Manyasi, E Zheltonozhskii, F Xia, F Siar, F Martínez-Plumed, F Happé, F Chollet, F Rong, G Mishra, G I Winata, G De Melo, G Kruszewski, G Parascandolo, G Mariani, G Wang, G Jaimovitch-López, G Betz, G Gur-Ari, H Galijasevic, H Kim, H Rashkin, H Hajishirzi, H Mehta, H Bogar, H Shevlin, H Schütze, H Yakura, H Zhang, H M Wong, I Ng, I Noble, J Jumelet, J Geissinger, J Kernion, J Hilton, J Lee, J F Fisac, J B Simon, J Koppel, J Zheng, J Zou, J Kocoń, J Thompson, J Wingfield, J Kaplan, J Radom, J Sohl-Dickstein, J Phang, J Wei, J Yosinski, J Novikova, J Bosscher, J Marsh, J Kim, J Taal, J Engel, J Alabi, J Xu, J Song, J Tang, J Waweru, J Burden, J Miller, J U Balis, J Batchelder, J Berant, J Frohberg, J Rozen, J Hernandez-Orallo, J Boudeman, J Guerr, J Jones, J B Tenenbaum, J S Rule, J Chua, K Kanclerz, K Livescu, K Krauth, K Gopalakrishnan, K Ignatyeva, K Markert, K D Dhole, K Gimpel, K Omondi, K Mathewson, K Chiafullo, K Shkaruta, K Shridhar, K Mcdonell, K Richardson, L Reynolds, L Gao, L Zhang, L Dugan, L Qin, L Contreras-Ochando, L.-P Morency, L Moschella, L Lam, L Noble, L Schmidt, L He, L O Colón, L Metz, L K Şenel, M Bosma, M Sap, M Ter Hoeve, M Farooqi, M Faruqui, M Mazeika, M Baturan, M Marelli, M Maru, M J R Quintana, M Tolkiehn, M Giulianelli, M Lewis, M Potthast, M L Leavitt, M Hagen, M Schubert, M O Baitemirova, M Arnaud, M Mcelrath, M A Yee, M Cohen, M Gu, M Ivanitskiy, M Starritt, M Strube, M Swędrowski, M Bevilacqua, M Yasunaga, M Kale, M Cain, M Xu, M Suzgun, M Walker, M Tiwari, M Bansal, M Aminnaseri, M Geva, M Gheini, T , M V Peng, N Chi, N A Lee, N Krakover, N G , .-A Cameron, N Roberts, N Doiron, N Martinez, N Nangia, N Deckers, N Muennighoff, N Keskar, N S Iyer, N S Constant, N Fiedel, N Wen, N Zhang, O Agha, O Elbaghdadi, O Levy, O Evans, O Casares, P A M Doshi, P Fung, P Liang, P P Vicol, P Alipoormolabashi, P Liao, P Liang, P Chang, P Eckersley, P Htut, P M Hwang, P Miłkowski, P Patil, P Pezeshkpour, P Oli, P Mei, Q Lyu, Q Chen, Q Banjade, R Rudolph, R E Gabriel, R Habacker, R Risco, R Millière, R Garg, R Barnes, R Saurous, R A Arakawa, R Raymaekers, R Frank, R Sikand, R Novak, R Sitelew, R Lebras, R Liu, R Jacobs, R Zhang, R Salakhutdinov, R Chi, R Lee, R Stovall, R Teehan, R Yang, R Singh, S Mohammad, S M Anand, S Dillavou, S Shleifer, S Wiseman, S Gruetter, S Bowman, S R Schoenholz, S S Han, S Kwatra, S Rous, S A Ghazarian, S Ghosh, S Casey, S Bischoff, S Gehrmann, S Schuster, S Sadeghi, S Hamdan, S Zhou, S Srivastava, S Shi, S Singh, S Asaadi, S Gu, S S Pachchigar, S Toshniwal, S Upadhyay, S Shyamolima, Debnath, S Shakeri, S Thormeyer, S Melzi, S Reddy, S P Makini, S.-H Lee, S Torene, S Hatwar, S Dehaene, S Divic, S Ermon, S Biderman, S Lin, S Prasad, S T Piantadosi, S M Shieber, S Misherghi, S Kiritchenko, S Mishra, T Linzen, T Schuster, T Li, T Yu, T Ali, T Hashimoto, T.-L Wu, T Desbordes, T Rothschild, T Phan, T Wang, T Nkinyili, T Schick, T Kornev, T Tunduny, T Gerstenberg, T Chang, T Neeraj, T Khot, T Shultz, U Shaham, V Misra, V Demberg, V Nyamai, V Raunak, V Ramasesh, V U Prabhu, V Padmakumar, V Srikumar, W Fedus, W Saunders, W Zhang, W Vossen, X Ren, X Tong, X Zhao, X Wu, X Shen, Y Yaghoobzadeh, Y Lakretz, Y Song, Y Bahri, Y Choi, Y Yang, Y Hao, Y Chen, Y Belinkov, Y Hou, Y Hou, Y Bai, Z Seid, Z Zhao, Z Wang, Z J Wang, Z Wang, Wu, 2023Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</p>
<p>W Sun, L Yan, X Ma, P Ren, D Yin, Z Ren, arXiv:2304.09542Is ChatGPT good at search? investigating large language models as re-ranking agent. 2023arXiv preprint</p>
<p>The bitter lesson. R Sutton, 2019blog post</p>
<p>Understanding the capabilities, limitations, and societal impact of large language models. A Tamkin, M Brundage, J Clark, D Ganguli, arXiv:2102.025032021arXiv preprint</p>
<p>Y Tay, M Dehghani, J Rao, W Fedus, S Abnar, H W Chung, S Narang, D Yogatama, A Vaswani, D Metzler, arXiv:2109.10686Scale efficiently: Insights from pretraining and fine-tuning transformers. 2021arXiv preprint</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, Stojnic, Galactica: A Large Language Model for Science. 2022</p>
<p>Understanding unintended memorization in language models under federated learning. O D Thakkar, S Ramaswamy, R Mathews, F Beaufays, Proceedings of the Third Workshop on Privacy in Natural Language Processing. the Third Workshop on Privacy in Natural Language Processing2021</p>
<p>The computational limits of deep learning. N C Thompson, K Greenewald, K Lee, G F Manso, 2022</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023barXiv preprint</p>
<p>Experimental standards for deep learning in natural language processing research. D Ulmer, E Bassignana, M Müller-Eberstein, D Varab, M Zhang, R Van Der Goot, C Hardmeier, B Plank, 10.18653/v1/2022.findings-emnlp.196Findings of the Association for Computational Linguistics: EMNLP 2022. Y Goldberg, Z Kozareva, Y Zhang, Abu Dhabi, United Arab EmiratesDecember 2022. Association for Computational Linguistics</p>
<p>Large Language Models are Not Models of Natural Language: They are Corpus Models. C Veres, 10.1109/ACCESS.2022.3182505IEEE Access. 2169-3536102022</p>
<p>Universal Adversarial Triggers for Attacking and Analyzing NLP. E Wallace, S Feng, N Kandpal, M Gardner, S Singh, 10.18653/v1/D19-1221Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>GPT-RE: In-context learning for relation extraction using large language models. Z Wan, F Cheng, Z Mao, Q Liu, H Song, J Li, S Kurohashi, doi: 10.18653Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>A Wang, Y Pruksachatkun, N Nangia, A Singh, J Michael, F Hill, O Levy, S R Bowman, Su, arXiv:1905.00537Stickier Benchmark for General-Purpose Language Understanding Systems. may 2019</p>
<p>S Wang, X Sun, X Li, R Ouyang, F Wu, T Zhang, J Li, G Wang, Gpt-Ner, Named Entity Recognition via Large Language Models. 2023</p>
<p>Y Wang, J Deng, A Sun, X Meng, arXiv:2210.05892Perplexity from PLM Is Unreliable for Evaluating Text Quality. 2022aarXiv preprint</p>
<p>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Naik, A Ashok, A S Dhanasekaran, A Arunkumar, D Stap, E Pathak, G Karamanolakis, H Lai, I Purohit, I Mondal, J Anderson, K Kuznia, K Doshi, K K Pal, M Patel, M Moradshahi, M Parmar, M Purohit, N Varshney, P R Kaza, P Verma, R S Puri, R Karia, S Doshi, S K Sampat, S Mishra, A Reddy, S Patro, S Dixit, T Shen, X , Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022b</p>
<p>Common arguments regarding emergent abilities. J Wei, May 2023</p>
<p>Finetuned Language Models are Zero-Shot Learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, International Conference on Learning Representations. oct 2021</p>
<p>Emergent Abilities of Large Language Models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, Transactions on Machine Learning Research. 2835-88562022a</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, 35</p>
<p>The steep cost of capture. M Whittaker, Interactions. 2862021</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, L Qiu, A Ross, E Akyürek, B Chen, B Wang, N Kim, J Andreas, Y Kim, 2023</p>
<p>OpenAI CEO tells Senate that he fears AI's potential to manipulate views. C E Zakrzewski, 2023</p>
<p>OPT: Open pre-trained transformer language models. S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.010682022arXiv preprint</p>
<p>S J Zhang, S Florin, A N Lee, E Niknafs, A Marginean, A Wang, K Tyser, Z Chin, Y Hicke, N Singh, M Udell, Y Kim, T Buonassisi, A Solar-Lezama, I Drori, Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. 2023</p>
<p>Calibrate before use: Improving few-shot performance of language models. Z Zhao, E Wallace, S Feng, D Klein, S Singh, Proceedings of the 38th International Conference on Machine Learning. M Meila, T Zhang, the 38th International Conference on Machine LearningPMLRJul 2021139</p>
<p>Multilingual Machine Translation with Large Language Models. W Zhu, H Liu, Q Dong, J Xu, S Huang, L Kong, J Chen, L Li, 2023Empirical Results and Analysis</p>
<p>. F Zhuang, Z Qi, K Duan, D Xi, Y Zhu, H Zhu, H Xiong, Q He, 10.1109/JPROC.2020.3004555A Comprehensive Survey on Transfer Learning. Proceedings of the IEEE. 1558-22561091January 2021</p>
<p>. C Ziems, W Held, J Yang, J Dhamala, R Gupta, D Yang, - Multi, Value, 2023A Framework for Cross-Dialectal English NLP</p>
<p>Universal and transferable adversarial attacks on aligned language models. A Zou, Z Wang, N Carlini, M Nasr, J Z Kolter, M Fredrikson, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
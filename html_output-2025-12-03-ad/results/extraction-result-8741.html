<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8741 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8741</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8741</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278789345</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16475v1.pdf" target="_blank">ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection</a></p>
                <p><strong>Paper Abstract:</strong> We present a novel pipeline, ReflectEvo, to demonstrate that small language models (SLMs) can enhance meta introspection through reflection learning. This process iteratively generates self-reflection for self-training, fostering a continuous and self-evolving process. Leveraging this pipeline, we construct ReflectEvo-460k, a large-scale, comprehensive, self-generated reflection dataset with broadened instructions and diverse multi-domain tasks. Building upon this dataset, we demonstrate the effectiveness of reflection learning to improve SLMs' reasoning abilities using SFT and DPO with remarkable performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral from 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the reasoning capability of the three prominent open-sourced models on BIG-bench without distillation from superior models or fine-grained human annotation. We further conduct a deeper analysis of the high quality of self-generated reflections and their impact on error localization and correction. Our work highlights the potential of continuously enhancing the reasoning performance of SLMs through iterative reflection learning in the long run.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8741.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8741.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReflectEvo (Llama-3-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReflectEvo reflection-learning pipeline applied to Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ReflectEvo is an end-to-end pipeline that (1) generates self-reflection data by having a Generator (G) produce an initial chain-of-thought answer and a Reflector (R) produce explicit self-reflection and self-correction, and (2) uses self-training (SFT and DPO) on the generated ReflectEvo-460k dataset to improve small LLMs' meta-introspection and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B (referred to as Llama-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source small language model family Llama-3, 8B parameter variant used in experiments (instruction-tuned base).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ReflectEvo reflection learning (one-stage / two-stage SFT and DPO variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generator G produces initial solution + interleaved thoughts (ReAct style). Reflector R produces an explicit self-reflection r (diagnose errors, plans) and a corrected answer â. Reflection data are sampled with reject sampling (k=2) and curated into D+, D±, and D_pref; models are trained via supervised fine-tuning (one-stage or two-stage) and Direct Preference Optimization (DPO) on pairwise preference data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple reasoning benchmarks (LogiQA, BIG-bench, MBPP, MATH, and aggregated ReflectEvo tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A mixture of logical reasoning, math, coding, reading comprehension, and BIG-bench subsets used to evaluate reasoning and correction capability after reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Two-stage SFT on D+ (best reported for Llama-3): Acc@t2 = 49.4% (∆(t1,t2) = +19.2% for the evaluated aggregated task in Table 2). On BIG-bench specifically, authors report boosting Llama-3 from 52.4% to 71.2% (absolute improvement reported in abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Prompt-based baseline (no reflection training) reported Acc@t2 = 36.2% for the same aggregated evaluation in Table 2; initial (turn-1) baseline varies by dataset (e.g., Acc@t1 reported elsewhere).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Implemented via prompt-engineered Generator and Reflector using ReAct-style interleaved thoughts and carefully designed multi-stage reflection prompts; data curated into ReflectEvo-460k and used for SFT (LoRA/PEFT and full fine-tuning) and DPO preference training. Teacher selection uses GPT-4o for D_pref.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: Two-stage SFT on D+ improved Acc@t2 from 36.2% (prompt baseline) to 49.4% (∆ +13.2–19.2% reported depending on metric/tabulation). Abstract reports BIG-bench improvement from 52.4% → 71.2%. Authors also show average ∆(t1,t2) ≈ 22% across tasks and task-specific gains (e.g., >20% ∆ for MBPP and BIG-bench). Qualitative: case studies and correlation analyses show higher similarity between reflection and second-turn thought correlates with higher Acc@t2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quality of improvement depends strongly on base model reasoning ability; SLMs with very weak reasoning generate low-quality reflections and gain little. Flawed reflections can perpetuate or produce repeated errors (shown in 'False to False' cases). Coding and math tasks saw smaller gains (lack of fine-grained critiques); reliance on oracle or imperfect verifiers can degrade advantages (self-verifier sometimes misjudges answers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against prompt-based reflection (no tuning) and selective baselines (STaR, Re-ReST, RISE). ReflectEvo SFT/DPO methods (especially two-stage w/ D+) outperform these baselines on the reported aggregated tasks (e.g., RISE showed +3.0% vs baseline while ReflectEvo two-stage yields +19.2% on the Llama-3 aggregated evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations across training regimes: one-stage w/ D+ (Acc@t2 = 43.8%, ∆ +13.6%), two-stage w/ D+ (Acc@t2 = 49.4%, ∆ +19.2%) , w/ D± (Acc@t2 = 41.8%, ∆ +11.6%), w/ D_pref (Acc@t2 = 39.2%, ∆ +9.0%). Two-stage SFT on D+ produced the largest gain for Llama-3 in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8741.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8741.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReflectEvo (Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReflectEvo reflection-learning pipeline applied to Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same ReflectEvo pipeline (Generator + Reflector + ReflectEvo-460k self-training) applied to Mistral-7B; shows large improvements in iterative correction performance after reflection learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B-parameter SLM (Mistral family) used as generator and reflector in data generation, training, and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ReflectEvo reflection learning (one-stage / two-stage SFT and DPO variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same procedure as for Llama-3: G produces initial CoT answer; R produces self-reflection and corrected answer; curated data used to fine-tune reflector via SFT or DPO. Reject sampling k=2; reflection performed once in main experiments (two-turn QA).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple reasoning benchmarks (LogiQA, BIG-bench, MBPP, MATH, and aggregated ReflectEvo tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same mixed suite of reasoning/math/coding/comprehension tasks used across models for cross-model evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>One-stage w/ D+ reported Acc@t2 = 52.0% for the aggregated evaluation block in Table 2 (improvement over baseline). Abstract reports boosting Mistral from 44.4% to 71.1% on BIG-bench.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Prompt-based baseline Acc@t2 reported as 46.2% (aggregated prompt-based block in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered generator/reflector (ReAct style) and self-training on ReflectEvo-460k; training used LoRA-based PEFT and DPO configurations; GPT-4o used for preference selection in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: One-stage w/ D+ improved Acc@t2 from 46.2% to 52.0% (∆ +5.8% absolute in aggregated table), and abstract-level BIG-bench jump from 44.4% to 71.1% reported. Multi-turn experiments show consistent improvement with up to 6 turns, with BIG-bench surpassing 80% after six turns (per text / Fig. 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Similar limitations as Llama-3: reflection quality tied to base model ability; some tasks (math/coding) see only modest gains and require finer-grained critiques; some datasets (e.g., MATH) were discarded for certain generator reflections due to very low-quality generated reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared favorably against baselines STaR, Re-ReST, RISE; authors report higher ∆ improvements for ReflectEvo variants than these baselines on the same evaluation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Across training regimes, varying results: one-stage and two-stage SFT and DPO variants all produced improvements; relative ordering differs by model and task (Table 2 and further per-task tables). Specific numbers for Mistral in Table 2: one-stage w/ D+ (Acc@t2 = 52.0%, ∆ +21.8%), two-stage w/ D+ (Acc@t2 = 41.2%, ∆ +11.0%), w/ D± (48.8%, ∆ +18.6%), w/ D_pref (48.0%, ∆ +17.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8741.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8741.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReflectEvo (Gemma-2-9B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReflectEvo reflection-learning pipeline applied to Gemma-2-9B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of ReflectEvo self-reflection data and training to Gemma-2-9B; shows more modest gains possibly due to Gemma's stronger base reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemma-2 family small language model (9B) used as a generator/reflector in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ReflectEvo reflection learning (one-stage / two-stage SFT and DPO variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same pipeline (G + R, reflect-and-correct, ReflectEvo-460k curation) and training regimes (SFT, DPO).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple reasoning benchmarks (BIG-bench, LogiQA, MATH, MBPP, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same mixed suite of tasks used to evaluate reflection/correction improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported as showing marginal improvement compared with Llama-3 and Mistral; table entries indicate improvements but smaller magnitudes (detailed per-task values in Tab. 9). Example: in some tasks Gemma-2 had Acc@t2 = 57.2% after training (see Table 5 block; context-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline (untuned) performance was already relatively strong (e.g., Acc@t1 values in table blocks such as 47.6% for some tasks), leading to smaller headroom for reflection gains.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Same ReflectEvo mechanism and training pipeline; authors note Gemma may require higher-quality reflection data or supervision from stronger models for further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: Tab. 9 and accompanying text state Gemma-2 shows marginal improvement compared to Llama-3 and Mistral; authors hypothesize this is because Gemma-2's inherent reasoning is already strong and may need different supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller improvements for stronger SLMs (Gemma-2) indicate diminishing returns; may require higher-quality reflection data, distillation from superior models, or tailored training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>ReflectEvo still outperforms untuned prompt-based reflection baseline for Gemma-2 in some settings, but absolute gains are smaller than for weaker SLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reporting indicates that the effect sizes across one-stage / two-stage / DPO variants differ and may be task-dependent; Gemma required either more data or teacher supervision to surpass peers (detailed numbers in Tab. 9).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8741.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8741.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generator G / Reflector R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generator (G) and Reflector (R) components of ReflectEvo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>G is the reasoning model that emits an initial interleaved chain-of-thought and answer; R is the reflection model that, given (q, a, f), produces an explicit self-reflection r identifying and diagnosing errors and then produces a corrected answer â.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Component-level (not a single LM) — implemented using the same base SLMs (Llama-3, Mistral, Gemma) as G and R</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>G and R are realized by instantiating the same base SLM (instruction-tuned) with different prompts/roles: G is prompted to produce interleaved Thought/Action (ReAct style); R is prompted to perform staged verification, localization of error, propose corrections, and produce corrected solution.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate-then-reflect (explicit reflect-and-correct with separate Reflector)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-phase generation: (1) G: produce chain-of-thought and answer a; environment/verifier returns binary feedback f (correct/incorrect). (2) If incorrect, R generates r = R(r | q,a,f) and then â = R(â | q,a,f,r). Rejection sampling (k=2) and multiple reflection prompts used to diversify data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used across all ReflectEvo data generation tasks (17 source datasets, multiple domains)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Data generation step for diverse tasks to create ReflectEvo-460k, including logic, math, coding, reading comprehension, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not directly a single performance number — when used as inference-time reflector after reflection learning, it increases Acc@t2 substantially (e.g., Llama-3 two-stage Acc@t2 improvements reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Without a separate Reflector (i.e., no explicit generate-then-reflect stage), prompt-only baselines showed lower Acc@t2 (e.g., 36.2% in aggregated Llama-3 prompt-based entry in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Explicit modular role separation implemented by prompting the same model as either G or R; R uses structured multi-stage reflection prompts (verify, locate errors, outline correction plans) as described in Appendix C.1 and C.2.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical: Using an explicit Reflector that produces textual localized critique and a corrected answer enabled the creation of training data that, after SFT/DPO, produced substantial Acc@t2 improvements across models and tasks. Multi-turn experiments show iterative application of R for up to 6 turns further increases performance on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>When R generates flawed or partially incorrect reflections, subsequent corrected answers can remain incorrect or become worse; success depends on quality of the reflection and the verifier signal (binary feedback is limited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>This generate-then-reflect modular setup differs from end-to-end direct mapping approaches (SFT on direct answers) and is compared in ablations (one-stage vs two-stage SFT, DPO). Two-stage training (explicitly separating r and â training) gave the largest gains for Llama-3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8741.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8741.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (teacher reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used as a stronger teacher for reflection preference selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o is used as an external stronger model to annotate and select preferred reflections from the self-generated pool (D_pref), yielding higher-quality pairwise preference data for DPO training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (teacher model for preference selection)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary larger model (GPT-4o) used as a stronger teacher to pick better reflections from pairs of candidate reflections generated by SLMs, thereby producing D_pref for preference optimization (DPO).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Teacher-guided preference selection (GPT-4o annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>GPT-4o is used post-hoc to compare pairs of candidate (r, â) and select the preferred one; these pairwise labels form D_pref for preference-based training (DPO) of the SLM reflector.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reflection curation and preference annotation for ReflectEvo-460k</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GPT-4o evaluates multiple generated reflections per (q,a,f) and selects preferred/refused pairs used for DPO training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reflections selected by GPT-4o yield larger improvements than self-reflections alone when used to train SLMs; authors report that teacher reflections produce more obvious improvements across different settings (Tab. 3 and related text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Self-generated reflections (without GPT-4o selection) also help but less so; self-generated data require less cost but yield smaller gains than GPT-4o-curated data.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External model-based pairwise ranking to curate high-quality reflection/correction pairs that serve as preference data for DPO.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: training on D_pref (GPT-4o-selected) improves ∆(t1,t2) more than raw self-reflection-only data in several settings; textual claim and Tab. 3 support this (exact per-task gains shown in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Using a stronger teacher increases annotation cost and resource requirements; authors emphasize the appeal of self-generated-only data due to lower cost though at some loss in absolute improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to purely self-generated D+ or D±, GPT-4o-curated D_pref improves the reflector more effectively but at higher cost; DPO on D_pref is one of the proposed settings (Setting 4).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Setting 4 (DPO on D_pref) yields improvements but in some aggregated comparisons was outperformed by two-stage SFT on D+ for certain models/tasks; authors provide per-setting performance in Table 2 and Section 4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8741.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8741.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines (STaR / Re-ReST / RISE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline self-improvement methods: STaR, Re-ReST, and RISE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Competing/self-improvement baselines evaluated by the authors: STaR (self-training with reasoning), Re-ReST (reflection-reinforced self-training), and RISE (recent iterative self-refinement method); used for comparative evaluation under the same binary feedback constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B baseline evaluations (methods applied to Llama-3 as in paper comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baselines are implemented/applied to the same small LLM (Llama-3-8B) for fair comparison in the paper's evaluation setting (binary external feedback only).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Baseline iterative/self-training or reflection/correction methods (STaR, Re-ReST, RISE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>STaR: self-training bootstrapping reasoning; Re-ReST: reflection-reinforced self-training; RISE: recent iterative-refinement method. All operate under limited external feedback and self-improvement paradigms but differ in data curation and training specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LogiQA (primary baseline comparison) and other aggregated tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same evaluation setup used for ReflectEvo comparisons (binary correct/incorrect feedback only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported baselines: STaR Acc@t1 = 40.0% (Table 7/8), Re-ReST Acc@t1 = 38.8%, RISE reported Acc@t1 = 31.4% and Acc@t2 = 34.4% (∆ +3.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>The baseline 'untuned' Llama-3 is reported as Acc@t1 = 30.2% and Acc@t2 = 36.2% (paper's reported untuned numbers quoted in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Varies by baseline: STaR and Re-ReST use self-training / reflection reinforcement; RISE uses iterative refinement. All are implemented as comparative baselines in the paper under the same binary-feedback constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper reports that ReflectEvo methods (notably two-stage SFT on D+) achieve larger ∆(t1,t2) gains than these baselines on the evaluated tasks (e.g., ReflectEvo two-stage ∆ +19.2% vs RISE ∆ +3.0% in Table 7/8 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes that some baseline methods do provide improvements but generally less than ReflectEvo's best settings in this evaluation; exact performance depends on implementation details and feedback used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Direct comparisons in Table 7/8 show ReflectEvo variants (one-stage and two-stage SFT, DPO variants) outperform these baselines on the selected evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Baselines are included in aggregated comparisons but no further ablation of the baseline algorithms themselves is reported beyond the comparative tables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Re-rest: Reflectionreinforced self-training for language agents <em>(Rating: 2)</em></li>
                <li>Self-play fine-tuning converts weak language models to strong language models <em>(Rating: 1)</em></li>
                <li>Self-reflection in llm agents: Effects on problem-solving performance <em>(Rating: 2)</em></li>
                <li>Recursive introspection: Teaching language model agents how to self-improve <em>(Rating: 2)</em></li>
                <li>Reflection and learning: Characteristics, obstacles, and implications <em>(Rating: 1)</em></li>
                <li>Reflection-tuning: Data recycling improves llm instruction-tuning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8741",
    "paper_id": "paper-278789345",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "ReflectEvo (Llama-3-8B)",
            "name_full": "ReflectEvo reflection-learning pipeline applied to Llama-3-8B",
            "brief_description": "ReflectEvo is an end-to-end pipeline that (1) generates self-reflection data by having a Generator (G) produce an initial chain-of-thought answer and a Reflector (R) produce explicit self-reflection and self-correction, and (2) uses self-training (SFT and DPO) on the generated ReflectEvo-460k dataset to improve small LLMs' meta-introspection and reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B (referred to as Llama-3)",
            "model_description": "Open-source small language model family Llama-3, 8B parameter variant used in experiments (instruction-tuned base).",
            "reflection_method_name": "ReflectEvo reflection learning (one-stage / two-stage SFT and DPO variants)",
            "reflection_method_description": "Generator G produces initial solution + interleaved thoughts (ReAct style). Reflector R produces an explicit self-reflection r (diagnose errors, plans) and a corrected answer â. Reflection data are sampled with reject sampling (k=2) and curated into D+, D±, and D_pref; models are trained via supervised fine-tuning (one-stage or two-stage) and Direct Preference Optimization (DPO) on pairwise preference data.",
            "task_name": "Multiple reasoning benchmarks (LogiQA, BIG-bench, MBPP, MATH, and aggregated ReflectEvo tasks)",
            "task_description": "A mixture of logical reasoning, math, coding, reading comprehension, and BIG-bench subsets used to evaluate reasoning and correction capability after reflection.",
            "performance_with_reflection": "Two-stage SFT on D+ (best reported for Llama-3): Acc@t2 = 49.4% (∆(t1,t2) = +19.2% for the evaluated aggregated task in Table 2). On BIG-bench specifically, authors report boosting Llama-3 from 52.4% to 71.2% (absolute improvement reported in abstract).",
            "performance_without_reflection": "Prompt-based baseline (no reflection training) reported Acc@t2 = 36.2% for the same aggregated evaluation in Table 2; initial (turn-1) baseline varies by dataset (e.g., Acc@t1 reported elsewhere).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Implemented via prompt-engineered Generator and Reflector using ReAct-style interleaved thoughts and carefully designed multi-stage reflection prompts; data curated into ReflectEvo-460k and used for SFT (LoRA/PEFT and full fine-tuning) and DPO preference training. Teacher selection uses GPT-4o for D_pref.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: Two-stage SFT on D+ improved Acc@t2 from 36.2% (prompt baseline) to 49.4% (∆ +13.2–19.2% reported depending on metric/tabulation). Abstract reports BIG-bench improvement from 52.4% → 71.2%. Authors also show average ∆(t1,t2) ≈ 22% across tasks and task-specific gains (e.g., &gt;20% ∆ for MBPP and BIG-bench). Qualitative: case studies and correlation analyses show higher similarity between reflection and second-turn thought correlates with higher Acc@t2.",
            "limitations_or_failure_cases": "Quality of improvement depends strongly on base model reasoning ability; SLMs with very weak reasoning generate low-quality reflections and gain little. Flawed reflections can perpetuate or produce repeated errors (shown in 'False to False' cases). Coding and math tasks saw smaller gains (lack of fine-grained critiques); reliance on oracle or imperfect verifiers can degrade advantages (self-verifier sometimes misjudges answers).",
            "comparison_to_other_methods": "Compared against prompt-based reflection (no tuning) and selective baselines (STaR, Re-ReST, RISE). ReflectEvo SFT/DPO methods (especially two-stage w/ D+) outperform these baselines on the reported aggregated tasks (e.g., RISE showed +3.0% vs baseline while ReflectEvo two-stage yields +19.2% on the Llama-3 aggregated evaluation).",
            "ablation_study_results": "Ablations across training regimes: one-stage w/ D+ (Acc@t2 = 43.8%, ∆ +13.6%), two-stage w/ D+ (Acc@t2 = 49.4%, ∆ +19.2%) , w/ D± (Acc@t2 = 41.8%, ∆ +11.6%), w/ D_pref (Acc@t2 = 39.2%, ∆ +9.0%). Two-stage SFT on D+ produced the largest gain for Llama-3 in Table 2.",
            "uuid": "e8741.0",
            "source_info": {
                "paper_title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ReflectEvo (Mistral-7B)",
            "name_full": "ReflectEvo reflection-learning pipeline applied to Mistral-7B",
            "brief_description": "Same ReflectEvo pipeline (Generator + Reflector + ReflectEvo-460k self-training) applied to Mistral-7B; shows large improvements in iterative correction performance after reflection learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_description": "Open-source 7B-parameter SLM (Mistral family) used as generator and reflector in data generation, training, and evaluation.",
            "reflection_method_name": "ReflectEvo reflection learning (one-stage / two-stage SFT and DPO variants)",
            "reflection_method_description": "Same procedure as for Llama-3: G produces initial CoT answer; R produces self-reflection and corrected answer; curated data used to fine-tune reflector via SFT or DPO. Reject sampling k=2; reflection performed once in main experiments (two-turn QA).",
            "task_name": "Multiple reasoning benchmarks (LogiQA, BIG-bench, MBPP, MATH, and aggregated ReflectEvo tasks)",
            "task_description": "Same mixed suite of reasoning/math/coding/comprehension tasks used across models for cross-model evaluation.",
            "performance_with_reflection": "One-stage w/ D+ reported Acc@t2 = 52.0% for the aggregated evaluation block in Table 2 (improvement over baseline). Abstract reports boosting Mistral from 44.4% to 71.1% on BIG-bench.",
            "performance_without_reflection": "Prompt-based baseline Acc@t2 reported as 46.2% (aggregated prompt-based block in Table 2).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered generator/reflector (ReAct style) and self-training on ReflectEvo-460k; training used LoRA-based PEFT and DPO configurations; GPT-4o used for preference selection in some settings.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: One-stage w/ D+ improved Acc@t2 from 46.2% to 52.0% (∆ +5.8% absolute in aggregated table), and abstract-level BIG-bench jump from 44.4% to 71.1% reported. Multi-turn experiments show consistent improvement with up to 6 turns, with BIG-bench surpassing 80% after six turns (per text / Fig. 4).",
            "limitations_or_failure_cases": "Similar limitations as Llama-3: reflection quality tied to base model ability; some tasks (math/coding) see only modest gains and require finer-grained critiques; some datasets (e.g., MATH) were discarded for certain generator reflections due to very low-quality generated reflections.",
            "comparison_to_other_methods": "Compared favorably against baselines STaR, Re-ReST, RISE; authors report higher ∆ improvements for ReflectEvo variants than these baselines on the same evaluation protocol.",
            "ablation_study_results": "Across training regimes, varying results: one-stage and two-stage SFT and DPO variants all produced improvements; relative ordering differs by model and task (Table 2 and further per-task tables). Specific numbers for Mistral in Table 2: one-stage w/ D+ (Acc@t2 = 52.0%, ∆ +21.8%), two-stage w/ D+ (Acc@t2 = 41.2%, ∆ +11.0%), w/ D± (48.8%, ∆ +18.6%), w/ D_pref (48.0%, ∆ +17.8%).",
            "uuid": "e8741.1",
            "source_info": {
                "paper_title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ReflectEvo (Gemma-2-9B)",
            "name_full": "ReflectEvo reflection-learning pipeline applied to Gemma-2-9B",
            "brief_description": "Application of ReflectEvo self-reflection data and training to Gemma-2-9B; shows more modest gains possibly due to Gemma's stronger base reasoning capability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-9B",
            "model_description": "Gemma-2 family small language model (9B) used as a generator/reflector in experiments.",
            "reflection_method_name": "ReflectEvo reflection learning (one-stage / two-stage SFT and DPO variants)",
            "reflection_method_description": "Same pipeline (G + R, reflect-and-correct, ReflectEvo-460k curation) and training regimes (SFT, DPO).",
            "task_name": "Multiple reasoning benchmarks (BIG-bench, LogiQA, MATH, MBPP, etc.)",
            "task_description": "Same mixed suite of tasks used to evaluate reflection/correction improvements.",
            "performance_with_reflection": "Reported as showing marginal improvement compared with Llama-3 and Mistral; table entries indicate improvements but smaller magnitudes (detailed per-task values in Tab. 9). Example: in some tasks Gemma-2 had Acc@t2 = 57.2% after training (see Table 5 block; context-dependent).",
            "performance_without_reflection": "Baseline (untuned) performance was already relatively strong (e.g., Acc@t1 values in table blocks such as 47.6% for some tasks), leading to smaller headroom for reflection gains.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Same ReflectEvo mechanism and training pipeline; authors note Gemma may require higher-quality reflection data or supervision from stronger models for further gains.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: Tab. 9 and accompanying text state Gemma-2 shows marginal improvement compared to Llama-3 and Mistral; authors hypothesize this is because Gemma-2's inherent reasoning is already strong and may need different supervision.",
            "limitations_or_failure_cases": "Smaller improvements for stronger SLMs (Gemma-2) indicate diminishing returns; may require higher-quality reflection data, distillation from superior models, or tailored training.",
            "comparison_to_other_methods": "ReflectEvo still outperforms untuned prompt-based reflection baseline for Gemma-2 in some settings, but absolute gains are smaller than for weaker SLMs.",
            "ablation_study_results": "Reporting indicates that the effect sizes across one-stage / two-stage / DPO variants differ and may be task-dependent; Gemma required either more data or teacher supervision to surpass peers (detailed numbers in Tab. 9).",
            "uuid": "e8741.2",
            "source_info": {
                "paper_title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Generator G / Reflector R",
            "name_full": "Generator (G) and Reflector (R) components of ReflectEvo",
            "brief_description": "G is the reasoning model that emits an initial interleaved chain-of-thought and answer; R is the reflection model that, given (q, a, f), produces an explicit self-reflection r identifying and diagnosing errors and then produces a corrected answer â.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Component-level (not a single LM) — implemented using the same base SLMs (Llama-3, Mistral, Gemma) as G and R",
            "model_description": "G and R are realized by instantiating the same base SLM (instruction-tuned) with different prompts/roles: G is prompted to produce interleaved Thought/Action (ReAct style); R is prompted to perform staged verification, localization of error, propose corrections, and produce corrected solution.",
            "reflection_method_name": "Generate-then-reflect (explicit reflect-and-correct with separate Reflector)",
            "reflection_method_description": "Two-phase generation: (1) G: produce chain-of-thought and answer a; environment/verifier returns binary feedback f (correct/incorrect). (2) If incorrect, R generates r = R(r | q,a,f) and then â = R(â | q,a,f,r). Rejection sampling (k=2) and multiple reflection prompts used to diversify data.",
            "task_name": "Used across all ReflectEvo data generation tasks (17 source datasets, multiple domains)",
            "task_description": "Data generation step for diverse tasks to create ReflectEvo-460k, including logic, math, coding, reading comprehension, etc.",
            "performance_with_reflection": "Not directly a single performance number — when used as inference-time reflector after reflection learning, it increases Acc@t2 substantially (e.g., Llama-3 two-stage Acc@t2 improvements reported in Table 2).",
            "performance_without_reflection": "Without a separate Reflector (i.e., no explicit generate-then-reflect stage), prompt-only baselines showed lower Acc@t2 (e.g., 36.2% in aggregated Llama-3 prompt-based entry in Table 2).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Explicit modular role separation implemented by prompting the same model as either G or R; R uses structured multi-stage reflection prompts (verify, locate errors, outline correction plans) as described in Appendix C.1 and C.2.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Empirical: Using an explicit Reflector that produces textual localized critique and a corrected answer enabled the creation of training data that, after SFT/DPO, produced substantial Acc@t2 improvements across models and tasks. Multi-turn experiments show iterative application of R for up to 6 turns further increases performance on some tasks.",
            "limitations_or_failure_cases": "When R generates flawed or partially incorrect reflections, subsequent corrected answers can remain incorrect or become worse; success depends on quality of the reflection and the verifier signal (binary feedback is limited).",
            "comparison_to_other_methods": "This generate-then-reflect modular setup differs from end-to-end direct mapping approaches (SFT on direct answers) and is compared in ablations (one-stage vs two-stage SFT, DPO). Two-stage training (explicitly separating r and â training) gave the largest gains for Llama-3.",
            "uuid": "e8741.3",
            "source_info": {
                "paper_title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4o (teacher reflections)",
            "name_full": "GPT-4o used as a stronger teacher for reflection preference selection",
            "brief_description": "GPT-4o is used as an external stronger model to annotate and select preferred reflections from the self-generated pool (D_pref), yielding higher-quality pairwise preference data for DPO training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (teacher model for preference selection)",
            "model_description": "Proprietary larger model (GPT-4o) used as a stronger teacher to pick better reflections from pairs of candidate reflections generated by SLMs, thereby producing D_pref for preference optimization (DPO).",
            "reflection_method_name": "Teacher-guided preference selection (GPT-4o annotation)",
            "reflection_method_description": "GPT-4o is used post-hoc to compare pairs of candidate (r, â) and select the preferred one; these pairwise labels form D_pref for preference-based training (DPO) of the SLM reflector.",
            "task_name": "Reflection curation and preference annotation for ReflectEvo-460k",
            "task_description": "GPT-4o evaluates multiple generated reflections per (q,a,f) and selects preferred/refused pairs used for DPO training.",
            "performance_with_reflection": "Reflections selected by GPT-4o yield larger improvements than self-reflections alone when used to train SLMs; authors report that teacher reflections produce more obvious improvements across different settings (Tab. 3 and related text).",
            "performance_without_reflection": "Self-generated reflections (without GPT-4o selection) also help but less so; self-generated data require less cost but yield smaller gains than GPT-4o-curated data.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External model-based pairwise ranking to curate high-quality reflection/correction pairs that serve as preference data for DPO.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: training on D_pref (GPT-4o-selected) improves ∆(t1,t2) more than raw self-reflection-only data in several settings; textual claim and Tab. 3 support this (exact per-task gains shown in tables).",
            "limitations_or_failure_cases": "Using a stronger teacher increases annotation cost and resource requirements; authors emphasize the appeal of self-generated-only data due to lower cost though at some loss in absolute improvement.",
            "comparison_to_other_methods": "Compared to purely self-generated D+ or D±, GPT-4o-curated D_pref improves the reflector more effectively but at higher cost; DPO on D_pref is one of the proposed settings (Setting 4).",
            "ablation_study_results": "Setting 4 (DPO on D_pref) yields improvements but in some aggregated comparisons was outperformed by two-stage SFT on D+ for certain models/tasks; authors provide per-setting performance in Table 2 and Section 4.",
            "uuid": "e8741.4",
            "source_info": {
                "paper_title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Baselines (STaR / Re-ReST / RISE)",
            "name_full": "Baseline self-improvement methods: STaR, Re-ReST, and RISE",
            "brief_description": "Competing/self-improvement baselines evaluated by the authors: STaR (self-training with reasoning), Re-ReST (reflection-reinforced self-training), and RISE (recent iterative self-refinement method); used for comparative evaluation under the same binary feedback constraint.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B baseline evaluations (methods applied to Llama-3 as in paper comparisons)",
            "model_description": "Baselines are implemented/applied to the same small LLM (Llama-3-8B) for fair comparison in the paper's evaluation setting (binary external feedback only).",
            "reflection_method_name": "Baseline iterative/self-training or reflection/correction methods (STaR, Re-ReST, RISE)",
            "reflection_method_description": "STaR: self-training bootstrapping reasoning; Re-ReST: reflection-reinforced self-training; RISE: recent iterative-refinement method. All operate under limited external feedback and self-improvement paradigms but differ in data curation and training specifics.",
            "task_name": "LogiQA (primary baseline comparison) and other aggregated tasks",
            "task_description": "Same evaluation setup used for ReflectEvo comparisons (binary correct/incorrect feedback only).",
            "performance_with_reflection": "Reported baselines: STaR Acc@t1 = 40.0% (Table 7/8), Re-ReST Acc@t1 = 38.8%, RISE reported Acc@t1 = 31.4% and Acc@t2 = 34.4% (∆ +3.0%).",
            "performance_without_reflection": "The baseline 'untuned' Llama-3 is reported as Acc@t1 = 30.2% and Acc@t2 = 36.2% (paper's reported untuned numbers quoted in tables).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Varies by baseline: STaR and Re-ReST use self-training / reflection reinforcement; RISE uses iterative refinement. All are implemented as comparative baselines in the paper under the same binary-feedback constraint.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Paper reports that ReflectEvo methods (notably two-stage SFT on D+) achieve larger ∆(t1,t2) gains than these baselines on the evaluated tasks (e.g., ReflectEvo two-stage ∆ +19.2% vs RISE ∆ +3.0% in Table 7/8 comparisons).",
            "limitations_or_failure_cases": "Paper notes that some baseline methods do provide improvements but generally less than ReflectEvo's best settings in this evaluation; exact performance depends on implementation details and feedback used.",
            "comparison_to_other_methods": "Direct comparisons in Table 7/8 show ReflectEvo variants (one-stage and two-stage SFT, DPO variants) outperform these baselines on the selected evaluation tasks.",
            "ablation_study_results": "Baselines are included in aggregated comparisons but no further ablation of the baseline algorithms themselves is reported beyond the comparative tables.",
            "uuid": "e8741.5",
            "source_info": {
                "paper_title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Re-rest: Reflectionreinforced self-training for language agents",
            "rating": 2,
            "sanitized_title": "rerest_reflectionreinforced_selftraining_for_language_agents"
        },
        {
            "paper_title": "Self-play fine-tuning converts weak language models to strong language models",
            "rating": 1,
            "sanitized_title": "selfplay_finetuning_converts_weak_language_models_to_strong_language_models"
        },
        {
            "paper_title": "Self-reflection in llm agents: Effects on problem-solving performance",
            "rating": 2,
            "sanitized_title": "selfreflection_in_llm_agents_effects_on_problemsolving_performance"
        },
        {
            "paper_title": "Recursive introspection: Teaching language model agents how to self-improve",
            "rating": 2,
            "sanitized_title": "recursive_introspection_teaching_language_model_agents_how_to_selfimprove"
        },
        {
            "paper_title": "Reflection and learning: Characteristics, obstacles, and implications",
            "rating": 1,
            "sanitized_title": "reflection_and_learning_characteristics_obstacles_and_implications"
        },
        {
            "paper_title": "Reflection-tuning: Data recycling improves llm instruction-tuning",
            "rating": 1,
            "sanitized_title": "reflectiontuning_data_recycling_improves_llm_instructiontuning"
        }
    ],
    "cost": 0.0194095,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection
22 May 2025</p>
<p>Jiaqi Li lijiaqi@bigai.ai 
State Key Laboratory of General Artificial Intellligence
BIGAI</p>
<p>Xinyi Dong 
Peking University</p>
<p>Yang Liu 
State Key Laboratory of General Artificial Intellligence
BIGAI</p>
<p>Zhizhuo Yang 
Peking University</p>
<p>Quansen Wang 
State Key Laboratory of General Artificial Intellligence
BIGAI</p>
<p>Peking University</p>
<p>Xiaobo Wang 
State Key Laboratory of General Artificial Intellligence
BIGAI</p>
<p>Songchun Zhu 
State Key Laboratory of General Artificial Intellligence
BIGAI</p>
<p>Peking University</p>
<p>Zixia Jia jiazixia@bigai.ai 
State Key Laboratory of General Artificial Intellligence
BIGAI</p>
<p>Zilong Zheng zlzheng@bigai.ai 
State Key Laboratory of General Artificial Intellligence
BIGAI</p>
<p>ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection
22 May 2025F686E34C7E26579D6EA29408741EA9C9arXiv:2505.16475v1[cs.AI]
We present a novel pipeline ReflectEvo to demonstrate that small language models (SLMs) can enhance meta introspection through reflection learning.This process iteratively generates self-reflection for self-training, fostering a continuous and self-evolving process.Leveraging this pipeline, we construct ReflectEvo-460k, a large scale, comprehensive self-generated reflection dataset with broadened instructions and diverse multi-domain tasks.Building upon this dataset, we demonstrate the effectiveness of reflection learning to improve SLMs' reasoning abilities using SFT and DPO with remarkable performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral from 44.4% to 71.1%.It validates that ReflectEvo can rival or even surpass the reasoning capability of the three prominent open-sourced models on BIG-bench without distillation from superior models or finegrained human annotation.We further conduct a deeper analysis on the high quality of selfgenerated reflections and their impact on error localization and correction.Our work highlights the potential of continuously enhancing the reasoning performance of SLMs through iterative reflection learning in the long run.</p>
<p>Introduction</p>
<p>Self-reflection involves meditating on, examining, and evaluating one's behaviors, thoughts, motivations, and desires (Atkins and Murphy, 1993;Von Wright, 1992;Denton, 2011).Typically, it inspects the reasoning process leading to the current solution, identifies errors in each step, generates critiques on the causes of the failure, and offers advice for refining the solution to improve the problemsolving performance of Large Language Models (LLMs) (Welleck et al., 2022;Ferraz et al., 2024;Li et al., 2024a;Tong Wu, 2024).Unlike the paradigm of learning directly from the reasoning process and final answer, we refer to it as the process of humanlike meta introspection, which explicitly generates self-reflection, providing textual differentiation and gradients as clear critiques and guidance on what to learn and how to improve based on the current state.</p>
<p>Recent research has demonstrated that LLMs can self-improve through their intrinsic capability of self-reflection (Huang et al., 2022;Renze and Guven, 2024;Guo et al., 2025;Wang et al., 2024b).However, conventional approaches rely closely on LLMs with large model sizes or supervision distilled from a superior model.In this study, we challenge whether the self-reflection capability of SLMs can be learned effectively from reflection data.However, it usually requires highcost on fine-grained human annotation to acquire high-quality data for fine-tuning and is impractical to scale.Therefore, we are also curious whether it is possible to effectively utilize both high-and low-quality self-generated data from weaker models for reflection learning.With this in mind, we aim to investigate the effectiveness of reflection learning via self-training (Luong et al., 2024;Qu et al., 2024;Pang et al., 2023;Tang et al., 2024) and further validate that the improvement of selfreflection can further strengthen LLM's inherent reasoning capabilities across various methods and tasks with more interpretability and generalization.We believe that this paradigm can act as a plug-andplay enhancement for various reasoning methods, which emulates human learning through a slower and deeper thought process that iteratively and ultimately derives self-evolution (Li et al., 2023;He et al., 2024;Li et al., 2024b;Tang et al., 2024Tang et al., , 2023)).</p>
<p>Therefore, in this paper, we propose a novel pipeline ReflectEvo (Sec.2), to automatically generate self-reflection data and leverage self-training to enhance LLM's reflection capability.To the best of our knowledge, we are the first to demonstrate the potential of meta introspection of LLMs that are asked to explicitly generate reflection as an in- termediate step-by-step process supervision rather than directly mapping an initial solution to a revised solution.</p>
<p>Building on this pipeline, we curate a largescale, diverse, and unsupervised reflection learning dataset ReflectEvo-460k containing 460k reflection samples derived from 17 source datasets spanning 10 tasks and domains.We explore the diversity of reflection instructions and bootstrap multiple comparative reflections conditioned on the same question and initial solution.Based on the data, we develop reflection learning (Sec.3) to further improve the self-reflection and self-correction capabilities of LLMs.</p>
<p>The evaluation results validate the effectiveness of reflection learning in boosting the reasoning of weak models.It shows significant improvements on Llama-3, exceeding the original base model by 10% on average tasks and outperforming its strongest counterpart with model size ×8.We conduct a deeper analysis of the self-generated reflection data including various error types identified from the reflection and observe their gains on corrected answers.</p>
<p>In summary, our main contributions are: • Novel Pipeline for Self-Reflection Generation: We propose ReflectEvo for automatic selfreflection generation and curation, which is the first to explore meta introspection of SLMs.</p>
<p>The ReflectEvo Generation Scheme</p>
<p>In this section, we introduce the end-to-end pipeline ReflectEvo for collecting self-generated reflections as the training data for Sec. 3, leveraging the inherent ability of SLMs (see Fig. 1).</p>
<p>Problem Definition and Prelinminary</p>
<p>Given a question q and its ground truth answer a * , the answer of the LLM after reasoning is denoted as a followed by its corresponding verbal feedback f from the environment, where f represents the evaluation function that assesses whether an answer is correct or incorrect by comparing it to the reference answer a * .The self-reflection r of an LLM explicitly locates and analyzes errors in a and makes further plans to mitigate the errors.Based on r and the context provided in the previous stage, the LLM is then asked to revise its original answer a to obtain â and solve q as correctly as possible.</p>
<p>Reflection Generation</p>
<p>Step 1: Collection of instruction pool To enhance the effectiveness and quality of the generated reflections r, we design instructions that target three key stages of reflection and correction, as Step 2: Data generation Based on the instructions outlined in Step 1, we introduce two components for reflection generation: a Generator G (reasoning model) that generates the initial answer with its reasoning process and a Reflector R (reflection model) that improves the incorrect answer through self-reflection and self-correction.</p>
<p>Generator G Given a q, G is built upon a base LLM instructed to generate interleaved thoughts and an initial answer G(a|q).It is implemented as described in ReAct (Yao et al., 2022), as the first step for self-reflection.We obtain the external environment feedback f by evaluating the correctness of a as a verifier.f is a binary signal "correct/incorrect" with limited information, which is usually the case in real scenarios, eliminating the need for enriched feedback from humans or more powerful models.If correct, a is directly used as the final answer.If incorrect, R is used to revise the solution iteratively.In this paper, we perform self-reflection once to maximize the efficiency of self-generated data; however, this approach can be extended to multiple iterations in future studies.</p>
<p>Reflector R We use exactly the same base LLM as G for R. The generation process for R is decom-posed into two phases: self-reflection and selfcorrection.Self-reflection generates R(r|q, a, f ) to identify errors in the reasoning process and conduct a deeper analysis of the causes.Self-correction refines a as R(â|q, a, f, r).To enrich the selftraining data, we sample k solution {r j , âj } k j=1 for each {q, a, f } conditioned on one specific prompt using reject sampling (Liu et al., 2023) to enrich the self-training data.We vary the prompts selected from the instruction pool to generate diverse selfreflection samples.</p>
<p>Reflection Curation</p>
<p>After the above-mentioned process, we obtain a reflection training set with M (N * k * m) samples:
D = {qi, ai, fi, (ri,j, âi,j) k * m j=1 } N i=1 ,(1)
where N is the number of QA pairs in D, m is the number of reflection instructions from pool, and k is the value of reject sampling.We aim to further curate the data for reflection learning as follows.</p>
<p>First, we filter r to include those followed by the correct â, indicating that these reflections are of high quality for error correction, denoted as D + :
D + = {(qi, ai, fi, ri, âi) | (âi = a * )} |D| i=1
(2) Subsequently, we leverage GPT-4o (Hurst et al., 2024) as a stronger teacher model to further select preferred reflection data from D + to create pairwise data, denoted as D pref :
D pref = {(qi, ai, fi, [y cho i , y rej i ]) | ∃ y cho i , y rej i } |D + | i=1 ,(3)
where y = (r, â) is the reflection and corresponding corrected answer.y cho and y rej are solutions randomly selected for each {q, a, f } whose r is chosen and rejected, respectively, by GPT-4o.</p>
<p>To fully utilize low-quality reflection data followed by âi that is still judged to be incorrect, we enrich the self-training data by incorporating both positive and negative samples as pairwise data for each {q, a, f }, denoted as D ± .
D ± = {(qi, ai, fi, [y + i , y − i ]) | ∃ y + i , y − i } |D| i=1 ,(4)
where y + and y − are solutions whose â is evaluated as correct or incorrect by a * .(Liu et al., 2020), MATH (Hendrycks et al., 2021), MBPP (Austin et al., 2021), and BIG-bench (bench authors, 2023), spanning diverse domains and categories.The Statistics of the dataset are shown in Tab. 1 and Fig. 2a.We use three commonly used SLMs including Llama-3-8B (Dubey et al., 2024), Mistral-7B (Jiang et al., 2023) and Gemma-2-9B (Team et al., 2024) for the entire process of data generation, training, and test.Implementation details and instructions are provided in Appendices B.1 and C.</p>
<p>Reflection Learning on Self-Generated Data</p>
<p>In this section, we further investigate the effectiveness of reflection learning on the reflector R by adopting self-training on ReflectEvo-460k using supervised fine-tuning (SFT) (Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2024).</p>
<p>Reflection Learning</p>
<p>We use SFT on D + in two different settings below.This strengthens the model to better leverage reflections as intermediate thoughts leading to positive â for refinement.</p>
<p>Setting 1: We train the capacity of self-reflection and self-correction in one stage:
L1 = −E (q,a,f,r,â)∼D + log R((r, â) | q, a, f )(5)
Setting 2: We train the capacity of self-reflection and self-correction respectively in two stages:
L2.1 = −E (q,a,f,r)∼D + log R(r | q, a, f ) (6) L2.2 = −E (q,a,f,r,â)∼D + log R(â | q, a, f, r)(7)
Inspired by error-driven learning from humans, we also leverage negative samples â− that comprise a large portion of D ± and offer valuable insights for model enhancement.In addition, we assume GPT-4o with better self-reflection, which is required for reflection preference annotation as D pref , guiding SLMs to continuously refine reflections.We use preference learning through DPO on the aforementioned pairwise data to better judge and distinguish high-quality reflections from suboptimal ones in the following settings.</p>
<p>Setting 3: We train self-reflection only on D ± :
L3 = −E (x,r + ,r − )∼D ± log σ[r θ (x, r + ) − r θ (x, r − )] (8) r θ (x, r) = β log π θ (r | x) πref(r | x) , x = (q, a, f )(9)
Setting 4: We train self-reflection only on D pref :
L4 = −E (x,r cho ,r rej )∼D pref log σ<a href="10">r θ (x, r cho ) − r θ (x, r rej )</a>
where R(•) is the policy model π θ and G(•) is the reference model π ref .</p>
<p>σ is the logistic function and β is a hyperparameter that controls the proximity to the reference policy G(•) in both settings 3 and 4. The objective is to steer R(•) towards increasing the likelihood of r + with the correct solutions â or chosen r cho and decreasing the likelihood of r − with incorrect solutions â or rejected r rej for given (q, a, f ).More details can be found in Appendix B.2.</p>
<p>Inference</p>
<p>During inference, the process follows the same steps as those of reflection data generation in Sec.2.2.We use the model after reflection learning as a reflector at the inference time for self-reflection and correction.It can be implemented as a multiturn rollout that terminates either when the current a is judged to be correct or when it reaches the predefined maximum number of turns (two turns in our setting using twice QA with one intermediate reflection).</p>
<p>Experiments 4.1 Performance Learning on ReflectEvo</p>
<p>We measure the performance of self-reflection by adopting the following metrics: 1) Acc@t1: the model's accuracy in the first turn; 2) Acc@t2: the model's accuracy in the second turn; 3) ∆(t1,t2): accuracy improvement between the first and second turns measuring the efficacy of self-reflection.</p>
<p>We compare three main methods in our experiments, including prompt-based QA with or without reflection without training, SFT training with direct answers, and self-training based reflection learning introduced in Sec. 3 from Setting 1 to Setting 4, noted as one-stage w/ D + , two-stage w/ D + , w/ D ± and w/ D pref respectively.Overall Performance on Different Tasks Tab. 2 illustrates the overall performance on ReflectEvo.We discard the self-generated reflection data by Mistral on MATH due to its extremely low quality.We observe that LLMs gain more from promptbased reflection, whereas SLMs show either minor improvements or degradation.This is primarily because without specialized training, SLMs inherently generate low-quality reflections and fail to leverage feedback effectively for self-correction.For comparison, experiments on our self-training methods show significant improvements in both models and various tasks.Specifically, it achieves over 20% in ∆(t1,t2) for Llama-3 on MBPP and BIG-bench as well as Mistral on LogiQA and BIGbench.Notably, all three evaluated models outperform their stronger model using ReflectEvo on BIG-bench.This indicates that different models and tasks benefit greatly from the four self-training methods, even surpassing the SFT on answers without step-by-step reasoning process, which paves the way for broader applications and scenarios for various SLMs.Fig. 3 provides an in-depth analysis on the reflection learning across tasks.Our method significantly contributes to various tasks, including reasoning, math, QA, and comprehension, with an average of 22% in ∆(t1,t2).For coding, it only improves to a certain degree probably due to the lack of finegrained step-by-step critiques on the erroneous solutions for reflection training on models that are not specialized in coding.</p>
<p>Effect of Reflection from Teacher Model</p>
<p>To investigate the influence of reflection sources, we compare different reflections generated by the SLM itself and a more advanced model like GPT-4o which acts as a teacher model with greater knowledge and reasoning capabilities in Tab. 3. Reflections from both models strengthen the ∆(t1,t2) of QA performance after tuning under different settings proposed in Sec. 3, while the self-generated data require less cost and resources in practice.To our expectation, reflections from the teacher model yields more obvious improvements underscoring the benefits of high-quality reflection data generation and selection for further improvement.Scaling Multi-turn Self-reflection We further extend the application of self-reflection to multi-
LogiQA MATH MBPP BIG-bench
Acc@t1 Acc@t2 ∆(t1,t2) Acc@t1 Acc@t2 ∆(t1,t2) Acc@t1 Acc@t2 ∆(t1,t2) Acc@t1 Acc@t2 ∆(t1,t2)   turn QA in Fig. 4. To our expectation, the results demonstrate a consistent improvement with increasing turns of reflection on different tasks.BIG-bench exhibits the most significant improvement, surpassing 80% accuracy after six turns and LogiQA also shows a notable upward trend, highlighting the effectiveness of iterative refinement.MBPP and MATH display relatively modest improvements with gradual increase, which suggesting that the impact of self-reflection learning is broadly beneficial but varies between tasks.It is encouraged to investigate the underlying factors that contribute to these differences to further enhance performance in various tasks.
Meta-Llama-3-8B-Instruct Prompt based → w/
Generalization across Different Tasks and Models We conduct deeper studies on the generalization of the self-reflection capability after tuning across different tasks (Tab.4) and models (Tab.5).</p>
<p>Our findings reveal that the benefits of reflection learning generalize across tasks, particularly for LogiQA and BIG-bench with 10% increase, which commonly require strong reasoning abilities from LLMs.Due to the divergence of MATH and MBPP, there is merely improvement when trained on reflections generated from the other three datasets.</p>
<p>We observe that all the test models in Tab. 5 benefit from the reflector after tuning for error correction in Acc@t2, especially for initial solutions from different generators.For Mistral and Gemma, even with a minor decrease compared with the corresponding results in Tab. 2 and Tab. 9, the result on these two models highlights the potential of our pipeline across different models and demonstrates the effectiveness of reflectors when applied to various generators.</p>
<p>LogiQA MATH BIG-bench MBPP Acc@t1 Acc@t2 ∆(t1,t2) Acc@t1 Acc@t2 ∆(t1,t2) Acc@t1 Acc@t2 ∆(t1,t2) Acc@t1 Acc@t2 ∆(t1,t2)  In Tab. 6, we further explore whether the selfreflection data of one LLM can be beneficial for the other.Compared with Tab. 2 in our paper, we find that the reflection data generated by LlaMA-3.1-8B is helpful for Mistral-7B on reflection learning with comparable or even better performance.It indicates that our dataset ReflectEvo-460k could be reusable for the community for future studies.Effect of Different Verifiers on Self-Reflection In this paper, self-reflection is performed only when the model's answer is verified as incorrect using the ground truth.Another potential approach is to train the model itself as a verifier or use an external reward function to score the model's answer based on a predefined threshold.We compare the effects of using oracle ground truth and self-judgments generated by the baseline model as verifiers in Tab. 7.For both verifiers, reflection learning improves Acc@t1 by an average of 13+% and enhances Acc@t2 by up to 7% compared with the untuned version.Although the baseline model, without specialized training, exhibits occasional misjudgments, its verification process results in minor performance degradation on the advantage of reflection learning.We leave this a direction for further exploration on the optimized verifiers in an   end-to-end pipeline.</p>
<p>In Tab. 8, we make further experiments by comparing with three well-acknowledged baselines for self-improvement through reflection or correction.To make the comparison fair, the external feedback used in all the experiments is only a binary signal "correct/incorrect" without further explanation or given ground truth.We follow the evaluation setting in the original paper.Comparing with STaR and Re-ReST, Ours benefit from improvement over turns.Our methods achieves much higher reasoning performance after self-reflection, which emulates human learning through a slower and deeper thought process that iteratively and ultimately derives self-evolution</p>
<p>In-depth Analysis on Reflection</p>
<p>Error Types Identified by Reflection To dissect the intrinsic properties of our reflection data, we analyze the error types in the initial thoughts specified by the reflection across all test sets.Potential error labels are generated heuristically by auto-tagging with GPT-4o and then calibrated by human annotators, achieving Cohen's kappa of 51.18% with moderate agreement (Landis and Koch, 1977) indi-Question: Simplify the expression $(9x^2+3x+7)+(3x^2+7x^5+2)$.Express your answer as a polynomial with terms arranged in decreasing order of degree.</p>
<p>Thought: We can combine like terms in the given expression.The terms with $x^2$ are $9x^2+3x^2=12x^2$, the terms with $x$ are $3x+3x^5+7x=3x+3x^5+7x$, and the constant term is $7+2=9$.Therefore, the simplified expression is $12x^2+3x^5+9$.</p>
<p>Reflection:</p>
<p>The failure occurred because I incorrectly combined the terms with x^5 by adding their coefficients, resulting in an invalid term, 3x^5.I also mistakenly added 9, not recognizing that there was no constant term in the original expression.New plan: Moving forward, I will first combine the terms with the highest power of x, then handle the terms with the next highest power, and finally deal with the constants.I will ensure terms are arranged in decreasing order of degree to maintain accuracy.</p>
<p>Corrected Thought: To simplify the expression $(9x^2 + 3x + 7) + (3x^2 + 7x^5 + 2)$, I first identified the terms with the highest powers of x, $7x^5$ and $0x^5$, and kept them separate since they have different coefficients.Then, I added the $9x^2$ and $3x^2$ terms, which resulted in $12x^2$.For the constant terms, $7$ and $2$, I added them to get $9$.Finally, I arranged the terms in decreasing order of degree: $7x^5 + 12x^2 + 3x + 9$.</p>
<p>Question: $\left(\frac{i}{4}\right)^4$.</p>
<p>Thought: We have $\left(\frac{i}{4}\right)^4 = \left(\frac{i^2} {16}\right) = \left(\frac{-1}{16}\right) = \boxed{-\frac{1}{16}}.</p>
<p>Reflection: A possible reason for failure is that I did not consider the fact that $i^2=-1$, but instead used the property $i^2=1$.This led me to inco -rrectly simplify the expression as $(i/4)^4=(i^2/16)=(-1/16)$.New Plan: To avoid this mistake, I will carefully evaluate each power of I separately before simplifying the expression.Specifically, I will use the property $i^n=i^{n\bmod 4}$, where n mod 4 gives the remainder when n is divided by 4.Then, I will substitute the value of i raised to the appropriate power into the original expression.</p>
<p>Corrected Thought: The mistake in the initial approach was using the incorre -ct property $i^2=1$ instead of $i^2=-1$, leading to an incorrect simplification.</p>
<p>To fix this, we use the fact that $i^n = i^{n \bmod 4}$ and evaluate powers of $i$ correctly.To find $\left(\frac{i}{4}\right)^4$, we can simplify step by step: [ \left(\frac{i}{4}\right)^4 = \frac{1}{16}(1)(-1)(1)(-1) = \frac{1}{16}.] Thus, the correct result is $\boxed{\frac{1}{16}}$.</p>
<p>FALSE TO TRUE</p>
<p>Thought &amp; reflection pathway</p>
<p>Correct constituents in thought / reflection Errorneous constituents in thought / reflection Key snippets of reflection in pathway  cating high annotation quality.Fig. 2b shows five coarse-grained and nine finegrained error types identified through human calibration.The most common errors are Logic and Reasoning Errors (88.4%) and Instruction Violation (47.9%), indicating that math and logic issues were the primary causes.We also provide detailed error distributions for the different subsets.MATH has a higher percentage of Calculation Errors (20.8%) than the other subsets, whereas COQA has more Context misinterpretation (43.1%).This shows that our method provides tailored reflections for specific domains rather than superficial or general advice.Correlation Analysis in Reflection We calculate the correlation between reflection and secondturn corrected thoughts, and we assess the association between the correlation and Acc@t2 after self-correction.Empirically, we hypothesize that they have a linear relationship, and we select the Pearson correlation coefficient by computing the semantic similarity for each pair of data (see the details in Fig. 7).As we have seen, reflection learning can improve the ability of models to correct errors; we argue that if reflection is indeed specific to the error in thought, then task performance should intuitively be enhanced as the correlation between reflection and corrected thought increases.</p>
<p>Measuring with the Pearson coefficient, Fig. 6 and Fig. 7 show that StrategyQA, Social IQa, Vita-minC, and SQuAD all have a clear linear relationship between the performance and the correlation of reflection -second-turn thought, while MATH and MBPP exhibit irrelevant tendency or show a slightly negative correlation implying their desire data of fine-grained reflection.Comparing the blue and red correlation curves, we find that more similarity between the reflection and corrected thought, more effective correction (i.e., higher performance) that outperforms the vanilla model.Case Studies We perform case studies to see how reflection interacts with the thought process by making critiques and refinements in Fig. 5.We random sampled 100 cases from the MATH test set and display two of them.In the case "False to True", reflection precisely recaps the key causes of error and explicitly bridges the logical pathway between the initial thought and the corrected one, which finally results in the correct answer.In contrast, we find that even tiny erroneous constituent in the reflection may lead to a false reasoning thought and final answer.It validates that high-quality reflection is helpful for incentivizing the model to generate thought with correct answer while flawed reflection still lead to repeated errors after self-correction, which aligns with similar findings on the impact of the reasoning steps in Shinn et al. ( 2024) and Zelikman et al. (2024).</p>
<p>Related Work</p>
<p>Self-training and Self-Improvement Selftraining allows a model to learn from its own outputs, reducing its reliance on human-annotated data or superior models (Zelikman et al., 2022;Yuan et al., 2024;Chen et al., 2024).Previous research has primarily concentrated on enhancing models' reasoning abilities through SFT (Yuan et al., 2023) with positive samples or preference learning using both positive and negative samples to potentially leverage valuable information in incorrect solutions and recent advances also extend self-training to agentic scenarios (Wang et al., 2024a;Wallace et al., 2024;Gulcehre et al., 2023;Song et al., 2024;Motwani et al., 2024;Li et al., 2024b).We further advocate reducing reliance on resource-heavy rationale annotations via self-training for SLMs.Learning for Self-reflection Recent research highlights the significant benefits of integrating self-reflection into LLMs to enhance their reasoning and problem-solving capabilities, by iteratively refining their responses (Kumar et al., 2024;Cheng et al., 2024;Qu et al., 2024;Yao et al., 2023;Zhou et al., 2024;Liang et al., 2024;Moskvoretskii et al., 2025).Shinn et al. ( 2024) reinforces the language agent to verbally reflect on task feedback and induce better plans in subsequent trials.Dou et al. (2024) employs the low-quality outputs generated from the weak model iteratively by fine-tuning the reflection module for self-refinement.Zhang et al. (2024) further validates that SLMs have the ability of self-correction on reasoning tasks by accumulating high-quality critique-correction data.We pioneer the exploration of reflection learning on self-generated data.</p>
<p>Conclusion</p>
<p>We propose ReflectEvo to enhance SLMs through reflection learning by iteratively generating selftraining data, which achieves substantial performance improvements, even surpassing much larger models highlighting its generalization across various models and tasks for future research.</p>
<p>Limitations</p>
<p>Despite the promising results of ReflectEvo through reflection learning, there are several limitations to our work.Firstly, the quality of the selfgenerated reflection data is highly dependent on the initial reasoning ability of the SLMs.Models with inherently weak reasoning capabilities may struggle to produce high-quality reflections, which in turn limits the effectiveness of the self-training process.Secondly, while our pipeline demonstrates significant improvements in certain tasks, tasks such as coding and mathematics require more specialized knowledge and step-by-step critiques than reasoning and comprehension tasks.Additionally, future work could explore more sophisticated feedback mechanisms with optimized verifiers or reward functions during inference to enhance the reflection learning process.Addressing these limitations is crucial for further advancing the selfreflection capabilities of SLMs.</p>
<p>Ethics Statement</p>
<p>We adhere to ethical principles to ensure the responsible development and application of our proposed techniques.Our work focuses on enhancing the self-reflection abilities of SLMs without directly involving human subjects or sensitive information.We acknowledge the potential broader impacts of our research, recognize the environmental and computational costs associated with LLM training, and strive to optimize our methods for efficiency.</p>
<p>LogiQA</p>
<p>MATH MBPP BIG-bench</p>
<p>Acc@t1 Acc@t2 ∆(t1,t2) Acc@t1 Acc@t2 ∆(t1,t2) Acc@t1 Acc@t2 ∆(t1,t2) Acc@t1 Acc@t2 ∆(t1,t2)  A Further analyais and results</p>
<p>Tab. 9 indicates that BIG-bench gains more from reflection tuning with Acc@t2 = 78.4% with a substantial increase of +17.2% compared with other tasks and the baseline methods.However, our method on Gemma-2 shows marginal improvement compared with Llama-3 and Mistral probably due to its inherent strong reasoning capability (comparable performance on different models in sizes of 9B &amp; 27B).It may either need selection of higher-quality reflection data or supervision from superior models and further optimization on the training methods for reflection enhancement.Due to the defect of SFT training without step-by-step reasoning process and the limited number of training data, the SFT performance of MATH and MBPP degrade due to the nature of fast thinking than thoses with reflection.</p>
<p>B Implementation Details</p>
<p>B.1 Reflection generation</p>
<p>In this paper, we conduct the self-reflection once during the process of the two turns of reasoning and answering for both data generation and inference across most experiments.The generalization performance of multi-turn self-reflection can be found in Fig. 4. The number of reject sampling k is 2. To validate the effectiveness of reflection tuning on various tasks, we incorporate 14 datasets selected from BIG-bench besides LogiQA, MATH and MBPP.Those datasets are delicately selected to focus more on the comprehension and reasoning abilities across diverse domains, comprising a comprehensive collection of dataset.The datasets includes: Commonsense Reasoning (RiddleSense, TimeDial, Known Unknowns), Social Reasoning (Social IQa, Implicit Interpersonal Relations), Reading Comprehension (VitaminC, SQuADShifts), Logical Reasoning (StrategyQA, Analytic Entailment), Contextual QA (CoQA Conversational Question Answering), Context Free QA (Truthful QA), Causal Reasoning (Causal Judgment, Cause and Effect), and Physics Reasoning (Physical Intuition).For datasets with more than 1000 samples, we randomly select 1000 QA pairs; for datasets with fewer than 1000 samples, we retain the entire original set.</p>
<p>Each reflection instruction consists of the three stages introduced in step 1 in Sec. 2 and different variants of prompts used in each stage can be seen Appendix C.1.The combination of them forms a diverse, comprehensive instruction pool with 32 (2<em>8</em>2) instructions used in step 2. For each dataset, we random select 5 or 6 of the instructions (M ) to generate the reflections in ReflectEvo-460k considering the data generation efficiency.</p>
<p>For each task, we use corresponding subset for training without using the whole ReflectEvo-460k.For example, we use the training set of LogiQA for reflection generation and learning for LlaMA-3.1-8B and evaluate the same model on the test set of LogiQA in the experiments.</p>
<p>Task Performance vs. Thought-Reflection Correlation</p>
<p>Correlation Coefficient (%) Performance (%)   2-6.Review your solution to ensure that it is relevant to the question and addresses each aspect of the question.2-7.Review your solution to ensure it conforms to the required format and guidelines in a well-organized structure.</p>
<p>2-8.Review your solution to ensure all provided facts are accurate and up to date.</p>
<h1>Stage 3: Outline strategies and plans on error correction and mitigation 3-1.Outline a high-level plan explaining how these changes will mitigate similar issues.3-2.Outline a low-level plan proposing specific strategies or corrections to address these issues.</h1>
<p>Please follow the instructions without any additional introductory or concluding statements.Do not provide the answer directly.You will be punished to output more than 100 words.</p>
<p>C.2 Self-reflection for Reflector</p>
<p>Instruction: You are an advanced reasoning agent that can improve based on self-reflection.You will be given a previous reasoning trial in which you were given a question to answer.You were unsuccessful in answering the question.In a few sentences, diagnose a possible reason for failure and devise a new, concise, high-level plan that aims to mitigate the same failure.Use complete sentences.C.5 Self-reflection and correct in one stage Instruction: You are an advanced reasoning agent that can improve based on self-reflection.You will be given a previous reasoning trial in which you were given a question to answer.You were unsuccessful in answering the question.In a few sentences, diagnose a possible reason for failure and devise a new, concise, high-level plan that aims to mitigate the same failure.Use complete sentences.Your response should be either "Student A" or "Student B" without providing any explanation or other words for your choice.Do NOT say both/neither are good.</p>
<p>C.8 Heuristic Error Constituent Annotation for Reflection</p>
<p>Instruction: You are a professional data annotator specializing in reasoning and chain-ofthought rationale analysis.Your task is to analyze the thought process provided and identify any fine-grained labels based on the given reflection and error taxonomy.</p>
<p>Figure 1 :
1
Figure 1: Overview pipeline of ReflectEvo.</p>
<p>• Large-Scale and Diverse Self-generated Reflection Dataset: We curate a comprehensive reflection training set ReflectEvo-460K from multiple data sources and tasks including various reflection instructions and comparative samples.• Learning Reflection Via Self-training: We develop four settings of reflection learning methods to effectively improve self-reflection and selfcorrection based on SFT and DPO, which significantly boost the reasoning abilities of SLMs as well as surpassing their stronger counterparts.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Performance training with ReflectEvo across different tasks on Llama-3-8B.</p>
<p>Figure 5 :
5
Figure 5: Qualitative examples from the MATH."False to True" and a "False to False" stand for successful and failed correction in the second turn respectively.The key snippets highlighted in green, red and yellow indicate correct, erroneous thought and reflection respectively.</p>
<p>Figure 6 :
6
Figure 6: Task performance (Acc@t2) versus the correlation between reflection and the second-turn thought.</p>
<p>Figure 7 :
7
Figure 7: Task performance (Acc@t2) versus the correlation between reflection and second-turn thoughts for Llama-3-8B with self-training reflection (blue . . . .dots and curve) and prompt-based reflection (red . . . . .dots and curve).The ideal correlation (green dashed curve) denotes a standard linear tendency for comparison purposes, and the black dashed line represents Llama-3-8B without reflection.Note that the y-values of the spots denote the average performance (axis-y), where an array of test data points is located in a specific interval of the correlation coefficient (axis-x), and the correlation coefficient of these spots is also averaged by the values in the same interval.</p>
<p>Figure 8 :
8
Figure8: Correlation of reflection between each pair of tasks.We obtain the semantic representation for all reflections via the Nv-Embed-v2 model(Lee et al., 2025) and calculate the Spearman correlation between each pair of tasks.The results are as follows: 1) Logical Reasoning has a moderate correlation with all tasks, which indicates that logic is a fundamental ability that supports other tasks; 2) Coding and Math have a high correlation, implying that similar thinking patterns are required for handling math and coding problems; and 3) Commonsense Reasoning and Social Reasoning show low correlation (0.14), suggesting that these abilities might require different cognitive skills.</p>
<p>Question: {Question} Previous trial and your incorrect solution: {Scratchpad} C.3 Reasoning for Generator Instruction: In this task, you are required to solve a question with interleaving Thought, Action, and Observation steps.Thought allows you to reason and analyze the current situation.Action calls the 'Finish' function and fill in the answer in [ ] to finish the task after Thought.The observations will be provided to you automatically after you action.You can think step-by-step to reach the answer.Here are some examples: {Examples} (END OF EXAMPLES) You are solving the following question: {Question} Below is your progress so far: (BEGIN) {Scratchpad} (END) Please complete the current step.C.4 Self-correct for Reflector Instruction: In this task, you are required to solve a question with interleaving Thought, Action, and Observation steps.Thought allows you to reason and analyze the current situation.Action calls the 'Finish' function and fill in the answer in [] to finish the task after Thought.The observations will be provided to you automatically after you action.You can think step-by-step to reach the answer.Here are some examples: {Examples} (END OF EXAMPLES) You are solving the following question: {Question} Below is your previous reflection that helps to revise the incorrect solutions and correctly answer the question.It localizes the errors, summarizes the potential reasons for your failure and outlines a plan to mitigate the same failure: {Reflections} Below is your progress so far: (BEGIN) {Scratchpad} (END) Please complete the current step.</p>
<p>Question: {Question} Previous trial and your incorrect solution: {Scratchpad} Based on your self-reflection, you can think step-by-step to generate a new answer to the question.Call the 'Finish' function and fill in the answer in [ ] to finish the task.C.6 Reasoning for direct QA for SFT Instruction: In this task, you are required to solve a question by generating the final answer directly.Call the 'Finish' function and fill in the answer in [ ] to finish the task.Here are some examples: {Examples} (END OF EXAMPLES) You are solving the following question: {Question} C.7 Reflection preference annotation by GPT Instruction: You are a helpful assistant in evaluating the quality of reflections on an unsuccessful attempt to answer a question.You will be provided with: Question: {Question} Groundtruth to the question: {Answer} Initial student's chain of thought and answer: {Scratchpad} Student A's reflection: {Reflections 1} Student B's reflection: {Reflections 2} Student A and Student B have both reflected on the initial student's unsuccessful attempt to answer the question.Above are their reflections that diagnose possible reasons for failure or devise a better plan to mitigate the same failure.Please determine which student's reflection is better.</p>
<p>The reasoning steps taken by a human or model to arrive at an answer.-Reflection: The self-reflection of the human or model on the reasoning thought process.[Error Type(s) Assigned] -Rationale: [Explanation for label assignment, with specific examples]</p>
<p>Table 1 :
1
Statistics of ReflectEvo-460k.The average length in the table is computed by tokens.
FeatureLogical ReasoningMathematicsCodingContextual QAContext-Free QAReading ComprehensionCommonsense ReasoningSocial ReasoningCausal ReasoningPhysics ReasoningTotal# Reflection training samples253,40592,9679,12519,3993,62432,13522,04419,1758,7571,168461,799# Q&amp;A-Reflection samples164,746106,4347,52017,4482,94020,76010,01211,4043,212468344,944% Correct after reflection16.6010.7715.314.269.6612.3933.8819.9939.9838.4613.93# Avg. question length14077993352001482195211839130# Avg. answer length (turn 1)1312671879111882158110118112189# Avg. answer length (turn 2)163299202119135116159133130124237# Avg. reflection length238222254267251261268252259256250defined below: i. Verify the failed solution. It ana-lyzes the initial solution by tracing and examiningthe reasoning process with or without step-by-stepverification. ii. Locate errors and diagnose po-tential reasons. It points out errors in specific rea-soning steps and identifies the causes (Zeng et al.,2024). We delicately design prompts to mitigatethe most common error types (Li et al., 2024c), in-cluding mathematical (calculation &amp; algorithm) er-rors, logic and reasoning errors (flawed rationale &amp;internal inconsistency), instruction violation (con-text misinterpretation, incomplete or irrelevant re-
sponse &amp; format discrepancy), factual errors.They are explicitly specified in the instructions for error elimination and accurate fault localization.iii.Outline strategies and plans for error correction and mitigation.It provides strategies and guidance to address the error by proposing a high or low-level plan to mitigate similar issues in the future.</p>
<p>Logica l Reaso ning (47.8% ) Coding (2.2%) Con textu al QA (5.1% ) Phy sics Rea son ing (0.1 %) R ea di ng C om pr eh en si on (6 .0 % ) C o m m o n se n se R ea so n in g (2 .9 % ) C on te xt Fr ee Q A (0 .9 % ) So ci al R ea so ni ng (3 .3 % ) Ca usa l Re aso nin g (0. 9% ) M a t h e m a t i c s ( 3 0 .9 % ) Log iQA (44 .99 %) Stra tegy QA (2.64 %) MBPP (2.18%) CoQ A (5.06 %) Vit am inC (1. 82% ) SQ uA D Sh if ts (4 .2 0% ) T im eD ia l (2 .6 4% ) So cia l IQ a (2 .99 % )
43.1%55.3%460k ReflectEvo7.3%6.1%M A T H ( 3 0 .8 6 % )12.2%35.8%13.1%22.1%(a)</p>
<p>Error Type Distribution by Taxonomy
Error TaxonomyMathematical Errors (7.3%)-Calculation Error (6.1%)-Algorithm Error (1.2%)Logic and Reasoning Errors (55.3%)-Flawed Rationale Error (43.1%)-Internal Inconsistency (12.2%)Instruction Violation (35.8%)-Context Misinterpretation (22.1%)-Incomplete or Irrelevant Response (13.1%)-Format Discrepancy (0.5%)Factual Errors (1.1%)-Factual Errors (1.1%)No Errors (0.5%)-No Errors (0.5%)(b)Figure 2: (a) Task-dataset hierarchy distribution of ReflectEvo-460k. (b) Error type distribution of corrected thoughtsidentified by reflection in the test sets.Following the above-mentioned steps in Sec. 2,We create a reflection training dataset ReflectEvo-460k by curating examples of 17 carefully selectedsource subsets from LogiQA</p>
<p>Table 2 :
2
Performance on Llama-3 and Mistral using ReflectEvo.
Dataset MethodAcc@t2 ∆(t1,t2)prompt-based36.2%+6.0%one stage w/ D +43.8%+13.6%SRtwo stage w/ D + 49.4% +19.2%w/ D ±41.8%+11.6%w/ D pref39.2%+9.0%prompt-based46.2%+16.0%one stage w/ D +52.0% +21.8%TRtwo stage w/ D +41.2%+11.0%w/ D ±48.8%+18.6%w/ D pref48.0%+17.8%</p>
<p>Table 3 :
3
Performance on LogiQA using different sources of reflections on Llama-3 (Acc@t1=30.2% from Tab. 2).SR and TR indicate self-reflection and teacherreflection respectively.</p>
<p>Table 4 :
4
Generalization across tasks for Llama-3 training one-stage with different task-specific subsets in D + .
Prompt based→ w/ reflection30.2%36.2%+6.0%14.4%16.0%+1.6%38.2%51.0%+12.8%28.4%45.8%+17.4%Self-training based (Ours)→ w/ D + on LogiQA--14.4%+0.0%60.0% +21.8%30.6%+2.2%→ w/ D + on MATH → w/ D + on BIG-bench30.2%36.6% 52.0% +21.8% +6.4%14.4%-14.4%-+0.0%38.2%54.8% +16.6% --28.4%28.8% 36.2%+0.4% +7.8%→ w/ D + on MBPP30.4%+0.2%14.6%+0.2%40.2%+2.0%--Different generators Acc@t1 Acc@t2 ∆(t1,t2)Mistral-7B28.8%45.8%+17.0%Gemma-2-9B47.6%57.2%+9.6%Llama-3.1-8B37.4%51.0%+13.6%</p>
<p>Table 5 :
5
Generalization across generators using the same reflector Llama-3 training one-stage with subset of LogiQA in D + .
Self-training method Acc@t1 Acc@t2 ∆(t1,t2)one-stage w/ D +28.8%40.2% +11.4%two-stage w/ D +28.8%38.0%+9.2%w/ D ±28.8%39.4%+10.6%w/ D pref28.8%38.6%+9.8%</p>
<p>Table 6 :
6
Generalization of generated reflection data on LogiQA of LlaMA-3.1-8Btraining on Mistral-7B</p>
<p>Table 7
7: Acc@t2 using different verifiers during infer-ence on LogiQA for Llama-3. (Acc@t1=30.2% andAcc@t2= 36.2% without tuning from Tab. 2)MethodsAcc@t1 Acc@t2 ∆(t1,t2)STaR (Zelikman et al., 2022) 40.0%--Re-ReST (Dou et al., 2024)38.8%--RISE (Qu et al., 2024)31.4% 34.4% +3.0%one-stage w/ D +30.2% 43.8% +13.6%two-stage w/ D +30.2% 49.4% +19.2%w/ D ±30.2% 41.8% +11.6%w/ D pref30.2% 39.2% +9.0%</p>
<p>Table 8 :
8
Performance on different baselines using LlaMA-3.1-8B on LogiQA.</p>
<p>Table 9 :
9
Performance on Gemma-2 using ReflectEvo.</p>
<p>Given the question and relevant context, you were unsuccessful in answering the question.As an advanced reasoning agent, you are required to enhance your incorrect solution and correctly answer the question based on self-reflection.Locate errors and diagnose potential reasons Identify specific steps where the errors occur and diagnose potential reasons.2-1.Review your calculation process to ensure that all the operations are accurate.2-2.Review your algorithm logic to ensure all steps follow the correct order.2-3.Review your solution to ensure it maintains logical coherence.2-4.Review your solution to check statements and conclusions for internal consistency.2-5.Review the context and requirements presented in the question to confirm that the response aligns with them.
C PromptsC.1 Reflection generation for ReflectEvo-460kInstruction: Question: {Question}Previous trial and your incorrect solution: {Scratchpad}Based on this information, please provide the following:# Stage 1: Verify the failed solution1-1. Analyze the failed solution by tracing and examining its execution with step-by-stepverification.1-2. Quickly go through the failed solution without step-by-step verification.# Stage 2:
B.2 TrainingAll the experiments for reflection tuning can be conducted on two Nvidia A100 80GB GPU, 32GB memory, 128 Core AMD CPU.The resource costs are mainly dependent on the tuning methods (fullparameter fine-tuning, parameter-efficient fine-tuning (PEFT) and DPO), the sizes of the models, and the sizes of the datasets.The main hyperparameters used for different settings of reflection tuning are shown in Tab.11.The learning rate varies based on different models and tasks.For one stage training with w/ D + , we use LoRA-based PEFT in this setting with 4-bit quantization via BitsAndBytes.We set LoRA rank r = 8, scaling factor α = 32, and a dropout rate of 0.1.The per-device batch size is set to 16 for LogiQA and 8 for others.For two stage training with w/ D + , we use the full-parameter SFT in this setting with bfloat16 data precision.The per-device batch size is set to 16 with gradient accumulation of 4. For DPO training with both D ± and D pref , the per-device training batch size is set to 2, and gradient accumulation is set to 32.
Reflection: a review of the literature. Sue Atkins, Kathy Murphy, Journal of advanced nursing. 1881993</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.07732Program synthesis with large language models. 2021arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. BIG bench authors. 2023</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, arXiv:2401.013352024arXiv preprint</p>
<p>Vision-language models can self-improve reasoning via reflection. Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, Yang Liu, arXiv:2411.008552024arXiv preprint</p>
<p>Reflection and learning: Characteristics, obstacles, and implications. David Denton, Educational Philosophy and Theory. 4382011</p>
<p>Re-rest: Reflectionreinforced self-training for language agents. Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, Nanyun Peng, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Llm self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints. Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, Nanyun Peng, arXiv:2410.064582024arXiv preprint</p>
<p>Reinforced selftraining (rest) for language modeling. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, arXiv:2308.089982023arXiv preprint</p>
<p>Self-correction is more than refinement: A learning framework for visual and language reasoning tasks. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948arXiv:2410.04055Jiayi He, Hehai Lin, Qingyun Wang, Yi Fung, and Heng Ji2025. 2024arXiv preprintDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Training language models to self-correct via reinforcement learning. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, 2024. 2024</p>
<p>The measurement of observer agreement for categorical data. Richard Landis, Gary G Koch, biometrics. 1977</p>
<p>NV-embed: Improved techniques for training LLMs as generalist embedding models. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Loogle: Can long-context language models understand long contexts?. Jiaqi Li, Mengmeng Wang, Zilong Zheng, Muhan Zhang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024a1</p>
<p>Jiaqi Li, Xiaobo Wang, Wentao Ding, Zihao Wang, Yipeng Kang, arXiv:2404.12045Zixia Jia, and Zilong Zheng. 2024b. Ram: Towards an ever-improving memory system by learning from communications. arXiv preprint</p>
<p>Reflection-tuning: Data recycling improves llm instruction-tuning. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, Tianyi Zhou, arXiv:2310.117162023arXiv preprint</p>
<p>Fb-bench: A fine-grained multi-task benchmark for evaluating llms' responsiveness to human feedback. Youquan Li, Miao Zheng, Fan Yang, Guosheng Dong, Bin Cui, Weipeng Chen, Zenan Zhou, Wentao Zhang, arXiv:2410.094122024carXiv preprint</p>
<ol>
<li>I-sheep: Self-alignment of llm from scratch through an iterative self-enhancement paradigm. Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, Lei Ma, arXiv:2408.08072arXiv preprint</li>
</ol>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, arXiv:2007.081242020arXiv preprint</p>
<p>Statistical rejection sampling improves preference optimization. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, Jialu Liu, arXiv:2309.066572023arXiv preprint</p>
<p>Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li, arXiv:2401.08967Reft: Reasoning with reinforced fine-tuning. 2024arXiv preprint</p>
<p>Self-taught self-correction for small language models. Viktor Moskvoretskii, arXiv:2503.08681Chris Biemann, and Irina Nikishina. 2025arXiv preprint</p>
<p>Ramesh Sumeet, Chandler Motwani, Smith, Jyoti Rocktim, Markian Das, Rybchuk, Ivan Philip Hs Torr, Fabio Laptev, Ronald Pizzati, Christian Clark, Schroeder De Witt, arXiv:2412.01928Malt: Improving reasoning with multi-agent llm training. 2024arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Language model self-improvement by reinforcement learning contemplation. Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, Yang Yu, arXiv:2305.144832023arXiv preprint</p>
<p>Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar, arXiv:2407.18219Recursive introspection: Teaching language model agents how to self-improve. 2024arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>Self-reflection in llm agents: Effects on problem-solving performance. Matthew Renze, Erhan Guven, Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2405.06682Advances in Neural Information Processing Systems. 2024. 202436arXiv preprintReflexion: Language agents with verbal reinforcement learning</p>
<p>Trial and error: Exploration-based trajectory optimization for llm agents. Yifan Song, Xiang Da Yin, Jie Yue, Sujian Huang, Bill Li, Lin Yuchen, arXiv:2403.025022024arXiv preprint</p>
<p>Mars: Situated inductive reasoning in an open-world environment. Xiaojuan Tang, Jiaqi Li, Yitao Liang, Muhan Zhang, Zilong Zheng, 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. 2024</p>
<p>Large language models are in-context semantic reasoners rather than symbolic reasoners. Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, Muhan Zhang, arXiv:2305.148252023arXiv preprint</p>
<p>. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.00118arXiv preprintet al. 2024. Gemma 2: Improving open language models at a practical size</p>
<p>An efficient recipe for long context extension via middlefocused positional encoding. Zilong Zheng, Tong Wu, Yanpeng Zhao, Advances in Neural Information Processing Systems (NeurIPS). 202437</p>
<p>Johan Von, Wright , Reflections on reflection. Learning and instruction. 19922</p>
<p>Diffusion model alignment using direct preference optimization. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Selftraining with direct preference optimization improves chain-of-thought reasoning. Tianduo Wang, Shichen Li, Wei Lu, 10.18653/v1/2024.acl-long.643Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2024a1</p>
<p>Exovip: Step-by-step verification and exploration with exoskeleton modules for compositional visual reasoning. Yuxuan Wang, Alan Yuille, Zhuowan Li, Zheng Zilong, The first Conference on Language Modeling (CoLM). 2024b</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, arXiv:2211.000532022arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, arXiv:2308.02151Retroformer: Retrospective large language agents with policy gradient optimization. 2023arXiv preprint</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, arXiv:2401.10020Self-rewarding language models. 2024arXiv preprint</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>Quiet-STar: Language models can teach themselves to think before speaking. Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah Goodman, First Conference on Language Modeling. 2024</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Advances in Neural Information Processing Systems. 202235Jesse Mu, and Noah Goodman</p>
<p>Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, arXiv:2406.13975Mr-ben: A comprehensive meta-reasoning benchmark for large language models. 2024arXiv preprint</p>
<p>Small language models need strong verifiers to self-correct reasoning. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang, arXiv:2404.171402024arXiv preprint</p>
<p>Reflect-rl: Two-player online rl fine-tuning for lms. Runlong Zhou, Simon S Du, Beibin Li, arXiv:2402.126212024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
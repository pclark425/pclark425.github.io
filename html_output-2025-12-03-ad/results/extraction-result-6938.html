<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6938 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6938</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6938</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276928610</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.08681v1.pdf" target="_blank">Self-Taught Self-Correction for Small Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Although large language models (LLMs) have achieved remarkable performance across various tasks, they remain prone to errors. A key challenge is enabling them to self-correct. While prior research has relied on external tools or large proprietary models, this work explores self-correction in small language models (SLMs) through iterative fine-tuning using solely self-generated data. We introduce the Self-Taught Self-Correction (STaSC) algorithm, which incorporates multiple algorithmic design choices. Experimental results on a question-answering task demonstrate that STaSC effectively learns self-correction, leading to significant performance improvements. Our analysis further provides insights into the mechanisms of self-correction and the impact of different design choices on learning dynamics and overall performance. To support future research, we release our user-friendly codebase and lightweight models.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6938.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6938.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaSC (Qwen-2.5-1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Taught Self-Correction (STaSC) applied to Qwen-2.5-1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic self-correction procedure where a small LM (Qwen-2.5-1.5B) iteratively samples initial answers, generates corrections conditioned on those answers, filters corrections by a reward, fine-tunes on the filtered corrections, and repeats this generate-then-reflect loop to improve QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source small language model used in experiments; default generation parameters were used. Treated as a small LM in the paper's experiments (part of the family of SLMs with <4B parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Taught Self-Correction (STaSC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For each input: sample N_init initial answers from a generator, sample N_corr corrections conditioned on each initial answer, score responses with a reward r (binary in-accuracy reward), filter corrections by an Improving or Non‑Decreasing criterion, fine-tune the model on selected corrections (either from the base model or evolving model), and iterate across N improvement iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect iterative fine-tuning (sample initial answers → sample corrections → filter by reward → fine-tune → repeat)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Questions (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Factual question answering; authors used 500-question subsets for train and test splits from the Natural Questions benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>In-accuracy (reward r: r(ŷ)=1 if generated answer contains reference answer, else 0); reported also as max{r(Ŷ1)} for initial answers and max{r(Ŷ2)} for corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Reported in paper as max{r(Ŷ1)} (see Table 1). Per-table values vary by sampling configuration; the paper reports initial-answer max-reward values in the range ≈0.23–0.33 across settings (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Reported in paper as max{r(Ŷ2)} (see Table 1). Per-table values vary by configuration; corrections' max-reward values reported ≈0.34–0.38 across settings (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Greedy/low-exploration setups can fail to find any improving corrections (e.g., small N_init caused no improving corrections in the first iteration for Qwen). Filtering selectivity and fine-tuning choices matter: overly lenient filters or inappropriate fine-tuning strategy can introduce noise and cause performance degradation. Fixed fine-tuning (no evolution) can lead to stagnation. Experiments were performed as single runs and only on a QA subset; model capacity may limit the extent of self-correction; binary reward (in-accuracy) may not capture nuanced improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Taught Self-Correction for Small Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6938.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6938.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaSC (Phi3-Mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Taught Self-Correction (STaSC) applied to Phi3-Mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The STaSC iterative self-correction algorithm applied to Phi3-Mini: generate initial answers, sample corrections conditioned on those answers, filter by reward, fine-tune on successful corrections, and repeat to improve QA outputs without external feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi3-Mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source small language model used in experiments; treated as a stronger SLM compared to Qwen in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Taught Self-Correction (STaSC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>See Qwen entry; STaSC variants control (a) initialization for sampling initial answers (Fixed vs Evolving), (b) filtering (Improving vs Non‑Decreasing), and (c) fine-tuning (Fixed vs Evolving), and iterate the generate→criticize→revise loop using only self-generated data.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect iterative fine-tuning (generate initial answer(s) → generate corrections → filter by reward → fine-tune → repeat)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Questions (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Factual question answering; authors used 500-question subsets for train and test splits from the Natural Questions benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>In-accuracy (binary reward r as above); reported max{r(Ŷ1)} and max{r(Ŷ2)} across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Reported in paper as max{r(Ŷ1)} (see Table 1). Per-table values and plots show initial-answer max-reward values around ≈0.23–0.32 depending on sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Reported in paper as max{r(Ŷ2)} (see Table 1 and Figures). Corrections' max-reward values are reported around ≈0.32–0.38 depending on configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Non‑Decreasing Filter consistently degraded performance for Phi3-Mini in many Evolving Initialization settings (training instability/overfitting when fewer corrections retained). Insufficient filtering selectivity introduced noisy trajectories that harmed the Corrector (negative correlation between leniency and gain: r = -0.51, p < .001). Fixed Fine-Tuning led to declines for Phi3 in some settings. Other general limitations: single-run experiments, restricted to a QA task subset, binary reward may miss nuanced improvements, and SLM capacity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Taught Self-Correction for Small Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generating sequences by learning to self-correct <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Training language models to selfcorrect via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>SCoRE <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6938",
    "paper_id": "paper-276928610",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "STaSC (Qwen-2.5-1.5B)",
            "name_full": "Self-Taught Self-Correction (STaSC) applied to Qwen-2.5-1.5B",
            "brief_description": "An intrinsic self-correction procedure where a small LM (Qwen-2.5-1.5B) iteratively samples initial answers, generates corrections conditioned on those answers, filters corrections by a reward, fine-tunes on the filtered corrections, and repeats this generate-then-reflect loop to improve QA performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-1.5B",
            "model_description": "Open-source small language model used in experiments; default generation parameters were used. Treated as a small LM in the paper's experiments (part of the family of SLMs with &lt;4B parameters).",
            "model_size": null,
            "reflection_method_name": "Self-Taught Self-Correction (STaSC)",
            "reflection_method_description": "For each input: sample N_init initial answers from a generator, sample N_corr corrections conditioned on each initial answer, score responses with a reward r (binary in-accuracy reward), filter corrections by an Improving or Non‑Decreasing criterion, fine-tune the model on selected corrections (either from the base model or evolving model), and iterate across N improvement iterations.",
            "iteration_type": "generate-then-reflect iterative fine-tuning (sample initial answers → sample corrections → filter by reward → fine-tune → repeat)",
            "num_iterations": null,
            "task_name": "Natural Questions (subset)",
            "task_description": "Factual question answering; authors used 500-question subsets for train and test splits from the Natural Questions benchmark.",
            "evaluation_metric": "In-accuracy (reward r: r(ŷ)=1 if generated answer contains reference answer, else 0); reported also as max{r(Ŷ1)} for initial answers and max{r(Ŷ2)} for corrections.",
            "performance_before_reflection": "Reported in paper as max{r(Ŷ1)} (see Table 1). Per-table values vary by sampling configuration; the paper reports initial-answer max-reward values in the range ≈0.23–0.33 across settings (see Table 1).",
            "performance_after_reflection": "Reported in paper as max{r(Ŷ2)} (see Table 1). Per-table values vary by configuration; corrections' max-reward values reported ≈0.34–0.38 across settings (see Table 1).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Greedy/low-exploration setups can fail to find any improving corrections (e.g., small N_init caused no improving corrections in the first iteration for Qwen). Filtering selectivity and fine-tuning choices matter: overly lenient filters or inappropriate fine-tuning strategy can introduce noise and cause performance degradation. Fixed fine-tuning (no evolution) can lead to stagnation. Experiments were performed as single runs and only on a QA subset; model capacity may limit the extent of self-correction; binary reward (in-accuracy) may not capture nuanced improvements.",
            "uuid": "e6938.0",
            "source_info": {
                "paper_title": "Self-Taught Self-Correction for Small Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "STaSC (Phi3-Mini)",
            "name_full": "Self-Taught Self-Correction (STaSC) applied to Phi3-Mini",
            "brief_description": "The STaSC iterative self-correction algorithm applied to Phi3-Mini: generate initial answers, sample corrections conditioned on those answers, filter by reward, fine-tune on successful corrections, and repeat to improve QA outputs without external feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Phi3-Mini",
            "model_description": "Open-source small language model used in experiments; treated as a stronger SLM compared to Qwen in the paper's comparisons.",
            "model_size": null,
            "reflection_method_name": "Self-Taught Self-Correction (STaSC)",
            "reflection_method_description": "See Qwen entry; STaSC variants control (a) initialization for sampling initial answers (Fixed vs Evolving), (b) filtering (Improving vs Non‑Decreasing), and (c) fine-tuning (Fixed vs Evolving), and iterate the generate→criticize→revise loop using only self-generated data.",
            "iteration_type": "generate-then-reflect iterative fine-tuning (generate initial answer(s) → generate corrections → filter by reward → fine-tune → repeat)",
            "num_iterations": null,
            "task_name": "Natural Questions (subset)",
            "task_description": "Factual question answering; authors used 500-question subsets for train and test splits from the Natural Questions benchmark.",
            "evaluation_metric": "In-accuracy (binary reward r as above); reported max{r(Ŷ1)} and max{r(Ŷ2)} across iterations.",
            "performance_before_reflection": "Reported in paper as max{r(Ŷ1)} (see Table 1). Per-table values and plots show initial-answer max-reward values around ≈0.23–0.32 depending on sampling.",
            "performance_after_reflection": "Reported in paper as max{r(Ŷ2)} (see Table 1 and Figures). Corrections' max-reward values are reported around ≈0.32–0.38 depending on configuration.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Non‑Decreasing Filter consistently degraded performance for Phi3-Mini in many Evolving Initialization settings (training instability/overfitting when fewer corrections retained). Insufficient filtering selectivity introduced noisy trajectories that harmed the Corrector (negative correlation between leniency and gain: r = -0.51, p &lt; .001). Fixed Fine-Tuning led to declines for Phi3 in some settings. Other general limitations: single-run experiments, restricted to a QA task subset, binary reward may miss nuanced improvements, and SLM capacity constraints.",
            "uuid": "e6938.1",
            "source_info": {
                "paper_title": "Self-Taught Self-Correction for Small Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generating sequences by learning to self-correct",
            "rating": 2,
            "sanitized_title": "generating_sequences_by_learning_to_selfcorrect"
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning",
            "rating": 2,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "Training language models to selfcorrect via reinforcement learning",
            "rating": 2,
            "sanitized_title": "training_language_models_to_selfcorrect_via_reinforcement_learning"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "SCoRE",
            "rating": 1
        }
    ],
    "cost": 0.010686999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Taught Self-Correction for Small Language Models
11 Mar 2025</p>
<p>Viktor Moskvoretskii vvmoskvoretskii@gmail.com 
HSE University</p>
<p>Chris Biemann 
University of Hamburg</p>
<p>Irina Nikishina 
University of Hamburg</p>
<p>Skoltech 
Self-Taught Self-Correction for Small Language Models
11 Mar 202564E68129A70B338CD20D2D6204D1C2B2arXiv:2503.08681v1[cs.CL]
Although large language models (LLMs) have achieved remarkable performance across various tasks, they remain prone to errors.A key challenge is enabling them to self-correct.While prior research has relied on external tools or large proprietary models, this work explores self-correction in small language models (SLMs) through iterative fine-tuning using solely self-generated data.We introduce the Self-Taught Self-Correction (STaSC) algorithm, which incorporates multiple algorithmic design choices.Experimental results on a question-answering task demonstrate that STaSC effectively learns self-correction, leading to significant performance improvements.Our analysis further provides insights into the mechanisms of self-correction and the impact of different design choices on learning dynamics and overall performance.To support future research, we release our user-friendly codebase and lightweight models.</p>
<p>Introduction</p>
<p>Recent advanced LLM employ complex reasoning (Guo et al., 2025) and meta-reasoning (Xiang et al., 2025), expanding their capabilities.However, even the most advanced models are prone to errors, including hallucinations (Huang et al., 2025) and logical inconsistencies (Ghosh et al., 2024), requiring external verification or human intervention.To address those problems, self-correction -the ability to revise their own outputs -has been evolved (Madaan et al., 2023).The existing approaches mostly use zero-shot prompting (Madaan et al., 2023;Shinn et al., 2024), external evaluators for correction or feedback (Zhang et al., 2024) or apply large proprietary models and focus specifically on mathematical tasks (Kumar et al., 2024).</p>
<p>In this study, we focus on self-correction without external information or evaluators, ensuring inference efficiency while relying solely on the model's intrinsic knowledge.We investigate the self-correction capabilities of small language models (SLMs) by applying iterative fine-tuning on self-generated data, allowing models to improve their outputs without external supervision.</p>
<p>We introduce the Self-Taught Self-Correction (STaSC) algorithm, which trains models to selfcorrect using self-generated trajectories, adapting the core idea of STaR (Zelikman et al., 2022).STaSC extends and unifies approaches that iteratively train with self-generated trajectories, including the Self-Correction (SC) algorithm (Welleck et al., 2022), by incorporating flexible design choices.Unlike prior methods, STaSC provides control over initial answer exploration, correction filtering, and iterative fine-tuning, encompassing SC as a special case and demonstrating how different algorithmic choices impact self-correction performance.</p>
<p>Our results on the Natural Questions dataset (Kwiatkowski et al., 2019) show that SLMs can learn to self-correct using self-synthesized data, while also improving their initial answer quality despite being trained solely for corrections.We release easy-to-use and adaptable code for selfcorrection algorithms at https://github.com/VityaVitalich/STASC.</p>
<p>The contributions of the paper are as follows:</p>
<p>• We propose the Self-Taught Self-Correction (STaSC) algorithm, unifying and extending existing self-correction methods trained with self-generated trajectories.</p>
<p>• We conduct extensive experiments on a purely Natural Language Processing (NLP) task -Question Answering -using an open-source SLMs, demonstrating their ability to learn selfcorrection with self-synthesized data.</p>
<p>• We release open-source, easily adaptable code for self-correction, along with efficient SLMs with fewer than 4B parameters, making selfcorrection practical and accessible.</p>
<p>2 Self-Taught Self-Correction</p>
<p>In this section, we introduce the Self-Taught Self-Correction (STaSC) algorithm, an adaptation of STaR (Zelikman et al., 2022) for selfcorrection through iterative fine-tuning on selfsynthesized data.STaSC unifies and extends various self-correction approaches using selfsynthesized data, including the Self-Correction (SC) algorithm (Welleck et al., 2022).</p>
<p>Foundations and Enhancements</p>
<p>Figure 1 presents three algorithms highlighting their similarities and dissimilarities.The left part describes the original STaR algorithm which focuses on generating reasoning paths, filtering correct ones, and fine-tuning them accordingly.</p>
<p>The central part of the Figure 1 demonstrates the Self-Correction (SC) algorithm which fine-tunes the corrector model on corrections leading to improvement, keeping the initial Generator model fixed.The right part refers to STaSC, adapting the idea of STaR of iteratively refining model outputs by sampling an initial answer, generating corrections, filtering correct trajectories, and fine-tuning on the successful revisions.This process is repeated over multiple iterations, progressively enhancing the model's accuracy.</p>
<p>The key extension of our algorithm over previ-ous baselines is the incorporation of flexible algorithmic choices.Initial answers can be drawn from either a frozen model or the previous iteration, corrections can be filtered based on strict improvements or by allowing non-worsening revisions, and fine-tuning can be performed from either the initial model or the latest iteration.</p>
<p>Formal Definition and Algorithmic Choices</p>
<p>Below, we outline the Algorithm 1 steps and key design choices, each of which influences the selfcorrection process.For each choice, we define an abbreviation that will be used to denote specific algorithm configurations, such as STaSC FIF , where subscripts specify the selected options.</p>
<p>Requirements and Notation</p>
<p>The algorithm begins with an initial language model state, M 0 , and an initial dataset, D 0 , consisting of input-output pairs (x, y).Additionally, we define a number of improvement iterations N , the number of sampled initial generations N init , the number of sampled corrections N corr , and a reward function r, which evaluates the quality of modelgenerated outputs.</p>
<p>Step 1: Sampling Initial Answers In the first step, we sample N init initial answers ŷ1 for each input x in the dataset D 0 .The primary design choice here is the selection of the model M used for sampling:</p>
<p>Algorithm 1 Self-Taught Self-Correction (STaSC)</p>
<p>Require: Initial model M 0 , dataset D 0 , number of iterations N , initial samples N init , correction samples N corr , reward function r 1: for n = 1 to N do 2:</p>
<p>Step 1: Sample Initial Answers 3:
Ŷ 1 i = {ŷ 1 ij } N init j=1 ∼ M (x i ), ∀x i ∈ D 0 4:
Option 1: M = M n−1 (Evolving Initialization) 5:</p>
<p>Option 2: M = M 0 (Fixed Initialization) 6:</p>
<p>Step 2: Sample Corrections 7:
Ŷ 2 i = {ŷ 2 ijk } Ncorr k=1 ∼ M n−1 (x i , ŷ1 ij ), ∀ŷ 1 ij ∈ Ŷ 1 i 8:
Step 3: Filter Corrections 9:
D + n = {(x i , ŷ1 ij , ŷ2 ijk ) | r(ŷ 2 ijk ) &gt; r(ŷ 1 ij )} 10: D = n = {(x i , ŷ1 ij , ŷ2 ijk ) | r(ŷ 2 ijk ) = r(ŷ 1 ij ) ≥ t} 11:
Option 1:
D n = D + n (Improving Filter) 12:
Option 2:
D n = D + n ∪ D = n (Non-Decreasing Filter) 13:
Step 4: Fine-Tuning 14:
M n = train(M, {ŷ 2 ijk | (x i , ŷ1 ij , ŷ2 ijk ) ∈ D n }) 15:
Option 1: M = M 0 (Fixed Fine-Tuning) 16:</p>
<p>Option 2: M = M n−1 (Evolvong Fine-Tuning) 17: end for • Fixed Initialization (STaSC F** ) -Initial answers are sampled using the frozen model M 0 , as in the SC algorithm (Welleck et al., 2022).</p>
<p>• Evolving Initialization (STaSC E** ) -Initial answers are sampled using the model from the previous iteration M n−1 , as in the original STaR (Zelikman et al., 2022).</p>
<p>Fixed Initialization ensures robustness to variations in self-improvement and dataset shifts, maintaining consistency across iterations.In contrast, Evolving Initialization allows for greater exploration, potentially leading to more diverse refinements and improved performance.</p>
<p>Step 2: Sample Corrections At the second step, we sample N corr corrections ŷ2 for each output ŷ1 in dataset D 0 using the model from the last iteration M n−1 .</p>
<p>Step 3: Filtering Corrections In this step, we filter the corrections using the reward function r(ŷ 2 ) to construct the fine-tuning dataset D n .The key design choice here is the filtering criterion for selecting corrections.</p>
<p>• Improving Filter (STaSC <em>I</em> ) -Selects only corrections that strictly improve the reward, ensuring r(ŷ 2 ) &gt; r(ŷ 1 ), as used in STaSC and SC.</p>
<p>• Non-Decreasing Filter (STaSC <em>N</em> ) -Selects corrections that either strictly improve the reward or retain the initial answer if it was already correct, ensuring (r(ŷ 2 ) = r(ŷ 1 )) ∩ (r(ŷ 1 ) ≥ t), as proposed in SCoRE (Kumar et al., 2024).This allows the model to preserve already correct answers while still refining incorrect ones.</p>
<p>The Improving Filter enforces strict improvement for every input, ensuring that only progressively better outputs are used for fine-tuning.In contrast, the Non-Decreasing Filter permits stable answers to be retained when already correct, allowing for a more conservative refinement process.</p>
<p>Step 4: Fine-Tuning In this step, we fine-tune the model on the dataset D n obtained from Step 3 to produce the improved model M n .The key design choice here is the selection of the model used for fine-tuning:</p>
<p>• Fixed Fine-Tuning (STaSC **F ) -Fine-tunes the initial model M 0 , as done in the original STaR recipe, ensuring stability across iterations and reducing the risk of accumulating errors.</p>
<p>• Evolving Fine-Tuning (STaSC **E ) -Finetunes the model from the previous iteration M n−1 , as in SC, allowing the model to progressively improve and adapt across iterations.Fixed Fine-Tuning maintains a stable learning process by always training from the original model, preventing drift.In contrast, Evolving Fine-Tuning enables iterative adaptation, potentially leading to greater long-term improvements but also introducing the risk of compounding errors.
N init N corr Qwen-2.5-1.5B Phi3-mini max{r( Ŷ 1 )} max{r( Ŷ 2 )} max{r( Ŷ 1 )} max{r( Ŷ 2 )} 1 1 - - 0.320 ± 0.</p>
<p>Experimental Setup</p>
<p>In this section, we describe the main experimental procedure, including dataset selection, evaluation metrics, and implementation details.</p>
<p>Dataset We evaluate our algorithm on the QA task using the Natural Questions dataset (Kwiatkowski et al., 2019), which consists of factual simple questions.We use a subset of 500 questions per train and test split, following previous studies (Trivedi et al., 2022;Jeong et al., 2024;Moskvoretskii et al., 2025), to ensure consistency and computational efficiency.</p>
<p>Evaluation We use In-accuracy as the primary evaluation metric and reward function, which is standard for this task (Trivedi et al., 2022;Jeong et al., 2024).It assesses whether the generated answer contains the reference answer, assigning r(ŷ) = 1 for correct responses and r(ŷ) = 0 for incorrect ones.</p>
<p>All metrics are reported on the test set, which remains unseen during training.Additionally, we report max{r( Ŷ 1 )} and max{r( Ŷ 2 )}, representing the highest reward obtained for initial answers and corrections across STaSC iterations, respectively.To ensure a fair real-world setup, the reward is not available to the model during inference.Unlike some prior studies (Shinn et al., 2024), where inference benefits from reward signals derived from ground-truth labels, our approach aligns with a fully unsupervised inference setting, ensuring a more realistic evaluation.</p>
<p>Implementation Details We conduct experiments using Qwen-2.5-1.5B(Qwen et al., 2025) and Phi3-Mini (Abdin et al., 2024), employing default generation parameters.The default setup for self-correction is 2-shot.STaSC fine-tuning is performed for 1 epoch with a batch size of 8 and a learning rate of 7 × 10 −6 .Additional implementation details are provided in Appendix A.</p>
<p>Results &amp; Discussion</p>
<p>In this section, we provide the results and discuss them, inspecting the STaSC algorithm design.</p>
<p>Impact of N init and N corr</p>
<p>Firstly, we examine how the selection of parameters N init and N corr affects algorithm performance, exploring values of 1, 3, and 5 for both models.To encourage exploration, we use STaSC EIF , where initial answers are sampled from the previous iteration, only improving corrections are retained, and fine-tuning is performed from the base model to ensure stability.</p>
<p>As shown in Table 1, a greedy approach for Qwen-2.5-1.5Bfails to ensure convergence, as no improving corrections emerge in the first iteration.In contrast, increased exploration significantly enhances performance, likely due to the weaker alignment of the initial model.Additionally, we observe  that exploring initial answers has a greater impact than exploring corrections.However, this trend does not hold for Phi3-Mini, where increasing the number of initial answers does not improve results, but greater correction exploration does.This discrepancy likely stems from differences in model capabilities.Phi3-Mini, being inherently stronger, benefits more from refining corrections, whereas Qwen-2.5-1.5B, with lower initial competence, requires a broader search for initial answers to gain useful knowledge early on.</p>
<p>For subsequent experiments, we adopt the most stable and well-performing configurations:</p>
<p>• Qwen-2.5-1.5B:(N init , N corr ) = (5, 5).</p>
<p>• Phi3-Mini: (N init , N corr ) = (1, 5).</p>
<p>STaSC With Evolving Initialization</p>
<p>Figure 2 illustrates the dynamics of correction performance for STaSC variants with Evolving Initialization.Below, we discuss the observed effects on performance, highlighting key trends and their implications.</p>
<p>Effect of the Non-Decreasing Filter A key observation is that STaSC with the Non-Decreasing Filter consistently degrades performance for Phi3-Mini.This is likely due to the difficulty in stabilizing training when fewer corrections are retained, increasing the risk of overfitting.Interestingly, Qwen does not exhibit the same decline, possibly due to a higher number of retained corrections, which augments the data and stabilizes training.</p>
<p>Impact of Filtering Selectivity</p>
<p>We further highlight the need to properly filter corrections when using Evolving Fine-Tuning, as all Phi3 settings with STaSC **E exhibit a negative correlation between the leniency of filtered trajectories and the gain in correction performance (r = −0.51,p &lt; .001).The more corrections were provided to the Corrector model, the worse it performed.This suggests that insufficiently selective filtering introduces noise, leading to overfitting and a decline in performance improvement.Notably, no other setting shows a significant correlation between the number of filtered corrections and performance.</p>
<p>Evolving Fine-Tuning Trends Evolving Fine-Tuning slightly improves correction performance for Phi3-Mini.For Qwen, performance increases rapidly at first, then declines slightly before rising again in later iterations.This suggests that accumulated knowledge gains take effect only in the later stages, once a sufficient number of corrections and initial answers have been processed.</p>
<p>Key Takeaways Evolving Initialization is most effective when combined with an Improving Filter and Evolving Fine-Tuning, as these either enhance correction performance or at least prevent degradation.Additionally, filtering selectivity is crucial-overly lenient filters introduce noise and cause overfitting.</p>
<p>STaSC With Fixed Initialization</p>
<p>Next, we analyze the trends and implications of Fixed Initialization, which aligns with the algorithmic choice used in SC. Figure 3 presents the correction performance for STaSC variants under this setting.</p>
<p>Effect of the Non-Decreasing Filter Unlike Evolving Initialization, the Non-Decreasing Fil-ter does not lead to a general decline in correction performance for either Phi3 or Qwen, with an exception of STaSC FNF ,where performance degrades.For Phi3, correction performance remains largely unchanged throughout most iterations, with noticeable improvements emerging only in the final stages.This delayed progress suggests that Evolving Fine-Tuning gradually accumulates knowledge, but its effects become apparent only after sufficient corrections and initial answers have been processed.A similar trend was previously observed in STaSC EIE , showing that iterative fine-tuning plays a key role in long-term performance gains.</p>
<p>Importance of Evolving Fine-Tuning We find that Evolving Fine-Tuning is crucial when using Fixed Initialization, particularly for Phi3 and, to a lesser extent, for Qwen.This is expected, as Evolving Fine-Tuning serves as the only source of exploration in this setting, driving the algorithm forward.In contrast, with Fixed Fine-Tuning, we observe a general decline in Phi3's performance and stagnation after the first iteration for Qwen.This suggests that without sufficient exploration, relying solely on corrections from previous steps is insufficient for SLMs.We also observe that when using both Fixed Initialization and Fixed Fine-Tuning applying the Non-Decreasing Filter further worsens the performance, underscoring the importance of filtering selectivity.In this setting, exploration is driven solely by corrections, making the filtering process critical.When corrections include too many noisy or uninformative trajectories due to improper filtering, the model struggles to improve, leading to significant performance degradation.This reinforces the need for a more controlled correction selection process to ensure meaningful updates during training.</p>
<p>Key Takeaways Fixed Initialization reduces exploration, making the model more tolerant to the Non-Decreasing Filter and benefiting more from Evolving Fine-Tuning.In contrast, Fixed Fine-Tuning restricts exploration solely to corrections, increasing reliance on selective filtering to maintain performance.</p>
<p>STaSC Impact on Initial Answers</p>
<p>In this section, we identify the best-performing STaSC configurations and analyze their behavior in terms of initial answers and correction dynamics.</p>
<p>Selection of High-Performing Variants Based on our previous analysis and Table 2 we determine that STaSC EIE and STaSC EIF exhibit the strongest performance.These results highlight the crucial role of Evolving Initialization and the Improving Filter, while leaving the impact of Fine-Tuning strategies open for further investigation.</p>
<p>Performance Comparison Figure 4 illustrates the performance trends of initial answers and corrections for STaSC EIE and STaSC EIF .</p>
<p>The effectiveness of Fine-Tuning strategies varies between Phi3 and Qwen.For Phi3, Evolving Fine-Tuning leads to a substantial increase in initial answer quality, surpassing Fixed Fine-Tuning, while yielding moderate improvements in corrections.In contrast, for Qwen, Fixed Fine-Tuning results in superior performance for both initial answers and corrections.</p>
<p>Effect of Correction-Only Training</p>
<p>Interestingly, despite the model being trained exclusively on corrections, initial answer quality also improves.Since gradient updates are applied only to correction tokens, this suggests that learning corrections either enhances the model's factual knowledge or improves its internal reasoning ability at the initial generation stage.</p>
<p>Alignment Between Initial Answers and Corrections Evolving Fine-Tuning progressively reduces the gap between initial answers and corrections, eventually leading to their alignment.For Phi3, initial answers gradually improve until they reach the quality of corrections.For Qwen, the opposite trend is observed-corrections improve until they match the initial answers.</p>
<p>This suggests that Evolving Fine-Tuning either helps internalize the correction process, leading to higher-quality initial answers as seen in Phi3, or stabilizes responses by preventing degradation, as observed in Qwen.</p>
<p>Related Works</p>
<p>Self-correction is a relatively new yet actively growing research domain.A systematic survey defines self-correction as a framework in which LLMs refine their responses using LLMs during inference, potentially incorporating external tools or knowledge (Kamoi et al., 2024).A significant body of work in this direction focuses on leveraging external feedback from external knowledge and verification tools (Jiang et al., 2023;Gou et al., 2024;Pan et al., 2023;Xu et al., 2023), as these approaches provide high-quality solution evaluation.However, in real-world applications, such external resources are often unavailable or computationally expensive.Moreover, relying on external verification does not pose a fundamentally challenging task for LLMs, limiting their ability to develop intrinsic reasoning and self-improvement capabilities.</p>
<p>An alternative approach is intrinsic selfcorrection, where the model refines its own outputs without relying on external critics.This can be implemented in a zero-shot setting, using the same model iteratively (Madaan et al., 2023), or through external models trained on synthetic errors (Paul et al., 2024) or self-generated corrections (Welleck et al., 2022).While these approaches have shown promise, they still rely on external critic models, making them closer to verification-based generation rather than true self-correction.</p>
<p>The only work exploring self-correction in its natural form is the SCoRE framework (Kumar et al., 2024), which was the first to experiment with intrinsic self-correction within the same model and introduced a multi-turn RL training approach for this purpose.However, SCoRE lacks a formalized</p>
<p>STaSC</p>
<p>You are a helpful reasoning assistant general domain question answering.Please reason through the question step by step very shortly before giving a final answer.Generate a short chain-of-thought rationale very shortly, and then</p>
<p>provide the final answer in the format: "Final answer: your answer"</p>
<p>Step-by-step reasoning: The question refers to two different movies, one about pregnancy and another about Tracey.</p>
<p>The actress who played in both needs to be identified.</p>
<p>movies about pregnancy, "Juno" comes to mind, starring Ellen Page (now Elliot Page).For a movie about Tracey, "Boys Don't Cry" features Hilary Swank playing the role of Tracey.</p>
<p>However, these are two different actresses, so we need to find a common actress.After factchecking, the actress who played in both movies is likely to be Reese Witherspoon, who starred in "Legally Blonde" (where her character is named Elle, not Tracey) and "Sweet Home Alabama" (which involves pregnancy).Final Answer: Reese Witherspoon input output Please review the given answer for accuracy.If it is incorrect, identify the incorrect information and provide the correct details.Ask and answer subquestions, and split the question into parts.Disregard the information you already have.</p>
<p>system prompt system prompt input</p>
<p>Step-by-step reasoning: The initial analysis contains several inaccuracies regarding the identification of an actress who starred in both a movie about pregnancy and another featuring a character named Tracey.Let's address these inaccuracies and provide the correct information.Question: played in the movies about and Tracey?step by step very shortly, then conclude with answer.</p>
<p>Inaccuracies in the</p>
<p>Step-by-step reasoning:</p>
<p>Conclusion</p>
<p>In this study, we introduced the Self-Taught Self-Correction (STaSC) algorithm, which incorporates multiple algorithmic choices to enable genuine intrinsic self-correction without relying on external tools or large proprietary models.Inspired by STaR (Zelikman et al., 2022), our approach trains exclusively on self-generated data.Experiments on a QA task with two small language models demonstrate that SLMs can learn to self-correct using their own generations and even improve initial answers, despite being trained solely for corrections.Furthermore, our analysis highlights key algorith-mic insights, emphasizing the importance of filtering selectivity, initial answer exploration, and the potential of iterative fine-tuning.To support future research, we have open-sourced our code and lightweight models.</p>
<p>Limitations</p>
<p>• The selected SLMs, while effective, may have certain capacity limitations that could influence the extent of the self-correction process.</p>
<p>• Experiments were conducted with a single run, which, while sufficient for initial insights, may introduce some variability.</p>
<p>• The evaluation focuses on a Question Answering (QA) task, leaving open the opportunity to explore performance across other tasks and domains.</p>
<p>• The chosen hyperparameters, though reasonable, may not fully optimize the model's learning efficiency or overall performance.</p>
<p>• A more detailed analysis of the types and patterns of corrections could further enrich our understanding of the self-correction mechanism.</p>
<p>• The reward function, while practical, may not perfectly capture all nuances of desired behavior, presenting room for refinement in future work.</p>
<p>Ethical Considerations</p>
<p>Our work enables small language models to self-correct using self-generated data.We employ advanced models like Qwen-2.5 and Phi-3, pre-trained on diverse datasets, including usergenerated content.While efforts have been made to remove harmful or biased data, some biases may persist in outputs.This does not undermine our methods, which are designed to self-correct factual inaccuracies and are adaptable to other rigorously debiased models.Beyond bias challenges, our work raises no additional ethical concerns.</p>
<p>A Technical Details</p>
<p>We apply a weight decay of 0.1 and use the Adam optimizer with default betas, along with a cosine annealing scheduler.Training is performed using Fully Sharded Data Parallel on two A100 GPUs, with an estimated total compute of 80 GPU hours.</p>
<p>The dataset and Qwen come under Apache License, Phi3-mini under MIT.We are open-sourcing all our code under the Apache License.</p>
<p>Initial answer sampling prompt</p>
<p>You are a helpful reasoning assistant in general domain question answering.</p>
<p>Please reason through the question step by step very shortly before giving a final answer.</p>
<p>Generate a short chain-of-thought rationale very shortly, and then provide the final answer.</p>
<p>Step-by-step reasoning: Final Answer:</p>
<p>Question: {question text} Reason step by step very shortly, then conclude with the answer.</p>
<p>Correction sampling prompt</p>
<p>You are a helpful reasoning assistant in general domain question answering.Your task is to correct the initial response if it is incorrect.</p>
<p>Below is the question and the initial answer.Generate a correction to the initial answer if it is incorrect.Disregard the information you already have, look for other options.Do not use the information that does not match your criteria.</p>
<p>Step-by-step reasoning:</p>
<p>Final Answer:</p>
<p>Question: Which Portuguese soccer player has the most goals?</p>
<p>Step-by-step reasoning: Christiano Ronaldo has scored 900 goals in a career spanning 22 years.I cannot think of anyone else scoring more than him.</p>
<p>Final Answer: Christiano Ronaldo</p>
<p>Step-by-step reasoning: Yes, Christiano Ronaldo is the correct answer.Other details are unimportant.</p>
<p>Final Answer: Christiano Ronaldo Question: Which actress played in both movies: one about preg-cy and the other about Tracey?</p>
<p>Step-by-step reasoning: 1. Identify movies about preg-cy and about Tracey.2. Movies about preg-cy include 'Juno' and 'Knocked Up'. 3. Movie about Tracey is likely 'Tracy Chapman' or a documentary, but 'Tracy' could be a reference to 'Tracy Flick' in the movie 'Election'.4. Considering well-known movies with these themes, actress Reese Witherspoon comes to mind as a possible match.</p>
<p>Final Answer: Reese Witherspoon</p>
<p>Step-by-step reasoning: Let's break down the answer.'Juno' is indeed a movie about preg-cy, but it starred Ellen Page (now Elliot Page), not Reese Witherspoon.Reese Witherspoon did not star in 'Juno.'Therefore, the association between Reese Witherspoon and 'Juno' is incorrect.Movies About Tracey is likely 'Tracey Fragments (2007)', a movie in which Ellen Page also starred as the lead character, Tracey Berkowitz.Ellen Page (Elliot Page) connects both movies.</p>
<p>Figure 1 :
1
Figure1: Illustration of the self-improvement method STaR (left)(Zelikman et al., 2022), self-correction method SC (center)(Welleck et al., 2022), and our method, STaSC (right).STaCS offers flexible control over initial answer exploration, correction filtering, and iterative fine-tuning.It is inspired by STaR and effectively encompasses SC as a special case.SC and STaSC allow several initial answers and corrections.The dotted line in the STaSC denotes two possible setups: fine-tuning the model and generating from it at the next iteration (Evolving Fine-Tuning) and keeping the Generator frozen and fine-tuning the Corrector model only (Fixed Fine-Tuning).</p>
<p>Figure 3 :
3
Figure 3: Correction In-accuracy for STaSC versions with Fixed Initialization for Phi3-mini and Qwen-2.5-1.5B.</p>
<p>Figure 5 :
5
Figure 5: Example of the STaSC pipeline.</p>
<p>Final answer: Ellen Page (now Elliot Page) Question: {question text} Initial Answer: {initial answer} Write a correction if the initial answer is incorrect.</p>
<p>Table 1 :
1
005 0.372 ± 0.010 3 0.248 ± 0.011 0.208 ± 0.011 0.326 ± 0.009 0.376 ± 0.010 5 0.230 ± 0.011 0.228 ± 0.021 0.352 ± 0.016 0.384 ± 0.012 Maximum reward r over iterations for initial answer r( Ŷ 1 ) and for correction r( Ŷ 2 ) for different number of samples and initial generations.Bold corresponds to the best performance.
10.236 ± 0.007 0.232 ± 0.018 0.334 ± 0.011 0.362 ± 0.024330.264 ± 0.015 0.238 ± 0.018 0.342 ± 0.010 0.372 ± 0.01250.273 ± 0.017 0.236 ± 0.019 0.332 ± 0.007 0.384 ± 0.01310.273 ± 0.012 0.250 ± 0.024 0.334 ± 0.008 0.378 ± 0.016530.295 ± 0.019 0.244 ± 0.023 0.336 ± 0.021 0.354 ± 0.02650.300 ± 0.020 0.248 ± 0.029 0.350 ± 0.012 0.376 ± 0.011</p>
<p>Initial Analysis: Elliot Page in "Juno": It's correctly noted that Elliot Page (formerly Ellen Page) starred in "Juno," a film centered around teenage pregnancy.However, the analysis doesn't connect Elliot Page to a film involving a character named Tracey.Hilary Swank in "Boys Don't Cry": Hilary Swank portrayed Brandon Teena in "Boys Don't Cry," not a character named Tracey.This film doesn't align with the criteria of featuring a character named Tracey.Reese Witherspoon in "Legally Blonde" and "Sweet Home Alabama": Reese Witherspoon's characters in these films are named Elle Woods and Melanie Smooter, respectively.Neither film prominently features a character named Tracey, nor are they primarily about pregnancy.
correctanswerAnswer: Ellen (Elliot) PageCorrect Information:"Juno" (2007): Elliot Page plays the titular character, Juno MacGuff, a teenager dealingwith an unplanned pregnancy."The Tracey Fragments" (2007): Elliot Page portrays Tracey Berkowitz, a 15-year-oldnavigating personal challenges.Final answer: Elliot Page</p>
<p>. Jyoti Marah Abdin, Hany Aneja, Ahmed Awadalla, Ammar Awadallah, Nguyen Ahmad Awan, Amit Bach, Arash Bahree, Jianmin Bakhtiari, Harkirat Bao, Alon Behl, Misha Benhaim, Johan Bilenko, Sébastien Bjorck, Martin Bubeck, Qin Cai, Vishrav Cai, Dong Chaudhary, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Chen, Parul Cheng, Xiyang Chopra, Matthew Dai, Ronen Dixon, Victor Eldan, Jianfeng Fragoso, Mei Gao, Min Gao, Amit Gao, Allie Garg, Abhishek Del Giorno, Suriya Goswami, Emman Gunasekar, Junheng Haider, Russell J Hao, Wenxiang Hewett, Jamie Hu, Dan Huynh, Sam Ade Iter, Mojan Jacobs, Xin Javaheripi, Nikos Jin, Piero Karampatziakis, Mahoud Kauffmann, Dongwoo Khademi, Young Kim, Jin Kim, Lev Kurilenko, James R Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak ; Sambudha, Olatunji Roy, Olli Ruwase, Amin Saarikivi, Adil Saied, Michael Salim, Shital Santacroce, Ning Shah, Hiteshi Shang, Yelong Sharma, Swadheen Shen, Xia Shukla, Masahiro Song, Andrea Tanaka, Praneetha Tupini, Chunyu Vaddamanu, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Wang, Wen Ward, Philipp Wen, Haiping Witte, Xiaoxia Wu, Michael Wu, Bin Wyatt, Can Xiao, Jiahang Xu, Weijian Xu, Jilong Xu, Sonali Xue, Fan Yadav, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yang, Lu Yu, Chenruidong Yuan, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Zhang, arXiv:2404.14219Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset,PreprintYunan Zhang, and Xiren Zhou. 2024. Phi-3 technical report: A highly capable language model locally on your phone</p>
<p>Naheed Anjum Arafat, and Arijit Khan. 2024. Logical consistency of large language models in fact-checking. Bishwamittra Ghosh, Sarah Hasan, arXiv:2412.16100Preprint</p>
<p>Critic: Large language models can selfcorrect with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382024Preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu, 10.1145/3703155ACM Transactions on Information Systems. 4322025</p>
<p>Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C Park, arXiv:2403.14403Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. 2024arXiv preprint</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2305.069832023Preprint</p>
<p>When can llms actually correct their own mistakes? a critical survey of selfcorrection of llms. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, Rui Zhang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Training language models to selfcorrect via reinforcement learning. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, arXiv:2409.129172024arXiv preprint</p>
<p>Natural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 72019</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, arXiv:2303.176512023Preprint</p>
<p>Viktor Moskvoretskii, Maria Lysyuk, Mikhail Salnikov, Nikolay Ivanov, Sergey Pletenev, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Irina Nikishina, Alexander Panchenko, arXiv:2501.12835Adaptive retrieval without self-knowledge? bringing uncertainty back home. 2025arXiv preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<dl>
<dt>REFINER: Reasoning feedback on intermediate representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers</dt>
<dd>
<p>Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, arXiv:2412.15115Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,Preprint</p>
</dd>
</dl>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, arXiv:2212.105092022arXiv preprint</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi ; Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, arXiv:2211.00053arXiv:2501.04682Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though. 2022. 2025arXiv preprint</p>
<p>Instructscore: Explainable text generation evaluation with finegrained feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang, Wang , Lei Li, arXiv:2305.142822023arXiv preprint</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, arXiv:2203.14465Star: Bootstrapping reasoning with reasoning. 2022Preprint</p>
<p>Small language models need strong verifiers to self-correct reasoning. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang, arXiv:2404.171402024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7913 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7913</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7913</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-e7175cb23b175c07644ad6f037e27f2f43203764</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e7175cb23b175c07644ad6f037e27f2f43203764" target="_blank">Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen, and proposes a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely outputs according to humans.</p>
                <p><strong>Paper Abstract:</strong> A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely outputs according to humans. We compare our method with several state-of-the-art controlled text generation models via automatic and human evaluations and show that our models' capability of generating less likely outputs is improved.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7913.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7913.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Brainstorm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Brainstorm (contrastive controlled generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-time controllable text-generation method that fine-tunes an encoder-decoder LM (BART) with an indicator token and two contrastive objectives (margin loss and a similarity/contrastive loss) to produce 'less likely' but relevant hypotheses/interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Liyan Tang et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Brainstorm</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune an encoder–decoder LM conditioned on an indicator (likely vs less-likely). Add a margin loss that enforces lower probability for the same target under the opposite indicator, and a contrastive similarity loss that pulls encoder–indicator representations toward correct decoder outputs and away from in-batch (or hard) negatives; at generation time, condition on the 'less-likely' indicator to shift distribution toward less-probable human hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Natural language findings/premises (brain MRI report findings; premise/observation pairs from ART and E-CARE)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual hypotheses/interpretations (controlled to be 'less likely' or 'likely')</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Indicator-conditioned fine-tuning (special control token) combined with contrastive/margin objectives (no few-shot prompting; training-time control)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-Large (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>400M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>ART (abductive reasoning), E-CARE (causal reasoning), MRIINTERPRET (radiology findings / interpretations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Automatic classifier fraction of less-likely outputs (Frac), perplexity (PPL), human relevance/fluency annotation categories, hallucination rate (human expert)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Significantly increased fraction of outputs judged 'less likely' vs baselines (e.g., Brainstorm achieved ~79.4% Frac on ART automatic eval vs ~56.6% for MLE-LL), maintained comparable perplexity in many settings, ablations show both margin and similarity losses are important; in MRIINTERPRET shifted generated diagnoses toward rarer but relevant interpretations though hallucination rates remained nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Hallucination in generated interpretations (especially in clinical setting); dataset imbalance and limited expert annotation (single neurologist for MRI evaluations); domain/generalization concerns (single center radiology reports); not a production-ready clinical tool.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7913.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7913.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quark: Controllable text generation with reinforced unlearning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-time controllable generation technique that uses a learned attribute/reward model to assign rewards and trains the generator to 'unlearn' undesired behaviors via reinforcement-style updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quark: Controllable text generation with reinforced unlearning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Quark: Controllable text generation with reinforced unlearning</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ximing Lu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Quark</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a generator with reinforcement-style updates to maximize a reward from a strong attribute classifier (reward model); used here to encourage generation of 'less likely' hypotheses by scoring candidates with a DeBERTa classifier and reinforcing outputs classified as less-likely.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text premises (ART, E-CARE) used in this study; generally any text input for conditional generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Reward-based fine-tuning / reinforced unlearning (training-time control via classifier reward), not few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied here on BART-based generator; reward model used: DeBERTa classifier</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>generator: BART-Large in this paper (400M); reward model: DeBERTa (size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>ART, E-CARE (in this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Frac (automatic classifier), perplexity, human relevancy/fluency categories</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Automatic eval: Quark attained higher automatic 'less-likely' fraction (e.g., 85.9% Frac on ART) and lower PPL in some settings; human eval showed Quark often produced lower-quality less-likely outputs (many contradictory outputs and negation shortcuts).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on reward/classifier that can be gamed (classifier shortcuts, e.g., negation tokens); can produce low-quality/contradictory outputs despite high automatic scores; requires a reliable attribute classifier which may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7913.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7913.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DExperts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DExperts (decoding-time experts and anti-experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time controlled generation method that reweights next-token logits by combining an 'expert' LM (desirable attribute) and an 'anti-expert' LM (undesirable attribute) to bias sampling without retraining the base model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DExperts: Decoding-time controlled text generation with experts and anti-experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>DExperts: Decoding-time controlled text generation with experts and anti-experts</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Alisa Liu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2021</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DExperts (modified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>At each decoding step, combine logits from models trained on desired and undesired corpora to produce a modified next-token distribution (here modified to combine models trained on y~ and y+ distributions); hyperparameter alpha controls strength of adjustment.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text premises/contexts (used here for ART, E-CARE)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual hypotheses generated at decoding time</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Decoding-time logit reweighting (no special prompting; uses separate expert/anti-expert LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied on BART-derived models in this paper (expert/anti-expert variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>ART, E-CARE (experimented in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Frac (automatic), perplexity, human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Can increase fraction of less-likely outputs but often at the cost of substantially increased perplexity and reduced fluency; effectiveness varied by dataset (more effective than CD on ART at certain settings).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher perplexity / degraded fluency at strong settings; distribution shift may break automatic classifiers; requires training separate expert/anti-expert models.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7913.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7913.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrastive Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Decoding (CD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time method that uses a contrast between a stronger 'expert' model and a weaker 'amateur' model to search for outputs that are favored by the expert but not by the amateur, improving open-ended generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Contrastive decoding: Open-ended text generation as optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Contrastive decoding: Open-ended text generation as optimization</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Xiang Lisa Li et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Contrastive Decoding (CD) (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute decoding scores that reward tokens where the expert model assigns higher likelihood than an amateur model, with a scaling factor tau_CD controlling the penalty; here adapted with same-size models and tuned tau to reduce 'likely' outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text premises/contexts (ART, E-CARE in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Decoding-time contrastive scoring between two models (no special prompt engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied with same-size models derived from BART in this work</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>ART, E-CARE (evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Frac (automatic), perplexity, human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Improved fraction of less-likely outputs at some tau values but often caused large increases in perplexity and degraded fluency; dataset-dependent effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Trade-off between attribute shift and fluency/perplexity; may push generation distribution out-of-domain for automatic classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7913.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7913.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plug and Play Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time control method that steers a frozen pre-trained LM using gradients from an attribute model to bias generation toward desired attributes without fine-tuning the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Plug and play language models: A simple approach to controlled text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Plug and play language models: A simple approach to controlled text generation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Sumanth Dathathri et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2020</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PPLM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use an attribute classifier to compute gradients that perturb the hidden activations of a frozen LM during decoding to increase likelihood of attribute-consistent tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text prompts / contexts (general)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Controlled textual generation</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Gradient-based decoding-time control (no fine-tuning required)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Originally demonstrated on GPT-2; not applied in this paper's experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned as representative decoding-time controlled generation method; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires attribute classifier; computational overhead at decoding; not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7913.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7913.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FUDGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FUDGE: Controlled text generation with future discriminators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time approach that trains a discriminator to predict desired attributes of completions and uses its scores to guide sampling from a base LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FUDGE: Controlled text generation with future discriminators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>FUDGE: Controlled text generation with future discriminators</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Kevin Yang and Dan Klein</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2021</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>FUDGE</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a future discriminator that predicts attribute labels for partial continuations, and bias generation by reweighting token probabilities according to discriminator scores at decoding time.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text prompts / contexts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Controlled textual generation</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Decoding-time discriminator reweighting (no generator fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited in related work as a decoding-time controlled generation method; not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mention only; not empirically evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7913.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7913.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CTRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CTRL: A conditional transformer language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large conditional transformer trained with control codes to enable controllable generation over many attributes by conditioning generation on special tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ctrl: A conditional transformer language model for controllable generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Ctrl: A conditional transformer language model for controllable generation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Nitish Shirish Keskar et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2019</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CTRL</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pre-train a transformer with special control codes prepended to inputs to steer generation to different attributes/domains at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text with control codes</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Controlled textual generation</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Control-code conditioning (training-time)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as a representative training-time controllable generation approach in related work; not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7913.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7913.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoNT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoNT: Contrastive neural text generation (Cont)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive learning approach for text generation that uses self-generated hard negatives to improve generation quality; referenced as similar in spirit to Brainstorm's in-batch/hard-negative contrastive objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cont: Contrastive neural text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Cont: Contrastive neural text generation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Chenxin An et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CoNT (Contrastive neural text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply contrastive learning to sequence generation by creating hard positives/negatives through perturbations or back-translation and optimizing objectives that pull correct outputs closer and push alternatives away.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text pairs / generated candidates</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved textual generation (less errors/firmer alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Contrastive training with hard negatives (training-time)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Referenced as related work and as inspiration for the similarity loss; not directly used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7913.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7913.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unlikelihood training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unlikelihood training for neural text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training objective that penalizes outputs with undesired tokens or behaviors by minimizing likelihood of negative tokens, used to reduce repetition or other unwanted attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural text generation with unlikelihood training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Neural text generation with unlikelihood training</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Sean Welleck et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2020</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Unlikelihood training</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Add an unlikelihood loss term to training that explicitly reduces probability mass on undesired tokens or sequences (e.g., repetitions), complementing MLE.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text sequences; training-time negative examples</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Text generation with fewer undesired attributes</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Training-time loss modification (no special prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned as prior controllable training objective; not directly evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Quark: Controllable text generation with reinforced unlearning <em>(Rating: 2)</em></li>
                <li>DExperts: Decoding-time controlled text generation with experts and anti-experts <em>(Rating: 2)</em></li>
                <li>Contrastive decoding: Open-ended text generation as optimization <em>(Rating: 2)</em></li>
                <li>Plug and play language models: A simple approach to controlled text generation <em>(Rating: 1)</em></li>
                <li>FUDGE: Controlled text generation with future discriminators <em>(Rating: 1)</em></li>
                <li>Cont: Contrastive neural text generation <em>(Rating: 1)</em></li>
                <li>Neural text generation with unlikelihood training <em>(Rating: 1)</em></li>
                <li>Ctrl: A conditional transformer language model for controllable generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7913",
    "paper_id": "paper-e7175cb23b175c07644ad6f037e27f2f43203764",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "Brainstorm",
            "name_full": "Brainstorm (contrastive controlled generation)",
            "brief_description": "A training-time controllable text-generation method that fine-tunes an encoder-decoder LM (BART) with an indicator token and two contrastive objectives (margin loss and a similarity/contrastive loss) to produce 'less likely' but relevant hypotheses/interpretations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
            "authors": "Liyan Tang et al.",
            "year": null,
            "method_name": "Brainstorm",
            "method_description": "Fine-tune an encoder–decoder LM conditioned on an indicator (likely vs less-likely). Add a margin loss that enforces lower probability for the same target under the opposite indicator, and a contrastive similarity loss that pulls encoder–indicator representations toward correct decoder outputs and away from in-batch (or hard) negatives; at generation time, condition on the 'less-likely' indicator to shift distribution toward less-probable human hypotheses.",
            "input_type": "Natural language findings/premises (brain MRI report findings; premise/observation pairs from ART and E-CARE)",
            "output_type": "Textual hypotheses/interpretations (controlled to be 'less likely' or 'likely')",
            "prompting_technique": "Indicator-conditioned fine-tuning (special control token) combined with contrastive/margin objectives (no few-shot prompting; training-time control)",
            "model_name": "BART-Large (fine-tuned)",
            "model_size": "400M parameters",
            "datasets_used": "ART (abductive reasoning), E-CARE (causal reasoning), MRIINTERPRET (radiology findings / interpretations)",
            "evaluation_metric": "Automatic classifier fraction of less-likely outputs (Frac), perplexity (PPL), human relevance/fluency annotation categories, hallucination rate (human expert)",
            "reported_results": "Significantly increased fraction of outputs judged 'less likely' vs baselines (e.g., Brainstorm achieved ~79.4% Frac on ART automatic eval vs ~56.6% for MLE-LL), maintained comparable perplexity in many settings, ablations show both margin and similarity losses are important; in MRIINTERPRET shifted generated diagnoses toward rarer but relevant interpretations though hallucination rates remained nontrivial.",
            "limitations": "Hallucination in generated interpretations (especially in clinical setting); dataset imbalance and limited expert annotation (single neurologist for MRI evaluations); domain/generalization concerns (single center radiology reports); not a production-ready clinical tool.",
            "counterpoint": true,
            "uuid": "e7913.0",
            "source_info": {
                "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Quark",
            "name_full": "Quark: Controllable text generation with reinforced unlearning",
            "brief_description": "A training-time controllable generation technique that uses a learned attribute/reward model to assign rewards and trains the generator to 'unlearn' undesired behaviors via reinforcement-style updates.",
            "citation_title": "Quark: Controllable text generation with reinforced unlearning",
            "mention_or_use": "use",
            "paper_title": "Quark: Controllable text generation with reinforced unlearning",
            "authors": "Ximing Lu et al.",
            "year": 2022,
            "method_name": "Quark",
            "method_description": "Train a generator with reinforcement-style updates to maximize a reward from a strong attribute classifier (reward model); used here to encourage generation of 'less likely' hypotheses by scoring candidates with a DeBERTa classifier and reinforcing outputs classified as less-likely.",
            "input_type": "Text premises (ART, E-CARE) used in this study; generally any text input for conditional generation",
            "output_type": "Textual hypotheses",
            "prompting_technique": "Reward-based fine-tuning / reinforced unlearning (training-time control via classifier reward), not few-shot prompting",
            "model_name": "Applied here on BART-based generator; reward model used: DeBERTa classifier",
            "model_size": "generator: BART-Large in this paper (400M); reward model: DeBERTa (size not specified)",
            "datasets_used": "ART, E-CARE (in this paper's experiments)",
            "evaluation_metric": "Frac (automatic classifier), perplexity, human relevancy/fluency categories",
            "reported_results": "Automatic eval: Quark attained higher automatic 'less-likely' fraction (e.g., 85.9% Frac on ART) and lower PPL in some settings; human eval showed Quark often produced lower-quality less-likely outputs (many contradictory outputs and negation shortcuts).",
            "limitations": "Relies on reward/classifier that can be gamed (classifier shortcuts, e.g., negation tokens); can produce low-quality/contradictory outputs despite high automatic scores; requires a reliable attribute classifier which may not generalize.",
            "counterpoint": true,
            "uuid": "e7913.1",
            "source_info": {
                "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DExperts",
            "name_full": "DExperts (decoding-time experts and anti-experts)",
            "brief_description": "A decoding-time controlled generation method that reweights next-token logits by combining an 'expert' LM (desirable attribute) and an 'anti-expert' LM (undesirable attribute) to bias sampling without retraining the base model.",
            "citation_title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
            "mention_or_use": "use",
            "paper_title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
            "authors": "Alisa Liu et al.",
            "year": 2021,
            "method_name": "DExperts (modified in this paper)",
            "method_description": "At each decoding step, combine logits from models trained on desired and undesired corpora to produce a modified next-token distribution (here modified to combine models trained on y~ and y+ distributions); hyperparameter alpha controls strength of adjustment.",
            "input_type": "Text premises/contexts (used here for ART, E-CARE)",
            "output_type": "Textual hypotheses generated at decoding time",
            "prompting_technique": "Decoding-time logit reweighting (no special prompting; uses separate expert/anti-expert LMs)",
            "model_name": "Applied on BART-derived models in this paper (expert/anti-expert variants)",
            "model_size": null,
            "datasets_used": "ART, E-CARE (experimented in this paper)",
            "evaluation_metric": "Frac (automatic), perplexity, human evaluation",
            "reported_results": "Can increase fraction of less-likely outputs but often at the cost of substantially increased perplexity and reduced fluency; effectiveness varied by dataset (more effective than CD on ART at certain settings).",
            "limitations": "Higher perplexity / degraded fluency at strong settings; distribution shift may break automatic classifiers; requires training separate expert/anti-expert models.",
            "counterpoint": true,
            "uuid": "e7913.2",
            "source_info": {
                "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Contrastive Decoding",
            "name_full": "Contrastive Decoding (CD)",
            "brief_description": "A decoding-time method that uses a contrast between a stronger 'expert' model and a weaker 'amateur' model to search for outputs that are favored by the expert but not by the amateur, improving open-ended generation.",
            "citation_title": "Contrastive decoding: Open-ended text generation as optimization",
            "mention_or_use": "use",
            "paper_title": "Contrastive decoding: Open-ended text generation as optimization",
            "authors": "Xiang Lisa Li et al.",
            "year": 2022,
            "method_name": "Contrastive Decoding (CD) (adapted)",
            "method_description": "Compute decoding scores that reward tokens where the expert model assigns higher likelihood than an amateur model, with a scaling factor tau_CD controlling the penalty; here adapted with same-size models and tuned tau to reduce 'likely' outputs.",
            "input_type": "Text premises/contexts (ART, E-CARE in experiments)",
            "output_type": "Textual hypotheses",
            "prompting_technique": "Decoding-time contrastive scoring between two models (no special prompt engineering)",
            "model_name": "Applied with same-size models derived from BART in this work",
            "model_size": null,
            "datasets_used": "ART, E-CARE (evaluated in this paper)",
            "evaluation_metric": "Frac (automatic), perplexity, human evaluation",
            "reported_results": "Improved fraction of less-likely outputs at some tau values but often caused large increases in perplexity and degraded fluency; dataset-dependent effectiveness.",
            "limitations": "Trade-off between attribute shift and fluency/perplexity; may push generation distribution out-of-domain for automatic classifiers.",
            "counterpoint": true,
            "uuid": "e7913.3",
            "source_info": {
                "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PPLM",
            "name_full": "Plug and Play Language Models",
            "brief_description": "A decoding-time control method that steers a frozen pre-trained LM using gradients from an attribute model to bias generation toward desired attributes without fine-tuning the generator.",
            "citation_title": "Plug and play language models: A simple approach to controlled text generation",
            "mention_or_use": "mention",
            "paper_title": "Plug and play language models: A simple approach to controlled text generation",
            "authors": "Sumanth Dathathri et al.",
            "year": 2020,
            "method_name": "PPLM",
            "method_description": "Use an attribute classifier to compute gradients that perturb the hidden activations of a frozen LM during decoding to increase likelihood of attribute-consistent tokens.",
            "input_type": "Text prompts / contexts (general)",
            "output_type": "Controlled textual generation",
            "prompting_technique": "Gradient-based decoding-time control (no fine-tuning required)",
            "model_name": "Originally demonstrated on GPT-2; not applied in this paper's experiments",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Mentioned as representative decoding-time controlled generation method; not evaluated in this paper.",
            "limitations": "Requires attribute classifier; computational overhead at decoding; not evaluated here.",
            "counterpoint": null,
            "uuid": "e7913.4",
            "source_info": {
                "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FUDGE",
            "name_full": "FUDGE: Controlled text generation with future discriminators",
            "brief_description": "A decoding-time approach that trains a discriminator to predict desired attributes of completions and uses its scores to guide sampling from a base LM.",
            "citation_title": "FUDGE: Controlled text generation with future discriminators",
            "mention_or_use": "mention",
            "paper_title": "FUDGE: Controlled text generation with future discriminators",
            "authors": "Kevin Yang and Dan Klein",
            "year": 2021,
            "method_name": "FUDGE",
            "method_description": "Train a future discriminator that predicts attribute labels for partial continuations, and bias generation by reweighting token probabilities according to discriminator scores at decoding time.",
            "input_type": "Text prompts / contexts",
            "output_type": "Controlled textual generation",
            "prompting_technique": "Decoding-time discriminator reweighting (no generator fine-tuning)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited in related work as a decoding-time controlled generation method; not used in experiments in this paper.",
            "limitations": "Mention only; not empirically evaluated here.",
            "counterpoint": null,
            "uuid": "e7913.5",
            "source_info": {
                "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CTRL",
            "name_full": "CTRL: A conditional transformer language model",
            "brief_description": "A large conditional transformer trained with control codes to enable controllable generation over many attributes by conditioning generation on special tokens.",
            "citation_title": "Ctrl: A conditional transformer language model for controllable generation",
            "mention_or_use": "mention",
            "paper_title": "Ctrl: A conditional transformer language model for controllable generation",
            "authors": "Nitish Shirish Keskar et al.",
            "year": 2019,
            "method_name": "CTRL",
            "method_description": "Pre-train a transformer with special control codes prepended to inputs to steer generation to different attributes/domains at inference time.",
            "input_type": "Text with control codes",
            "output_type": "Controlled textual generation",
            "prompting_technique": "Control-code conditioning (training-time)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited as a representative training-time controllable generation approach in related work; not used in experiments here.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7913.6",
            "source_info": {
                "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoNT",
            "name_full": "CoNT: Contrastive neural text generation (Cont)",
            "brief_description": "A contrastive learning approach for text generation that uses self-generated hard negatives to improve generation quality; referenced as similar in spirit to Brainstorm's in-batch/hard-negative contrastive objective.",
            "citation_title": "Cont: Contrastive neural text generation",
            "mention_or_use": "mention",
            "paper_title": "Cont: Contrastive neural text generation",
            "authors": "Chenxin An et al.",
            "year": 2022,
            "method_name": "CoNT (Contrastive neural text generation)",
            "method_description": "Apply contrastive learning to sequence generation by creating hard positives/negatives through perturbations or back-translation and optimizing objectives that pull correct outputs closer and push alternatives away.",
            "input_type": "Text pairs / generated candidates",
            "output_type": "Improved textual generation (less errors/firmer alignment)",
            "prompting_technique": "Contrastive training with hard negatives (training-time)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Referenced as related work and as inspiration for the similarity loss; not directly used in experiments.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7913.7",
            "source_info": {
                "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Unlikelihood training",
            "name_full": "Unlikelihood training for neural text generation",
            "brief_description": "A training objective that penalizes outputs with undesired tokens or behaviors by minimizing likelihood of negative tokens, used to reduce repetition or other unwanted attributes.",
            "citation_title": "Neural text generation with unlikelihood training",
            "mention_or_use": "mention",
            "paper_title": "Neural text generation with unlikelihood training",
            "authors": "Sean Welleck et al.",
            "year": 2020,
            "method_name": "Unlikelihood training",
            "method_description": "Add an unlikelihood loss term to training that explicitly reduces probability mass on undesired tokens or sequences (e.g., repetitions), complementing MLE.",
            "input_type": "Text sequences; training-time negative examples",
            "output_type": "Text generation with fewer undesired attributes",
            "prompting_technique": "Training-time loss modification (no special prompting)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Mentioned as prior controllable training objective; not directly evaluated in this work.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7913.8",
            "source_info": {
                "paper_title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Quark: Controllable text generation with reinforced unlearning",
            "rating": 2
        },
        {
            "paper_title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
            "rating": 2
        },
        {
            "paper_title": "Contrastive decoding: Open-ended text generation as optimization",
            "rating": 2
        },
        {
            "paper_title": "Plug and play language models: A simple approach to controlled text generation",
            "rating": 1
        },
        {
            "paper_title": "FUDGE: Controlled text generation with future discriminators",
            "rating": 1
        },
        {
            "paper_title": "Cont: Contrastive neural text generation",
            "rating": 1
        },
        {
            "paper_title": "Neural text generation with unlikelihood training",
            "rating": 1
        },
        {
            "paper_title": "Ctrl: A conditional transformer language model for controllable generation",
            "rating": 1
        }
    ],
    "cost": 0.021096749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</h1>
<p>Liyan Tang ${ }^{\text { }}$ Yifan Peng ${ }^{\text {A }}$ Yanshan Wang ${ }^{\text {A }}$ Ying Ding ${ }^{\text { }}$<br>Greg Durrett ${ }^{\text { }}$ Justin F. Rousseau ${ }^{\text { }}$<br>${ }^{\text {The }}$ University of Texas at Austin<br>${ }^{\text {A }}$ Weill Cornell Medicine ${ }^{\text {A }}$ University of Pittsburgh<br>lytang@utexas.edu</p>
<h4>Abstract</h4>
<p>A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely outputs according to humans. We compare our method with several state-of-the-art controlled text generation models via automatic and human evaluations and show that our models' capability of generating less likely outputs is improved. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Cognitive errors occur when an abnormality is identified, but its importance is incorrectly understood, resulting in an incorrect final diagnosis (Onder et al., 2021; Bruno et al., 2015). For example, radiologists may look for confirmatory evidence to support a diagnostic hypothesis and ignore or discount evidence that refutes the hypothesis (confirmation bias; Busby et al. (2018); Onder et al. (2021)). One</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples from MRIINTERPRET and ECARE datasets. The task is to generate interpretations or hypotheses that humans would consider to be "less likely" to happen but still relevant to the context. " + " and " $\sim$ " represent likely and less likely outputs, respectively.
way to reduce the likelihood of such cognitive errors is to provide cognitive "help" by having a devil's advocate (Seah et al., 2021; Waite et al., 2017). For this purpose, we propose a new text generation task called "less likely brainstorming" to produce less likely but relevant consultations to bring fresh eyes to examine a case-a powerful way to correct diagnostic errors.</p>
<p>Here, we consider less likely hypotheses in two scenarios. First, they can be hypotheses that humans think are likely but not among the most likely to happen. These hypotheses are critical to providing second opinion of a prior clinical study but are often difficult to generate by traditional decoding techniques. Second, they can be hypotheses that are indeed impossible according to humans, but are close to being true if certain counterfactual assumptions about the input hold. These hypotheses are also helpful as they are often ignored by clinicians. There is a tendency for clinicians to look for a confirmatory diagnostic hypothesis but ignore a refutable one. Note that a less likely hypothesis</p>
<p>reflects the likelihood of a potential diagnosis from the human perspective, not from the probability of model output.</p>
<p>We propose Brainstorm, a novel contrastive learning strategy to generate "less likely" hypotheses. We treat this problem as a text generation task as text generation models are the most flexible for providing predictions and explanations for complex tasks; they can generalize to new examples and produce complex, structured diagnoses in many formats. Generation of the "less likely hypotheses" is conditioned on an indicator variable set to trigger the model to prefer outputs are less likely according to humans. For this purpose, we propose two additional loss objectives to effectively learn the relationship between input context, the indicator, and outputs. Without our training strategy, using naive controlled generation training, we find that conditioning on the indicator often leads to generating "highly likely" or irrelevant outputs.</p>
<p>We explore this task in two settings: everyday commonsense reasoning and brain magnetic resonance imaging (MRI) interpretation generation (more details in Section 5). In the everyday commonsense reasoning setting, we adapt Art (Bhagavatula et al., 2020) and E-CARE (Du et al., 2022), which both contain "less plausible" or "implausible" hypotheses that fit our definition of less likely. An illustrative example asking for less likely hypotheses can be found in Figure 1. We show that our approach can generate more "less likely" hypotheses than baselines, including models directly fine-tuned on this set, past controllable generation approaches (Lu et al., 2022), or models with alternate decoding (Li et al., 2022; Liu et al., 2021). In the brain MRI interpretation setting, we experiment with predicting diagnoses from brain MRI reports (see Figure 1). Assessment by a neurologist reveals that our model successfully shifts the distribution of generated diagnoses further toward the tail while still generating relevant diagnoses.</p>
<h2>2 Related Work</h2>
<p>Uncertainty in Radiology Interpretation Uncertainty plays a significant role in the process of clinical decision making (Croskerry, 2013). When facing uncertainty, physicians may resort to various erroneous strategies, such as denying the presence of uncertainty resulting in various interpretation biases. These biases could lead to unexpected consequences (Kim and Lee, 2018; Eddy, 1984), including missed diagnoses, misdiagnoses, unnecessary diagnostic examinations and even lifethreatening situations (Farnan et al., 2008). Recent work (Seah et al., 2021; Waite et al., 2017) have provided deep-learning based methods and suggestions in reducing errors from interpretation bias on medical imaging. To the best of our knowledge, we are the first to explore reducing bias from interpreting radiology reports via our less likely text generation framework.</p>
<p>Controllable text generation and decoding methods Controllable text generation is the task of generating text that adheres certain attributes, such as language detoxification (Zhang and Song, 2022; Liu et al., 2021; Dathathri et al., 2020), formality modification (Mireshghallah et al., 2022; Yang and Klein, 2021) and open-ended story generation (Mori et al., 2022; Lin and Riedl, 2021; Fan et al., 2018). The task of controllable text generation encompasses both training-time and decoding-time methods. Training-time approaches include CTRL (Keskar et al., 2019), which learns to utilize control codes to govern attributes in order to generate the desired text, and Quark (Lu et al., 2022), which leverages a strong attribute classifier as a reward function to unlearn unwanted attributes. These methods typically rely on training data that contains both the desired and undesired attributes to be effective in the supervised setting. Our method falls into this category.</p>
<p>On the other hand, decoding-time methods utilize off-the-shelf pre-trained LMs (PLMs) and aim to re-rank the probability of generated text based on specific constraints. PPLM (Dathathri et al., 2020) and FUDGE (Yang and Klein, 2021) are typical methods in this category that train an attribute classifier to guide PLMs to generating desired text. DExperts (Liu et al., 2021) and Contrastive Decoding (Li et al., 2022) are more recent methods that re-weight generation probabilities by contrasting the output distributions between different LMs. We select those two as strong baselines for comparison against our proposed model.</p>
<p>Contrastive Learning in NLP Contrastive learning (CL) has been applied to a wide range of representation learning tasks in NLP, such as learning task-agnostic sentence representation (Gao et al., 2021) and improving natural language understanding (Jaiswal et al., 2021; Qu et al., 2021). It has recently been applied to text generation tasks as</p>
<p>well (An et al., 2022; Cao and Wang, 2021; Lee et al., 2021) where additional hard positive or negative examples are created through techniques such as back-translation or perturbation.</p>
<h2>3 Problem Setting</h2>
<p>The problem we tackle in this work can be viewed as a controllable text generation task. Let $x$ be a premise or a brain MRI report findings, we want a model to generate a likely/less likely hypothesis or interpretation $y$ given an indicator $i$ by drawing from the distribution $P(y \mid x, i)$. The indicator $i$ can take two values: + to indicate generating likely outputs and $\sim$ to generate less likely outputs.</p>
<p>For example, given a premise $x=$ "Tom goes to the gym every day," in Figure 1 from the ECARE dataset (more details in Section 5), we want a model to generate a hypothesis $y^{\sim}$ that is less likely to happen $(i=\sim)$ after $x$, such as "He gets a promotion from his manager who saw him in the gym.". Although this hypothesis fits into the same scenario as the premise as it directly connects to the premise involving Tom's daily gym attendance, it is less likely to happen since the causal relationship between going to the gym and receiving a promotion is not common. The understanding of what is "less likely" can be based on the concept of bounded rationality (Simon, 1955), where likely hypotheses are those that are likely given known premises, but less likely hypotheses may stem from additional unknown premises.</p>
<p>It is important to note that when we refer to an output as "less likely/likely", we mean that it is less likely/likely based on human understanding of $x$. All models we experiment with in this work generate outputs that have high probability according to the model, regardless of whether they are likely or less likely to happen according to humans.</p>
<h2>4 Methodology</h2>
<p>In this section, we present our method as well as baseline models we compare against. Requirements for these models can be found in Table 1. We use BART (Lewis et al., 2020) as the backbone LM for all experimental settings.</p>
<h3>4.1 Brainstorm</h3>
<p>Our encoder-decoder system takes the concatenation of a pair $(x, i)$ as input and returns one or multiple generated output sequences $y$. At decoding time $t$, our model iteratively decodes the next token conditioned on the left-hand context, i.e., $y_{&lt;t}$:</p>
<p>$$
P_{\mathrm{LM}}(y)=\prod_{t}^{T} P_{\mathrm{LM}}\left(y_{t} \mid x, i, y_{&lt;t}\right)
$$</p>
<p>where $P_{\mathrm{LM}}\left(y_{t} \mid x, i, y_{&lt;t}\right)$ is the next token distribution given the context. The task inputs are described in Section 5.</p>
<p>Besides the standard maximum likelihood training with human reference, we incorporate two additional loss objectives to guide models to associate the context, indicators, and target sequences. The training approach is illustrated in Figure 2.</p>
<p>Margin Loss First, given the indicator $i$, we want the model to assign a higher estimated probability to human reference $y$ than its opposite indicator $\neg i$. Therefore, we apply a margin-based loss:
$\mathcal{L}_{\text {margin }}=\max (0, P(y \mid x, \neg i)-P(y \mid x, i)+m)$
where $m$ is the margin value. This loss objective tells models that if the indicator is modified, then the target sequence should have lower probability. Margin loss does not require both likely and less likely outputs $y^{+}$and $y^{\sim}$.</p>
<p>Similarity Loss We propose two versions of a contrastive similarity loss based on the availability of examples that can be used in CL. When both positive and negative examples are available in the same batch, we define the similarity loss as</p>
<p>$$
\mathcal{L}<em i="i" x_="x,">{\text {sim }}=-\log \frac{\exp \left(\operatorname{sim}\left(\mathbf{z}</em>}, \mathbf{z<em _hat_y="\hat{y">{y}\right) / \tau\right)}{\sum</em>} \in \text { batch }} \exp \left(\operatorname{sim}\left(\mathbf{z<em _hat_y="\hat{y">{x, i}, \mathbf{z}</em>
$$}}\right) / \tau\right)</p>
<p>Here, $\mathbf{z}<em y="y">{x, i}, \mathbf{z}</em>}$, and $\mathbf{z<em _sim="{sim" _text="\text">{\hat{y}}$ represent the hidden representations of input $(x, i)$, human reference $y$, and an output $\hat{y}$ in the same batch. $\mathcal{L}</em>}}$ encourages the model to maximize the agreement between $\mathbf{z<em y="y">{x, i}$ and its corresponding output $\mathbf{z}</em>$. This loss objective encourages a model to learn the relation between certain indicators and the target sequence by contrasting the target sequence with all negative outputs in the batch.</p>
<p>This objective term resembles that in CoNT (An et al., 2022) which takes self-generated outputs as negative samples; here, we conditioned the input on special indicators. Note that at the training time, the indicator $i$ could be either + or $\sim$. When the indicator $i=+$, the hard negative is the human reference of $y^{\sim}$, and vice versa. We set the weight of the term in Equation (3) associated with the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: An overview of Brainstorm using an example from E-CARE, which consists of three objectives. $\mathbf{z}<em y_="y^{+">{x, i}$ is the encoder representation of the input $x$ conditioned on an indicator $i$. $\mathbf{z}</em>}}, \mathbf{z<em _hat_y="\hat{y">{y^{\sim}}$ and $\mathbf{z}</em>$ objective is highlighted in red where it requires both likely and less likely data.
hard negative to 10 throughout the experiment to increase its importance relative to in-batch negatives.}}$ are the decoder representations of positive, hard negative, and other negative target sequences within the same batch, respectively. The $\mathcal{L}_{\text {sim }</p>
<p>When positive and negative examples are not available at the same time (denoted by a lack of a "pair" check in Table 1), we propose an alternative similarity loss objective $\mathcal{L}<em i="i" x_="x,">{\text {sim }}^{\prime}$ that minimizes the similarity of encoder representation $\mathbf{z}</em>$, without comparing to outputs in the batch:}$ and $\mathbf{z}_{x, \neg i</p>
<p>$$
\mathcal{L}<em i="i" x_="x,">{\text {sim }}^{\prime}=\operatorname{sim}\left(\mathbf{z}</em>\right)
$$}, \mathbf{z}_{x, \neg i</p>
<p>We use cosine similarity for both versions.
Final Loss The overall training objective of Brainstorm is the combination of the standard maximum likelihood estimation (MLE) $\mathcal{L}_{\text {MLE }}$, margin loss, and similarity loss:</p>
<p>$$
\mathcal{L}<em _mathrm_CE="\mathrm{CE">{\text {final }}=\mathcal{L}</em>}}+w_{s} \mathcal{L<em m="m">{\text {sim }}+w</em>
$$} \mathcal{L}_{\text {margin }</p>
<p>where $w_{s}$ and $w_{m}$ are hyperparameters. BRAINSTORM ${ }^{\prime}$ replaces $\mathcal{L}<em _sim="{sim" _text="\text">{\text {sim }}$ by $\mathcal{L}</em>$.}}^{\prime</p>
<h3>4.2 Baselines</h3>
<h3>4.2.1 Training-Time Baselines</h3>
<p>Mle and Mle-LL Mle is trained on all data. It is a conditional model $p(y \mid x, i)$ that learns to generate both $y^{+}$and $y^{\sim}$ depending on $i$. Mle-LL learns to generate less likely outputs $y^{\sim}$ by only training on $\left(x, y^{\sim}\right)$. Both models are trained with standard MLE.</p>
<p>Quark (Lu et al., 2022) is a state-of-the-art controllable text generation method that outperforms
methods such as unlikelihood training (Welleck et al., 2020). Quark trains an LM to generate text with fewer undesirable properties by maximizing rewards assigned by a reward function. In this study, we use the DeBERTa model (He et al., 2020) as the reward function to help generate more $y^{\sim}$ (more details in Section 6).</p>
<h3>4.2.2 Decoding-Time Baselines</h3>
<p>Modified DExperts DExperts (Liu et al., 2021) combines a base LM $M$ along with two language models called "expert" ( $M_{\text {exp }}$ ) and "antiexpert" ( $M_{\text {anti }}$ ) that model text with desired and undesired properties, respectively. The next token distribution is determined by $P_{\text {DExperts }}\left(y_{t}\right)=$ $\sigma\left(z_{t}^{\prime}+\alpha\left(z_{t}^{\text {exp }}-z_{t}^{\text {anti }}\right)\right)$ where $z$ is the logits for the next token $y_{t}$ and $z_{t}^{\prime}$ is the truncated logits from $M$ under any truncation sampling methods such as top$k$ sampling. For simplicity, we omit the preceding context in the notation. The hyperparameter $\alpha$ controls how far the final token distribution deviates from model $M$.</p>
<p>In our setting, we modify this definition to be</p>
<p>$$
P_{\text {DExperts }^{\prime}}\left(y_{t}\right)=\sigma\left(z_{t}^{\sim}+\alpha\left(z_{t}^{\text {neu }}-z_{t}^{+}\right)\right)
$$</p>
<p>Here, $z_{t}^{+}$is from the model that learns to generate $\hat{y}^{+}$by only training on $\left(x, y^{+}\right)$pairs. $z_{t}^{\text {neu }}$ is from the model that learns to generate both $y^{+}$and $y^{\sim}$ conditioned on the indicator. Unlike MLE, this model does not condition on indicators to generate hypotheses. Instead, it leverages text with both desired (generating $y^{\sim}$ ) and undesired properties (generating $y^{+}$). It is shown to effectively maintain</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Data</th>
<th></th>
<th></th>
<th>Need</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$+$</td>
<td>$\sim$</td>
<td>pair</td>
<td>Clf.</td>
</tr>
<tr>
<td>Training-time Method</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MLE-LL</td>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MLE</td>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td></td>
</tr>
<tr>
<td>Quark</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Brainstorm</td>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td></td>
</tr>
<tr>
<td>Brainstorm</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Decoding-time Method</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DExperts</td>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td></td>
</tr>
<tr>
<td>CD</td>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Requirements for various methods. $+$/ $\sim$/pair means a method requires $y^{+}$/y^ both for $x$. Quark can take any type of data as inputs but requires a trained classifier. We use Brainstorm as an alternative of Brainstorm if $y^{+}$and $y^{\sim}$ are not both available for $x$. DExperts and CD require that both $y^{+}$and $y^{\sim}$ could be available for $x$ (which is not the case for MRIInTERPRET, Section 7).
the fluency of the generated text <em>Liu et al. (2021)</em>. $z_{t}^{\sim}$ is from a base LM that generates $y^{\sim}$ only. It can be MLE-LL or Brainstorm.</p>
<p>Modified Contrastive Decoding Contrastive Decoding (CD) combines a larger $M_{\text {exp }}$ and a smaller "amateur" model ( $M_{\text {ama }}$ ) and searches for text under a constrained search space <em>Li et al. (2022)</em>. The resulting outputs are intended to amplify the strengths of $M_{\text {exp }}$ and remove undesired properties that appear in $M_{\text {ama }}$. A scaling factor $\tau_{\mathrm{CD}}$ controls the penalties of the amateur model in CD.</p>
<p>In our setting, two models have the same size. $M_{\text {ama }}$ learns to generate $y^{+}$; $M_{\text {exp }}$ can be MLELL or Brainstorm. Intuitively, the ability to generate $y^{\sim}$ is preserved, while the tendency to generate $y^{+}$is factored out.</p>
<p>Hyperparameters We experiment with a wide range of values for $\alpha$ in DExperts and $\tau_{\mathrm{CD}}$ in CD and show how the fraction changes across these values in Figure 3. We keep the recommended value for the remaining hyperparameters. Unless specified otherwise, we generate outputs using diverse beam search <em>Vijayakumar et al. (2016)</em>.</p>
<h2>5 Experimental Settings</h2>
<p>We investigate our methods in both brain MRI settings and everyday commonsense reasoning settings (Table 5).</p>
<h3>5.1 Everyday Commonsense Reasoning</h3>
<p>Two datasets from the commonsense reasoning domain were adapted. See examples in Figure 4 from Appendix.</p>
<p>ART (Abductive Reasoning in narrative Text; Bhagavatula2020) is a large-scale benchmark dataset that tests models’ language-based abductive reasoning skills over narrative contexts. Each instance in the dataset consists of two observations $O_{1}$ and $O_{2}$ ( $O_{1}$ happened before $O_{2}$ ), as well as a likely and a less likely hypothesis event (happening in between $O_{1}$ and $O_{2}$ ) collected from crowd workers. Each "likely" hypothesis is causally related to two observations and each "less likely" hypothesis is created by editing each "likely" hypothesis. The original task is to generate a likely hypothesis given the observation pair $\left(O_{1}, O_{2}\right)$.</p>
<p>E-CARE (Explainable CAusal REasoning; Du et al. 2022) tests models’ causal reasoning skills. Each instance in the dataset consists of a premise, a "likely" and a "less likely" hypothesis, and a conceptual explanation of the causality. The likely hypothesis can form a valid causal fact with the premise. Two tasks are introduced: (1) causal reasoning: choosing the "likely" hypothesis given a premise and (2) explanation generation: generating an explanation for the causal fact.</p>
<p>Adapted Setting In our adapted setting, we want a model $F$ to generate $y^{\sim}$ given either an observation pair (ART) or a premise (E-CARE) $x$. Formally, let $E$ be a binary evaluator $E(x, y) \in{1,0}$ that classifies an output $y$ into either $y^{+}$or $y^{\sim}$ based on $x$. We want a model $F$ that generates $\hat{y}=F(x, i=\sim)$, where $E(x, \hat{y})=0$.</p>
<p>Evaluation For ART, we use the default training, validation and test sets to evaluate our models. For E-CARE, we randomly construct training and validation sets from the original training set and use the default validation set as the test set since the original test set is not available. All hyperparameters are determined on the validation set.</p>
<p>For each instance $x$ in the test set, we ask a model $F$ to generate $\hat{y}=F(x, i=\sim)$, then measure the fraction of less likely hypotheses according to an evaluator $E$.</p>
<p>To reduce ambiguity and encourage more consistent human evaluations, we formally define all relevancy categories from rounds of pilot studies. More detailed definitions and annotation instructions can be found in Appendix B and C. We measure both</p>
<p>the (1) relevancy and (2) fluency of generated hypothesis in human evaluation.</p>
<h3>5.2 MRIInTERPRET</h3>
<p>We present a new dataset MRIInTERPRET based on the findings and impression sections of a set of de-identified radiology reports we collected from brain MRIs. Each instance consists of a findings $x$, an indicator $i$, and a likely/less likely interpretation $y$ of the findings $x$ depending on $i$.</p>
<p>Dataset Construction We first find phrases such as "likely represents", "consistent with", and "may be unrelated to" that represent uncertainty from each sentence of reports. We view these phrases as indicators of the presence of interpretations; denote them by $s^{+}$or $s^{\sim}$. A likely or less likely indicator (Appendix F) suggests a likely or less likely interpretation of a finding. For each likely indicator $s^{+}$, we treat the sub-sentence preceding $s^{+}$concatenated with prior 6 sentences as the findings $x$, and the completion of the sentence following $s^{+}$as the likely interpretation $y^{+}$of the findings $x$. We include prior sentences to provide more context for reaching interpretations. For less likely indicators $s^{\sim}$, we treat the sub-sentence either following or preceding $s^{\sim}$ as the less likely interpretation of the findings depending on how $s^{\sim}$ is stated. An example can be found in Figure 4.</p>
<p>Indicator Unification We have collected a variety of indicators and decided to unify them into a minimum set for both likely and less likely indicators. More details of indicator unification can be found in Appendix F.</p>
<p>Evaluation To ensure the human evaluation for MRIInTERPRET to be as reliable as possible, we carefully curate a thorough annotation instruction guideline with precise definitions for all relevancy labels in Section 7 and Appendix E.</p>
<h2>6 Evaluation on Commonsense Reasoning</h2>
<h3>6.1 Automatic Evaluation</h3>
<p>Our first evaluation relies on automatically assessing whether system outputs are likely or less likely according to humans. We fine-tune DeBERTa models (He et al., 2020) for our automatic evaluation on two everyday commonsense datasets. They take the pair of $(x, y)$ as input and predict whether $y$ is a likely or less likely hypothesis. In our settings,</p>
<table>
<thead>
<tr>
<th></th>
<th>ART</th>
<th></th>
<th>E-CARE</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Model</td>
<td>$\operatorname{Frac}(\uparrow)$</td>
<td>$\operatorname{PPL}(\downarrow)$</td>
<td>$\operatorname{Frac}(\uparrow)$</td>
<td>$\operatorname{PPL}(\downarrow)$</td>
</tr>
<tr>
<td>Mle</td>
<td>54.1</td>
<td>42.6</td>
<td>54.5</td>
<td>80.4</td>
</tr>
<tr>
<td>Mle-LL</td>
<td>56.6</td>
<td>42.5</td>
<td>52.6</td>
<td>84.8</td>
</tr>
<tr>
<td>+ CD</td>
<td>59.9</td>
<td>49.8</td>
<td>63.4</td>
<td>107.3</td>
</tr>
<tr>
<td>+ DExPERTS</td>
<td>56.2</td>
<td>51.7</td>
<td>57.2</td>
<td>108.3</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Brainstorm</td>
<td>79.4</td>
<td>40.7</td>
<td>58.1</td>
<td>69.2</td>
</tr>
<tr>
<td>+ CD</td>
<td>79.7</td>
<td>50.2</td>
<td>67.2</td>
<td>88.1</td>
</tr>
<tr>
<td>+ DExPERTS</td>
<td>79.0</td>
<td>51.5</td>
<td>58.1</td>
<td>89.3</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Quark</td>
<td>85.9</td>
<td>27.5</td>
<td>68.2</td>
<td>80.8</td>
</tr>
<tr>
<td>Brainstorm</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$-\mathcal{L}_{\text {margin }}$</td>
<td>69.3</td>
<td>44.9</td>
<td>54.6</td>
<td>73.2</td>
</tr>
<tr>
<td>$-\mathcal{L}_{\text {sim }}$</td>
<td>58.2</td>
<td>52.6</td>
<td>53.2</td>
<td>83.7</td>
</tr>
<tr>
<td>Brainstorm $^{\prime}$</td>
<td>58.3</td>
<td>52.0</td>
<td>55.1</td>
<td>71.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of generating less likely hypothesis on ART test set and E-CARE validation set. For DExPERTS and CD, we list the fractions where models reach minimum PPL. The ablation study of our proposed method is shown at the bottom.
the fine-tuned DeBERTa model achieves $85 \%$ accuracy on the test set of ART and achieves $80 \%$ on the original validation set of E-CARE.</p>
<p>Table 2 compares a number of methods on our commonsense reasoning datasets. We answer several questions based on these results. We perform a paired bootstrap test for each result by comparing to Mle-LL. We highlight results that are better at 0.05 level of significance.</p>
<p>Can we just train on $\left(x, y^{\sim}\right)$ ? Interestingly, the baseline model Mle-LL that only trained on $\left(x, y^{\sim}\right)$ pairs generates "likely" hypotheses approximately half of the time. This is possibly an effect of the pre-training regimen; furthermore, generating likely hypotheses may be easier and past work has shown that seq2seq models can amplify behaviors like copying that are easy to learn (Goyal et al., 2022).</p>
<p>Are the proposed two loss objectives effective? We see that compared to Mle-LL, our proposed BRAINSTORM method achieves substantially higher fractions of less likely hypotheses with no cost to quality in terms of perplexity. At the bottom of Table 2, we show that ablating either of the proposed loss objectives worsens performance (and note that ablating both yields MLE). BRAINSTORM $^{\prime}$ is not as effective since it does not compare with outputs in the batch, but we can see its merits in MRIInTERPRET (Section 7).</p>
<p>Can decoding-time methods alleviate the problem of generating likely outputs? We explore</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Fraction-perplexity trade-off of decoding-time methods CD and DExperts on Art test set and original E-CARE validation set (our test set). We show the trade-off across various values for τCD in CD and α in DExperts. Both CD and DExperts can improve the fraction of less likely hypotheses, but at a very high cost to perplexity.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Art</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>E-CARE</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Likely</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(↓)</td>
<td>L-Likely</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(↑)</td>
<td>Contra.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(?)</td>
<td>Rep.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(↓)</td>
<td>Irrel.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(↓)</td>
<td>Likely</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(↓)</td>
<td>L-Likely</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(↑)</td>
<td>Contra.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(?)</td>
<td>Rep.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(↓)</td>
<td>Irrel.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(↓)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mle-LL</td>
<td>42.3</td>
<td>15.2</td>
<td>22.7</td>
<td>9.5</td>
<td>10.3</td>
<td>35.4</td>
<td>15.6</td>
<td>5.7</td>
<td>18.6</td>
<td>24.7</td>
</tr>
<tr>
<td>Quark</td>
<td>14.7</td>
<td>20.8</td>
<td>51.0</td>
<td>4.3</td>
<td>9.2</td>
<td>35.2</td>
<td>15.1</td>
<td>5.7</td>
<td>3.3</td>
<td>40.7</td>
</tr>
<tr>
<td>Brainstorm</td>
<td>20.9</td>
<td>20.2</td>
<td>41.3</td>
<td>4.8</td>
<td>12.8</td>
<td>37.1</td>
<td>20.1</td>
<td>4.7</td>
<td>12.7</td>
<td>25.4</td>
</tr>
</tbody>
</table>
<p>Table 3: Human evaluations on Art and E-CARE. We see that our method is able to produce more "less likely" (L-Likely) outputs on both datasets. We calculated the mean of the ratings from multiple annotators for each sample.</p>
<p>whether DExperts and CD can further raise the fraction of less likely generations when combined with either Mle-LL or Brainstorm. These methods have hyperparameters that trade off how much of the "undesired" behavior each can remove from the system. We compute several fraction-perplexity trade-off curves in Figure 3. Notably, although the fraction of less likely outputs can improve, <strong>both of these methods significantly increase the perplexity of generations</strong>, which corresponds with notably worse fluency of the text. Although these points apparently have high less likely fractions, we caution that the distribution of the text may deviate from the text that DeBERTa was fine-tuned on, meaning that our classifiers may not work well in these ranges. The green lines reflect thresholds where we observe serious degradation in output quality starting to occur. Below this perplexity threshold, the automatic evaluation suggests that both methods demonstrate some capability in alleviating the models' tendency in generating "likely" hypotheses without too great a cost to perplexity. Note that DExperts is more effective than CD in Art and vice versa in E-CARE.</p>
<p>Table 2 reports the settings where models achieve the minimum perplexities; at these points, perplexity is substantially increased but the fraction of less likely hypotheses is not substantially changed for the majority of results.</p>
<p><strong>Can Quark yield improvement?</strong> In Table 2, the automatic evaluation results show that Quark exceeds Brainstorm by generating 6% more "less likely" hypothesis in Art and 10% more in E-CARE. It also has lower perplexity in Art. To further compare the two models, we conducted a human evaluation on the outputs from two models, and the result shows that Quark generates lower-quality "less likely" hypotheses (Section 6.2).</p>
<h3>6.2 Human Evaluation</h3>
<p>To further validate the results, we conduct a fine-grained human evaluation on a sample of 100 examples from the test sets of both datasets along two axes – relevancy and fluency. We refined our relevancy evaluation by dividing the "relevancy" category into four subcategories, resulting in a total of five categories for evaluation: (1) <em>Likely</em>; (2) <em>Less likely</em>; (3) <em>Contradictory</em> - the output is impossible if we assume the input is true; (4) <em>Repetition</em> - the output is describing the same meaning as the input; and (5) <em>Irrelevant</em> - the output has little connection with input. More thorough category definitions with examples, annotation instruc</p>
<p>tion and quality checks for AMT annotators can be found in Appendix C. We compare the performance of three models: MLE-LL, Brainstorm, and Quark (Table 3). As Quark demonstrates better performance in automatic evaluation, we include its generated text in our human evaluation.</p>
<p>Our results show a high level of agreement between the automatic evaluation (Table 2) and human evaluation (Table 3) regarding the fraction of "likely" hypotheses on both datasets. On Art, Quark and Brainstorm decrease the fraction of "likely" hypotheses by $60 \%$ and $50 \%$, respectively, compared to MLE-LL. However, on E-CARE, the human evaluation indicates that all three models generate an equivalent number of "likely" hypotheses. By further breaking down the "relevancy" category used in the automatic evaluation, we then have a clearer understanding of the distribution of categories among the models' outputs.</p>
<p>Low-Quality Hypotheses It is not desirable for models to generate outputs that are repetitions of the input (Repetition) or have little connection to the input (Irrelevant). On the Art dataset, all models generate a small proportion of irrelevant outputs, with Quark and Brainstorm reducing the fraction of "Repetition" hypotheses by half, compared to MLE-LL. However, we get more low-quality outputs on E-CARE. While Brainstorm is able to reduce the fraction of Repetition hypotheses by a large margin, it is not as effective as Quark. One possible reason for this is that Quark is trained to generate outputs that the DeBERTa classifier (the reward model) predicts as less likely; Repetition cases are rarely classified as less likely due to their similarity with the input, but Irrelevant outputs are more likely to be classified this way.</p>
<p>Less Likely versus Contradictory While less likely hypotheses are desirable, contradictory hypotheses are less so. A typical way of generating a contradictory hypothesis is by simply adding negation: Lisa went laptop shopping yesterday $\rightarrow$ Lisa didn't go laptop shopping yesterday. However, such examples have little value as the negation brings no new information to the input and is not a useful counterfactual for a user to see.</p>
<p>We evaluate the models' outputs on the Art dataset, where a significant number of contradictory hypotheses are generated, and find that 43 out of 100 hypotheses generated by Quark include the words "didn't" or "not," while only 10 hypotheses generated by Brainstorm and Mle-LL did so. We posit that this is likely due to the DeBERTa classifier assigning high rewards for hypotheses that include negation words, and Quark effectively learning this shortcut.</p>
<h2>7 Human Evaluation on MRIINTERPRET</h2>
<p>To evaluate the models' performance on the radiological interpretation generation setting, we select 30 findings from our validation set that ask for less likely interpretation. For each finding, we select the human reference and generate the top 5 less likely interpretations from 2 baselines (MLE-LL and MLE) and Brainstorm', resulting in $30 \times$ $(5 \times 3+1)=480$ interpretations. We randomized the order of these interpretations before evaluation.</p>
<p>Due to the structure of the indicators in this dataset, methods that require examples to have both $y^{+}$and $y^{-}$for the same data (see "pair" in Table 1) are not able to be used. Since Quark relies on a trained classifier, we choose not to use Quark as well. A trained classifier on MRIINTERPRET is not reliable since the training set only consists of naturally occurring data, which is highly imbalanced (see Table 5 in Appendix). This leads the classifier to perform poorly on the "less likely" class, which is the minority class but is also the class of greatest interest in this study. We find that augmenting the training data with counterfactual cases is not easy. For example, "the lack of evidence of restricted diffusion makes it less likely to be" is a naturally occurring prompt from a less likely example, and attempting to change it to a sentence such as "the lack of evidence of restricted diffusion could represent" yields a statement that turns out to be out of distribution from the training data and models do not behave reliably in these counterfactual cases.</p>
<p>For each generated interpretation, we evaluate its (1) relevancy to the findings and (2) whether it contains any hallucinations about findings (Appendix E.2). For relevancy, we asked a neurologist to classify each interpretation into: (1) Relevant and likely; (2) Relevant and less likely; and (3) Irrelevant. Further, for those classified as "Relevant and less likely", we further evaluate how well the interpretation fits into the context of the findings by grading them on three levels: high, medium and low, ranging from high matches that represent the most obvious less likely interpretations to low matches that represent relevant but exceedingly rare diagnosis. We provide detailed definitions for these</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Likely</th>
<th>Less likely</th>
<th></th>
<th></th>
<th>Irrel.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>High</td>
<td>Med.</td>
<td>Low</td>
<td></td>
</tr>
<tr>
<td>MLE-LL</td>
<td>6.7</td>
<td>40.7</td>
<td>21.2</td>
<td>14.7</td>
<td>16.7</td>
</tr>
<tr>
<td>MLE</td>
<td>7.3</td>
<td>50.0</td>
<td>22.1</td>
<td>13.3</td>
<td>7.3</td>
</tr>
<tr>
<td>BRAINSTORM $^{r}$</td>
<td>6.7</td>
<td>42.0</td>
<td>32.6</td>
<td>8.7</td>
<td>10.0</td>
</tr>
<tr>
<td>Reference</td>
<td>3.3</td>
<td>76.7</td>
<td>13.4</td>
<td>3.3</td>
<td>3.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Human Evaluation on MRIINTERPRET. Results are shown as percentages. We evaluated $30 \times 5$ $=150$ less likely interpretations generated from each model and 30 less likely interpretations from human reference. Results show that our proposed model successfully shifts the distribution of generated interpretations further toward the tail of the "relevant but less likely" category but still generates relevant diagnoses.
categories and include comprehensive annotation guidelines in Appendix E to facilitate consistency in future studies.</p>
<p>Results are shown in Table 4. Most human references (which the neurologist was blinded to) are annotated as either a high or medium match under the relevant but less likely category, suggesting the reliability of the neurologist's annotation. We find that training on all data (MLE) instead of exclusively on less likely data (MLE-LL) would effectively help generate more relevant but less likely interpretations and reduce the amount of irrelevant ones. One possible reason is that MRIINTERPRET is a highly imbalanced dataset (Table 5).</p>
<p>By comparing the outcomes between human reference and Brainstorm, we find that Brainstorm tends to shift the distribution of generated interpretations towards generating lower matched interpretations, which effectively extends the beam of potential diagnoses that meet the criteria of "relevant but less likely" based on refuting findings. Anecdotally, interpretations in this medium category reflect the sort of alternative hypotheses and "outside-the-box" suggestions that represent the original goal of our approach.</p>
<h2>8 Conclusion</h2>
<p>In this work, we propose a new text generation task "less likely brainstorming" for reducing cognitive errors in interpreting findings of MRI reports. We found that simply training on less likely data does not help with generating less likely interpretations and hence propose a novel CL method to tackle the problem. In two settings, we show that our proposed training technique can effectively generate more "less likely" hypotheses, producing interpre-
tations that radiologists may not think of, outperforming past training- and decode-time modifications to generation models.</p>
<h2>Limitations</h2>
<p>Our brain MRI interpretations were evaluated by a single neurologist. Such annotations require deep expertise and are not easily carried out with high quality by trainees, which limited the amount of data we were able to collect. To ensure that the annotation would be as reliable as possible, we carefully thought of the dimensions in evaluating the generated interpretations and proposed a thorough annotation instruction guideline. We believe that future work can conduct more extensive studies using our annotation guidelines as a starting point. Further, the radiology reports we experiment with are from a single academic medical center, which makes the generalizability unclear. Future work is needed to evaluate the performance of our models on data from different medical centers. Finally, future work is needed to evaluate relevant and likely outputs from MRI interpretations to address different forms of interpretation bias and to expand the beam of potential likely diagnoses based on the findings.</p>
<p>Beyond the brain MRI interpretation experiments, our generation experiments are limited to a set of pre-trained models optimized for carrying out generation tasks in English. It is possible that multilingual models generating in languages other than English will show different properties. We are limited by the availability of resources for automatic evaluation in these settings, but a more extensive multilingual evaluation with human users could be conducted in the future.</p>
<h2>Ethical Risks</h2>
<p>We are proposing better ways for incorporating systems into the radiological diagnostic process. This is aimed at helping improve human decisionmaking and mitigating the limitations of traditional fully-automatic approaches. However, we believe that it is imperative to rigorously test and evaluate these methods before they can be put into practical clinical settings. We are not claiming that these methods are ready for real-world adoption at this stage.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Darcey Riley and TAUR lab at UT for discussion about DExperts and for providing feedback on this work. We acknowledge the funding support from National Science Foundation AI Center Institute for Foundations of Machine Learning (IFML) at University of Texas at Austin (NSF 2019844), as well as NSF CAREER Award IIS-2145280 and IIS-2145640, National Library of Medicine under Award No. 4R00LM013001, and a gift from Salesforce, Inc.</p>
<h2>References</h2>
<p>Chenxin An, Jiangtao Feng, Kai Lv, Lingpeng Kong, Xipeng Qiu, and Xuanjing Huang. 2022. Cont: Contrastive neural text generation. abs/2205.14690.</p>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In International Conference on Learning Representations.</p>
<p>Michael A. Bruno, Eric A. Walker, and Hani H. Abujudeh. 2015. Understanding and confronting our mistakes: The epidemiology of error in radiology and strategies for error reduction. RadioGraphics, 35(6):1668-1676.</p>
<p>Lindsay P. Busby, Jesse L. Courtier, and Christine M. Glastonbury. 2018. Bias in radiology: The how and why of misses and misinterpretations. RadioGraphics, 38(1):236-247.</p>
<p>Shuyang Cao and Lu Wang. 2021. CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633-6649, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Pat Croskerry. 2013. From mindless to mindful practice - cognitive bias and clinical decision making. New England Journal of Medicine, 368(26):2445-2448.</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations.</p>
<p>Li Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin. 2022. e-CARE: a new dataset for exploring explainable causal reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 432-446, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>David M. Eddy. 1984. Variations in physician practice: The role of uncertainty. Health Affairs, 3(2):74-89.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>J M Farnan, J K Johnson, D O Meltzer, H J Humphrey, and V M Arora. 2008. Resident uncertainty in clinical decision making and impact on patient care: a qualitative study. Quality and Safety in Health Care, 17(2):122-126.</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Tanya Goyal, Jiacheng Xu, Junyi Jessy Li, and Greg Durrett. 2022. Training dynamics for text summarization models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 20612073, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decodingenhanced bert with disentangled attention. ArXiv, abs/2006.03654.</p>
<p>Ajay Jaiswal, Liyan Tang, Meheli Ghosh, Justin F. Rousseau, Yifan Peng, and Ying Ding. 2021. Radbert-cl: Factually-aware contrastive learning for radiology report classification. In Proceedings of Machine Learning for Health, volume 158 of Proceedings of Machine Learning Research, pages 196-208. PMLR.</p>
<p>Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. ArXiv, abs/1909.05858.</p>
<p>Kangmoon Kim and Young-Mee Lee. 2018. Understanding uncertainty in medicine: concepts and implications in medical education. Korean Journal of Medical Education, 30(3):181-188.</p>
<p>Seanie Lee, Dong Bok Lee, and Sung Ju Hwang. 2021. Contrastive learning with adversarial perturbations for conditional text generation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training</p>
<p>for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2022. Contrastive decoding: Open-ended text generation as optimization.</p>
<p>Zhiyu Lin and Mark Riedl. 2021. Plug-and-blend: A framework for controllable story generation with blended control codes. In Proceedings of the Third Workshop on Narrative Understanding, pages 62-71, Virtual. Association for Computational Linguistics.</p>
<p>Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021. DExperts: Decoding-time controlled text generation with experts and anti-experts. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6691-6706, Online. Association for Computational Linguistics.</p>
<p>Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning. ArXiv, abs/2205.13636.</p>
<p>Fatemehsadat Mireshghallah, Kartik Goyal, and Taylor Berg-Kirkpatrick. 2022. Mix and match: Learningfree controllable text generationusing energy language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 401-415, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yusuke Mori, Hiroaki Yamane, Ryohei Shimizu, and Tatsuya Harada. 2022. Plug-and-play controller for story completion: A pilot study toward emotionaware story writing assistance. In Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022), pages 46-57, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Omer Onder, Yasin Yarasir, Aynur Azizova, Gamze Durhan, Mehmet Ruhi Onur, and Orhan Macit Ariyurek. 2021. Errors, discrepancies and underlying bias in radiology with case examples: a pictorial review. Insights into Imaging, 12(1).</p>
<p>Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Weizhu Chen, and Jiawei Han. 2021. Coda: Contrastenhanced and diversity-promoting data augmentation for natural language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Jarrel C Y Seah, Cyril H M Tang, Quinlan D Buchlak, Xavier G Holt, Jeffrey B Wardman, Anuar Aimoldin, Nazanin Esmaili, Hassan Ahmad, Hung Pham, John F Lambert, Ben Hachey, Stephen J F Hogg, Benjamin P Johnston, Christine Bennett, Luke Oakden-Rayner, Peter Brotchie, and Catherine M Jones. 2021. Effect of a comprehensive deeplearning model on the accuracy of chest x-ray interpretation by radiologists: a retrospective, multireader multicase study. The Lancet Digital Health, 3(8):e496-e506.</p>
<p>Herbert A. Simon. 1955. A behavioral model of rational choice. The Quarterly Journal of Economics, 69(1):99.</p>
<p>Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2016. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424.</p>
<p>Stephen Waite, Jinel Scott, Brian Gale, Travis Fuchs, Srinivas Kolla, and Deborah Reede. 2017. Interpretive error in radiology. American Journal of Roentgenology, 208(4):739-749.</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In International Conference on Learning Representations.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Kevin Yang and Dan Klein. 2021. FUDGE: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3511-3535, Online. Association for Computational Linguistics.</p>
<p>Hongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang, Yutao Xie, and Sheng Yu. 2022. Biobart: Pretraining and evaluation of a biomedical generative language model.</p>
<p>Hanqing Zhang and Dawei Song. 2022. Discup: Discriminator cooperative unlikelihood prompt tuning for controllable text generation. In The 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi.</p>
<h2>A Dataset statistics</h2>
<p>Dataset statistics can be found in Table 5.</p>
<h2>B Definition of Relevancy Categories on Everyday Commonsense</h2>
<p>To encourage more consistent human evaluations, we formally define all relevancy categories as the following. These definitions are refined from rounds of pilot studies to reduce ambiguity for human annotations. Example outputs and explanations for each relevancy category can be found in the annotation interface (Figure 5 and 7).</p>
<h3>B.1 E-CARE</h3>
<p>Relevant A hypothesis is relevant if it fits with the same scenario as the premise. It should not introduce new people, places, or things that are not at least plausibly in the same source scenario.</p>
<p>Likely For the hypothesis to be likely, it must also be causally related to the premise - either the premise causes the hypothesis or the hypothesis causes the premise (you will see both versions of the task below). There should not be clearly more likely hypotheses than it.</p>
<p>Relevant and Less likely The hypothesis is still the same scenario as the premise (relevant). However, it is less likely to be causally related to the premise. There could be other hypotheses that are superior to the given hypothesis.</p>
<p>Irrelevant The generated hypothesis does not describe the same scenario as the premise or is not causally related to the premise.</p>
<p>Contradictory The hypothesis contradicts the premise - it says something that is impossible if we assume the premise to be true (e.g., the premise states that something happened and the hypothesis states that that thing did not happen).</p>
<p>Repetition The hypothesis is very similar to the premise - it either contains a text span that is a repetition of the premise, or it is expressing nearly the same meaning as the premise.</p>
<h3>B. 2 ART</h3>
<p>Relevant A hypothesis is relevant if it fits with the same scenario as the observation pair. It should not introduce new people, places, or things that are not at least plausibly in the same source scenario.</p>
<p>Likely For the hypothesis to be likely, it must also be strongly related to $O_{1}$ and $O_{2}$ in a causal fashion - to the extent possible, the first observation $O_{1}$ should cause the hypothesis and the hypothesis causes the second observation $O_{2}$. There should not be clearly more likely hypotheses than it.</p>
<p>Relevant and Less likely The hypothesis is still the same scenario as the observation pair (relevant). However, it is less likely to be causally related to the observation pair - maybe it could happen following $O_{1}$, but not necessarily. There could be other hypotheses that are superior to the given hypothesis.</p>
<p>Irrelevant The hypothesis does not describe the same scenario as the observation pair: it either involves different people, places, or things, or the events it describes have very little connection to $O_{1}$ and $O_{2}$.</p>
<p>Contradictory The hypothesis contradicts either observation $O_{1}$ or observation $O_{2}$ - it says something that is impossible if we assume $O_{1}$ and $O_{2}$ to be true (e.g., $O_{2}$ states that something happened and the hypothesis states that that thing did not happen).</p>
<p>Repetition The hypothesis is very similar to either $O_{1}$ or $O_{2}$ - it either contains a text span that is a repetition of $O_{1}$ or $O_{2}$, or it is expressing nearly the same meaning as $O_{1}$ or $O_{2}$.</p>
<h2>C Annotation on Everyday Commonsense</h2>
<p>The human evaluation by crowdworkers has been judged to be IRB exempt. We hired crowd annotators from US through Amazon Mechanical Turk. These annotators have lifetime approval rates over $99 \%$ and more than 1000 approved HITs. We first conducted a quality check on ART and E-CARE. For each dataset, we randomly selected 100 examples from the test set and each example is evaluated by 7 annotators, resulting in $100 \times 7=700$ annotations for each dataset. We finally selected 7 qualified crowdworkers from each of the datasets. The procedure of filtering out non-qualified workers is shown below. For qualified crowdworkers, we randomly select another 100 examples from each dataset and conduct a final annotation round, resulting in $100 \times 7 \times 2=1400$ annotations in total. We set maximum time on completing each HIT to 1 hour and each HIT takes approximately 1.5 minutes. We paid annotators $\$ 0.3 /$ HIT, which</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Train</th>
<th></th>
<th></th>
<th></th>
<th>Val</th>
<th></th>
<th>Test</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Likely</td>
<td></td>
<td>Less Likely</td>
<td></td>
<td>Less Likely</td>
<td></td>
<td>Less Likely</td>
<td></td>
</tr>
<tr>
<td>MRIINTERPRET</td>
<td>10097</td>
<td></td>
<td>1005</td>
<td></td>
<td>121</td>
<td></td>
<td>—</td>
<td></td>
</tr>
<tr>
<td>ART</td>
<td>50509</td>
<td></td>
<td>50509</td>
<td></td>
<td>1781</td>
<td></td>
<td>3562</td>
<td></td>
</tr>
<tr>
<td>E-CARE</td>
<td>cause</td>
<td>effect</td>
<td>cause</td>
<td>effect</td>
<td>cause</td>
<td>effect</td>
<td>cause</td>
<td>effect</td>
</tr>
<tr>
<td></td>
<td>6855</td>
<td>6580</td>
<td>6855</td>
<td>6580</td>
<td>762</td>
<td>731</td>
<td>1088</td>
<td>1044</td>
</tr>
</tbody>
</table>
<p>Table 5: A summary of dataset statistics. All datasets are in English. For ART and E-CARE, we show the stats of our adapted versions. Since E-CARE has a hidden test set, we randomly split the original training set into a training and a validation set, and we use the original validation set as our test set. Note that each example in E-CARE asks for either the cause or the effect of the premise.
is equivalent to $\$ 12 / \mathrm{hr}$ and is higher than the minimum USA wage.</p>
<p>Category definitions and annotation instructions with examples are shown in Figure 5, 6, 7 and 8.</p>
<p>Selecting Qualified Workers After we collected all annotations from the pilot study. We filter out workers by following these steps:</p>
<ol>
<li>We first filter out workers that annotated less than 4 HITs. With limited amount of annotated HITs, it is hard to evaluate the consistency of their annotations.</li>
<li>For any HIT, if two output sequences are exactly the same but the annotator assigned them different categories, then we remove the worker. For example, in E-CARE, if the premise is "Tom goes to the gym every day." and we have the hypotheses "He gets a promotion from his manager who saw him in the gym." that appears twice, then if one hypothesis is classified as "Relevant and Likely" and another one is classified as "Relevant but Less Likely", we will filter out this annotator.</li>
<li>We use the "Repetition" category to further filter out annotators. We believe "Repetition" is the least subjective category in our annotation instruction, and using this category to filter out annotations would lead to minimum bias we can project to the selected annotators. This consists of two steps: (1) A model many generate an output that is exactly the input. For example, a model takes as input "Tom goes to the gym every day." and generate "Tom goes to the gym every day." as well. This happens sometimes across all models. For those cases, we will filter out annotators that assigned categories other than "Repetition"; (2) Besides the exact match, there are cases where a model's
output is a paraphrase of the input. For these, to minimize our bias, we choose to use models' outputs that only differs from the input by at most two words to filter out annotators. For example, in ART, if one observation is "Lisa went laptop shopping yesterday", and the model's output is "She went laptop shopping yesterday", then we filter out annotators that do not assign "Repetition" to it.</li>
</ol>
<p>After we collected all the annotations from qualified workers, we use the above steps to further filter out works that do not meet our standard. Finally, we got valid annotations by three annotators from each datasets. We use Fleiss kappa to calculate the agreement between annotators. The annotators achieve moderate agreement $(\kappa=0.447)$ on ART and fair agreement $(\kappa=0.354)$ on E-CARE for relevancy evaluation. This is within our expectation since evaluating whether a hypothesis is likely or less likely is subjective.</p>
<h2>D Fluency Evaluation on Everyday Commonsense Reasoning</h2>
<p>Fluency evaluation can be found in Table 6. Most of generations from models are fluent and grammatically correct.</p>
<h2>E Annotation on Brain MRI Interpretation</h2>
<p>The use of the brain MRI data is covered by an IRB. A neurologist reviewed each finding sample and evaluated the interpretation on multiple metrics.</p>
<h2>E. 1 Relevancy</h2>
<p>The overall objective of the interpretation generation was to produce less likely diagnoses, or interpretations, based on the absence of specific findings. The findings followed a common pattern of "Absence of [finding x] makes it unlikely to</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">ART</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">E-CARE</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gram. Correct <br> Fluent</td>
<td style="text-align: center;">Contain Flu. <br> Errors</td>
<td style="text-align: center;">Gram. Correct <br> Fluent</td>
<td style="text-align: center;">Contain Flu. <br> Errors</td>
</tr>
<tr>
<td style="text-align: center;">MLE-LL</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">QuARK</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">Brainstorm</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">4.1</td>
</tr>
</tbody>
</table>
<p>Table 6: Human evaluation of fluency on everyday commonsense reasoning datasets. Annotators reached substantial agreement on both datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Examples</th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Brain MRI</td>
<td style="text-align: center;">$\qquad$ Absence of evidence of restricted diffusion makes it unlikely to be <br> 0</td>
<td style="text-align: center;">$\sim$ acute ischemia <br> $\sim$ infarct</td>
<td style="text-align: center;">In diffusion weighted imaging sequences on MRI of the brain, one of the most common causes of diffusion restriction finding is due to acute ischemic stroke, also known as an infarct. Thus, the absence of restricted diffusion within brain tissue makes an interpretation unlikely to be acute ischemia/infarct.</td>
</tr>
<tr>
<td style="text-align: center;">ANLG</td>
<td style="text-align: center;">O1: Lisa went laptop shopping yesterday. O2: She was thankful she bought it.</td>
<td style="text-align: center;">+ The price raised on the next day.</td>
<td style="text-align: center;">This scenario makes sense - Lisa was grateful that she had made her laptop purchase the day before, as the price un-expectedly increased the following day.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$ Lisa decided to buy a car.</td>
<td style="text-align: center;">The event focus on Lisa's purchase of a laptop. It is not likely that she would suddenly decide to buy a car without any prior indication of her interest in doing so.</td>
</tr>
<tr>
<td style="text-align: center;">E-CARE</td>
<td style="text-align: center;">Tom goes to the gym every day.</td>
<td style="text-align: center;">+ Tom improves his physical fitness.</td>
<td style="text-align: center;">It directly relates to Tom's daily gym attendance. Regular exercise is a common and effective method for improving physical fitness.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$ He gets a promotion from his manager who saw him in the gym.</td>
<td style="text-align: center;">It directly connects to the premise involving Tom's daily gym attendance. However, the connection between gym attendance and job promotion is indirect.</td>
</tr>
</tbody>
</table>
<p>Figure 4: Examples from MRIINTERPRET, ART and E-CARE. The example shown in the table for E-CARE asks for a likely/less likely effect of the premise. " $+$ "/" $\sim$ " indicates whether humans would consider the output to be likely/less likely according to the context under the Examples column. We explain why humans would consider these outputs as likely/less likely in the Explanation column (this is not in the training data).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Hallucination (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MLE-LL</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">30.0</td>
</tr>
<tr>
<td style="text-align: center;">Brainstorm</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">6.6</td>
</tr>
</tbody>
</table>
<p>Table 7: Human evaluation on hallucinations. The result shows the percentage of hallucinations found in 150 generated interpretations from each model.
be [interpretation y]." The finding of interest was modified to be standardized across all findings if it used varying terminologies in a similar pattern (see Appendix F for more details). Because the interpretations are oriented in this negated valence, the objective of the output is to produce "relevant but unlikely" interpretations. The annotator rated the interpretation on 3 metrics: (1) relevant and likely, (2) relevant but less likely, and (3) irrelevant.</p>
<p>Relevant and Likely Output was judged as "relevant and likely" if the interpretation erroneously suggested a diagnosis that would be likely, not unlikely, despite the absence of [finding x]. For instance, "Absence of restricted diffusion within the previously described fluid collections along the right convexity makes it unlikely to be". An interpretation of "the presence of a small subdural hematoma" is actually a likely diagnosis given the lack of restricted diffusion in the fluid collection since subdural hematomas do not normally demonstrate restricted diffusion.</p>
<p>Relevant but Less Likely Output was judged as "relevant but less likely" if the interpretation correctly provides a less likely diagnosis due to the absence of [finding x]. For example, "absence of restricted diffusion makes it unlikely to be". An interpretation of "acute ischemia" is unlikely since diffusion restriction is often associated with acute ischemia.</p>
<p>If the interpretation was judged as "relevant but unlikely", the degree to which the interpretation fits with the findings was graded on three levels: (1) high, (2) medium, and (3) low.</p>
<ul>
<li>Less likely interpretations were high matches if they were within the top 5 diagnoses to fit the statement. These were the most obvious interpretations.</li>
<li>Less likely interpretations were medium
matches if they were further down the bar of potential interpretations. They still were relevant to the findings and made sense as being less likely given the absence of the finding of interest, but are less obvious and fall outside of the top 5 diagnoses.</li>
<li>Less likely interpretations were low matches if the interpretation was relevant to the findings, but was an exceedingly rare diagnosis to make it of low value to mention as an interpretation.</li>
</ul>
<p>Irrelevant Output was judged as "irrelevant" if it was not related to the finding of interest or the structure that the finding of interest is referring to.</p>
<h2>E. 2 Presence of Hallucination</h2>
<p>Lastly, no matter the rating of relevance, presence or absence of hallucination was noted. It was possible to have a relevant but unlikely interpretation with high degree of fit with the finding, but a hallucination that does not appear in the original findings was added. We therefore evaluate whether each interpretation contains hallucinations.</p>
<p>The results are shown in Table 7. The models listed contain a large proportion of hallucinated content especially for MLE and Brainstorm. We examined what these hallucinations look like. We found that in the most cases, models hallucinate about the findings (generating some findings that do not actually written in the report) and concatenate those hallucinated findings after their interpretations. For examples, a generated interpretation would be "an acute infarction although this is limited by the presence of contrast enhancement", "intracranial abscess although this is limited by the presence of significant soft tissue swelling", or "blood products in the ventricular system as seen on prior CT."</p>
<p>However, unlike other text generation tasks such as text summarization where hallucinations are hard to identify, hallucinations in MRIINTERPRET follow a pattern of interpretation followed by the non-existent findings. Although future work could work on how to directly generate interpretations without hallucination, a rule-based heuristics can remove the majority of hallucinations in the current version of our system.</p>
<p>O1: Riley went to the store with her mother.
O2: Riley wore her cowboy boots to school the next day.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Mle-LL</th>
<th style="text-align: left;">Riley's mother bought her cowboy boots.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">+ CD ( $\tau_{\mathrm{CD}}=0.5)$</td>
<td style="text-align: left;">Riley had bought cowboy shoes that she had not worn before.</td>
</tr>
<tr>
<td style="text-align: left;">+ CD ( $\tau_{\mathrm{CD}}=1.0)$</td>
<td style="text-align: left;">Her mother bought a new cowboy shirt for Riley.</td>
</tr>
<tr>
<td style="text-align: left;">+ CD ( $\tau_{\mathrm{CD}}=1.5)$</td>
<td style="text-align: left;">Riiley got her new cowboy boots torn.</td>
</tr>
<tr>
<td style="text-align: left;">+ DEXPERTS $(\alpha=0.1)$</td>
<td style="text-align: left;">Riley's mother bought her cowboy boots.</td>
</tr>
<tr>
<td style="text-align: left;">+ DEXPERTS $(\alpha=0.5)$</td>
<td style="text-align: left;">Riley's mother bought her cowboy boots.</td>
</tr>
<tr>
<td style="text-align: left;">+ DEXPERTS $(\alpha=1.0)$</td>
<td style="text-align: left;">Riley didn't have cowboy boots.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>BRAINSTORM</p>
<ul>
<li>CD ( $\tau_{\mathrm{CD}}=0.5)$
$+\mathrm{CD}\left(\tau_{\mathrm{CD}}=1.0\right)$
$+\mathrm{CD}\left(\tau_{\mathrm{CD}}=1.5\right)$</li>
<li>DEXPERTS $(\alpha=0.1)$</li>
<li>DEXPERTS $(\alpha=0.5)$</li>
<li>DEXPERTS $(\alpha=1.0)$
Riley bought a pair of cowboy boots for her mother.
Rice bought cowboy hats with her money and jeans.
Rice's mother bought her a pair of cowboy boots to wear to school next to her cowboy outfit.
Her mom bought her a new pair of cowboy boots.
Riley's mother bought her a pair of cowboy boots. Riley hated them.
Riley bought a pair of cowboy pants.
Riley bought a pair of cowboy pants.
QUARK Riley's mother bought her cowboy boots at the store and Riley hated them.
Table 8: Examples of generating less likely hypotheses across different methods on Art.</li>
</ul>
<p>Premise: The whole kitchen was burnt down more quickly.
Ask for: less likely cause</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Mle-LL</th>
<th style="text-align: left;">The whole kitchen was burnt down more quickly.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">+ CD ( $\tau_{\mathrm{CD}}=0.5)$</td>
<td style="text-align: left;">Tom poured water in the pan of acid.</td>
</tr>
<tr>
<td style="text-align: left;">+ CD ( $\tau_{\mathrm{CD}}=1.0)$</td>
<td style="text-align: left;">There was a big thunderstorm yesterday.</td>
</tr>
<tr>
<td style="text-align: left;">+ CD ( $\tau_{\mathrm{CD}}=1.5)$</td>
<td style="text-align: left;">There is a big storm outside.</td>
</tr>
<tr>
<td style="text-align: left;">+ DEXPERTS $(\alpha=0.1)$</td>
<td style="text-align: left;">The whole kitchen was burnt down more quickly.</td>
</tr>
<tr>
<td style="text-align: left;">+ DEXPERTS $(\alpha=0.5)$</td>
<td style="text-align: left;">The fire in the kitchen spread outwards.</td>
</tr>
<tr>
<td style="text-align: left;">+ DEXPERTS $(\alpha=1.0)$</td>
<td style="text-align: left;">There are more and more fires in this place.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>BRAINSTORM</p>
<ul>
<li>CD ( $\tau_{\mathrm{CD}}=0.5)$
$+\mathrm{CD}\left(\tau_{\mathrm{CD}}=1.0\right)$</li>
<li>DEXPERTS $(\alpha=0.1)$</li>
<li>DEXPERTS $(\alpha=0.5)$</li>
<li>DEXPERTS $(\alpha=1.0)$
Tom put a lot of fuel on the fire.
Tom poured a bucket of water to a sink which has a high temperature.
There was an accident at night.
Tom poured gasoline to the stove.
There is a fire in the kitchen.
The whole kitchen was filled with smoke.
Tom's kitchen is leaking water.</li>
</ul>
<p>QUARK
The fire in the kitchen was very hot.
Table 9: Examples of generating less likely hypotheses across different methods on E-CARE.</p>
<h2>F Indicator Unification for MRIINTERPRET</h2>
<p>We narrowed down the indicators to a smaller set to ensure that our model sees sufficient data for each indicator during training. The indicator mappings are shown in Figure 9 and 10. We also include the way we flip these indicators for the margin loss objective.</p>
<h2>G Example of generated outputs</h2>
<p>We show examples of generated outputs for both everyday commonsense reasoning datasets in Table 8 and 9.</p>
<h2>H Implementation Details</h2>
<h2>H. 1 Significance Test</h2>
<p>We perform a paired bootstrap test for each result by comparing to Mle-LL. We highlight results that are better at 0.05 level of significance.</p>
<h2>H. 2 Computing Infrastructure</h2>
<p>We use BART from HuggingFace Transformers (Wolf et al., 2020), which is implemented in the PyTorch framework.</p>
<h2>H. 3 Training Details</h2>
<p>We fine-tune BART-Large (400M parameters) with 1 NVIDIA RTX A6000 GPU on all experiments and it converges in 2 epochs. We use AdamW as our optimizer with adam epsilon set to 1e-8. Learning rate is set to $5 \mathrm{e}-5$ with linear schedule warmup. There is no warm-up step.</p>
<h2>H.3.1 Everyday Commomsense Reasoning</h2>
<p>We initialize the model from facebook/bartlarge. The batch size is set to 64 if only using MLE objective and 42 otherwise. We set maximum input length to 100 and maximum output length to 64. Most text should fit into these lengths. The average training time for each model is around 0.8 GPU hours if only using MLE objective and 1.5 GPU hours otherwise.</p>
<h2>H.3.2 MRIINTERPRET</h2>
<p>We initialize the model from GanjinZero/biobart-large (Yuan et al., 2022). The batch size is set to 32. We set maximum input length to 256 and maximum output length to 60 . Most text should fit into these lengths. The average training time for each model is around 0.8 GPU hours if only using MLE objective and 1.2 GPU hours otherwise.</p>
<h2>H. 4 Hyperparameter Setups</h2>
<p>Brainstorm For the margin loss $\mathcal{L}<em s="s">{\text {margin }}$ (Equation (2)), we chose $m$ within in the range of $1 \times 10^{-3}$ and $1 \times 10^{-2}$ and set it to 0.005 in the $\log$ space as it works well throughout our experiments. $w</em>$ are set to 1.0 and 10.0 , respectively, as they achieve the best result on the validation set.}$ and $w_{m</p>
<p>Quark We follows the default parameter setups in the original work with 6000 training steps for both commonsense reasoning datasets.</p>
<p>Decoding We use diverse beam search for all experiments with diversity penalty set to 1.0. We set $\tau_{\mathrm{CD}}$ in CD from $2 \times 10^{-1}$ to $1 \times 10^{3}$, and $\alpha$ in DExperts from $1 \times 10^{-3}$ to 1 . We keep the recommended values for the remaining hyperparameters.</p>
<p>In this HIT, you will be presented with an observation pair (Observation 1, Observation 2) on the left. These are two events that we assume have happened. Then, you are presented with multiple hypotheses on the right that could have happened between O1 and O2. Your job is to evaluate the quality of the hypothesis along two axes - Relevancy and Fluency.</p>
<h1>Relevancy</h1>
<p>Classify the hypothesis into one of the following categories:</p>
<ol>
<li>Relevant and likely
II. Relevant but less likely
III. Irrelevant
IV. Contradictory
V. Repetition</li>
</ol>
<h2>Relevant</h2>
<p>A hypothesis is relevant if it fits with the same scenario as the observation pair. It should not introduce new people, places, or things that are not at least plausibly in the same source scenario.</p>
<h2>Likely</h2>
<p>For the hypothesis to be likely, it must also be strongly related to O1 and O2 in a causal fashion - to the extent possible, the first observation O1 should cause the hypothesis and the hypothesis causes the second observation O2. There should not be clearly more likely hypotheses than it.</p>
<h2>Relevant and Less likely</h2>
<p>The hypothesis is still the same scenario as the observation pair (relevant). However, it is less likely to be causally related to the observation pair - maybe it could happen following O1, but not necessarily. There could be other hypotheses that are superior to the given hypothesis.</p>
<h2>Irrelevant</h2>
<p>The hypothesis does not describe the same scenario as the observation pair: it either involves different people, places, or things, or the events it describes have very little connection to O1 and O2.</p>
<h2>Contradictory</h2>
<p>The hypothesis contradicts either observation O1 or observation O2 - it says something that is impossible if we assume O1 and O2 to be true (e.g., O2 states that something happened and the hypothesis states that that thing did not happen).</p>
<h2>Repetition</h2>
<p>The hypothesis is very similar to either O1 or O2 - it either contains a text span that is a repetition of O1 or O2, or it is expressing nearly the same meaning as O1 or O2.</p>
<h2>Fluency</h2>
<p>Classify the hypothesis into one of the following categories:</p>
<ol>
<li>Contains fluency errors
II. Grammatically correct and fluent</li>
</ol>
<p>Note: Please only evaluate the fluency of the hypothesis as a standalone piece of text. That is, evaluate if that one sentence looks okay to you, as opposed to whether or not it makes sense in context.</p>
<h2>Relevancy</h2>
<h2>Example 1</h2>
<p>O1: Bally Jake needed a bath because he had not bathed in two days.
O2: Jake's mom finished by taking him out and drying him with the towel.
Hypothesis 1: Jake's mom gave him a bath and then he fell asleep.
Hypothesis 2: Jake's mom gave him a bath and he loved it.
Hypothesis 3: Jake's mom didn't bathe him and didn't give him a bath.
Hypothesis 4: Jake's mom didn't bathe him.
Hypothesis 5: Jake's mom cooked him a delicious meal.
Hypothesis 6: Jake's dad put him in the bath tub.</p>
<h2>Relevancy</h2>
<ul>
<li>Relevant and likely</li>
</ul>
<p>Hypothesis 1, 2. These all involve Jake and his mom (relevant) and make sense in the scenario.</p>
<ul>
<li>Relevant but less likely</li>
</ul>
<p>Hypothesis 6. This hypothesis could happen. However, "Jake's mom put him in the bath tub" would be a more likely option given the observation O2.</p>
<ul>
<li>Contradictory</li>
</ul>
<p>Hypothesis 3, 4. These hypotheses are contradictory to the Observation O2. Observation O2 implies that Jake took a bath, but Hypothesis 3, 4 directly say that Jake did not take a bath, which is a contradiction.</p>
<ul>
<li>Irrelevant</li>
</ul>
<p>Hypothesis 5. There is no connection between the observation pair and the hypothesis, although they involve the same people.</p>
<h2>Fluency</h2>
<ul>
<li>Contains fluency errors</li>
</ul>
<p>Hypothesis 3. The phrase "didn't bathe him" and "didn't give him a bath" both refer to the same action (not bathing Jake), so using both phrases in the same sentence is redundant and can be confusing.</p>
<ul>
<li>Grammatically correct and fluent</li>
</ul>
<p>Hypothesis 1, 2, 4, 5, 6. These hypotheses alone are grammatically correct and convey a clear and logical message.</p>
<h2>Example 2</h2>
<p>O1: Janice looked at her bags of trash with pride.
O2: Instead, she took a friend and they both got small yogurts together.
Hypothesis 1: Janice was going to buy herself yogurt.
Hypothesis 2: Janice decided to throw away all the trash in the trash.
Hypothesis 3: Janice wanted to go to the store and buy a large bag of trash.
Hypothesis 4: Janice looked at her bags of trash with pride.
Hypothesis 5: Janice was proud of trash that she had collected.</p>
<h2>Relevancy</h2>
<ul>
<li>Relevant and likely</li>
</ul>
<p>Hypothesis 1, 2.</p>
<ul>
<li>Relevant and less likely</li>
</ul>
<p>Hypothesis 3. It is unusual to buy a bag of trash from a store.</p>
<ul>
<li>Repetition</li>
</ul>
<p>Hypothesis 4, 5. Hypothesis 4 is a repetition of Observation O1. Hypothesis 5 is expressing the same meaning as Observation O1.</p>
<h2>Fluency</h2>
<ul>
<li>Contains fluency errors</li>
</ul>
<p>Hypothesis 2. "All the trash in the trash" could be interpreted as redundant because "trash" is already included in "all the trash." It might sound clearer to say "all the trash" or "all the trash in the bin." Both of these phrases would eliminate the potential for redundancy and make the meaning of the sentence more clear.</p>
<ul>
<li>Grammatically correct and fluent</li>
</ul>
<p>Hypothesis 1, 3, 4, 5. These hypotheses alone are grammatically correct and convey a clear and logical message.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Annotation Interface (II) for ART.</p>
<p>In this HIT, you will be presented with a premise statement introducing a scenario, followed by multiple hypotheses statements. These hypotheses statements are supposed to be either causes or effects of the premise. Your job is to evaluate the quality of each of the hypothesis statements along two axes - Relevancy and Fluency.</p>
<p>Note: You may search for information to verify certain hypotheses.</p>
<h1>Relevancy</h1>
<p>Classify the hypothesis into one of the following categories:</p>
<ol>
<li>Relevant and likely
II. Relevant but less likely
III. Irrelevant
IV. Contradictory
V. Repetition</li>
</ol>
<h2>Relevant</h2>
<p>A hypothesis is relevant if it fits with the same scenario as the premise. It should not introduce new people, places, or things that are not at least plausibly in the same source scenario.</p>
<h2>Likely</h2>
<p>For the hypothesis to be likely, it must also be causally related to the premise - either the premise causes the hypothesis or the hypothesis causes the premise (you will see both versions of the task below). There should not be clearly more likely hypotheses than it.</p>
<h2>Relevant and Less likely</h2>
<p>The hypothesis is still the same scenario as the premise (relevant). However, it is less likely to be causally related to the premise. There could be other hypotheses that are superior to the given hypothesis.</p>
<h2>Irrelevant</h2>
<p>The generated hypothesis does not describe the same scenario as the premise or is not causally related to the premise.</p>
<h2>Contradictory</h2>
<p>The hypothesis contradicts the premise - it says something that is impossible if we assume the premise to be true (e.g., the premise states that something happened and the hypothesis states that that thing did not happen).</p>
<h2>Repetition</h2>
<p>The hypothesis is very similar to the premise - it either contains a text span that is a repetition of the premise, or it is expressing nearly the same meaning as the premise.</p>
<h2>Fluency</h2>
<p>Classify the hypothesis into one of the following categories:</p>
<ol>
<li>Contains fluency errors
II. Grammatically correct and fluent</li>
</ol>
<p>Note: Please only evaluate the fluency of the hypothesis as a standalone piece of text. That is, evaluate if that one sentence looks okay to you, as opposed to whether or not it makes sense in context.</p>
<h2>Example 1</h2>
<p>Premise: My mom keeps cleaning my room.
What would be the possible effect of the premise?</p>
<p>Hypothesis 1: My mom cleans my room every day.
Hypothesis 2: The dust in my room is getting worse.
Hypothesis 3: My mom never cleans my room.
Hypothesis 4: It's time for her to leave for work.
Hypothesis 5: My mom keeps cleaning my room.
Hypothesis 6: My mom is constantly cleaning my room.
Hypothesis 7: My room keeps clean.</p>
<h2>Relevancy</h2>
<ul>
<li>Relevant and likely</li>
</ul>
<p>Hypothesis 7. This scenario makes sense.</p>
<ul>
<li>Relevant and less likely.</li>
</ul>
<p>Hypothesis 2. It is less likely that there are more dust in the room if the room keeps being cleaned.</p>
<ul>
<li>Contradictory</li>
</ul>
<p>Hypothesis 3. The premise and the hypothesis 3 cannot both be true at the same time, so they contradict each other.</p>
<ul>
<li>Irrelevant</li>
</ul>
<p>Hypothesis 4. Hypothesis 4 is not related to the premise, although they may involve the same person.</p>
<ul>
<li>Repetition</li>
</ul>
<p>Hypothesis 1, 5, 6. Hypothesis 5 repeats the premise. Hypothesis 1, 6 are phrraphrases of the premise. They express the same meaning as the premise.</p>
<h2>Fluency</h2>
<ul>
<li>Grammatically correct and fluent</li>
</ul>
<p>Hypothesis 1, 2, 3, 4, 5, 6, 7. These hypotheses alone are grammatically correct and convey a clear and logical message.</p>
<h2>Example 2</h2>
<p>Premise: We can see many stripes on their backs.
What would be the possible cause of the premise?</p>
<p>Hypothesis 1: There are many zebras in the zoo.
Hypothesis 2: Kudus are African animals.
Hypothesis 3: We can see many stripes on their backs.
Hypothesis 4: There are many Perennities in the zoo.
Hypothesis 5: There are many zebras in the zoo the zoo.</p>
<h2>Relevancy</h2>
<ul>
<li>Relevant and likely</li>
</ul>
<p>Hypothesis 1, 2, 5. For hypothesis 2, Kudus has stripes on their backs. This can be verified from online search.</p>
<ul>
<li>Relevant and less likely.</li>
</ul>
<p>Hypothesis 4. Perennities does not commonly has stripes on their backs (from online search).</p>
<ul>
<li>Repetition</li>
</ul>
<p>Hypothesis 3.</p>
<h2>Fluency</h2>
<ul>
<li>Contains fluency errors</li>
</ul>
<p>Hypothesis 5. It is repetitive and does not make grammatical sense. A more fluent version of this sentence would be "There are many zebras in the zoo."</p>
<ul>
<li>Grammatically correct and fluent</li>
</ul>
<p>Hypothesis 1, 2, 3, 4. These hypotheses alone are grammatically correct and convey a clear and logical message.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: Annotation Interface (II) for E-CARE.</p>
<h1>Mappings of likely indicators</h1>
<h2>likely suggestive of:</h2>
<p>(with suggestion of, a reflection of, likely representing, likely reflective of, likely relating, suggesting, in favor of, most likely consistent with, likely consistent with, perhaps related to, possibly related to, most likely related to, raising the possibility of, most likely reflecting, likely relating to, potentially related to, likely the result of, likely reflecting, concerning for, favor of, favored to represent, most likely representing, in keeping with, to be related to, to represent, probably representing, likely due to, probably related to, likely related to, compatible with, more likely to be related to, most likely, possibly representing, most consistent with, suggestive of, potentially reflecting, consistent with, most likely to be related to, representing, potentially representing)</p>
<h2>could represent:</h2>
<p>(most likely to represent, most likely represent, likely represents, suggests the possibility of, is favored to represent, potentially reflect, could be an indication of, are diagnostic of, may also reflect, could indicate, likely reflects, may be seen with, potentially represent, may be seen in, can represent, likely represent, could possibly be related to, may represent, likely suggest, most likely represents, likely indicate, suggest the possibility of, may be due to, likely reflect, represents, may be a reflection of, could be related to, could reflect, most likely diagnosis is, could potentially be related to, raises possibility of, probably represent, can be seen in the setting of, most likely reflect, raise the possibility of, may reflect, can be seen in, may well represent, would have to represent, may also represent, probably also represent, may be in part related to, could be due to, may indicate, could be consistent with, could represent, likely indicates, could be a reflection of, likely suggests, could also represent, may be related to)</p>
<h2>findings could represent:</h2>
<p>(considerations would include, differential diagnosis would include, differential considerations include, differential includes, differential would include, diagnostic possibilities include, differential diagnosis also includes)</p>
<p>Figure 9: Unifying "likely" indicators in MRIINTERPRET.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Code is available at https://github.com/ Liyan06/Brainstorm.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
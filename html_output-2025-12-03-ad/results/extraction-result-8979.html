<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8979 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8979</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8979</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-c3a3c163f25b9181f1fb7e71a32482a7393d2088</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c3a3c163f25b9181f1fb7e71a32482a7393d2088" target="_blank">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs, is proposed, observing that GCN layers are complementary to LSTM ones.</p>
                <p><strong>Paper Abstract:</strong> Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8979.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8979.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Syntactic GCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Syntactic Graph Convolutional Network (directed, labeled)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalization of graph convolutional networks tailored to syntactic dependency graphs: edges are labeled with syntactic functions and directionality (along / opposite / self-loop), with label-specific bias terms and direction-specific weight matrices, plus edge-wise gates to weight neighbor contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph convolutional encoding (syntactic GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent each word (graph node) by a learned vector produced by GCN layers that aggregate messages from graph neighbors. Parameters are specialized by edge direction (head->dependent, dependent->head, self-loop) while syntactic function labels are encoded in bias terms; each edge has a learned scalar gate to modulate its contribution. Stacking K layers encodes up to K-hop neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>directed labeled dependency trees / directed labeled graphs (syntactic dependency graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No explicit conversion to text/sequence; the graph is processed directly via message passing: for each node, sum gated linear transforms of neighbor hidden states using direction-specific weight matrices and label-specific biases, followed by ReLU; stacked layers increase receptive field.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Dependency-based semantic role labeling (SRL); (authors also propose applicability to machine translation, question answering, and other NLP tasks though not evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>English dev (SRL-only): LSTMs baseline F1=82.7; LSTMs+GCN (K=1) F1=83.3 (P=85.2, R=81.6). Chinese dev: LSTMs baseline F1=75.2; LSTMs+GCN (K=1) F1=77.1 (P=79.9, R=74.4). English test (combined with predicate disambiguation): Ours (local) F1=88.0 (P=89.1, R=86.8); ensemble 3x F1=89.1 (P=90.5, R=87.7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms syntax-agnostic BiLSTM baseline (Marcheggiani et al., 2017) by +0.6% F1 (English dev) and +1.9% (Chinese dev). Outperforms prior published local and many global SRL models on English test (e.g., Roth & Lapata 2016 local F1=86.7; FitzGerald et al. 2015 local F1=86.7). When LSTMs are removed, multiple GCN layers can recover much of the performance, but when LSTMs are present, stacking >1 GCN layer gives no further gain.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Effectively incorporates syntactic structure at the word level; complementary to BiLSTM sequential encoders (yields best results when stacked on BiLSTMs); edge-wise gating makes the model robust to noisy automatic parses; computationally cheaper per layer than LSTMs; applicable to general directed labeled graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>GCN layers capture only up to K-hop neighborhoods (limited receptive field) unless stacked; potential over-parameterization if labels are fully parameterized (authors mitigate by sharing direction matrices and encoding function in biases); stacked GCN layers become redundant when LSTMs already capture long-range context.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performance depends on quality of syntactic parses — noisy parses (out-of-domain) reduce gains (model still competitive but does not beat syntax-agnostic model in some out-of-domain settings). When used without BiLSTMs, single-layer GCNs underperform significantly on long-distance dependencies unless multiple GCN layers are stacked. Removing edge-wise gates degrades performance (reported drops: English -0.3% F1, Chinese -0.6% F1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8979.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8979.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiLSTM+GCN hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional LSTM encoder stacked with Syntactic Graph Convolutional Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid encoder where BiLSTM produces contextual token representations which are then re-encoded by syntactic GCN layers to inject syntactic neighborhood information into word representations used for SRL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>BiLSTM + GCN sentence encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>First compute token-level contextual vectors with stacked bidirectional LSTMs; feed these vectors as initial node features into syntactic GCN layers that perform gated message passing over the dependency graph, producing final syntactically-aware token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>directed labeled dependency trees (syntactic dependency graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No conversion to text; sequential encoder provides contextualized vectors, and GCN consumes these as node features to integrate graph structure via message passing.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Dependency-based semantic role labeling (SRL) — argument identification and labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>English dev: BiLSTM-only F1=82.7; BiLSTM+GCN (K=1) F1=83.3. Chinese dev: BiLSTM-only F1=75.2; BiLSTM+GCN (K=1) F1=77.1. English test combined scores: Ours (local) F1=88.0; ensemble 89.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Hybrid outperforms BiLSTM-only (syntax-agnostic) baselines. Authors argue complementarity: BiLSTM captures long sequential context and GCN 'teleports' along syntactic edges to reduce effective LSTM distance for long predicate-argument pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Combines strengths of sequential modeling (LSTM) and structured graph encoding (GCN), leading to improved SRL especially on long-distance predicate-argument dependencies; empirically yields state-of-the-art local models on CoNLL-2009.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds architectural complexity and reliance on syntactic parses; when parses are poor, gains may shrink. Additional GCN layers beyond 1 provide little benefit when BiLSTM is present.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Out-of-domain syntactic parser errors can reduce effectiveness; single GCN layer is most effective — stacking more layers provides diminishing or no returns when BiLSTM exists.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8979.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8979.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kipf & Welling GCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Networks (as in Kipf and Welling 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A node representation learning method for undirected graphs that computes node embeddings by applying layer-wise propagation rules based on neighbor feature aggregation and tied weight matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semisupervised classification with graph convolutional networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph convolutional networks (original formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute node embeddings by summing linear transformations of neighbor feature vectors (including self-loop), followed by non-linearity; stacking layers expands receptive field to K-hop neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>undirected unlabeled graphs (original); here generalized to directed labeled graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No conversion to text; operates directly on graph adjacency via matrix operations/message passing.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Originally node classification (semi-supervised); referenced here as the conceptual basis for syntactic encodings for SRL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated on SRL in this paper; Kipf & Welling originally reported strong semi-supervised node classification performance (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors adapt Kipf & Welling GCN to labeled directed syntactic graphs (introducing direction-specific matrices, label biases, and edge-wise gating), arguing original normalization factors were dropped and labels/directions handled differently.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, effective, and computationally efficient per layer; strong empirical performance on graph-based node classification tasks in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Original formulation expects undirected unlabeled graphs and uses normalization heuristics; naively applying it to directed labeled dependency trees would ignore direction/label info.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Needs adaptation to handle directed labeled graphs and noisy edges for NLP tasks; original normalization factors were dropped in syntactic adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8979.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8979.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dependency Path Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependency path embeddings (LSTM representations of syntactic paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that serializes dependency paths between predicate and argument into sequences and encodes them with LSTMs to create features for SRL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural semantic role labeling with dependency path embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>dependency path serialization + LSTM embedding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent the syntactic path between two nodes (predicate and candidate argument) as a sequence of items (e.g., node labels and/or dependency relations and directions), and encode this sequence with an LSTM to produce a fixed-length path embedding used as features for SRL.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>dependency trees / paths within dependency graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize the dependency path between two nodes into a linear sequence (sequence of dependency relations and/or tokens/directions) and process it with an LSTM to obtain an embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Dependency-based semantic role labeling (SRL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in paper as prior work: Roth & Lapata (2016) local F1 on English test = 86.7 (P=88.1, R=85.3) according to Table 3 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The current paper contrasts path-embedding approaches with their GCN approach: GCNs encode neighborhood information directly at node level and are complementary to LSTM sequence encoders; authors report outperforming Roth & Lapata (2016) on the CoNLL-2009 English test.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures explicit structural information along predicate-argument paths; LSTM path encoders can model ordered information on the path.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires explicit path extraction and serialization per predicate-argument pair (potentially expensive); focuses on pairwise paths rather than producing a unified node-level embedding for all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May not scale well if many predicate-argument pairs or paths are long; authors found methods that integrate syntax at word-level (GCNs) can be more effective and more directly complementary to BiLSTMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8979.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8979.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-based Convolution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discriminative neural sentence modeling by tree-based convolution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolutional approach over tree structures that applies convolutional filters to tree fragments to produce sentence representations; related to graph convolution but single-layer and tree-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discriminative neural sentence modeling by tree-based convolution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>tree-based convolution</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply convolutional operations over fixed tree-substructures (e.g., parent-child windows) in a syntactic tree to obtain features; single-layer bottom-up computations produce sentence/node-level representations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>syntactic trees (dependency or constituency trees)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No conversion to text; operate directly on tree structure applying convolutional filters over tree neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Sentence classification tasks such as sentiment analysis and question classification (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated here; referenced as related prior work (Mou et al., 2015).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors note Mou et al.'s approach is related to GCNs but is single-layer, bottom-up, tree-specific, does not share parameters across syntactic functions and lacks edge-wise gates; GCN generalizes to arbitrary graphs and supports gating.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Structure-aware convolution tailored to trees; effective for certain sentence-level classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Single-layer and tree-specific (not naturally generalizable to arbitrary labeled/directed graphs); lacks the gate mechanism and parameter sharing strategies used in syntactic GCN.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Less flexible than GCNs for labeled/directed graphs and multi-layer stacking; may not model arbitrary graph structures or require additional engineering to handle labels/directions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8979.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8979.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-Structured Recursive NN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-structured recursive neural networks (recursive nets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural architectures that build representations by recursively composing child node vectors following tree structure, producing structured embeddings for nodes and entire trees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recursive deep models for semantic compositionality over a sentiment treebank</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>tree-structured recursive encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute node and phrase embeddings by recursively applying composition functions bottom-up (or other orders) over tree nodes; parameters may depend on node types or positions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>syntactic trees / constituency or dependency trees</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No conversion to text; directly traverse the tree (usually bottom-up) composing child representations to obtain parent/node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Sentence-level tasks such as sentiment analysis, compositional semantics; previously used in parsing and other NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated in this paper; cited as prior work and contrasted with GCNs which operate at the word/node level and generalize beyond trees.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors contrast recursive nets with GCNs: recursive nets are tree-specific and typically bottom-up, whereas GCNs allow arbitrary graph structures, multi-directional message flow, parameter sharing by edge type, and edge-wise gating.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Natural fit for compositional tree structures and phrase-level representations; proven useful for sentiment and compositional semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Tree-specific, often bottom-up (less flexible), and may not share parameters across syntactic functions; harder to apply to arbitrary labeled/directed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Less directly applicable to non-tree graphs or to tasks that require integrating multiple graph types; may not capture multi-hop interactions as efficiently as stacked GCNs when used in combination with sequential encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8979.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8979.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gated Graph Sequence NN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gated Graph Sequence Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph neural network variant that uses gated recurrent units to propagate information across graph nodes, typically applied to graph-structured sequence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gated graph sequence neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>gated graph sequence encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use GRU-like gating to iteratively propagate and update node hidden states across the graph for multiple time steps, producing node or sequence-level outputs; gates operate between layers/time-steps.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs (molecular graphs, program graphs, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No conversion to text; iterative message passing with gated updates across graph nodes; can be used to produce sequence outputs via readout functions.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Various graph tasks (e.g., program reasoning, chemistry); cited here as related work using gates in graph neural architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated in this paper; referenced for methodological similarity (use of gates) but differs in gating granularity (their gates are between layers/time steps whereas syntactic GCN uses edge-wise gates).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors cite Li et al. (2016) as previous use of gates in graph NNs but note gating in that work operates between GCN layers/time-steps rather than per-edge as in their syntactic GCN.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Gating stabilizes propagation and can model long-range dependencies via iterative updates.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Gates applied at layer/time-step granularity may be less fine-grained than edge-wise gating for noisy input graphs; possibly more computationally expensive due to recurrent update steps.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not discussed in this paper beyond methodological contrast; authors argue edge-wise gating better suits noisy syntactic parses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Semisupervised classification with graph convolutional networks <em>(Rating: 2)</em></li>
                <li>Neural semantic role labeling with dependency path embeddings <em>(Rating: 2)</em></li>
                <li>Discriminative neural sentence modeling by tree-based convolution <em>(Rating: 2)</em></li>
                <li>Gated graph sequence neural networks <em>(Rating: 2)</em></li>
                <li>Recursive deep models for semantic compositionality over a sentiment treebank <em>(Rating: 1)</em></li>
                <li>Greedy, joint syntactic-semantic parsing with stack LSTMs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8979",
    "paper_id": "paper-c3a3c163f25b9181f1fb7e71a32482a7393d2088",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Syntactic GCN",
            "name_full": "Syntactic Graph Convolutional Network (directed, labeled)",
            "brief_description": "A generalization of graph convolutional networks tailored to syntactic dependency graphs: edges are labeled with syntactic functions and directionality (along / opposite / self-loop), with label-specific bias terms and direction-specific weight matrices, plus edge-wise gates to weight neighbor contributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "graph convolutional encoding (syntactic GCN)",
            "representation_description": "Represent each word (graph node) by a learned vector produced by GCN layers that aggregate messages from graph neighbors. Parameters are specialized by edge direction (head-&gt;dependent, dependent-&gt;head, self-loop) while syntactic function labels are encoded in bias terms; each edge has a learned scalar gate to modulate its contribution. Stacking K layers encodes up to K-hop neighborhoods.",
            "graph_type": "directed labeled dependency trees / directed labeled graphs (syntactic dependency graphs)",
            "conversion_method": "No explicit conversion to text/sequence; the graph is processed directly via message passing: for each node, sum gated linear transforms of neighbor hidden states using direction-specific weight matrices and label-specific biases, followed by ReLU; stacked layers increase receptive field.",
            "downstream_task": "Dependency-based semantic role labeling (SRL); (authors also propose applicability to machine translation, question answering, and other NLP tasks though not evaluated here).",
            "performance_metrics": "English dev (SRL-only): LSTMs baseline F1=82.7; LSTMs+GCN (K=1) F1=83.3 (P=85.2, R=81.6). Chinese dev: LSTMs baseline F1=75.2; LSTMs+GCN (K=1) F1=77.1 (P=79.9, R=74.4). English test (combined with predicate disambiguation): Ours (local) F1=88.0 (P=89.1, R=86.8); ensemble 3x F1=89.1 (P=90.5, R=87.7).",
            "comparison_to_others": "Outperforms syntax-agnostic BiLSTM baseline (Marcheggiani et al., 2017) by +0.6% F1 (English dev) and +1.9% (Chinese dev). Outperforms prior published local and many global SRL models on English test (e.g., Roth & Lapata 2016 local F1=86.7; FitzGerald et al. 2015 local F1=86.7). When LSTMs are removed, multiple GCN layers can recover much of the performance, but when LSTMs are present, stacking &gt;1 GCN layer gives no further gain.",
            "advantages": "Effectively incorporates syntactic structure at the word level; complementary to BiLSTM sequential encoders (yields best results when stacked on BiLSTMs); edge-wise gating makes the model robust to noisy automatic parses; computationally cheaper per layer than LSTMs; applicable to general directed labeled graphs.",
            "disadvantages": "GCN layers capture only up to K-hop neighborhoods (limited receptive field) unless stacked; potential over-parameterization if labels are fully parameterized (authors mitigate by sharing direction matrices and encoding function in biases); stacked GCN layers become redundant when LSTMs already capture long-range context.",
            "failure_cases": "Performance depends on quality of syntactic parses — noisy parses (out-of-domain) reduce gains (model still competitive but does not beat syntax-agnostic model in some out-of-domain settings). When used without BiLSTMs, single-layer GCNs underperform significantly on long-distance dependencies unless multiple GCN layers are stacked. Removing edge-wise gates degrades performance (reported drops: English -0.3% F1, Chinese -0.6% F1).",
            "uuid": "e8979.0",
            "source_info": {
                "paper_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "BiLSTM+GCN hybrid",
            "name_full": "Bidirectional LSTM encoder stacked with Syntactic Graph Convolutional Network",
            "brief_description": "A hybrid encoder where BiLSTM produces contextual token representations which are then re-encoded by syntactic GCN layers to inject syntactic neighborhood information into word representations used for SRL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "BiLSTM + GCN sentence encoding",
            "representation_description": "First compute token-level contextual vectors with stacked bidirectional LSTMs; feed these vectors as initial node features into syntactic GCN layers that perform gated message passing over the dependency graph, producing final syntactically-aware token embeddings.",
            "graph_type": "directed labeled dependency trees (syntactic dependency graphs)",
            "conversion_method": "No conversion to text; sequential encoder provides contextualized vectors, and GCN consumes these as node features to integrate graph structure via message passing.",
            "downstream_task": "Dependency-based semantic role labeling (SRL) — argument identification and labeling.",
            "performance_metrics": "English dev: BiLSTM-only F1=82.7; BiLSTM+GCN (K=1) F1=83.3. Chinese dev: BiLSTM-only F1=75.2; BiLSTM+GCN (K=1) F1=77.1. English test combined scores: Ours (local) F1=88.0; ensemble 89.1.",
            "comparison_to_others": "Hybrid outperforms BiLSTM-only (syntax-agnostic) baselines. Authors argue complementarity: BiLSTM captures long sequential context and GCN 'teleports' along syntactic edges to reduce effective LSTM distance for long predicate-argument pairs.",
            "advantages": "Combines strengths of sequential modeling (LSTM) and structured graph encoding (GCN), leading to improved SRL especially on long-distance predicate-argument dependencies; empirically yields state-of-the-art local models on CoNLL-2009.",
            "disadvantages": "Adds architectural complexity and reliance on syntactic parses; when parses are poor, gains may shrink. Additional GCN layers beyond 1 provide little benefit when BiLSTM is present.",
            "failure_cases": "Out-of-domain syntactic parser errors can reduce effectiveness; single GCN layer is most effective — stacking more layers provides diminishing or no returns when BiLSTM exists.",
            "uuid": "e8979.1",
            "source_info": {
                "paper_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Kipf & Welling GCN",
            "name_full": "Graph Convolutional Networks (as in Kipf and Welling 2017)",
            "brief_description": "A node representation learning method for undirected graphs that computes node embeddings by applying layer-wise propagation rules based on neighbor feature aggregation and tied weight matrices.",
            "citation_title": "Semisupervised classification with graph convolutional networks",
            "mention_or_use": "mention",
            "representation_name": "graph convolutional networks (original formulation)",
            "representation_description": "Compute node embeddings by summing linear transformations of neighbor feature vectors (including self-loop), followed by non-linearity; stacking layers expands receptive field to K-hop neighborhoods.",
            "graph_type": "undirected unlabeled graphs (original); here generalized to directed labeled graphs",
            "conversion_method": "No conversion to text; operates directly on graph adjacency via matrix operations/message passing.",
            "downstream_task": "Originally node classification (semi-supervised); referenced here as the conceptual basis for syntactic encodings for SRL.",
            "performance_metrics": "Not evaluated on SRL in this paper; Kipf & Welling originally reported strong semi-supervised node classification performance (not reproduced here).",
            "comparison_to_others": "Authors adapt Kipf & Welling GCN to labeled directed syntactic graphs (introducing direction-specific matrices, label biases, and edge-wise gating), arguing original normalization factors were dropped and labels/directions handled differently.",
            "advantages": "Simple, effective, and computationally efficient per layer; strong empirical performance on graph-based node classification tasks in prior work.",
            "disadvantages": "Original formulation expects undirected unlabeled graphs and uses normalization heuristics; naively applying it to directed labeled dependency trees would ignore direction/label info.",
            "failure_cases": "Needs adaptation to handle directed labeled graphs and noisy edges for NLP tasks; original normalization factors were dropped in syntactic adaptation.",
            "uuid": "e8979.2",
            "source_info": {
                "paper_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Dependency Path Embeddings",
            "name_full": "Dependency path embeddings (LSTM representations of syntactic paths)",
            "brief_description": "A method that serializes dependency paths between predicate and argument into sequences and encodes them with LSTMs to create features for SRL.",
            "citation_title": "Neural semantic role labeling with dependency path embeddings",
            "mention_or_use": "mention",
            "representation_name": "dependency path serialization + LSTM embedding",
            "representation_description": "Represent the syntactic path between two nodes (predicate and candidate argument) as a sequence of items (e.g., node labels and/or dependency relations and directions), and encode this sequence with an LSTM to produce a fixed-length path embedding used as features for SRL.",
            "graph_type": "dependency trees / paths within dependency graphs",
            "conversion_method": "Serialize the dependency path between two nodes into a linear sequence (sequence of dependency relations and/or tokens/directions) and process it with an LSTM to obtain an embedding.",
            "downstream_task": "Dependency-based semantic role labeling (SRL).",
            "performance_metrics": "Reported in paper as prior work: Roth & Lapata (2016) local F1 on English test = 86.7 (P=88.1, R=85.3) according to Table 3 in this paper.",
            "comparison_to_others": "The current paper contrasts path-embedding approaches with their GCN approach: GCNs encode neighborhood information directly at node level and are complementary to LSTM sequence encoders; authors report outperforming Roth & Lapata (2016) on the CoNLL-2009 English test.",
            "advantages": "Captures explicit structural information along predicate-argument paths; LSTM path encoders can model ordered information on the path.",
            "disadvantages": "Requires explicit path extraction and serialization per predicate-argument pair (potentially expensive); focuses on pairwise paths rather than producing a unified node-level embedding for all tasks.",
            "failure_cases": "May not scale well if many predicate-argument pairs or paths are long; authors found methods that integrate syntax at word-level (GCNs) can be more effective and more directly complementary to BiLSTMs.",
            "uuid": "e8979.3",
            "source_info": {
                "paper_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Tree-based Convolution",
            "name_full": "Discriminative neural sentence modeling by tree-based convolution",
            "brief_description": "A convolutional approach over tree structures that applies convolutional filters to tree fragments to produce sentence representations; related to graph convolution but single-layer and tree-specific.",
            "citation_title": "Discriminative neural sentence modeling by tree-based convolution",
            "mention_or_use": "mention",
            "representation_name": "tree-based convolution",
            "representation_description": "Apply convolutional operations over fixed tree-substructures (e.g., parent-child windows) in a syntactic tree to obtain features; single-layer bottom-up computations produce sentence/node-level representations.",
            "graph_type": "syntactic trees (dependency or constituency trees)",
            "conversion_method": "No conversion to text; operate directly on tree structure applying convolutional filters over tree neighborhoods.",
            "downstream_task": "Sentence classification tasks such as sentiment analysis and question classification (as cited).",
            "performance_metrics": "Not evaluated here; referenced as related prior work (Mou et al., 2015).",
            "comparison_to_others": "Authors note Mou et al.'s approach is related to GCNs but is single-layer, bottom-up, tree-specific, does not share parameters across syntactic functions and lacks edge-wise gates; GCN generalizes to arbitrary graphs and supports gating.",
            "advantages": "Structure-aware convolution tailored to trees; effective for certain sentence-level classification tasks.",
            "disadvantages": "Single-layer and tree-specific (not naturally generalizable to arbitrary labeled/directed graphs); lacks the gate mechanism and parameter sharing strategies used in syntactic GCN.",
            "failure_cases": "Less flexible than GCNs for labeled/directed graphs and multi-layer stacking; may not model arbitrary graph structures or require additional engineering to handle labels/directions.",
            "uuid": "e8979.4",
            "source_info": {
                "paper_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Tree-Structured Recursive NN",
            "name_full": "Tree-structured recursive neural networks (recursive nets)",
            "brief_description": "Neural architectures that build representations by recursively composing child node vectors following tree structure, producing structured embeddings for nodes and entire trees.",
            "citation_title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "mention_or_use": "mention",
            "representation_name": "tree-structured recursive encoding",
            "representation_description": "Compute node and phrase embeddings by recursively applying composition functions bottom-up (or other orders) over tree nodes; parameters may depend on node types or positions.",
            "graph_type": "syntactic trees / constituency or dependency trees",
            "conversion_method": "No conversion to text; directly traverse the tree (usually bottom-up) composing child representations to obtain parent/node embeddings.",
            "downstream_task": "Sentence-level tasks such as sentiment analysis, compositional semantics; previously used in parsing and other NLP tasks.",
            "performance_metrics": "Not evaluated in this paper; cited as prior work and contrasted with GCNs which operate at the word/node level and generalize beyond trees.",
            "comparison_to_others": "Authors contrast recursive nets with GCNs: recursive nets are tree-specific and typically bottom-up, whereas GCNs allow arbitrary graph structures, multi-directional message flow, parameter sharing by edge type, and edge-wise gating.",
            "advantages": "Natural fit for compositional tree structures and phrase-level representations; proven useful for sentiment and compositional semantics.",
            "disadvantages": "Tree-specific, often bottom-up (less flexible), and may not share parameters across syntactic functions; harder to apply to arbitrary labeled/directed graphs.",
            "failure_cases": "Less directly applicable to non-tree graphs or to tasks that require integrating multiple graph types; may not capture multi-hop interactions as efficiently as stacked GCNs when used in combination with sequential encoders.",
            "uuid": "e8979.5",
            "source_info": {
                "paper_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Gated Graph Sequence NN",
            "name_full": "Gated Graph Sequence Neural Networks",
            "brief_description": "A graph neural network variant that uses gated recurrent units to propagate information across graph nodes, typically applied to graph-structured sequence outputs.",
            "citation_title": "Gated graph sequence neural networks",
            "mention_or_use": "mention",
            "representation_name": "gated graph sequence encoding",
            "representation_description": "Use GRU-like gating to iteratively propagate and update node hidden states across the graph for multiple time steps, producing node or sequence-level outputs; gates operate between layers/time-steps.",
            "graph_type": "general graphs (molecular graphs, program graphs, etc.)",
            "conversion_method": "No conversion to text; iterative message passing with gated updates across graph nodes; can be used to produce sequence outputs via readout functions.",
            "downstream_task": "Various graph tasks (e.g., program reasoning, chemistry); cited here as related work using gates in graph neural architectures.",
            "performance_metrics": "Not evaluated in this paper; referenced for methodological similarity (use of gates) but differs in gating granularity (their gates are between layers/time steps whereas syntactic GCN uses edge-wise gates).",
            "comparison_to_others": "Authors cite Li et al. (2016) as previous use of gates in graph NNs but note gating in that work operates between GCN layers/time-steps rather than per-edge as in their syntactic GCN.",
            "advantages": "Gating stabilizes propagation and can model long-range dependencies via iterative updates.",
            "disadvantages": "Gates applied at layer/time-step granularity may be less fine-grained than edge-wise gating for noisy input graphs; possibly more computationally expensive due to recurrent update steps.",
            "failure_cases": "Not discussed in this paper beyond methodological contrast; authors argue edge-wise gating better suits noisy syntactic parses.",
            "uuid": "e8979.6",
            "source_info": {
                "paper_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Semisupervised classification with graph convolutional networks",
            "rating": 2
        },
        {
            "paper_title": "Neural semantic role labeling with dependency path embeddings",
            "rating": 2
        },
        {
            "paper_title": "Discriminative neural sentence modeling by tree-based convolution",
            "rating": 2
        },
        {
            "paper_title": "Gated graph sequence neural networks",
            "rating": 2
        },
        {
            "paper_title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "rating": 1
        },
        {
            "paper_title": "Greedy, joint syntactic-semantic parsing with stack LSTMs",
            "rating": 1
        }
    ],
    "cost": 0.01589825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</h1>
<p>Diego Marcheggiani ${ }^{1}$ Ivan Titov ${ }^{1,2}$<br>${ }^{1}$ ILLC, University of Amsterdam<br>${ }^{2}$ ILCC, School of Informatics, University of Edinburgh<br>marcheggiani@uva.nl ititov@inf.ed.ac.uk</p>
<h4>Abstract</h4>
<p>Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-theart LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.</p>
<h2>1 Introduction</h2>
<p>Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) can be informally described as the task of discovering who did what to whom. For example, consider an SRL dependency graph shown above the sentence in Figure 1. Formally, the task includes (1) detection of predicates (e.g., makes); (2) labeling the predicates with a sense from a sense inventory (e.g., make.01); (3) identifying and assigning arguments to semantic roles (e.g., Sequa is A0, i.e., an agent / 'doer' for the corresponding predicate, and engines is A1, i.e., a patient / 'an affected entity'). SRL is often regarded
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example sentence annotated with semantic (top) and syntactic dependencies (bottom).
as an important step in the standard NLP pipeline, providing information to downstream tasks such as information extraction and question answering.</p>
<p>The semantic representations are closely related to syntactic ones, even though the syntaxsemantics interface is far from trivial (Levin, 1993). For example, one can observe that many arcs in the syntactic dependency graph (shown in black below the sentence in Figure 1) are mirrored in the semantic dependency graph. Given these similarities and also because of availability of accurate syntactic parsers for many languages, it seems natural to exploit syntactic information when predicting semantics. Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favor of neural sequence models, namely LSTMs (Zhou and Xu, 2015; Marcheggiani et al., 2017), and outperformed syntactically-driven methods on standard benchmarks. We believe that one of the reasons for this radical choice is the lack of simple and effective methods for incorporating syntactic information into sequential neural networks (namely, at the level of words). In this paper we</p>
<p>propose one way how to address this limitation.
Specifically, we rely on graph convolutional networks (GCNs) (Duvenaud et al., 2015; Kipf and Welling, 2017; Kearnes et al., 2016), a recent class of multilayer neural networks operating on graphs. For every node in the graph (in our case a word in a sentence), GCN encodes relevant information about its neighborhood as a real-valued feature vector. GCNs have been studied largely in the context of undirected unlabeled graphs. We introduce a version of GCNs for modeling syntactic dependency structures and generally applicable to labeled directed graphs.</p>
<p>One layer GCN encodes only information about immediate neighbors and $K$ layers are needed to encode $K$-order neighborhoods (i.e., information about nodes at most $K$ hops aways). This contrasts with recurrent and recursive neural networks (Elman, 1990; Socher et al., 2013) which, at least in theory, can capture statistical dependencies across unbounded paths in a trees or in a sequence. However, as we will further discuss in Section 3.3, this is not a serious limitation when GCNs are used in combination with encoders based on recurrent networks (LSTMs). When we stack GCNs on top of LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for English and Chinese. ${ }^{1}$</p>
<p>Interestingly, again unlike recursive neural networks, GCNs do not constrain the graph to be a tree. We believe that there are many applications in NLP, where GCN-based encoders of sentences or even documents can be used to incorporate knowledge about linguistic structures (e.g., representations of syntax, semantics or discourse). For example, GCNs can take as input combined syntactic-semantic graphs (e.g., the entire graph from Figure 1) and be used within downstream tasks such as machine translation or question answering. However, we leave this for future work and here solely focus on SRL.</p>
<p>The contributions of this paper can be summarized as follows:</p>
<ul>
<li>we are the first to show that GCNs are effective for NLP;</li>
<li>we propose a generalization of GCNs suited</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to encoding syntactic information at word level;</p>
<ul>
<li>we propose a GCN-based SRL model and obtain state-of-the-art results on English and Chinese portions of the CoNLL-2009 dataset;</li>
<li>we show that bidirectional LSTMs and syntax-based GCNs have complementary modeling power.</li>
</ul>
<h2>2 Graph Convolutional Networks</h2>
<p>In this section we describe GCNs of Kipf and Welling (2017). Please refer to Gilmer et al. (2017) for a comprehensive overview of GCN versions.</p>
<p>GCNs are neural networks operating on graphs and inducing features of nodes (i.e., real-valued vectors / embeddings) based on properties of their neighborhoods. In Kipf and Welling (2017), they were shown to be very effective for the node classification task: the classifier was estimated jointly with a GCN, so that the induced node features were informative for the node classification problem. Depending on how many layers of convolution are used, GCNs can capture information only about immediate neighbors (with one layer of convolution) or any nodes at most $K$ hops aways (if $K$ layers are stacked on top of each other).</p>
<p>More formally, consider an undirected graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}(|V|=n)$ and $\mathcal{E}$ are sets of nodes and edges, respectively. Kipf and Welling (2017) assume that edges contain all the self-loops, i.e., $(v, v) \in \mathcal{E}$ for any $v$. We can define a matrix $X \in \mathbb{R}^{m \times n}$ with each its column $x_{v} \in \mathbb{R}^{m}(v \in \mathcal{V})$ encoding node features. The vectors can either encode genuine features (e.g., this vector can encode the title of a paper if citation graphs are considered) or be a one-hot vector. The node representation, encoding information about its immediate neighbors, is computed as</p>
<p>$$
h_{v}=\operatorname{ReLU}\left(\sum_{u \in \mathcal{N}(v)}\left(W x_{u}+b\right)\right)
$$</p>
<p>where $W \in \mathbb{R}^{m \times m}$ and $b \in \mathbb{R}^{m}$ are a weight matrix and a bias, respectively; $\mathcal{N}(v)$ are neighbors of $v ; \operatorname{ReLU}$ is the rectifier linear unit activation function. ${ }^{2}$ Note that $v \in \mathcal{N}(v)$ (because of selfloops), so the input feature representation of $v$ (i.e. $x_{v}$ ) affects its induced representation $h_{v}$.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A simplified syntactic GCN (bias terms and gates are omitted); the syntactic graph of the sentence is shown with dashed lines at the bottom. Parameter matrices are sub-indexed with syntactic functions, and apostrophes (e.g., subj') signify that information flows in the direction opposite of the dependency arcs (i.e., from dependents to heads).</p>
<p>As in standard convolutional networks (LeCun et al., 2001), by stacking GCN layers one can incorporate higher degree neighborhoods:</p>
<p>$$
h_{v}^{(k+1)}=\operatorname{ReLU}\left(\sum_{u \in \mathcal{N}(v)} W^{(k)} h_{u}^{(k)}+b^{(k)}\right)
$$</p>
<p>where $k$ denotes the layer number and $h_{v}^{(1)}=x_{v}$.</p>
<h2>3 Syntactic GCNs</h2>
<p>As syntactic dependency trees are directed and labeled (we refer to the dependency labels as syntactic functions), we first need to modify the computation in order to incorporate label information (Section 3.1). In the subsequent section, we incorporate gates in GCNs, so that the model can decide which edges are more relevant to the task in question. Having gates is also important as we rely on automatically predicted syntactic representations, and the gates can detect and downweight potentially erroneous edges.</p>
<h3>3.1 Incorporating directions and labels</h3>
<p>Now, we introduce a generalization of GCNs appropriate for syntactic dependency trees, and in
general, for directed labeled graphs. First note that there is no reason to assume that information flows only along the syntactic dependency arcs (e.g., from makes to Sequa), so we allow it to flow in the opposite direction as well (i.e., from dependents to heads). We use a graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where the edge set contains all pairs of nodes (i.e., words) adjacent in the dependency tree. In our example, both (Sequa, makes) and (makes, Sequa) belong to the edge set. The graph is labeled, and the label $L(u, v)$ for $(u, v) \in \mathcal{E}$ contains both information about the syntactic function and indicates whether the edge is in the same or opposite direction as the syntactic dependency arc. For example, the label for (makes, Sequa) is subj, whereas the label for (Sequa, makes) is subj', with the apostrophe indicating that the edge is in the direction opposite to the corresponding syntactic arc. Similarly, self-loops will have label self. Consequently, we can simply assume that the GCN parameters are label-specific, resulting in the following computation, also illustrated in Figure 2:
$h_{v}^{(k+1)}=\operatorname{ReLU}\left(\sum_{u \in \mathcal{N}(v)} W_{L(u, v)}^{(k)} h_{u}^{(k)}+b_{L(u, v)}^{(k)}\right)$.
This model is over-parameterized, ${ }^{3}$ especially given that SRL datasets are moderately sized, by deep learning standards. So instead of learning the GCN parameters directly, we define them as</p>
<p>$$
W_{L(u, v)}^{(k)}=V_{\operatorname{dir}(u, v)}^{(k)}
$$</p>
<p>where $\operatorname{dir}(u, v)$ indicates whether the edge $(u, v)$ is directed (1) along, (2) in the opposite direction to the syntactic dependency arc, or (3) is a selfloop; $V_{\text {dir }(u, v)}^{(k)} \in \mathbb{R}^{m \times m}$. Our simplification captures the intuition that information should be propagated differently along edges depending whether this is a head-to-dependent or dependent-to-head edge (i.e., along or opposite the corresponding syntactic arc) and whether it is a self-loop. So we do not share any parameters between these three very different edge types. Syntactic functions are important, but perhaps less crucial, so they are encoded only in the feature vectors $b_{L(u, v)}$.</p>
<h3>3.2 Edge-wise gating</h3>
<p>Uniformly accepting information from all neighboring nodes may not be appropriate for the SRL</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>setting. For example, we see in Figure 1 that many semantic arcs just mirror their syntactic counterparts, so they may need to be up-weighted. Moreover, we rely on automatically predicted syntactic structures, and, even for English, syntactic parsers are far from being perfect, especially when used out-of-domain. It is risky for a downstream application to rely on a potentially wrong syntactic edge, so the corresponding message in the neural network may need to be down-weighted.</p>
<p>In order to address the above issues, inspired by recent literature (van den Oord et al., 2016; Dauphin et al., 2016), we calculate for each edge node pair a scalar gate of the form</p>
<p>$$
g_{u, v}^{(k)}=\sigma\left(h_{u}^{(k)} \cdot \hat{v}<em L_u_="L(u," v_="v)">{d i r(u, v)}^{(k)}+\hat{b}</em>\right)
$$}^{(k)</p>
<p>where $\sigma$ is the logistic sigmoid function, $\hat{v}<em L_u_="L(u," v_="v)">{d i r(u, v)}^{(k)} \in \mathbb{R}^{m}$ and $\hat{b}</em>$ are weights and a bias for the gate. With this additional gating mechanism, the final syntactic GCN computation is formulated as}^{(k)} \in \mathbb{R</p>
<p>$$
\begin{aligned}
h_{v}^{(k+1)} &amp; =\operatorname{ReLU} \
&amp; \sum_{u \in \mathcal{N}(v)} g_{v, u}^{(k)}\left(V_{d i r(u, v)}^{(k)} h_{u}^{(k)}+\hat{b}_{L(u, v)}^{(k)}\right)
\end{aligned}
$$</p>
<h3>3.3 Complementarity of GCNs and LSTMs</h3>
<p>The inability of GCNs to capture dependencies between nodes far away from each other in the graph may seem like a serious problem, especially in the context of SRL: paths between predicates and arguments often include many dependency arcs (Roth and Lapata, 2016). However, when graph convolution is performed on top of LSTM states (i.e., LSTM states serve as input $x_{v}=h_{v}^{(1)}$ to GCN) rather than static word embeddings, GCN may not need to capture more than a couple of hops.</p>
<p>To elaborate on this, let us speculate what role GCNs would play when used in combinations with LSTMs, given that LSTMs have already been shown very effective for SRL (Zhou and Xu, 2015; Marcheggiani et al., 2017). Though LSTMs are capable of capturing at least some degree of syntax (Linzen et al., 2016) without explicit syntactic supervision, SRL datasets are moderately sized, so LSTM models may still struggle with harder cases. Typically, harder cases for SRL involve arguments far away from their predicates. In fact, $20 \%$ and $30 \%$ of arguments are more than 5 tokens away from their predicate, in our English and
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Predicting an argument and its label with an LSTM + GCN encoder.</p>
<p>Chinese collections, respectively. However, if we imagine that we can 'teleport' even over a single (longest) syntactic dependency edge, the 'distance' would shrink: only $9 \%$ and $13 \%$ arguments will now be more than 5 LSTM steps away (again for English and Chinese, respectively). GCNs provide this 'teleportation' capability. These observations suggest that LSTMs and GCNs may be complementary, and we will see that empirical results support this intuition.</p>
<h2>4 Syntax-Aware Neural SRL Encoder</h2>
<p>In this work, we build our semantic role labeler on top of the syntax-agnostic LSTM-based SRL model of Marcheggiani et al. (2017), which already achieves state-of-the-art results on the CoNLL-2009 English dataset. Following their approach we employ the same bidirectional (BiLSTM) encoder and enrich it with a syntactic GCN.</p>
<p>The CoNLL-2009 benchmark assumes that predicate positions are already marked in the test set (e.g., we would know that makes, repairs and engines in Figure 1 are predicates), so no predicate identification is needed. Also, as we focus here solely on identifying arguments and labeling them with semantic roles, for predicate disambiguation</p>
<p>(i.e., marking makes as make.01) we use of an off-the-shelf disambiguation model (Roth and Lapata, 2016; Björkelund et al., 2009). As in Marcheggiani et al. (2017) and in most previous work, we process individual predicates in isolation, so for each predicate, our task reduces to a sequence labeling problem. That is, given a predicate (e.g., disputed in Figure 3) one needs to identify and label all its arguments (e.g., label estimates as A1 and label those as 'NULL', indicating that those is not an argument of disputed).</p>
<p>The semantic role labeler we propose is composed of four components (see Figure 3):</p>
<ul>
<li>look-ups of word embeddings;</li>
<li>a BiLSTM encoder that takes as input the word representation of each word in a sentence;</li>
<li>a syntax-based GCN encoder that re-encodes the BiLSTM representation based on the automatically predicted syntactic structure of the sentence;</li>
<li>a role classifier that takes as input the GCN representation of the candidate argument and the representation of the predicate to predict the role associated with the candidate word.</li>
</ul>
<h3>4.1 Word representations</h3>
<p>For each word $w_{i}$ in the considered sentence, we create a sentence-specific word representation $x_{i}$. We represent each word $w$ as the concatenation of four vectors: ${ }^{4}$ a randomly initialized word embedding $x^{r e} \in \mathbb{R}^{d_{w}}$, a pre-trained word embedding $x^{p e} \in \mathbb{R}^{d_{w}}$ estimated on an external text collection, a randomly initialized part-of-speech tag embedding $x^{p o s} \in \mathbb{R}^{d_{p}}$ and a randomly initialized lemma embedding $x^{l e} \in \mathbb{R}^{d_{l}}$ (active only if the word is a predicate). The randomly initialized embeddings $x^{r e}, x^{p o s}$, and $x^{l e}$ are fine-tuned during training, while the pre-trained ones are kept fixed. The final word representation is given by $x=x^{r e} \circ x^{p e} \circ x^{p o s} \circ x^{l e}$, where $\circ$ represents the concatenation operator.</p>
<h3>4.2 Bidirectional LSTM layer</h3>
<p>One of the most popular and effective ways to represent sequences, such as sentences (Mikolov et al., 2010), is to use recurrent neural networks</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(RNN) (Elman, 1990). In particular their gated versions, Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Cho et al., 2014), have proven effective in modeling long sequences (Chiu and Nichols, 2016; Sutskever et al., 2014).</p>
<p>Formally, an LSTM can be defined as a function $L S T M_{\theta}\left(x_{1: i}\right)$ that takes as input the sequence $x_{1: i}$ and returns a hidden state $h_{i} \in \mathbb{R}^{d_{h}}$. This state can be regarded as a representation of the sentence from the start to the position $i$, or, in other words, it encodes the word at position $i$ along with its left context. However, the right context is also important, so Bidirectional LSTMs (Graves, 2008) use two LSTMs: one for the forward pass, and another for the backward pass, $L S T M_{F}$ and $L S T M_{B}$, respectively. By concatenating the states of both LSTMs, we create a complete context-aware representation of a word $\operatorname{BiLSTM}\left(x_{1: n}, i\right)=L S T M_{F}\left(x_{1: i}\right) \circ$ $L S T M_{B}\left(x_{n: i}\right)$. We follow Marcheggiani et al. (2017) and stack $J$ layers of bidirectional LSTMs, where each layer takes the lower layer as its input.</p>
<h3>4.3 Graph convolutional layer</h3>
<p>The representation calculated with the BiLSTM encoder is fed as input to a GCN of the form defined in Equation (4). The neighboring nodes of a node $v$, namely $\mathcal{N}(v)$, and their relations to $v$ are predicted by an external syntactic parser.</p>
<h3>4.4 Semantic role classifier</h3>
<p>The classifier predicts semantic roles of words given the predicate while relying on word representations provided by GCN; we concatenate hidden states of the candidate argument word and the predicate word and use them as input to a classifier (Figure 3, top). The softmax classifier computes the probability of the role (including special 'NULL' role):</p>
<p>$$
p\left(r \mid t_{i}, t_{p}, l\right) \propto \exp \left(W_{l, r}\left(t_{i} \circ t_{p}\right)\right)
$$</p>
<p>where $t_{i}$ and $t_{p}$ are representations produced by the graph convolutional encoder, $l$ is the lemma of predicate $p$, and the symbol $\propto$ signifies proportionality. ${ }^{5}$ As FitzGerald et al. (2015) and Marcheggiani et al. (2017), instead of using a fixed matrix $W_{l, r}$ or simply assuming that $W_{l, r}=W_{r}$,</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: $F_{1}$ as function of word distance. The distance starts from zero, since nominal predicates can be arguments of themselves.
we jointly embed the role $r$ and predicate lemma $l$ using a non-linear transformation:</p>
<p>$$
W_{l, r}=\operatorname{ReLU}\left(U\left(q_{l} \circ q_{r}\right)\right)
$$</p>
<p>where $U$ is a parameter matrix, whereas $q_{l} \in \mathbb{R}^{d_{l}^{\prime}}$ and $q_{r} \in \mathbb{R}^{d_{r}}$ are randomly initialized embeddings of predicate lemmas and roles. In this way each role prediction is predicate-specific, and, at the same time, we expect to learn a good representation for roles associated with infrequent predicates. As our training objective we use the categorical cross-entropy.</p>
<h2>5 Experiments</h2>
<h3>5.1 Datasets and parameters</h3>
<p>We tested the proposed SRL model on the English and Chinese CoNLL-2009 dataset with standard splits into training, test and development sets. The predicted POS tags for both languages were provided by the CoNLL-2009 shared-task organizers. For the predicate disambiguator we used the ones from Roth and Lapata (2016) for English and from Björkelund et al. (2009) for Chinese. We parsed English sentences with the BIST Parser (Kiperwasser and Goldberg, 2016), whereas for Chinese we used automatically predicted parses provided by the CoNLL-2009 shared-task organizers.</p>
<p>For English, we used external embeddings of Dyer et al. (2015), learned using the structured skip n-gram approach of Ling et al. (2015). For Chinese we used external embeddings produced with the neural language model of Bengio et al. (2003). We used edge dropout in GCN: when
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance with dependency arcs of given type dropped, on Chinese development set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System (English)</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;">R</th>
<th style="text-align: center;">$\mathrm{F}_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LSTMs</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">82.7</td>
</tr>
<tr>
<td style="text-align: left;">LSTMs + GCNs (K=1)</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">83.3</td>
</tr>
<tr>
<td style="text-align: left;">LSTMs + GCNs (K=2)</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">82.7</td>
</tr>
<tr>
<td style="text-align: left;">LSTMs + GCNs (K=1), no gates</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr>
<td style="text-align: left;">GCNs (no LSTMs), K=1</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">74.9</td>
</tr>
<tr>
<td style="text-align: left;">GCNs (no LSTMs), K=2</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">78.7</td>
</tr>
<tr>
<td style="text-align: left;">GCNs (no LSTMs), K=3</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">79.5</td>
</tr>
<tr>
<td style="text-align: left;">GCNs (no LSTMs), K=4</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">79.2</td>
</tr>
</tbody>
</table>
<p>Table 1: SRL results without predicate disambiguation on the English development set.
computing $h_{v}^{(k)}$, we ignore each node $v \in \mathcal{N}(v)$ with probability $\beta$. Adam (Kingma and Ba, 2015) was used as an optimizer. The hyperparameter tuning and all model selection were performed on the English development set; the chosen values are shown in Appendix.</p>
<h3>5.2 Results and discussion</h3>
<p>In order to show that GCN layers are effective, we first compare our model against its version which lacks GCN layers (i.e. essentially the model of Marcheggiani et al. (2017)). Importantly, to measure the genuine contribution of GCNs, we first tuned this syntax-agnostic model (e.g., the number of LSTM layers) to get best possible performance on the development set. ${ }^{6}$</p>
<p>We compare the syntax-agnostic model with 3 syntax-aware versions: one GCN layer over syntax $(K=1)$, one layer GCN without gates and two GCN layers $(K=2)$. As we rely on the same</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">System (Chinese)</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;">R</th>
<th style="text-align: center;">$\mathrm{F}_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LSTMs</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">75.2</td>
</tr>
<tr>
<td style="text-align: left;">LSTMs + GCNs (K=1)</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">77.1</td>
</tr>
<tr>
<td style="text-align: left;">LSTMs + GCNs (K=2)</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">76.2</td>
</tr>
<tr>
<td style="text-align: left;">LSTMs + GCNs (K=1), no gates</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">76.5</td>
</tr>
<tr>
<td style="text-align: left;">GCNs (no LSTMs), K=1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">67.1</td>
</tr>
<tr>
<td style="text-align: left;">GCNs (no LSTMs), K=2</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: left;">GCNs (no LSTMs), K=3</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">GCNs (no LSTMs), K=4</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">70.4</td>
</tr>
</tbody>
</table>
<p>Table 2: SRL results without predicate disambiguation on the Chinese development set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: right;">P</th>
<th style="text-align: right;">R</th>
<th style="text-align: right;">$\mathrm{F}_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lei et al. (2015) (local)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">86.6</td>
</tr>
<tr>
<td style="text-align: left;">FitzGerald et al. (2015) (local)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">86.7</td>
</tr>
<tr>
<td style="text-align: left;">Roth and Lapata (2016) (local)</td>
<td style="text-align: right;">88.1</td>
<td style="text-align: right;">85.3</td>
<td style="text-align: right;">86.7</td>
</tr>
<tr>
<td style="text-align: left;">Marcheggiani et al. (2017) (local)</td>
<td style="text-align: right;">88.7</td>
<td style="text-align: right;">86.8</td>
<td style="text-align: right;">87.7</td>
</tr>
<tr>
<td style="text-align: left;">Ours (local)</td>
<td style="text-align: right;">$\mathbf{8 9 . 1}$</td>
<td style="text-align: right;">$\mathbf{8 6 . 8}$</td>
<td style="text-align: right;">$\mathbf{8 8 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Björkelund et al. (2010) (global)</td>
<td style="text-align: right;">88.6</td>
<td style="text-align: right;">85.2</td>
<td style="text-align: right;">86.9</td>
</tr>
<tr>
<td style="text-align: left;">FitzGerald et al. (2015) (global)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">87.3</td>
</tr>
<tr>
<td style="text-align: left;">Foland and Martin (2015) (global)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">86.0</td>
</tr>
<tr>
<td style="text-align: left;">Swayamdipta et al. (2016) (global)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">85.0</td>
</tr>
<tr>
<td style="text-align: left;">Roth and Lapata (2016) (global)</td>
<td style="text-align: right;">90.0</td>
<td style="text-align: right;">85.5</td>
<td style="text-align: right;">87.7</td>
</tr>
<tr>
<td style="text-align: left;">FitzGerald et al. (2015) (ensemble)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">87.7</td>
</tr>
<tr>
<td style="text-align: left;">Roth and Lapata (2016) (ensemble)</td>
<td style="text-align: right;">90.3</td>
<td style="text-align: right;">85.7</td>
<td style="text-align: right;">87.9</td>
</tr>
<tr>
<td style="text-align: left;">Ours (ensemble 3x)</td>
<td style="text-align: right;">$\mathbf{9 0 . 5}$</td>
<td style="text-align: right;">$\mathbf{8 7 . 7}$</td>
<td style="text-align: right;">$\mathbf{8 9 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on the test set for English.
off-the-shelf disambiguator for all versions of the model, in Table 1 and 2 we report SRL-only scores (i.e., predicate disambiguation is not evaluated) on the English and Chinese development sets. For both datasets, the syntax-aware model with one GCN layers $(K=1)$ performs the best, outperforming the LSTM version by $1.9 \%$ and $0.6 \%$ for Chinese and English, respectively. The reasons why the improvements on Chinese are much larger are not entirely clear (e.g., both languages are relative fixed word order ones, and the syntactic parses for Chinese are considerably less accurate), this may be attributed to a higher proportion of longdistance dependencies between predicates and arguments in Chinese (see Section 3.3). Edge-wise gating (Section 3.2) also appears important: removing gates leads to a drop of $0.3 \% \mathrm{~F}<em 1="1">{1}$ for English and $0.6 \% \mathrm{~F}</em>$ for Chinese.</p>
<p>Stacking two GCN layers does not give any benefit. When BiLSTM layers are dropped altogether, stacking two layers $(K=2)$ of GCNs greatly improves the performance, resulting in a $3.8 \%$ jump in $\mathrm{F}<em 1="1">{1}$ for English and a $3.0 \%$ jump in $\mathrm{F}</em>$ for Chi-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: right;">P</th>
<th style="text-align: right;">R</th>
<th style="text-align: right;">$\mathrm{F}_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zhao et al. (2009) (global)</td>
<td style="text-align: right;">80.4</td>
<td style="text-align: right;">75.2</td>
<td style="text-align: right;">77.7</td>
</tr>
<tr>
<td style="text-align: left;">Björkelund et al. (2009) (global)</td>
<td style="text-align: right;">82.4</td>
<td style="text-align: right;">75.1</td>
<td style="text-align: right;">78.6</td>
</tr>
<tr>
<td style="text-align: left;">Roth and Lapata (2016) (global)</td>
<td style="text-align: right;">83.2</td>
<td style="text-align: right;">75.9</td>
<td style="text-align: right;">79.4</td>
</tr>
<tr>
<td style="text-align: left;">Ours (local)</td>
<td style="text-align: right;">$\mathbf{8 4 . 6}$</td>
<td style="text-align: right;">$\mathbf{8 0 . 4}$</td>
<td style="text-align: right;">$\mathbf{8 2 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on the Chinese test set.
nese. Adding a 3rd layer of GCN $(K=3)$ further improves the performance. ${ }^{7}$ This suggests that extra GCN layers are effective but largely redundant with respect to what LSTMs already capture.</p>
<p>In Figure 4, we show the $F_{1}$ scores results on the English development set as a function of the distance, in terms of tokens, between a candidate argument and its predicate. As expected, GCNs appear to be more beneficial for long distance dependencies, as shorter ones are already accurately captured by the LSTM encoder.</p>
<p>We looked closer in contribution of specific dependency relations for Chinese. In order to assess this without retraining the model multiple times, we drop all dependencies of a given type at test time (one type at a time, only for types appearing over 300 times in the development set) and observe changes in performance. In Figure 5, we see that the most informative dependency is COMP (complement). Relative clauses in Chinese are very frequent and typically marked with particle的 (de). The relative clause will syntactically depend on 的 as COMP, so COMP encodes important information about predicate-argument structure. These are often long-distance dependencies and may not be accurately captured by LSTMs. Although TMP (temporal) dependencies are not as frequent ( $\sim 2 \%$ of all dependencies), they are also important: temporal information is mirrored in semantic roles.</p>
<p>In order to compare to previous work, in Table 3 we report test results on the English indomain (WSJ) evaluation data. Our model is local, as all the argument detection and labeling decisions are conditionally independent: their interaction is captured solely by the LSTM+GCN encoder. This makes our model fast and simple, though, as shown in previous work, global modeling of the structured output is beneficial. ${ }^{8}$ We leave this extension for future work. Interestingly,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: right;">P</th>
<th style="text-align: right;">R</th>
<th style="text-align: right;">$\mathrm{F}_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lei et al. (2015) (local)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">75.6</td>
</tr>
<tr>
<td style="text-align: left;">FitzGerald et al. (2015) (local)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">75.2</td>
</tr>
<tr>
<td style="text-align: left;">Roth and Lapata (2016) (local)</td>
<td style="text-align: right;">76.9</td>
<td style="text-align: right;">73.8</td>
<td style="text-align: right;">75.3</td>
</tr>
<tr>
<td style="text-align: left;">Marcheggiani et al. (2017) (local)</td>
<td style="text-align: right;">79.4</td>
<td style="text-align: right;">76.2</td>
<td style="text-align: right;">77.7</td>
</tr>
<tr>
<td style="text-align: left;">Ours (local)</td>
<td style="text-align: right;">$\mathbf{7 8 . 5}$</td>
<td style="text-align: right;">$\mathbf{7 5 . 9}$</td>
<td style="text-align: right;">$\mathbf{7 7 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Björkelund et al. (2010) (global)</td>
<td style="text-align: right;">77.9</td>
<td style="text-align: right;">73.6</td>
<td style="text-align: right;">75.7</td>
</tr>
<tr>
<td style="text-align: left;">FitzGerald et al. (2015) (global)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">75.2</td>
</tr>
<tr>
<td style="text-align: left;">Foland and Martin (2015) (global)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">75.9</td>
</tr>
<tr>
<td style="text-align: left;">Roth and Lapata (2016) (global)</td>
<td style="text-align: right;">78.6</td>
<td style="text-align: right;">73.8</td>
<td style="text-align: right;">76.1</td>
</tr>
<tr>
<td style="text-align: left;">FitzGerald et al. (2015) (ensemble)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">75.5</td>
</tr>
<tr>
<td style="text-align: left;">Roth and Lapata (2016) (ensemble)</td>
<td style="text-align: right;">79.7</td>
<td style="text-align: right;">73.6</td>
<td style="text-align: right;">76.5</td>
</tr>
<tr>
<td style="text-align: left;">Ours (ensemble 3s)</td>
<td style="text-align: right;">$\mathbf{8 0 . 8}$</td>
<td style="text-align: right;">$\mathbf{7 7 . 1}$</td>
<td style="text-align: right;">$\mathbf{7 8 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results on the out-of-domain test set.
we outperform even the best global model and the best ensemble of global models, without using global modeling or ensembles. When we create an ensemble of 3 models with the product-of-expert combination rule, we improve by $1.2 \%$ over the best previous result, achieving $89.1 \% \mathrm{~F}_{1} .{ }^{9}$</p>
<p>For Chinese (Table 4), our best model outperforms the state-of-the-art model of Roth and Lapata (2016) by even larger margin of $3.1 \%$.</p>
<p>For English, in the CoNLL shared task, systems are also evaluated on the out-of-domain dataset. Statistical models are typically less accurate when they are applied to out-of-domain data. Consequently, the predicted syntax for the out-ofdomain test set is of lower quality, which negatively affects the quality of GCN embeddings. However, our model works surprisingly well on out-of-domain data (Table 5), substantially outperforming all the previous syntax-aware models. This suggests that our model is fairly robust to mistakes in syntax. As expected though, our model does not outperform the syntax-agnostic model of Marcheggiani et al. (2017).</p>
<h2>6 Related Work</h2>
<p>Perhaps the earliest methods modeling syntaxsemantics interface with RNNs are due to (Henderson et al., 2008; Titov et al., 2009; Gesmundo et al., 2009), they used shift-reduce parsers for joint SRL and syntactic parsing, and relied on RNNs to model statistical dependencies across syntactic and semantic parsing actions. A more</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>modern (e.g., based on LSTMs) and effective reincarnation of this line of research has been proposed in Swayamdipta et al. (2016). Other recent work which considered incorporation of syntactic information in neural SRL models include: FitzGerald et al. (2015) who use standard syntactic features within an MLP calculating potentials of a CRF model; Roth and Lapata (2016) who enriched standard features for SRL with LSTM representations of syntactic paths between arguments and predicates; Lei et al. (2015) who relied on low-rank tensor factorizations for modeling syntax. Also Foland and Martin (2015) used (nongraph) convolutional networks and provided syntactic features as input. A very different line of research, but with similar goals to ours (i.e. integrating syntax with minimal feature engineering), used tree kernels (Moschitti et al., 2008).</p>
<p>Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016). One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015), including stacking them on top of a sequential BiLSTM (Miwa and Bansal, 2016). An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community, is related to graph convolution. However, it is inherently single-layer and tree-specific, uses bottom-up computations, does not share parameters across syntactic functions and does not use gates. Gates have been previously used in GCNs (Li et al., 2016) but between GCN layers rather than for individual edges.</p>
<p>Previous approaches to integrating syntactic information in neural models are mainly designed to induce representations of sentences or syntactic constituents. In contrast, the approach we presented incorporates syntactic information at word level. This may be attractive from the engineering perspective, as it can be used, as we have shown, instead or along with RNN models.</p>
<h2>7 Conclusions and Future Work</h2>
<p>We demonstrated how GCNs can be used to incorporate syntactic information in neural models and specifically to construct a syntax-aware SRL</p>
<p>model, resulting in state-of-the-art results for Chinese and English. There are relatively straightforward steps which can further improve the SRL results. For example, we relied on labeling arguments independently, whereas using a joint model is likely to significantly improve the performance. Also, in this paper we consider the dependency version of the SRL task, however the model can be generalized to the span-based version of the task (i.e. labeling argument spans with roles rather that syntactic heads of arguments) in a relatively straightforward fashion.</p>
<p>More generally, given simplicity of GCNs and their applicability to general graph structures (not necessarily trees), we believe that there are many NLP tasks where GCNs can be used to incorporate linguistic structures (e.g., syntactic and semantic representations of sentences and discourse parses or co-reference graphs for documents).</p>
<h2>Acknowledgements</h2>
<p>We would thank Anton Frolov, Michael Schlichtkrull, Thomas Kipf, Michael Roth, Max Welling, Yi Zhang, and Wilker Aziz for their suggestions and comments. The project was supported by the European Research Council (ERC StG BroadSem 678254), the Dutch National Science Foundation (NWO VIDI 639.022.518) and an Amazon Web Services (AWS) grant.</p>
<h2>References</h2>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155.</p>
<p>Anders Björkelund, Bernd Bohnet, Love Hafdell, and Pierre Nugues. 2010. A high-performance syntactic and semantic dependency parser. In Proceedings of COLING: Demonstrations.</p>
<p>Anders Björkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of CoNLL.</p>
<p>Jason P. C. Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional LSTM-CNNs. TACL, $4: 357-370$.</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of EMNLP.</p>
<p>Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2016. Language modeling with gated convolutional networks. arXiv preprint arXiv:1612.08083.</p>
<p>David K. Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In Proceedings of NIPS.</p>
<p>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transitionbased dependency parsing with stack long shortterm memory. In Proceedings of ACL.</p>
<p>Jeffrey L. Elman. 1990. Finding structure in time. Cognitive Science, 14(2):179-211.</p>
<p>Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun Cho. 2017. Learning to parse and translate improves neural machine translation. arXiv preprint arXiv:1702.03525.</p>
<p>Nicholas FitzGerald, Oscar Täckström, Kuzman Ganchev, and Dipanjan Das. 2015. Semantic role labeling with neural network factors. In Proceedings of EMNLP.</p>
<p>William Foland and James Martin. 2015. Dependencybased semantic role labeling using convolutional neural networks. In Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM).</p>
<p>Andrea Gesmundo, James Henderson, Paola Merlo, and Ivan Titov. 2009. Latent variable model of synchronous syntactic-semantic parsing for multiple languages. In Proceedings of CoNLL.</p>
<p>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics, 28(3):245-288.</p>
<p>Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. 2017. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212.</p>
<p>Alex Graves. 2008. Supervised sequence labelling with recurrent neural networks. Ph.D. thesis, München, Techn. Univ., Diss., 2008.</p>
<p>James Henderson, Paola Merlo, Gabriele Musillo, and Ivan Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In Proceedings of CoNLL.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780.</p>
<p>Richard Johansson and Pierre Nugues. 2008. The effect of syntactic representation on semantic role labeling. In Proceedings of COLING.</p>
<p>Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. 2016. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):595608 .</p>
<p>Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of $I C L R$.</p>
<p>Eliyahu Kiperwasser and Yoav Goldberg. 2016. Simple and accurate dependency parsing using bidirectional LSTM feature representations. TACL, 4:313327.</p>
<p>Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In Proceedings of ICLR.</p>
<p>Phong Le and Willem Zuidema. 2014. The insideoutside recursive neural network model for dependency parsing. In Proceedings of EMNLP.</p>
<p>Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. 2001. Gradient-based learning applied to document recognition. In Proceedings of Intelligent Signal Processing.</p>
<p>Tao Lei, Yuan Zhang, Lluís Màrquez, Alessandro Moschitti, and Regina Barzilay. 2015. High-order lowrank tensors for semantic role labeling. In Proceedings of NAACL.</p>
<p>Beth Levin. 1993. English verb classes and alternations: A preliminary investigation. University of Chicago press.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated graph sequence neural networks. In Proceedings of ICLR.</p>
<p>Wang Ling, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proceedings of NAACL.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. TACL, 4:521-535.</p>
<p>Diego Marcheggiani, Anton Frolov, and Ivan Titov. 2017. A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling. In Proceedings of CoNLL.</p>
<p>Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan Cernocký, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of INTERSPEECH.</p>
<p>Makoto Miwa and Mohit Bansal. 2016. End-to-end relation extraction using lstms on sequences and tree structures. In Proceedings of ACL.</p>
<p>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2):193-224.</p>
<p>Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2015. Discriminative neural sentence modeling by tree-based convolution. In Proceedings of EMNLP.</p>
<p>Aäron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, Oriol Vinyals, and Alex Graves. 2016. Conditional image generation with PixelCNN decoders. In Proceedings of NIPS.</p>
<p>Sameer Pradhan, Kadri Hacioglu, Wayne H. Ward, James H. Martin, and Daniel Jurafsky. 2005. Semantic role chunking combining complementary syntactic views. In Proceedings of CoNLL.</p>
<p>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257-287.</p>
<p>Michael Roth and Mirella Lapata. 2016. Neural semantic role labeling with dependency path embeddings. In Proceedings of ACL.</p>
<p>Rico Sennrich and Barry Haddow. 2016. Linguistic input features improve neural machine translation. In Proceedings of WMT.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of NIPS.</p>
<p>Swabha Swayamdipta, Miguel Ballesteros, Chris Dyer, and Noah A. Smith. 2016. Greedy, joint syntacticsemantic parsing with stack LSTMs. In Proceedings of CoNLL.</p>
<p>Cynthia A. Thompson, Roger Levy, and Christopher D. Manning. 2003. A generative model for semantic role labeling. In Proceedings of ECML.</p>
<p>Ivan Titov, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online projectivisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of IJCAI.</p>
<p>Hai Zhao, Wenliang Chen, Jun'ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies. In Proceedings of CoNLL.</p>
<p>Jie Zhou and Wei Xu. 2015. End-to-end learning of semantic role labeling using recurrent neural networks. In Proceedings of ACL.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ To compare to previous work, we report combined scores which also include predicate disambiguation. As we use disambiguators from previous work (see Section 5.1), actual gains in argument identification and labeling are even larger.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ We abuse the notation and refer as $p$ both to the predicate word and to its position in the sentence.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
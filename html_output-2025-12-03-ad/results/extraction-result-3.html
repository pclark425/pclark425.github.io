<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-1.html">extraction-schema-1</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games or interactive fiction, focusing on how they use memory to improve task performance, including types of memory used, memory representations, and performance comparisons with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-37088dec26231bc5a4937054ebc862bb83a3db4d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/37088dec26231bc5a4937054ebc862bb83a3db4d" target="_blank">Neural Episodic Control</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them, and shows across a wide range of environments that the agent learns significantly faster than other state-of-the-art, general purpose deep reinforcementlearning agents.</p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games or interactive fiction, focusing on how they use memory to improve task performance, including types of memory used, memory representations, and performance comparisons with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NEC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Episodic Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep reinforcement learning agent that utilizes a memory architecture called Differentiable Neural Dictionary (DND) to rapidly integrate experiences and improve learning efficiency in complex environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural Episodic Control (NEC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>NEC consists of a convolutional neural network for processing pixel images, a set of memory modules (DNDs) for each action, and a final network that converts memory read-outs into Q-values. It employs a semi-tabular representation of experience to enhance learning speed.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Atari Learning Environment</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>The DND allows for rapid updates of values associated with keys representing past experiences. It uses context-based lookups to retrieve values during action selection, and the memory is append-only, allowing it to grow large over time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>NEC achieved a score of 72.0% at 20 million frames in the Atari Learning Environment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not explicitly reported, but NEC outperforms traditional methods like DQN and A3C in initial learning phases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits_summary</strong></td>
                            <td>NEC demonstrates significantly faster learning in the initial phases compared to other algorithms, benefiting from its memory architecture that allows for quick integration of recent experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_challenges</strong></td>
                            <td>The paper does not explicitly mention limitations of the memory architecture, but it suggests that further improvements are needed for long-term performance compared to parametric agents.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_training_method</strong></td>
                            <td>Reinforcement learning with an epsilon-greedy policy for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_training_method</strong></td>
                            <td>Memory updates occur by appending new key-value pairs during agent actions, with values updated based on a mixture of on-policy and off-policy returns.</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>The Atari Learning Environment presents diverse challenges, including sparse rewards and varying magnitudes of scores, requiring efficient learning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Episodic Control', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Model-free episodic control <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with double Q-learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3",
    "paper_id": "paper-37088dec26231bc5a4937054ebc862bb83a3db4d",
    "extraction_schema_id": "extraction-schema-1",
    "extracted_data": [
        {
            "name_short": "NEC",
            "name_full": "Neural Episodic Control",
            "brief_description": "A deep reinforcement learning agent that utilizes a memory architecture called Differentiable Neural Dictionary (DND) to rapidly integrate experiences and improve learning efficiency in complex environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Neural Episodic Control (NEC)",
            "agent_description": "NEC consists of a convolutional neural network for processing pixel images, a set of memory modules (DNDs) for each action, and a final network that converts memory read-outs into Q-values. It employs a semi-tabular representation of experience to enhance learning speed.",
            "text_game_name": "Atari Learning Environment",
            "memory_used": true,
            "memory_type": "Episodic memory",
            "memory_mechanism_description": "The DND allows for rapid updates of values associated with keys representing past experiences. It uses context-based lookups to retrieve values during action selection, and the memory is append-only, allowing it to grow large over time.",
            "performance_with_memory": "NEC achieved a score of 72.0% at 20 million frames in the Atari Learning Environment.",
            "performance_without_memory": "Not explicitly reported, but NEC outperforms traditional methods like DQN and A3C in initial learning phases.",
            "performance_comparison_reported": true,
            "memory_benefits_summary": "NEC demonstrates significantly faster learning in the initial phases compared to other algorithms, benefiting from its memory architecture that allows for quick integration of recent experiences.",
            "memory_limitations_or_challenges": "The paper does not explicitly mention limitations of the memory architecture, but it suggests that further improvements are needed for long-term performance compared to parametric agents.",
            "agent_training_method": "Reinforcement learning with an epsilon-greedy policy for action selection.",
            "memory_training_method": "Memory updates occur by appending new key-value pairs during agent actions, with values updated based on a mixture of on-policy and off-policy returns.",
            "task_complexity_description": "The Atari Learning Environment presents diverse challenges, including sparse rewards and varying magnitudes of scores, requiring efficient learning strategies.",
            "uuid": "e3.0",
            "source_info": {
                "paper_title": "Neural Episodic Control",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Model-free episodic control",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning with double Q-learning",
            "rating": 1
        }
    ],
    "cost": 0.0026811,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural Episodic Control</h1>
<p>Alexander Pritzel<br>Benigno Uria<br>Sriram Srinivasan<br>Adrià Puigdomènech<br>Oriol Vinyals<br>Demis Hassabis<br>Daan Wierstra<br>Charles Blundell<br>DeepMind, London UK</p>
<p>APRITZEL@GOOGLE.COM<br>BURIA@GOOGLE.COM<br>SRSRINIVASAN@GOOGLE.COM<br>ADRIAP@GOOGLE.COM<br>VINYALS@GOOGLE.COM<br>DEMISHASSABIS@GOOGLE.COM<br>WIERSTRA@GOOGLE.COM<br>CBLUNDELL@GOOGLE.COM</p>
<h2>Abstract</h2>
<p>Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.</p>
<h2>1. Introduction</h2>
<p>Deep reinforcement learning agents have achieved state-of-the-art results in a variety of complex environments (Mnih et al., 2015; 2016), often surpassing human performance (Silver et al., 2016). Although the final performance of these agents is impressive, these techniques usually require several orders of magnitude more interactions with their environment than a human in order to reach an equivalent level of expected performance. For example, in the Atari 2600 set of environments (Bellemare et al., 2013), deep Q-networks (Mnih et al., 2016) require more than 200 hours of gameplay in order to achieve scores similar to those a human player achieves after two hours (Lake et al., 2016).</p>
<p>The glacial learning speed of deep reinforcement learning has several plausible explanations and in this work we focus on addressing these:</p>
<ol>
<li>Stochastic gradient descent optimisation requires the use of small learning rates. Due to the global approximation nature of neural networks, high learning rates cause catastrophic interference (McCloskey \&amp; Cohen, 1989). Low learning rates mean that experience can only be incorporated into a neural network slowly.</li>
<li>Environments with a sparse reward signal can be difficult for a neural network to model as there may be very few instances where the reward is non-zero. This can be viewed as a form of class imbalance where low-reward samples outnumber high-reward samples by an unknown number. Consequently, the neural network disproportionately underperforms at predicting larger rewards, making it difficult for an agent to take the most rewarding actions.</li>
<li>Reward signal propagation by value-bootstrapping techniques, such as Q-learning, results in reward information being propagated one step at a time through the history of previous interactions with the environment. This can be fairly efficient if updates happen in reverse order in which the transitions occur. However, in order to train on uncorrelated minibatches DQN-style, algorithms train on randomly selected transitions, and, in order to further stabilise training, require the use of a slowly updating target network further slowing down reward propagation.</li>
</ol>
<p>In this work we shall focus on addressing the three concerns listed above; we must note, however, that other recent advances in exploration (Osband et al., 2016), hierarchical reinforcement learning (Vezhnevets et al., 2016) and transfer learning (Rusu et al., 2016; Fernando et al., 2017) also make substantial contributions to improving data efficiency in deep reinforcement learning over baseline agents.</p>
<p>In this paper we propose Neural Episodic Control (NEC), a method which tackles the limitations of deep reinforcement learning listed above and demonstrates dramatic im-</p>
<p>provements on the speed of learning for a wide range of environments. Critically, our agent is able to rapidly latch onto highly successful strategies as soon as they are experienced, instead of waiting for many steps of optimisation (e.g., stochastic gradient descent) as is the case with DQN (Mnih et al., 2015) and A3C (Mnih et al., 2016).</p>
<p>Our work is in part inspired by the hypothesised role of the Hippocampus in decision making (Lengyel \&amp; Dayan, 2007; Blundell et al., 2016) and also by recent work on one-shot learning (Vinyals et al., 2016) and learning to remember rare events with neural networks (Kaiser et al., 2016). Our agent uses a semi-tabular representation of its experience of the environment possessing several of the features of episodic memory such as long term memory, sequentiality, and context-based lookups. The semi-tabular representation is an append-only memory that binds slow-changing keys to fast updating values and uses a context-based lookup on the keys to retrieve useful values during action selection by the agent. Thus the agent's memory operates in much the same way that traditional table-based RL methods map from state and action to value estimates. A unique aspect of the memory in contrast to other neural memory architectures for reinforcement learning (explained in more detail in Section 3) is that the values retrieved from the memory can be updated much faster than the rest of the deep neural network. This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton \&amp; Plaut, 1987), although the architecture we present is quite different. Another unique aspect of the memory is that unlike other memory architectures such as LSTM and the differentiable neural computer (DNC; Graves et al., 2016), our architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time. Instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to Oh et al. (2015); Graves et al. (2016) where the memory is wiped at the end of each episode). Reading from this large memory is made efficient using kd-tree based nearest neighbour (Bentley, 1975).</p>
<p>The remainder of the paper is organised as follows: in Section 2 we review deep reinforcement learning, in Section 3 the Neural Episodic Control algorithm is described, in Section 4 we report experimental results in the Atari Learning Environment, in Section 5 we discuss other methods that use memory for reinforcement learning, and finally in Section 6 we outline future work and summarise the main advantages of the NEC algorithm.</p>
<h2>2. Deep Reinforcement Learning</h2>
<p>The action-value function of a reinforcement learning agent (Sutton \&amp; Barto, 1998) is defined as $Q^{\pi}(s, a)=$ $\mathbb{E}<em t="t">{\pi}\left[\sum</em> \mid s, a\right]$, where $a$ is the initial action taken by the agent in the initial state $s$ and the expectation denotes that the policy $\pi$ is followed thereafter. The discount factor $\gamma \in(0,1)$ trades off favouring short vs. long term rewards.} \gamma^{t} r_{t</p>
<p>Deep Q-Network agents (DQN; Mnih et al., 2015) use Qlearning (Watkins \&amp; Dayan, 1992) to learn a value function $Q\left(s_{t}, a_{t}\right)$ to rank which action $a_{t}$ is best to take in each state $s_{t}$ at step $t$. The agent then executes an $\epsilon$-greedy policy based upon this value function to trade-off exploration and exploitation: with probability $\epsilon$ the agent picks an action uniformly at random, otherwise it picks the action $a_{t}=$ $\arg \max <em t="t">{a} Q\left(s</em>, a\right)$.
In DQN, the action-value function $Q\left(s_{t}, a_{t}\right)$ is parameterised by a convolutional neural network that takes a 2D pixel representation of the state $s_{t}$ as input, and outputs a vector containing the value of each action at that state. When the agent observes a transition, DQN stores the $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$ tuple in a replay buffer, the contents of which are used for training. This neural network is trained by minimizing the squared error between the network's output and the $Q$-learning target $y_{t}=r_{t}+\gamma \max <em t_1="t+1">{a} \hat{Q}\left(s</em>, a\right)$ is an older version of the value network that is updated periodically. The use of a target network and uncorrelated samples from the replay buffer are critical for stable training.}, a\right)$, for a subset of transitions sampled at random from the replay buffer. The target network $\hat{Q}\left(s_{t+1</p>
<p>A number of extensions have been proposed that improve DQN. Double DQN (Van Hasselt et al., 2016) reduces bias on the target calculation. Prioritised Replay (Schaul et al., 2015b) further improves Double DQN by optimising the replay strategy. Several authors have proposed methods of improving reward propagation and the back up mechanism of $Q$ learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation. $\mathrm{Q}^{*}(\lambda)$ (Harutyunyan et al., 2016) and $\operatorname{Retrace}(\lambda)$ (Munos et al., 2016) change the form of the Q-learning target to incorporate on-policy samples and fluidly switch between on-policy learning and off-policy learning. Munos et al. (2016) show that by incorporating on-policy samples allows an agent to learn faster in Atari environments, indicating that reward propagation is indeed a bottleneck to efficiency in deep reinforcement learning.</p>
<p>A3C (Mnih et al., 2016) is another well known deep reinforcement learning algorithm that is very different from DQN. It is based upon a policy gradient, and learns both a policy and its associated value function, which is learned</p>
<p>entirely on-policy (similar to the $\lambda=1$ case of Q( $\lambda$ )). Interestingly, Mnih et al. (2016) also added an LSTM memory to the otherwise convolutional neural network architecture to give the agent a notion of memory, although this did not have significant impact on the performance on Atari games.</p>
<h2>3. Neural Episodic Control</h2>
<p>Our agent consists of three components: a convolutional neural network that processes pixel images $s$, a set of memory modules (one per action), and a final network that converts read-outs from the action memories into $Q(s, a)$ values. For the convolutional neural network we use the same architecture as DQN (Mnih et al., 2015).</p>
<h3>3.1. Differentiable Neural Dictionary</h3>
<p>For each action $a \in \mathcal{A}$, NEC has a simple memory module $M_{a}=\left(K_{a}, V_{a}\right)$, where $K_{a}$ and $V_{a}$ are dynamically sized arrays of vectors, each containing the same number of vectors. The memory module acts as an arbitrary association from keys to corresponding values, much like the dictionary data type found in programs. Thus we refer to this kind of memory module as a differentiable neural dictionary (DND). There are two operations possible on a DND: lookup and write, as depicted in Figure 1. Performing a lookup on a DND maps a key $h$ to an output value $o$ :</p>
<p>$$
o=\sum_{i} w_{i} v_{i}
$$</p>
<p>where $v_{i}$ is the $i$ th element of the array $V_{a}$ and</p>
<p>$$
w_{i}=k\left(h, h_{i}\right) / \sum_{j} k\left(h, h_{j}\right)
$$</p>
<p>where $h_{i}$ is the $i$ th element of the array $K_{a}$ and $k(x, y)$ is a kernel between vectors $x$ and $y$, e.g., Gaussian or inverse kernels. Thus the output of a lookup in a DND is a weighted sum of the values in the memory, whose weights are given by normalised kernels between the lookup key and the corresponding key in memory. To make queries into very large memories scalable we shall make two approximations in practice: firstly, we shall limit (1) to the top $p$-nearest neighbours (typically $p=50$ ). Secondly, we use an approximate nearest neighbours algorithm to perform the lookups, based upon kd-trees (Bentley, 1975).</p>
<p>After a DND is queried, a new key-value pair is written into the memory. The key written corresponds to the key that was looked up. The associated value is application-specific (below we specify the update for the NEC agent). Writes to a DND are append-only: keys and values are written to the memory by appending them onto the end of the arrays $K_{a}$ and $V_{a}$ respectively. If a key already exists in the memory, then its corresponding value is updated, rather than being duplicated.</p>
<p>Note that a DND is a differentiable version of the memory module described in Blundell et al. (2016). It is also a generalisation to the memory and lookup schemes described in (Vinyals et al., 2016; Kaiser et al., 2016) for classification.</p>
<h3>3.2. Agent Architecture</h3>
<p>Figure 2 shows a DND as part of the NEC agent for a single action, whilst Algorithm 1 describes the general outline of the NEC algorithm. The pixel state $s$ is processed by a convolutional neural network to produce a key $h$. The key $h$ is then used to lookup a value from the DND, yielding weights $w_{i}$ in the process for each element of the memory arrays. Finally, the output is a weighted sum of the values in the DND. The values in the DND, in the case of an NEC agent, are the $Q$ values corresponding to the state that originally resulted in the corresponding key-value pair to be written to the memory. Thus this architecture produces an estimate of $Q(s, a)$ for a single given action $a$. The architecture is replicated once for each action $a$ the agent can take, with the convolutional part of the network shared among each separate DND $M_{a}$. The NEC agent acts by taking the action with the highest $Q$-value estimate at each time step. In practice, we use $\epsilon$-greedy policy during training with a low $\epsilon$.</p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">1</span> Neural Episodic Control
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}:</span><span class="err">\</span><span class="p">)</span> replay memory<span class="o">.</span>
    <span class="err">\</span><span class="p">(</span>M_<span class="p">{</span>a<span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="p">:</span> a DND for each action <span class="err">\</span><span class="p">(</span>a<span class="err">\</span><span class="p">)</span><span class="o">.</span>
    <span class="err">\</span><span class="p">(</span>N<span class="err">\</span><span class="p">)</span> <span class="p">:</span> horizon for <span class="err">\</span><span class="p">(</span>N<span class="err">\</span><span class="p">)</span><span class="o">-</span>step <span class="err">\</span><span class="p">(</span>Q<span class="err">\</span><span class="p">)</span> estimate<span class="o">.</span>
    for each episode do
        for <span class="err">\</span><span class="p">(</span><span class="ss">t</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="err">\</span>ldots<span class="p">,</span> T<span class="err">\</span><span class="p">)</span> do
            Receive observation <span class="err">\</span><span class="p">(</span>s_<span class="p">{</span>t<span class="p">}</span><span class="err">\</span><span class="p">)</span> from environment <span class="k">with</span> em-
            bedding <span class="err">\</span><span class="p">(</span>h<span class="err">\</span><span class="p">)</span><span class="o">.</span>
            Estimate <span class="err">\</span><span class="p">(</span>Q<span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a<span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> for each action <span class="err">\</span><span class="p">(</span>a<span class="err">\</span><span class="p">)</span> via <span class="p">(</span><span class="mi">1</span><span class="p">)</span> from <span class="err">\</span><span class="p">(</span>M_<span class="p">{</span>a<span class="p">}</span><span class="err">\</span><span class="p">)</span>
            <span class="err">\</span><span class="p">(</span>a_<span class="p">{</span>t<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>epsilon<span class="err">\</span><span class="p">)</span><span class="o">-</span>greedy policy based on <span class="err">\</span><span class="p">(</span>Q<span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a<span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
            Take action <span class="err">\</span><span class="p">(</span>a_<span class="p">{</span>t<span class="p">}</span><span class="err">\</span><span class="p">),</span> receive reward <span class="err">\</span><span class="p">(</span>r_<span class="p">{</span>t<span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
            Append <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span>h<span class="p">,</span> Q<span class="err">^</span><span class="p">{(</span>N<span class="p">)}</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a_<span class="p">{</span>t<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> to <span class="err">\</span><span class="p">(</span>M_<span class="p">{</span>a_<span class="p">{</span>t<span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
            Append <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a_<span class="p">{</span>t<span class="p">},</span> Q<span class="err">^</span><span class="p">{(</span>N<span class="p">)}</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a_<span class="p">{</span>t<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> to <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
            Train on a random minibatch from <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
        end for
    end for
</code></pre></div>

<h3>3.3. Adding $(s, a)$ pairs to memory</h3>
<p>As an NEC agent acts, it continually adds new key-value pairs to its memory. Keys are appended to the memory of the corresponding action, taking the value of the query key $h$ encoded by the convolutional neural network. We now turn to the question of an appropriate corresponding value. In Blundell et al. (2016), Monte Carlo returns were written to memory. We found that a mixture of Monte Carlo returns (on-policy) and off-policy backups worked better and so for NEC we elect to use $N$-step $Q$-learning as in Mnih et al.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Illustration of operations on a Differentiable Neural Dictionary.
(2016) (see also Watkins, 1989; Peng \&amp; Williams, 1996). This adds the following $N$ on-policy rewards and bootstraps the sum of discounted rewards for the rest of the trajectory, off-policy. The $N$-step $Q$-value estimate is then</p>
<p>$$
Q^{(N)}\left(s_{t}, a\right)=\sum_{j=0}^{N-1} \gamma^{j} r_{t+j}+\gamma^{N} \max <em t_N="t+N">{a^{\prime}} Q\left(s</em>\right)
$$}, a^{\prime</p>
<p>The bootstrap term of (3), $\max <em t_N="t+N">{a^{\prime}} Q\left(s</em>$ for each action $a$ and taking the highest estimated $Q$-value returned. Note that the earliest such values can be added to memory is $N$ steps after a particular $(s, a)$ pair occurs.
When a state-action value is already present in a DND (i.e the exact same key $h$ is already in $K_{a}$ ), the corresponding value present in $V_{a}, Q_{i}$, is updated in the same way as the classic tabular $Q$-learning algorithm:}, a^{\prime}\right)$ is found by querying all memories $M_{a</p>
<p>$$
Q_{i} \leftarrow Q_{i}+\alpha\left(Q^{(N)}(s, a)-Q_{i}\right)
$$</p>
<p>where $\alpha$ is the learning rate of the $Q$ update. If the state is not already present $Q^{(N)}\left(s_{t}, a\right)$ is appended to $V_{a}$ and $h$ is appended to $K_{a}$. Note that our agent learns the value function in much the same way that a classic tabular $Q$-learning agent does, except that the $Q$-table grows with time. We found that $\alpha$ could take on a high value, allowing repeatedly visited states with a stable representation to rapidly update their value function estimate. Additionally, batching up memory updates (e.g., at the end of the episode) helps with computational performance. We overwrite the item that has least recently shown up as a neighbour when we reach the memory's maximum capacity.</p>
<h3>3.4. Learning</h3>
<p>Agent parameters are updated by minimising the $L 2$ loss between the predicted $Q$ value for a given action and the $Q^{(N)}$ estimate on randomly sampled mini-batches from a replay buffer. In particular, we store tuples $\left(s_{t}, a_{t}, R_{t}\right)$ in
the replay buffer, where $N$ is the horizon of the $N$-step Q rule, and $R_{t}=Q^{(N)}\left(s_{t}, a\right)$ plays the role of the target network seen in DQN (our replay buffer is significantly smaller than DQN's). These $\left(s_{t}, a_{t}, R_{t}\right)$-tuples are then sampled uniformly at random to form minibatches for training. Note that the architecture in Figure 2 is entirely differentiable and so we can minimize this loss by gradient descent. Backpropagation updates the the weights and biases of the convolutional embedding network and the keys and values of each action-specific memory using gradients of this loss, using a lower learning rate than is used for updating pairs after queries $(\alpha)$.</p>
<h2>4. Experiments</h2>
<p>We investigated whether neural episodic control allows for more data efficient learning in practice in complex domains. As a problem domain we chose the Atari Learning Environment(ALE; Bellemare et al., 2013). We tested our method on the 57 Atari games used by Schaul et al. (2015a), which form an interesting set of tasks as they contain diverse challenges such as sparse rewards and vastly different magnitudes of scores across games. Most common algorithms applied in these domains, such as variants of DQN and A3C, require in the thousands of hours of in-game time, i.e. they are data inefficient.</p>
<p>We consider 5 variants of A3C and DQN as baselines as well as MFEC (Blundell et al., 2016). We compare to the basic implementations of A3C (Mnih et al., 2016) and DQN (Mnih et al., 2015). We also compare to two algorithms incorporating $\lambda$ returns (Sutton, 1988) aiming at more data efficiency by faster propagation of credit assignments, namely $Q^{*}(\lambda)$ (Harutyunyan et al., 2016) and $\operatorname{Retrace}(\lambda)$ (Munos et al., 2016). We also compare to DQN with Prioritised Replay, which improves data efficiency by replaying more salient transitions more frequently. We did not directly compare to DRQN (Hausknecht \&amp; Stone, 2015) nor FRMQN (Oh et al., 2016) as results were not available</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Architecture of episodic memory module for a single action $a$. Pixels representing the current state enter through a convolutional neural network on the bottom left and an estimate of $Q(s, a)$ exits top right. Gradients flow through the entire architecture.
for all Atari games. Note that in the case of DRQN, reported performance is lower than that of Prioritised Replay.</p>
<p>All algorithms were trained using discount rate $\gamma=0.99$, except MFEC that uses $\gamma=1$. In our implementation of MFEC we used random projections as an embedding function, since in the original publication it obtained better performance on the Atari games tested.</p>
<p>In terms of hyperparameters for NEC, we chose the same convolutional architecture as DQN, and store up to $5 \times$ $10^{5}$ memories per action. We used the RMSProp algorithm (Tieleman \&amp; Hinton, 2012) for gradient descent training. We apply the same preprocessing steps as (Mnih et al., 2015), including repeating each action four times. For the $N$-step $Q$ estimates we picked a horizon of $N=100$. Our replay buffer stores the only last $10^{5}$ states (as opposed to $10^{6}$ for DQN) observed and their $N$-step $Q$ estimates. We do one replay update for every 16 observed frames with a minibatch of size 32. We set the number of nearest neighbours $p=50$ in all our experiments. For the kernel function we chose a function that interpolates between the mean for short distances and weighted inverse distance for large distances, more precisely:</p>
<p>$$
k\left(h, h_{i}\right)=\frac{1}{\left|h-h_{i}\right|_{2}^{2}+\delta}
$$</p>
<p>Intuitively, when all neighbours are far away we want to avoid putting all weight onto one data point. A Gaussian kernel, for example, would exponentially suppress all neighbours except for the closest one. The kernel we chose has the advantage of having heavy tails. This makes the algorithm more robust and we found it to be less sensitive to kernel hyperparameters. We set $\delta=10^{-3}$.</p>
<p>In order to tune the remaining hyperparameters (SGD learning-rate, fast-update learning-rate $\alpha$ in Equation 4, dimensionality of the embeddings, $Q^{(N)}$ in Equation 3, and $\epsilon$ greedy exploration-rate) we ran a hyperparameter sweep on six games: Beam Rider, Breakout, Pong, Q*Bert, Seaquest and Space Invaders. We picked the hyperparameter values that performed best on the median for this subset of games (a
common cross validation procedure described by Bellemare et al. (2013), and adhered to by Mnih et al. (2015)).</p>
<p>Data efficiency results are summarised in Table 1. In the small data regime (less than 20 million frames) NEC clearly outperforms all other algorithms. The difference is especially pronounced before 5 million frames have been observed. Only at 40 million frames does DQN with Prioritised Replay outperform NEC on average; note that this corresponds to 185 hours of gameplay.</p>
<p>In order to provide a more detailed picture of NEC's performance, Figures 3 to 7 show learning curves on 6 games (Alien, Bowling, Boxing, Frostbite, HERO, Ms. Pac-Man, Pong), where several stereotypical cases of NEC's performance can be observed. All learning curves show the average performance over 5 different initial random seeds. We evaluate MFEC and NEC every 200.000 frames, and the other algorithms are evaluated every million steps.</p>
<p>Across most games, NEC is significantly faster at learning in the initial phase (see also Table 1), only comparable to MFEC, which also uses an episodic-like $Q$-function.</p>
<p>NEC also outperforms MFEC on average (see Table 2). In contrast with MFEC, NEC uses the reward signal to learn an embedding adequate for value interpolation. This difference is especially significant in games where a few pixels determine the value of each action. The simpler version of MFEC uses an approximation to $L 2$ distances in pixel-space by means of random projections, and cannot focus on the small but most relevant details. Another version of MFEC calculated distances on the latent representation of a variational autoencoder (Kingma \&amp; Welling, 2013) trained to model frames. This latent representation does not depend on rewards and will be subject to irrelevant details like, for example, the display of the current score.</p>
<p>A3C, DQN and related algorithms require rewards to be clipped to the range $[-1,1]$ for training stability ${ }^{1}$ (Mnih</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Frames</th>
<th style="text-align: left;">Nature DQN</th>
<th style="text-align: left;">$\mathrm{Q}^{*}(\lambda)$</th>
<th style="text-align: left;">$\operatorname{Retrace}(\lambda)$</th>
<th style="text-align: left;">Prioritised Replay</th>
<th style="text-align: left;">A3C</th>
<th style="text-align: left;">NEC</th>
<th style="text-align: left;">MFEC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1 M</td>
<td style="text-align: left;">$-0.7 \%$</td>
<td style="text-align: left;">$-0.8 \%$</td>
<td style="text-align: left;">$-0.4 \%$</td>
<td style="text-align: left;">$-2.4 \%$</td>
<td style="text-align: left;">$0.4 \%$</td>
<td style="text-align: left;">$\mathbf{1 6 . 7 \%}$</td>
<td style="text-align: left;">$12.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2 M</td>
<td style="text-align: left;">$0.0 \%$</td>
<td style="text-align: left;">$0.1 \%$</td>
<td style="text-align: left;">$0.2 \%$</td>
<td style="text-align: left;">$0.0 \%$</td>
<td style="text-align: left;">$0.9 \%$</td>
<td style="text-align: left;">$\mathbf{2 7 . 8 \%}$</td>
<td style="text-align: left;">$16.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">4 M</td>
<td style="text-align: left;">$2.4 \%$</td>
<td style="text-align: left;">$1.8 \%$</td>
<td style="text-align: left;">$3.3 \%$</td>
<td style="text-align: left;">$2.7 \%$</td>
<td style="text-align: left;">$1.9 \%$</td>
<td style="text-align: left;">$\mathbf{3 6 . 0 \%}$</td>
<td style="text-align: left;">$26.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">10 M</td>
<td style="text-align: left;">$15.7 \%$</td>
<td style="text-align: left;">$13.0 \%$</td>
<td style="text-align: left;">$17.3 \%$</td>
<td style="text-align: left;">$22.4 \%$</td>
<td style="text-align: left;">$3.6 \%$</td>
<td style="text-align: left;">$\mathbf{5 4 . 6 \%}$</td>
<td style="text-align: left;">$45.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">20 M</td>
<td style="text-align: left;">$26.8 \%$</td>
<td style="text-align: left;">$26.9 \%$</td>
<td style="text-align: left;">$30.4 \%$</td>
<td style="text-align: left;">$38.6 \%$</td>
<td style="text-align: left;">$7.9 \%$</td>
<td style="text-align: left;">$\mathbf{7 2 . 0 \%}$</td>
<td style="text-align: left;">$55.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">40 M</td>
<td style="text-align: left;">$52.7 \%$</td>
<td style="text-align: left;">$59.6 \%$</td>
<td style="text-align: left;">$60.5 \%$</td>
<td style="text-align: left;">$\mathbf{8 9 . 0 \%}$</td>
<td style="text-align: left;">$18.4 \%$</td>
<td style="text-align: left;">$83.3 \%$</td>
<td style="text-align: left;">$61.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1. Median across games of human-normalised scores for several algorithms at different points in training</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Frames</th>
<th style="text-align: left;">Nature DQN</th>
<th style="text-align: left;">$\mathrm{Q}^{*}(\lambda)$</th>
<th style="text-align: left;">$\operatorname{Retrace}(\lambda)$</th>
<th style="text-align: left;">Prioritised Replay</th>
<th style="text-align: left;">A3C</th>
<th style="text-align: left;">NEC</th>
<th style="text-align: left;">MFEC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1 M</td>
<td style="text-align: left;">$-10.5 \%$</td>
<td style="text-align: left;">$-11.7 \%$</td>
<td style="text-align: left;">$-10.5 \%$</td>
<td style="text-align: left;">$-14.4 \%$</td>
<td style="text-align: left;">$5.2 \%$</td>
<td style="text-align: left;">$\mathbf{4 5 . 6 \%}$</td>
<td style="text-align: left;">$28.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2 M</td>
<td style="text-align: left;">$-5.8 \%$</td>
<td style="text-align: left;">$-7.5 \%$</td>
<td style="text-align: left;">$-5.4 \%$</td>
<td style="text-align: left;">$-5.4 \%$</td>
<td style="text-align: left;">$8.0 \%$</td>
<td style="text-align: left;">$\mathbf{5 8 . 3 \%}$</td>
<td style="text-align: left;">$39.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">4 M</td>
<td style="text-align: left;">$8.8 \%$</td>
<td style="text-align: left;">$6.2 \%$</td>
<td style="text-align: left;">$6.2 \%$</td>
<td style="text-align: left;">$10.2 \%$</td>
<td style="text-align: left;">$11.8 \%$</td>
<td style="text-align: left;">$\mathbf{7 3 . 3 \%}$</td>
<td style="text-align: left;">$53.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">10 M</td>
<td style="text-align: left;">$51.3 \%$</td>
<td style="text-align: left;">$46.3 \%$</td>
<td style="text-align: left;">$52.7 \%$</td>
<td style="text-align: left;">$71.5 \%$</td>
<td style="text-align: left;">$22.3 \%$</td>
<td style="text-align: left;">$\mathbf{9 9 . 8 \%}$</td>
<td style="text-align: left;">$85.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">20 M</td>
<td style="text-align: left;">$94.5 \%$</td>
<td style="text-align: left;">$135.4 \%$</td>
<td style="text-align: left;">$\mathbf{2 7 3 . 7 \%}$</td>
<td style="text-align: left;">$165.2 \%$</td>
<td style="text-align: left;">$59.7 \%$</td>
<td style="text-align: left;">$121.5 \%$</td>
<td style="text-align: left;">$113.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">40 M</td>
<td style="text-align: left;">$151.2 \%$</td>
<td style="text-align: left;">$\mathbf{4 4 0 . 9 \%}$</td>
<td style="text-align: left;">$386.5 \%$</td>
<td style="text-align: left;">$332.3 \%$</td>
<td style="text-align: left;">$255.4 \%$</td>
<td style="text-align: left;">$144.8 \%$</td>
<td style="text-align: left;">$142.2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2. Mean human-normalised scores for several algorithms at different points in training
et al., 2015). NEC and MFEC do not require reward clipping, which results in qualitative changes in behaviour and better performance relative to other algorithms on games requiring clipping (Bowling, Frostbite, H.E.R.O., Ms. PacMan, Alien out of the seven shown).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Learning curve on Bowling.
Alien and Ms. Pac-Man both involve controlling a character, where there is an easy way to collect small rewards by collecting items of which there are plenty, while avoiding enemies, which are invulnerable to the agent. On the other hand the agent can pick up a special item making enemies vulnerable, allowing the agent to attack them and get significantly larger rewards than from collecting the small rewards. Agents trained using existing parametric methods tend to show little interest in this as clipping implies there is no difference between large and small rewards. Therefore, as NEC does not need reward clipping, it can strongly</p>
<p>Pop-Art.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Learning curve on Frostbite.
outperform other algorithms, since NEC is maximising the non-clipped score (the true score). This can also be seen when observing the agents play: parametric methods will tend to collect small rewards, while NEC will try to actively make the enemies vulnerable and attack them to get large rewards.</p>
<p>NEC also outperforms the other algorithms on Pong and Boxing where reward clipping does not affect any of the algorithms as all original rewards are in the range $[-1,1]$; as can be expected, NEC does not outperform others in terms of maximally achieved score, but it is vastly more data efficient.</p>
<p>In Figure 10 we show a chart of human-normalised scores across all 57 Atari games at 10 million frames comparing to Prioritised Replay and MFEC. We rank the games independently for each algorithm, and on the $y$-axis the deciles are</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Learning curve on H.E.R.O.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Learning curve on Ms. Pac-Man.
shown.
We can see that NEC gets to a human level performance in about $25 \%$ of the games within 10 million frames. As we can see NEC outperforms MFEC and Prioritised Replay.</p>
<h2>5. Related work</h2>
<p>There has been much recent work on memory architectures for neural networks (LSTM; Hochreiter \&amp; Schmidhuber, 1997), DNC (Graves et al., 2016), memory networks (Sukhbaatar et al., 2015; Miller et al., 2016)). Recurrent neural network representations of memory (LSTMs and DNCs) are trained by truncated backpropagation through time, and are subject to the same slow learning of non-recurrent neural networks.</p>
<p>Some of these models have been adapted to their use in RL agents (LSTMs; Bakker et al., 2003; Hausknecht \&amp; Stone, 2015), DNCs (Graves et al., 2016), memory networks (Oh et al., 2016). However, the contents of these memories is typically reset at the beginning of every episode. This is ap-
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Learning curve on Alien.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Learning curve on Pong.
propriate when the goal of the memory is tracking previous observations in order to maximise rewards in partially observable or non-Markovian environments. Therefore, these implementations can be thought of as a type of working memory, and solve a different problem than the one addressed in this work.</p>
<p>RNNs can learn to quickly write highly rewarding states into memory and may even be able to learn entire reinforcement learning algorithms (Wang et al., 2016; Duan et al., 2016). However, doing so can take an arbitrarily long time and the learning time likely scales strongly with the complexity of the task.</p>
<p>The work of Oh et al. (2016) is also reminiscent of the ideas presented here. They introduced (FR)MQN, an adaptation of memory networks used in the top layers of a $Q$-network.</p>
<p>Kaiser et al. (2016) introduced a differentiable layer of keyvalue pairs that can be plugged into a neural network. This layer uses cosine similarity to calculate a weighted average of the values associated with the $k$ most similar memories. Their use of a moving average update rule is reminiscent of</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Learning curve on Boxing.
the one presented in Section 3. The authors reported results on a set of supervised tasks, however they did not consider applications to reinforcement learning. Other deep RL methods keep a history of previous experience. Indeed, DQN itself has an elementary form of memory: the replay buffer central to its stable training can be viewed as a memory that is frequently replayed to distil the contents into DQN's value network. Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals. DQN's replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN's replay buffer to hold millions of $\left(s, a, r, s^{\prime}\right)$ tuples. The use of local regression techniques for $Q$-function approximation has been suggested before: Santamaría et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories. Munos \&amp; Moore (1998) proposed barycentric interpolators to model the value function and proved their convergence to the optimal value function under mild conditions, but no empirical results were presented. Gabel \&amp; Riedmiller (2005) also suggested the use of local regression, under the paradigm of case-based-reasoning that included heuristics for the deletion of stored cases. Blundell et al. (2016, MFEC) recently used local regression for $Q$-function estimation using the mean of the k-nearest neighbours, except in the case of an exact match of the query point, in which case the stored value was returned. They also propose the use of the latent variable obtained from a variational autoencoder (Rezende et al., 2014) as an embedding space, but showed random projections often obtained better results. In contrast with the ideas presented here, none of the localregression work aforementioned uses the reward signal to learn an embedding space of covariates in which to perform the local-regression. We learn this embedding space using temporal-difference learning; a crucial difference, as we
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Human-normalised scores of games, independently ranked per algorithm; labels on y-axis denote quantiles.
showed in the experimental comparison to MFEC.</p>
<h2>6. Discussion</h2>
<p>We have proposed Neural Episodic Control (NEC): a deep reinforcement learning agent that learns significantly faster than other baseline agents on a wide range of Atari 2600 games. At the core of NEC is a memory structure: a Differentiable Neural Dictionary (DND), one for each potential action. NEC inserts recent state representations paired with corresponding value functions into the appropriate DND.</p>
<p>Our experiments show that NEC requires an order of magnitude fewer interactions with the environment than agents previously proposed for data efficiency, such as Prioritised Replay (Schaul et al., 2015b) and Retrace $(\lambda)$ (Munos et al., 2016). We speculate that NEC learns faster through a com-</p>
<p>bination of three features of the agent: the memory architecture (DND), the use of $N$-step $Q$ estimates, and a state representation provided by a convolutional neural network.</p>
<p>The memory architecture, DND, rapidly integrates recent experience-state representations and corresponding value estimates-allowing this information to be rapidly integrated into future behaviour. Such memories persist across many episodes, and we use a fast approximate nearest neighbour algorithm (kd-trees) to ensure that such memories can be efficiently accessed. Estimating $Q$-values by using the $N$-step $Q$ value function interpolates between Monte Carlo value estimates and backed up off-policy estimates. Monte Carlo value estimates reflect the rewards an agent is actually receiving, whilst backed up off-policy estimates should be more representative of the value function at the optimal policy, but evolve much slower. By using both estimates, NEC can trade-off between these two estimation procedures and their relative strengths and weaknesses (speed of reward propagation vs optimality). Finally, by having a slow changing, stable representation provided by a convolutional neural network, keys stored in the DND remain relative stable.</p>
<p>Our work suggests that non-parametric methods are a promising addition to the deep reinforcement learning toolbox, especially where data efficiency is paramount. In our experiments we saw that at the beginning of learning NEC outperforms other agents in terms of learning speed. We saw that later in learning Prioritised Replay has higher performance than NEC. We leave it to future work to further improve NEC so that its long term final performance is significantly superior to parametric agents. Another avenue of further research would be to apply the method discussed in this paper to a wider range of tasks such as visually more complex 3D worlds or real world tasks where data efficiency is of great importance due to the high cost of acquiring data.</p>
<p>Acknowledgements The authors would like to thank Daniel Zoran, Dharshan Kumaran, Jane Wang, Dan Belov, Ruiqi Guo, Yori Zwols, Jack Rae, Andreas Kirsch, Peter Dayan, David Silver and many others at DeepMind for insightful discussions and feedback. We also thank Georg Ostrovski, Tom Schaul, and Hubert Soyer for providing baseline learning curves.</p>
<h2>References</h2>
<p>Ba, Jimmy, Hinton, Geoffrey E, Mnih, Volodymyr, Leibo, Joel Z, and Ionescu, Catalin. Using fast weights to attend to the recent past. In Advances In Neural Information Processing Systems, pp. 4331-4339, 2016.</p>
<p>Bakker, Bram, Zhumatiy, Viktor, Gruener, Gabriel, and Schmidhuber, Jürgen. A robot that reinforcement-learns to identify and memorize important previous observations.</p>
<p>In Intelligent Robots and Systems, 2003.(IROS 2003). Proceedings. 2003 IEEE/RSJ International Conference on, volume 1, pp. 430-435. IEEE, 2003.</p>
<p>Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 06 2013.</p>
<p>Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and Bowling, Michael. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47:253-279, 2013.</p>
<p>Bentley, Jon Louis. Multidimensional binary search trees used for associative searching. Commun. ACM, 18(9): 509-517, September 1975.</p>
<p>Blundell, Charles, Uria, Benigno, Pritzel, Alexander, Li, Yazhe, Ruderman, Avraham, Leibo, Joel Z, Rae, Jack, Wierstra, Daan, and Hassabis, Demis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.</p>
<p>Duan, Yan, Schulman, John, Chen, Xi, Bartlett, Peter L, Sutskever, Ilya, and Abbeel, Pieter. R ${ }^{2}$ : Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.</p>
<p>Fernando, Chrisantha, Banarse, Dylan, Blundell, Charles, Zwols, Yori, Ha, David, Rusu, Andrei A, Pritzel, Alexander, and Wierstra, Daan. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.</p>
<p>Gabel, Thomas and Riedmiller, Martin. Cbr for state value function approximation in reinforcement learning. In International Conference on Case-Based Reasoning, pp. 206-221. Springer, 2005.</p>
<p>Graves, Alex, Wayne, Greg, Reynolds, Malcolm, Harley, Tim, Danihelka, Ivo, Grabska-Barwińska, Agnieszka, Colmenarejo, Sergio Gómez, Grefenstette, Edward, Ramalho, Tiago, Agapiou, John, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471-476, 2016.</p>
<p>Harutyunyan, Anna, Bellemare, Marc G, Stepleton, Tom, and Munos, Rémi. Q ( $\backslash$ lambda) with off-policy corrections. In International Conference on Algorithmic Learning Theory, pp. 305-320. Springer, 2016.</p>
<p>Hausknecht, Matthew and Stone, Peter. Deep recurrent qlearning for partially observable mdps. arXiv preprint arXiv:1507.06527, 2015.</p>
<p>He, Frank S, Liu, Yang, Schwing, Alexander G, and Peng, Jian. Learning to play in a day: Faster deep reinforcement learning by optimality tightening. arXiv preprint arXiv:1611.01606, 2016.</p>
<p>Hinton, Geoffrey E and Plaut, David C. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pp. 177-186, 1987.</p>
<p>Hochreiter, Sepp and Schmidhuber, Jürgen. Long short-term memory. Neural Comput., 9(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8. 1735 .</p>
<p>Kaiser, Lukasz, Nachum, Ofir, Roy, Aurko, and Bengio, Samy. Learning to remember rare events. 2016.</p>
<p>Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Kumaran, Dharshan, Hassabis, Demis, and McClelland, James L. What learning systems do intelligent agents need? complementary learning systems theory updated. Trends in Cognitive Sciences, 20(7):512-534, 2016.</p>
<p>Lake, Brenden M, Ullman, Tomer D, Tenenbaum, Joshua B, and Gershman, Samuel J. Building machines that learn and think like people. arXiv preprint arXiv:1604.00289, 2016.</p>
<p>Lengyel, M. and Dayan, P. Hippocampal contributions to control: The third way. In NIPS, volume 20, pp. 889-896, 2007.</p>
<p>McCloskey, Michael and Cohen, Neal J. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24: 109-165, 1989.</p>
<p>Miller, Alexander, Fisch, Adam, Dodge, Jesse, Karimi, Amir-Hossein, Bordes, Antoine, and Weston, Jason. Keyvalue memory networks for directly reading documents. arXiv preprint arXiv:1606.03126, 2016.</p>
<p>Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Mnih, Volodymyr, Badia, Adria Puigdomenech, Mirza, Mehdi, Graves, Alex, Lillicrap, Timothy P, Harley, Tim, Silver, David, and Kavukcuoglu, Koray. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016.</p>
<p>Munos, Remi and Moore, Andrew W. Barycentric interpolators for continuous space and time reinforcement learning. In NIPS, pp. 1024-1030, 1998.</p>
<p>Munos, Rémi, Stepleton, Tom, Harutyunyan, Anna, and Bellemare, Marc. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1046-1054, 2016.</p>
<p>Oh, Junhyuk, Guo, Xiaoxiao, Lee, Honglak, Lewis, Richard L, and Singh, Satinder. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pp. 2845-2853, 2015.</p>
<p>Oh, Junhyuk, Chockalingam, Valliappa, Lee, Honglak, et al. Control of memory, active perception, and action in minecraft. In Proceedings of The 33rd International Conference on Machine Learning, pp. 2790-2799, 2016.</p>
<p>Osband, Ian, Blundell, Charles, Pritzel, Alexander, and Van Roy, Benjamin. Deep exploration via bootstrapped dqn. In Advances In Neural Information Processing Systems, pp. 4026-4034, 2016.</p>
<p>Peng, Jing and Williams, Ronald J. Incremental multi-step q-learning. Machine learning, 22(1-3):283-290, 1996.</p>
<p>Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of The 31st International Conference on Machine Learning, pp. 1278-1286, 2014.</p>
<p>Rusu, Andrei A, Rabinowitz, Neil C, Desjardins, Guillaume, Soyer, Hubert, Kirkpatrick, James, Kavukcuoglu, Koray, Pascanu, Razvan, and Hadsell, Raia. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.</p>
<p>Santamaría, Juan C, Sutton, Richard S, and Ram, Ashwin. Experiments with reinforcement learning in problems with continuous state and action spaces. Adaptive behavior, 6(2):163-217, 1997.</p>
<p>Schaul, Tom, Quan, John, Antonoglou, Ioannis, and Silver, David. Prioritized experience replay. CoRR, abs/1511.05952, 2015a.</p>
<p>Schaul, Tom, Quan, John, Antonoglou, Ioannis, and Silver, David. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015b.</p>
<p>Silver, David, Huang, Aja, Maddison, Chris J, Guez, Arthur, Sifre, Laurent, Van Den Driessche, George, Schrittwieser, Julian, Antonoglou, Ioannis, Panneershelvam, Veda, Lanctot, Marc, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587): 484-489, 2016.</p>
<p>Sukhbaatar, Sainbayar, Weston, Jason, Fergus, Rob, et al. End-to-end memory networks. In Advances in neural information processing systems, pp. 2440-2448, 2015.</p>
<p>Sutton, Richard S. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9-44, 1988.</p>
<p>Sutton, Richard S and Barto, Andrew G. Reinforcement learning: An introduction. MIT press, 1998.</p>
<p>Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4:2, 2012.
van Hasselt, H., Guez, A., Hessel, M., and Silver, D. Learning functions across many orders of magnitudes. ArXiv e-prints, February 2016.</p>
<p>Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep reinforcement learning with double q-learning. In AAAI, pp. 2094-2100, 2016.</p>
<p>Vezhnevets, Alexander, Mnih, Volodymyr, Osindero, Simon, Graves, Alex, Vinyals, Oriol, Agapiou, John, et al. Strategic attentive writer for learning macro-actions. In Advances in Neural Information Processing Systems, pp. 3486-3494, 2016.</p>
<p>Vinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra, Daan, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630-3638, 2016.</p>
<p>Wang, Jane X, Kurth-Nelson, Zeb, Tirumala, Dhruva, Soyer, Hubert, Leibo, Joel Z, Munos, Remi, Blundell, Charles, Kumaran, Dharshan, and Botvinick, Matt. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.</p>
<p>Watkins, Christopher JCH and Dayan, Peter. Q-learning. Machine learning, 8(3-4):279-292, 1992.</p>
<p>Watkins, Christopher John Cornish Hellaby. Learning from delayed rewards. PhD thesis, University of Cambridge England, 1989.</p>
<h1>A. Scores on Atari Games</h1>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">A3C</th>
<th style="text-align: center;">Nature DQN</th>
<th style="text-align: center;">MFEC</th>
<th style="text-align: center;">NEC</th>
<th style="text-align: center;">Prioritised <br> Replay</th>
<th style="text-align: center;">$\mathrm{Q}^{*}(\lambda)$</th>
<th style="text-align: center;">$\operatorname{Retrace}(\lambda)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Alien</td>
<td style="text-align: center;">415.5</td>
<td style="text-align: center;">634.8</td>
<td style="text-align: center;">1717.7</td>
<td style="text-align: center;">3460.6</td>
<td style="text-align: center;">800.5</td>
<td style="text-align: center;">476.8</td>
<td style="text-align: center;">541.2</td>
</tr>
<tr>
<td style="text-align: center;">Amidar</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">126.8</td>
<td style="text-align: center;">370.9</td>
<td style="text-align: center;">811.3</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">134.5</td>
<td style="text-align: center;">162.9</td>
</tr>
<tr>
<td style="text-align: center;">Assault</td>
<td style="text-align: center;">720.8</td>
<td style="text-align: center;">1489.5</td>
<td style="text-align: center;">510.2</td>
<td style="text-align: center;">599.9</td>
<td style="text-align: center;">1339.9</td>
<td style="text-align: center;">1026.6</td>
<td style="text-align: center;">1331.1</td>
</tr>
<tr>
<td style="text-align: center;">Asterix</td>
<td style="text-align: center;">301.6</td>
<td style="text-align: center;">2989.1</td>
<td style="text-align: center;">1776.6</td>
<td style="text-align: center;">2480.4</td>
<td style="text-align: center;">2599.7</td>
<td style="text-align: center;">2588.6</td>
<td style="text-align: center;">2520.3</td>
</tr>
<tr>
<td style="text-align: center;">Asteroids</td>
<td style="text-align: center;">1360.1</td>
<td style="text-align: center;">395.3</td>
<td style="text-align: center;">4706.8</td>
<td style="text-align: center;">2496.1</td>
<td style="text-align: center;">854.0</td>
<td style="text-align: center;">569.8</td>
<td style="text-align: center;">579.2</td>
</tr>
<tr>
<td style="text-align: center;">Atlantis</td>
<td style="text-align: center;">36383</td>
<td style="text-align: center;">14210.5</td>
<td style="text-align: center;">95499.4</td>
<td style="text-align: center;">51208.0</td>
<td style="text-align: center;">12579.1</td>
<td style="text-align: center;">28818.8</td>
<td style="text-align: center;">44771.1</td>
</tr>
<tr>
<td style="text-align: center;">Bank Heist</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">163.7</td>
<td style="text-align: center;">343.3</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: center;">Battlezone</td>
<td style="text-align: center;">2354.2</td>
<td style="text-align: center;">6961.0</td>
<td style="text-align: center;">19053.6</td>
<td style="text-align: center;">13345.5</td>
<td style="text-align: center;">13500.0</td>
<td style="text-align: center;">8227.2</td>
<td style="text-align: center;">6762.2</td>
</tr>
<tr>
<td style="text-align: center;">Beamrider</td>
<td style="text-align: center;">450.2</td>
<td style="text-align: center;">3741.7</td>
<td style="text-align: center;">858.8</td>
<td style="text-align: center;">749.6</td>
<td style="text-align: center;">3249.6</td>
<td style="text-align: center;">656.2</td>
<td style="text-align: center;">725.4</td>
</tr>
<tr>
<td style="text-align: center;">Berzerk</td>
<td style="text-align: center;">593.6</td>
<td style="text-align: center;">484.2</td>
<td style="text-align: center;">924.2</td>
<td style="text-align: center;">852.8</td>
<td style="text-align: center;">575.6</td>
<td style="text-align: center;">647.9</td>
<td style="text-align: center;">701.5</td>
</tr>
<tr>
<td style="text-align: center;">Bowling</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">39.9</td>
</tr>
<tr>
<td style="text-align: center;">Boxing</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">30.7</td>
</tr>
<tr>
<td style="text-align: center;">Breakout</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">10.2</td>
</tr>
<tr>
<td style="text-align: center;">Centipede</td>
<td style="text-align: center;">3228</td>
<td style="text-align: center;">4401.4</td>
<td style="text-align: center;">20608.8</td>
<td style="text-align: center;">12314.5</td>
<td style="text-align: center;">4694.1</td>
<td style="text-align: center;">4097.5</td>
<td style="text-align: center;">4792.9</td>
</tr>
<tr>
<td style="text-align: center;">Chopper Command</td>
<td style="text-align: center;">1036.7</td>
<td style="text-align: center;">827.2</td>
<td style="text-align: center;">3075.6</td>
<td style="text-align: center;">5070.3</td>
<td style="text-align: center;">1426.5</td>
<td style="text-align: center;">760.6</td>
<td style="text-align: center;">801.6</td>
</tr>
<tr>
<td style="text-align: center;">Crazy Climber</td>
<td style="text-align: center;">70103.5</td>
<td style="text-align: center;">66061.6</td>
<td style="text-align: center;">9892.2</td>
<td style="text-align: center;">34344.0</td>
<td style="text-align: center;">76574.1</td>
<td style="text-align: center;">64980.6</td>
<td style="text-align: center;">54177.6</td>
</tr>
<tr>
<td style="text-align: center;">Defender</td>
<td style="text-align: center;">4596</td>
<td style="text-align: center;">2877.9</td>
<td style="text-align: center;">10052.8</td>
<td style="text-align: center;">6126.1</td>
<td style="text-align: center;">3486.4</td>
<td style="text-align: center;">3260.8</td>
<td style="text-align: center;">3275.6</td>
</tr>
<tr>
<td style="text-align: center;">Demon Attack</td>
<td style="text-align: center;">346.8</td>
<td style="text-align: center;">5541.9</td>
<td style="text-align: center;">1081.8</td>
<td style="text-align: center;">641.4</td>
<td style="text-align: center;">6503.6</td>
<td style="text-align: center;">4914.8</td>
<td style="text-align: center;">4836.6</td>
</tr>
<tr>
<td style="text-align: center;">Double Dunk</td>
<td style="text-align: center;">$-17.2$</td>
<td style="text-align: center;">$-19.0$</td>
<td style="text-align: center;">$-13.2$</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">$-15.9$</td>
<td style="text-align: center;">$-18.2$</td>
<td style="text-align: center;">$-18.3$</td>
</tr>
<tr>
<td style="text-align: center;">Enduro</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">364.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">1125.8</td>
<td style="text-align: center;">396.0</td>
<td style="text-align: center;">440.6</td>
</tr>
<tr>
<td style="text-align: center;">Fishing Derby</td>
<td style="text-align: center;">$-89.5$</td>
<td style="text-align: center;">$-81.6$</td>
<td style="text-align: center;">$-90.3$</td>
<td style="text-align: center;">$-72.2$</td>
<td style="text-align: center;">$-48.2$</td>
<td style="text-align: center;">$-84.2$</td>
<td style="text-align: center;">$-79.8$</td>
</tr>
<tr>
<td style="text-align: center;">Freeway</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;">Frostbite</td>
<td style="text-align: center;">218.9</td>
<td style="text-align: center;">339.1</td>
<td style="text-align: center;">925.1</td>
<td style="text-align: center;">2747.4</td>
<td style="text-align: center;">711.3</td>
<td style="text-align: center;">407.2</td>
<td style="text-align: center;">325.0</td>
</tr>
<tr>
<td style="text-align: center;">Gopher</td>
<td style="text-align: center;">854.1</td>
<td style="text-align: center;">1111.2</td>
<td style="text-align: center;">4412.6</td>
<td style="text-align: center;">2432.3</td>
<td style="text-align: center;">1235.3</td>
<td style="text-align: center;">2292.4</td>
<td style="text-align: center;">3050.4</td>
</tr>
<tr>
<td style="text-align: center;">Gravitar</td>
<td style="text-align: center;">215.8</td>
<td style="text-align: center;">154.7</td>
<td style="text-align: center;">1011.3</td>
<td style="text-align: center;">1257.0</td>
<td style="text-align: center;">218.9</td>
<td style="text-align: center;">121.9</td>
<td style="text-align: center;">108.9</td>
</tr>
<tr>
<td style="text-align: center;">H.E.R.O.</td>
<td style="text-align: center;">4598.2</td>
<td style="text-align: center;">1050.7</td>
<td style="text-align: center;">14767.7</td>
<td style="text-align: center;">16265.3</td>
<td style="text-align: center;">5164.5</td>
<td style="text-align: center;">2223.3</td>
<td style="text-align: center;">3298.2</td>
</tr>
<tr>
<td style="text-align: center;">Ice Hockey</td>
<td style="text-align: center;">$-8.1$</td>
<td style="text-align: center;">$-4.5$</td>
<td style="text-align: center;">$-6.5$</td>
<td style="text-align: center;">$-1.6$</td>
<td style="text-align: center;">$-10.2$</td>
<td style="text-align: center;">$-11.1$</td>
<td style="text-align: center;">$-9.1$</td>
</tr>
<tr>
<td style="text-align: center;">James Bond</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">165.9</td>
<td style="text-align: center;">244.7</td>
<td style="text-align: center;">376.8</td>
<td style="text-align: center;">203.8</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">67.2</td>
</tr>
<tr>
<td style="text-align: center;">Kangaroo</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">519.6</td>
<td style="text-align: center;">2465.7</td>
<td style="text-align: center;">2489.1</td>
<td style="text-align: center;">616.7</td>
<td style="text-align: center;">520.7</td>
<td style="text-align: center;">554.6</td>
</tr>
<tr>
<td style="text-align: center;">Krull</td>
<td style="text-align: center;">3627.6</td>
<td style="text-align: center;">6015.1</td>
<td style="text-align: center;">4555.2</td>
<td style="text-align: center;">5179.2</td>
<td style="text-align: center;">6700.7</td>
<td style="text-align: center;">8169.8</td>
<td style="text-align: center;">7399.3</td>
</tr>
<tr>
<td style="text-align: center;">Kung Fu Master</td>
<td style="text-align: center;">6634.6</td>
<td style="text-align: center;">17166.1</td>
<td style="text-align: center;">12906.5</td>
<td style="text-align: center;">30568.1</td>
<td style="text-align: center;">21456.2</td>
<td style="text-align: center;">13874.7</td>
<td style="text-align: center;">18065.8</td>
</tr>
<tr>
<td style="text-align: center;">Montezuma's Revenge</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: center;">Ms. Pac-Man</td>
<td style="text-align: center;">770</td>
<td style="text-align: center;">1657.0</td>
<td style="text-align: center;">3802.7</td>
<td style="text-align: center;">4142.8</td>
<td style="text-align: center;">1558.3</td>
<td style="text-align: center;">1289.9</td>
<td style="text-align: center;">1401.6</td>
</tr>
<tr>
<td style="text-align: center;">Name This Game</td>
<td style="text-align: center;">2745.1</td>
<td style="text-align: center;">6380.2</td>
<td style="text-align: center;">4845.1</td>
<td style="text-align: center;">5532.0</td>
<td style="text-align: center;">7525.0</td>
<td style="text-align: center;">5378.5</td>
<td style="text-align: center;">5227.8</td>
</tr>
<tr>
<td style="text-align: center;">Phoenix</td>
<td style="text-align: center;">2542.5</td>
<td style="text-align: center;">5357.0</td>
<td style="text-align: center;">5334.5</td>
<td style="text-align: center;">5756.5</td>
<td style="text-align: center;">11813.3</td>
<td style="text-align: center;">5771.2</td>
<td style="text-align: center;">6046.7</td>
</tr>
<tr>
<td style="text-align: center;">Pitfall!</td>
<td style="text-align: center;">$-43.9$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-79.0$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-4.4$</td>
<td style="text-align: center;">$-1.5$</td>
</tr>
<tr>
<td style="text-align: center;">Pong</td>
<td style="text-align: center;">$-20.3$</td>
<td style="text-align: center;">$-3.2$</td>
<td style="text-align: center;">$-20.0$</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">$-18.9$</td>
<td style="text-align: center;">$-13.3$</td>
</tr>
<tr>
<td style="text-align: center;">Private Eye</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">3963.8</td>
<td style="text-align: center;">162.2</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">1230.4</td>
<td style="text-align: center;">80.2</td>
</tr>
<tr>
<td style="text-align: center;">Q*bert</td>
<td style="text-align: center;">438.9</td>
<td style="text-align: center;">2372.5</td>
<td style="text-align: center;">12500.4</td>
<td style="text-align: center;">7419.2</td>
<td style="text-align: center;">839.0</td>
<td style="text-align: center;">1812.4</td>
<td style="text-align: center;">2582.1</td>
</tr>
<tr>
<td style="text-align: center;">River Raid</td>
<td style="text-align: center;">2312.6</td>
<td style="text-align: center;">3144.9</td>
<td style="text-align: center;">4195.0</td>
<td style="text-align: center;">5498.1</td>
<td style="text-align: center;">4871.8</td>
<td style="text-align: center;">2787.1</td>
<td style="text-align: center;">2671.0</td>
</tr>
<tr>
<td style="text-align: center;">Road Runner</td>
<td style="text-align: center;">759.9</td>
<td style="text-align: center;">7285.4</td>
<td style="text-align: center;">5432.1</td>
<td style="text-align: center;">12661.4</td>
<td style="text-align: center;">24746.6</td>
<td style="text-align: center;">3133.1</td>
<td style="text-align: center;">6285.0</td>
</tr>
<tr>
<td style="text-align: center;">Robot Tank</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">9.1</td>
</tr>
<tr>
<td style="text-align: center;">Seaquest</td>
<td style="text-align: center;">514.1</td>
<td style="text-align: center;">618.7</td>
<td style="text-align: center;">711.6</td>
<td style="text-align: center;">1015.3</td>
<td style="text-align: center;">1192.2</td>
<td style="text-align: center;">611.7</td>
<td style="text-align: center;">574.3</td>
</tr>
<tr>
<td style="text-align: center;">Skiing</td>
<td style="text-align: center;">-20002.7</td>
<td style="text-align: center;">-19818.0</td>
<td style="text-align: center;">-15278.9</td>
<td style="text-align: center;">-26340.7</td>
<td style="text-align: center;">-12762.4</td>
<td style="text-align: center;">-17055.7</td>
<td style="text-align: center;">-13880.4</td>
</tr>
<tr>
<td style="text-align: center;">Solaris</td>
<td style="text-align: center;">2932.7</td>
<td style="text-align: center;">1343.0</td>
<td style="text-align: center;">8717.5</td>
<td style="text-align: center;">7201.0</td>
<td style="text-align: center;">1397.1</td>
<td style="text-align: center;">2460.0</td>
<td style="text-align: center;">3211.8</td>
</tr>
<tr>
<td style="text-align: center;">Space Invaders</td>
<td style="text-align: center;">201</td>
<td style="text-align: center;">642.2</td>
<td style="text-align: center;">2027.8</td>
<td style="text-align: center;">1016.0</td>
<td style="text-align: center;">673.0</td>
<td style="text-align: center;">545.6</td>
<td style="text-align: center;">527.9</td>
</tr>
<tr>
<td style="text-align: center;">Stargunner</td>
<td style="text-align: center;">613.6</td>
<td style="text-align: center;">604.8</td>
<td style="text-align: center;">14843.9</td>
<td style="text-align: center;">1171.4</td>
<td style="text-align: center;">1131.4</td>
<td style="text-align: center;">877.0</td>
<td style="text-align: center;">886.7</td>
</tr>
<tr>
<td style="text-align: center;">Surround</td>
<td style="text-align: center;">$-9.9$</td>
<td style="text-align: center;">$-9.7$</td>
<td style="text-align: center;">$-9.9$</td>
<td style="text-align: center;">$-7.9$</td>
<td style="text-align: center;">$-8.5$</td>
<td style="text-align: center;">$-9.8$</td>
<td style="text-align: center;">$-9.9$</td>
</tr>
<tr>
<td style="text-align: center;">Tennis</td>
<td style="text-align: center;">$-23.8$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-23.7$</td>
<td style="text-align: center;">$-1.8$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-4.3$</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Time Pilot</td>
<td style="text-align: center;">3683.5</td>
<td style="text-align: center;">1952.0</td>
<td style="text-align: center;">10751.3</td>
<td style="text-align: center;">10282.7</td>
<td style="text-align: center;">2430.2</td>
<td style="text-align: center;">2323.7</td>
<td style="text-align: center;">2576.0</td>
</tr>
<tr>
<td style="text-align: center;">Tutankham</td>
<td style="text-align: center;">108.3</td>
<td style="text-align: center;">148.7</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">121.6</td>
<td style="text-align: center;">194.0</td>
<td style="text-align: center;">108.3</td>
<td style="text-align: center;">122.4</td>
</tr>
<tr>
<td style="text-align: center;">Up'n Down</td>
<td style="text-align: center;">3322.3</td>
<td style="text-align: center;">18964.9</td>
<td style="text-align: center;">22320.8</td>
<td style="text-align: center;">39823.3</td>
<td style="text-align: center;">11856.2</td>
<td style="text-align: center;">11961.2</td>
<td style="text-align: center;">13308.4</td>
</tr>
<tr>
<td style="text-align: center;">Venture</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: center;">Video Pinball</td>
<td style="text-align: center;">30548.5</td>
<td style="text-align: center;">14316.0</td>
<td style="text-align: center;">90507.7</td>
<td style="text-align: center;">22842.6</td>
<td style="text-align: center;">24254.5</td>
<td style="text-align: center;">11507.3</td>
<td style="text-align: center;">14178.9</td>
</tr>
<tr>
<td style="text-align: center;">Wizard of Wor</td>
<td style="text-align: center;">876</td>
<td style="text-align: center;">401.4</td>
<td style="text-align: center;">12803.1</td>
<td style="text-align: center;">8480.7</td>
<td style="text-align: center;">1146.6</td>
<td style="text-align: center;">526.8</td>
<td style="text-align: center;">420.4</td>
</tr>
<tr>
<td style="text-align: center;">Yars' Revenge</td>
<td style="text-align: center;">9953</td>
<td style="text-align: center;">7614.1</td>
<td style="text-align: center;">5956.7</td>
<td style="text-align: center;">21490.5</td>
<td style="text-align: center;">9228.5</td>
<td style="text-align: center;">8884.4</td>
<td style="text-align: center;">8532.7</td>
</tr>
<tr>
<td style="text-align: center;">Zaxxon</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">200.3</td>
<td style="text-align: center;">6288.1</td>
<td style="text-align: center;">10082.4</td>
<td style="text-align: center;">3123.5</td>
<td style="text-align: center;">278.3</td>
<td style="text-align: center;">168.3</td>
</tr>
</tbody>
</table>
<p>Table 3. Scores at 10 Million Frames</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ See Pop-Art (van Hasselt et al., 2016) for a DQN-like algorithm that does not require reward-clipping. NEC also outperforms&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
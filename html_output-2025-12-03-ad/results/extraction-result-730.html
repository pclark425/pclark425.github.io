<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-730 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-730</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-730</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-256105699</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2301.09028v2.pdf" target="_blank">Characterization and Learning of Causal Graphs with Small Conditioning Sets</a></p>
                <p><strong>Paper Abstract:</strong> Constraint-based causal discovery algorithms learn part of the causal graph structure by systematically testing conditional independences observed in the data. These algorithms, such as the PC algorithm and its variants, rely on graphical characterizations of the so-called equivalence class of causal graphs proposed by Pearl. However, constraint-based causal discovery algorithms struggle when data is limited since conditional independence tests quickly lose their statistical power, especially when the conditioning set is large. To address this, we propose using conditional independence tests where the size of the conditioning set is upper bounded by some integer $k$ for robust causal discovery. The existing graphical characterizations of the equivalence classes of causal graphs are not applicable when we cannot leverage all the conditional independence statements. We first define the notion of $k$-Markov equivalence: Two causal graphs are $k$-Markov equivalent if they entail the same conditional independence constraints where the conditioning set size is upper bounded by $k$. We propose a novel representation that allows us to graphically characterize $k$-Markov equivalence between two causal graphs. We propose a sound constraint-based algorithm called the $k$-PC algorithm for learning this equivalence class. Finally, we conduct synthetic, and semi-synthetic experiments to demonstrate that the $k$-PC algorithm enables more robust causal discovery in the small sample regime compared to the baseline algorithms.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e730.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e730.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>k-PC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-PC algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sound constraint-based causal discovery algorithm that only uses conditional independence (CI) tests whose conditioning sets have size at most k, and augments FCI orientation rules with two extra rules (R11,R12) to learn invariant edge marks under this restriction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>k-PC</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Starts from a complete graph with circle marks; (1) finds separating sets S_{a,b} for each pair a,b using only conditioning subsets of size ≤ k; (2) removes edges for separable pairs to learn the skeleton and orients unshielded colliders using those separating sets; (3) runs the FCI orientation phase (R1–R10 simplified for no selection bias) and then applies two additional orientation rules R11 and R12 specific to k-closure semantics to orient tails and undirected marks where sound; the algorithm outputs the k-essential graph representing invariant features across all DAGs that entail the same degree-k CI constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic and semi-synthetic observational datasets (including Asia BN)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline observational data experiments: random synthetic DAGs with discrete CPTs and some linear SCMs; semi-synthetic experiment on the 8-variable Asia Bayesian network with 500 samples. These are not open-ended or interactive/active experimental environments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>addresses spurious CI conclusions caused by unreliable conditional independence tests from small samples (finite-sample statistical noise) and the combinatorial unreliability of large conditioning sets.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects unreliable/undesirable CI tests implicitly by rejecting all CI tests whose conditioning set size would exceed k; identifies k-covered pairs (pairs not separable by any conditioning set ≤ k) and uses that structure to avoid over-conditioning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Uses graphical k-closure construction and k-essential graph (edge-union of k-closures) plus orientation rules R11/R12 to rule out orientations inconsistent with any k-closure in the Markov equivalence class; thus spurious CI-derived orientations that require large conditioning sets are effectively excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitatively: substantially improved arrowhead F1 (better orientation of arrowheads) and improved skeleton discovery in low-sample regimes (e.g., tens to a few hundred samples); advantages diminish when sample size is large (>~500) since full PC becomes reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline PC (which uses larger conditioning sets) produced much sparser graphs and worse arrowhead F1 in the small-sample regime; NOTEARS slightly outperformed PC in some settings but k-PC still outperformed both in low-sample regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Restricting CI tests to conditioning sets of size ≤ k (k-PC) improves robustness to finite-sample errors: it reduces orientation mistakes caused by unreliable large-condition CI tests and yields higher arrowhead and skeleton F1 in the small-sample regime; additional orientation rules (R11,R12) exploit k-closure properties to recover extra invariant tail/orientation information compared to AnytimeFCI/LOCI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterization and Learning of Causal Graphs with Small Conditioning Sets', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e730.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e730.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>k-closure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-closure graph (C_k(D))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixed-graph representation (directed and bidirected edges) constructed from a DAG D that makes adjacent precisely those pairs that are not separable by any conditioning set of size ≤ k, with edge marks reflecting ancestrality in D.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>k-closure graph construction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given DAG D and integer k, add an undirected/bidirected/directed adjacency between every pair that is k-covered (no separation by sets ≤ k); orient k-covered pairs according to ancestrality in the original DAG (→ if ancestor, ↔ if neither ancestor, etc.); the resulting C_k(D) is a maximal ancestral graph (MAG) that entails the same degree-k d-separation relations as D.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Theoretical / applied to offline datasets (synthetic and semi-synthetic used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not an interactive environment; used as a representational object to characterize what is learnable from degree-k CI constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Designed to eliminate spurious CI claims that would arise only when conditioning on large sets; represents invariances robust to small-conditioning CI testing.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Operationalized by enumerating conditioning sets up to size k to determine which pairs are k-covered versus separable; k-covered status detects pairs that cannot be made independent with limited-size conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By construction: if a conditional independence would require conditioning > k, the corresponding non-adjacency is not represented in C_k(D), effectively refuting any orientation relying on such high-cardinality conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>k-closure graphs are MAGs that exactly capture the degree-k d-separation relations of the generating DAG; they form the basis for the k-essential graph and enable sound learning when limiting CI conditioning set size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterization and Learning of Causal Graphs with Small Conditioning Sets', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e730.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e730.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>k-essential</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-essential graph (ε_k(D))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The edge-union of all k-closure graphs Markov-equivalent to C_k(D), representing the invariant tails/arrowheads and circles across DAGs that share the same degree-k CI relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>k-essential graph</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Take the union over all k-closure graphs in the Markov equivalence class: preserve an arrowhead or tail only if it appears in every Markov-equivalent k-closure; variant marks (head/tail) across equivalent graphs are replaced with circles, while invariant arrowheads/tails are kept.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Theoretical / used in offline experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Represents the best causal information identifiable under degree-k CI constraints; used to evaluate k-PC outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Helps avoid spurious causal claims that depend on high-order CI tests by only encoding invariants under degree-k constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Computed by enumerating Markov-equivalent k-closures and taking edge-union; uses degree-k CI constraints to decide invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Any orientation not present in every Markov-equivalent k-closure is marked with circle (uncertain) rather than committed—this effectively refuses to accept orientations that would rely on unreliable (large-conditioning) CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>k-essential graph is a more informative representation than the graph union used by LOCI or the PAG from AnytimeFCI in the causally sufficient setting, because it leverages the restriction to degree-k CI tests to recover extra invariant arrowheads/tails.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterization and Learning of Causal Graphs with Small Conditioning Sets', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e730.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e730.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Degree-k CI constraint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Degree-k conditional independence constraints (bounded conditioning sets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodological restriction to only consider conditional independence statements whose conditioning sets have cardinality at most k, used to improve robustness of constraint-based discovery in small samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>degree-k CI constraints</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Restrict CI testing to subsets with |S| ≤ k; define k-covered pairs as those that cannot be separated by any such S; base the learning and graphical characterization (k-closure, k-essential) on these degree-k relations to avoid noisy/unreliable high-order CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline synthetic and semi-synthetic datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not interactive; intended as a robustness device for small-sample observational settings.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Targets spurious CI judgments due to high-dimensional conditioning (finite-sample variance and lack of statistical power).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>By construction: a pair is considered separable only if a separating set of size ≤ k is found via enumeration/testing; otherwise considered k-covered.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Prevents accepting CI claims that would rely on large conditioning sets (which are more prone to spurious conclusions); thus spurious orientations that depend on such CI tests are not committed to.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using only degree-k CI constraints reduces errors from unreliable high-order CI tests and improves practical performance (arrowhead/skeleton F1) in low-sample regimes; the optimal k depends on sample size (larger k as more data becomes available).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterization and Learning of Causal Graphs with Small Conditioning Sets', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e730.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e730.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R11/R12</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Orientation rules R11 and R12 (k-PC augmentations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two additional orientation rules added to the FCI orientation phase in k-PC to exploit k-closure structural properties and orient tails/circles that FCI alone would leave ambiguous under degree-k CI information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>R11 and R12 orientation rules</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>R11: if a node a has no incoming edges and there is a partition of neighbors into B (ao→ b) and C (ao-oc) and a subset B* of B that are non-adjacent to all nodes in C, orient edges a o→ b as a → b for b in B*; R12: similar rule to orient a o-oc into a - c (undirected) when C* meets criteria (nodes in C non-adjacent to other C nodes). These rules use non-adjacency relations among neighbor sets to infer invariance from k-closure constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied within k-PC on offline datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Used after running FCI orientation on the PAG(C_k(D)) output to further orient edges permitted by k-closure structure.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Rule-based refutation of orientations that would be inconsistent with any k-closure, thereby avoiding spurious orientations derived from noisy CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects candidate orientations by examining neighbor partitions (B,C) around nodes with no incoming edges and checking non-adjacency constraints among those neighbors under the current PAG; uses existence/non-existence of adjacencies to infer invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Guarantees soundness: if an orientation would contradict the existence of some k-closure consistent with the observed degree-k CI, the rules prevent that orientation (i.e., they refute spurious arrows by reasoning about k-coveredness).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>R11/R12 provide extra orientation power beyond FCI when learning k-essential graphs; they are sound (provably) and allow k-PC to recover orientations that AnytimeFCI/FCI would leave ambiguous under degree-k CI constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterization and Learning of Causal Graphs with Small Conditioning Sets', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e730.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e730.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOCI (low-order CI graph union)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing method that learns the union of all causal graphs consistent with conditional independence statements up to a fixed conditioning-set cardinality; used as a close baseline and point of comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recovering causal structures from low-order conditional independencies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LOCI</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Enumerates/uses low-order (bounded-size) CI statements to infer an edge-union representation of all DAGs consistent with those statements; orients some edges based on CI patterns but is less expressive than k-essential graphs in the causally sufficient case.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Relevant prior work / baseline comparisons in synthetic experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not an interactive environment; designed for offline observational data using low-order CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Aims at the same issue: reduces reliance on high-order CI tests that are error-prone in small samples.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses CI pattern templates on degree-k CI statements to infer adjacencies and some orientations.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Forms a conservative union over DAGs consistent with low-order CI, so it avoids committing to orientations that are not supported by low-order CI; however, k-PC can produce strictly richer information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>LOCI performs similarly to k-PC on many settings; k-PC showed better arrowhead and skeleton F1 in experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LOCI and k-PC both leverage low-order CI statements; k-PC (via k-closures and R11/R12) can recover strictly more orientation information than LOCI in causally sufficient settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterization and Learning of Causal Graphs with Small Conditioning Sets', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e730.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e730.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnytimeFCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anytime FCI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early-stopped variant of the FCI algorithm that can be terminated after testing CI up to a given conditioning set size, producing sound but partial PAG output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An anytime algorithm for causal inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AnytimeFCI</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Run FCI but stop searching for separating sets once all conditioning subsets up to a given size k have been exhausted; produces a PAG that is sound for the d-separations that were tested but may be less informative than a k-essential graph in causally sufficient settings.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Related work / baseline conceptual comparator</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline observational datasets; applicable where full FCI is too expensive and low-order CI tests are preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Addresses spurious CI conclusions resulting from over-conditioning by limiting search to small conditioning sets.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit via stopping criterion on conditioning-set size.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Produces a PAG consistent with tested low-order CI; k-PC can refine this to a k-essential graph in causally sufficient systems.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AnytimeFCI is sound but not as informative as k-PC for causally sufficient systems; k-PC can be seen as an improved version for that setting because it leverages k-closure structure to extract more invariant orientations without exhaustive high-order CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterization and Learning of Causal Graphs with Small Conditioning Sets', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e730.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e730.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PC algorithm (and stable / conservative variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical constraint-based causal discovery algorithm that progressively tests conditional independences and orients edges to produce an essential graph; sensitive to CI test errors when sample size is small and conditioning sets become large.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PC algorithm (and stable/conservative variants)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Progressively removes edges by finding separating sets (potentially of unbounded size), then orients unshielded colliders and applies Meek rules to orient further edges; sequential nature causes propagation of CI test errors into many incorrect orientations in small-sample regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic and semi-synthetic offline datasets used as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not interactive; standard observational causal discovery benchmark settings.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Prone to spurious orientations caused by unreliable CI tests with large conditioning sets (finite-sample noise).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>N/A (classical CI testing, no special detection for spurious signals beyond statistical tests themselves).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>In experiments: PC often produced very sparse graphs (low sensitivity) in small-sample regimes due to over-conditioning and loss of statistical power; stable/conservative variants alleviate some ordering dependence but do not remove fundamental finite-sample CI unreliability.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PC is consistent in the large-sample limit but fragile in small-sample regimes; limiting conditioning set size (k-PC) mitigates many of these practical failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterization and Learning of Causal Graphs with Small Conditioning Sets', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e730.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e730.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NOTEARS (continuous optimization for DAG learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A score-based continuous optimization approach that enforces acyclicity via a smooth constraint (trace/exponential characterization) to learn DAG structure via gradient-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dags with no tears: Continuous optimization for structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formulates DAG learning as continuous optimization with an acyclicity constraint transformed into a differentiable function; optimizes a regularized score (e.g., likelihood) via gradient-based methods to produce a weighted adjacency then thresholds to obtain a DAG.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Compared as a baseline in synthetic linear SCM experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline observational datasets (linear SCM synthetic data); not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Does not explicitly target distractors; uses continuous regularization and score-based criteria which can be more stable than naive CI testing in some small-sample regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit via regularization in the score objective (penalizes complex graphs), but no specific mechanism for distractor downweighting discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In the paper's linear SCM experiments, NOTEARS performed slightly better than PC but k-PC outperformed both in the small-sample regime.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Score-based continuous methods (NOTEARS) can be competitive, but bounding CI conditioning (k-PC) has advantages in very small-sample regimes for discrete SCM experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterization and Learning of Causal Graphs with Small Conditioning Sets', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recovering causal structures from low-order conditional independencies <em>(Rating: 2)</em></li>
                <li>An anytime algorithm for causal inference <em>(Rating: 2)</em></li>
                <li>Improving the reliability of causal discovery from small data sets using argumentation <em>(Rating: 2)</em></li>
                <li>DAGs with no tears: Continuous optimization for structure learning <em>(Rating: 1)</em></li>
                <li>Learning high-dimensional directed acyclic graphs with latent and selection variables <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-730",
    "paper_id": "paper-256105699",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "k-PC",
            "name_full": "k-PC algorithm",
            "brief_description": "A sound constraint-based causal discovery algorithm that only uses conditional independence (CI) tests whose conditioning sets have size at most k, and augments FCI orientation rules with two extra rules (R11,R12) to learn invariant edge marks under this restriction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "k-PC",
            "method_description": "Starts from a complete graph with circle marks; (1) finds separating sets S_{a,b} for each pair a,b using only conditioning subsets of size ≤ k; (2) removes edges for separable pairs to learn the skeleton and orients unshielded colliders using those separating sets; (3) runs the FCI orientation phase (R1–R10 simplified for no selection bias) and then applies two additional orientation rules R11 and R12 specific to k-closure semantics to orient tails and undirected marks where sound; the algorithm outputs the k-essential graph representing invariant features across all DAGs that entail the same degree-k CI constraints.",
            "environment_name": "Synthetic and semi-synthetic observational datasets (including Asia BN)",
            "environment_description": "Offline observational data experiments: random synthetic DAGs with discrete CPTs and some linear SCMs; semi-synthetic experiment on the 8-variable Asia Bayesian network with 500 samples. These are not open-ended or interactive/active experimental environments.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "addresses spurious CI conclusions caused by unreliable conditional independence tests from small samples (finite-sample statistical noise) and the combinatorial unreliability of large conditioning sets.",
            "detection_method": "Detects unreliable/undesirable CI tests implicitly by rejecting all CI tests whose conditioning set size would exceed k; identifies k-covered pairs (pairs not separable by any conditioning set ≤ k) and uses that structure to avoid over-conditioning errors.",
            "downweighting_method": null,
            "refutation_method": "Uses graphical k-closure construction and k-essential graph (edge-union of k-closures) plus orientation rules R11/R12 to rule out orientations inconsistent with any k-closure in the Markov equivalence class; thus spurious CI-derived orientations that require large conditioning sets are effectively excluded.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitatively: substantially improved arrowhead F1 (better orientation of arrowheads) and improved skeleton discovery in low-sample regimes (e.g., tens to a few hundred samples); advantages diminish when sample size is large (&gt;~500) since full PC becomes reliable.",
            "performance_without_robustness": "Baseline PC (which uses larger conditioning sets) produced much sparser graphs and worse arrowhead F1 in the small-sample regime; NOTEARS slightly outperformed PC in some settings but k-PC still outperformed both in low-sample regimes.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Restricting CI tests to conditioning sets of size ≤ k (k-PC) improves robustness to finite-sample errors: it reduces orientation mistakes caused by unreliable large-condition CI tests and yields higher arrowhead and skeleton F1 in the small-sample regime; additional orientation rules (R11,R12) exploit k-closure properties to recover extra invariant tail/orientation information compared to AnytimeFCI/LOCI.",
            "uuid": "e730.0",
            "source_info": {
                "paper_title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "k-closure",
            "name_full": "k-closure graph (C_k(D))",
            "brief_description": "A mixed-graph representation (directed and bidirected edges) constructed from a DAG D that makes adjacent precisely those pairs that are not separable by any conditioning set of size ≤ k, with edge marks reflecting ancestrality in D.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "k-closure graph construction",
            "method_description": "Given DAG D and integer k, add an undirected/bidirected/directed adjacency between every pair that is k-covered (no separation by sets ≤ k); orient k-covered pairs according to ancestrality in the original DAG (→ if ancestor, ↔ if neither ancestor, etc.); the resulting C_k(D) is a maximal ancestral graph (MAG) that entails the same degree-k d-separation relations as D.",
            "environment_name": "Theoretical / applied to offline datasets (synthetic and semi-synthetic used in experiments)",
            "environment_description": "Not an interactive environment; used as a representational object to characterize what is learnable from degree-k CI constraints.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Designed to eliminate spurious CI claims that would arise only when conditioning on large sets; represents invariances robust to small-conditioning CI testing.",
            "detection_method": "Operationalized by enumerating conditioning sets up to size k to determine which pairs are k-covered versus separable; k-covered status detects pairs that cannot be made independent with limited-size conditioning.",
            "downweighting_method": null,
            "refutation_method": "By construction: if a conditional independence would require conditioning &gt; k, the corresponding non-adjacency is not represented in C_k(D), effectively refuting any orientation relying on such high-cardinality conditioning.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "k-closure graphs are MAGs that exactly capture the degree-k d-separation relations of the generating DAG; they form the basis for the k-essential graph and enable sound learning when limiting CI conditioning set size.",
            "uuid": "e730.1",
            "source_info": {
                "paper_title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "k-essential",
            "name_full": "k-essential graph (ε_k(D))",
            "brief_description": "The edge-union of all k-closure graphs Markov-equivalent to C_k(D), representing the invariant tails/arrowheads and circles across DAGs that share the same degree-k CI relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "k-essential graph",
            "method_description": "Take the union over all k-closure graphs in the Markov equivalence class: preserve an arrowhead or tail only if it appears in every Markov-equivalent k-closure; variant marks (head/tail) across equivalent graphs are replaced with circles, while invariant arrowheads/tails are kept.",
            "environment_name": "Theoretical / used in offline experiments",
            "environment_description": "Represents the best causal information identifiable under degree-k CI constraints; used to evaluate k-PC outputs.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Helps avoid spurious causal claims that depend on high-order CI tests by only encoding invariants under degree-k constraints.",
            "detection_method": "Computed by enumerating Markov-equivalent k-closures and taking edge-union; uses degree-k CI constraints to decide invariance.",
            "downweighting_method": null,
            "refutation_method": "Any orientation not present in every Markov-equivalent k-closure is marked with circle (uncertain) rather than committed—this effectively refuses to accept orientations that would rely on unreliable (large-conditioning) CI tests.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "k-essential graph is a more informative representation than the graph union used by LOCI or the PAG from AnytimeFCI in the causally sufficient setting, because it leverages the restriction to degree-k CI tests to recover extra invariant arrowheads/tails.",
            "uuid": "e730.2",
            "source_info": {
                "paper_title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Degree-k CI constraint",
            "name_full": "Degree-k conditional independence constraints (bounded conditioning sets)",
            "brief_description": "A methodological restriction to only consider conditional independence statements whose conditioning sets have cardinality at most k, used to improve robustness of constraint-based discovery in small samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "degree-k CI constraints",
            "method_description": "Restrict CI testing to subsets with |S| ≤ k; define k-covered pairs as those that cannot be separated by any such S; base the learning and graphical characterization (k-closure, k-essential) on these degree-k relations to avoid noisy/unreliable high-order CI tests.",
            "environment_name": "Offline synthetic and semi-synthetic datasets",
            "environment_description": "Not interactive; intended as a robustness device for small-sample observational settings.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Targets spurious CI judgments due to high-dimensional conditioning (finite-sample variance and lack of statistical power).",
            "detection_method": "By construction: a pair is considered separable only if a separating set of size ≤ k is found via enumeration/testing; otherwise considered k-covered.",
            "downweighting_method": null,
            "refutation_method": "Prevents accepting CI claims that would rely on large conditioning sets (which are more prone to spurious conclusions); thus spurious orientations that depend on such CI tests are not committed to.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Using only degree-k CI constraints reduces errors from unreliable high-order CI tests and improves practical performance (arrowhead/skeleton F1) in low-sample regimes; the optimal k depends on sample size (larger k as more data becomes available).",
            "uuid": "e730.3",
            "source_info": {
                "paper_title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "R11/R12",
            "name_full": "Orientation rules R11 and R12 (k-PC augmentations)",
            "brief_description": "Two additional orientation rules added to the FCI orientation phase in k-PC to exploit k-closure structural properties and orient tails/circles that FCI alone would leave ambiguous under degree-k CI information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "R11 and R12 orientation rules",
            "method_description": "R11: if a node a has no incoming edges and there is a partition of neighbors into B (ao→ b) and C (ao-oc) and a subset B* of B that are non-adjacent to all nodes in C, orient edges a o→ b as a → b for b in B*; R12: similar rule to orient a o-oc into a - c (undirected) when C* meets criteria (nodes in C non-adjacent to other C nodes). These rules use non-adjacency relations among neighbor sets to infer invariance from k-closure constraints.",
            "environment_name": "Applied within k-PC on offline datasets",
            "environment_description": "Used after running FCI orientation on the PAG(C_k(D)) output to further orient edges permitted by k-closure structure.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Rule-based refutation of orientations that would be inconsistent with any k-closure, thereby avoiding spurious orientations derived from noisy CI tests.",
            "detection_method": "Detects candidate orientations by examining neighbor partitions (B,C) around nodes with no incoming edges and checking non-adjacency constraints among those neighbors under the current PAG; uses existence/non-existence of adjacencies to infer invariance.",
            "downweighting_method": null,
            "refutation_method": "Guarantees soundness: if an orientation would contradict the existence of some k-closure consistent with the observed degree-k CI, the rules prevent that orientation (i.e., they refute spurious arrows by reasoning about k-coveredness).",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "R11/R12 provide extra orientation power beyond FCI when learning k-essential graphs; they are sound (provably) and allow k-PC to recover orientations that AnytimeFCI/FCI would leave ambiguous under degree-k CI constraints.",
            "uuid": "e730.4",
            "source_info": {
                "paper_title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "LOCI",
            "name_full": "LOCI (low-order CI graph union)",
            "brief_description": "An existing method that learns the union of all causal graphs consistent with conditional independence statements up to a fixed conditioning-set cardinality; used as a close baseline and point of comparison.",
            "citation_title": "Recovering causal structures from low-order conditional independencies",
            "mention_or_use": "mention",
            "method_name": "LOCI",
            "method_description": "Enumerates/uses low-order (bounded-size) CI statements to infer an edge-union representation of all DAGs consistent with those statements; orients some edges based on CI patterns but is less expressive than k-essential graphs in the causally sufficient case.",
            "environment_name": "Relevant prior work / baseline comparisons in synthetic experiments",
            "environment_description": "Not an interactive environment; designed for offline observational data using low-order CI tests.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Aims at the same issue: reduces reliance on high-order CI tests that are error-prone in small samples.",
            "detection_method": "Uses CI pattern templates on degree-k CI statements to infer adjacencies and some orientations.",
            "downweighting_method": null,
            "refutation_method": "Forms a conservative union over DAGs consistent with low-order CI, so it avoids committing to orientations that are not supported by low-order CI; however, k-PC can produce strictly richer information.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "LOCI performs similarly to k-PC on many settings; k-PC showed better arrowhead and skeleton F1 in experiments reported.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "LOCI and k-PC both leverage low-order CI statements; k-PC (via k-closures and R11/R12) can recover strictly more orientation information than LOCI in causally sufficient settings.",
            "uuid": "e730.5",
            "source_info": {
                "paper_title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "AnytimeFCI",
            "name_full": "Anytime FCI",
            "brief_description": "An early-stopped variant of the FCI algorithm that can be terminated after testing CI up to a given conditioning set size, producing sound but partial PAG output.",
            "citation_title": "An anytime algorithm for causal inference",
            "mention_or_use": "mention",
            "method_name": "AnytimeFCI",
            "method_description": "Run FCI but stop searching for separating sets once all conditioning subsets up to a given size k have been exhausted; produces a PAG that is sound for the d-separations that were tested but may be less informative than a k-essential graph in causally sufficient settings.",
            "environment_name": "Related work / baseline conceptual comparator",
            "environment_description": "Offline observational datasets; applicable where full FCI is too expensive and low-order CI tests are preferred.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Addresses spurious CI conclusions resulting from over-conditioning by limiting search to small conditioning sets.",
            "detection_method": "Implicit via stopping criterion on conditioning-set size.",
            "downweighting_method": null,
            "refutation_method": "Produces a PAG consistent with tested low-order CI; k-PC can refine this to a k-essential graph in causally sufficient systems.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "AnytimeFCI is sound but not as informative as k-PC for causally sufficient systems; k-PC can be seen as an improved version for that setting because it leverages k-closure structure to extract more invariant orientations without exhaustive high-order CI tests.",
            "uuid": "e730.6",
            "source_info": {
                "paper_title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "PC",
            "name_full": "PC algorithm (and stable / conservative variants)",
            "brief_description": "Classical constraint-based causal discovery algorithm that progressively tests conditional independences and orients edges to produce an essential graph; sensitive to CI test errors when sample size is small and conditioning sets become large.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "PC algorithm (and stable/conservative variants)",
            "method_description": "Progressively removes edges by finding separating sets (potentially of unbounded size), then orients unshielded colliders and applies Meek rules to orient further edges; sequential nature causes propagation of CI test errors into many incorrect orientations in small-sample regimes.",
            "environment_name": "Synthetic and semi-synthetic offline datasets used as baseline",
            "environment_description": "Not interactive; standard observational causal discovery benchmark settings.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Prone to spurious orientations caused by unreliable CI tests with large conditioning sets (finite-sample noise).",
            "detection_method": "N/A (classical CI testing, no special detection for spurious signals beyond statistical tests themselves).",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": "In experiments: PC often produced very sparse graphs (low sensitivity) in small-sample regimes due to over-conditioning and loss of statistical power; stable/conservative variants alleviate some ordering dependence but do not remove fundamental finite-sample CI unreliability.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "PC is consistent in the large-sample limit but fragile in small-sample regimes; limiting conditioning set size (k-PC) mitigates many of these practical failures.",
            "uuid": "e730.7",
            "source_info": {
                "paper_title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "NOTEARS",
            "name_full": "NOTEARS (continuous optimization for DAG learning)",
            "brief_description": "A score-based continuous optimization approach that enforces acyclicity via a smooth constraint (trace/exponential characterization) to learn DAG structure via gradient-based optimization.",
            "citation_title": "Dags with no tears: Continuous optimization for structure learning",
            "mention_or_use": "mention",
            "method_name": "NOTEARS",
            "method_description": "Formulates DAG learning as continuous optimization with an acyclicity constraint transformed into a differentiable function; optimizes a regularized score (e.g., likelihood) via gradient-based methods to produce a weighted adjacency then thresholds to obtain a DAG.",
            "environment_name": "Compared as a baseline in synthetic linear SCM experiments",
            "environment_description": "Offline observational datasets (linear SCM synthetic data); not interactive.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Does not explicitly target distractors; uses continuous regularization and score-based criteria which can be more stable than naive CI testing in some small-sample regimes.",
            "detection_method": null,
            "downweighting_method": "Implicit via regularization in the score objective (penalizes complex graphs), but no specific mechanism for distractor downweighting discussed in this paper.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "In the paper's linear SCM experiments, NOTEARS performed slightly better than PC but k-PC outperformed both in the small-sample regime.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Score-based continuous methods (NOTEARS) can be competitive, but bounding CI conditioning (k-PC) has advantages in very small-sample regimes for discrete SCM experiments reported here.",
            "uuid": "e730.8",
            "source_info": {
                "paper_title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recovering causal structures from low-order conditional independencies",
            "rating": 2,
            "sanitized_title": "recovering_causal_structures_from_loworder_conditional_independencies"
        },
        {
            "paper_title": "An anytime algorithm for causal inference",
            "rating": 2,
            "sanitized_title": "an_anytime_algorithm_for_causal_inference"
        },
        {
            "paper_title": "Improving the reliability of causal discovery from small data sets using argumentation",
            "rating": 2,
            "sanitized_title": "improving_the_reliability_of_causal_discovery_from_small_data_sets_using_argumentation"
        },
        {
            "paper_title": "DAGs with no tears: Continuous optimization for structure learning",
            "rating": 1,
            "sanitized_title": "dags_with_no_tears_continuous_optimization_for_structure_learning"
        },
        {
            "paper_title": "Learning high-dimensional directed acyclic graphs with latent and selection variables",
            "rating": 2,
            "sanitized_title": "learning_highdimensional_directed_acyclic_graphs_with_latent_and_selection_variables"
        }
    ],
    "cost": 0.020382499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Characterization and Learning of Causal Graphs with Small Conditioning Sets
October 31, 2023</p>
<p>Murat Kocaoglu 
Purdue University</p>
<p>Characterization and Learning of Causal Graphs with Small Conditioning Sets
October 31, 20236177B51BCA477F0164FAAB241B5F9558arXiv:2301.09028v2[cs.AI]
Constraint-based causal discovery algorithms learn part of the causal graph structure by systematically testing conditional independences observed in the data.These algorithms, such as the PC algorithm and its variants, rely on graphical characterizations of the so-called equivalence class of causal graphs proposed by Pearl.However, constraint-based causal discovery algorithms struggle when data is limited since conditional independence tests quickly lose their statistical power, especially when the conditioning set is large.To address this, we propose using conditional independence tests where the size of the conditioning set is upper bounded by some integer k for robust causal discovery.The existing graphical characterizations of the equivalence classes of causal graphs are not applicable when we cannot leverage all the conditional independence statements.We first define the notion of k-Markov equivalence: Two causal graphs are k-Markov equivalent if they entail the same conditional independence constraints where the conditioning set size is upper bounded by k.We propose a novel representation that allows us to graphically characterize k-Markov equivalence between two causal graphs.We propose a sound constraint-based algorithm called the k-PC algorithm for learning this equivalence class.Finally, we conduct synthetic, and semi-synthetic experiments to demonstrate that the k-PC algorithm enables more robust causal discovery in the small sample regime compared to the baseline algorithms.</p>
<p>Introduction</p>
<p>Causal reasoning is a critical tool for machine learning and artificial intelligence research with benefits ranging from domain adaptation to planning, explainability and fairness [27,12,26].Estimating the effect of an action or an intervention from observational data is called causal inference.A very rich literature of causal inference algorithms have been developed to address this task in the literature [11,21,17,1].The function that is used to write an interventional distribution in terms of the observational distribution is called the estimand.Estimand depends on the causal relations between the system variables, which are represented in the form of a directed acyclic graph (DAG) called the causal graph.Thus, causal graph is required for solving most causal inference problems.</p>
<p>For small and well-studied systems, it might be possible to construct a causal graph using expert knowledge.However, in modern complex systems with changing structure, we need to learn causal graphs from data.This is called causal discovery.In most domains, we have access to plenty of observational data, but no interventional data.An important task then is to understand how much causal knowledge we can extract from observational data about a system.</p>
<p>The classical approach for addressing this problem is to use the conditional independence (CI) relations in the data to narrow down the space of plausible causal graphs [19,9].These are called constraint-based methods.Even though the number of causal graphs that entail the same set of conditional independence relations are typically exponentially many, we can use a graphical characterization of equivalence between causal graphs to compactly represent all of them using a single mixed graph called the essential graph.This notion of equivalence is called Markov equivalence.</p>
<p>Even though such causal discovery algorithms are consistent, i.e., they output the correct essential graph in the large sample limit, in practice, they struggle due to finite number of samples since not all CI tests can be performed accurately [16].For constraint-based algorithms, however, it is important for every test to be accurate since previously learned edge orientations are used to orient more edges due to their sequential nature.Thus, they may output very different causal graphs compared to the ground truth with just a few incorrect CI statements.Despite the efforts to stabilize PC output [2,5], this fundamental issue still causes problems today.Furthermore, the existing Markov equivalence class characterization and causal discovery algorithms that rely on this characterization, such as the PC/IC algorithms [19,22], require access to every CI relation, which is a significant practical limitation for causal discovery from data.</p>
<p>There are several alternatives to constraint-based causal discovery.For example, score-based approaches, such as GES [3,4] optimize a regularized score function by greedily searching over the space of DAGs to output a graph within the Markov equivalence class.A line of works, such as NOTEARS [28] converts the graph learning problem to a continuous optimization problem by converting the acyclicity constraint to a continuous constraint via trace formulation.Note that our goal in this paper is not to beat the state-of-the-art causal discovery algorithm, but provide a theoretical basis to characterize what is learnable on a fundamental level by using conditional independence tests with restricted cardinality conditioning sets.</p>
<p>Variations of the causal discovery problem with limited-size conditioning sets have been considered in the literature.The special case of marginal dependence is considered in [13] and [20].The most related existing work is [23], where the authors aim learning the graph union of all causal graphs that are consistent with a set of conditional independence statements up to a fixed cardinality conditioning set.They propose a concise algorithm that modifies the steps of PC which they show recovers the union of all equivalent graphs.Similarly in the case with latent variables, AnytimeFCI [18] shows that one can stop FCI algorithm after exhausting all conditional independence tests up to a fixed cardinality conditioning set and the output of the algorithm is still sound for learning parts of the partial ancestral graph (PAG).We propose an alternative route: First, we formally define the equivalence class of causal graphs and propose a graphical object to capture this equivalence class called the k-closure graphs.We identify a necessary and sufficient graphical condition to test equivalence between k-closure graphs.Finally, we develop a learning algorithm that leverages the representative power of partial ancestral graphs (PAGs), which are typically used in the case with latents, to obtain a provably finer characterization of the set of equivalent causal structures than [23].</p>
<p>In this paper, we propose learning causal graphs using CI tests with bounded conditioning set size.This allows us to ignore CI tests that become unreliable with limited data, and avoid some of the mistakes made by constraint-based causal discovery algorithms.We call CI constraints where the conditioning set size is upper bounded by k as the degree-k CI constraints.We call two causal graphs k-Markov equivalent if they entail the same degree-k CI constraints.We propose k-closure graphs that entail the same degree-k CI relations as the causal graph, and show that we can characterize k-Markov equivalence via Markov equivalence between k-closure graphs.We then propose a constraint-based learning algorithm for learning this equivalence class from data, represented by the so-called k-essential graph.Finally, we demonstrate that our algorithm can help alleviate the finite-sample issues faced by the PC algorithm, especially while orienting arrowhead marks and for correctly learning the adjacencies in the graph.We also compare our algorithm with NOTEARS [28] and LOCI [23].</p>
<p>Background</p>
<p>In this section, we give some basic graph terminology as well as background on constraint-based causal discovery: An(x) represents the set of ancestors of x in a given graph that will be clear from context.De(x) similarly represents the descendants of x.N e(x) represents the neighbors of x, i.e., the nodes that are adjacent to x. Definition 2.1.A path in a causal graph is a sequence of nodes where every pair of consecutive nodes are adjacent in the graph and no node appears more than once.Definition 2.2 (Skeleton).The skeleton of a causal graph D is the undirected graph obtained by making every adjacent pair connected via an undirected edge.These examples show that different structures may induce the same degree-k d-separation relations.One might think that, if a collider actually changes the k-Markov equivalence class, then perhaps there is hope that a local characterization around all such equivalence class-changing colliders might be possible.However, this is not true.Our example in Section D.1 shows that a local characterization similar to Theorem 2.10 is not possible for k-Markov equivalence.</p>
<p>k-closure Graphs</p>
<p>Our goal is to come up with a graphical representation of k-Markov equivalence class that captures the invariant causal information across equivalent DAGs.This will later be useful for learning identifiable parts of the causal structure from data by CI tests.First, we introduce the notion of a k-covered pair.Therefore, k-covered pairs are pairs of variables that cannot be made independent by conditioning on subsets of size at most k.For example, a, b in Figure 1 are k-covered for k = 1.In any graphical representation of k-Markov equivalence class, k-covered pairs should be adjacent.This is because it is not always possible to distinguish if they are adjacent or not by degree-k d-separation relations.</p>
<p>We propose k-closure graphs as a useful representation to characterize k-Markov equivalence class.</p>
<p>Definition 3.4.Given a DAG D = (V, E) and an integer k, the k-closure of D is defined as the graph shown by C k (D) that satisfies the following:
1. If: a, b are k-covered in D i) if a ∈ An(b), then a → b in C k (D), ii) if b ∈ An(a), then a ← b in C k (D), iii) else a ↔ b in C k (D) 2. Else: a, b are non-adjacent in C k (D)
The definition of k-closure trivially implies the following: Furthermore, there is a straightforward algorithm one can employ to recover a k-closure from the DAG: Make the non-adjacent pairs that cannot be d-separated with conditioning sets of size at most k adjacent according to the definition.Even though this may not be a poly-time operation, it is unavoidable to characterize the k-Markov equivalence class.</p>
<p>The following lemma shows that k-closure graphs can be used to capture all the conditional independence statements with bounded-size conditioning sets imposed by a DAG.Due to bidirected edges, k-closure graphs are not DAGs.However, we can show that they are still acyclic.In fact, it is worth noting that k-closure graphs are a special class of ancestral graphs [15].Lemma 3.8.For any DAG D, the k-closure graph C k (D) is a maximal ancestral graph (MAG).</p>
<p>MAGs have been successfully applied for learning causal graphs with latent variables [15].Similar to MAGs, k-closure graphs are simply graph objects to help us compactly represent the k-Markov equivalence class of causal DAGs, rather than expressing the underlying physical system directly.They do, however, represent ancestrality relations between variables by construction.</p>
<p>The relation between k-closure graphs and MAGs is not an if and only if relation.In a MAG, one can have a bidirected edge between any pair of nodes as long as it does not create a (almost) directed cycle.This is because they represent latent confounders and one might have latent confounders between any pair of observed nodes.However, bidirected edges in k-closure graphs represent k-covered pairs and cannot be added arbitrarily.Accordingly, there are MAGs that are not valid k-closure graphs.An example is given in Section D.2.We have the following characterization: Theorem 3.9.A mixed graph K = (V, E) is a k-closure graph if and only if it is a maximal ancestral graph and for any bidirected edge a ↔ b ∈ E the following is true:
• ∄c ⊂ V : |c| ≤ k, (a ⊥ ⊥ b |c ) K ′ , where K ′ = (V, E − {a ↔ b}).
The Markov equivalence between MAGs has been characterized in the literature.This relies on not only skeletons and unshielded colliders, but also colliders on discriminating paths being identical.Definition 3.10.A path p = ⟨a, z 1 , . . .z m , u, Y, v⟩ is called a discriminating path for u, Y, v if a, v are not adjacent and every vertex {z i } i , u are colliders on p and parents of v. Theorem 3.11.[ [15]] Two MAGs M 1 , M 2 are Markov equivalent if and only if i) They have the same skeleton, ii) They have the same unshielded colliders, iii) For any node Y for which there is a discriminating path p, Y has the same collider status on p in M 1 , M 2 .</p>
<p>One important observation for our characterization is that Markov equivalence of k-closure graphs do not rely on discriminating paths unlike arbitrary ancestral graphs.Lemma 3.12.Suppose two k-closure graphs K 1 , K 2 have the same skeleton and unshielded colliders.Then along any discriminating path p for a node Y , Y has the same collider status in K 1 and K 2 .</p>
<p>The above lemma shows that discriminating paths, although may exist in k-closure graphs, do not alter the equivalence class by themselves.Hence, for the graphical characterization of the equivalence between k-closure graphs, we can drop the discriminating path condition.In the next section, we will prove a k-Markov equivalence characterization based on Lemma 3.6, which will later be useful for learning, since we can employ algorithms devised for learning MAGs.</p>
<p>k-Markov Equivalence</p>
<p>Our main result in this section is the following theorem that characterizes k-Markov equivalence: Thus, k-Markov equivalence of two DAGs can be reduced to checking Markov equivalence of their kclosures, which can be checked locally using the equivalence condition in Corollary 3.13.Based on this result, it is clear that, just by using conditional independence tests, we can only hope to narrow down our search up to the equivalence class of k-closure graphs.</p>
<p>Note that when all the CI tests can be conducted, we can learn the arrowheads and tails that consistently appear in all Markov equivalent DAGs.By operating at the k-closure graph-level, we can attain a similar objective and hope to learn the invariant arrowheads and tails.Definition 3.15 (Edge union).For our purposes, we define the edge union operation as follows: For example, among Markov equivalent k-closures, if a, b are adjacent only as a → b the k-essential graph will have the edge a → b.Thus, the k-essential graph will preserve the invariant arrowhead and tail marks.The difference between a -b and a o-o b is significant from a causal perspective: In the former, two variables cause each other.In the latter, there is some k-Markov equivalent DAG where a, b do not cause each other and are simply not separable by conditioning sets of size at most k.
a → b ∪ a ← b = a -b, a → b ∪ a ← b ∪ a ↔ b = a o-o b, a → b ∪ a ↔ b = a o→ b
For any edge type proposed in the edge union definition, there are relevant instances of k-essential graphs.In Figure 2, the edges a-b, ao→ c, c ← b appear in the k-essential graph.Similarly, the k-essential graph of D in Figure 8 3 shows an instance where ↔ appears in the k-essential graph.This example also demonstrates that our representation is strictly richer than the graph union that is recovered by LOCI [23], which orients d → c, a → c, and hence cannot distinguish them from the possible edges between c ← b unlike our representation.Similarly, consider the simple graph u → v with k = 0. Our representation yields u − v since there is no DAG where u, v are confounded in any way other than the direct edge.In a triangle graph w → u, w → v, u → v, our representation recovers uo-ov which shows there are graphs where u, v do not cause each other.LOCI cannot make such distinction as it uses u − v in both graphs.</p>
<p>Therefore, the variant edges that can be arrowheads or tails among different k-Markov equivalent graphs are replaced with circles and invariant arrowheads and invariant tails are preserved in the k-essential graph.Thus, k-essential graph captures the causal information that is preserved across all k-Markov equivalent causal graphs.Different from essential graphs where we can test all the conditional independence relations, existence of an arrow a → b in a k-essential graph does not mean a causes b in every k-Markov equivalent DAG.Rather, it means that in any k-Markov equivalent DAG where a, b are adjacent, a causes b.</p>
<p>It is worth noting that k-essential graphs are in general more informative than partial ancestral graphs (PAGs), which are defined as the edge union of all Markov equivalent MAGs, where the union of ←, → is defined to give o-o instead of the undirected edge.Since every Markov equivalent k-closure graph is a MAG but not every Markov equivalent MAG is a k-closure graph, in general k-essential graphs may have invariant arrowheads and tails where PAGs only have circles.We call any graph that contains arrowheads, tails, or circles as edge marks a partial mixed graph (PMG).Definition 3.17.For two partial mixed graphs A, B with the same skeleton, A is said to be a subset of B, shown by A ⊆ B, iff the following conditions hold for any pair a, b
1. (a- * b) A ⇐ (a- * b) B , 2. (a ← * b) A ⇐ (a ← * b) B .(1)
Note that asterisk * stands for a wild-card which can either be an arrowhead, tail, or circle.According to the definition, any circle mark in A is also a circle mark in B. We have the following lemma that relates k-essential graphs to partial ancestral graphs of k-closures.Proof.By Theorem 3.9, every k-closure graph is a MAG whereas every MAG is not a k-closure graph.Thus the set of Markov equivalent k-closure graphs form a subset of the set of Markov equivalent MAGs.Thus, the arrowheads and tails that appear in all Markov equivalent MAGs must also appear in all the Markov equivalent k-closure graphs.The result follows from Definition 3.17.</p>
<p>In the next section, we propose a sound algorithm for learning k-essential graphs from data.</p>
<p>Learning k-essential Graphs</p>
<p>Constraint-based causal discovery algorithms use CI tests to extract all the causal knowledge that can be identified from data.In the previous section, we proposed a compact graphical characterization of what is learnable from such statistical tests.In this section, we propose a constraint-based learning algorithm.Since k-closure graphs are a special class of maximal ancestral graphs, we can use FCI algorithm that is devised for learning the invariant arrowheads and tails of a maximal ancestral graph.</p>
<p>FCI algorithm is sound and complete for learning PAGs: It can recover all invariant arrowheads and all invariant tails.One might think that FCI algorithm may also be sound and complete for our task.However, this is not true.Although sound, FCI is not complete for learning k-essential graphs. 2or soundness, we need the following lemma, which shows that discriminating paths do not carry extra information about the underlying causal structure.Lemma 4.1.In any k-closure graph, if there is a discriminating path p for ⟨u, Y, v⟩ and u ↔ Y ← * v is a collider along p, then the orientations u * → Y and Y ← * v can be learned by first finding all unshielded colliders, and then applying the orientation rules R1 and R2 of F CI.</p>
<p>Algorithm 1 k-PC Algorithm</p>
<p>Input: Observational Data, V, k, CI Tester (. ⊥ ⊥ .|. ).</p>
<p>Step 0: Initiate a complete graph K between V with circle edges o-o.</p>
<p>Step 1: Find separating sets S a,b for every pair a, b ∈ V by conditioning on subsets of size at most k .</p>
<p>Step 2: Update K by removing the edges between pairs that are separable.• R12: Orient ao-oc as a-c, ∀c ∈ C * .</p>
<p>Output: K</p>
<p>The above lemma shows that the colliders that are part of discriminating paths can be learned by simply orienting the unshielded colliders and then applying the orientation rules of FCI.This is useful since we do not need to search for colliders along discriminating paths during learning.</p>
<p>Our constraint-based causal discovery algorithm, called k-PC , is given in Algorithm 1.It uses FCI Orient algorithm in Section B, with two extra rules specific for learning k-essential graph.Proof.By Lemma A.18, any non-adjacent pair are separable by a set of size at most k.Thus, a valid separating set for any non-adjacent pair will be found in Step 1, and will be used to learn the skeleton in Step 2, and to orient all unshielded colliders in Step 3. [25] proved arrowhead and tail completeness of FCI with orientation rules R1 to R10.By Lemma 4.1, colliders on discriminating paths will be oriented at the end of Step 4. Thus, R4 that is concerned with discriminating path colliders is not applicable.Similarly, R5, R6, R7 are only applicable in graphs with selection bias.Thus, they are not applicable.Since the rules that we omit are never applicable during the execution of the algorithm, Step 4 correctly returns the PAG(C k (D)) since the algorithm at that point is identical to the FCI algorithm for learning PAGs.
Definition 4.3. A distribution p is said to be k-faithful to a causal graph D = (V, E), iff (a ⊥ ⊥ b |c ) p implies (a ⊥ ⊥ b |c ) D for all c ⊂ V : |c| ≤ k.-PC returns K, we have ε k (D) ⊆ K ⊆ PAG(C k (D))
Proof Sketch.From Corollary 4.2, K obtained at the end of Step 4 is PAG(C k (D)) and by Lemma 3.18 we know ε k (D) ⊆ PAG(C k (D)), i.e., every edge and tail orientation of K is consistent with ε k (D).Thus, we only need to show that orientation rules R11, R12 are sound, i.e., there exists no k-closure graph in the Markov equivalence class that is inconsistent with these orientation rules.By Lemma A.2, we know that if a ↔ x for some x, then conditioned on any subset of size k, there exists a d-connecting path that starts with arrow at a and x.This means that, irrespective of k, a must have some incoming edge.The rules use this fact together with the fact that if there were two incoming edges from two non-adjacent neighbors, this would create an unshielded collider, which would change the Markov equivalence class.Please see Section A.8 for the proof.For two sample runs of the algorithm, please see Figure 5 and Figure 6 in Section C of Appendix.Note also that our algorithm can be seen as an improved version of AnytimeFCI algorithm [18] for causally sufficient systems, which the author shows can be stopped once every conditioning set of size at most k are tested.While FCI aims at learning arbitrary PAGs partially, k-PC learns k-closure graphs for causally sufficient systems.This allows extracting more causal information, evidenced by the additional orientation rules employed by k-PC .For more details on the differences and an example where AnytimeFCI is less informative than k-PC , please see Section D.7.</p>
<p>Computational Complexity.Suppose we are given two causal graphs.Our characterization gives an algorithmic way of testing k-Markov equivalence: Construct the k-closure graphs given the two DAGs and check whether they are Markov equivalent.The step to make two variables adjacent requires one to loop through all conditioning sets of size at most k.This would take O(n k ).This is the main time-consuming step.Afterward, one can test Markov equivalence using the existing approaches.For example, [7] show that this can be done in O(ne 2 + n 2 e) time, where e is the number of edges.Thus, the overall algorithm will indeed be polynomial-time when k = O(1).</p>
<p>The complexity of the learning algorithm, k-PC , will be similar to Anytime FCI, an early-stopped version of FCI [18].Although this is more complicated and would depend on other parameters in the graph, such as primitive inducing paths, and thus the number and location of unobserved confounders, we can also roughly bound this by O(n k+2 ) since for any pair, we will not be searching for separating sets beyond the O(n k ) subsets of size at most k.Further runtime improvements, such as RFCI by [6] might be possible, but this requires a further understanding of the structure of k-closure graphs.
X1 X2 X6 X3 X4 X5 X8 X7 (a) Ground Truth X1 X2 X6 X7 X8 X3 X4 X5 (b) k-PC , k = 0 X1 X2 X6 X3 X4 X5 X8 X7 (c) k-PC , k = 1 X1 X2 X3 X4 X6 X5 X8 X7 (d) PC Alg.
Figure 4: Causal discovery on Asia dataset using 500 random samples.k-PC can learn the approximate causal order and some edges accurately, whereas PC outputs a very sparse graph since it conditions on subsets of size 2, which over-sparsifies the graph.For example, k-PC with k = 1 can recover that X 2 is an ancestor of X 6 , which is an ancestor of X 7 , whereas PC cannot.</p>
<p>Experiments</p>
<p>Synthetic Data</p>
<p>In this section, we test our algorithm against the stable version of PC algorithm in causal-learn package.We randomly sample DAGs using the pyAgrum package.Variation of this experiment with a different sparsity level can be found in Section E.3 and gives similar results.All variables are binary with conditional probability tables filled randomly uniformly from the corresponding dimensional probability simplex, except the linear SCM experiments for comparing with NOTEARS.We observed similar results with a larger number of states, which are not reported.We report the F 1 scores for identifying arrowheads, tails and skeleton correctly.The results are given in Figure 3.We observe that k-PC provides significant improvement to the arrowhead F 1 score in the small-sample regime at little cost to the tail score.k-PC also helps improve skeleton discovery.Similar improvements are seen even for 10 samples whereas the advantage disappears for more than 500 samples (see Section E.3) since PC becomes reliable enough and k-PC does not use some of the informative CI tests in that high-sample regime.We report combined metrics such as the sum of all F 1 scores in Section E. 4.</p>
<p>In Section E.5, we compare with the conservative version of PC [14] and observe similar results.In Section E.2 we generate linear SCMs randomly and compare the performance of our algorithm with NOTEARS in addition to PC.Even though NOTEARS performs slightly better than PC in general, k-PC maintains an advantage over both in the small-sample regime.In Section E.1, we compare our algorithm with LOCI [23].As expected, the two algorithms perform similarly.We observe that our algorithm shows better arrowhead and skeleton F 1 score performance.As a side note, we show that k-PC output is at least as informative as LOCI in Section D.6.See Section D for further discussions.The Python code is provided at https://github.com/CausalML-Lab/kPC.</p>
<p>Semi-synthetic Data</p>
<p>We test our algorithm on the semi-synthetic Asia dataset from bnlearn repository and compare it with PC algorithm.The dataset contains 8 binary variables.We randomly sample 500 datapoints from the observational distribution and run PC as well as k-PC for k = 0 and k = 1.k-PC for k = 0 correctly gets some of the causal and ancestral relations (Figure 4).PC, on the other hand, recovers a very sparse graph as it utilizes unreliable conditional independence tests with conditioning size of 2. k-PC with k = 1 recovers a graph in-between the two, still more informative about the structure than PC.</p>
<p>Discussion</p>
<p>A limitation of the method is that it assumes all independence statements up to degree k can be tested for some k.In some cases, the set of available, or the set of reliable CI statements might not have such a structure.Our method is not directly applicable in such scenarios.Another limitation is that we assume that we can run CI tests for the tests that are deemed reliable.This is a non-trivial problem when the data is non IID, such as time-series data.We also make some other usual assumptions that are commonly made in causal discovery, such as acyclicity.</p>
<p>For more remarks, such as a demonstration of incompleteness of k-PC , please see Section D.</p>
<p>Conclusion</p>
<p>We proposed a new notion of equivalence between causal graphs called the k-Markov equivalence.This new equivalence allows us to learn parts of the causal structure from data without relying on CI tests with large conditioning sets that tend to be unreliable.We showed that our learning algorithm is sound, and demonstrated that it can help correct some errors induced by other algorithms in practice.</p>
<p>Appendix A Proofs</p>
<p>In this section, we provide proofs for the lemmas and theorems in the paper.We also present FCI orientation rules for completeness, demonstrate why k-PC is incomplete, and give two sample runs of the k-PC algorithm.</p>
<p>Each subsection starts from a new page to clearly separate the proofs of different lemmas/theorems.</p>
<p>A.1 Proof of Lemma 3.6</p>
<p>We will crucially use the following three lemmas to prove our main results.We say a collider ⟨u, v, w⟩ is closed, or blocks the path in context if no node in De(v) is in the conditioning set.Similarly, a path is called closed if it is not d-connecting, and open otherwise.</p>
<p>Lemma A.1.Consider a DAG where X / ∈ An(Y ).Suppose there is a d-connecting path p between X, Y given T that starts with an arrow out of X.</p>
<p>1.There is at least one collider along p.</p>
<ol>
<li>Let K be the collider closest to X on p. Then conditioning on T ′ = T − De(K) instead of T does not introduce new d-connecting paths that start with an arrowhead at X.</li>
</ol>
<p>Proof. 1.Any path that starts with X → . . .must either be directed, or there must be at least one collider along the path.Since the path is between X, Y and X / ∈ An(Y ), it must be that the path has at least one collider on it.</p>
<ol>
<li>First note that without loss of generality, p has the following form for some integer m ≥ 0 (m = 0 means X → K):
X → U 1 → U 2 . . . → U m → K ← . . . Y.(2)
Suppose for the sake of contradiction that conditioned on T ′ there is a new d-connecting path q that starts with an arrowhead into X.q was clearly closed conditioned on T and become open by us removing nodes from the conditioning set T .This can only happen if we removed some node from T that is a non-collider along q.Consider the non-collider we removed that was closest to X, call this M .Thus, we have the path q that looks like this:
X ← W . . . M . . . Y,(3)
for some W , where M is a non-collider on this path and is in De(K).</li>
</ol>
<p>We observe that the subpath between M and X cannot be directed from M to X.Because this would create the following cycle:
M → . . . → X → U 1 . . . U m → K → . . . → M.(4)
Thus a closer look at the path q reveals the following structure for some integer m ′ and node V :
X ← W 1 ← W 2 . . . ← W m ′ → V . . . M . . . Y(5)
We will consider the following two cases: The edge adjacent to M along the subpath between W m ′ and M is a tail or an arrowhead.</p>
<p>Suppose the edge adjacent to M along the subpath between W m ′ and M is a tail: This means there is at least one collider between W m ′ and M .Any such collider must be active since q is active given T ′ .Consider the collider that is closest to M .Since it is active, this collider must be an ancestor of T ′ .However, observe that K is an ancestor of this collider which implies that K is an ancestor of T ′ as well.However, we obtained T ′ by removing all descendants of K from T , which is a contradiction.</p>
<p>This establishes that the edge adjacent to M along the subpath between W m ′ and M is an arrowhead.Thus, this reveals the following structure for q:
X ← W 1 ← W 2 . . . ← W m ′ → . . . → M . . . Y(6)
Suppose the directed path from K to M is as follows:
K → θ 1 → . . . θ m ′′ → M (7)
Recall that M is a non-collider along q.Thus the subpath of q between M and Y must start with a tail as M → . . .Y .Now observe that if the subpath of q between M and Y had no collider, then we would have the following directed path from X to Y :
X → U 1 → U m → K → θ 1 → . . . → θ m ′′ → M → . . . Y(8)
However, we know X is a non-ancestor of Y .Thus, there must be at least one collider between M and Y along p, all of which are open given T ′ .Consider the collider that is closest to M .There is a directed path from M to this collider, and a directed path from this collider to a member of T ′ since it is open conditioned on T ′ .But this means there is a directed path from K to a member of T ′ since there is a directed path from K to this collider.This is a contradiction since we obtained T ′ by removing all descendants of K from T .This establishes the claim that removing descendants of the collider along any d-connecting path that starts with a tail at X cannot introduce a d-connecting path that starts with an arrow into X when X / ∈ An(Y ).</p>
<p>Lemma A.2. Consider a DAG D where X / ∈ An(Y ) and Y / ∈ An(X), X, Y are non-adjacent and k-covered.Then conditioned on any subset T : |T | ≤ k, there exists a d-connecting path between X, Y that has an arrowhead at both X and Y .</p>
<p>Proof.For the sake of contradiction suppose, conditioned on c, there is no d-connecting path with an arrow into X and an arrow into Y .Since neither X is an ancestor of Y nor Y is an ancestor of X, it must be that all d-connecting paths have colliders on them.And all such colliders must be ancestors of T .</p>
<p>Consider such a path p where the edge adjacent to X has a tail at X. Let K be the collider that is closest to X.</p>
<p>Thus we have
X → U 1 → . . . → U m → K ← V . . . Y
for some {U i } i∈m , V .Since the path is open it must be that K ∈ An(T ).Let T ′ = T − De(K), where De(K) are all descendants of K. Clearly, q is no longer open conditioned on T ′ .We investigate other open paths now, keeping in mind that X, Y being k-covered implies that
X ̸⊥ ⊥ Y |T ′ since |T ′ | ≤ |T | ≤ k.
Claim 1: Removing the descendants of the collider closest to X from the conditioning set can only add d-connecting paths that start with a tail at X but no d-connecting paths that start with an arrowhead at X.</p>
<p>Proof of Claim 1: Since X is not an ancestor of Y , by Lemma A.1, we know that removing the descendants of K from T can only introduce d-connecting paths that are out of X.Now consider the d-connecting paths under conditioning on T ′ .We know that these paths must have a tail either at X or at Y since we started with such d-connecting paths by assumption and by Claim 1, removing De(K) from T can only introduce new d-connecting paths that have tails at X. Using the fact that no path that has an arrowhead into both endpoints are opened, we can use recursion and claim that we can make X, Y d-separated by removing descendants of colliders (that are closest to the endpoint that is adjacent to a tail) of active paths, which gives the following:</p>
<p>Claim 2: There exists a set T * of size at most k such that X ⊥ ⊥ Y |T * , which leads to a contradiction since X, Y are k-covered by assumption.</p>
<p>Proof of Claim 2: Given claim 1, we can continue removing descendants of the colliders of the active paths that are closest to the tail-end node from the set T .Either no d-connecting path is left at some point in this process, or that we end up removing all the variables from the conditioning set.If the former, this is a contradiction since X, Y are k-covered .If the latter, then this is another contradiction due to the following: This means that given empty set, paths that have a tail adjacent to one of the endpoints, i.e., the paths with colliders on them (since all paths that have a tail adjacent to one of the endpoints must have a collider because X / ∈ An(Y ) and Y / ∈ An(X)) are d-connecting, which is not possible.This proves Claim 2. Due to the symmetry between X, Y , the supposition that the only d-connecting paths must have a tail adjacent to either endpoint must be wrong, which proves the lemma.Proof.For the sake of contradiction, suppose otherwise.Given T , all the d-connecting paths start with a tail at X.We will show that we can find some T ′ of size at most k that d-separates X, Y , which lead to a contradiction since X, Y are assumed to be k-covered .</p>
<p>Consider any path q that is d-connecting given T which starts with a tail at X. Since X / ∈ An(Y ), by Lemma A.1 it must be that this path has at least one collider on it.Let K be the collider that is closest to X. Thus we have
X → U 1 → . . . → U m → K ← V . . . Y
for some {U i } i , V .Since the path is open, this collider cannot be blocking the path q.It must be that K ∈ An(T ).Let T ′ = T − De(K), where De(K) are all descendants of K. Clearly, q is no longer open conditioned on T ′ .We investigate other open paths now, keeping in mind that X, Y being k-covered implies that
X ̸⊥ ⊥ Y |T ′ since |T ′ | ≤ |T | ≤ k.
Claim 1: Removing the descendants of the collider closest to X from the conditioning set can only add d-connecting paths that start with a tail at X but no d-connecting paths that start with an arrowhead at X.</p>
<p>Proof of Claim 1: Since X is not an ancestor of Y , by Lemma A.1 we know that removing the descendants of K from T can only introduce d-connecting paths that are out of X.</p>
<p>Now consider the d-connecting paths under conditioning on T ′ .We know that these paths must have a tail at X since we started with only such d-connecting paths by assumption and by Claim 1, removing De(K) from T can only introduce d-connecting paths that have tails at X. Using the fact that no path that has an arrowhead into X, we can use recursion and claim that we can make X, Y d-separated by removing descendants of colliders (that are closest to X) of active paths, which gives the following:</p>
<p>Claim 2: There exists a set T * of size at most k such that X ⊥ ⊥ Y |T * , which leads to a contradiction since X, Y are k-covered by assumption.</p>
<p>Proof of Claim 2: Given claim 1, we can continue removing descendants of the first colliders of active paths that are closest to X from the set T .Either, no d-connecting path is left at some point in this process or that we end up removing all the variables from the conditioning set.If the former, this is a contradiction since X, Y are k-covered.If the latter, then this is another contradiction due to the following: This means that given empty set, paths that start with a tail and have colliders on them (as they cannot be directed and collider-free since X is not an ancestor of Y ) are d-connecting, which is not possible.This proves Claim 2.</p>
<p>. Therefore, the supposition that all the d-connecting paths must have a tail adjacent to X must be wrong, which proves the lemma.</p>
<p>The above lemmas will be crucial in proving Lemma 3.6.Now consider a d-connecting path between x, z given c and a d-connecting path between z, y given c.We have the following lemma: Lemma A.4.Let p be an active path between x, z given c, and q be an active path between z, y given c, where x, y, z / ∈ c.If x and y are d-separated given c, then 1. Paths p, q must have no overlapping nodes and 2. Y must be a collider along the concatenated path and Y / ∈ An(c).</p>
<p>Proof.We would like to allow the possibility that these paths might go through the same nodes.To address this, it helps to consider walks.</p>
<p>Definition A.5.A walk on a DAG is any sequence of edges.</p>
<p>Definition A.6.A path on a DAG is a sequence of edges where each node appears at most once.</p>
<p>There is a direct relation between active walks and d-connecting paths [8].Indeed, this relation is leveraged to efficiently check dependence using paths, rather than having to search over all walks, which is a much larger space.</p>
<p>Definition A.7.A walk between two nodes a, b is called active given c if each collider along the walk is in c and each non-collider is not in c.Consider an active walk where a node t appears multiple times.Observe that t must appear with the same collider status, since otherwise the walk would not be active.If the appearance is of the form a . . .
α − → t → . . . ← t β ← − . . . b,(9)
then there must be a collider that is in c between the two appearances of t's.We can skip the intermediate subpath between the two appearances of t's to obtain the walk a . . .
α − → t β ← − . . . b,(10)
Since there is at least one collider that is in c along the skipped subpath, we have that t ∈ An(c).Therefore, t will not be blocking the path that is obtained after repeatedly applying this and other shortening steps.If the appearances is of the form: a . . .
α − → t → . . . → t β − → . . . b,(11)
we can similarly skip the subpath between the two appearances of t's to obtain the shorter walk
a . . . α − → t β − → . . . b,(12)
and repeat this process until t is not repeated.The resulting walk/path is still open since t will appear in the same collider status, namely as a non-collider and if it was not blocking the walk, it will not be blocking the path either.This argument holds for any configuration where t is a non-collider.If the appearances is of the form: a . . .
α − → t ← . . . → t β ← − . . . b,(13)
then it must be that t ∈ c, and thus the walk obtained by skipping the subwalk between the two appearances of t's, i.e., a . . .
α − → t β ← − . . . b,(14)
must be d-connecting.This shows that each active walk corresponds to a d-connecting path and vice verse.Now we can proceed with the proof of the lemma:</p>
<ol>
<li>Suppose p, q have overlapping nodes.Let w p be the walk that corresponds to p and w q be the walk that corresponds to q.Consider the concatenated walk w = w p , w q .If any repeated node has different collider status along w, then the path is not active.But this means that that node was blocking either w p or w q , which would be a contradiction.Therefore, repeated nodes cannot have different collider status along w.</li>
</ol>
<p>Suppose a node t is repeated in w p and w q and has the same collider status.In this case, consider the walk obtained by concatenating the sub-walk of w p between x and t, with the sub-walk of w q from t to y.By repeating this process for any repeated node, we can obtain a path that corresponds to this walk, which would always be active since the repeated nodes have the same collider status along this path that they had in w p or w q and were not blocking these walks.Therefore, they cannot block the concatenated path obtained this way either, which is a contradiction since we are given that x, y are d-separated given c.Therefore if any node is repeated in w p and w q then the concatenated walk is always active.Thus, it must be the case that there is no repeated nodes.</p>
<ol>
<li>Since there is no repeated nodes from 1., we can operate at the path level instead of considering walks.Suppose Y is not a collider.Since Y / ∈ c, it would be d-connecting and thus the concatenating path would be d-connecting, a contradiction.Suppose Y is a collider but Y ∈ An(c).In this case, Y would not block the concatenated path either, which is a contradiction.This establishes the result.</li>
</ol>
<p>The next lemma shows that colliders that are closed in D must remain closed in the k-closure C k (D).</p>
<p>Lemma A.9.If a collider is blocked in D conditioned on some c : |c| ≤ k then it must also be blocked in C k (D) conditioned on c.</p>
<p>Proof.Suppose (X → Z ← Y ) D is a collider that is blocked given c.Thus, it must be that Z / ∈ An(c) in D. For the sake of contradiction, suppose that this collider is unblocked in C k (D).Thus, it must be the case that Z ∈ An(c) in C k (D).This means there is a new directed path from Z to c in C k (D).If this path existed in D, the collider would be unblocked, which is a contradiction.Thus, at least one of the edges along this path must have been added during the construction of C k (D).Consider the collection of edges on this path that do not exist in D. Note that by construction of C k (D), a directed edge α → β is added between a k-covered pair α, β only if there is a directed path from α to β.Consider the path obtained by replacing the directed edge between any k−covered pair along this path with the corresponding directed path in D. The resulting directed path must be in D. This shows that there was at least one path already in D that implied Z ∈ An(c), which is a contradiction.Therefore, any collider in p that is unblocked in C k (D) must also be unblocked in D. Proof.Adding edges to a graph, directed or bidirected, cannot decrease the set of ancestors of any node.We only need to show that the set of ancestors in C k (D) is not larger than the set of ancestors in D.</p>
<p>Suppose otherwise: A node a ∈ An(c) in C k (D) but a / ∈ An(c) in D. This can only happen if a collection of edges added during the construction of C k (D) render a an ancestor of c.However, each such edge is added only if there is a directed path between its endpoints in D. Consider the path obtained by replacing each such added edge along the path that renders a an ancestor of c in C k (D) with the corresponding directed paths in D. This directed path must be in D, which means that a was an ancestor of c in D as well, which is a contradiction.</p>
<p>We are finally ready for the proof of Lemma 3.For the sake of contradiction, suppose otherwise.Then there must be a d-connecting path p between a, b given c in C k (D).The length of any such path must be greater than 1 since otherwise, whether this edge already existed in D or it was added during the construction of C k (D), a, b must have been dependent given c in D, which is a contradiction.Since the orientation of the existing edges in D did not change in C k (D), either this path did not exist in D or that it existed but it was blocked by some collider that is not in An(c) in D. The latter is not possible due to Lemma A.9, since any unblocked collider in C k (D) must also be unblocked in D. Thus, it must be that this d-connecting path did not exist in D.</p>
<p>Suppose p does not exist in D. At least one edge must have been added to form this path in C k (D) during the construction of C k (D).</p>
<p>For any added edge u → v, the following is true: Since u → v was added in C k (D), it must be the case that v / ∈ An(u) since otherwise there would be a cycle.By Lemma A.3, conditioned on c, there exists a d-connecting path between u, v where the edge adjacent to v is into v.For any added edge u ↔ v, the following is true: Since u ↔ v was added in C k (D), it must be the case that u / ∈ An(v) and v / ∈ An(u).By Lemma A.2, conditioned on c, there exists a d-connecting path between u, v where the edge adjacent to u is into u and the edge adjacent to v is into v.Call any such path implied by these lemmas a replacement path.Note that a replacement path might be directed or not.</p>
<p>Consider a path q in D that is obtained from p by switching the edges added during the construction of C k (D) with the replacement paths using the following policy: Suppose u → v in C k (D) for some k-covered pair u, v.If a directed path is open given c in D, use that path as the replacement path for the edge a → b.</p>
<p>If not, use any other path.This means that either the path that replaces an edge u → v is directed or that both the endpoints have an arrowhead and that u ∈ An(c).</p>
<p>Observe that each replacement path is d-connecting and the subpaths of p that remain intact in q must be d-connecting since p is d-connecting.By Lemma A.4, any two paths -whether it is a pair of replacement paths or a replacement path and a subpath of p -have overlapping nodes, then their concatenation must be d-connecting.Since we assumed that q was not d-connecting, it must be that one of the endpoints of one of the added edges must be blocking q.We investigate each such node to verify that q is indeed d-connecting to arrive at a contradiction.</p>
<p>In other words, the collider status of some of these nodes must have changed due to replacing some edges with replacement paths.Specifically due to Lemma A.4, one of the endpoints of replacement paths must be a collider and not an ancestor of c.Since ancestrality status cannot change from D to C k (D) due to Lemma A.10, the only way for q to not be d-connecting is if some node that is not an ancestor of c changes status from being a non-collider along p to being a collider along q.</p>
<p>Note that for an edge u ↔ v, the nodes u and v are adjacent to an arrowhead in the replacement path.Thus, if some node t changes collider status in q compared to p, it cannot be due to bidirected edges along p.Now consider the directed edges u → v.If the replacement path is directed from u to v, similarly u is adjacent to a tail and v is adjacent to an arrowhead on the replacement path.Therefore, such edges cannot alter the collider status of nodes at the junction of different paths.Finally consider the directed edges u → v where u and v are both adjacent to an arrowhead on the replacement path.Observe that this edge cannot change the collider status of v.We now focus on u.If the other edge adjacent to u along q is a tail, u remains a non-collider and cannot block q.Now suppose the other edge adjacent to u along q is an arrowhead.This makes u a collider in q whereas u was a non-collider along p since we had u → v along p.However, by construction of q, as we ended up adding a path with arrowheads at both endpoints, it must be that the directed path between u, v (which exists since u → v was added during the construction of C k (D)) must be blocked via conditioning.This means u ∈ An(c) which means that although the status of u changes from non-collider to collider, it must be that this collider does not block q since it is an ancestor of c.Therefore, no replacement path can alter the status of a node to block the path q, and q must be d-connecting, which contradicts with the assumption that the path was not d-connecting in D.</p>
<p>This establishes that any d-connecting path in C k (D) is also d-connecting in D, which establishes that if a ̸⊥ ⊥ b |c in C k (D), then a ̸⊥ ⊥ b |c in D. This establishes the lemma.</p>
<p>A.2 Proof of Lemma 3.8</p>
<p>For a mixed graph to be a maximal ancestral graph, we need to show that it does not have directed or almost directed cycles and that any non-adjacent pair of nodes can be made conditionally independent by conditioning on some subset of observed variables [24].We first define almost directed cycle, and propose a lemma that shows that k-closure graphs do not have directed or almost directed cycles.Suppose, for the sake of contradiction that there is an almost directed cycle in C k (D), i.e., we have a directed path from a to b for two nodes a ↔ b.Since a ↔ b is added during construction of C k (D), it must be the case that neither a nor b are ancestors of each other.However, from the above argument, there must be a directed path from a to b in D, which is a contradiction.Thus, C k (D) cannot have almost directed cycles.</p>
<p>The other condition for a mixed graph to be a maximal ancestral graph is that for any non-adjacent pair of nodes, there exists a subset of the observed variables that make them conditionally independent.For the k-closure graphs, this simply follows by construction: Any pair of nodes that are non-adjacent in C k (D) can be made conditionally independent given some set of size at most k in D by construction of C k (D).From Lemma 3.6, this conditional independence relation must be retained in C k (D).Thus any non-adjacent pair of nodes in C k (D) can be d-separated in C k (D) by some conditioning set of size at most k.This establishes the claim.</p>
<p>However, we know X is a non-ancestor of Y .Thus, there must be at least one collider between M and Y along p, all of which are open given T ′ .Consider the collider that is closest to M .There is a directed path from M to this collider, and a directed path from this collider to a member of T ′ .But this means there is a directed path from K to a member of T ′ since there is a directed path from K to this collider.This is a contradiction since we obtained T ′ by removing all descendants of K from T.</p>
<p>This establishes the claim that removing descendants of the collider along any d-connecting path that starts with a tail at X cannot introduce a d-connecting path that starts with an arrow into X when X / ∈ An(Y ).</p>
<p>The following is the parallel lemma to Lemma A.2 for any mixed graph K that satisfies the conditions of Theorem 3.9.</p>
<p>Lemma A.14. Consider a bidirected edge X ↔ Y in K. Suppose conditioned on any subset
T : |T | ≤ k, X ̸ ⊥ ⊥ Y |T in G − (X ↔ Y ).
Then conditioned on any T : |T | ≤ k, there exists a d-connecting path between X, Y that starts with an arrow into X and an arrow into Y .</p>
<p>Proof.For the sake of contradiction suppose, conditioned on some T : |T | ≤ k, there is no d-connecting path with an arrow into X and an arrow into Y .Since neither X is an ancestor of Y nor Y is an ancestor of X, all d-connecting paths must have colliders on them.And all such colliders must be ancestors of T .</p>
<p>Consider such a path p where, without loss of generality, the edge adjacent to X has a tail at X. Let K be the collider that is closest to X.</p>
<p>Thus we have
X → U 1 → . . . → U m → K ← * V . . . Y(22)
for some {U i } i , V and integer m.Since the path is open, this collider must be unblocked.It must be that
K ∈ An(T ). Let T ′ = T − De(K),T ′ since |T ′ | ≤ k. Claim 1:
Removing the descendants of the collider closest to X from the conditioning set can only add d-connecting paths that start with a tail at X but no d-connecting path that starts with an arrowhead at X. Proof of Claim 1: Since a bidirected edge exists between X, Y , and that K is a MAG, neither X nor Y are ancestors of one another, since then we would have an almost directed cycle.By Lemma A.13, we know that removing the descendants of K from T can only introduce d-connecting paths that are out of X.Now consider the d-connecting paths under conditioning on T ′ .We know that these paths must have a tail either at X or at Y .Using the above claim that no path that has an arrowhead into both endpoints are opened, we can use recursion and claim that we can make X, Y d-separated by removing descendants of colliders (that are closest to the endpoint that is adjacent to a tail) of active paths, which gives the following:</p>
<p>Claim 2: There exists a set T * of size at most k such that X ⊥ ⊥ Y |T * , which leads to a contradiction since X, Y cannot be made independent by conditioning on sets of size at most k by the assumption.</p>
<p>Proof of Claim 2. Given claim 1, we can continue removing descendants of the colliders of the active paths that are closest to the tail-end node from the set T .Either no d-connecting path is left at some point in this process, or that we end up removing all the variables from the conditioning set.If former, this is a contradiction since X, Y cannot be made conditionally independent given empty set.If the latter is true, then there is another contradiction due to the following: This means that given empty set, paths that have a tail adjacent to one of the endpoints, i.e., the paths with colliders on them (since all paths that have a tail adjacent to one of the endpoints must have a collider because X / ∈ An(Y ) and Y / ∈ An(X)) are d-connecting, which is not possible.This proves Claim 2.</p>
<p>Therefore, the supposition that the only d-connecting paths must have a tail adjacent to either endpoint must be wrong, which proves the lemma.</p>
<p>Proof of Theorem 3.9.Now, we are ready to prove the main characterization theorem.We will need the following lemma: Lemma A.15.Let K be a mixed graph that satisfies the conditions in Theorem 3.9.Let K ′ be the graph obtained by removing all the bidirected edges from K. Then 1. K ′ is a DAG and 2. K ′ ∼ k K.</p>
<p>Proof.Since the only difference between K ′ and K is the removal of bidirected edges, any directed cycle that exists in K ′ would also have existed in K, which contradicts with the assumption that K is a MAG.This establishes that K has no directed cycles.</p>
<p>Clearly, any independence statement in K holds in K ′ , since it is obtained from K by removing edges.Thus any degree-k d-separation relation that holds in K also holds in K ′ .Therefore, we only need to show that for any c of size at most k (a
̸⊥ ⊥ b |c ) K implies (a ̸⊥ ⊥ b |c ) K ′ .
Suppose for the sake of contradiction that a ̸⊥ ⊥ b |c in K but a ⊥ ⊥ b |c in K ′ .Let p be a d-connecting path between a, b given c in K.This path must be closed in K ′ .Since the only difference between the two graphs is the removal of bidirected edges, ancestrality relations cannot be different.Thus, it cannot be the case that a collider that was open in K is now closed in K ′ and is closing the path p.Any collider that was open must still be open.Thus, the only way for p to be closed in K ′ is if some bidirected edge X ↔ Y along p is removed.However, by Lemma A.14, for any such bidirected edge in K, and for any conditioning set of size at most c, we have a d-connecting path called a replacement path with an incoming edge to both X and Y .Consider the path q obtained by replacing every bidirected edge along p with a corresponding replacement path.Since a, b are d-separated by assumption, this path cannot be open.As this path is a concatenation of several d-connecting paths -either sub-paths of p, which must be open, or replacement paths which must be open, by Lemma A.4, they must have no overlapping nodes, and some node at the junction of these paths must be a collider and non-ancestor of c.However, since we replaced bidirected edges X ↔ Y with paths of the form X ← * . . .* → Y , both X and Y must have the same collider status on both p and q.Thus, they cannot be blocking q since they are not blocking p.This means that q is d-connecting in K ′ , which is a contradiction.This proves the lemma that K and K ′ must entail the same degree-k d-separation relations, which implies they are k-Markov equivalent.</p>
<p>The only if direction: Suppose a mixed graph is a k-closure graph, i.e., K = C k (D) for some DAG D and has the edge a ↔ b.Suppose for the sake of contradiction that a, b are not k-covered in K − (a ↔ b).Let K ′ be the graph obtained from K by removing all the bidirected edges.Note that K ′ is a DAG since K has no directed cycles.Also note that all edges in D must appear in K ′ by construction of k-closure graphs.D can therefore be obtained from K by removing edges.Thus, any d-separation statement in K must also hold in D. Therefore, a, b must be conditionally independent given some subset c of size at most k in D. This means K, in which a, b are adjacent, cannot be the k-closure graph of D, which is a contradiction.</p>
<p>If direction: Suppose a mixed graph K satisfies the conditions in Theorem 3.9.By Lemma A.15, for any such mixed graph K, there is a DAG whose k-closure is K, which shows that any such K is a valid k-closure graph, proving the theorem.</p>
<p>A.4 Proof of Lemma 3.12
Let K 1 = C k (D 1 ), K 2 = C k (D 2
) be two k-closure graphs with the same skeleton and unshielded colliders.Suppose for the sake of contradiction that there is a path p that is discriminating for a triple ⟨u, Y, v⟩ in both such that Y is a collider along p in C k (D 1 ) and a non-collider in C k (D 2 ).Thus, in C k (D 1 ) we have the path p as
a * → z 1 ↔ z 2 ↔ . . . ↔ z m ↔ u ↔ Y ↔ v(23)
where z i → v, ∀i and u → v and a, v are non-adjacent.Note that we cannot have
Y ← v instead of Y ↔ v since this would create the almost directed cycle u → v → Y ↔ u.
The same path with Y as a non-collider can take two configurations in C k (D 2 ), either as
a * → z 1 ↔ z 2 ↔ . . . ↔ z m ↔ u ↔ Y → v(24)
or as
a * → z 1 ↔ z 2 ↔ . . . ↔ z m ↔ u ← Y → v(25)
Other paths where Y is a non-collider would either render u a non-collider, which cannot happen by definition of a discriminating path, or create a directed or almost directed cycle.Since a, v are non-adjacent by definition of a discriminating path, there must be some S : |S| ≤ k where (a ⊥ ⊥ v |S ) C k (D1) .Note that S must include all z i 's and u, and not include Y since otherwise there would be d-connecting paths between a, v in C k (D 1 ) due to the discriminating path.This means that (a
̸⊥ ⊥ v |S ) C k (D2) . Since u ↔ Y in C k (D 1 )
, by Lemma A.2, there must be a d-connecting path between u, Y in D 1 conditioned on S that has an arrowhead at Y .By construction, this path must also appear in C k (D 1 ).Since the path is inherited from D 1 , it does not have bidirected edges.Consider the shortest of all such d-connecting paths, call this path q.Let X be the node adjacent to Y along q.Thus, q has the form
u ← . . . → X → Y.(26)
We have that
X → Y in both D 1 and C k (D 1 ). In C k (D 1 ), we have X → Y ↔ v. Since the edge between Y, v has a tail at Y in C k (D 2 ), this collider cannot exist in C k (D 2
).Thus, it must be the case that this collider is shielded in C k (D 1 ), i.e., X and v are adjacent in C k (D 1 ).Since C k (D 1 ), C k (D 2 ) have the same skeleton, they must also be adjacent in C k (D 2 ).Now consider the path obtained by concatenating the subpath of p a * → . . .u, and the subpath of q between u and X, and the edge between X and v in C k (D 1 ).Call this path r.Note that the subpath of q is d-connecting given S, as well as the subpath of p since z i 's and u are in S. Thus, unless X is a collider on it, the path r between a, v will be open, which would lead to a contradiction since a, v are d-separated given S in C k (D 1 ).Thus, the edge between X, v must have an arrowhead at X. Let W be the node before X along q.Thus we have
W → X ↔ v in C k (D 1 ). Note that X ← v is not possible since this would create an almost directed cycle X → Y ↔ v → X in C k (D 1 ).
Suppose this collider is unshielded and appears in C k (D 2 ) as well:
W * → X ← * v in C k (D 2 ). Thus in C k (D 2 ), we have Y → v * → X ← * W . Since X, Y are adjacent, it must be that X ← Y or X ↔ Y to avoid a directed or almost directed cycle. Thus in C k (D 2 ), we have X ← * Y . However, this creates the collider W * → X ← * Y in C k (D 2
).Note that this collider cannot appear in C k (D 1 ) since the edge between X, Y has a tail at X in C k (D 1 ).Thus the collider must be shielded, meaning that W, Y must be adjacent, and both in
C k (D 2 ) and in C k (D 1 ). Since we have W → X → Y in C k (D 1 ), the edge must be W → Y in C k (D 1 ).
Furthermore, similar to X, W cannot be in the conditioning set since this would block the path q.This means there is a d-connecting path that has an arrowhead at Y that is shorter than q, which is a contradiction.</p>
<p>Thus the collider W → X ↔ v in C k (D 1 ) must be shielded.Similar to the above argument, W must be a collider along the path constructed by concatenating the subpath a * → . . .u of p, and the subpath of q between u and W , and the edge between W and v since otherwise this path would be open, which would contradict with a ⊥ ⊥ v |S .Let V be the node next to W along q.Thus we have V → W → X along q and V → W ← * v is a collider in C k (D 1 ).In fact, it must be that W ↔ v since otherwise there would be an
almost directed cycle v → W → X → Y ↔ v in C k (D 1 ). Suppose the collider V → W ↔ v in C k (D 1
) is unshielded and also appears in C k (D 2 ).Note that if V, X were adjacent in C k (D 1 ), the orientation would have to be as V * → X since otherwise there would be a directed cycle V → W → X → V in C k (D 1 ).But this would imply that there is a shorter path than q that connects u, Y and has an arrow into Y .Thus, V, X must be non-adjacent in C k (D 1 ) and hence in C k (D 2 ).Thus, ⟨V, W, X⟩ is an unshielded non-collider in C k (D 1 ) and must also be in
C k (D 2 ). Thus it must be that W → X in C k (D 2 ). Since v * → W → X in C k (D 2 ), it must be that v * → X in C k (D 2 ) to avoid a directed or almost directed cycle. Since Y → v * → X in C k (D 2 ), it must be that X ← * Y to avoid a cycle or almost directed cycle in C k (D 2 ). However, now we have a collider W → X ← * Y in C k (D 2 ) that is a non-collider in C k (D 1 ) since in C k (D 1 )
we have X → Y .Thus, this collider must be shielded, i.e, W, Y must be adjacent in C k (D 2 ).Thus, they must also be adjacent in
C k (D 1 ). Since W → X → Y in C k (D 1 ), it must be that W → Y in C k (D 1
) to avoid a cycle.But this means there is a shorter d-connecting path between u, Y given S with an arrowhead at Y , which is a contradiction.</p>
<p>Therefore, the collider V → W ↔ v must be shielded in C k (D 1 ).We can repeat the above argument as many times as needed continuing from the parent of V along q.As we keep shielding more and more colliders in C k (D 1 ), eventually when we shield the first node along q next to u, we will end up with a directed path from u to Y .However, this is a contradiction since bidirected edge was added between u, Y which implies that u is not an ancestor of Y .</p>
<p>Therefore, if two k-closure graphs C k (D 1 ), C k (D 2 ) have the same skeleton and unshielded colliders, then they cannot have different colliders along discriminating paths, which proves the lemma.For completeness, we restate the definition of unshielded collider in k-closure graphs, which is identical to how it is defined in MAGs.</p>
<p>Clearly, S 1 c, S 2 ∋ c since c is a collider between a, b in C k (D 1 ) and a non-collider in C k (D 2 ).If we switch the conditioning sets, due to the different collider status of c in both graphs the d-separation statements will switch to d-connection statements:
(a ̸⊥ ⊥ b |S 1 ) C k (D2) , (a ̸⊥ ⊥ b |S 2 ) C k (D1) .(28)
Since S 1 and S 2 have size of at most k, then from Lemma 3.6 we have that: A.8 Proof of Theorem 4.4 We show soundness of the two new rules with the following two lemmas: Lemma A.19.Let K be a mixed graph that is sandwiched between ε k (D) and PAG(C k (D)), i.e., ε k (D) ⊆ K ⊆ PAG(C k (D)).R11 is sound on K for learning the k-essential graph, i.e., if
K ′ =R11(K), then ε k (D) ⊆ K ′ ⊆ K.
Proof.For the sake of contradiction, suppose otherwise: R11 orients an edge ao→ b in K as a → b, and there is a DAG D ′ with a k-closure graph C k (D ′ ) that is Markov equivalent to C k (D) and is consistent with K where a ↔ b.This means a, b are k-covered in D ′ .Then from Lemma A.2, conditioned on any subset S of size at most k, there must be a d-connecting path that starts with an arrow into both a and b in D. By construction of the k-closure graph, this path must also exist in C k (D ′ ).Therefore, there must be some node w such that a ← w.Since a has no incoming edges, it must be the case that w ∈ C.However, b is chosen so that b is non-adjacent to any node in C. Therefore, b must be non-adjacent to w in K.However, this creates the unshielded collider w → a ↔ b.However, note that wo-oao→ b in PAG(C k (D)), and thus ⟨w, a, b⟩ is a non-collider in C k (D).Therefore, C k (D ′ ) cannot be Markov equivalent to C k (D), which is a contradiction.</p>
<p>Lemma A.20.Let K be a mixed graph that is sandwiched between ε k (D) and PAG(C k (D)), i.e., ε k (D) ⊆ K ⊆ PAG(C k (D)).R12 is sound on K for learning the k-essential graph, i.e., if
K ′ =R12(K), then ε k (D) ⊆ K ′ ⊆ K.
Proof.For the sake of contradiction, suppose otherwise: R12 orients an edge ao-oc in K as a-c, and there is a DAG D ′ with a k-closure graph C k (D ′ ) that is Markov equivalent to C k (D) and is consistent with K where a ↔ c.This means a, c are k-covered in D ′ .Then from Lemma A.2, conditioned on any subset S of size at most k, there must be a d-connecting path that starts with an arrow into both a and c in D. By construction of the k-closure graph, this path must also exist in C k (D ′ ).Therefore, there must be some node w such that a ← w.Since a has no incoming edges, it must be the case that w ∈ C.However, c is chosen so that c is non-adjacent to any other node in C. Therefore, c must be non-adjacent to w in K.However, note that wo-oao-oc in PAG(C k (D)), and thus ⟨w, a, c⟩ is a non-collider in C k (D).</p>
<p>B FCI Orientation Rules</p>
<p>Algorithm 2 FCI Orient Input: Mixed graph K Apply the orientation rules of R1, R2, R3 of [25] to K until none applies.Apply the orientation rules of R8, R9, R10 of [25].Output: K</p>
<p>We restate the FCI orientation rules in detail and demonstrate how they are applicable for learning k-closure graphs.The following definitions are from [25].R11 and R12 cannot replace any of the above rules.For example, consider Figure 5. None of the FCI rules apply to the output of Step 3, thus we can only learn of the unshielded colliders at the end of Step 4 of the algorithm.The completeness of FCI implies that any edge xo→ y can be oriented as x ↔ y or x → y and give a MAG consistent with the PAG.However, not all such MAGs are valid k-closure graphs.R11 can be applied to orient several tails, which gives the graph in (d).Similarly in Figure 6, R12 helps orient the tail edges between a, b which cannot be learned by FCI rules.In this section, we give an example for a MAG that is not a valid k-closure graph.Consider the graph in Figure 8. K in (b) is a valid k-closure graph for k = 1 since it is the k-closure graph of D. However, K is not a valid k-closure graph for k = 2.This is because the bidirected edge c ↔ d is added between a pair that is not k-covered for k = 2: We have c ⊥ ⊥ d |u 1 , u 2 in D. In fact, using the if and only if characterization in Theorem 3.9, we can show that there does not exist any D ′ with the given k-closure graph where c, d have a bidirected edge.</p>
<p>C Sample Runs of k-PC Algorithm</p>
<p>D.3 Bidirected Edge in k-essential Graphs</p>
<p>In Figure 9, since every endpoint is an arrowhead and is part of an unshielded collider, there is no other Markov equivalent k-closure graphs, which implies that the k-essential graph is the same as the k-closure graph.Thus, the edge c ↔ e is in ε k (D).This example shows that we can learn that two nodes do not cause each other using conditional independence tests that are not even powerful enough to make them conditionally independent.It is worth noting that LOCI [23] can also infer this fact by removing this edge.</p>
<p>D.4 k-PC is Incomplete</p>
<p>One might hope that k-PC is complete and outputs the k-essential graph ε k (D).This, however, is not true.We discuss an example where k-PC cannot orient an invariant tail mark.First, observe that k-PC does not leverage the value of k.If we had an efficient way to answer the question "Is there a k-closure graph that is consistent with K in which a, b are k-covered ?" then we could leverage this to orient more o→ edges as → edges.As an example, consider the causal graph in Figure 10.We observe that a local algorithm such as k-PC cannot be used to assess if there is some k-closure graph that is consistent with the current graph in which two nodes are k-covered without the edge between them.One practical strategy would be to take the output of the k-PC algorithm and list all MAGs consistent with the circle edges, and then check if they are valid k-closure graphs by pruning every edge using Theorem 3.9.For small or sparse k-closure graphs, k-PC could be a practical way to reduce the search space efficiently, and then we can conduct an exhaustive search as the next step to obtain a sound and complete algorithm for learning the k-essential graph.</p>
<p>D.5 Heuristic Uses of Bounded Size Conditioning Sets</p>
<p>For large number of variables, and except for very sparse graphs, constraint-based algorithms take significant time to complete.This is because the progressive nature of such tests is not able to sufficiently sparsify the graph with low-degree conditional independence tests, and they have to perform many tests: If the neighbor size is O(n), where n is the number of nodes, algorithm needs to check exponentially many subsets of nodes in the conditioning set.</p>
<p>Case 2.b.j and beyond: Finally, if a ′′′ , c are k-covered , following a similar argument, either the node adjacent to a ′′′ along p (towards a) is separable with c, in which case following a similar argument as above would orient b ← * c, or we continue until a, c become adjacent.The latter cannot happen since that contradicts with the fact that a ⊥ ⊥ c |S .Thus, there must exist some node u along p that is separable from c, and the subpath of p between u, b is directed.Following the argument above, repeated application of Meek rules one and two will result in the orientation of the edge between b, c as b ← * c.This establishes that k-PC orients at least as much arrowheads as LOCI.</p>
<p>The corollary of this lemma is that k-PC orients all arrowheads oriented by LOCI: Therefore, the only non-trivial case is when only one of the two pairs is k-covered .For this case, since the pre-condition of Lemma D.1 is identical to the condition of LOCI to orient any edge.</p>
<p>LOCI applies the three Meek rules after orienting these arrowhead marks.Since k-PC repeatedly applies a set of Meek rules that include these three rules, k-PC orients at least as many arrowheads as LOCI.Thus, the corollary follows.</p>
<p>Next, we show that they both carry the same adjacency information.These results can be combined with the example in Figure 2 to show that the k-PC output carries strictly more information about the set of causal graphs that entail the set of degree-k d-separation statements than the output of LOCI, even though k-PC is not complete for learning our equivalence class as discussed in Section D.4.</p>
<p>D.7 Comparison of k-PC Output to AnytimeFCI Output</p>
<p>The undirected edges in our representation do not represent selection bias.We use them to represent a different graph union operation.The lack of latents gives us this flexibility, which allows us to distinguish different sets of graphs by using both -and o-o edge marks for this.</p>
<p>For a concrete example on why AnytimeFCI is less informative than k-PC in our setting, consider the causal graph in Figure 2. Here, k-PC will learn the representation on the right: We can infer that either a causes d or d causes a, noted by the undirected edge a -d in our representation, rather than the circle edge o-o, which would allow a, d to be non-adjacent in some DAGs.Anytime FCI here will instead output circle edges between a and d, i.e., ao-od.Even if we try to incorporate the knowledge that the underlying graph is causally sufficient, it is not obvious how one can conclude that a and d must be adjacent in every DAG that induces the same degree-k d-separation statements from the Anytime FCI output.One way to do this would be to remove the edge from this PAG and check whether a, d can now be d-separated by some conditioning set of size at most k.If yes, this changes the equivalence class, which means the edge must be there in any DAG.Our local rule R12 can instead orient this as an undirected edge, signifying that a and d must be adjacent in every underlying DAG by leveraging our equivalence class representation, without having to run this type of post-processing which might be computationally intensive for larger graphs.</p>
<p>Definition 2 . 3 (
23
(Unshielded) Collider).A path of three nodes (a, c, b) is a collider if the edges adjacent to c along the path are into c.A collider is called an unshielded collider if in addition the endpoints of the path a, b are non-adjacent.</p>
<p>Figure 1 :
1
Figure 1: (a), (b): Both D 1 and D 2 entail the same degree-1 CI relations although they have different skeletons, thus they are in the same k-Markov equivalence class.(c), (d): Both D 3 and D 4 entail the same degree-1 CI relations although they have different unshielded colliders, thus they are in the same k-Markov equivalence class.</p>
<p>Definition 3 . 3 .
33
Given a DAG D = (V, E) and an integer k, a pair of nodes a, b are said to be k-covered if ∄c ⊂ V : |c| ≤ k and a ⊥ ⊥ b |c .</p>
<p>Lemma 3 . 5 .
35
Given a DAG D and an integer k, the k-closure graph C k (D) is unique.</p>
<p>Lemma 3 . 6 .
36
k-closure C k (D) of a DAG D entails the same degree-k d-separation statements as the DAG, i.e., (a ⊥ ⊥ b |c ) D ⇐⇒ (a ⊥ ⊥ b |c ) C k (D) , ∀c ⊂ V : |c| ≤ k.Proof Sketch.The proof relies on two crucial lemmas.If a ↔ b in C k (D), then in D, conditioned on any subset of size at most k, there is a d-connecting path with arrowheads on both a and b.Similarly, if a → b in C k (D), then in D, conditioned on any subset of size at most k there is a d-connecting path with an arrowhead at b.Using these, we can show that any d-connecting path in C k (D) implies a d-connecting path in D. The other direction is straightforward as the d-connection statements in D hold in C k (D) since it is obtained from D by adding edges.Please see Section A.1 for the proof.Inspired by ancestral graphs [15], k-closure graphs are mixed graphs with directed and bidirected edges, where k-covered pairs in the DAG are made adjacent in C k (D) based on their ancestrality.Definition 3.7.A mixed graph is a graph that contains directed edges a → b, a ← b or bidirected edges a ↔ b where every pair of nodes is connected by at most one edge.</p>
<p>Figure 2 :Corollary 3 . 13 .
2313
Figure 2: Two k-Markov equivalent DAGs for k = 0 with the same k-essential graph.D 1 ∼ k D 2 for k = 0. Thus, C k (D 1 ) ∼ C k (D 2 ).Thus, they have the same k-essential graphs ε k (D 1 ) = ε k (D 2 ) = ε k , obtained as the edge union of their k-closures.Note that there are no Markov equivalent k-closures, where a, d are connected with a bidirected edge since removing that edge from the k-closure graph would make a, d separable by empty set, which means it would not be a valid k-closure graph by Theorem 3.9.Thus, a, d is connected via undirected edge.Similarly, there is no Markov equivalent k-closure where c ↔ b since c, b would not be k-covered in any Markov equivalent k-closure graph.</p>
<p>Theorem 3 . 14 .
314
Two DAGs D 1 , D 2 are k-Markov equivalent if and only if C k (D 1 ) and C k (D 2 ) are Markov equivalent.</p>
<p>Definition 3 .
3
16 (k-essential graph).For any DAG D, the edge union of all k-closure graphs that are Markov equivalent to C k (D) is called the k-essential graph 1 of D, shown by ε k (D).</p>
<p>will contain the edge c o-o d since c → d, c ← d, c ↔ d are all possible edges in different Markov equivalent k-closures.Figure 9 in Section D.</p>
<p>Lemma 3 .
3
18. ε k (D) ⊆ PAG(C k (D)).</p>
<p>Step 3 : 5 :
35
Orient unshielded colliders of K: For any induced subgraph a o-o c o-o b, set a o→ c ←o b for any non-adjacent pair a, b where S a,b does not contain c.Step 4: K ← FCI Orient(K) # See Algorithm 2 in Section B. Step For any node a that has no incoming edges (i.e., a ←, a ↔, a ←o) construct the sets B, C: B = {b ∈ N e(a) : ao→ b}, C = {c ∈ N e(a) : ao-oc} and define sets B * as the set of nodes that are non-adjacent to any of the nodes in C and C * as the set of nodes that are non-adjacent to other nodes in C: B * = {b ∈ B : b, c are non-adjacent ∀c ∈ C}, C * = {c ′ ∈ C : c ′ , c are non-adjacent ∀c ′ ̸ = c, c ′ ∈ C} • R11: Orient ao→ b as a → b, ∀b ∈ B * .</p>
<p>Corollary 4 . 2 .
42
k-PC without Step 5 is sound and complete for learning PAG(C k (D)) of any DAG D.</p>
<p>Theorem 4 . 4 .
44
k-PC algorithm is sound for learning k-essential graph given a conditional independence oracle under the causal Markov and k-faithfulness assumptions, i.e., if k</p>
<p>Figure 3 :
3
Figure 3: Empirical cumulative distribution function of various F 1 scores on 100 random DAGs on 10 nodes.For each DAG, conditional probability tables are independently and uniformly randomly filled from the corresponding probability simplex.Three datasets are sampled per model instance.The lower the curve the better.The maximum number of edges is 15.N is the number of data samples.</p>
<p>Lemma A. 3 .
3
Consider a DAG D where X / ∈ An(Y ), X, Y are non-adjacent and k-covered.Then conditioned on any subset T : |T | ≤ k, there exists a d-connecting path between X, Y that starts with an arrow into X.</p>
<p>Definition A. 8 .
8
A path between two nodes a, b is called open given c if each collider along the path is in An(c) and each non-collider is not in c.</p>
<p>Lemma A. 10 .
10
The set of ancestors of any set c of nodes in C k (D) are identical to the set of ancestors of c in C k (D).</p>
<p>6 .
6
Proof of Lemma 3.6: Since no edge is removed during the construction of the k-closure, one direction immediately follows: If a ⊥ ⊥ b |c in C k (D), then a ⊥ ⊥ b |c in D. The implication is clearly true for any set c of size at most k as well.Therefore we only need to show the other direction.Suppose a ⊥ ⊥ b |c in D where |c| ≤ k.We will show that a ⊥ ⊥ b |c in C k (D).</p>
<p>Definition A. 11 (
11
[24]).A directed path p from a to b and the edge a ↔ b is called an almost directed cycle.Lemma A.12.For any DAG D, and integer k, C k (D) does not have directed or almost directed cycles.Proof.Suppose, for the sake of contradiction that there is a directed cycle in C k (D).Since each edge X → Y in C k (D) either exists in D or for each such edge in C k (D), there is a directed path from X to Y in D, existence of a directed cycle in C k (D) would imply a directed cycle in D, which contradicts with the DAG assumption of D.</p>
<p>A. 5
5
Proof of Corollary 3.13 (⇒) If they are Markov equivalent then by Lemma 3.8 they are two Markov equivalent MAGs.Therefore by Theorem 3.11 they have the same skeleton, and the same unshielded colliders.(⇐) If they have the same skeleton and the same unshielded colliders, then by Lemma 3.12, they must have the same colliders along discriminating paths.Thus, by Theorem 3.11 they are equivalent.A.6 Proof of Theorem 3.14 (⇒) Suppose D 1 , D 2 are k-Markov equivalent.For the sake of contradiction suppose that C k (D 1 ) and C k (D 2 ) are not Markov equivalent.By Corollary 3.13, this happens when either they have different skeletons, or different unshielded colliders.Thus, there are two cases: k-closures have different skeletons: C k (D 1 ) and C k (D 2 ) have different skeletons.Suppose without loss of generality that C k (D 1 ) has an extra edge, i.e., a, b are adjacent in C k (D 1 ) but not in C k (D 2 ).This can only happen if ∃S ⊂ V : |S| ≤ k such that (a ⊥ ⊥ b |S ) D2 , while there is no such separating set in D 1 , implying that (a ̸⊥ ⊥ b |S ) D1 .This is a contradiction with the supposition that D 1 , D 2 are k-Markov equivalent.Therefore, C k (D 1 ) and C k (D 2 ) must have the same skeletons.</p>
<p>Definition A. 16 .
16
A triple ⟨a, c, b⟩ in a k-closure graph is called an unshielded collider if a, b are non-adjacent, a, c and c, b are adjacent and the edges adjacent to c have an arrowhead mark at c.According to the definition, a triple ⟨a, c, b⟩ in a k-closure graph C k (D) can be an unshielded collider if the induced subgraph on the nodes take either of the following configurations: 1. a → c ← b 2. a → c ↔ b 3. a ↔ c ← b 4. a ↔ c ↔ b We use asterisk to represent either an arrowhead or a tail.* → represents either →, ↔.Similarly, ← * represents either ← or ↔. k-closures have different unshielded colliders: Without loss of generality assume that (a * → c ← * b) C k (D1) but this unshielded collider does not exist in C k (D 2 ), i.e., ⟨a, c, b⟩ is an unshielded non-collider in C k (D 2 ).Lemma A.17.In a k-closure graph C k (D), two nodes a, b are non-adjacent iff (a ⊥ ⊥ b |S ) C k (D) for some S ⊂ V .Proof.(⇒) Suppose a, b are non-adjacent in C k (D).Thus it must be the case that (a ⊥ ⊥ b |S ) D for some S such that |S| ≤ k, since otherwise a, b would be made adjacent during the construction of C k (D).By Lemma 3.6, this means (a ⊥ ⊥ b |S ) C k (D) .This establishes the only if direction.(⇐) Suppose now that (a ⊥ ⊥ b |S ) C k (D) .By definition of d-separation, adjacent nodes cannot be dseparated and thus a, b must be non-adjacent in C k (D).Lemma A.18.In a k-closure graph C k (D), any pair of non-adjacent nodes a, b are separable by a set of size at most k, i.e., ∃S : |S| ≤ k, (a ⊥ ⊥ b |S ) C k (D) .Proof.Suppose otherwise: For some non-adjacent pair a, b all d-separating sets in C k (D) have size greater than k.Let S be the smallest subset that makes a, b d-separated, i.e., (a ⊥ ⊥ b |S ) C k (D) -one exists by Lemma A.17. Clearly, |S| &gt; k.Note that non-adjacency of a, b in C k (D) implies that a, b are separable in D with some set T of size at most k: (a ⊥ ⊥ b |T ) D .Since |T | ≤ k &lt; |S| and S is the smallest subset that d-separates a, b in C k (D), it must be that (a ̸⊥ ⊥ b |T ) C k (D) .However, this contradicts with Lemma 3.6 which says that D and C k (D) must entail the same d-separation constraints for conditioning sets of size up to k.Since a, b are non-adjacent in both graphs, by Lemma A.18 there are two subsets S 1 , S 2 of size at most k such that (a ⊥ ⊥ b |S 1 ) C k (D1) , (a ⊥ ⊥ b |S 2 ) C k (D2) .</p>
<p>(a ⊥ ⊥ b |S 1 )
1
D1 , (a ̸⊥ ⊥ b |S 1 ) D2 (29) This implies that D 1 , D 2 are not k-Markov equivalent which is a contradiction.This establishes that if D 1 , D 2 are k-Markov equivalent then C k (D 1 ), C k (D 2 ) must have the same skeleton and the same unshielded colliders.By Corollary 3.13, C k (D 1 ), C k (D 2 ) are Markov equivalent.(⇐) Suppose that C k (D 1 ) and C k (D 2 ) are Markov equivalent.Then they impose the same d-separation statements.Therefore they impose the same d-separation statements when the conditioning set is restricted to size at most k.By Lemma 3.6, this means that D 1 , D 2 must also impose the same d-separation statements for conditioning sets of size of at most k.This establishes that D 1 , D 2 are k-Markov equivalent.</p>
<p>Therefore, C k (D ′ ) cannot be Markov equivalent to C k (D), which is a contradiction.Now consider the execution of the algorithm k-PC .When the algorithm completes Step 4, from Corollary 4.2 we have that K = PAG(C k (D)).Since we start Step 5 with K = P AG(C k (D)), from Lemma A.19 and A.20, any arrowhead and tail orientation of the K obtained at the end of step 5 must be consistent with the k-essential graph of D. Therefore, we have that ε k (D) ⊆ K.</p>
<p>Definition B. 1 (
1
Partial Mixed Graph (PMG)).Any graph that contains the edge marks arrowhead, tail, circle is called a partial mixed graph (PMG).Definition B.2 (Uncovered path).In a PMG, a path ⟨u 1 , u 2 . . .u m ⟩ is called an uncovered path if u i , u i+2 are non-adjacent for all i ∈ {1, 2, . . . .m − 2}.Definition B.3 (Potentially directed path).In a PMG, a path ⟨u 1 , u 2 . . .u m ⟩ is called a potentially directed (p.d.) path if the edge between u i and u i+1 does not have an arrowhead at u i for all i ∈ {1, 2, . . . .m − 1}.Definition B.4 (Circle path).In a PMG, a path ⟨u 1 , . . .u m ⟩ is called a circle path if u i o-ou i+1 for all i ∈ {1, 2, . . . .m − 1}.Note that circle paths are special cases of p.d. paths.Rules 1, 2, 3 are straightforward extensions of the orientation rules for constraint-based learning to mixed graphs.For completeness, we restate them below.The star marks that appear both before and after the application of the rules are edge marks that remain unchanged by the rule.R1: If a * → bo- * c, and a, c are not adjacent, then orient the triple as a * → b → c.R2: If a → b * → c or a * → b → c and a * -oc, then orient a * -oc as a * → c.R3: If a * → b ← * c, a * -odo- * c, a, c are non-adjacent, and d * -ob then orient d * -ob are d * → b.We now restate FCI+ rules 3 R8, R9, R10 and explain their relevance for learning k-closure graphs.Note that the rules are simplified since we do not have undirected edges that represent selection bias, and our undirected edges are treated as if they are circle edges.R8: If a → b → c and ao→ c, orient ao→ c as a → c.R9: If ao→ c, and p = ⟨a, b, u 1 , u 2 . . .u m , c⟩ is an uncovered p.d. path from a to c such that b, c are non-adjacent, then orient ao→ c as a → c.R10: Suppose ao→ c, b → c, d → c, p 1 is an uncovered p.d. path from a to d and p 2 is an uncovered p.d. path from a to b.Let t d be the node adjacent to a on p 1 (t d can be d) and t c be the node adjacent to a on p 2 (t c can be c).If t d , t c are distinct and non-adjacent, then orient ao→ c as a → c.</p>
<p>Consider the figures below for two sample runs of k-PC algorithm.Note that k-PC outputs the k-essential graph in these examples, i.e., it can orient every invariant arrowhead and tail mark in the k-closure graph of DC k (D ′ ) is consistent with the output of k-PC .</p>
<p>Figure 5 :
5
Figure 5: An example where R11 helps orient tails.(a) A DAG D. (b) k-closure graph of D for k = 0. (c) K after Step 4 of k−P C, the same as P AG(C k (D)).(d) R11 helps orient several tail edges.(e) A DAG D ′ that is k-Markov to D. (f ) k-closure graph of D ′ , which is Markov equivalent to C k (D), showing that the circle at bo→ c is not an invariant tail.Thus k-PC outputs k-essential graph ε k (D) in this case.</p>
<p>Figure 6 :
6
Figure 6: (a) A causal graph D. (b) k-closure of D for k = 0. (c) K at the end of Step 4. (d) Node a has one edge ao-o, ao-ob.Thus we have C = C * since b is non-adjacent to any other nodes in C since there are no other nodes in C. Thus it is oriented as a-b due to R12. e → c is oriented due to R11, similarly since C = ∅ and the node c is trivially non-adjacent to all nodes in C. (e, f ) D ′ , D ′′ are k-Markov equivalent to D and their k-closure graphs contain a ↔ c and b ↔ c, respectively.This shows that the graph in (d) given by k-PC is the k-essential graph ε k (D).</p>
<p>Figure 7 : 2 (
72
Figure 7: As a collider, d blocks the path (a, c, d, e, b) in D 3 , but not in D 4 .Accordingly, (a ⊥ ⊥ b) D1 (a ̸ ⊥ ⊥ b) D2 and D 1 , D 2 are not k-Markov for k = 0.However, this does not appear as a local condition since c, e are not separable by conditioning sets of size up to 0 in both graphs.Therefore, a local characterization of equivalence like Verma and Pearl is not possible when we are only allowed to check degree-k d-separation tests.</p>
<p>Figure 8 :
8
Figure 8: The mixed graph on the right is a valid k-closure graph for k = 1: It is the k-closure graph of the DAG in (a).However, it is not a valid k-closure graph for k = 2.Because after removing c ↔ d, it is possible to d-separate c, d by conditioning on u 1 , u 2 .</p>
<p>Figure 9 :
9
Figure9: A DAG with a size-1 k-Markov equivalence class for k = 1.Observe that C k (D) only has unshielded colliders and thus there is no other k-closure graph that is Markov equivalent.Thus, this k-closure is at the same time the k-essential graph of D and can be learned from data.In this case, we can learn c, e do not cause each other despite not being separable in the data.</p>
<p>d may have an incoming edge that prevents us from eliminating the possibility that do→ b is a bidirected edge d ↔ b.However, we can only have a single d-connecting path between d, b.Since d is a non-collider along a-d-c, one of the edges must be out of d.Suppose d ← a and d → c.For c to not block this path it has to be a non-collider, and thus c → b.This is the only way we can have two d-connecting paths between d, b in the underlying DAG.However, now we have the path d → c → b, which makes d an ancestor of b.Therefore the edge d ↔ b is inconsistent.Thus in k-essential graph of D, we must have a tail at d as d → b.This cannot be learned by k-PC .</p>
<p>Figure 10 :
10
Figure 10: A graph where k-essential graph contains an invariant tail that cannot be learned by k-PC algorithm.k = 1; thus d, b are k-covered but not a, c, which gives the k-closure graph on the right.k-PC algorithm outputs the graph in (c).However, there is no k-closure in the Markov equivalence class where d ↔ b.Thus the edge in the k-essential graph should be d → b.Similarly, ao→ b should be a → b, however this requires reasoning about the number of d-connecting paths between a, b in K that is not captured in k-PC .</p>
<p>Corollary D. 2 .
2
Any arrowhead oriented by LOCI is also oriented by k-PC .Proof.Suppose a ̸⊥ ⊥ b |S , b ̸⊥ ⊥ c |S , a ⊥ ⊥ c |S for some set S : |S| ≤ k and b / ∈ S. Observe that if a, b and b, c are k-covered then k-PC would orient the edges between them as a * → b ← * c due to the independence statement a ⊥ ⊥ c |S .Moreover, if a, b and b, c are both separable by some sets of size at most k, then both LOCI and k-PC would make a, b and b, c non-adjacent and thus neither algorithm orients an edge between them.</p>
<p>Corollary D. 3 .
3
Any pair that is non-adjacent in LOCI output is either non-adjacent, or adjacent via a bidirected edge in k-PC output.Proof.LOCI makes a pair non-adjacent in two ways.If LOCI makes a pair non-adjacent since they are separable, k-PC also will make them non-adjacent.Suppose LOCI makes a pair a, b non-adjacent due to the following CI pattern, which are otherwise k-covered :u ̸⊥ ⊥ a |S 1 , a ̸⊥ ⊥ b |S 1 , u ⊥ ⊥ b |S 1 and a ̸⊥ ⊥ b |S 2 , b ̸⊥ ⊥ v |S 2 , a ⊥ ⊥ v |S 2 forsome u, v, S 1 , S 2 .Note that by Lemma D.1, k-PC marks both endpoints of the edge between a, b as arrowheads.This concludes the proof.</p>
<p>The reader will notice k-essential graph visually resembles a partial ancestral graph (PAG)[25] more than an essential graph due to the circle marks. Our choice of the name k-essential is motivated by the fact that we assume no latent confounders in the system and thus it positions our work better with respect to the related work.
It is worth noting that FCI uses undirected edges to represent selection bias. We use undirected edges for a different purpose. Thus, orientation rules of FCI aimed at orienting undirected edges should not be used here.
This version was originally called A-FCI, short for augmented FCI rules by[25]. Augmented graphs are recently used in a different context in the causality literature, which is why in this work we are calling this version FCI+ to avoid confusion.
AcknowledgementsThis research has been supported in part by NSF CAREER 2239375.We would like to thank Marcel Wienöbst for his insightful comments and suggestions on an earlier version of the manuscript that led to a more thorough comparison with LOCI[23].We would like to also thank the anonymous NeurIPS'23 reviewers for their constructive comments and feedback during the review process.A.3 Proof of Theorem 3.9Our main observation is that a parallel of Lemma A.2 works for MAGs with k-covered bidirected edges.The following lemmas are for any mixed graph K that satisfies the constraints in Theorem 3.9, i.e., those that are MAGs and that satisfy the condition that for any bidirected edge a ↔ b, a, b are k-covered in the graph K − (a ↔ b).Lemma A.13.Suppose X / ∈ An(Y ).Suppose there is a d-connecting path p between X, Y given T that starts with an arrow out of X.1.There is at least one collider along p.2. Let K be the collider closest to X on p. Then conditioning on T ′ = T − De(K) instead of T does not introduce new d-connecting paths that start with an arrowhead at X.Proof. 1.Any path that starts with X → . . .must either be directed, or there must be at least one collider along the path.Since the path is between X, Y and X / ∈ An(Y ), it must be that the path has at least one collider on it.2. First note that without loss of generality, p has the following form for some integer m ≥ 0 (m = 0 means X → K):* is a wildcard representing either an arrowhead or a tail.Suppose for the sake of contradiction that conditioned on T ′ , there is a new d-connecting path q that starts with an arrowhead into X.q was clearly closed conditioned on T and became open by us removing nodes from the conditioning set T .This can only happen if we removed some node from T that is a non-collider along q.Consider the non-collider we removed that was closest to X, call this M .Thus we have the path q that looks like this:where M is a non-collider on this path and is in De(K).We observe that the subpath between M and X cannot be directed from M to X.Because this would create the following cycle, since K is assumed to be the first collider along p, and an ancestor of M .Thus a closer look at the path q reveals the following structure for some integer m ′ and node V :We will consider the following two cases: The edge mark adjacent to M along the subpath between W m ′ and M is a tail or an arrowhead.Suppose the edge mark adjacent to M along the subpath between W m ′ and M is a tail: This means there is at least one collider between W m ′ and M .Any such collider must be active since q is active given T ′ .Consider the collider that is closest to M .Since it is active, this collider must be an ancestor of T ′ .However, observe that K is an ancestor of this collider which implies that K is an ancestor of T ′ as well.However, we obtained T ′ by removing all descendants of K from T , which is a contradiction.This establishes that the edge mark adjacent to M along the subpath between W m ′ and M is an arrowhead.Thus, this reveals the following structure for q:Suppose the directed path from K to M is as follows for some {θ i } i for some integer m ′′ :Recall that M is a non-collider along q.Thus, the subpath of q between M and Y must start with a tail as M → . . .Y .Now observe that if the subpath of p between M and Y had no collider, then we would have the following directed path from X to Y :A.7 Proof of Lemma 4.1Suppose in the k-closure graph C k (D) for some DAG D and integer k, we have a discriminating path for Y between the nodes a, v of the form a * →↔ . . .By definition of discriminating path, u must be a collider along the path, and u → v.If Y ← v, then we would have an almost directed cycle u → v → Y ↔ u.Thus, we have u ↔ Y ↔ v.First, we show that the arrowhead at Y of the edge Y ↔ v can be learned by first orienting unshielded colliders and then applying R1 and R2.Consider the bidirected edge u ↔ Y .By definition of discriminating path a, v must be non-adjacent and thus separable by a set of size at most k by Lemma A.18.Therefore, we have a set S : |S| ≤ k such that a ⊥ ⊥ v |S .By the discriminating path definition, every collider along the path must be a parent of v, and therefore it must be the case that every collider along the discriminating path including u must be in S, since otherwise there would be a d-connecting path.From Lemma A.2, conditioned on any set of size at most k, we have a d-connecting path that starts with an edge into Y .Consider the shortest such path q.Let X be the node immediately before Y along q.Since this path exists in the DAG by the lemma, we have X → Y .If X, v are non-adjacent, then X → Y ↔ v would be an unshielded collider and we are done.Suppose X, v are adjacent.Note that conditioned on S, a and X are d-connected.If X is a non-collider along the path obtained by concatenating the subpath of q between a, X and the edge between X, v, then a, v would be d-connected given S, which is a contradiction.Therefore, X must be a collider along this path.Thus we have X ↔ v.Note that we cannot have X ← v since this would create an almost directed cycle in C k (D).Let V be the node immediately before X along q.Thus we have V → X ↔ v (not V ↔ X since this edge exists in D).Suppose V, v are non-adjacent.Thus, the collider V → X ← * v is unshielded, and therefore can be learned.Furthermore, V, Y must be non-adjacent since otherwise, we must have V → Y to avoid a cycle and there would be a path that is shorter than q, which "jumps over" the node X along q.Thus, we can learn that X → Y from R1. Finally, since now we have learned v * → X → Y and that Y, v are adjacent, by R2, we must have Y ← * v. Thus the arrowhead mark at Y of the edge Y ↔ v can be learned, and we are done.Suppose V, v are adjacent.Following a similar argument, we either have some unshielded collider that can be propagated using the argument above to orient Y ← * v, or we can continue covering unshielded colliders, which would imply the previous nodes are always parents along q.But this implies that u has a directed path to Y , which cannot happen since we have u ↔ Y in C k (D).This establishes that Y ← * v can be learned by orienting unshielded colliders and applying the rules R1 and R2.For the arrowhead mark at Y of the edge u ↔ Y , similarly consider the shortest d-connecting path q between Y, v in D given S that starts with an arrow into Y .The argument follows similarly that either there would be directed path from v to Y , which is a contradiction with the existence of the edge Y ↔ v in the k-closure graph, or that there exists an unshielded collider along q that can be learned by orienting unshielded colliders, which can be propagated to learn u * → Y using rules R1, and R2.This establishes the lemma.To prevent this issue, several implementations of these algorithms have the added functionality to restrict this search by limiting the size of the conditioning set.For example, causal-learn package has this functionality.However, this is a heuristic that simply prematurely stops the search algorithms.The results of our paper build the theoretical understanding of what is learnable in this setting with a new equivalence class and its graphical representation.Proof.In the following, we show that given the CI pattern that LOCI uses to orient edges, k-PC also orients the same edges.Consider the CI pattern that LOCI uses between three nodes a, b, c.Conditioned on S, there is a d-connecting path between a, b; let us call this path p. Conditioned on S, there is a d-connecting path between b, c; let us call this path q.Consider the path obtained by concatenating p, q.Since this path must be d-separating, b must be a collider on it and it must be the case that b / ∈ An(S).Let the node that is adjacent to b along p be a ′ and the node adjacent to b along q be c ′ .Thus we haveCase 1: Now suppose that a ′ and c are separable by some T : |T | ≤ k. k-PC would then orient a ′ * → b ← * c since b, c remains adjacent throughout the execution of k-PC .Thus b ← * c would be oriented in this case.Case 2: Suppose that a ′ , c are k-covered .Then, given S, there is a d-connecting path between a ′ , c. Now consider the path obtained by concatenating the subpath of p between a, a ′ and this d-connecting path.Since a, c are d-separated given S, a ′ must be a collider along this path and a non-ancestor of S. Thus we must have a ′′ → a ′ → b as the last three nodes of path p. Case 2.b: Now suppose a ′′ , c are k-covered .Thus, there must be a d-connecting path between a ′′ , c given S. Now consider the path obtained by concatenating the subpath of p between a, a ′′ and this d-connecting path.Since a ⊥ ⊥ c |S , it must be that a ′′ is a collider along this path and that a ′′ / ∈ An(S).Thus, along this path we have a ′′′ → a ′′ ← . ... Therefore, the last four nodes of the path p is a ′′′ → a ′′ → a ′ → b, and a ′ , c are k-covered , a ′′ , c are k-covered .We pick the parents of each node in a fixed total order randomly so that the expected value of parents is 3.The conditional distribution of every node given any configuration of its parent set is sampled independently, uniformly randomly from the corresponding dimensional probability simplex.Three datasets are sampled per instance.Performance of k-PC and LOCI[23]are similar as expected.We still observe a similar trend as PC that arrowhead score of k-PC is better.For tail accuracy, the result depends on the value of k.For small k, LOCI outperforms k-PC whereas for larger k, k-PC outperforms LOCI in the small sample regime.Different from Figure3, we compare the outputs to the true DAG instead of essential graph since no algorithm in this comparison can achieve essential graph.For each DAG, a linear SCM is sampled as follows: Each coefficient is chosen randomly in the range [−3, 3].Exogenous noise terms are jointly independent unit Gaussian.Performance of k-PC vs. NOTEARS[28].We observe a similar trend as PC.NOTEARS is slightly better than PC consistently.Despite this, k-PC outperforms both in the low-sample regime.Metrics are computed against the true DAG.E.2 Experiments vs. NOTEARSE.3 More Experiments vs. PCIn this section, we show a larger range of N (number of samples).We also explore the behaivor for graphs with different edge densities and higher number of nodes(10).Next, we present combined metrics for this same setup.Namely, we show the advantage of our algorithm in terms of the sum of arrowhead and tail F 1 scores, and the sum of arrowhead, tail and skeleton F 1 scores.For each DAG, conditional probability tables are independently and uniformly randomly filled from the corresponding probability simplex.Three datasets are sampled per instance.The lower the curve the better.The maximum number of edges is 30.Even in the extreme case of just 10 samples (10 node-graphs), k-PC for k = 0 provides improvement to all scores.k should be gradually increased as more samples are available to make best use of the available data.For example, for 1000 samples, k = 2 provides the best arrowhead score while not giving up as much tail score as k = 0.For each DAG, conditional probability tables are independently and uniformly randomly filled from the corresponding probability simplex.One dataset is sampled per instance.The lower the curve the better.The maximum number of edges is 15.k-PC maintains an advantage against conservative PC in the arrowhead and skeleton F 1 scores in the low-sample regime.
Causal inference and the data-fusion problem. Elias Bareinboim, Judea Pearl, Proceedings of the National Academy of Sciences. 113272016</p>
<p>Improving the reliability of causal discovery from small data sets using argumentation. Facundo Bromberg, Dimitris Margaritis, Journal of Machine Learning Research. 1022009</p>
<p>Optimal structure identification with greedy search. David Maxwell, Chickering , Journal of machine learning research. 3Nov. 2002</p>
<p>Statistically efficient greedy equivalence search. Max Chickering, Conference on Uncertainty in Artificial Intelligence. PMLR2020</p>
<p>A modification of the pc algorithm yielding order-independent skeletons. Diego Colombo, H Marloes, Maathuis, CoRR, abs/1211.32952012</p>
<p>Learning highdimensional directed acyclic graphs with latent and selection variables. The Annals of Statistics. Diego Colombo, H Marloes, Markus Maathuis, Thomas S Kalisch, Richardson, 2012</p>
<p>Faster algorithms for markov equivalence. Zhongyi Hu, Robin Evans, Conference on Uncertainty in Artificial Intelligence. PMLR2020</p>
<p>Unifying markov properties for graphical models. Steffen Lauritzen, Kayvan Sadeghi, The Annals of Statistics. 4652018</p>
<p>Graphical models. Steffen L Lauritzen, 1996Clarendon Press17</p>
<p>Probabilistic reasoning in intelligent systems: networks of plausible inference. Judea Pearl, 1988Morgan kaufmann</p>
<p>Causal diagrams for empirical research. Judea Pearl, Biometrika. 1995</p>
<p>Probabilistic evaluation of sequential plans from causal models with hidden variables. Judea Pearl, James M Robins, arXiv:1302.49772013arXiv preprint</p>
<p>When can association graphs admit a causal interpretation. Judea Pearl, Nanny Wermuth, Selecting Models from Data: Artificial Intelligence and Statistics IV. 199489</p>
<p>Adjacency-faithfulness and conservative causal inference. Joseph Ramsey, Peter Spirtes, Jiji Zhang, Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence. the Twenty-Second Conference on Uncertainty in Artificial Intelligence2006</p>
<p>Ancestral graph markov models. The Annals of Statistics. Thomas Richardson, Peter Spirtes, 200230</p>
<p>The hardness of conditional independence testing and the generalised covariance measure. D Rajen, Jonas Shah, Peters, Annals of Statistics. 4832020</p>
<p>Complete identification methods for the causal hierarchy. Ilya Shpitser, Judea Pearl, Journal of Machine Learning Research. 92008</p>
<p>An anytime algorithm for causal inference. Peter Spirtes, International Workshop on Artificial Intelligence and Statistics. PMLR2001</p>
<p>Causation, prediction, and search. Peter Spirtes, Clark N Glymour, Richard Scheines, David Heckerman, 2000MIT press</p>
<p>Learning from pairwise marginal independencies. Johannes Textor, Alexander Idelberger, Maciej Liśkiewicz, Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence. the Thirty-First Conference on Uncertainty in Artificial Intelligence2015</p>
<p>On the testable implications of causal models with hidden variables. Jin Tian, Judea Pearl, Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence. the Eighteenth conference on Uncertainty in artificial intelligence2002</p>
<p>Equivalence of causal models. S Tom, Judea Verma, Pearl, 1990</p>
<p>Recovering causal structures from low-order conditional independencies. Marcel Wienöbst, Maciej Liskiewicz, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Causal reasoning with ancestral graphs. Jiji Zhang, Journal of Machine Learning Research. 92008</p>
<p>On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. Jiji Zhang, Artificial Intelligence. 1722008</p>
<p>Fairness in decision-making-the causal explanation formula. Junzhe Zhang, Elias Bareinboim, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Domain adaptation under target and conditional shift. Kun Zhang, Bernhard Schölkopf, Krikamol Muandet, Zhikun Wang, International Conference on Machine Learning. PMLR2013</p>
<p>Dags with no tears: Continuous optimization for structure learning. Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, Eric P Xing, Advances in neural information processing systems. 312018</p>            </div>
        </div>

    </div>
</body>
</html>
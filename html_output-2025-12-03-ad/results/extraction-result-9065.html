<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9065 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9065</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9065</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-93a9fe0ea501e83a8f2a73ceb6a0431671bc707a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/93a9fe0ea501e83a8f2a73ceb6a0431671bc707a" target="_blank">Large language models predict human sensory judgments across six modalities</a></p>
                <p><strong>Paper Venue:</strong> Scientific Reports</p>
                <p><strong>Paper TL;DR:</strong> Surprisingly, a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality, and provides highly correlated predictions with human data irrespective of whether direct visual input is provided or purely textual descriptors.</p>
                <p><strong>Paper Abstract:</strong> Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality, and provides highly correlated predictions with human data irrespective of whether direct visual input is provided or purely textual descriptors. To study the impact of specific languages, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interaction of language and perception.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9065.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9065.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent OpenAI ChatGPT-series large language model variant used in this study; reported to have been trained in a multimodal way (text + images) and probed without fine-tuning to elicit psychophysical judgments and color naming in multiple languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI ChatGPT-series LLM variant; in this paper described as trained on text and (unlike the other variants) in a multimodal fashion enabling access to images in training. Queried via the OpenAI Chat Completion API for similarity ratings and color naming tasks without any task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Psychophysical similarity judgments across six modalities (pitch, loudness, colors, consonants, taste, timbre) + color naming (English and Russian)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Pairwise similarity judgment tasks (numerical 0-1 scale mapped from human 0-6 Likert in behavioral data) across six perceptual domains — pitch (harmonic complex tones across two octaves), loudness (pure-tone loudness confusion matrix), colors (14–23 colors, hex codes), vocal consonants (16 IPA consonant recordings), taste (10 flavors confusion matrix), timbre (12 instrument timbres) — plus a forced-choice color naming task using 330 Munsell colors and a predefined list of 15 color names in English and Russian.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Pitch: r = 0.92 (95% CI [0.91, 0.92]); Colors (similarity): r = 0.89 (95% CI [0.87, 0.91]); Consonants: r = 0.57 (95% CI [0.56, 0.59]); (other modalities: reported correlations for GPT-4 were >0.6 for most models; exact loudness/taste/timbre values for GPT-4 not always singled out in main text). Color naming (Adjusted Rand Index): English ARI = 0.59 (95% CI [0.56,0.63]); Russian ARI = 0.54 (95% CI reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Inter-rater split-half reliability (IRR): Pitch IRR = 0.90 (95% CI [0.87,0.92]); Consonants IRR = 0.46 (95% CI [0.36,0.56]). Color naming (lab baselines from Lindsey & Brown): constrained naming ARI = 0.73 (95% CI [0.65,0.74]); free naming ARI = 0.75 (95% CI [0.66,0.75]). (For other modalities human numeric baselines were the empirical similarity matrices; exact single-number baselines not provided for all modalities.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On pitch, GPT-4 correlations (~0.92) match or reach human inter-rater reliability (IRR ~0.90), i.e., on par with human performance; for consonants GPT-4 (r~0.57) approaches the (lower) human IRR (0.46) and is described as comparable; for colors GPT-4 similarity correlations are high (r~0.89) and GPT-4 color naming is more human-like than GPT-3/3.5 but below controlled lab human ARI (0.73–0.75). Overall GPT-4 is among top two models in 5/6 domains and often performs best or near-best.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Models were prompted with few-shot templates containing three in-context example pairs and asked to return numerical similarity ratings on a 0–1 scale; for each stimulus pair, 10 ratings were elicited and averaged. Similarity prompt temperature = 0.7; color-naming: GPT-4 first used to elicit a 15-item basic color list (temperature = 0), then asked 10 times per Munsell color with the shuffled 15-item list (temperature = 0.7) to force-choice a single name; Russian-language prompts were translated and verified. Similarity datasets included direct ratings and confusion matrices converted to similarity (s_xy = sqrt(p_xy p_yx / (p_xx p_yy))). Evaluation metric for similarity matrices: Pearson correlation between flattened upper triangles; CIs computed via bootstrap over model predictions (1,000 reps). Color naming evaluated with adjusted Rand index vs human naming partitions; bootstrapped CIs computed by resampling responses per color.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Model sizes and precise training corpora are not reported in this paper. Although GPT-4 is multimodal, the paper finds no modality-specific advantage limited to colors (improvements appear across modalities), suggesting gains may derive from richer textual training rather than image grounding; authors caution that results provide a lower bound on perceptual information extractable from language, not proof of grounded perception. Comparisons are at the population-average level (not individual-level variability), some human data were collected online (less control than lab), and LLM outputs can inherit biases from training data. For several modalities human baseline numeric summaries beyond IRR for pitch/consonants and lab ARI for color naming were not provided, limiting fine-grained human-vs-LLM comparisons for every modality.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper explicitly states GPT-4 achieved IRR-level performance without fine-tuning on the newly collected pitch and vocal consonants datasets and that GPT-4 reproduced cross-linguistic color naming differences (English vs Russian).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9065.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9065.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (ChatGPT variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intermediate ChatGPT-series LLM variant (between GPT-3 and GPT-4) evaluated in this study on the same psychophysical similarity and color naming tasks as GPT-3 and GPT-4, queried via OpenAI Chat Completion API without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT-series LLM variant (GPT-3.5) used via the OpenAI Chat Completion API; trained on large text corpora (paper does not specify exact corpus or size). Used as a comparison point to GPT-3 and GPT-4 for psychophysical similarity and color naming.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Psychophysical similarity judgments across six modalities + color naming (English and Russian)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same as for GPT-4: pairwise similarity ratings across pitch, loudness, colors, consonants, taste, timbre; forced-choice color naming across 330 Munsell colors using a 15-item color list in English and Russian.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Loudness: reported high correlation r = 0.89 (95% CI [0.87,0.91]) attributed to GPT-3.5 in the main text; Taste: GPT-3.5 reported r = 0.54 (95% CI [0.48,0.61]); Timbre: one place in text reports timbre r = 0.42 for GPT-3.5 (95% CI [0.40,0.44]). Color naming (Adjusted Rand Index): English ARI = 0.50 (95% CI [0.46,0.52]); Russian ARI = 0.50 (95% CI [0.45,0.52]). (Across modalities, correlations for GPT-3.5 were generally >0.6 for many domains according to the paper's summary.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Same human baselines as reported: pitch IRR = 0.90 (95% CI [0.87,0.92]) and consonants IRR = 0.46 (95% CI [0.36,0.56]); color naming lab baselines (Lindsey & Brown) ARI = 0.73–0.75 (95% CIs reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 achieves strong correlations with human similarity judgments in several modalities (e.g., loudness r~0.89) but generally performs below GPT-4; on color naming GPT-3.5 is more human-like than GPT-3 but less so than GPT-4. Overall described as a mid-performing model relative to GPT-3 and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same prompt templates and procedure as described for GPT-4: few-shot prompts with three example pairs, 10 elicited ratings per stimulus pair averaged; temperature = 0.7 for similarity judgments; color-naming evaluations conducted with the 15-color list elicited from GPT-4 but responses for naming elicited from each model with repeated shuffling, 10 attempts per color, default temperature 0.7, with invalid outputs re-queried up to 10 times.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper does not report model size or detailed training corpora. Performance differences between GPT-3.5 and GPT-4 are not clearly attributable to multimodal training vs. richer textual data; authors suggest GPT-4 gains likely reflect richer textual training rather than image grounding. As with GPT-4, comparisons are at group-average level; some human baselines come from lab studies while others were collected online, complicating direct comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-3.5 is reported as having the top loudness correlation in the main text and moderate performance across other modalities; in color naming it outperforms GPT-3 but underperforms GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9065.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9065.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The original large transformer language model (GPT-3) used here as a baseline ChatGPT-series variant; evaluated without fine-tuning on the psychophysical similarity tasks and color naming.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 Chat-completion Text model variant used via OpenAI Text Completion API for similarity judgments (temperature and prompt templating as for other variants); trained on large text corpora (specifics and size not reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Psychophysical similarity judgments across six modalities + color naming (English and Russian)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same psychophysical similarity and color naming tasks as for GPT-4 and GPT-3.5 (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Color naming (Adjusted Rand Index): English ARI = 0.39 (95% CI [0.37,0.42]); Russian ARI = 0.35 (95% CI [0.29,0.35]). For similarity tasks the paper reports that GPT-3 correlations were generally >0.6 in most domains but numerically lower than GPT-3.5 and GPT-4 (exact per-modality r-values for GPT-3 not consistently enumerated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>As above: pitch IRR = 0.90 (95% CI [0.87,0.92]); consonants IRR = 0.46 (95% CI [0.36,0.56]); lab color naming ARI = 0.73–0.75 (Lindsey & Brown).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 performs worse than GPT-3.5 and GPT-4 on the color naming ARI and in similarity correlations; it is described as the lowest-performing of the three variants evaluated in the naming task.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>GPT-3 completions used the OpenAI Text Completion API with the same few-shot similarity prompt templates (three example pairs), 10 queries per pair, temperature = 0.7 for similarity judgments. Color naming used the same forced-choice list elicited from GPT-4; each Munsell color queried up to 10 times and invalid outputs re-queried up to 10 attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Model size and corpus specifics are not given in the paper. GPT-3's lower performance is reported but the paper emphasizes relative differences rather than exact causal attribution; as with other models, results reflect language-derived information and do not prove grounded sensory experience.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Used as the lower baseline among the three ChatGPT-series models in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can language models encode perceptual structure without grounding? a case study in color <em>(Rating: 2)</em></li>
                <li>How does chatgpt rate sound semantics? <em>(Rating: 2)</em></li>
                <li>Words are all you need? capturing human sensory similarity with textual descriptors <em>(Rating: 2)</em></li>
                <li>Can ai language models replace human participants? <em>(Rating: 2)</em></li>
                <li>Shared computational principles for language processing in humans and deep language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9065",
    "paper_id": "paper-93a9fe0ea501e83a8f2a73ceb6a0431671bc707a",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A recent OpenAI ChatGPT-series large language model variant used in this study; reported to have been trained in a multimodal way (text + images) and probed without fine-tuning to elicit psychophysical judgments and color naming in multiple languages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI ChatGPT-series LLM variant; in this paper described as trained on text and (unlike the other variants) in a multimodal fashion enabling access to images in training. Queried via the OpenAI Chat Completion API for similarity ratings and color naming tasks without any task-specific fine-tuning.",
            "model_size": null,
            "test_battery_name": "Psychophysical similarity judgments across six modalities (pitch, loudness, colors, consonants, taste, timbre) + color naming (English and Russian)",
            "test_description": "Pairwise similarity judgment tasks (numerical 0-1 scale mapped from human 0-6 Likert in behavioral data) across six perceptual domains — pitch (harmonic complex tones across two octaves), loudness (pure-tone loudness confusion matrix), colors (14–23 colors, hex codes), vocal consonants (16 IPA consonant recordings), taste (10 flavors confusion matrix), timbre (12 instrument timbres) — plus a forced-choice color naming task using 330 Munsell colors and a predefined list of 15 color names in English and Russian.",
            "llm_performance": "Pitch: r = 0.92 (95% CI [0.91, 0.92]); Colors (similarity): r = 0.89 (95% CI [0.87, 0.91]); Consonants: r = 0.57 (95% CI [0.56, 0.59]); (other modalities: reported correlations for GPT-4 were &gt;0.6 for most models; exact loudness/taste/timbre values for GPT-4 not always singled out in main text). Color naming (Adjusted Rand Index): English ARI = 0.59 (95% CI [0.56,0.63]); Russian ARI = 0.54 (95% CI reported in paper).",
            "human_baseline_performance": "Inter-rater split-half reliability (IRR): Pitch IRR = 0.90 (95% CI [0.87,0.92]); Consonants IRR = 0.46 (95% CI [0.36,0.56]). Color naming (lab baselines from Lindsey & Brown): constrained naming ARI = 0.73 (95% CI [0.65,0.74]); free naming ARI = 0.75 (95% CI [0.66,0.75]). (For other modalities human numeric baselines were the empirical similarity matrices; exact single-number baselines not provided for all modalities.)",
            "performance_comparison": "On pitch, GPT-4 correlations (~0.92) match or reach human inter-rater reliability (IRR ~0.90), i.e., on par with human performance; for consonants GPT-4 (r~0.57) approaches the (lower) human IRR (0.46) and is described as comparable; for colors GPT-4 similarity correlations are high (r~0.89) and GPT-4 color naming is more human-like than GPT-3/3.5 but below controlled lab human ARI (0.73–0.75). Overall GPT-4 is among top two models in 5/6 domains and often performs best or near-best.",
            "experimental_details": "Models were prompted with few-shot templates containing three in-context example pairs and asked to return numerical similarity ratings on a 0–1 scale; for each stimulus pair, 10 ratings were elicited and averaged. Similarity prompt temperature = 0.7; color-naming: GPT-4 first used to elicit a 15-item basic color list (temperature = 0), then asked 10 times per Munsell color with the shuffled 15-item list (temperature = 0.7) to force-choice a single name; Russian-language prompts were translated and verified. Similarity datasets included direct ratings and confusion matrices converted to similarity (s_xy = sqrt(p_xy p_yx / (p_xx p_yy))). Evaluation metric for similarity matrices: Pearson correlation between flattened upper triangles; CIs computed via bootstrap over model predictions (1,000 reps). Color naming evaluated with adjusted Rand index vs human naming partitions; bootstrapped CIs computed by resampling responses per color.",
            "limitations_or_caveats": "Model sizes and precise training corpora are not reported in this paper. Although GPT-4 is multimodal, the paper finds no modality-specific advantage limited to colors (improvements appear across modalities), suggesting gains may derive from richer textual training rather than image grounding; authors caution that results provide a lower bound on perceptual information extractable from language, not proof of grounded perception. Comparisons are at the population-average level (not individual-level variability), some human data were collected online (less control than lab), and LLM outputs can inherit biases from training data. For several modalities human baseline numeric summaries beyond IRR for pitch/consonants and lab ARI for color naming were not provided, limiting fine-grained human-vs-LLM comparisons for every modality.",
            "notes": "The paper explicitly states GPT-4 achieved IRR-level performance without fine-tuning on the newly collected pitch and vocal consonants datasets and that GPT-4 reproduced cross-linguistic color naming differences (English vs Russian).",
            "uuid": "e9065.0",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (ChatGPT variant)",
            "brief_description": "An intermediate ChatGPT-series LLM variant (between GPT-3 and GPT-4) evaluated in this study on the same psychophysical similarity and color naming tasks as GPT-3 and GPT-4, queried via OpenAI Chat Completion API without task-specific fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "ChatGPT-series LLM variant (GPT-3.5) used via the OpenAI Chat Completion API; trained on large text corpora (paper does not specify exact corpus or size). Used as a comparison point to GPT-3 and GPT-4 for psychophysical similarity and color naming.",
            "model_size": null,
            "test_battery_name": "Psychophysical similarity judgments across six modalities + color naming (English and Russian)",
            "test_description": "Same as for GPT-4: pairwise similarity ratings across pitch, loudness, colors, consonants, taste, timbre; forced-choice color naming across 330 Munsell colors using a 15-item color list in English and Russian.",
            "llm_performance": "Loudness: reported high correlation r = 0.89 (95% CI [0.87,0.91]) attributed to GPT-3.5 in the main text; Taste: GPT-3.5 reported r = 0.54 (95% CI [0.48,0.61]); Timbre: one place in text reports timbre r = 0.42 for GPT-3.5 (95% CI [0.40,0.44]). Color naming (Adjusted Rand Index): English ARI = 0.50 (95% CI [0.46,0.52]); Russian ARI = 0.50 (95% CI [0.45,0.52]). (Across modalities, correlations for GPT-3.5 were generally &gt;0.6 for many domains according to the paper's summary.)",
            "human_baseline_performance": "Same human baselines as reported: pitch IRR = 0.90 (95% CI [0.87,0.92]) and consonants IRR = 0.46 (95% CI [0.36,0.56]); color naming lab baselines (Lindsey & Brown) ARI = 0.73–0.75 (95% CIs reported in paper).",
            "performance_comparison": "GPT-3.5 achieves strong correlations with human similarity judgments in several modalities (e.g., loudness r~0.89) but generally performs below GPT-4; on color naming GPT-3.5 is more human-like than GPT-3 but less so than GPT-4. Overall described as a mid-performing model relative to GPT-3 and GPT-4.",
            "experimental_details": "Same prompt templates and procedure as described for GPT-4: few-shot prompts with three example pairs, 10 elicited ratings per stimulus pair averaged; temperature = 0.7 for similarity judgments; color-naming evaluations conducted with the 15-color list elicited from GPT-4 but responses for naming elicited from each model with repeated shuffling, 10 attempts per color, default temperature 0.7, with invalid outputs re-queried up to 10 times.",
            "limitations_or_caveats": "Paper does not report model size or detailed training corpora. Performance differences between GPT-3.5 and GPT-4 are not clearly attributable to multimodal training vs. richer textual data; authors suggest GPT-4 gains likely reflect richer textual training rather than image grounding. As with GPT-4, comparisons are at group-average level; some human baselines come from lab studies while others were collected online, complicating direct comparisons.",
            "notes": "GPT-3.5 is reported as having the top loudness correlation in the main text and moderate performance across other modalities; in color naming it outperforms GPT-3 but underperforms GPT-4.",
            "uuid": "e9065.1",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT-3",
            "name_full": "GPT-3",
            "brief_description": "The original large transformer language model (GPT-3) used here as a baseline ChatGPT-series variant; evaluated without fine-tuning on the psychophysical similarity tasks and color naming.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "GPT-3 Chat-completion Text model variant used via OpenAI Text Completion API for similarity judgments (temperature and prompt templating as for other variants); trained on large text corpora (specifics and size not reported in this paper).",
            "model_size": null,
            "test_battery_name": "Psychophysical similarity judgments across six modalities + color naming (English and Russian)",
            "test_description": "Same psychophysical similarity and color naming tasks as for GPT-4 and GPT-3.5 (see other entries).",
            "llm_performance": "Color naming (Adjusted Rand Index): English ARI = 0.39 (95% CI [0.37,0.42]); Russian ARI = 0.35 (95% CI [0.29,0.35]). For similarity tasks the paper reports that GPT-3 correlations were generally &gt;0.6 in most domains but numerically lower than GPT-3.5 and GPT-4 (exact per-modality r-values for GPT-3 not consistently enumerated in main text).",
            "human_baseline_performance": "As above: pitch IRR = 0.90 (95% CI [0.87,0.92]); consonants IRR = 0.46 (95% CI [0.36,0.56]); lab color naming ARI = 0.73–0.75 (Lindsey & Brown).",
            "performance_comparison": "GPT-3 performs worse than GPT-3.5 and GPT-4 on the color naming ARI and in similarity correlations; it is described as the lowest-performing of the three variants evaluated in the naming task.",
            "experimental_details": "GPT-3 completions used the OpenAI Text Completion API with the same few-shot similarity prompt templates (three example pairs), 10 queries per pair, temperature = 0.7 for similarity judgments. Color naming used the same forced-choice list elicited from GPT-4; each Munsell color queried up to 10 times and invalid outputs re-queried up to 10 attempts.",
            "limitations_or_caveats": "Model size and corpus specifics are not given in the paper. GPT-3's lower performance is reported but the paper emphasizes relative differences rather than exact causal attribution; as with other models, results reflect language-derived information and do not prove grounded sensory experience.",
            "notes": "Used as the lower baseline among the three ChatGPT-series models in this study.",
            "uuid": "e9065.2",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can language models encode perceptual structure without grounding? a case study in color",
            "rating": 2
        },
        {
            "paper_title": "How does chatgpt rate sound semantics?",
            "rating": 2
        },
        {
            "paper_title": "Words are all you need? capturing human sensory similarity with textual descriptors",
            "rating": 2
        },
        {
            "paper_title": "Can ai language models replace human participants?",
            "rating": 2
        },
        {
            "paper_title": "Shared computational principles for language processing in humans and deep language models",
            "rating": 1
        }
    ],
    "cost": 0.0125545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large language models predict human sensory judgments across six modalities</h1>
<p>Raja Marjieh ${ }^{\mathrm{a}, 1}$, Ilia Sucholutsky ${ }^{\mathrm{b}}$, Pol van Rijn ${ }^{\mathrm{c}}$, Nori Jacoby ${ }^{\mathrm{c}, 2}$, and Thomas L. Griffiths ${ }^{\mathrm{a}, 1 \mathrm{c}, 2}$<br>${ }^{a}$ Department of Psychology, Princeton University, USA; ${ }^{\text {b }}$ Department of Computer Science, Princeton University, USA; ${ }^{\text {c }}$ Max Planck Institute for Empirical Aesthetics, Germany</p>
<p>This manuscript was compiled on June 16, 2023</p>
<h4>Abstract</h4>
<p>Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality. To study the influence of specific languages on perception, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interaction of language and perception.</p>
<p>perception | representation | NLP | AI | cognitive science
Imagine that you were chosen to be part of an expedition aimed at studying a newly discovered alien species on a distant planet. Your task is to understand the perceptual system of that species. You arrive at the planet and, to your dismay, discover that its inhabitants are long departed and the only thing they left behind is a huge archive of text. How much of the perceptual world of that species can you recover based on text alone? Versions of this question have occupied philosophers for centuries $(1,2)$, and decades of psychological research are beginning to provide glimpses into the rich perceptual content of language and its influence on perception (3-11).</p>
<p>But how can the amount of information that language provides about perception be quantified? Here we propose to do so by eliciting psychophysical judgments from large language models (LLMs) such as GPT-3 and its recent "ChatGPT" variants GPT-3.5 and GPT-4 $(12,13)$. These models are trained on massive text corpora reflecting a substantial chunk of human language and can be queried in a way that is analogous to humans.</p>
<p>Recent research shows that LLMs can be used to study various aspects of cognition (14), including, language processing in the brain (15-17), perception (18-20), cross-modal alignment (21), and morality (22, 23). Here, we use the fact that they are trained on large amounts of language to gain insight into the classic problem of the relationship between language and perception: these models provide a lower bound on the amount of information about perceptual experience that can be extracted from language alone.</p>
<p>We explored this empirically across six modalities, namely, pitch, loudness, colors, consonants, taste, and timbre. Given a stimulus space (e.g., colors) and its stimulus specification (e.g.,
wavelengths or their corresponding hex-codes) we elicit pairwise similarity judgments in a direct analogy to the widespread paradigm of similarity in cognitive science (24) using a carefully crafted prompt that is given to the model to complete (Figure 1A; see Methods). Importantly, whereas four of the modalities were based on classical results from the literature (colors (25), loudness (26), timbre (27) and taste (28)), two human datasets (pitch and vocal consonants) were novel and thus were not part of the training set of the models.</p>
<p>It is worthwhile to note that, unlike the other models, GPT-4 was trained in a multi-modal approach, enabling it to access both written text (similar to the other two variants) and images. This allowed us to examine if the additional sensory information resulted in enhanced performance in the color modality relative to the other domains. Moreover, to further interrogate whether the LLMs' sensory representation was language-dependent, we tested whether they would behave differently in the presence of the same sensory information (a color hex-code), but respond in different languages. To that end, we conducted a color-naming task using a paradigm similar to that of (29) and the World Color Survey $(30,31)$, and constructed human and GPT-3, GPT-3.5, and GPT-4 color-naming maps in both English and Russian.</p>
<h2>Results</h2>
<p>Similarity study. For each dataset, we designed a tailored prompt template that could be filled in with in-context examples and the pair of target stimuli for which we would want the LLM to produce a similarity rating (see Methods and SI for full specification of the prompts and datasets). Across all domains, we elicited 10 ratings per pair of stimuli from each of the GPT models and then constructed aggregate similarity ratings by averaging. We then evaluated the resulting scores by correlating them with human data. The Pearson correlation coefficients between human data and model predictions are shown in Figure 1B (See SI for details regarding computing the correlations and CIs). We see that across all domains, the correlations were significant, and were particularly high for pitch $(r=.92,95 \% \mathrm{CI}[.91, .92]$ for GPT-4), loudness $(r=.89$, $95 \%$ CI $[.87, .91]$ for GPT-3.5), and colors $(r=.89,95 \%$ CI $[.87, .91]$ for GPT-4) (and $&gt;.6$ for all models), followed by moderate but highly significant correlations for consonants $(r=.57,95 \% \mathrm{CI}[.56, .59]$ for GPT-4), taste $(r=.54,95 \% \mathrm{CI}$ $[.48, .61]$ for GPT-3.5) and timbre $(r=.42,95 \% \mathrm{CI}[.40, .44]$</p>
<p>All authors contributed to the design of the experiments and editing the manuscript. Authors RM, NJ, and TLG conceptualized the project. Author IS conducted the LLM experiments. Authors RM and PVR conducted the human experiments. Authors RM, IS, PVR, and NJ analyzed the results.
The authors declare no competing interests.
${ }^{2}$ NJ contributed equally to this work with TLG.
${ }^{1}$ To whom correspondence should be addressed. E-mail: raja.marjieh@princeton.edu</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. A. Schematic of the LLM-based and human similarity judgment elicitation paradigms. B. Correlations between models and human data across six perceptual modalities, namely, pitch, loudness, colors, consonants, taste, and timbre (Pearson $r ; 95 \%$ CIs).
for GPT-3.5). For the two modalities for which we collected data, we could compare model performance to the inter-rater split-half reliability (IRR). The IRRs for pitch and consonants were $r=.90(95 \% \mathrm{CI}[.87, .92])$ and $r=.46(95 \% \mathrm{CI}[.36, .56])$, respectively, suggesting that the performance of GPT-4 is on par with human performance.</p>
<p>We also note that in five out of the six domains, GPT-4 was among the top two models. Interestingly, the improvement relative to the average performance of the other models happened across all modalities with the exception of timbre and loudness, and was not restricted or particularly large for the domain of colors (compare e.g. $\Delta r=.1595 \% \mathrm{CI}[.13, .16]$ for colors vs. $\Delta r=.1695 \% \mathrm{CI}[.15, .17]$ for pitch, $\Delta r=.16$ for taste $95 \% \mathrm{CI}[.12, .19]$, and $\Delta r=.1495 \% \mathrm{CI}[.12, .16]$ for consonants) suggesting that this improvement is driven by richer textual training in GPT-4 rather than the possibility of its inclusion of images in its training set as is currently being debated (18).</p>
<p>Next, to get a finer picture of the LLM-based judgments and see to what extent they reflect human representations, we performed the following analyses. Starting from the domain of pitch, we wanted to see to what extent the LLM data captures a well-known psychological phenomenon that Western listeners tend to associate particular musical intervals or ratios of frequencies (such as the octave or 2:1 frequency ratio) with enhanced similarity (32). To test this we computed the average similarity score over groups of pitch pairs that are separated by the same fixed interval (i.e., the same frequency ratios). Figure 2A shows the resulting average similarity per interval for the models and humans along with an example corresponding smoothed similarity matrix for GPT-3 (smoothing was done by averaging the raw similarity matrix over its sub-diagonals). We can see that apart from the decay as a function of separation (i.e., tones that are far apart in log frequency are perceived as increasingly dissimilar) there is a clear spike precisely at 12 semitones (octave), consistent with the aforementioned phenomenon of "octave equivalence" (33). Moreover, applying multi-dimensional scaling (MDS) (24) to the smoothed similarity matrix whereby the different stimuli are mapped into points in a Euclidean space (also known as "psychological space") such that similar stimuli are mapped to nearby points reveals a clear helical structure with twists that correspond to precisely 12 semitone separations (i.e., octaves) recovering the pitch spiral representation (Figure 2A).</p>
<p>Likewise, applying MDS to the domains of consonants and colors (Figure 2B) reveals highly interpretable representations, namely, the familiar color wheel and a production-based representation for consonants." As an additional test, we asked GPT-4 to provide explanations for the judgments it made (Figure 2C-D), and remarkably, the model resorted to explanations involving the octave, ratios, and harmonic relations for pitch, places of articulation in the vocal tract for consonants, and hue, brightness and color spectra for colors, consistent with the MDS solutions.</p>
<p>Color naming study. The results so far suggest that LLMs can use textual information to form perceptual representations. If this is indeed the case, we hypothesized that representations in LLMs using different languages could be different even in the presence of identical input. This would be consistent with cross-cultural differences in language and perception that were observed in humans (30). To test this, we propose for the first time to test LLMs on an explicit naming task proposed by the seminal work of (29) and further explored across cultures around the globe $(30,31)$.</p>
<p>We thus tested whether LLMs would yield different naming patterns of color hex codes depending on the language of the prompt used to elicit those names (see Methods; Figure 3). Specifically, we presented both humans and LLMs with different colors and asked them to perform a forced-choice naming task by selecting from a pre-specified list of 15 color names (see Methods). We specifically focused on English and Russian as test cases, since Russian speakers are documented to use richer vocabulary to describe what English speakers would otherwise describe as blue and purple $(29,34,35)$. We collected data from 103 native English speakers and 51 native Russian speakers and compared them against LLMs performing the same task, and to the in-lab data of (31) as an additional baseline. ${ }^{\dagger}$</p>
<p>The results are shown in Figure 3. Our first finding was that GPT-4 in both English and Russian were more human-like than the other variants when compared with an adjusted Rand index (see Figure 3A; English: GPT-4 $0.5995 \%$ CI $[0.56,0.63]$, GPT$3.5,0.5095 \%$ CI $[0.46,0.52]$, GPT-3 $0.3995 \%$ CI $[0.37,0.42]$;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. A. Human and LLM similarity marginals and an example GPT-3 corresponding similarity matrix and its three-dimensional MDS solution for pitch. B. MDS solutions for vocal consonants and colors for GPT-4 similarity matrices. To illustrate the structure of the results, we highlighted consonants with the same place of articulation in the vocal tract with the same shape and color, and added a rotated HSV color wheel for the color MDS. C. Example GPT-4 explanations for similarity judgment scores. D. Word clouds for GPT-4 explanations in the domain of pitch, vocal consonants, and colors.</p>
<p>Russian: GPT-4 0.54 95% CI [0.46, 0.54], GPT-3.5 0.50 95% CI [0.45, 0.52], GPT-3 0.35 95% CI [0.29, 0.35]; see Methods). It is evident from our data, however, that LLMs are still not perfect in predicting human color naming as compared to a separate lab-based experiment conducted by Lindsey and Brown (31) (dashed line in Figure 3A top, constrained naming task 0.73 95% CI [0.65,0.73], free naming task 0.75 95% CI [0.66,0.75]). Moreover, the naming of GPT-4 colors differs from human data in some important cases, including the color turquoise, which was selected as the dominant color for 46 Munsell colors in GPT4 versus only 15 in human data. Note, however, that our human English results conducted online and with relative less control over color presentation were highly consistent with (31) that were conducted in the lab and under controlled environment, even though (31) used slightly different paradigms: free naming (consistency to our human data: 0.75 95% CI [0.66, 0.75]) and forced-choice list with a different set of items (0.73 95% CI [0.65, 0.74]).</p>
<p>Importantly, however, GPT-4 appears to replicate crosslingual differences (Figure 3B), for example separating Russian blue and purple into distinct categories for lighter and darker areas (35). Indeed, the color sinij / Синий (Blue) was the dominant category for 18 and 29 Munsell colors for GPT-4 and humans, respectively, and the color golubój / Голубой (Light-blue) was accordingly the dominant category for 33 and 26 colors. Similarly, the color fiołétovyj / Фиолетовый (Violet) was the dominant category for 27 and 32 Munsell colors for GPT-4 and humans, respectively, and the color lilóvyj / Лиловый (Lilac) was accordingly the dominant category for 20 and 18 Munsell colors.</p>
<h3>Discussion</h3>
<p>In this work, we showed how recent advances in large language models and, in particular, their flexible prompt engineering capabilities provide an elegant way for extracting clear quantitative psychophysical signals from text corpora. Our results contribute to a variety of thought-provoking issues in perception and language research. In particular, our findings further support recent research suggesting that people that lack direct sensory experiences (e.g., congenitally blind individuals) could still possess a rich understanding of perceptual concepts through language (e.g., colors (10)). Likewise, the language-dependence of the color representation of GPT-4 in the naming task suggests that the physical stimulus alone (in our case, the color hex code) is insufficient for explaining the behavioral patterns of the model since the same hex codes triggered different color categorizations in Russian and English. This supports the idea that language is not only shaped by perception but can also reshape it (35). Finally, the fact that GPT-4 achieved IRR-level performance without any fine-tuning in the newly collected datasets of pitch and vocal consonants, contributes to our understanding of LLMs' ability to mimic human behavior (13).</p>
<p>We end by discussing some limitations which point towards</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Color naming experiment using 330 Munsell colors from the World Color Survey (top, color space). A. Adjusted Rand index illustrating the alignment between human and LLM experiments ( $95 \%$ CIs). The dashed lines for English represent lab-based free naming and forced-choice naming experiments collected by Lindsey and Brown (31) (data reproduced with permission). B. Data comparison between humans and LLMs in Russian and English. Participants and LLMs were shown colors and were asked to choose from the same 15 -color list. The count of chosen colors for each option is given in parentheses. The color of a response cluster in the maps represents its average color.
future research directions. First, while in our work we experimented with three different types of behavioral data (similarity, explanations, and naming), there are other perceptual measures that one could consider (for example, odd-one-out triplet judgments (36)). Future work could explore these measures and interrogate to what extent they too yield a consistent representation. Second, our work is restricted to population-level averages as a leading-order analysis, a follow-up study could look into the natural variability of the LLM judgments and see to what extent they capture individual-level differences in humans. Third, while our work provides some evidence for LLMs' ability to capture cross-cultural differences, it remains to be seen how far this holds for other languages, especially those that are underrepresented (37). Finally, it is important to point out that the very same massive training pipelines that make LLMs a powerful proxy of human language also make them particularly susceptible to inheriting biases (38). Researchers should be particularly cautious when interpreting the patterns of behavior elicited from LLMs and should always benchmark them against genuine human data.</p>
<p>To conclude, our work showcases how LLMs can be used to explore the limits of what information about the world can be recovered from language alone, and more broadly, it highlights the potential of combining human and machine experiments in order to understand fundamental questions in cognitive science.</p>
<h2>Materials and Methods</h2>
<p>GPT prompt elicitation. The general structure of the prompt elicitation template for the similarity experiments was: one sentence describing the dataset (e.g., "people described pairs of colors using their hex code."), one sentence describing the similarity rating task and scale (e.g., "how similar are the colors in each pair on a scale of 0-1 where 0 is completely dissimilar and 1 is completely similar?"), three sets of three lines each corresponding to two stimuli and their actual similarity rating taken from behavioral experiments which serve as few-shot in-context examples, and an additional set of three lines corresponding to the pair of target stimuli and an associated empty rating field for the model to fill in ("Color one: #FF0000:</p>
<p>Color two: #A020F0. Rating:"). These were necessary to ensure that the model provided numerical values, and in all cases consisted of three fixed and randomly chosen comparisons so that most of the content was left for the model to produce. For each pair of target stimuli, we elicited ten ratings from each GPT model. Across all repetitions of all pairs of stimuli for a given dataset, we used only the same three in-context examples to ensure that the model is exposed to only a very small fraction of the similarity judgments against which its ratings were compared (see SI for full prompts).</p>
<p>For the color-naming experiments, we first elicited 15 basic color names from GPT-4 using the prompt "Name 15 basic colors." and a temperature of 0 (to get the highest probability answers). We then had GPT-4 name the hex code corresponding to each of the WCS colors using the following prompt: "Here is a list of 15 basic color names: <shuffled basic color list>. Which of these names best describes the following color: <hex-code>? Respond only using the name." We repeated this prompt ten times for each WCS color with the basic color list shuffled each time and temperature set to the default 0.7 to elicit ten names per WCS color. We repeated the full procedure with both prompts translated to Russian (GPT-4 also responded to both of these in Russian; see SI for full prompt).</p>
<p>Stimuli. The six human similarity datasets we considered come in two flavors - direct (dis-)similarity ratings and confusion matrices - and from two sources - previous psychological studies from the literature and newly collected datasets. Confusion matrices provide an alternative way to compute similarity scores between stimuli by counting the number of times a stimulus $x$ is confused for a stimulus $y$. By normalizing the counts one gets confusion probabilities $p_{x y}$ which can be converted into similarity scores using the formula $s_{x y}=\sqrt{p_{x y} p_{y x} / p_{x x} p_{y y}}(39,40)$.</p>
<p>Colors This dataset was taken from (25) (also reproduced in (24)) and comprised direct similarity judgments across a set of 14 colors with wavelengths in the range $434-674$ nanometers. We converted wavelengths into RGB using the script at https://hasanyavuz.ozderya. net/?p=211 and then we used the webcolors Python package to convert into hex codes. To get better coverage of the color wheel in Figure 2B we extended the space to 23 color stimuli by interpolating between the original colors in the dataset and eliciting an extended similarity matrix from GPT-4.</p>
<p>Pitch This dataset was collected and made publicly available very recently by a subset of the authors in (41) (see details below). It contains similarity judgments over pairs of 25 harmonic complex tones ( 10 partials and 3dB/octave roll-off) over a two octave range from C4 ( 60 MIDI; 261.626 Hz ) to C6 ( 84 MIDI; 1046.502 Hz ). The</p>
<p>pitch values were separated by 1 semitone steps to account for the fact that pitch perception is logarithmic (33) where the mapping between frequencies $f$ in Hertz and pitch $p$ in semitones are given by $p=12 \log _{2} f / 440+69$.
Vocal consonants This dataset was also collected via an online study (see below) and comprised similarity judgments over 16 recordings of vocal consonants taken from the International Phonetic Association ${ }^{1}$. The vocal consonants considered were b (bay), p (pay), m (may), n (no), g (go), k (cake), d (die), t (tie), f (fee), v (vow), s (so), $\theta$ (thigh), $\delta$ (they), $\chi$ (Jacques), and $f$ (show). The recordings came from two speakers, one male and one female.
Loudness We accessed this dataset via (40) which itself takes the data from (26). The dataset comes in the form of a confusion matrix over 8 pure tones of different loudness values ranging from 71.1 to 74.6 decibels.</p>
<p>Taste This dataset was also accessed via (40) and is taken from (28). The data comes in the form of a confusion matrix over 10 flavors described to participants as salt, salt-substitute, MSG, quinine, acid, sugar, artificial sweetener, salt-sugar, acid-sugar, and quinine-sugar.
Timbre This dataset was assembled in (27) based on 1217 subject's judgments from 5 prior publications. It comprises dissimilarity judgments over 12 instrument timbres: clarinet, saxophone, trumpet, cello, French horn, oboe, flute, English horn, bassoon, trombone, violin, and piano.</p>
<p>Behavioral experiments. To collect similarity judgments over pitch and vocal consonants we deployed two online experiments on Amazon Mechanical Turk (AMT). Overall, 55 participants completed the pitch study and 64 participants completed the vocal consonants study. In addition, we collected color naming data in Russian and British English participants using Prolific ${ }^{\circledR}$. Overall, we recruited 154 participants of which 103 were UK participants and 51 were Russian participants. Experiments were implemented using PsyNet ${ }^{\circledR}$ and Dallinger ${ }^{5}$. See additional details in SI.</p>
<p>Code and Data availability. All data and code used in this work can be accessed via the following link: https://tinyurl.com/fudaby5p, with the exception of the (31) color naming dataset as it is only available upon request from the authors. An interactive visualization of two-dimensional MDS spaces for the 6 modalities is available at: https://computational-audition.github.io/ LLM-psychophysics/all-modalities.html. The human color naming data can be interactively explored via: https://computational-audition.github. io/LLM-psychophysics/color.html.</p>
<p>ACKNOWLEDGMENTS. This research project and related results were made possible with the support of the NOMIS Foundation, and an NSERC fellowship (567554-2022) to IS.</p>
<ol>
<li>D Hume, An Abstract of a Treatise of Human Nature, 1740. (CUP Archive), (1740).</li>
<li>J Locke, An essay concerning human understanding. (Kay \&amp; Troutman), (1847).</li>
<li>RL Goldstone, BJ Rogosky, Using relations within conceptual systems to translate across conceptual systems. Cognition 84, 295-320 (2002).</li>
<li>T Regier, P Kay, N Khetarpal, Color naming reflects optimal partitions of color space. Proc. Natl. Acad. Sci. 104, 1436-1441 (2007).</li>
<li>T Regier, P Kay, Language, thought, and color: Whorf was half right. Trends cognitive sciences 13, 439-446 (2009).</li>
<li>S Dotscheid, S Shayan, A Majid, D Casasanto, The thickness of musical pitch: Psychophysical evidence for linguistic relativity. Psychol. science 24, 613-621 (2013).</li>
<li>N Zaslavsky, C Kemp, T Regier, N Tishby, Efficient compression in color naming and its evolution. Proc. Natl. Acad. Sci. 115, 7937-7942 (2018).</li>
<li>JS Kim, GV EIi, M Bedny, Knowledge of animal appearance among sighted and blind adults. Proc. Natl. Acad. Sci. 116, 11213-11222 (2019).</li>
<li>G Lupyan, RA Rahman, L Boroditsky, A Clark, Effects of language on visual perception. Trends cognitive sciences 24, 930-944 (2020).</li>
<li>JS Kim, B Aheimer, V Montanil-Marrara, M Bedny, Shared understanding of color among sighted and blind adults. Proc. Natl. Acad. Sci. 118, e2020192118 (2021).
${ }^{5}$ https://www.internationalphoneticassociation.org/
${ }^{6}$ https://www.prolific.co
${ }^{7}$ https://psynet.dev/
${ }^{8}$ https://dallinger.readthedocs.io/</li>
<li>G Kawakita, A Zeleznikow-Johnston, N Tsuchiya, M Oizumi, Is my "red" your "red"?: Unsupervised alignment of qualia structures via optimal transport. PsyArXiv (2023).</li>
<li>T Brown, et al., Language models are few-shot learners. Adv. neural information processing systems 33, 1877-1901 (2020).</li>
<li>OpenAI, Opt-4 technical report (2023).</li>
<li>A Srivastava, et al., Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022).</li>
<li>A Goldstein, et al., Shared computational principles for language processing in humans and deep language models. Nat. neuroscience 25, 369-380 (2022).</li>
<li>S Kumar, et al., Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model. BioRxiv pp. 2022-06 (2022).</li>
<li>R Tikochniski, A Goldstein, Y Yeshurun, U Hasson, R Reichart, Perspective changes in human listeners are aligned with the contextual transformation of the word embedding space. Cereb. Cortex p. bhad082 (2023).</li>
<li>M Abdou, et al., Can language models encode perceptual structure without grounding? a case study in color. arXiv preprint arXiv:2109.06129 (2021).</li>
<li>K Siedenburg, C Sallis, How does chatgpt rate sound semantics? arXiv preprint arXiv:2304.07830 (2023).</li>
<li>C Zhang, B Van Durme, Z Li, E Stengel-Eskin, Visual commonsense in pretrained unimodal and multimodal models. arXiv preprint arXiv:2205.01850 (2022).</li>
<li>R Marjieh, et al., Words are all you need? capturing human sensory similarity with textual descriptors. The Eleventh Int. Conf. on Learn. Represent. (2022).</li>
<li>D Dillion, N Tandon, Y Gu, K Gray, Can ai language models replace human participants? Trends Cogn. Sci. (2023).</li>
<li>D Ganguli, et al., The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459 (2023).</li>
<li>RN Shepard, Multidimensional scaling, tree-fitting, and clustering. Science 210, 390-398 (1980).</li>
<li>G Ekman, Dimensions of color vision. The J. Psychol. 38, 467-474 (1954).</li>
<li>DE Konibrot, Theoretical and empirical comparison of lucera choice model and logistic thurstone model of categorical judgment. Percept. \&amp; Psychophys. 24, 193-208 (1978).</li>
<li>P Esting, A Bitton., et al., Generative timbre spaces: regularizing variational auto-encoders with perceptual metrics. arXiv preprint arXiv:1805.08501 (2018).</li>
<li>TP Hettinger, JF Gent, LE Marks, ME Frank, study of taste perception. Percept. \&amp; Psychophys. 61, 1510-1521 (1999).</li>
<li>B Berlin, P Kay. Basic color terms: Their universality and evolution. (Univ of California Press), (1991).</li>
<li>P Kay, B Berlin, L Maffi, WR Merrifield, R Cook, The world color survey. (Citesøer), (2009).</li>
<li>DT Lindsey, AM Brown, The color lexicon of american english. J. vision 14, 17-17 (2014).</li>
<li>RN Shepard, Geometrical approximations to the structure of musical pitch. Psychol. review 89, 305 (1982).</li>
<li>N Jacoby, et al., Universal and non-universal features of musical pitch perception revealed by singing. Curr. Biol. 29, 3229-3243 (2019).</li>
<li>GV Paramei, YA Griber, D Mylonas, An online color naming experiment in russian using munset color samples. Color. Res. \&amp; Appl. 43, 358-374 (2018).</li>
<li>J Winawer, et al., Russian blues reveal effects of language on color discrimination. Proc. national academy sciences 104, 7780-7785 (2007).</li>
<li>MN Hebart, CY Zheng, F Pereira, CI Baker, Revealing the multidimensional mental representations of natural objects underlying human similarity judgements. Nat. human behaviour 4, 1173-1185 (2020).</li>
<li>DE Blasi, J Henrich, E Adamou, D Kemmerer, A Majid, Over-reliance on english hinders cognitive science. Trends cognitive sciences (2022).</li>
<li>TY Zhuo, Y Huang, C Chen, Z Xing, Exploring ai ethics of chatgpt: A diagnostic analysis. arXiv preprint arXiv:2301.12867 (2023).</li>
<li>RN Shepard, Toward a universal law of generalization for psychological science. Science 237, 1317-1323 (1987).</li>
<li>CR Sims, Efficient coding explains the universal law of generalization in human perception. Science 360, 652-656 (2018).</li>
<li>R Marjieh, TL Griffiths, N Jacoby, Musical pitch has multiple psychological geometries. bioRxiv (2023).</li>
<li>P Harrison, et al., Gibbs sampling with people in Advances in Neural Information Processing Systems, eds. H Larochelle, M Ranzato, R Hadsell, MF Balcan, H Lin. (Curran Associates, Inc.), Vol. 33, pp. 10659-10671 (2020).</li>
<li>KJ Woods, MH Siegel, J Tsien ,JH McDermott, Headphone screening to facilitate web-based auditory experiments. Attention, Perception, \&amp; Psychophys. 79, 2064-2072 (2017).</li>
<li>N Kriegeskorte, M Mur, PA Bandettini, Representational similarity analysis-connecting the branches of systems neuroscience. Front. systems neuroscience 2, 4 (2008).</li>
<li>J Clark, The ishihara test for color blindness. Am. J. Physiol. Opt. (1924).</li>
<li>WM Rand, Objective criteria for the evaluation of clustering methods. J. Am. Stat. association 66, 846-850 (1971).</li>
</ol>
<h2>Extended Methods</h2>
<p>Explicit GPT prompts. GPT prompt elicitation experiments were conducted using the OpenAI Text Completion (for GPT-3) and Chat Completion (for GPT-3.5 and GPT-4) APIs. The temperature was set to 0.7 for similarity judgments and 0 for color naming experiments. The exact prompt formats used are shown below.</p>
<h2>Similarity judgments.</h2>
<h2>Color:</h2>
<p>People described pairs of colors using their hex codes. How similar are the two colors in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar? Respond only with the numerical similarity rating. Color 1: #ff5700 Color 2: #ff9b00 Rating 0.76 Color 1: #b3ff00 Color 2: #00ff61 Rating: 0.45 Color 1: #FF0000 Color 2: #00b2ff Rating: 0.02 Color 1: <hex-code1> Color 2: <hex-code2> Rating:</p>
<h2>Pitch:</h2>
<p>People described pairs of musical notes using their frequencies in hertz.
How similar are the musical notes in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Note 1: 587.3295358348151 Hz
Note 2: 987.7666025122483 Hz
Rating: 0.46083740655517463
Note 1: 349.2282314330039 Hz
Note 2: 277.1826309768721 Hz
Rating: 0.743838237117938
Note 1: 415.3046975799451 Hz
Note 2: 987.7666025122483 Hz
Rating: 0.19874605585261726
Note 1: <frequency1>
Note 2: <frequency2>
Rating:</p>
<h2>Vocal consonants:</h2>
<p>People described vocal consonants using the international phonetic alphabet (IPA).
How similar do the vocal consonants in each pair sound on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar? Respond only with the numerical similarity rating.</p>
<p>Vocal Consonant 1: f
Vocal Consonant 2: m
Rating: 0.5
Vocal Consonant 1: n
Vocal Consonant 2: 3
Rating: 0.40740740740740744
Vocal Consonant 1: $\int$
Vocal Consonant 2: $\int$
Rating: 1.0
Vocal Consonant 1: <consonant1>
Vocal Consonant 2: <consonant2>
Rating:</p>
<h2>Loudness:</h2>
<p>People described the loudness of pure tones in decibels (dB).
How similar do the pure tones in each pair sound on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Pure Tone 1: 72.6 dB
Pure Tone 2: 74.1 dB
Rating: 0.3495324720283043
Pure Tone 1: 74.6 dB
Pure Tone 2: 73.6 dB
Rating: 0.5055839477695901
Pure Tone 1: 74.1 dB
Pure Tone 2: 74.1 dB
Rating: 1.0
Pure Tone 1: <loudness1>
Pure Tone 2: <loudness2>
Rating:
Taste:
People described flavors they tasted using words. How similar are the flavors in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Flavor 1: quinine
Flavor 2: artificial sweetener
Rating: 0.0
Flavor 1: artificial sweetener
Flavor 2: salt
Rating: 0.015433904145892428
Flavor 1: quinine-sugar
Flavor 2: acid-sugar
Rating: 0.2539115246067999
Flavor 1: <flavor1>
Flavor 2: <flavor2>
Rating:
Timbre:
People listened to pairs of musical instruments and rated the similarity of their timbre.
How similar is the timbre of the instruments in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Instrument 1: Cello
Instrument 2: Flute
Rating: 0.5604846433040316
Instrument 1: Flute
Instrument 2: Clarinet
Rating: 0.270932601836378
Instrument 1: Trombone
Instrument 2: Bassoon
Rating: 0.2893895067551666
Instrument 1: <instrument1>
Instrument 2: <instrument2>
Rating:</p>
<h2>Color naming.</h2>
<h2>Basic color free-elicitation:</h2>
<p>English:
Name 15 basic colors.
Russian:
Перечислите 15 основных цветов.</p>
<h2>Color naming elicitation:</h2>
<p>English:
Here is a list of 15 basic color names: <shuffled basic color list $>$.
Which of these names best describes the following color: $&lt;$ hex-code $&gt;$ ?
Respond only using the name.
Russian:
Вот список из 15 названий основных цветов: <shuffled basic color list $>$.
Какое из названий цветов лучше всего описывает следуюший цвет: <hex-code>?
Отвечайте только названием одного цвета из списка.
We repeated this prompt ten times for each WCS color with the basic color list shuffled each time and temperature set to the default 0.7 to elicit ten names per WCS color. For each of the ten elicitations per color, if the output was not one of the 15 basic colors we would keep re-querying GPT until it did give a valid color output (GPT-4 is slightly non-deterministic even at temperature 0 due to changes in hardware). If after 10 attempts the response was still invalid, we would return "error" as the color (this response is later discarded from the analysis).</p>
<h2>Additional details of behavioral experiments.</h2>
<p>Similarity Experiments: Participants. To collect similarity judgments over pitch and vocal consonants we deployed two online experiments on Amazon Mechanical Turk (AMT). ${ }^{<em> </em>}$ The recruitment and experimental pipelines were automated using PsyNet (42), a modern framework for experiment design ${ }^{11}$ and deployment which builds on the Dallinger ${ }^{12}$ platform for recruitment automation. Overall, 55 participants completed the pitch study and 64 participants completed the vocal consonants study. Participants were recruited from the United States, were paid $\$ 9-12$ USD per hour, and provided informed consent as approved by the Princeton IRB (#10859) and the Max Planck Ethics Council (#2021_42).</p>
<p>To enhance data quality, participants had to pass a standardized headphone check (43) that ensures good listening conditions and task comprehension, and were required to have successfully completed at least 3000 tasks on AMT. Upon passing the prescreening stage, participants were randomly assigned to rate the similarity between different pairs of stimuli and provided numerical judgments on a 7 Likert scale ranging from 0 (completely dissimilar) to 6 (completely similar). In the pitch experiment, participants provided an average of 80 judgments, and in the vocal consonants experiment an average of 55 judgments. See SI for explicit instructions.</p>
<p>Similarity Experiments: Procedure. Upon providing informed consent and passing the headphone check, participants received the following instructions. In the case of the pitch experiment: "In this experiment we are studying how people perceive sounds. In each round you will be presented with two sounds and your task will be to simply judge how similar those sounds are. You will have seven response options, ranging from 0 ('Completely Dissimilar') to 6 ('Completely Similar'). Choose the one you think is most appropriate. You will also have access to a replay button that will allow you to replay the sounds if needed. Note: no prior expertise is required to complete this task, just choose what you intuitively think is the right answer." Participants were then informed of an additional small quality bonus "The quality of your responses will be automatically monitored, and you will receive a bonus at the end of the experiment in proportion</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to your quality score. The best way to achieve a high score is to concentrate and give each round your best attempt". While the task is subjective in nature, we used consistency as a proxy for quality by repeating 5 random trials at the end of the experiment and computing the Spearman correlation $s$ between the original responses and their repetitions. The final bonus was computed using the formula $\min (\max (0.0,0.1 s), 0.1)$ yielding at most 10 cents. In the main experiment participants were assigned to random stimulus pairs and were instructed to rate their similarity using the following prompt: "How similar are the pair of sounds you just heard?" and provided a response on a Likert scale. The procedure for the vocal consonants similarity experiment was identical up to the specific instructions. Specifically, participants received the following instructions: "In this experiment we are studying how people perceive the sound of vocal consonants. A consonant is a speech sound that is pronounced by partly blocking air from the vocal tract. For example, the sound of the letter $c$ in cat is a consonant, and so is $t$ but not $a$. Similarly, the sound of the combination $s h$ in sheep is a consonant, and so is $p$ but not $e e$. In general, vowel sounds like those of the letters $a, e, i, o$, u are not consonants." The instructions then proceeded: "In each round you will be presented with two different recordings each including one consonant sound and your task will be to simply judge how similar are the sounds of the two spoken consonants. We are not interested in the vowel sounds nor in the voice height, just the sound of the consonants. You will have seven response options, ranging from 0 ('Completely Dissimilar') to 6 ('Completely Similar'). Choose the one you think is most appropriate. Note: no prior expertise is required to complete this task, just choose what you intuitively think is the right answer." Participants were then informed of the quality bonus which was identical to the pitch task, and then rated the similarity between pairs of random consonants based on the following prompt "How similar is the sound of the consonants pronounced by the two speakers?" and a Likert scale as before.</p>
<p>Similarity Experiments: Model Evaluation. We quantified model performance in predicting human similarity judgments by computing the Pearson correlation coefficient between the flattened upper triangle of the LLM-based and human-based similarity matrices (to account for the fact that these matrices are symmetric). This approach is similar to representational similarity analysis (44). To compute $95 \%$ confidence intervals, we bootstrapped with replacement over model predictions with 1,000 repetitions and computed for each repetition the average similarity matrix. We then correlated the upper triangles of each of those matrices with human data to produce a list of correlation coefficients on which we computed confidence intervals.</p>
<p>Color Naming Experiments: Participants. To collect the color naming data in Russian and British English participants, we ran online experiments on Prolific ${ }^{\text {S5 }}$. Overall, we recruited 103 UK participants and 51 Russian participants. All texts in the interface of the experiment (e.g., buttons, instructions, etc) were presented in the native language of the participant. The Russian texts were first automatically translated using DeepL ${ }^{\text {TM }}$ and then manually checked and corrected by a native speaker of Russian (author I.S). Participants had to be raised monolingually and to speak the target language as their mother tongue. Each participant was paid 9 GBP per hour and provided informed consent according to an approved protocol (Max Planck Ethics Council #2021_42). The experiment was implemented using PsyNet (42). Each session starts with a free-elicitation task where participants are asked to provide basic colors:</p>
<h2>Color Naming Experiments: Procedure.</h2>
<h2>Basic color free-elicitation:</h2>
<p>English:
Please name at least 8 basic color names.
Press enter after each color name.
Only use lower-case letters.</p>
<p>Russian:
Укажите не менее 8 названий основных цветов.
Нажмите клавишу Enter после каждого названия цвета.
Используйте только строчные буквы.</p>
<p>Participants may only submit color names without spaces, numbers, or special characters and can only submit the page if they have provided at least eight names. The list of obtained colors is highly overlapping with the GPT-4 list, justifying our choice to use GPT-4 as the basis for the word naming task (colors are sorted by their naming frequency).</p>
<p>Top 15 terms in English:
"blue", "green", "yellow", "red", "purple", "orange", "black", "pink", "white", "brown", "grey", "violet", "indigo", "turquoise", "silver"</p>
<p>Top 15 terms in Russian:
"красный", "синий", "белый", "зеленый", "оранжевый", "желтый", "фиолетовый", "черный", "голубой", "коричневый", "розовый", "серый", "жёлтый", "зелёный", "чёрный"</p>
<p>From the top 15 color terms, 11 (English) and 12 (Russian) color terms are overlapping with the list provided by GPT-4. Before the main experiments, participants received the following instructions:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Color naming instructions:</h2>
<p>English:
During this experiment, you will be presented with a square of a particular color and will be required to select the most suitable color term from a list of options.
Please be aware that some of the colors may be repeated to verify consistency of your choices.
If we detect any inconsistencies in your answers, we may terminate the experiment prematurely.
The best strategy is to answer each question truthfully, as attempting to memorize responses may prove difficult.</p>
<p>Russian:
В ходе этого эксперимента вам будет представлен квадрат определенного цвета, и вам нужно будет выбрать наиболее подходящее название цвета из списка вариантов.
Имейте в виду, что некоторые цвета могут повторяться, чтобы убедиться в согласованности вашего выбора. Если мы обнаружим какие-либо несоответствия в ваших ответах, мы можем досрочно прекратить эксперимент.
Лучшая стратегия - отвечать на каждый вопрос правдиво, так как попытка запомнить ответы может оказаться сложной.</p>
<p>The participants then went through the main experiment:</p>
<h2>Color naming task:</h2>
<p>English:
<square of a particular color>
You will see below a list of 15 basic color names. Which of these names best describes the color above?
$&lt;$ shuffled basic color list presented as buttons $&gt;$</p>
<p>Russian:
<square of a particular color>
Ниже Вы увидите список из 15 основных названий цветов.
Какое из этих названий лучше всего описывает вышеуказанный цвет?
$&lt;$ shuffled basic color list presented as buttons $&gt;$</p>
<p>At the end of the experiment, the participant took a color blindness test (45). Some participants abandoned the experiment prematurely, but we nevertheless included their responses ( 42 English participants, 3 Russian participants). Only a fraction of the participants failed the color blindness test ( 5 of 103 English participants, and 2 of 51 Russian participants). Consistent with the WCS we included all participants including those who failed the color blindness test. In a control analysis, we excluded all color blind individuals and all participants that did not complete the entire session and got nearly identical results (the adjusted Rand index was 0.92 for English experiments and 0.97 for Russian experiments).</p>
<p>Color Naming Experiments: Analysis. For each color, we collected at least 10 responses per LLM variant, and at least 10 forcedchoice human selections per color (English mean 19.30 responses, Russian mean 12.17 responses). Consistent with previous literature for each Munsell's color we selected the most frequently reported term. We then presented the dominant colors in Figure 3B. To aid visualization we average the RGB values of all colors with the same color term, and presented them as the legend and clustered color in that figure. We also listed per color the number of Munsell's colors that were associated with each dominant color term. Figure 3B provides additional information on the degree of agreement for each color. Colors for which less than $50 \%$ and $90 \%$ of the times the dominant color term was selected were indicated by "-" and "*", respectively. If the dominant color term was selected more than $90 \%$ of the time, no marking was used.</p>
<p>Adjusted Rand index. The Rand index (46) is a label-insensitive measure of clustering similarity that instead of relying on specific labels (e.g. "Blue") quantifies the similarity between two clustering partitions by counting pairs of items (in our case Munsell colors) that are clustered consistently and dividing them by the overall number of pairs. This allows to compare different clustering schemes when the vocabulary of labels is not aligned (e.g. English and Russian). Formally, we computed: $R=(b+c) / a$; where $b$ is the number of pairs of items that are in the same subset in one clustering and in the same subset in the other, $c$ is the number of pairs of items that are in different subsets in one clustering and in different subsets in the other and $a$ is the total number of pairs. The Rand index provides high values for two random clusterings, to adjust for this we used the corrected-for-chance version of the Rand index (46), which normalizes the raw value by the expected value of the Rand index for random clusterings. Formally, we have $A R I=\left(R I-R I_{\text {rand }}\right) /(1-$ $R I_{\text {rand }}$ ) where $A R I$ is the adjusted Rand index, $R I$ is the raw Rand index and $R I_{\text {rand }}$ is the expected Rand index for random clusterings. The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples, exactly 1.0 when the clusterings are identical (up to a permutation), and reaches -0.5 for "orthogonal clusters" that are less consistent relative to what is expected by chance. In our case, all values were strictly positive suggesting consistency across languages and experiments. To compute confidence intervals, we created bootstrapped datasets by sampling the responses of each color with replacement and recomputing the dominant selected color name. We then obtained CIs by computing the adjusted Rand index for 1,000 pairs of bootstrapped datasets.</p>
<p>Lindsey and Brown dataset. We compared our experimental data to a dataset by (31), reproduced with permission by Delwin Lindsey. The data contains two experimental conditions conducted in the lab with the same 51 participants. In the first condition, participants were instructed to provide free naming responses. In the second condition, participants were instructed to choose from a pre-specified list of 11 color terms: (Green, Blue, Purple, Pink, White, Brown, Orange, Yellow, Red, Black, and Gray). Despite the fact that the Lindsey \&amp; Brown experiment was conducted in the lab (and not online like our experiments) and that the constrained list in our experiment was somewhat different, the results of both experiments were highly consistent with our human English data (Constrained, $A R I=0.73$ $95 \%$ CI $[0.65,0.74]$, free naming $0.7595 \%$ CI $[0.66,0.75]$ ). In addition to putting an upper bound on the consistency with which the LLM can predict human data (by comparing it with another human experiment), these results prove that despite less control over color presentation compared to the lab, online presentation still provides high-quality color naming data.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{55}$ https://www.prolific.com
${ }^{\text {TM }}$ https://www.deepl.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
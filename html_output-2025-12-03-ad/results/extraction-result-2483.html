<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2483 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2483</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2483</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-258179505</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.07665v2.pdf" target="_blank">Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling</a></p>
                <p><strong>Paper Abstract:</strong> Active learning provides a framework to adaptively query the most informative experiments towards learning an unknown black-box function. Various approaches of active learning have been proposed in the literature, however, they either focus on exploration or exploitation in the design space. Methods that do consider exploration-exploitation simultaneously employ fixed or ad-hoc measures to control the trade-off that may not be optimal. In this paper, we develop a Bayesian hierarchical approach, referred as BHEEM, to dynamically balance the exploration-exploitation trade-off as more data points are queried. To sample from the posterior distribution of the trade-off parameter, We subsequently formulate an approximate Bayesian computation approach based on the linear dependence of queried data in the feature space. Simulated and real-world examples show the proposed approach achieves at least 21% and 11% average improvement when compared to pure exploration and exploitation strategies respectively. More importantly, we note that by optimally balancing the trade-off between exploration and exploitation, BHEEM performs better or at least as well as either pure exploration or pure exploitation.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2483.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2483.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BHEEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Hierarchical Exploration-Exploitation Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian hierarchical active-learning framework that dynamically estimates a trade-off parameter η to combine exploration and exploitation acquisition functions, using ABC-MCMC sampling and Gaussian Process Regression to minimize generalization error under a limited experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BHEEM (Bayesian Hierarchical Exploration-Exploitation Model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BHEEM builds a hierarchical Bayesian model over the exploration-exploitation trade-off parameter η with a Beta(α,β) prior and hyperpriors on α and β. At each query stage j it models η_j | θ_j ~ p(η|θ_j) and θ_j | ψ ~ p(θ|ψ) to capture within-stage and across-stage variability. Because the likelihood p(y,X|η) is intractable, BHEEM uses an ABC-MCMC (Metropolis-within-Gibbs) sampler to draw η samples; acceptance of candidate η is determined via a novel summary-statistic test based on approximate linear dependence (ALD) in feature/Hilbert space (via kernel evaluations) to ensure newly queried points provide novel information. BHEEM combines two acquisition functions F1 (exploration) and F2 (exploitation) into a linear objective η F1(x) + (1-η) F2(x) and selects the next query x* = argmax_x of that objective, updating η adaptively as data arrive. Gaussian Process Regression is used as the predictive model; hyperparameters such as MCMC proposal variance τ and ALD threshold ν are tuned/specified.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General active learning for regression; demonstrated on synthetic benchmark functions and materials science (MAX phase lattice constant prediction); applicable to experimental design and automated discovery where experiments are costly.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Adaptive selection of the next experiment x*: at each iteration compute a composite acquisition score η F1(x) + (1-η) F2(x) where η is sampled from the hierarchical posterior; candidate x is accepted only if it is approximately linearly independent (ALD) of previously queried points in feature space (δ ≥ ν) to avoid wasting resources on redundant experiments. The posterior over η is updated using ABC-MCMC to re-balance exploration vs exploitation as budget is consumed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock computation time for the active-learning loop (reported as average time to query 100 points); additional computational cost arises from MCMC sampling (ABC-MCMC) and repeated GP posterior evaluations. (Paper reports BHEEM is ~2.0–2.5× slower than compared acquisition strategies in wall-clock time.)</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Indirect: objective minimizes expected generalization error via combination of acquisition functions; uses GP posterior variance (for variance-based acquisition), Shannon entropy (for entropy-based acquisition), and committee disagreement (for QBC) as the component information metrics. The ABC acceptance uses an ALD novelty metric (kernel-based linear dependence) as a summary statistic to decide whether a proposed query adds new information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Encodes the balance via a learned trade-off parameter η∈[0,1] that weights exploration acquisition F1 and exploitation acquisition F2 in a linear combination. η is treated hierarchically to capture within-stage and between-stage variability and is sampled via ABC-MCMC; η is updated iteratively as new labeled data arrive, making the allocation dynamic rather than static or ad-hoc.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: enforces a kernel-based approximate linear dependence (ALD) test that rejects candidate queries if their feature mapping can be reconstructed (within threshold ν) from previously queried points. When paired with exploration acquisition functions (e.g., improved greedy sampling), the ALD filter promotes coverage/diversity in the set of experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments (bounded labeling budget); the paper frames active learning under a finite (often small) budget of physical/simulation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>BHEEM optimizes allocation of a fixed query budget by adaptively re-weighting exploration vs exploitation through the posterior over η so that each new query is chosen to minimize future generalization error given remaining budget; redundant candidate queries are rejected via ALD to preserve budget for novel information.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary metric is RMSE (root mean squared error) on held-out test points (reported as values aggregated over repeated runs). Reported improvements include: average RMSE improvements across simulated studies vs pure exploration and pure exploitation ranging ~7%–24% depending on baseline and experimental setup; example aggregate statements: 'at least 21% and 11% average improvement when compared to pure exploration and exploitation strategies respectively' (abstract), 'about 21% and 24% lower average RMSE from pure exploration and exploitation respectively' (conclusion), and in the MAX materials case study BHEEM achieved ~10.4% and ~11.3% improvement over the pure exploration and exploitation strategies respectively. Computational cost: BHEEM takes ~2.0× longer than QBC and ~2.5× longer than iGS for 100 queries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against pure exploration (improved greedy sampling iGS, maximum variance, maximum entropy), pure exploitation (Query-by-Committee, QBC), static trade-off settings (η fixed at {0.25,0.5,0.75}), probabilistic (ε-decreasing / simulated-annealing inspired) trade-off, and random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BHEEM consistently performed as well as or better than the best of the two component strategies (exploration or exploitation). Reported numerical comparisons: average RMSE reductions vs iGS and QBC varied by function (examples: ~7% and ~21% lower RMSE vs iGS and QBC respectively in one aggregated comparison; vs maximum variance and QBC the improvements were ~5.7% and ~2.3% in another). Across six simulated functions BHEEM showed consistent lower RMSE or faster convergence and in the materials case study delivered ~10–11% RMSE improvement over pure baselines. However, BHEEM incurred ~2–2.5× higher wall-clock computational time.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>In terms of labeled-data efficiency (fewer queries to achieve lower RMSE): reported average RMSE reductions in the range ~5%–24% depending on baseline and dataset; computational efficiency (time) is lower—BHEEM requires about 2–2.5× more compute time than simple acquisition functions for the experimental setups reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper analyzes the trade-off between exploration and exploitation by learning η hierarchically and shows that no single static η is optimal across functions or stages. It demonstrates that adaptive η leads to faster convergence and lower RMSE, at the cost of higher computational overhead (MCMC sampling and ABC checks). Diversity reduction of redundant queries is explicitly handled via ALD; however, the paper does not present a formal multi-objective optimization explicitly trading compute FLOPs vs information gain or breakthrough probability — only empirical timing and RMSE trade-offs are reported. The authors note BHEEM is more suitable for offline/expensive experiments due to higher computation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key findings: (1) dynamically estimating η via a hierarchical Bayesian model that captures within- and across-stage variability yields better allocation of limited queries than static or probabilistic schedules; (2) rejecting queries that are approximately linearly dependent in feature space preserves budget for novel information and improves efficiency; (3) no universal static η works well across functions or iterations, so adaptive methods outperform fixed heuristics; (4) the method trades off increased computational time (MCMC/ABC overhead) for improved labeled-data efficiency and lower generalization error, making it preferable where experiments are costly but offline compute is available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2483.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2483.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ABC-ALD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Approximate Bayesian Computation with Approximate Linear Dependence summary (ABC-ALD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ABC-MCMC implementation where acceptance of proposed trade-off parameter η is determined by whether the candidate query yields novel information, evaluated via an approximate linear dependence (ALD) criterion in feature (Hilbert) space computed through kernel evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ABC-ALD (Approximate Bayesian Computation with ALD summary for η sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Within the Gibbs sampling scheme for (α,β,η), the conditional for η uses an ABC-MCMC Metropolis step: propose η' ~ N(η_old, τ^2). For the candidate η' the corresponding nominated query x is checked for novelty using ALD: compute δ = K(x,x) - K(X_o,x)^T K(X_o,X_o)^{-1} K(X_o,x) and compare δ to threshold ν; if δ ≥ ν (i.e., not approximately linear dependent on existing set) the candidate is considered informative and can be accepted with Metropolis acceptance ratio based on priors (ABC replaces likelihood ratio by the indicator of the ALD test). If δ < ν the candidate is rejected. This replaces an intractable likelihood with a computationally cheap kernel-based summary statistic that directly links ABC acceptance to information novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Sampling hierarchical posteriors for active learning trade-off parameters in expensive experimental design and active learning contexts; used here in regression active learning with Gaussian processes.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates queries by admitting only candidate samples that pass the ALD novelty threshold—thus conserving experimental budget by preventing selection of queries whose features are approximately linearly reconstructable from prior data. The ABC acceptance couples η sampling with whether the chosen acquisition under η yields novel data.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Kernel-matrix operations (inversion or solving linear systems) and MCMC sampling iterations; measured as wall-clock time in experiments. ALD check requires computing K(X_o,X_o)^{-1} once per candidate or incremental updates; overall overhead significant relative to simpler acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>ALD-derived novelty metric δ (kernel residual) is used as the summary statistic; larger δ indicates more novel information (less linear dependence), acting as a proxy for information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>By coupling ABC acceptance to the ALD novelty metric, the sampler biases η towards values that nominate queries adding novel information (exploration) while still allowing η values that emphasize exploitation when the committee/variance-based acquisitions indicate high local uncertainty; η itself controls the exploration/exploitation weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Yes — ALD explicitly enforces diversity by rejecting candidates whose feature map is approximately in the span of previously queried points, thus promoting exploration of novel regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget (number of queries); ALD acts as a filter to avoid wasting budget on redundant samples.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Rejects redundant candidates so that each accepted query consumes budget only when it contributes novel information; MCMC continues proposing until acceptable sample is found, implicitly trading compute for budget preservation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Indirect: contributes to overall RMSE improvements of BHEEM as reported; increases compute time because of MCMC and kernel operations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicitly compared to ABC variants without ALD and to standard Metropolis acceptance that would require likelihoods; in experiments ABC-ALD is a component of BHEEM compared against simple acquisition functions without ABC/ALD.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>When included in BHEEM, ALD-based ABC enabled lower RMSE outcomes relative to static/probabilistic baselines; no separate ablation numbers for ABC-ALD alone provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Preserves experimental budget by filtering redundant queries; quantitative gain reported as part of BHEEM's overall RMSE improvements (see BHEEM entry).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Explicitly trades additional computation (MCMC and kernel linear-algebra) for budget efficiency by preventing redundant experiments; the paper emphasizes this is appropriate when experiments are expensive and offline compute is available.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Using a kernel-based ALD summary within ABC provides a practical and effective surrogate for the intractable likelihood when allocating experiments: it prioritizes novel/independent queries and integrates naturally with the hierarchical learned η to produce better overall allocation under a fixed budget.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2483.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2483.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iGS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Improved Greedy Sampling (iGS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration-focused acquisition function that selects the next query by maximizing the product of distance in input space to nearest labeled point and distance in predicted output space to nearest labeled output, promoting diversity in both input and output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Improved Greedy Sampling (iGS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At candidate x_j compute u(x_j)=||x_j - X_o||_2 (input-space nearest-neighbor distance) and v(x_j)=||f_j - y_o||_2 (predicted output distance), then score as min(u(x_j), v(x_j)) or equivalently via product u(x_j)*v(x_j); choose x* that maximizes this score to force selection of points far from labeled points in both input and output spaces. This encourages wide coverage and sampling of unobserved regions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for regression; used as an exploration acquisition component in experimental design and materials discovery examples in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates queries towards maximally novel/diverse points in input-output space (furthest from current labeled set), thereby prioritizing coverage over local refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Computes distances between candidate points and existing labeled set and GP predictions; cost measured in wall-clock time in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Does not explicitly optimize mutual information or expected reduction in uncertainty; uses geometric/diversity proxies (distance) as an exploration heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration strategy (η=1 when used as exploration component); no exploitation component by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Yes — explicit distance-based diversity in input and predicted output spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments; used to spread queries across budget to maximize coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Prioritizes coverage so budget is spent on diverse samples; no adaptive balancing of exploration/exploitation by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported RMSE when used alone as an acquisition function; in experiments iGS sometimes approaches BHEEM's performance on smooth functions but underperforms near discontinuities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to QBC, maximum variance, maximum entropy, random sampling, and BHEEM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>iGS often outperforms random sampling and can be competitive with other methods for certain functions; averaged across tasks, BHEEM outperforms iGS by several percent RMSE in many cases. Specific improvements vary by problem (e.g., iGS sometimes within ~5% of BHEEM on F1).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Provides efficient coverage so fewer queries are wasted exploring already-sampled regions; no explicit numeric claim beyond RMSE comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Acts solely on exploration objective; when combined with exploitation inside BHEEM, yields better overall balance than iGS alone.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>iGS is a reliable exploration baseline but suboptimal alone for functions with localized sharp features; combining with an exploitation strategy (and learning weighting) improves allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2483.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2483.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Query-by-Committee</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-based active-learning strategy that selects queries maximizing disagreement among a committee of models, effectively focusing on regions where the model family is uncertain or where predictions most disagree.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Query by Committee</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Query-by-Committee (QBC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintain a committee of Q predictive models (in experiments: 10 GPRs with different kernels). For candidate x_j compute pairwise prediction differences |h_l(x_j)-h_p(x_j)| and take the maximum disagreement as the acquisition score; select x* maximizing this score. Emphasizes refinement in regions where model hypotheses disagree (often exploitation of boundaries or sharp features).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for regression; used as an exploitation-focused acquisition function in the paper's experiments including materials case study.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates queries to regions of maximal model disagreement, directing budget to resolve local uncertainty and refine predictive models in those regions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Computes predictions of all committee members for candidate points; cost measured as additional wall-clock time relative to single-model acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Committee disagreement is used as a proxy for expected model change or uncertainty reduction; not a formal mutual information computation but correlated with learning value.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploitation strategy in this paper's framing (η=0 when used as exploitation component); focuses on resolving local uncertainty rather than exploring coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism; tends to concentrate queries in regions of disagreement and may ignore global coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments; directs budget to uncertain regions.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Consumes budget to refine uncertain areas; can lead to over-concentrated sampling if used alone.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RMSE when used as standalone acquisition and as component in BHEEM; QBC often quickly reduces error near discontinuities but may under-explore elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against iGS, variance, entropy, random, static/probabilistic tradeoffs, and BHEEM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>QBC is strong at resolving sharp local features and performs comparably to BHEEM in some high-uncertainty tasks; however, BHEEM outperforms QBC on average across datasets due to better balance. Example: BHEEM achieved ~21% lower average RMSE than QBC in a reported aggregated comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Efficient at local error reduction but can be inefficient overall if global coverage is required; quantitative gains context-dependent in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes QBC's exploitation bias and tendency to oversample regions of discontinuity, motivating the need to combine it adaptively with exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>QBC is a useful exploitation tool but should be combined with exploration and adaptively weighted (as in BHEEM) under fixed-budget experimental design to avoid neglecting other regions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2483.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2483.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaxVar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Variance Acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration acquisition that selects the next point with the highest posterior predictive variance under the Gaussian Process model, aiming to reduce model variance and thereby expected generalization error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Maximum Variance acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute GP posterior covariance for unlabeled candidates and select x with maximal predicted variance V(x) (the diagonal elements of cov(f) from GP posterior). This targets less-explored regions where the model is uncertain and is an exploration-biased information-based acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for regression and experimental design where uncertainty quantification via GPs is available.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Directs budget to points that will maximally reduce predicted variance (proxy for future generalization error reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Requires GP posterior covariance evaluations; cost measured in wall-clock time in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Posterior variance used as proxy for information gain / expected reduction in generalization error.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration acquisition when used alone.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via variance landscape; no explicit coverage/diversity enforcement like ALD or iGS.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Spends budget on high-uncertainty points; may achieve effective variance reduction but could ignore structured exploration needs.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RMSE performance reported in comparisons; BHEEM showed modest improvements relative to maximum variance in aggregate (e.g., ~5.7% RMSE improvement in one comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with iGS, QBC, entropy, random, static/probabilistic tradeoffs, and BHEEM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Generally effective for exploration-driven tasks; combined adaptively (as in BHEEM) yields better overall performance.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reduces model variance efficiently per query, but gains depend on problem structure.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Acts purely to reduce uncertainty; paper uses it as a component to be balanced with exploitation via η.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Maximum variance is a principled exploration objective but needs to be balanced with exploitation to avoid missing local high-error regions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2483.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2483.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaxEnt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Entropy (Uncertainty Sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic acquisition that selects the candidate maximizing Shannon entropy of the GP posterior, targeting points of highest predictive uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Maximum Entropy acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute Shannon entropy H[f] = 1/2 log |cov(f)| + (D/2) log(2πe) for the GP predictive distribution at candidate points and select the point with maximum entropy; in practice entropy reduces to functions of predictive variance for Gaussian marginals, focusing queries on high-uncertainty regions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for regression with Gaussian Process models.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates budget to points with maximal epistemic uncertainty (entropy) aiming to reduce model uncertainty globally.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>GP covariance determinant or variance evaluations; measured via wall-clock time in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Shannon entropy over predictive distribution used as the measure of expected information content.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration strategy when used alone.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via entropy landscape but no explicit ALD-like filter.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Allocates queries to high-entropy regions until budget is exhausted; no dynamic balancing unless combined with other acquisitions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RMSE comparisons reported; generally more exploration-biased and competitive on some tasks but outperformed by adaptive combination in BHEEM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with iGS, QBC, maximum variance, random, static/probabilistic tradeoffs, and BHEEM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Provides solid exploration behavior but BHEEM often yields lower RMSE by adaptively combining entropy-driven exploration with exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Effective at reducing uncertainty per query but may miss localized sharp features unless balanced with exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Serves as exploration component in trade-off; paper uses it to demonstrate that exploration-only approaches are suboptimal compared to adaptive weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2483.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2483.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static/ProbTrade</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static and Probabilistic Trade-off Schedules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline strategies for balancing exploration and exploitation by either fixing the trade-off parameter η for all iterations (static) or making it decay probabilistically over time (probabilistic/ε-decreasing).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Static and Probabilistic trade-off baselines</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Static trade-off: choose a fixed η (e.g., 0.25, 0.5, 0.75) a priori and use it for all query iterations, reducing computational complexity. Probabilistic trade-off: define exploration probability p_R = α^(t-1); at each iteration sample a uniform random Z and perform exploration (η=1) if Z ≤ p_R else exploitation (η=0), with α<1 (paper uses α=0.7), leading to a decaying exploration schedule akin to simulated annealing.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for regression; used here as baselines to compare adaptive allocation methods for expensive experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Static: simple fixed budget partitioning determined by η; Probabilistic: early-budget emphasis on exploration with decaying probability transitioning toward exploitation as iterations progress.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Low computational overhead (no MCMC) measured in wall-clock time; presented as much faster than BHEEM.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>None explicitly for static schedule; probabilistic schedule uses randomness to provide exploration early and exploitation later but does not optimize explicit information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Static uses fixed linear weighting of exploration/exploitation; probabilistic uses a stochastic schedule that decays exploration probability over time.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Only insofar as the chosen exploration acquisition encourages diversity; no explicit novelty filter like ALD.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Static: simple reduction in compute and straightforward budget allocation; Probabilistic: heuristic time-dependent allocation that does not adapt to observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RMSE reported at multiple iteration counts; heatmap comparisons show no single static η works well across functions or iterations. BHEEM outperforms both static and probabilistic approaches in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>These are the baselines used for comparison to BHEEM and other acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BHEEM 'distinctly provides better results than all the static and probabilistic trade-offs' per the paper; static/probabilistic schedules sometimes perform well on specific problems but are not generally robust.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Static/probabilistic are computationally cheap but can be suboptimal in labeled-data efficiency; BHEEM trades extra compute for better RMSE per labeled point.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights limitations of fixed/trial-and-error schedules and motivates the hierarchical adaptive approach; shows via heatmaps that no one fixed η is optimal across problems or iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: prefer adaptive, data-driven reweighting (BHEEM) over static or hand-tuned probabilistic schedules when experimental cost justifies extra computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active learning with statistical models <em>(Rating: 2)</em></li>
                <li>Information-based objective functions for active data selection <em>(Rating: 2)</em></li>
                <li>A novel active learning regression framework for balancing the exploration-exploitation trade-off <em>(Rating: 2)</em></li>
                <li>Active learning for regression using greedy sampling <em>(Rating: 2)</em></li>
                <li>Query by Committee <em>(Rating: 2)</em></li>
                <li>Markov chain Monte Carlo without likelihoods <em>(Rating: 1)</em></li>
                <li>A hierarchical expected improvement method for bayesian optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2483",
    "paper_id": "paper-258179505",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "BHEEM",
            "name_full": "Bayesian Hierarchical Exploration-Exploitation Model",
            "brief_description": "A Bayesian hierarchical active-learning framework that dynamically estimates a trade-off parameter η to combine exploration and exploitation acquisition functions, using ABC-MCMC sampling and Gaussian Process Regression to minimize generalization error under a limited experimental budget.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BHEEM (Bayesian Hierarchical Exploration-Exploitation Model)",
            "system_description": "BHEEM builds a hierarchical Bayesian model over the exploration-exploitation trade-off parameter η with a Beta(α,β) prior and hyperpriors on α and β. At each query stage j it models η_j | θ_j ~ p(η|θ_j) and θ_j | ψ ~ p(θ|ψ) to capture within-stage and across-stage variability. Because the likelihood p(y,X|η) is intractable, BHEEM uses an ABC-MCMC (Metropolis-within-Gibbs) sampler to draw η samples; acceptance of candidate η is determined via a novel summary-statistic test based on approximate linear dependence (ALD) in feature/Hilbert space (via kernel evaluations) to ensure newly queried points provide novel information. BHEEM combines two acquisition functions F1 (exploration) and F2 (exploitation) into a linear objective η F1(x) + (1-η) F2(x) and selects the next query x* = argmax_x of that objective, updating η adaptively as data arrive. Gaussian Process Regression is used as the predictive model; hyperparameters such as MCMC proposal variance τ and ALD threshold ν are tuned/specified.",
            "application_domain": "General active learning for regression; demonstrated on synthetic benchmark functions and materials science (MAX phase lattice constant prediction); applicable to experimental design and automated discovery where experiments are costly.",
            "resource_allocation_strategy": "Adaptive selection of the next experiment x*: at each iteration compute a composite acquisition score η F1(x) + (1-η) F2(x) where η is sampled from the hierarchical posterior; candidate x is accepted only if it is approximately linearly independent (ALD) of previously queried points in feature space (δ ≥ ν) to avoid wasting resources on redundant experiments. The posterior over η is updated using ABC-MCMC to re-balance exploration vs exploitation as budget is consumed.",
            "computational_cost_metric": "Wall-clock computation time for the active-learning loop (reported as average time to query 100 points); additional computational cost arises from MCMC sampling (ABC-MCMC) and repeated GP posterior evaluations. (Paper reports BHEEM is ~2.0–2.5× slower than compared acquisition strategies in wall-clock time.)",
            "information_gain_metric": "Indirect: objective minimizes expected generalization error via combination of acquisition functions; uses GP posterior variance (for variance-based acquisition), Shannon entropy (for entropy-based acquisition), and committee disagreement (for QBC) as the component information metrics. The ABC acceptance uses an ALD novelty metric (kernel-based linear dependence) as a summary statistic to decide whether a proposed query adds new information.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Encodes the balance via a learned trade-off parameter η∈[0,1] that weights exploration acquisition F1 and exploitation acquisition F2 in a linear combination. η is treated hierarchically to capture within-stage and between-stage variability and is sampled via ABC-MCMC; η is updated iteratively as new labeled data arrive, making the allocation dynamic rather than static or ad-hoc.",
            "diversity_mechanism": "Explicit: enforces a kernel-based approximate linear dependence (ALD) test that rejects candidate queries if their feature mapping can be reconstructed (within threshold ν) from previously queried points. When paired with exploration acquisition functions (e.g., improved greedy sampling), the ALD filter promotes coverage/diversity in the set of experiments.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-number-of-experiments (bounded labeling budget); the paper frames active learning under a finite (often small) budget of physical/simulation experiments.",
            "budget_constraint_handling": "BHEEM optimizes allocation of a fixed query budget by adaptively re-weighting exploration vs exploitation through the posterior over η so that each new query is chosen to minimize future generalization error given remaining budget; redundant candidate queries are rejected via ALD to preserve budget for novel information.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Primary metric is RMSE (root mean squared error) on held-out test points (reported as values aggregated over repeated runs). Reported improvements include: average RMSE improvements across simulated studies vs pure exploration and pure exploitation ranging ~7%–24% depending on baseline and experimental setup; example aggregate statements: 'at least 21% and 11% average improvement when compared to pure exploration and exploitation strategies respectively' (abstract), 'about 21% and 24% lower average RMSE from pure exploration and exploitation respectively' (conclusion), and in the MAX materials case study BHEEM achieved ~10.4% and ~11.3% improvement over the pure exploration and exploitation strategies respectively. Computational cost: BHEEM takes ~2.0× longer than QBC and ~2.5× longer than iGS for 100 queries.",
            "comparison_baseline": "Compared against pure exploration (improved greedy sampling iGS, maximum variance, maximum entropy), pure exploitation (Query-by-Committee, QBC), static trade-off settings (η fixed at {0.25,0.5,0.75}), probabilistic (ε-decreasing / simulated-annealing inspired) trade-off, and random sampling.",
            "performance_vs_baseline": "BHEEM consistently performed as well as or better than the best of the two component strategies (exploration or exploitation). Reported numerical comparisons: average RMSE reductions vs iGS and QBC varied by function (examples: ~7% and ~21% lower RMSE vs iGS and QBC respectively in one aggregated comparison; vs maximum variance and QBC the improvements were ~5.7% and ~2.3% in another). Across six simulated functions BHEEM showed consistent lower RMSE or faster convergence and in the materials case study delivered ~10–11% RMSE improvement over pure baselines. However, BHEEM incurred ~2–2.5× higher wall-clock computational time.",
            "efficiency_gain": "In terms of labeled-data efficiency (fewer queries to achieve lower RMSE): reported average RMSE reductions in the range ~5%–24% depending on baseline and dataset; computational efficiency (time) is lower—BHEEM requires about 2–2.5× more compute time than simple acquisition functions for the experimental setups reported.",
            "tradeoff_analysis": "The paper analyzes the trade-off between exploration and exploitation by learning η hierarchically and shows that no single static η is optimal across functions or stages. It demonstrates that adaptive η leads to faster convergence and lower RMSE, at the cost of higher computational overhead (MCMC sampling and ABC checks). Diversity reduction of redundant queries is explicitly handled via ALD; however, the paper does not present a formal multi-objective optimization explicitly trading compute FLOPs vs information gain or breakthrough probability — only empirical timing and RMSE trade-offs are reported. The authors note BHEEM is more suitable for offline/expensive experiments due to higher computation.",
            "optimal_allocation_findings": "Key findings: (1) dynamically estimating η via a hierarchical Bayesian model that captures within- and across-stage variability yields better allocation of limited queries than static or probabilistic schedules; (2) rejecting queries that are approximately linearly dependent in feature space preserves budget for novel information and improves efficiency; (3) no universal static η works well across functions or iterations, so adaptive methods outperform fixed heuristics; (4) the method trades off increased computational time (MCMC/ABC overhead) for improved labeled-data efficiency and lower generalization error, making it preferable where experiments are costly but offline compute is available.",
            "uuid": "e2483.0",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ABC-ALD",
            "name_full": "Approximate Bayesian Computation with Approximate Linear Dependence summary (ABC-ALD)",
            "brief_description": "An ABC-MCMC implementation where acceptance of proposed trade-off parameter η is determined by whether the candidate query yields novel information, evaluated via an approximate linear dependence (ALD) criterion in feature (Hilbert) space computed through kernel evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ABC-ALD (Approximate Bayesian Computation with ALD summary for η sampling)",
            "system_description": "Within the Gibbs sampling scheme for (α,β,η), the conditional for η uses an ABC-MCMC Metropolis step: propose η' ~ N(η_old, τ^2). For the candidate η' the corresponding nominated query x is checked for novelty using ALD: compute δ = K(x,x) - K(X_o,x)^T K(X_o,X_o)^{-1} K(X_o,x) and compare δ to threshold ν; if δ ≥ ν (i.e., not approximately linear dependent on existing set) the candidate is considered informative and can be accepted with Metropolis acceptance ratio based on priors (ABC replaces likelihood ratio by the indicator of the ALD test). If δ &lt; ν the candidate is rejected. This replaces an intractable likelihood with a computationally cheap kernel-based summary statistic that directly links ABC acceptance to information novelty.",
            "application_domain": "Sampling hierarchical posteriors for active learning trade-off parameters in expensive experimental design and active learning contexts; used here in regression active learning with Gaussian processes.",
            "resource_allocation_strategy": "Allocates queries by admitting only candidate samples that pass the ALD novelty threshold—thus conserving experimental budget by preventing selection of queries whose features are approximately linearly reconstructable from prior data. The ABC acceptance couples η sampling with whether the chosen acquisition under η yields novel data.",
            "computational_cost_metric": "Kernel-matrix operations (inversion or solving linear systems) and MCMC sampling iterations; measured as wall-clock time in experiments. ALD check requires computing K(X_o,X_o)^{-1} once per candidate or incremental updates; overall overhead significant relative to simpler acquisition functions.",
            "information_gain_metric": "ALD-derived novelty metric δ (kernel residual) is used as the summary statistic; larger δ indicates more novel information (less linear dependence), acting as a proxy for information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "By coupling ABC acceptance to the ALD novelty metric, the sampler biases η towards values that nominate queries adding novel information (exploration) while still allowing η values that emphasize exploitation when the committee/variance-based acquisitions indicate high local uncertainty; η itself controls the exploration/exploitation weighting.",
            "diversity_mechanism": "Yes — ALD explicitly enforces diversity by rejecting candidates whose feature map is approximately in the span of previously queried points, thus promoting exploration of novel regions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experimental budget (number of queries); ALD acts as a filter to avoid wasting budget on redundant samples.",
            "budget_constraint_handling": "Rejects redundant candidates so that each accepted query consumes budget only when it contributes novel information; MCMC continues proposing until acceptable sample is found, implicitly trading compute for budget preservation.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Indirect: contributes to overall RMSE improvements of BHEEM as reported; increases compute time because of MCMC and kernel operations.",
            "comparison_baseline": "Implicitly compared to ABC variants without ALD and to standard Metropolis acceptance that would require likelihoods; in experiments ABC-ALD is a component of BHEEM compared against simple acquisition functions without ABC/ALD.",
            "performance_vs_baseline": "When included in BHEEM, ALD-based ABC enabled lower RMSE outcomes relative to static/probabilistic baselines; no separate ablation numbers for ABC-ALD alone provided.",
            "efficiency_gain": "Preserves experimental budget by filtering redundant queries; quantitative gain reported as part of BHEEM's overall RMSE improvements (see BHEEM entry).",
            "tradeoff_analysis": "Explicitly trades additional computation (MCMC and kernel linear-algebra) for budget efficiency by preventing redundant experiments; the paper emphasizes this is appropriate when experiments are expensive and offline compute is available.",
            "optimal_allocation_findings": "Using a kernel-based ALD summary within ABC provides a practical and effective surrogate for the intractable likelihood when allocating experiments: it prioritizes novel/independent queries and integrates naturally with the hierarchical learned η to produce better overall allocation under a fixed budget.",
            "uuid": "e2483.1",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "iGS",
            "name_full": "Improved Greedy Sampling (iGS)",
            "brief_description": "An exploration-focused acquisition function that selects the next query by maximizing the product of distance in input space to nearest labeled point and distance in predicted output space to nearest labeled output, promoting diversity in both input and output.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Improved Greedy Sampling (iGS)",
            "system_description": "At candidate x_j compute u(x_j)=||x_j - X_o||_2 (input-space nearest-neighbor distance) and v(x_j)=||f_j - y_o||_2 (predicted output distance), then score as min(u(x_j), v(x_j)) or equivalently via product u(x_j)*v(x_j); choose x* that maximizes this score to force selection of points far from labeled points in both input and output spaces. This encourages wide coverage and sampling of unobserved regions.",
            "application_domain": "Active learning for regression; used as an exploration acquisition component in experimental design and materials discovery examples in the paper.",
            "resource_allocation_strategy": "Allocates queries towards maximally novel/diverse points in input-output space (furthest from current labeled set), thereby prioritizing coverage over local refinement.",
            "computational_cost_metric": "Computes distances between candidate points and existing labeled set and GP predictions; cost measured in wall-clock time in comparisons.",
            "information_gain_metric": "Does not explicitly optimize mutual information or expected reduction in uncertainty; uses geometric/diversity proxies (distance) as an exploration heuristic.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Pure exploration strategy (η=1 when used as exploration component); no exploitation component by itself.",
            "diversity_mechanism": "Yes — explicit distance-based diversity in input and predicted output spaces.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-number-of-experiments; used to spread queries across budget to maximize coverage.",
            "budget_constraint_handling": "Prioritizes coverage so budget is spent on diverse samples; no adaptive balancing of exploration/exploitation by itself.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Reported RMSE when used alone as an acquisition function; in experiments iGS sometimes approaches BHEEM's performance on smooth functions but underperforms near discontinuities.",
            "comparison_baseline": "Compared directly to QBC, maximum variance, maximum entropy, random sampling, and BHEEM.",
            "performance_vs_baseline": "iGS often outperforms random sampling and can be competitive with other methods for certain functions; averaged across tasks, BHEEM outperforms iGS by several percent RMSE in many cases. Specific improvements vary by problem (e.g., iGS sometimes within ~5% of BHEEM on F1).",
            "efficiency_gain": "Provides efficient coverage so fewer queries are wasted exploring already-sampled regions; no explicit numeric claim beyond RMSE comparisons.",
            "tradeoff_analysis": "Acts solely on exploration objective; when combined with exploitation inside BHEEM, yields better overall balance than iGS alone.",
            "optimal_allocation_findings": "iGS is a reliable exploration baseline but suboptimal alone for functions with localized sharp features; combining with an exploitation strategy (and learning weighting) improves allocation.",
            "uuid": "e2483.2",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "QBC",
            "name_full": "Query-by-Committee",
            "brief_description": "An ensemble-based active-learning strategy that selects queries maximizing disagreement among a committee of models, effectively focusing on regions where the model family is uncertain or where predictions most disagree.",
            "citation_title": "Query by Committee",
            "mention_or_use": "use",
            "system_name": "Query-by-Committee (QBC)",
            "system_description": "Maintain a committee of Q predictive models (in experiments: 10 GPRs with different kernels). For candidate x_j compute pairwise prediction differences |h_l(x_j)-h_p(x_j)| and take the maximum disagreement as the acquisition score; select x* maximizing this score. Emphasizes refinement in regions where model hypotheses disagree (often exploitation of boundaries or sharp features).",
            "application_domain": "Active learning for regression; used as an exploitation-focused acquisition function in the paper's experiments including materials case study.",
            "resource_allocation_strategy": "Allocates queries to regions of maximal model disagreement, directing budget to resolve local uncertainty and refine predictive models in those regions.",
            "computational_cost_metric": "Computes predictions of all committee members for candidate points; cost measured as additional wall-clock time relative to single-model acquisition functions.",
            "information_gain_metric": "Committee disagreement is used as a proxy for expected model change or uncertainty reduction; not a formal mutual information computation but correlated with learning value.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploitation strategy in this paper's framing (η=0 when used as exploitation component); focuses on resolving local uncertainty rather than exploring coverage.",
            "diversity_mechanism": "No explicit diversity mechanism; tends to concentrate queries in regions of disagreement and may ignore global coverage.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed-number-of-experiments; directs budget to uncertain regions.",
            "budget_constraint_handling": "Consumes budget to refine uncertain areas; can lead to over-concentrated sampling if used alone.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "RMSE when used as standalone acquisition and as component in BHEEM; QBC often quickly reduces error near discontinuities but may under-explore elsewhere.",
            "comparison_baseline": "Compared against iGS, variance, entropy, random, static/probabilistic tradeoffs, and BHEEM.",
            "performance_vs_baseline": "QBC is strong at resolving sharp local features and performs comparably to BHEEM in some high-uncertainty tasks; however, BHEEM outperforms QBC on average across datasets due to better balance. Example: BHEEM achieved ~21% lower average RMSE than QBC in a reported aggregated comparison.",
            "efficiency_gain": "Efficient at local error reduction but can be inefficient overall if global coverage is required; quantitative gains context-dependent in experiments.",
            "tradeoff_analysis": "Paper notes QBC's exploitation bias and tendency to oversample regions of discontinuity, motivating the need to combine it adaptively with exploration.",
            "optimal_allocation_findings": "QBC is a useful exploitation tool but should be combined with exploration and adaptively weighted (as in BHEEM) under fixed-budget experimental design to avoid neglecting other regions.",
            "uuid": "e2483.3",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MaxVar",
            "name_full": "Maximum Variance Acquisition",
            "brief_description": "An exploration acquisition that selects the next point with the highest posterior predictive variance under the Gaussian Process model, aiming to reduce model variance and thereby expected generalization error.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Maximum Variance acquisition",
            "system_description": "Compute GP posterior covariance for unlabeled candidates and select x with maximal predicted variance V(x) (the diagonal elements of cov(f) from GP posterior). This targets less-explored regions where the model is uncertain and is an exploration-biased information-based acquisition.",
            "application_domain": "Active learning for regression and experimental design where uncertainty quantification via GPs is available.",
            "resource_allocation_strategy": "Directs budget to points that will maximally reduce predicted variance (proxy for future generalization error reduction).",
            "computational_cost_metric": "Requires GP posterior covariance evaluations; cost measured in wall-clock time in experiments.",
            "information_gain_metric": "Posterior variance used as proxy for information gain / expected reduction in generalization error.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploration acquisition when used alone.",
            "diversity_mechanism": "Implicit via variance landscape; no explicit coverage/diversity enforcement like ALD or iGS.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed-number-of-experiments",
            "budget_constraint_handling": "Spends budget on high-uncertainty points; may achieve effective variance reduction but could ignore structured exploration needs.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "RMSE performance reported in comparisons; BHEEM showed modest improvements relative to maximum variance in aggregate (e.g., ~5.7% RMSE improvement in one comparison).",
            "comparison_baseline": "Compared with iGS, QBC, entropy, random, static/probabilistic tradeoffs, and BHEEM.",
            "performance_vs_baseline": "Generally effective for exploration-driven tasks; combined adaptively (as in BHEEM) yields better overall performance.",
            "efficiency_gain": "Reduces model variance efficiently per query, but gains depend on problem structure.",
            "tradeoff_analysis": "Acts purely to reduce uncertainty; paper uses it as a component to be balanced with exploitation via η.",
            "optimal_allocation_findings": "Maximum variance is a principled exploration objective but needs to be balanced with exploitation to avoid missing local high-error regions.",
            "uuid": "e2483.4",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MaxEnt",
            "name_full": "Maximum Entropy (Uncertainty Sampling)",
            "brief_description": "An information-theoretic acquisition that selects the candidate maximizing Shannon entropy of the GP posterior, targeting points of highest predictive uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Maximum Entropy acquisition",
            "system_description": "Compute Shannon entropy H[f] = 1/2 log |cov(f)| + (D/2) log(2πe) for the GP predictive distribution at candidate points and select the point with maximum entropy; in practice entropy reduces to functions of predictive variance for Gaussian marginals, focusing queries on high-uncertainty regions.",
            "application_domain": "Active learning for regression with Gaussian Process models.",
            "resource_allocation_strategy": "Allocates budget to points with maximal epistemic uncertainty (entropy) aiming to reduce model uncertainty globally.",
            "computational_cost_metric": "GP covariance determinant or variance evaluations; measured via wall-clock time in comparisons.",
            "information_gain_metric": "Shannon entropy over predictive distribution used as the measure of expected information content.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploration strategy when used alone.",
            "diversity_mechanism": "Implicit via entropy landscape but no explicit ALD-like filter.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed-number-of-experiments",
            "budget_constraint_handling": "Allocates queries to high-entropy regions until budget is exhausted; no dynamic balancing unless combined with other acquisitions.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "RMSE comparisons reported; generally more exploration-biased and competitive on some tasks but outperformed by adaptive combination in BHEEM.",
            "comparison_baseline": "Compared with iGS, QBC, maximum variance, random, static/probabilistic tradeoffs, and BHEEM.",
            "performance_vs_baseline": "Provides solid exploration behavior but BHEEM often yields lower RMSE by adaptively combining entropy-driven exploration with exploitation.",
            "efficiency_gain": "Effective at reducing uncertainty per query but may miss localized sharp features unless balanced with exploitation.",
            "tradeoff_analysis": "Serves as exploration component in trade-off; paper uses it to demonstrate that exploration-only approaches are suboptimal compared to adaptive weighting.",
            "uuid": "e2483.5",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Static/ProbTrade",
            "name_full": "Static and Probabilistic Trade-off Schedules",
            "brief_description": "Baseline strategies for balancing exploration and exploitation by either fixing the trade-off parameter η for all iterations (static) or making it decay probabilistically over time (probabilistic/ε-decreasing).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Static and Probabilistic trade-off baselines",
            "system_description": "Static trade-off: choose a fixed η (e.g., 0.25, 0.5, 0.75) a priori and use it for all query iterations, reducing computational complexity. Probabilistic trade-off: define exploration probability p_R = α^(t-1); at each iteration sample a uniform random Z and perform exploration (η=1) if Z ≤ p_R else exploitation (η=0), with α&lt;1 (paper uses α=0.7), leading to a decaying exploration schedule akin to simulated annealing.",
            "application_domain": "Active learning for regression; used here as baselines to compare adaptive allocation methods for expensive experiments.",
            "resource_allocation_strategy": "Static: simple fixed budget partitioning determined by η; Probabilistic: early-budget emphasis on exploration with decaying probability transitioning toward exploitation as iterations progress.",
            "computational_cost_metric": "Low computational overhead (no MCMC) measured in wall-clock time; presented as much faster than BHEEM.",
            "information_gain_metric": "None explicitly for static schedule; probabilistic schedule uses randomness to provide exploration early and exploitation later but does not optimize explicit information gain.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Static uses fixed linear weighting of exploration/exploitation; probabilistic uses a stochastic schedule that decays exploration probability over time.",
            "diversity_mechanism": "Only insofar as the chosen exploration acquisition encourages diversity; no explicit novelty filter like ALD.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed-number-of-experiments",
            "budget_constraint_handling": "Static: simple reduction in compute and straightforward budget allocation; Probabilistic: heuristic time-dependent allocation that does not adapt to observed data.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "RMSE reported at multiple iteration counts; heatmap comparisons show no single static η works well across functions or iterations. BHEEM outperforms both static and probabilistic approaches in aggregate.",
            "comparison_baseline": "These are the baselines used for comparison to BHEEM and other acquisition functions.",
            "performance_vs_baseline": "BHEEM 'distinctly provides better results than all the static and probabilistic trade-offs' per the paper; static/probabilistic schedules sometimes perform well on specific problems but are not generally robust.",
            "efficiency_gain": "Static/probabilistic are computationally cheap but can be suboptimal in labeled-data efficiency; BHEEM trades extra compute for better RMSE per labeled point.",
            "tradeoff_analysis": "Paper highlights limitations of fixed/trial-and-error schedules and motivates the hierarchical adaptive approach; shows via heatmaps that no one fixed η is optimal across problems or iterations.",
            "optimal_allocation_findings": "Recommendation: prefer adaptive, data-driven reweighting (BHEEM) over static or hand-tuned probabilistic schedules when experimental cost justifies extra computation.",
            "uuid": "e2483.6",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active learning with statistical models",
            "rating": 2,
            "sanitized_title": "active_learning_with_statistical_models"
        },
        {
            "paper_title": "Information-based objective functions for active data selection",
            "rating": 2,
            "sanitized_title": "informationbased_objective_functions_for_active_data_selection"
        },
        {
            "paper_title": "A novel active learning regression framework for balancing the exploration-exploitation trade-off",
            "rating": 2,
            "sanitized_title": "a_novel_active_learning_regression_framework_for_balancing_the_explorationexploitation_tradeoff"
        },
        {
            "paper_title": "Active learning for regression using greedy sampling",
            "rating": 2,
            "sanitized_title": "active_learning_for_regression_using_greedy_sampling"
        },
        {
            "paper_title": "Query by Committee",
            "rating": 2,
            "sanitized_title": "query_by_committee"
        },
        {
            "paper_title": "Markov chain Monte Carlo without likelihoods",
            "rating": 1,
            "sanitized_title": "markov_chain_monte_carlo_without_likelihoods"
        },
        {
            "paper_title": "A hierarchical expected improvement method for bayesian optimization",
            "rating": 1,
            "sanitized_title": "a_hierarchical_expected_improvement_method_for_bayesian_optimization"
        }
    ],
    "cost": 0.020056749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling
30 Sep 2023</p>
<p>Upala Junaida Islam uislam@asu.edu 
School of Computing and Augmented Intelligence
Arizona State University
USA</p>
<p>Kamran Paynabar 
H. Milton Stewart School of Industrial &amp; Systems Engineering
Georgia Institute of Technology
USA</p>
<p>George Runger 
School of Computing and Augmented Intelligence
Arizona State University
USA</p>
<p>Ashif Sikandar Iquebal 
School of Computing and Augmented Intelligence
Arizona State University
USA</p>
<p>Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling
30 Sep 2023224DD6367E2AD1F5C329761DB612BCE6arXiv:2304.07665v2[cs.LG]Active learning regressionExploration-exploitation trade-offBayesian hierarchical modelApproximate Bayesian Computation
Active learning provides a framework to adaptively query the most informative experiments towards learning an unknown black-box function.Various approaches of active learning have been proposed in the literature, however, they either focus on exploration or exploitation in the design space.Methods that do consider exploration-exploitation simultaneously employ fixed or ad-hoc measures to control the trade-off that may not be optimal.In this paper, we develop a Bayesian hierarchical approach, referred as BHEEM, to dynamically balance the explorationexploitation trade-off as more data points are queried.To sample from the posterior distribution of the trade-off parameter, We subsequently formulate an approximate Bayesian computation approach based on the linear dependence of queried data in the feature space.Simulated and realworld examples show the proposed approach achieves at least 21% and 11% average improvement when compared to pure exploration and exploitation strategies respectively.More importantly, we note that by optimally balancing the trade-off between exploration and exploitation, BHEEM performs better or at least as well as either pure exploration or pure exploitation.</p>
<p>Introduction</p>
<p>The past decade has witnessed a widespread adoption of machine learning techniques across a range of applications such as design and discovery of novel materials [1], monitoring and control of advanced manufacturing [2], and decision-making in healthcare [3].However, a majority of the success stories of machine learning methods have been limited to fitting large experimental and simulated datasets [4].With rising concerns about resource availability and increasing costs of conducting physical experiments and obtaining labeled datasets, there is a need to develop methodologies that will guide experimental efforts towards gathering the most informative experiments.</p>
<p>Active learning provides a framework to address these challenges by allowing the learning algorithm to adaptively select the most informative experiments in learning a concept (a classification or a regression model), reducing the need for obtaining large datasets [5].Here, the informativeness of data is usually assessed via an acquisition function.Following the seminal work of Angluin on "Queries and Concept Learning" [6] where the acquisition function was a majority vote among a set of candidate hypotheses, several acquisition functions were introduced that are based on entropy [7],</p>
<p>prediction uncertainty [8], etc.However, recent studies have shown that they may have exploration or exploitation bias [9].For instance, a greedy sampling-based acquisition function proposed in [10] (see Equation (11)), tends to acquire the data points uniformly in the unexplored search space and therefore favors exploration, irrespective of the underlying target concept.An example of this is shown in Figure 3(a).In contrast, a Query-by-Committee (QBC) approach for active learning [11] tends to exploit the search space in the neighborhood where the target concept violates the continuity and, in some cases, uniform continuity assumptions.An example is shown in Figure 3(b).This raises the classic, albeit important question: how can we encode exploration-exploitation trade-off in active learning for regression problems.Based on our survey of the literature, we note two prominent schools of thought.The first is based on constructing new acquisition functions or appropriate modifications thereof to balance exploration and exploitation [12].An example of the latter is the hierarchal expected improvement [13] that was introduced in an attempt of achieving non-myopic search in the context of Bayesian optimization.This was accomplished by modifying expected improvement [14] to encourage more exploration.But the most commonly investigated approach is based on optimizing a combination of different acquisition functions for exploration and exploitation through a trade-off parameter [9] as,
x * = argmax x (ηF 1 (x) + (1 − η)F 2 (x))(1)
Here, η ∈ [0, 1] is a parameter controlling the trade-off between F 1 (•) and F 2 (•), the objective functions corresponding to exploration and exploitation acquisition functions respectively, and x * is the optimal data to query according to the new strategy.Nonetheless, the challenge with this approach is that η is unknown and needs to be estimated during the learning process.Since it is difficult to estimate η without observing future data points, existing approaches have relied on trial and error [15] or predefined ad-hoc measures depending on the number of queried data [16].</p>
<p>The exploration-exploitation bias has received attention in the reinforcement learning literature as well.However, unlike active learning, the agent in reinforcement learning is only concerned with maximizing the long-term reward by finding an optimal sequence of actions via explorationexploitation.Conversely, in active learning, we are concerned with maximizing the reward over a finite (often very small) number of experiments that we can perform.The active learner, therefore, needs to iteratively and effectively update the trade-off parameter as new data are queried.</p>
<p>During active learning, the trade-off parameter is influenced by the noise in the labeled data at each query stage and by the variability in the labeled data across different query stages.This bi-level structure allows us to use a Bayesian hierarchical model to find the optimal trade-off parameter during the learning process.Here, the dependency of parameters related to the hierarchy is reflected in a joint probability model.The posterior density and uncertainty quantification of the parameters are obtained via Bayesian inference [17].In the absence of closed-form expressions for the posterior distribution, a sampling-based Markov chain Monte Carlo (MCMC) method is used to obtain the marginal posterior distributions [18].</p>
<p>To this end, we present a Bayesian Hierarchical Model to automatically balancing Exploration-Exploitation (referred as BHEEM) trade-off in actively learning an unknown black box function.In particular, BHEEM iteratively updates the trade-off parameter as data points are queried to minimize the generalization error.By imposing a hierarchical structure, we account for the variability in η arising from within each query iteration as well as across subsequent queries.We present an Approximate Bayesian Computation (ABC) approach along with Metropolis within Gibbs algorithm to sample from the posterior distribution of η.We subscribe to Gaussian Process Regression to obtain the best fit for the underlying black-box function.It is critical to note that the proposed methodology is generalizable and not contingent on the choice of exploration and exploitation or the choice of the regression function.We evaluate the efficacy of BHEEM on six simulated and benchmark datasets, and one real-world example in materials discovery.We also provide sensitivity analysis and numerical convergence results to establish the consistency of BHEEM.</p>
<p>The rest of the paper is organized as follows.Section 2 presents a brief relevant literature review.</p>
<p>Section 3.1 unfolds the Bayesian hierarchical model for estimating the trade-off parameter to balance exploration and exploitation.Section 3.2 explains the Gaussian process regression methodology we employed active learning with.Exploration and exploitation approaches adopted in this work are presented in Section 3.3 followed by trade-off strategies commonly used in the literature summarized in Section 3.4.Experimental setup, results from simulation and real-world experiments, and analysis over sensitivity and convergence are presented in Section 4. The paper is concluded in Section 5.</p>
<p>Literature Review</p>
<p>The roots of active learning can be traced back to the early 1960s with initial works focused on space-filling designs via sequential experimentation [19].Later studies developed model-based sequential experimental designs where an underlying data-generating model is considered to guide the search for the subsequent experiments [20]- [22].Model-based sequential design is commonly referred to as active learning in the machine learning community [5].Over the past two decades, active learning has witnessed significant developments in intelligently querying and labeling data, both in the classification [23]- [25] and regression tasks [10], [11], [26].In this section, we investigate active learning in regression problems with a particular emphasis on the exploration-exploitation trade-off.</p>
<p>Two pioneering works in active learning for regression problems are Active Learning -Mackay (ALM) based on maximizing the expected information gain [27] and Active Learning -Cohn (ALC) based on minimizing the generalization error [8].Mackay defined the information gain as the change of Shannon's entropy [28] before and after labeling data.Entropy has also been formulated as a measure for querying data in classification problems within the framework of "uncertainty sampling" [29].Cohn et al. [8], on the other hand, demonstrated that the expected generalization error is composed of data noise, model bias, and variance.Data noise is independent of the model, and model bias is invariant given a fixed model.Thus the criteria for querying data was simplified to the minimization of the total variance.Meka et al. [30] extended the ALC strategy by adding a regularizing term to integrate the information of both queried and unqueried data.</p>
<p>A more theoretically-motivated active learning strategy called Query-by-Committee (QBC) [31] was first developed for regression by Krogh and Helseby [32].QBC maintains a committee of hypotheses that are simultaneously trained on the labeled data.The data that maximizes the disagreement between the committee members is deemed the most informative.A variation of QBC, Expected Model Change Maximization (EMCM) was also developed in a regression setting where the expected model change was constructed using the gradient of the error concerning candidate data [26].O'Neil et al. [33] compared QBC and EMCM along with a few model-free strategies that select the unlabeled data based on density, or diversity, and through an acquisition function named Exploration Guided Active Learning (EGAL) combining both density and diversity [34].Results</p>
<p>from O'Neal et al. [33] indicated that the integral properties of the dataset, such as its geometry, can be promising candidates for active learning strategies.A similar diversity-based passive sampling strategy, Greedy sampling (GS) was adapted as improved Greedy Sampling (iGS) for active learning by incorporating the response information ensuring exploration in both input and output space [10].</p>
<p>While significant developments have taken place in the active learning literature, only a handful of the efforts have considered the problem of exploration-exploitation trade-off, the majority of which is limited to classification problems.For example, past researches have used maximum entropy [7], and distance and similarity-based metrics for exploration [35] while subscribing to mostly uncertainty and redundancy to exploit near the current decision boundary [36].Approaches to handle the exploration-exploitation bias include switching between exploration and exploitation based on their performance at each iteration [35], explore-then exploit approach [37], random probabilistic measure [16], or combining exploration and exploitation in a predefined ratio [9], [33], [38].Combining the acquisition functions based on cross-validation, sensitivity analysis, or trial-and-error [15] can only be performed retrospectively and does not help with prospective data queries.</p>
<p>Beyond active learning, the exploration-exploitation problem has also been investigated in the reinforcement learning literature.Here, the exploitation of the greedy approach has been confronted with various exploratory approaches.Upper Confidence Bound (UCB) is one of the most celebrated approaches against this bias.Here, uncertainty is used to balance the exploration and exploitation instead of choosing the greedy action according to current knowledge [39].The ϵ−greedy approach decays the randomness or the rate of exploration over the learning process to encourage more exploitation of the observed knowledge [40].The Boltzmann exploration approach uses exponential weighting schemes to balance exploration and exploitation in a probabilistic fashion [41].The balancing parameter has also been obtained using a trial-and-error process [42] or controlled based on a variation of action results and perception of environmental change [43].</p>
<p>Based on the literature review, we notice that the existing methods have either considered either a predefined ratio, probabilistic function or trial and error methods to control the explorationexploitation trade-off.None of the efforts have focused on adaptively updating the trade-off parameter as more data is queried.This study advances the research on dynamically updating the trade-off between exploration and exploitation as more knowledge is gathered about the black-box function from the queried data.</p>
<p>Methodology</p>
<p>In this section, we present the general schema of active learning along with the models and strategies we employ in this study.Let X = {x 1 , x 2 , . ..} and y = {y 1 , y 2 , . ..} represent the possible collection of all design points and their corresponding responses respectively.In the generic problem of active learning, we consider that the actively growing set of queried (i.e., labeled) data
D N = {(x o i , y o i )} N i=1
is accessed by the underlying model.Here, x o i ∈R d and y o i ∈R are the queried data and their corresponding output responses respectively.The objective of active learning in a regression setting is to select the next data x o N +1 from the search space, X, which will be the most informative towards learning the unknown black-box function f (x) given the current knowledge.The response y o N +1 is obtained via simulation or physical experiments.By actively selecting the data to learn from, it can rapidly reduce the generalization error which is the prediction error of the algorithm at the data unobserved so far.The process is iterated until stopping criteria e.g., the maximum number of labeled data, or predicted change, have been satisfied.See Figure 1 for a flow chart of the active learning approach.</p>
<p>Exploration-exploitation trade-off</p>
<p>In machine learning, exploration refers to generating novel information in the search space while exploitation focuses on improving decisions (e.g., maximizing reward) based on the available information.Although this is a generally accepted definition of exploration and exploitation [44], its usage varies depending on the context.Particularly, in the regression setting, exploration refers to sampling data from the unobserved regions in the search space to gather more (potentially novel) information about the black-box function, such as local minima/maxima.In contrast, exploitation aims at accurately capturing the black-box function in the regions where it is highly unpredictable and has a sharp change or discontinuity.As mentioned in the foregoing, it is possible to encode exploration and exploitation separately in an acquisition function [34], however, it is challenging to dynamically balance between the two [45].We consider a general framework for sampling data points that maximizes a linear combination of acquisition functions aimed at simultaneous exploration and exploitation as presented in Equation (1).The next data is queried through pure exploitation when η = 0, and pure exploration when η = 1.By increasing η from 0 to 1, the degree of exploration increases in the constructed acquisition function.</p>
<p>To estimate η dynamically, we consider two levels of variability.First is the variability in queried data emerging from the noise and measurement errors in y at each querying stage.The second level of variability is associated with the dynamic nature of η between querying stages due to the nature of the unknown black-box function (e.g., jumps or discontinuities).This bi-level nature of variability motivates the need for a hierarchical model.A Bayesian hierarchical model allows us to capture this hierarchy while simultaneously encoding the uncertainty at each of the levels.The trade-off parameter η j captures the first level of variability at querying stage j given the set of hyperparameters θ.We model {η j |θ j } iid ∼ p(η|θ j ).Here, the conditional independence of {η j |θ j } tells us that the trade-off parameters at one sampling stage are exchangeable (de Finetti's theorem [46]), but not independent.To capture the variability in η across multiple sampling stages, we model
{θ j |ψ} iid ∼ p(θ|ψ)
where ψ is another set of hyper-parameters fixed from a priori assumptions.</p>
<p>De Finetti's theorem from this conditional independence shows that the trade-off parameters at subsequent querying stages are exchangeable as well.Indeed, this aligns with our assumption that the trade-off parameter at any querying stage is not completely independent of the previous actions (exploration or exploitation).In the next two subsections, we discuss the prior and the sampling distributions employed in this study.</p>
<p>Prior distribution</p>
<p>Towards optimally estimating η using a Bayesian hierarchical framework, we begin by considering a prior distribution based on the knowledge that it lies between 0 (pure exploitation) and 1 (pure exploration).With this knowledge, we let η follow a beta distribution in the form of η ∼ Beta(α, β)</p>
<p>with hyperparameters α and β.The combined selection of α and β defines the shape of the prior distribution of η as observed in Figure 2. A larger value of α shifts the bulk of the probability towards 1 and emphasizes exploration (e.g., α = 5, β = 0.1), whereas an increase in β moves the distributions towards 0 and encourages exploitation (e.g., α = 0.1, β = 5).α = β &lt; 1 creates a U-shape distribution with maximum probability near 0 and 1 imposing a similar higher probability on both exploration and exploitation (e.g., α = β = 0.1).On the other hand, α = β &gt; 1 generates a bell-shape distribution with the maximum probability at the middle (η = 0.5) imposing a lower probability on pure exploration and exploitation, and a higher probability on a mixture of them</p>
<p>Posterior and sampling distribution</p>
<p>The joint posterior distribution of η and the hyper-parameters is, p(η, α, β|y, X) ∝ p(α)p(β)p(η|α, β)p(y, X|η)</p>
<p>where {(X, y)} represents the collection of both labeled and unlabeled data.In Equation (2), p(y, X|η) is the likelihood of η denoted by L(η; y, X), and p(η|α, β) is the prior.The hyperprior p(α)p(β) = p(α|a)p(β|b) due to the independence of α and β as well as a priori selection of a and b.The joint posterior distribution in Equation ( 2) is not available in the closed form due to the intractable likelihood function, p(y, X|η).Therefore, the unknown parameters are sampled from their respective full conditional distributions using Gibbs sampling.The individual full conditional distribution of α, β, η is factorized as,
p(α|β, η) ∝ p(α)p(η|α, β) p(β|α, η) ∝ p(β)p(η|α, β) p(η|α, β, y, X) ∝ p(η|α, β)p(y, X|η)(3)
The Gibbs sampler proceeds by iteratively constructing a dependent sequence of parameter values whose distribution converges to the target joint posterior distribution [47].But the likelihood of η is intractable unlike that of α and β since the generation of new data from observed η does not follow any known distribution.In the absence of a tractable likelihood, we subscribe to Approximate Bayesian Computation (ABC) to approximate the posterior distribution of η which has been used in numerous previous applications where evaluation of the likelihood is expensive or infeasible [48].</p>
<p>The traditional ABC algorithm [49], also known as ABC rejection sampler, follows a two-step procedure to sample from the posterior distribution.First, it draws samples of the unknown parameter from its prior distribution.Second, the sampled parameter values are accepted based on the similarity between the data simulated under some model specified by the sampled parameter values and the queried data, or between the summary statistics thereof.In other words, ABC uses summary statistics to filter samples that do not agree with the queried data.Though efficient in simple probability models, ABC rejection is limited in complex ones where the posterior is greatly different from the prior since it involves sampling from the prior [50].Different computational methods have been presented in the literature to overcome this limitation of ABC inference, among which ABC-MCMC and its variants have been widely used and accepted [50].The ABC-MCMC algorithm combines ABC with Metropolis-Hastings or Metropolis algorithm for iterative approximate Bayesian computation.In the following, we will discuss the Metropolis sampling followed by a new approach to implement ABC and summary statistics into BHEEM.</p>
<p>Metropolis sampling is a simplified form of the Metropolis-Hastings sampling algorithm.It proposes a symmetric proposal distribution J(θ|θ old ) for the parameter vector θ given the most recent accepted sample θ old at each sampling stage.Each proposed sample is either accepted or rejected depending on an acceptance ratio computed from the prior and likelihood of the proposed and old samples.ABC-MCMC algorithm shadows the steps of a typical Metropolis algorithm.The initial sample is drawn from the prior, and the next ones are sampled from a proposal distribution centered at the most recently accepted sample.But the acceptance ratio constructed by ABC uses summary statistics, therefore replacing the likelihood ratio like in the regular Metropolis algorithm.</p>
<p>We initiate the Gibbs sampler with θ (0) = {α (0) , β (0) , η (0) }, and generate a dependent sequence of the parameter vector, {θ (1) , θ (2) , . . ., θ (s) }.Given a current state of the parameters θ (s) = {α (s) , β (s) , η (s) }, the next state of the parameter vector is generated as follows,</p>
<p>Step 1: Sample α (s+1) ∼ p(α|β (s) , η (s) )</p>
<p>Step 2: Sample β (s+1) ∼ p(β|α (s+1) , η (s) )</p>
<p>Step 3: Sample η (s+1) ∼ p(η|α (s+1) , β (s+1) , y, X): As mentioned before, we employ ABC-MCMC at this stage.Similar to a typical Metropolis algorithm, we sample η (s+1) till it converges.</p>
<p>We propose η from a proposal symmetric distribution centered at η in the absence of the likelihood, ABC reconstructs the acceptance ratio r η as,
r η = p η|α (s+1) new , β (s+1) new p η (s+1) old |α (s+1) new , β (s+1) new I{d(S(x), S(X o )) ≥ ν}(4)
where S(x) and S(X o ) are the sufficient statistics for data nominated by η and the queried data respectively, d(•, •) is a distance measure between the two statistics, and I{•} is an indicator function.</p>
<p>In traditional ABC, the fitness of a candidate sample is measured in terms of its similarity to the observed data.However, in active learning, it is measured in terms of the new information generated by each new query.To measure the fitness of candidate queries, we utilize their linear dependency on the already queried data in the Hilbert space.Any data x is considered approximately linearly dependent (ALD) on the already queried set
X o if, δ = min a N −1 i=1 a i ϕ(x o i ) − ϕ(x) 2 ≤ ν(5)
ν is an threshold parameter determining the level of sparsity, and ϕ is a finite-dimensional mapping of the feature vectors, x, to a Hilbert space.If the ALD condition in Equation ( 5) holds, ϕ(x) can be inferred by the already queried data, X o , as shown in the expression that ϕ(x) = i a i ϕ(x o i ).In this case, no new information is gained and we reject the candidate query.In contrast, if the ALD criterion is not satisfied, i.e., δ &gt; ν, then we select the candidate query and collect its response.</p>
<p>However, to use the ALD criterion in order to measure the fitness of candidate queries, we need to define the mapping ϕ.In the absence of the function phi, we invoke Mercer's Theorem [51] that guarantees the existence of a kernel function k(x, x ′ ) such that k = ⟨ϕ(x), ϕ(x ′ )⟩.As a result, all the calculations in the feature space can be performed by defining the kernel function instead of the function ϕ.With the substitution of ϕ with the kernel function, we obtain,
a * = K(X o , X o ) −1 K(X o , x) and δ = K(x, x) − K(X o , x) T a *(6)
For each candidate η proposed by the Metropolis algorithm, we check the linear dependency of the nominated query x.In particular, x is accepted for querying if δ ≥ ν such that I{δ ≥ ν} = 1.In that case, we compare the prior of η and η irrespective of the priors.Authors in [52] experimented with different values for ν using cross-validation.Through the sensitivity analysis ν as discussed in Section 4.5, we fixed ν = 0.001 for all our experimentation.</p>
<p>Step 4: Let θ (s+1) = {α (s+1) , β (s+1) , η (s+1) }.After generating {θ (1) , θ (2) , . . ., θ (s) }, the approximation of η j is obtained by averaging the observed {η
(1) j , η(2)
j , . . ., η
(s) j }.
In this work, we defined the proposal distribution as N (η old , τ 2 ) where η old is the sample parameter accepted in the last iteration, and τ 2 is the variance.The performance of the MCMC chain depends on the variance of proposal distribution, τ 2 .If τ is too small, i.e., 0.001, the Metropolis can get stuck at one sample and may take a long time for the chain to converge.On the other hand, if τ is too large, i.e., 0.25, it is possible to sample highly diverse and sometimes infeasible values increasing the rejection rate as well as hampering the rate of convergence.A detailed sensitivity analysis for the effect of τ on the convergence of the algorithm is discussed in Section 4.4.</p>
<p>Gaussian Process Regression</p>
<p>In the absence of any known functional form of f (x), we consider Gaussian process regression (GPR) as our underlying learning model.GPR considers a distribution over the underlying function f (x) and aims to specify the black-box function by its mean and covariance.Due to the noise inherited in labeled data without the learner's knowledge, the output y comprises f (x) as y = f (x) + ε where ε ∼ N 0, σ 2 n .Observing the noisy outputs, y o , GPR attempts to reconstruct the underlying function f (x) by removing the contaminating noise, ε [53].We denote the queried dataset as (X o , y o ).GPR imposes a zero-mean Gaussian process prior over the noisy outputs such that,
y o ∼ N 0, K(X o , X o ) + σ 2 n I(7)
The joint prior distribution between the training output set y and test output set f is as following,
   y f    ∼ N m   0,    K(X o , X o ) + σ 2 n I K(X o , X u ) K(X u , X o ) K(X u , X u )       (8)
where X u is the set of unlabeled testing points.The posterior distribution at the test data is given
as { f |X o , y o , X u } ∼ N f , cov( f ) where, f = K(X u , X o )[K(X o , X o ) + σ 2 n I] −1 y o (9) cov( f ) = K(X u , X u ) − K(X u , X o )[K(X o , X o ) + σ 2 n I] −1 K(X o , X u )(10)</p>
<p>Acquisition functions</p>
<p>In this section, we present some of the most commonly used acquisition functions and characterize them either as an exploration or an exploitation strategy based on the literature and our extensive simulation studies.</p>
<p>• Improved Greedy Sampling: As mentioned in Section 2, the concept of improved greedy sampling (iGS) was introduced to ensure diversity in the queried data [10].At every iteration, iGS determines the unexplored region by searching over the entire region in both the input and output spaces.It queries the next data which is located the farthest from its nearest training point or in an unobserved region according to the then prediction by the regression model.The acquisition function at x j is calculated as min (u(x j )v(x j )) where u(
x j ) = ∥x j − X o ∥ 2 and v(x j ) = ∥ fj − y o ∥ 2
refer to the distance of x j with training points in input and output space respectively.Here y o is the set of the observed output or the labels at training points X o , fj is the predicted output at test point x j , and ∥ • ∥ 2 is the L2 norm.IGS defines the distance at x j as u(x j )v(x j ) instead of u(x j ) + v(x j ) or (u(x j )) 2 + (v(x j )) 2 due to the possibility of significantly different scale of u and v which can hamper the latter two formulas with the dominance of one measure over another.It then selects the next data, x * iGS , such that,
x * iGS = argmax x j min ∥x j − X o ∥ 2 ∥ fj − y o ∥ 2(11)
By selecting data from unexplored regions in both input and output spaces, iGS avoids sampling from any concentrated region and hence satisfies the requirement for an exploration strategy.</p>
<p>• Query by Committee: Successfully applied in classification and regression-based problems, Query by Committee (QBC) is a framework rooted in the concept of utilizing an ensemble of hypotheses [31].Maintaining a committee of models, QBC queries the data where the committee members disagree the most about a measure of criteria.The committee members are all trained on the same set of training points but with competing hypotheses or regression models.Considering a committee of Q models denoted as h 1 , h 2 , . . ., h Q , we define the measure of disagreement between two models h l and h p at x j as the absolute difference of prediction by the respective models at that point, or |h l (x j )−h p (x j )|.Then the acquisition function at x j is defined as max l,p (|h l (x j )−h p (x j )|), representing the maximum disagreement at x j where l, p = 1, 2, . . ., Q. Then the next data, x * QBC is selected such that,
x * QBC = argmax x j max l,p (|h l (x j ) − h p (x j )|)(12)
QBC approach based on GPR allows us to exploit the regions in the search space where the function's behavior is uncertain i.e., discontinuities or change points.As indicated in [53], the mean square (MS) continuity and differentiability of kernel functions control their flexibility.For example, let us compare GPR models with exponential, Matérn 3/2, Matérn 5/2, and squared exponential kernel functions.The non-differentiable exponential kernel generates the roughest prediction, whereas the infinitely differentiable squared exponential kernel produces the smoothest ones.An intermediate level of smoothness can be observed in Matérn 3/2 and Matérn 5/2 kernels which are one and two times MS differentiable respectively [53].The sharp changes in underlying functions are captured by the rough predictions via dense queries.But the smoother prediction deviates from the underlying model at the region.Therefore, kernel functions with different MS continuity and differentiability behave differently where the functional form is unpredictable with discontinuity or sharp change.When QBC selects the data at which committee members differ the most in their prediction, it keeps exploiting that very region.</p>
<p>• Maximum variance: This strategy defines the acquisition function as the variance predicted by the regression model [54].A learner's expected error can be decomposed into noise, bias, and variance [8].Since the noise is independent of the model or data, and bias is invariant given a fixed model, minimizing the variance is intuitively guaranteed to minimize the future generalization error of the model [5].Equation (10) provides the Gaussian process posterior covariance, and the diagonal elements of cov( f ) provide the predicted variance, V.The next data is queried following,
x * VAR = argmax x j (V(x j ))(13)
Since predicted variance is higher mostly in less-explored regions, it can be implemented as an adequate exploration acquisition function to query the next data [54].</p>
<p>• Maximum entropy: Another active learning strategy formulated based on uncertainty is the maximum entropy strategy [5].Entropy is an information-theoretic measure that often has been defined as the amount of information needed to "encode" a distribution and has been related to the uncertainty of the underlying model [5].Thus minimizing model entropy can reasonably lead to revealing the model uncertainty.Shannon's entropy has been used as an acquisition function named maximum entropy sampling [7] or uncertainty sampling [5].Since the posterior prediction of the Gaussian process follows a multivariate normal distribution, Shannon's entropy of the distribution,
H[ f , cov( f )] = 1 2 log |cov( f )| + D 2 log(2πe)
where D is the dimension of the variable, and f and cov( f ) are the predicted mean and covariance according to Eqs. ( 9) and ( 10) respectively [53].By minimizing the learning model uncertainty, this strategy is also more prone to exploration than exploitation.The maximum entropy strategy queries the data where it has the highest entropy following,
x * ENT = argmax x j H<a href="14"> f , cov( f )</a></p>
<p>Exploration-exploitation trade-off functions</p>
<p>As touched on briefly in the introduction, the existing approaches have tried achieving the trade-off between exploration and exploitation using different methodologies [16], [38], [55], [56].</p>
<p>Here we have listed a few that we will compare BHEEM with.</p>
<p>• Static trade-off: Many of the previous studies have held on to static trade-offs between exploration and exploitation [38].Prior information can be used to decide on the trade-off (e.g., equal importance to both or more importance to one based on the nature of the function).Existing approaches have conducted trial and error with different trade-off values between exploration and exploitation between appropriate exploration and exploitation acquisition functions to conduct the simulated experiments, and then select the one that promises overall better accuracy [55].By fixing the exploration-exploitation trade-off throughout the whole learning process, this method reduces the computation complexity to a great extent.Nevertheless, the same trade-off cannot be expected to demonstrate similar performance for every function (as can be seen in the experimental results).</p>
<p>• Probabilistic trade-off: Inspired by simulated annealing [57] and built on the ϵ−decreasing greedy algorithm [56], this updates the exploration-exploitation combination in a probabilistic approach [16].For instance, the exploration probability is defined as p R = α t−1 where α is less than 1 and t is the current time step or iteration number.To query new data, a uniform random variable Z is generated, if Z ≤ p R , we consider η = 1, and exploration is performed, otherwise, we consider η = 0, and exploitation is applied [16].Hence the strategy starts with pure exploration.</p>
<p>The exploration probability decays gradually, and the learning model gets more prone to pure exploitation over time.Intuitively, this transition is practical since the initial queried data should focus more on exploration, and after learning the overall trend of the function, we can identify the irregular regions via exploitation.However, the transition rate depends on the choice of α which we fixed at 0.7 following Elreedy et al. [16].But again, the transition rate should depend on the nature of the function and fixing it will decrease the efficiency of the learning process.</p>
<p>Experimental results</p>
<p>In this section, we present the experimental setup and focus on the performance evaluation of the proposed methodology.We compare the performance of BHEEM with other active learning strategies over six simulated experiments and one real-world case study for predicting the property of MAX phase materials.Later on, we discuss convergence and sensitivity analysis of the process.</p>
<p>Experimental setup</p>
<p>To demonstrate the efficacy of BHEEM, we employ the Matérn 3/2 kernel function in the GPR model to avoid too rough (exponential kernel) or too wavy (squared exponential kernel) prediction.</p>
<p>The kernel function is defined as,
K ν=3/2 (z) = σ 2 f 1 + √ 3z/l exp − √ 3z/l (15)
where σ 2 f is the signal variance that we fixed at 1, z = ||x − x ′ || 2 , and l is the length-scale parameter optimized by maximizing the log marginal likelihood of the Gaussian process regression model.</p>
<p>While applying the QBC strategy, we maintained a committee of ten Gaussian process models, each with a different kernel function; (i) squared exponential, (ii) exponential, (iii) Matérn 3/2, (iv) Matérn 5/2, (v) rational quadratic, (vi) product of dot product and constant kernel, (vii) product of i and iii, (viii) product of i and iv, (ix) product of ii and iii, (x) product of ii and iv [53].For the generation of the data, we fixed the signal-to-noise (SN) ratio to 10 which we define as the decibel of the ratio of signal power to the power of the data noise.For the performance evaluation, we calculated the root mean squared error as,
RMSE = 1000 k=1 ( f (x k ) − f (x k )) 2 1000 (16)
where f (•) and f (•) represent the true and predicted response respectively at the set of equidistant test points, {x k } 1000 k=1 .We repeated all simulated experiments 100 times to achieve a consistent estimate of the performance.</p>
<p>Simulated experiments</p>
<p>To demonstrate the performance of BHEEM against the existing strategies, we considered the six following functions.
• F 1 (x) =        3.5 exp − (x−10) 2 200 + ϵ, if x ≤ 25 8 − 3.5 exp − (x−35) 2 200 + ϵ, otherwise.(17)• F 2 (x) = sin(x) + 2 exp(−30x 2 ) + ϵ x ∈ <a href="18">−2, 2</a>
• Three-hump camel function:
F 3 (x) = 2x 2 1 − 1.05x 4 1 + x 6 1 /6 + x 1 x 2 + x 2 2 + ϵ x 1 , x 2 ∈ <a href="19">−5, 5</a>
• Six-hump camel function:
F 4 (x) = (4 − 2.1x 2 1 + x 4 1 /3)x 2 1 + x 1 x 2 + (4x 4 2 − 4)x 2 2 + ϵ x 1 , x 2 ∈ <a href="20">−2, 2</a>
• Hartmann 3D function:
F 5 (x) = − 4 i=1 α i exp − 3 j=1 A ij (x j − P ij ) 2 + ϵ x i ∈ [0, 1] ∀i = 1, 2, 3(21)
where α = (1.0,1.2, 3.0, 3.2)
T , A =         3         • F 6 (x) = 1 2 10 i=1 x 2 i − cos (2πx i ) + ϵ x i ∈ [−5, 5] ∀i = 1, 2, . . . , 10(22)
First, we implement BHEEM by employing iGS and QBC as the pure exploration and exploitation strategy respectively.With three random initial queried data, Figure 3 shows the result of iGS, QBC, and BHEEM after the 12 th iteration for the underlying function from Equation (18).Unsurprisingly, iGS spreads out its queried data in both the x and y direction predicting the smooth portions of the function competently, but fails to fit the peak of the function due to the sudden change near x = 0. QBC exploits this uncertainty in predictions across different committee members who lead to different predictions near the sharp peak.Therefore, QBC queries most of the data in this region while ignoring other regions in the process.BHEEM queries data near x = 0 enough times to fit the peak there but has also explored and provided satisfactory prediction in other regions achieving an overall lower error than the other two acquisition functions.17), (b) Equation ( 18), (c) Equation ( 19), (d) Equation ( 20), (e) Equation ( 21), (f) Equation ( 22) as well as average computational time for all strategies.</p>
<p>Nevertheless, BHEEM is either better (Figure 4(b,e)) or at least as accurate as one of the approaches (Figure 4(c)).</p>
<p>Among the one-dimensional problems, BHEEM achieved the lowest RMSE compared to others after the query of the first 10 data in the case of F 1 (x) as observed in Figure 4(a).IGS was the closest one in this case and achieved only about 5% higher RMSE on an average.While predicting F 2 (x), BHEEM converged the fastest and performed the best as shown in Figure 4 (b).The second best strategy was iGS which scored about 60% higher RMSE on average from BHEEM after querying 25 data.Among the two-dimensional problems, in F 3 (x), BHEEM surpassed others most of the time and at the 50 th addition of point, achieved about 8.5% lower RMSE from QBC which scored the closest to BHEEM for the corresponding functions.For F 4 (x), the consistency of lower generalization error and fast convergence persisted for BHEEM.For the three-dimensional F 5 (x), BHEEM and QBC were the two strategies achieving the lowest RMSE across the querying stages.BHEEM was able to achieve significantly lower RMSE than the pure exploration strategy, iGS.In the ten-dimensional problem of F 6 (x) (Figure 4(f)), all the active learning strategies are observed to compete with each other over the learning process.The pure exploration strategy, iGS, is performing better than all other acquisition functions at the 50 th addition of data whereas QBC seems to achieve the lowest RMSE at the 100 th addition.In most of these cases, all active learning strategies performed better than the random sampling strategy.Our result was affected by a few decisions including our choice of kernel, sampling algorithm and proposal distribution, number of points in the initial set of queried data, etc.However, the overall result demonstrates that our proposed approach tends to perform at least tantamount to pure exploration or pure exploitation.</p>
<p>To compare the computational time taken by the applied methodologies, Figure 4 also presents the average time for each methodology to query 100 data during the learning process.Our proposed methodology, BHEEM, takes a significantly larger processing time, about 2.5 and 2 times higher than the duration of iGS and QBC, respectively.Hence, BHEEM is more suitable for offline applications that involve time-consuming physical experiments than the onlineones, which is the case for most applications in manufacturing and materials that involve conducting time consuming physical experiments.We also compare our proposed methodology with static and probabilistic updates of η.For the static trade-off, we considered η = 0.25, 0.5, 0.75 during the learning process individually and calculated the RMSE for each of them.To note, η = 0 and η = 1 refer to pure exploitation and pure exploration respectively which we have already compared with BHEEM in Figure 5 and Figure 6.</p>
<p>For the probabilistic update of η, we followed the strategy described in Section 3.3.2.A heatmap for the average RMSE scaled for each of the comparative trade-off methodologies is presented in Figure 6 where blue and red cells represent more and less accurate models respectively.The heatmap clearly shows that no one value of η works well for every function, or even for every iteration in one function.BHEEM distinctly provides better results than all the static and probabilistic trade-offs.Finally, Figure 7 presents the average improvement achieved by BHEEM from pure exploration and exploitation strategy across the functions.Here, we have used iGS and maximum variance as the exploration strategy in Figure 7(a) and Figure 7(b) respectively, and QBC as the exploitation strategy in both cases.In Figure 7(a), we observe about 7% and 21% lower average RMSE from pure exploration (iGS) and exploitation (QBC) respectively.In Figure 7(b), we observe about 5.7% and 2.3% lower average RMSE from pure exploration (maximum variance) and exploitation (QBC) respectively.Overall, our proposed methodology of trade-off always promises better accuracy than pure exploration and exploitation irrespective of the choice of strategies.</p>
<p>Case study: MAX phase materials</p>
<p>The layered ternary carbides and nitrides with the general formula M n+1 AX n are called MAX phase materials where M and A are early transition metal and A-group elements respectively, whereas X is Nitrogen or Carbon and n is an integer between 1 and 4 [58].Their layered structures kink and delaminate the materials during deformation resulting in an unusual and unique combination of both ceramic and metallic properties which makes them attractive candidates for structural and fuel coating applications.In this study, we intended to predict the lattice constants of MAX phase materials by analyzing their compositions and elastic constants using the data from Aryal et al. [59].The lattice constant is an important piece of information to define the overall lattice structure, which helps model the microstructure evolution.To represent the discrete categorical composition features into numeric input to the models, we used one-hot encoding, a popular approach replacing the categorical variable with as many variables as categories [60].Assuming that we wish to differentiate between two materials with the same elements in M and A, and the same value of n, one of them is a carbide, while the other is a nitride.It is possible to represent this information with two binary variables using one-hot encoding where 0 and 1 represent the non-existence and existence of that element in the material respectively.</p>
<p>Each category value is represented as a 2-dimensional, sparse vector of 1 for one of the dimensions, and 0 for the other.In general, for variables of cardinality d, the one-hot encoding would transfer it to d number of binary variables where each observation indicates the presence (1) or absence (0) of the dichotomous binary variable [60].In the MAX phase problem, we have ten elements in M, twelve elements A, and two elements in X.Hence, after using one-hot encoding and adding the numerical variable for n, we have a total of 25 variables representing the composition of the materials.Including the composition and the elastic constants, there is a total of 30 predictors to predict the lattice constant.</p>
<p>Convergence</p>
<p>In this section, we discuss the convergence of sampled η one querying stage, its relation with the choice of hyperparameters, and the dynamics η during the active learning process.The results in this section are all based on the learning process of F 1 (x).</p>
<p>• Convergence of Metropolis: We visualize the progression of the Metropolis algorithm in</p>
<p>Conclusion</p>
<p>In this work, we address the exploration-exploitation problem in active learning for regression problems.Our approach, BHEEM, is based on dynamically balancing the trade-off between exploration and exploitation during the learning process using a Bayesian hierarchical model.BHEEM captures the variability in the trade-off parameter during each query stages and across successive query stages.In the absence of a likelihood function, we devised an approximate Bayesian computation based on the linear dependence of the data in the Hilbert space to sample from the posterior distribution of the trade-off parameter.We demonstrated and compared BHEEM with exploration, exploitation, and other well-known strategies to balance the two in the six simulated and one real-world case study.From the average percentage improvement in the simulated experiments, BHEEM achieved about 21% and 24% lower average RMSE from pure exploration and exploitation respectively irrespective of the chosen strategies.Overall, when compared to existing active learning approaches, BHEEM converged with fewer iterations in all cases and performed better or at least as effective as the more efficient strategy among the two it combines.The proposed approach to dynamically balancing exploration and exploitation has wide applications in various domains where conducting experiments and labeling data is costly such as materials characterization, mechanical testing, manufacturing, and medical sciences.Some of the limitations of the current studies include the assumption on the filtering distance threshold ν and more importantly, a restricted prior on the trade-off parameter η.In future works, we plan to consider a more expressive and flexible Dirichlet process prior to ensure consistency and faster convergence of the hierarchical model.]</p>
<p>Figure 1 :
1
Figure 1: General schema of an active learner.</p>
<p>Figure 2 :
2
Figure 2: Probability density function for the beta distribution.</p>
<p>new = η with probability min(1, r η ) or rejected η (s+1) new = η (s+1) old with probability 1 − r η where the acceptance ratio, r η , is constructed as, r η = p (η|α, β, y, X) p η (s+1) old |α, β, y, X old But we do not have access to an explicit expression for the likelihood p(y, X|η).To check the fitness of the proposed sample η and to compare it with the current sample η (s+1) old</p>
<p>old before accepting one of them as η (s+1) new .On the other hand, if δ &lt; ν, it makes I{δ ≥ ν} = 0, implying η</p>
<p>Figure 3 :Figure 4
34
Figure 3: (a) Queried data and fitted function using exploration, (b) queried data and fitted function using exploitation, (c) queried data and fitted function using BHEEM with a dynamic trade-off between exploration and exploitation.</p>
<p>Figure 4 :
4
Figure 4: RMSE for BHEEM, iGS, QBC, maximum variance, maximum entropy, and random sampling strategy for (a) Equation (17), (b) Equation (18), (c) Equation (19), (d) Equation (20), (e) Equation (21), (f) Equation (22) as well as average computational time for all strategies.</p>
<p>Figure 5
5
Figure 5 presents the percentage improvement of BHEEM from the pure exploration (iGS) and pure exploitation (QBC) calculated from the average RMSE obtained in the simulated examples.</p>
<p>Figure 5 :
5
Figure 5: Percentage improvement of BHEEM from iGS and QBC acquisition function for the six simulated studies.</p>
<p>Figure 6 :
6
Figure 6: Average RMSE for BHEEM, static trade-off, and probabilistic trade-off after adding the 5th, 10th, and 25th data for the simulated functions.</p>
<p>Figure 7 :
7
Figure 7: (a) Average percentage improvement of BHEEM from pure exploration (iGS) and pure exploitation (QBC) acquisition function across simulated studies, (b) Average percentage improvement of BHEEM from pure exploration (maximum variance) and pure exploitation (QBC) acquisition function across simulated studies.</p>
<p>Figure 8
8
provides a comparative analysis of RMSE of different methodologies employed in this case study.Our proposed methodology (combining iGS and QBC) outperformed the other strategies and surpassed at least one of the pure exploration or exploitation in almost every iteration.BHEEM achieved about 10.4% and 11.3% average improvement from the pure exploration and exploitation strategy respectively.</p>
<p>Figure 8 :
8
Figure 8: Comparison of RMSE for different strategies for Max phase material case study.</p>
<p>Figure 9
9
Figure 9 for a representative simulation example of F 1 (x).It plots the accepted η at one stage of the querying process.The chains were initiated with different ranges of values for η and continued till</p>
<p>Figure 9 :
9
Figure 9: (a) Metropolis sampling chain with first 50 accepted η in one querying iteration with initial samples, (b) Metropolis sampling chain with first 10,000 accepted η in one querying iteration with different initial samples.Different colored plots in each figure represent chains with initial samples at different ranges.(c) Progression of posterior mean of η while learning a function.</p>
<p>Figure 10(c) and Figure 10(d) present the RMSE and the η selected respectively for different values of τ , 0.001, 0.01, 0.1, and 0.25, while keeping ν = 0.001.Here also, we observe the behavior of RMSE and η to be indifferent towards the value of τ .But due to our previous discussion on the relation between τ and the speed of convergence of Metropolis algorithm in the last paragraph of Section 3.1.2,we chose τ = 0.1 throughout all experiments.</p>
<p>Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design. T Lookman, Computational Materials. 512019</p>
<p>Machine learning in manufacturing: Advantages, challenges, and applications. T Wuest, Production &amp; Manufacturing Research. 412016</p>
<p>Implementing machine learning in health care-addressing ethical challenges. D S Char, The New England Journal of Medicine. 378119812018</p>
<p>Metaheuristic-based inverse design of materials -A survey. T W Liao, G Li, 10.1016/j.jmat.2020.02.011Journal of Materiomics. 2352-847862Jun. 2020</p>
<p>Active learning literature survey. B Settles, 16482009University of Wisconsin-Madison, Computer SciencesTechnical Report</p>
<p>Queries and concept learning. D Angluin, Machine Learning. 19882</p>
<p>Entropy-based active learning for object recognition. A Holub, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE2008</p>
<p>Active learning with statistical models. D A Cohn, Journal of Artificial Intelligence Research. 41996</p>
<p>Active learning for object classification: From exploration to exploitation. N Cebron, M R Berthold, Data Mining and Knowledge Discovery. 182009</p>
<p>Active learning for regression using greedy sampling. D Wu, Information Sciences. 4742019</p>
<p>Active learning for regression based on query by committee. R Burbidge, Lecture Notes in Computer Science. 2007</p>
<p>Stream-based joint exploration-exploitation active learning. C C Loy, 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE2012</p>
<p>A hierarchical expected improvement method for bayesian optimization. Z Chen, arXiv:1911.07285Online</p>
<p>Efficient Global Optimization of Expensive Black-Box Functions. D R Jones, 10.1023/A:1008306431147Journal of Global Optimization. 1573-2916134Dec. 1998</p>
<p>An adaptive exploration-exploitation algorithm for constructing metamodels in random simulation using a novel sequential experimental design. A Ajdari, H Mahlooji, Communications in Statistics -Simulation and Computation. 201443</p>
<p>A novel active learning regression framework for balancing the explorationexploitation trade-off. D Elreedy, Entropy. 2172019</p>
<p>Bayesian data analysis. A Gelman, 1995Chapman and Hall/CRC</p>
<p>A bayesian hierarchical model for analysis of single-nucleotide polymorphisms diversity in multilocus, multipopulation samples. F Guo, Journal of the American Statistical Association. 1044852009</p>
<p>Sequential design of experiments. H Chernoff, The Annals of Mathematical Statistics. 3031959</p>
<p>Sequential exploration of complex surfaces using minimum energy designs. V R Joseph, Technometrics. 5712015</p>
<p>Sequential designs based on bayesian uncertainty quantification in sparse representation surrogate modeling. R.-B Chen, Technometrics. 5922017</p>
<p>Strategies for sequential design of experiments and augmentation. L Lu, C M Anderson-Cook, Quality and Reliability Engineering International. 3752021</p>
<p>Hierarchical sampling for active learning. S Dasgupta, D Hsu, Proceedings of the 25th International Conference on Machine Learning. the 25th International Conference on Machine Learning2008</p>
<p>Adaptive active learning for image classification. X Li, Y Guo, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2013</p>
<p>The power of ensembles for active learning in image classification. W H Beluch, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Maximizing expected model change for active learning in regression. W Cai, 2013 IEEE 13th International Conference on Data Mining. IEEE2013</p>
<p>Information-based objective functions for active data selection. D J Mackay, Neural Computation. 441992</p>
<p>A mathematical theory of communication. The Bell system technical journal. C E Shannon, 1948</p>
<p>Heterogeneous uncertainty sampling for supervised learning. D D Lewis, J Catlett, Machine Learning Proceedings. Elsevier1994. 1994</p>
<p>An active learning methodology for efficient estimation of expensive noisy black-box functions using gaussian process regression. R Meka, IEEE Access. 82020</p>
<p>Query by committee. H S Seung, 10.1145/130385.130417Proceedings of the fifth annual workshop on Computational learning theory. the fifth annual workshop on Computational learning theoryNew York, NY, USAAssociation for Computing MachineryJul. 1992</p>
<p>Neural network ensembles, cross validation, and active learning. A Krogh, J Vedelsby, Advances in Neural Information Processing Systems. 19957</p>
<p>Model-free and model-based active learning for regression. J O'neill, Advances in Computational Intelligence Systems. Springer2017</p>
<p>Egal: Exploration guided active learning for tcbr. R Hu, International Conference on Case-Based Reasoning. Springer2010</p>
<p>Online choice of active learning algorithms. Y Baram, Journal of Machine Learning Research. 5Mar. 2004</p>
<p>Deep similarity-based batch mode active learning with exploration-exploitation. C Yin, 2017 IEEE International Conference on Data Mining (ICDM). IEEE2017</p>
<p>Imprecise action selection in substance use disorder: Evidence for active learning impairments when solving the explore-exploit dilemma. R Smith, Drug and alcohol dependence. 2152020</p>
<p>Query-by-committee improvement with diversity and density in batch active learning. S Kee, Information Sciences. 4542018</p>
<p>Using confidence bounds for exploitation-exploration trade-offs. P Auer, Journal of Machine Learning Research. 3Nov. 2002</p>
<p>Reinforcement learning for virtual network embedding in wireless sensor networks. H Afifi, H Karl, 2020 16th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob. IEEE202050308</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>Reinforcement learning and evolutionary algorithms for non-stationary multi-armed bandit problems. D E Koulouriotis, A Xanthopoulos, Applied Mathematics and Computation. 19622008</p>
<p>Control of exploitation-exploration meta-parameter in reinforcement learning. S Ishii, Neural Networks. 154-62002</p>
<p>J D Cohen, Should i stay or should i go? how the human brain manages. </p>
<p>Errors and error correction in choice-response tasks. P A Rabbitt, Journal of Experimental Psychology. 7122641966</p>
<p>The concept of exchangeability and its applications. J M Bernardo, Far East Journal of Mathematical Sciences. 41996</p>
<p>A first course in Bayesian statistical methods. P D Hoff, 2009Springer580</p>
<p>Stable bayesian parameter estimation for biological dynamical systems. A G Busetto, J M Buhmann, 2009 International Conference on Computational Science and Engineering, IEEE. 20091</p>
<p>Population growth of human y chromosomes: A study of y chromosome microsatellites. J K Pritchard, Molecular Biology and Evolution. 16121999</p>
<p>Markov chain monte carlo without likelihoods. P Marjoram, Proceedings of the National Academy of Sciences. the National Academy of Sciences2003100328</p>
<p>Xvi. functions of positive and negative type, and their connection the theory of integral equations. J Mercer, Philosophical Transactions of the Royal Society of London. Series A. 209441-4581909</p>
<p>The kernel recursive least-squares algorithm. Y Engel, IEEE Transactions on Signal Processing. 5282004</p>
<p>C K Williams, C E Rasmussen, Gaussian Processes for Machine Learning. The MIT Press2006</p>
<p>A variance maximization criterion for active learning. Y Yang, M Loog, Pattern Recognition. 201878</p>
<p>A new active learning approach for adsorbate-substrate structural elucidation in silico. M P Lourenço, Journal of Molecular Modeling. 2862022</p>
<p>Algorithms for multi-armed bandit problems. V Kuleshov, D Precup, arXiv:1402.60282014arXiv preprint</p>
<p>Simulated annealing. P J Van Laarhoven, E H Aarts, Simulated annealing: Theory and applications. Springer1987</p>
<p>Homoepitaxial growth of ti-si-c max-phase thin films on bulk Ti3SiC2 substrates. P Eklund, Journal of Crystal Growth. 30412007</p>
<p>A genomic approach to the stability, elastic, and electronic properties of the max phases. S , Physica Status Solidi (b). 25182014</p>
<p>A comparative study of categorical variable encoding techniques for neural network classifiers. K Potdar, International Journal of Computer Applications. 17542017</p>            </div>
        </div>

    </div>
</body>
</html>
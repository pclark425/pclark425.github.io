<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3218 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3218</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3218</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-1266477120913d274346b044b4cc72ea893b1382</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1266477120913d274346b044b4cc72ea893b1382" target="_blank">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces MemWalker, a method that first processes the long context into a tree of summary nodes, and upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3218.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3218.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MEMWALKER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MEMWALKER: An Interactive Reading Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive agent that constructs a hierarchical memory tree of summaries from a long text and navigates that tree via iterative LLM prompting to locate and read only relevant segments for a query, optionally carrying working memory along the navigation path.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MEMWALKER</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based interactive reader that first transforms a long input into a multi-level tree of summary nodes (leaf nodes summarize fixed-size segments; internal nodes recursively summarize child summaries). At query time the underlying LLM (Stable Beluga 2 in experiments) navigates the tree from the root using triage and leaf prompts, chooses child nodes to inspect, can revert to parent nodes, and can commit at a leaf to generate the final answer. Navigation decisions are produced along with natural-language reasoning justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structured hierarchical memory (memory tree) + working memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory is implemented as a precomputed tree T(x) where each leaf stores a summary of a fixed-size text segment and each internal node stores a concise summary of its children's summaries. During navigation the LLM inspects child summaries (triage) or leaf content (leaf prompt), chooses actions (enter child, revert, commit) and may append previously visited node contents to a working memory context m that is carried and truncated to fit the LLM context window. Summaries are produced and consumed via zero-shot LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QuALITY; SummScreenFD; GovReport (SCROLLS benchmark datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-context question answering tasks where the input document can exceed the LLM context window (multi-choice QA for QuALITY; QA derived from long scripts and government reports), requiring selective reading and long-range memory access.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering (long-context)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>QuALITY: 67.4% (Orig) / 73.6% (Long); SummScreenFD: 67.3% (Orig) / 64.5% (Long); GovReport: 59.4% (Orig) / 60.4% (Long) — accuracy (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation removing working memory yields a 5–13% absolute drop in accuracy across tasks (paper reports a 5–13% drop but does not list exact per-dataset ablation numbers in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MEMWALKER outperforms recurrence-, retrieval-, and full-context (truncation) baselines on long-sequence subsets and other open long-context models (MPT 13B, LongChat 13B). Working memory carried along navigation significantly improves accuracy (5–13% drop if removed). Effective navigation depends on a sufficiently capable/reasoning LLM; reasoning prompts improve performance for strong LLMs. MEMWALKER reads only ~63–69% of tokens on average and recovers from stray navigation paths (revert action) in 59–79% of such cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Memory-tree construction can grow costly for extremely long sequences (scaling issue). Method requires a strong reasoning-capable LLM (70B, instruction-tuned in their experiments) — weaker models perform worse and may be harmed by the reasoning prompt. Zero-shot prompting was used; no finetuning was applied to improve navigation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3218.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3218.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrence baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrence-based summarization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that carries forward compressed summaries between segments in a recurrent fashion so the model has access to compressed history while processing long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Recurrence baseline (summarization recurrence)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Implements a recurrence-style memory by summarizing previous segments and carrying the summary into subsequent segments; in this paper the authors follow an approach where each segment is 2,500 tokens and a summary of up to 500 tokens is carried forward.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrence / compressed episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Information from previous segments is compressed (summarized) and appended to the next segment's input so the LLM receives a rolling summary representing earlier context; this compresses long context into a fixed-size summary carried recurrently.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QuALITY; SummScreenFD; GovReport</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same long-context QA tasks used to evaluate memory access and long-range understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering (long-context)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>QuALITY: 51.3% (Orig) / 56.0% (Long); SummScreenFD: 47.7% / 45.4%; GovReport: 35.6% / 33.8% — accuracy (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recurrence performs substantially worse than MEMWALKER across tasks, demonstrating the limitation that recurrence-compressed representations can lose recall of earlier, potentially relevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Information loss across recurrence steps; the summarization objective is not task-guided, so recall of older sequence information becomes weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3218.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3218.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval baseline (Contriever)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-based baseline using Contriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented baseline that embeds text segments and selects high-scoring segments by query similarity to fill the LLM context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval baseline (Contriever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embeds long-text segments using Contriever and retrieves top-scoring segments conditioned on the query; retrieved segments are concatenated until the LLM context is filled and then used to produce the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory (document/segment retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Segments of the long document are embedded and stored; at query time, segments most similar to the query are retrieved and concatenated as the model input; retrieval thus provides targeted memory access beyond the local context window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QuALITY; SummScreenFD; GovReport</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-context QA where relevant information must be retrieved from coherent long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering (long-context)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>QuALITY: 63.1% (Orig) / 64.8% (Long); SummScreenFD: 63.7% / 62.2%; GovReport: 54.0% / 52.1% — accuracy (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval improves over some baselines but is outperformed by MEMWALKER on these coherent long-text tasks; retrieval methods may be less effective when distinguishing relevant parts inside a single coherent document.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval systems can be tuned for distinguishing different documents and may be less effective at intra-document localization; selecting segments by similarity may miss context needed for correct answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3218.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3218.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full Context (truncation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full-context baseline with truncation (keep left / keep right)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that use the full available LLM context window (4,096 tokens) by truncating the long input either from the right (keep left) or from the left (keep right).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Full Context (keep left / keep right)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses the LLM's available context window to consume as much of the long text as possible; if the document exceeds the window, either the beginning (keep left) or the end (keep right) is preserved and the remainder truncated.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context window only (no external memory beyond context extension)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>No external memory: the agent relies solely on the model's context window; truncation chooses which portion of the long text to include (left or right) prior to answering.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QuALITY; SummScreenFD; GovReport</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-context QA where truncation decisions determine whether relevant information is available in the LLM input.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering (long-context)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Full Context (keep left) — QuALITY: 56.7% / 64.8%; SummScreenFD: 62.7% / 62.7%; GovReport: 59.4% / 56.3%. Full Context (keep right) — QuALITY: 70.1% / 72.5%; SummScreenFD: 64.7% / 63.1%; GovReport: 50.5% / 50.0% — accuracy (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Full-context truncation can perform well for shorter documents or when relevant content is concentrated at one end of the document, but its performance is dataset-dependent and falls behind MEMWALKER on long-sequence subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires choosing which part to truncate; unable to access information outside the context window; performance sensitive to positional biases in the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3218.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3218.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorizing Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorizing Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that attends to a large external memory (e.g., kNN-style) to recall and condition on memorized tokens or segments, designed to extend effective context and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorizing transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memorizing Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer variants that incorporate an external memory mechanism enabling kNN-like or memory-augmented attention to recall relevant cached representations beyond the immediate context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory / kNN-style memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Stores representations of tokens or segments externally and retrieves them (e.g., via nearest-neighbor) to augment model inputs or attention, enabling the model to access information beyond its direct context window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned in related work as a memory-augmented approach for long-range context and recall.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>related work (memory augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of methods that permit access to external memory via retrieval/nearest-neighbor mechanisms for extended-range modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3218.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3218.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PEARL: Prompting large language models to plan and execute actions over long documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that asks an LLM to generate pseudo-APIs or plan-based actions to focus on parts of long documents, improving focus and execution without external memory structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pearl: Prompting large language models to plan and execute actions over long documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompting framework that directs an LLM to plan and call pseudo-APIs (abstract actions) to identify and operate on relevant parts of a long document within the model's context window.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>planning/pseudo-APIs (not external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Operates within the LLM context window by planning and issuing structured calls rather than constructing an external memory tree; used to focus attention on relevant content.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Referenced as related work for planning and executing actions in long-document settings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>related work (planning over long text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3218.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3218.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that interacts with the web (browsing/search) to retrieve information and answer questions, using external tools and memory of interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Webgpt: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based agent that issues web searches and browses web content as part of a multi-step decision process, using retrieved pages as external information sources to answer queries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval / tool-mediated memory (web pages)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Performs interactive retrieval by issuing queries, following links and reading web pages; uses retrieved content as external memory to inform answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned as an example of agentic, interactive retrieval systems used to find answers beyond a fixed context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>related work (interactive retrieval/agent)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3218.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3218.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebShop: Towards scalable real-world web interaction with grounded language agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent framework that enables LLMs to interact with web APIs and pages to complete tasks like shopping by grounding actions in web interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Webshop: Towards scalable real-world web interaction with grounded language agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Demonstrates LLM-based agents performing grounded multi-step web interactions (search, click, purchase) using a sequence of actions and external state obtained from the web as memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval / interaction memory (web state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Agent issues actions against web interfaces and consumes retrieved content/state as transient memory to make subsequent decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cited as an example of agentic interaction over web resources rather than long coherent-document reading.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>related work (interactive agent)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3218.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3218.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Notes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Notes: Learning to reason and memorize with self-notes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that interleaves self-generated notes with input data to support multi-step reasoning and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to reason and memorize with self-notes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-Notes</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Approach where the model writes intermediate notes during reasoning which are appended to the context and can be used as working memory to improve subsequent reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>scratchpad / working memory (self-generated notes)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>The model produces and stores intermediate notes that remain in context and aid later steps of reasoning, serving as an explicit working memory buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cited as related work that uses interleaved notes to enhance reasoning over longer processes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>related work (reasoning & memory)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3218.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3218.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecurrentGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RecurrentGPT: Interactive generation of (arbitrarily) long text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach using iterative prompting and recurrence-like procedures to generate or process arbitrarily long text via repeated interaction with an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrentgpt: Interactive generation of (arbitrarily) long text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RecurrentGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses iterative prompting and a recurrent interaction pattern to extend generation/processing to arbitrarily long texts by carrying information across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>iterative/recurrent memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Maintains a recurrent state or summary across iterations so that the LLM can continue working with an effectively unbounded length sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned as an iterative prompting approach relevant to long-text generation and processing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>related work (long-text generation/processing)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memorizing transformers <em>(Rating: 2)</em></li>
                <li>Pearl: Prompting large language models to plan and execute actions over long documents <em>(Rating: 2)</em></li>
                <li>Webgpt: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>Webshop: Towards scalable real-world web interaction with grounded language agents <em>(Rating: 2)</em></li>
                <li>Learning to reason and memorize with self-notes <em>(Rating: 2)</em></li>
                <li>Recurrentgpt: Interactive generation of (arbitrarily) long text <em>(Rating: 2)</em></li>
                <li>Recursively summarizing books with human feedback <em>(Rating: 1)</em></li>
                <li>Compressive transformers for long-range sequence modelling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3218",
    "paper_id": "paper-1266477120913d274346b044b4cc72ea893b1382",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "MEMWALKER",
            "name_full": "MEMWALKER: An Interactive Reading Agent",
            "brief_description": "An interactive agent that constructs a hierarchical memory tree of summaries from a long text and navigates that tree via iterative LLM prompting to locate and read only relevant segments for a query, optionally carrying working memory along the navigation path.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MEMWALKER",
            "agent_description": "An LLM-based interactive reader that first transforms a long input into a multi-level tree of summary nodes (leaf nodes summarize fixed-size segments; internal nodes recursively summarize child summaries). At query time the underlying LLM (Stable Beluga 2 in experiments) navigates the tree from the root using triage and leaf prompts, chooses child nodes to inspect, can revert to parent nodes, and can commit at a leaf to generate the final answer. Navigation decisions are produced along with natural-language reasoning justifications.",
            "memory_used": true,
            "memory_type": "structured hierarchical memory (memory tree) + working memory",
            "memory_mechanism_description": "Memory is implemented as a precomputed tree T(x) where each leaf stores a summary of a fixed-size text segment and each internal node stores a concise summary of its children's summaries. During navigation the LLM inspects child summaries (triage) or leaf content (leaf prompt), chooses actions (enter child, revert, commit) and may append previously visited node contents to a working memory context m that is carried and truncated to fit the LLM context window. Summaries are produced and consumed via zero-shot LLM prompts.",
            "task_name": "QuALITY; SummScreenFD; GovReport (SCROLLS benchmark datasets)",
            "task_description": "Long-context question answering tasks where the input document can exceed the LLM context window (multi-choice QA for QuALITY; QA derived from long scripts and government reports), requiring selective reading and long-range memory access.",
            "task_type": "question answering (long-context)",
            "performance_with_memory": "QuALITY: 67.4% (Orig) / 73.6% (Long); SummScreenFD: 67.3% (Orig) / 64.5% (Long); GovReport: 59.4% (Orig) / 60.4% (Long) — accuracy (see Table 2).",
            "performance_without_memory": "Ablation removing working memory yields a 5–13% absolute drop in accuracy across tasks (paper reports a 5–13% drop but does not list exact per-dataset ablation numbers in-text).",
            "has_performance_comparison": true,
            "key_findings": "MEMWALKER outperforms recurrence-, retrieval-, and full-context (truncation) baselines on long-sequence subsets and other open long-context models (MPT 13B, LongChat 13B). Working memory carried along navigation significantly improves accuracy (5–13% drop if removed). Effective navigation depends on a sufficiently capable/reasoning LLM; reasoning prompts improve performance for strong LLMs. MEMWALKER reads only ~63–69% of tokens on average and recovers from stray navigation paths (revert action) in 59–79% of such cases.",
            "limitations_or_challenges": "Memory-tree construction can grow costly for extremely long sequences (scaling issue). Method requires a strong reasoning-capable LLM (70B, instruction-tuned in their experiments) — weaker models perform worse and may be harmed by the reasoning prompt. Zero-shot prompting was used; no finetuning was applied to improve navigation behavior.",
            "uuid": "e3218.0",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Recurrence baseline",
            "name_full": "Recurrence-based summarization baseline",
            "brief_description": "A baseline that carries forward compressed summaries between segments in a recurrent fashion so the model has access to compressed history while processing long sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Recurrence baseline (summarization recurrence)",
            "agent_description": "Implements a recurrence-style memory by summarizing previous segments and carrying the summary into subsequent segments; in this paper the authors follow an approach where each segment is 2,500 tokens and a summary of up to 500 tokens is carried forward.",
            "memory_used": true,
            "memory_type": "recurrence / compressed episodic memory",
            "memory_mechanism_description": "Information from previous segments is compressed (summarized) and appended to the next segment's input so the LLM receives a rolling summary representing earlier context; this compresses long context into a fixed-size summary carried recurrently.",
            "task_name": "QuALITY; SummScreenFD; GovReport",
            "task_description": "Same long-context QA tasks used to evaluate memory access and long-range understanding.",
            "task_type": "question answering (long-context)",
            "performance_with_memory": "QuALITY: 51.3% (Orig) / 56.0% (Long); SummScreenFD: 47.7% / 45.4%; GovReport: 35.6% / 33.8% — accuracy (Table 2).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Recurrence performs substantially worse than MEMWALKER across tasks, demonstrating the limitation that recurrence-compressed representations can lose recall of earlier, potentially relevant information.",
            "limitations_or_challenges": "Information loss across recurrence steps; the summarization objective is not task-guided, so recall of older sequence information becomes weaker.",
            "uuid": "e3218.1",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Retrieval baseline (Contriever)",
            "name_full": "Retrieval-based baseline using Contriever",
            "brief_description": "A retrieval-augmented baseline that embeds text segments and selects high-scoring segments by query similarity to fill the LLM context.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Retrieval baseline (Contriever)",
            "agent_description": "Embeds long-text segments using Contriever and retrieves top-scoring segments conditioned on the query; retrieved segments are concatenated until the LLM context is filled and then used to produce the answer.",
            "memory_used": true,
            "memory_type": "retrieval-augmented memory (document/segment retrieval)",
            "memory_mechanism_description": "Segments of the long document are embedded and stored; at query time, segments most similar to the query are retrieved and concatenated as the model input; retrieval thus provides targeted memory access beyond the local context window.",
            "task_name": "QuALITY; SummScreenFD; GovReport",
            "task_description": "Long-context QA where relevant information must be retrieved from coherent long documents.",
            "task_type": "question answering (long-context)",
            "performance_with_memory": "QuALITY: 63.1% (Orig) / 64.8% (Long); SummScreenFD: 63.7% / 62.2%; GovReport: 54.0% / 52.1% — accuracy (Table 2).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Retrieval improves over some baselines but is outperformed by MEMWALKER on these coherent long-text tasks; retrieval methods may be less effective when distinguishing relevant parts inside a single coherent document.",
            "limitations_or_challenges": "Retrieval systems can be tuned for distinguishing different documents and may be less effective at intra-document localization; selecting segments by similarity may miss context needed for correct answers.",
            "uuid": "e3218.2",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Full Context (truncation)",
            "name_full": "Full-context baseline with truncation (keep left / keep right)",
            "brief_description": "Baselines that use the full available LLM context window (4,096 tokens) by truncating the long input either from the right (keep left) or from the left (keep right).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Full Context (keep left / keep right)",
            "agent_description": "Uses the LLM's available context window to consume as much of the long text as possible; if the document exceeds the window, either the beginning (keep left) or the end (keep right) is preserved and the remainder truncated.",
            "memory_used": false,
            "memory_type": "context window only (no external memory beyond context extension)",
            "memory_mechanism_description": "No external memory: the agent relies solely on the model's context window; truncation chooses which portion of the long text to include (left or right) prior to answering.",
            "task_name": "QuALITY; SummScreenFD; GovReport",
            "task_description": "Long-context QA where truncation decisions determine whether relevant information is available in the LLM input.",
            "task_type": "question answering (long-context)",
            "performance_with_memory": "Full Context (keep left) — QuALITY: 56.7% / 64.8%; SummScreenFD: 62.7% / 62.7%; GovReport: 59.4% / 56.3%. Full Context (keep right) — QuALITY: 70.1% / 72.5%; SummScreenFD: 64.7% / 63.1%; GovReport: 50.5% / 50.0% — accuracy (Table 2).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Full-context truncation can perform well for shorter documents or when relevant content is concentrated at one end of the document, but its performance is dataset-dependent and falls behind MEMWALKER on long-sequence subsets.",
            "limitations_or_challenges": "Requires choosing which part to truncate; unable to access information outside the context window; performance sensitive to positional biases in the dataset.",
            "uuid": "e3218.3",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Memorizing Transformers",
            "name_full": "Memorizing Transformers",
            "brief_description": "A model that attends to a large external memory (e.g., kNN-style) to recall and condition on memorized tokens or segments, designed to extend effective context and retrieval.",
            "citation_title": "Memorizing transformers",
            "mention_or_use": "mention",
            "agent_name": "Memorizing Transformers",
            "agent_description": "Transformer variants that incorporate an external memory mechanism enabling kNN-like or memory-augmented attention to recall relevant cached representations beyond the immediate context.",
            "memory_used": true,
            "memory_type": "external memory / kNN-style memory",
            "memory_mechanism_description": "Stores representations of tokens or segments externally and retrieves them (e.g., via nearest-neighbor) to augment model inputs or attention, enabling the model to access information beyond its direct context window.",
            "task_name": "",
            "task_description": "Mentioned in related work as a memory-augmented approach for long-range context and recall.",
            "task_type": "related work (memory augmentation)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Cited as an example of methods that permit access to external memory via retrieval/nearest-neighbor mechanisms for extended-range modeling.",
            "limitations_or_challenges": null,
            "uuid": "e3218.4",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PEARL",
            "name_full": "PEARL: Prompting large language models to plan and execute actions over long documents",
            "brief_description": "A prompting method that asks an LLM to generate pseudo-APIs or plan-based actions to focus on parts of long documents, improving focus and execution without external memory structures.",
            "citation_title": "Pearl: Prompting large language models to plan and execute actions over long documents",
            "mention_or_use": "mention",
            "agent_name": "PEARL",
            "agent_description": "A prompting framework that directs an LLM to plan and call pseudo-APIs (abstract actions) to identify and operate on relevant parts of a long document within the model's context window.",
            "memory_used": null,
            "memory_type": "planning/pseudo-APIs (not external memory)",
            "memory_mechanism_description": "Operates within the LLM context window by planning and issuing structured calls rather than constructing an external memory tree; used to focus attention on relevant content.",
            "task_name": "",
            "task_description": "Referenced as related work for planning and executing actions in long-document settings.",
            "task_type": "related work (planning over long text)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": null,
            "limitations_or_challenges": null,
            "uuid": "e3218.5",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT: Browser-assisted question-answering with human feedback",
            "brief_description": "An agent that interacts with the web (browsing/search) to retrieve information and answer questions, using external tools and memory of interactions.",
            "citation_title": "Webgpt: Browser-assisted question-answering with human feedback",
            "mention_or_use": "mention",
            "agent_name": "WebGPT",
            "agent_description": "An LLM-based agent that issues web searches and browses web content as part of a multi-step decision process, using retrieved pages as external information sources to answer queries.",
            "memory_used": true,
            "memory_type": "external retrieval / tool-mediated memory (web pages)",
            "memory_mechanism_description": "Performs interactive retrieval by issuing queries, following links and reading web pages; uses retrieved content as external memory to inform answers.",
            "task_name": "",
            "task_description": "Mentioned as an example of agentic, interactive retrieval systems used to find answers beyond a fixed context.",
            "task_type": "related work (interactive retrieval/agent)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": null,
            "limitations_or_challenges": null,
            "uuid": "e3218.6",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "WebShop",
            "name_full": "WebShop: Towards scalable real-world web interaction with grounded language agents",
            "brief_description": "An agent framework that enables LLMs to interact with web APIs and pages to complete tasks like shopping by grounding actions in web interactions.",
            "citation_title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "mention_or_use": "mention",
            "agent_name": "WebShop",
            "agent_description": "Demonstrates LLM-based agents performing grounded multi-step web interactions (search, click, purchase) using a sequence of actions and external state obtained from the web as memory.",
            "memory_used": true,
            "memory_type": "external retrieval / interaction memory (web state)",
            "memory_mechanism_description": "Agent issues actions against web interfaces and consumes retrieved content/state as transient memory to make subsequent decisions.",
            "task_name": "",
            "task_description": "Cited as an example of agentic interaction over web resources rather than long coherent-document reading.",
            "task_type": "related work (interactive agent)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": null,
            "limitations_or_challenges": null,
            "uuid": "e3218.7",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Notes",
            "name_full": "Self-Notes: Learning to reason and memorize with self-notes",
            "brief_description": "A method that interleaves self-generated notes with input data to support multi-step reasoning and memory.",
            "citation_title": "Learning to reason and memorize with self-notes",
            "mention_or_use": "mention",
            "agent_name": "Self-Notes",
            "agent_description": "Approach where the model writes intermediate notes during reasoning which are appended to the context and can be used as working memory to improve subsequent reasoning steps.",
            "memory_used": true,
            "memory_type": "scratchpad / working memory (self-generated notes)",
            "memory_mechanism_description": "The model produces and stores intermediate notes that remain in context and aid later steps of reasoning, serving as an explicit working memory buffer.",
            "task_name": "",
            "task_description": "Cited as related work that uses interleaved notes to enhance reasoning over longer processes.",
            "task_type": "related work (reasoning & memory)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": null,
            "limitations_or_challenges": null,
            "uuid": "e3218.8",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RecurrentGPT",
            "name_full": "RecurrentGPT: Interactive generation of (arbitrarily) long text",
            "brief_description": "An approach using iterative prompting and recurrence-like procedures to generate or process arbitrarily long text via repeated interaction with an LLM.",
            "citation_title": "Recurrentgpt: Interactive generation of (arbitrarily) long text",
            "mention_or_use": "mention",
            "agent_name": "RecurrentGPT",
            "agent_description": "Uses iterative prompting and a recurrent interaction pattern to extend generation/processing to arbitrarily long texts by carrying information across iterations.",
            "memory_used": true,
            "memory_type": "iterative/recurrent memory",
            "memory_mechanism_description": "Maintains a recurrent state or summary across iterations so that the LLM can continue working with an effectively unbounded length sequence.",
            "task_name": "",
            "task_description": "Mentioned as an iterative prompting approach relevant to long-text generation and processing.",
            "task_type": "related work (long-text generation/processing)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": null,
            "limitations_or_challenges": null,
            "uuid": "e3218.9",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memorizing transformers",
            "rating": 2
        },
        {
            "paper_title": "Pearl: Prompting large language models to plan and execute actions over long documents",
            "rating": 2
        },
        {
            "paper_title": "Webgpt: Browser-assisted question-answering with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "rating": 2
        },
        {
            "paper_title": "Learning to reason and memorize with self-notes",
            "rating": 2
        },
        {
            "paper_title": "Recurrentgpt: Interactive generation of (arbitrarily) long text",
            "rating": 2
        },
        {
            "paper_title": "Recursively summarizing books with human feedback",
            "rating": 1
        },
        {
            "paper_title": "Compressive transformers for long-range sequence modelling",
            "rating": 1
        }
    ],
    "cost": 0.01652325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Walking Down the Memory Maze: Beyond CONTEXT LIMIT THROUGH INTERACTIVE READING</h1>
<p>Howard Chen*<br>Princeton University</p>
<p>Ramakanth Pasunuru Meta AI</p>
<p>Jason Weston Meta AI</p>
<p>Asli Celikyilmaz Meta AI</p>
<h2>AbSTRACT</h2>
<h4>Abstract</h4>
<p>Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue - the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MEMWALKER, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MEMWALKER enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have witnessed significant advancements due to the increased model size, expanded pretraining data, and the adoption of the Transformer architecture with self-attention (Vaswani et al., 2017). As LLMs evolve in capability, users increasingly seek to use longer input sequences during inference. This results in a growing demand in querying for information in long documents, analyzing legal or scientific papers, and managing extended conversational dialogues. These tasks involve consuming large amount of information, highlighting the importance of longer context processing.</p>
<p>Despite the rapid development, the limitation of the self-attention mechanism becomes apparent as its memory usage increases with longer sequences, consequently limiting the size of the context window. To address this, different approaches have been employed, such as designing lighter and more efficient attention schemes (Zaheer et al., 2020), finetuning with extrapolated or interpolated positional embeddings (Press et al., 2022; Chen et al., 2023), incorporating recurrence to bring forward information from preceding text segments into the next (Rae et al., 2019; Fan et al., 2020; Xu et al., 2022), or retrieving relevant parts of the text (Lewis et al., 2020; Izacard \&amp; Grave, 2020). However, these approaches are still limited by design. The context window, no matter how long it is extended, assumes a fixed size, and not all positions within it hold equivalent significance (Liu et al., 2023). While recurrence can manage infinite-length sequences, it often misses out on retaining information from earlier segments. Additionally, retrieving segments from the coherent long-text might be ineffective, given that many retrieval systems are tailored to distinguish similar but distinct documents (Chen et al., 2017).</p>
<p>To address these issues, we develop a fundamentally different approach which treats the model with a finite context window as an interactive agent, rather than simply processing the entire sequence in one go. To this end, we introduce MEMWALKER, a method that enables the model to read the long-text interactively via iterative LLM prompting. MEMWALKER operates through a two-stage</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Stage 1: Memory Tree Construction</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The two-stage procedure of MEMWALKER. Top (stage 1): the memory tree is constructed. The long text is split into segments of a predetermined size and each segment is first summarized into a summary node. The summary nodes are recursively summarized into higher level nodes until it reaches the root. Bottom (stage 2): Given a query, the LLM navigates the tree structure via iterative prompting and finds the node that contains relevant segment to form the answer. At each node, the LLM decides the action by first reasoning about the child summary nodes by sampling from the distribution $\operatorname{LLM}($ reasoning, action $\mid$ summ, query). The LLM can choose the revert action to return to the parent node if it chose the wrong path or the segment at hand is irrelevant (dashed red arrow). See Table 1 for a detailed example showing the LLM prompts that enable navigation.
approach: 1) memory tree construction and 2) navigation. During the first stage, the long-text is segmented into small chunks that fit within the LLM's context window. The LLM then subsequently summarizes each segment into a textual summary node. These summary nodes are progressively further summarized into higher-level summary nodes, thus building a tree structure (Figure 1). To answer a user query, the LLM begins navigation from the tree's root node. It traverses the tree, inspecting various parts of the text to identify the path and segment relevant to answer the query. As a result, MEMWALKER can go beyond the context limit, efficiently processing texts and localizing the important segments of the long-text, without additional finetuning.</p>
<p>We evaluate MEMWALKER on three long context question answering tasks and show superior performance against recurrence, retrieval, and vanilla LLM baselines. MEMWALKER also outperforms other open long context systems that can take 8,000 to 16,000 tokens. We provide an analysis of the effectiveness of MEMWALKER, and show it can reason about navigation decisions, incorporate working memory during traversal, and recover from errors made in early navigational steps.</p>
<h2>2 Related Work</h2>
<p>Context window scaling. A straightforward approach to enable a longer context sequence is to tune the pre-trained language models and extrapolate their positional embeddings on longer text sequences (Press et al., 2022; Chen et al., 2023). Another direction is modified self-attention (Beltagy et al., 2020; Zaheer et al., 2020; Guo et al., 2022; Ainslie et al., 2023). This approach has advanced in large strides thanks to training techniques such as Flash Attention (Dao et al., 2022) that greatly</p>
<p>reduce the memory footprint. Despite the recent advances, this approach comes with two natural limitations: 1) to enable models on longer sequences, the model needs to be fine-tuned, incurring a non-negligible cost and 2) the attention mechanism may become less effective due to positional biases as the sequence length becomes very long (Liu et al., 2023).</p>
<p>Recurrence. Recurrent architectures have been extensively studied to tackle long sequence problems, from recurrent neural network based models Hochreiter \&amp; Schmidhuber (1997); Miller et al. (2016) to the modern Transformer based models (Dai et al., 2019; Rae et al., 2019; Fan et al., 2020; Xu et al., 2022; Bulatov et al., 2023; Chevalier et al., 2023). However, each recurrence step incurs information loss and the training objective does not guide "how to compress" with regard to downstream tasks. Typically this compression means that recall of older sequence information is weaker compared to recent information.</p>
<p>Retrieval. Retrieval systems are commonly used to select relevant documents from a large pool of documents, and have been incorporated into neural models in various ways (Chen et al., 2017; Dinan et al., 2018; Lewis et al., 2020). For long sequence reading, retrieval based methods typically first embed the text segments into vector representations and retrieve them based on the query instead of feeding the entire sequence into the model such as in Fusion-in-Decoder Izacard \&amp; Grave (2020) or kNN variants that attend to external memory such as Memorizing Transformers (Wu et al., 2022).</p>
<p>Reasoning agents. Instead of taking the long text as a single monolithic input, a model can act as an agent that reads part of the text and takes flexible actions. Work such as WebGPT (Nakano et al., 2021) and WebShop (Yao et al., 2022) allow the model to scroll through the internet and search for the requested answer or item. While their atomic actions allow for interactive search for relevant content, the models were not designed for understanding long and coherent texts. On the other hand, PEARL (Sun et al., 2023) prompts the model to generate pseudo APIs for the model to call in order to focus on the right parts of the long text. However, the method operates within the LLM's context window, rather than being a memory-access approach that goes beyond the context limit. Other works leveraged iterative prompting to reason and plan for long text generation tasks such as Re3 (Yang et al., 2022) and RecurrentGPT (Zhou et al., 2023). Self-Notes (Lanchantin et al., 2023) interleaved self-generating notes and the input data to perform better reasoning. Prior to current LLMs, LSTMs were also applied to searching through document structures (titles, subsections) Geva \&amp; Berant (2018). Recursive tree structure has also been explored in the context of summarization of long text such as books in (Wu et al., 2021), but was not used for memory navigation in that work.</p>
<h1>3 MEMWALKER: AN INTERACTIVE READER</h1>
<p>We study tasks related to long-context question answering - given a long-text $x$ and a query $q$, the model aims to generate the response $r$.</p>
<p>MEMWALKER follows two steps: 1) memory tree construction, where the long-context is broken down into a tree data structure. This construction does not depend on the query, and can hence be computed in advance if the sequence data is available beforehand. 2) navigation, in which the model navigates this structure upon receiving a query, gathering information to craft a suitable response. MEMWALKER assumes access to an underlying LLM, and both construction and navigation are achieved through iterative LLM prompting.</p>
<p>Memory tree construction. MEMWALKER first creates a tree data structure, $\mathcal{T}(x)$, from the long-text $x$. Each node is represented by text that encapsulates the summaries of all its child nodes below it. Specifically, the long-text $x$ is divided into segments $\left(c_{1}, \ldots, c_{n}\right)$. The LLM then summarizes each segment into a summary at the first level, represented as $s_{i}^{l+1}=\operatorname{LLM}\left(c_{i}:\right)$, $i=1 . . n$. The initial summary nodes are subsequently summarized further into higher level nodes, $s_{j}^{l+1}=\operatorname{LLM}\left(s_{i}^{l}, \ldots, s_{i+M_{t}}^{l}\right)$ where $M_{t}$ denotes the number of nodes in the $t$-th grouping at level $l$. This process continues until the topmost root node, $s^{L}$ is generated. The complete tree generation process is illustrated in Figure 1. Summarization is performed using LLM prompting. We include the prompts for memory tree construction in Appendix A.1.</p>
<p>Table 1: Example trajectory from the QuALITY dataset. The LLM first sees the content of the children nodes at the root node (summ 9 in Figure 1) and generates the response (takes action 0 to enter summ 7). When arriving at the leaf node (summ 2), the LLM determines that there is not enough information, therefore takes the action to revert (action -1) to the parent node. After hopping back-and-forth between nodes, the LLM commits to a Leaf node (summ 3) and answers the question. Yellow indicates triage prompt and purple indicates leaf prompt described in $\S 3$. Text after // denotes comments that are not processed by the LLM.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Navigation Trajectory</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">summ 9</td>
<td style="text-align: center;">The following passage(s) are the summaries of the different parts of a story. <br> To answer the question: Why did Ro change his mind about the people on Mars being backwards? <br> Which of the following summary is MOST LIKELY to contain information about the answer? <br> First provide reasoning to compare the summaries before you make the decision. <br> Summary 0: The story is set on Mars and follows the adventures of Ro, [...] // summ 7 <br> Summary 1: Ro, a young Martian, is climbing down a cliff to rescue [...] // summ 8 <br> Relpy with the passage number as your action. <br> You MUST choose one summary number and you should reply with the following format: <br> ####################################################################################################\</td>
</tr>
</tbody>
</table>
<ul>
<li>choosing one of the child nodes to further inspect, or to revert to the parent node. At leaf node $s_{i}^{l=1}$, the LLM can decide one of two actions: commit to the leaf node and respond to the query or revert to the parent node $\left(s_{j}^{l+1}\right)$ if the information in the leaf node (i.e., $c_{i}$ ) is insufficient. To make a navigation decision, we can also ask the LLM (via prompting) to first generate a reason in natural language to justify the action, followed by the action choice itself. Specifically, at each node, the model generates a response $r \sim \operatorname{LLM}(r \mid s, q)$ where the response is either of the two tuples: 1) $r=$ (reasoning, action, answer) when the LLM is at a leaf node or 2) $r=$ (reasoning, action) when the LLM is at non-leaf nodes.</li>
</ul>
<p>Navigational prompt design. We enable LLM navigation through zero-shot prompting. Our method requires two types of prompt: 1) triage prompt and 2) leaf prompt (highlighted in Table 1). Triage prompt contains the the query, the summaries of the children nodes, and instructions for the LLM to follow. Triage prompt is used at non-leaf nodes. Leaf prompt contains the content of the segment, the query (and options), and instructions that ask the LLM to either generate the answer or revert to the parent node. Both the triage prompt and leaf prompt specify an output format that the LLM needs to follow. Failure to conform to the format results in invalid actions and the LLM is required to regenerate. If the LLM fails to generate parsable output three consecutive times, the navigation terminates and returns "no answer".</p>
<p>Working memory. As the LLM traverses the tree, it can keep information throughout the navigation trajectory and add it to the context. Formally, the LLM generates the response $r \sim \operatorname{LLM}(r \mid$ $s, q, m)$ where the extra working memory $m \in{\varnothing} \cup\left{\left(s_{i}, s_{i+1}, \ldots\right)\right}$ is either empty or consists of contents from previously visited nodes. We truncate the working memory such that they can fit in the LLM's context window.* Table 1 illustrates the way working memory is added via [WORKING_MEMORY] in the prompt.</p>
<h1>4 EXPERIMENTAL SETUP</h1>
<h3>4.1 Datasets \&amp; Evaluation</h3>
<p>We use three datasets: QuALITY, SummScreenFD, and GovReport from the SCROLLS benchmark (Shaham et al., 2022). We report accuracy for all datasets.</p>
<p>QuALITY. QuALITY is a multiple choice question answering dataset collected by Pang et al. (2022). The dataset contains long-form stories sourced from Project Gutenberg and questions annotated by human annotators. We use a subset of 187 examples for our experiments.</p>
<p>SummScreenFD. SummScreenFD (Chen et al., 2022) is a dataset of TV and movie scripts in the form of dialogues among actors originally designed for summarization. We repurpose the dataset into a question answering task where the original provided ground truth summary text is used to generate a "who" question using Stable Beluga 2, with answers then checked by a human expert. The question paired with the original long text becomes the repurposed QA task of 306 examples.</p>
<p>GovReport. The GovReport dataset aggregates documents from Congressional Research Service and the U.S. Government Accountability Office together with summaries provided by experts (Huang et al., 2021). We repurpose the dataset into a question answering dataset of 101 examples the same way as for SummScreenFD.</p>
<p>All three datasets feature long contexts per example of varying length - some shorter examples, and some longer sequences. We therefore both report results on the original dataset, and also report on a subset of each task containing only longer sequences, to better evaluate memory access in the harder, longer context case. The thresholds are above 8, 000 tokens for QuALITY, 6, 000 tokens for SummScreenFD, and 12, 000 tokens for GovReport.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Results on the three question answering tasks, reporting test accuracy. Orig. denotes using the entire dataset and Long denotes the subset of longer sequences. Top: comparison to open long context models. Bottom: baselines and MEMWALKER performance, with all methods using the underlying Stable Beluga 2 LLM with a maximum 4,096 -token context length. MEMWALKER outperforms all other systems on longer sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">QuALITY <br> Orig. / Long</th>
<th style="text-align: center;">SummScreenFD <br> Orig. / Long</th>
<th style="text-align: center;">GovReport <br> Orig. / Long</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MPT 13B (8k)</td>
<td style="text-align: center;">$44.4 / 47.3$</td>
<td style="text-align: center;">$65.0 / 63.5$</td>
<td style="text-align: center;">$44.6 / 43.8$</td>
</tr>
<tr>
<td style="text-align: left;">LongChat 13B (16k)</td>
<td style="text-align: center;">$43.3 / 48.4$</td>
<td style="text-align: center;">$62.4 / 61.1$</td>
<td style="text-align: center;">$54.5 / 52.1$</td>
</tr>
<tr>
<td style="text-align: left;">Recurrence</td>
<td style="text-align: center;">$51.3 / 56.0$</td>
<td style="text-align: center;">$47.7 / 45.4$</td>
<td style="text-align: center;">$35.6 / 33.8$</td>
</tr>
<tr>
<td style="text-align: left;">Retrieval</td>
<td style="text-align: center;">$63.1 / 64.8$</td>
<td style="text-align: center;">$63.7 / 62.2$</td>
<td style="text-align: center;">$54.0 / 52.1$</td>
</tr>
<tr>
<td style="text-align: left;">Full Context (keep left)</td>
<td style="text-align: center;">$56.7 / 64.8$</td>
<td style="text-align: center;">$62.7 / 62.7$</td>
<td style="text-align: center;">$\mathbf{5 9 . 4 / 5 6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Full Context (keep right)</td>
<td style="text-align: center;">$\mathbf{7 0 . 1 / 7 2 . 5}$</td>
<td style="text-align: center;">$64.7 / 63.1$</td>
<td style="text-align: center;">$50.5 / 50.0$</td>
</tr>
<tr>
<td style="text-align: left;">MEMWALKER</td>
<td style="text-align: center;">$67.4 / \mathbf{7 3 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 3 / 6 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 4 / 6 0 . 4}$</td>
</tr>
</tbody>
</table>
<h1>4.2 MODEL</h1>
<p>We use Stable Beluga 2 (Mahan et al.) as the base LLM for the majority of our experiments, as it provides state-of-the-art performance compared to several other LLM variants, as we will show. Stable Beluga 2 is an instruction-tuned model built on top of 70B LLaMA-2(Touvron et al., 2023), where the finetuning does not overlap with our evaluation tasks. It has a maximum 4,096 token context length. We use the model in a zero-shot prompting fashion without further fine-tuning or incontext few shot examples for our tasks. We use top- $p$ sampling for both memory tree construction as well as generating action and reasoning for navigation. We set the maximum number of nodes $\max <em t="t">{t} M</em>=8,5,8$ and segment size $|c|=1000,1000,1200$ for QuALITY, SummScreenFD, and GovReport respectively.</p>
<h3>4.3 BASELINES</h3>
<p>We compare with three baselines memory techniques all based on the same underlying LLM, Stable Beluga 2: 1) full context window, 2) recurrence, and 3) retrieval. The full context window baselines utilize the full 4,096 tokens to process both the long input text and generation. Since the instances in the dataset often exceed the context limit, we perform truncation of the length to the right (most recent) or left (least recent) of the text as the input, as evaluate both approaches. For retrieval, we use Contriever (Izacard et al., 2022) to select segments from the long context based on the query. The highest scored segments are concatenated as the input context to the LLM until they fill the context. Finally, we implement a baseline that recurrently carries information from previous segment tokens to the current one through summarization (Xu et al., 2022), where each segment is 2,500 tokens and the maximum summary size is 500 tokens.</p>
<h2>5 ReSULTS \&amp; ANALYSIS</h2>
<p>Main results. Table 2 shows comparisons between MEMWALKER and other baselines. MEMWALKER outperforms both the recurrence baseline across all tasks by a large margin. This shows the limitation of recurrence, where relevant information to the query is lost after several steps. MEMWALKER also outperforms retrieval where the segments are from a coherent long story instead of separate documents. On these tasks, the full context baselines can perform well in the "Original" task setting, which can contain relatively shorter sequences, although choosing either left or right truncate for best performance seems to be dataset dependent. Still, MEMWALKER achieves higher performance in the Original setting against the Full Context baselines except for the keep right variant on QuALITY and the keep left variant on GovReport, likely due to the positional bias in the dataset where relevant segment often appears at the beginning or the end of the text. However, on the Long version of all three tasks MEMWALKER outperforms all baselines, that is it shows strong performance when memory access becomes more critical. MEMWALKER also outperforms other publicly available models, including LongChat (Li et al., 2023) and MPT (MosaicML, 2023).</p>
<p>Table 3: MemWALKER performance using different underlying LLMs with different reasoning capabilities, and an ablation on their reason justification component when making a navigation decision ("w/o reasoning" simply predicts the action, with no reason generated, see e.g. Table 1). Valid Action shows the percent of generated actions that are a valid navigation action. We find that the strongest performing LLM (Stable Beluga 2) benefits from reasoning with improved accuracy, while weaker performing LLMs do not (get worse in terms of accuracy and valid actions).</p>
<table>
<thead>
<tr>
<th></th>
<th>QuALITY</th>
<th>SummScreenFD</th>
<th>GovReport</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc. / Valid Action (\%)</td>
<td>Acc. / Valid Action (\%)</td>
<td>Acc. / Valid Action (\%)</td>
<td></td>
</tr>
<tr>
<td>LLaMA 2 Chat (13B)</td>
<td>$39.6 / 73.2$</td>
<td>$20.9 / 75.5$</td>
<td>$15.8 / 69.0$</td>
<td></td>
</tr>
<tr>
<td>w/o reasoning</td>
<td>$48.1 / 97.4$</td>
<td>$25.8 / 95.8$</td>
<td>$21.8 / 93.1$</td>
<td></td>
</tr>
<tr>
<td>LLaMA 2 Chat (70B)</td>
<td>$52.0 / 86.1$</td>
<td>$55.6 / 99.5$</td>
<td>$41.6 / 97.8$</td>
<td></td>
</tr>
<tr>
<td>w/o reasoning</td>
<td>$59.9 / 100.0$</td>
<td>$58.5 / 100.0$</td>
<td>$42.6 / 100.0$</td>
<td></td>
</tr>
<tr>
<td>Stable Beluga 2 (70B)</td>
<td>$67.4 / 92.5$</td>
<td>$67.3 / 95.1$</td>
<td>$59.4 / 97.0$</td>
<td></td>
</tr>
<tr>
<td>w/o reasoning</td>
<td>$66.8 / 100.0$</td>
<td>$64.1 / 90.5$</td>
<td>$52.5 / 98.2$</td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance breakdown by context length (in tokens). Each dataset is thresholded into two bucket of equal sizes. MEMWALKER outperforms full context baselines (truncated either left or right, when the sequence does not fit) on longer context sequences, for all three tasks.</p>
<p>MEMWALKER improves performance on long sequences. We provide a breakdown of performance by input sequence length for each task in Figure 2. MEMWALKER is not advantageous over Full Context (with truncation left or right) baselines when the text length is short, but outperforms both types of truncation for all tasks for longer sequences. The benefit of interactive reading emerges after the text length is suitably large, i.e. showing better performance once the sequence length is sufficiently larger than the LLM context length of 4,096 .</p>
<p>Reasoning capability is essential for memory tree navigation. The effectiveness of MEMWALKER is highly dependent on the underlying LLM's reasoning capability. For each navigation decision, we employ an LLM prompt that requires the LLM to first generate a reason in natural language that justifies the following predicted action, see Table 1. We show in Table 3 how reasoning impacts performance by comparing Llama 2 Chat (13B and 70B parameter variants) and Stable Beluga 2 (70B) with and without the reasoning justification by removing the line "First provide reasoning ...before you make your decision" from the prompt. With the smaller, less capable models (13B), the performance lags behind 70B models by a large margin due to its inability to follow instructions. In fact, asking for reasoning justifications for weaker models decreases performance, presumably due to their inability to generate and make use of such reasons. Stable Beluga 2 outperforms Llama 2 Chat for the same LLM size, and also displays heightened reasoning ability. For Stable Beluga 2, asking for reasoning justification improves performance across all tasks. This highlights the main characteristic of MEMWALKER: if an LLM passes a critical reasoning ability threshold, it can reason about a long input in multiple rounds without errors cascading quickly across rounds. For weaker LLMs that cannot make good navigation decisions, errors could compound and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: MemWalker performance comparisons between using working memory and without (i.e., the LM only looks at the content of the children memory tree nodes, rather than memory from all the nodes it has traversed). Inclusion of working memory yields large gains.</p>
<p>Table 4: MEMWALKER navigation analysis. Stray ratio: percentage of paths that contain the revert action. Recovery Rate: percentage of stray paths that recover and answer the query correctly.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Stray <br> Ratio</th>
<th style="text-align: center;">Recovery <br> Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QuALITY</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: left;">SummScreenFD</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">59.6</td>
</tr>
<tr>
<td style="text-align: left;">GovReport</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">79.0</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Percentage comparison of total tokens processed against the tokens of the original example between all paths vs. successful paths.
overall performance suffers. As LLMs will only improve in reasoning ability over the coming years, we expect methods like MEMWALKER will become more and more effective.</p>
<p>Navigating the memory tree requires working memory. As MEMWALKER makes decisions to traverse the memory tree and read relevant segments, it might lose sight of the overall context. The model thus carries information from the nodes along the navigation path as working memory, where the content of the working memory updates as the model selects the next path. We evaluate the performance of MEMWALKER with and without working memory, with results given in Figure 3. We find a significant performance degradation without working memory across all tasks, with a $5-13 \%$ drop in accuracy, showing the importance of this component.</p>
<p>MEMWALKER can recover from stray paths. As MEMWALKER navigates the memory tree, it needs to not only find the path towards the most pertinent segments, but also potentially to recover from traversal errors should they occur. We report recovery statistics in Table 4. MEMWALKER executes a revert navigation action (and hence changes path) for around $15 \%-20 \%$ of examples, but of those examples can recover and get those examples correct $70 \%$ of the time for QuALITY, $\sim 60 \%$ for SummScreenFD, and $\sim 80 \%$ for GovReport.</p>
<p>MEMWALKER enables efficient reading. Since MEMWALKER determines which parts of the long text it needs to read, the effective content that needs to be read may be smaller than the entire sequence. We report the percentage of the long context read averaged over all examples, for each of the three tasks, in Figure 4. We find that between only $63 \%-69 \%$ of the text on average needs to</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance trade-off of different memory construction configurations on QuALITY. x-axis: maximum number of nodes that can be connected to a parent node. Red: summarizing 1, 000 -token segments. Blue: summarizing 500 -token segments.
be read to answer the question including the content of the tree nodes. Among successful paths, the reading required further reduces to $59 \%-64 \%$.</p>
<p>Memory tree construction trade-off. A fundamental trade-off arises as we construct the memory tree - summarizing larger segments compresses more information into a node to reduce the depth of the tree, but risks losing fidelity of the content. Similarly, connecting many lower level nodes to the upper one can help flatten the tree, yet render the navigation task harder for the LLM at each node. Figure 5 shows the performance of different configurations of the memory tree on QuALITY. Summarizing larger segments is generally more beneficial than smaller segments as well as connecting more children nodes to the parent. However, the performance plateaus as the maximum number of nodes increases, showing the trade-off with respect to how much information can be packed into the nodes during memory tree construction.</p>
<h1>6 CONCLUSION</h1>
<p>We propose MEMWALKER, an interactive reading agent which uses iterative LLM prompting to decide which part of the content should be read closely based on its own reasoning. Our approach first builds a structured memory given long context sequence data, and then makes navigation decisions of the pertinent parts to read given a query. Our method shows superior performance against a number of baselines including various long context length models, retrieval and recurrence baselines, in particular for longer sequence tasks. Detailed analysis highlights a number of important factors, including our method's ability to reason about navigation decisions, ability to revert navigation to a different path when necessary, and incorporation of a working memory. Future work should explore many new directions that MEMWALKER opens up, in particular its application to different data structures other than trees, and finetuning its performance specific to the interactive reading goal.</p>
<h2>7 LIMITATIONS</h2>
<p>MEMWALKER exhibits three major limitations. First, the memory tree generation might not scale too well if the sequence's length becomes extremely long. The increase in sequence length entails more nodes in the tree and hence renders the tree construction process onerous. Workaround such as trading off the granularity of the summary in exchange for speed might be viable. Nonetheless, the issue of scaling remains a limit. In this setting it may make sense to generalize MEMWALKER to a combination of tree and hash Bawa et al. (2005) or other alternative data structure, whilst retaining its travesersal ability via LLM prompting. Second, MEMWALKER only works when the LLM exhibits a strong enough reasoning capability, which according to our experiments is required to be large (over 70B) and instruction-tuned. If the reasoning capability falls short, the error compounds</p>
<p>and the method would fail. Enabling a smaller model that can perform a similar instruction following procedure could be useful for scaling the method. This could be made possible by removing the following third limitation. Third, MEMWALKER only uses zero-shot prompting and does not leverage fine-tuning to further improve the interactive reading capability. This could be done, for example, by performing interactive reading and collect the successful paths for further fine-tuning.</p>
<h1>REFERENCES</h1>
<p>Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit Sanghai. Colt5: Faster long-range transformers with conditional computation. In preprint, 2023.</p>
<p>Mayank Bawa, Tyson Condie, and Prasanna Ganesan. Lsh forest: self-tuning indexes for similarity search. In Proceedings of the 14th international conference on World Wide Web, pp. 651-660, 2005.</p>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. In preprint, 2020.</p>
<p>Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scaling transformer to 1m tokens and beyond with rmt. In preprint, 2023.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer opendomain questions. arXiv preprint arXiv:1704.00051, 2017.</p>
<p>Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: A dataset for abstractive screenplay summarization. In Association for Computational Linguistics (ACL), 2022.</p>
<p>Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. preprint, 2023.</p>
<p>Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In preprint, 2023.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Association for Computational Linguistics (ACL), 2019.</p>
<p>Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022.</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241, 2018.</p>
<p>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. In preprint, 2020.</p>
<p>Mor Geva and Jonathan Berant. Learning to search in long documents using document structure. arXiv preprint arXiv:1806.03529, 2018.</p>
<p>Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. Longt5: Efficient text-to-text transformer for long sequences. In North American Association for Computational Linguistics (NAACL), 2022.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. In Neural Computation, 1997.
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In North American Association for Computational Linguistics (NAACL), 2021.</p>
<p>Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In preprint, 2020.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. 2022.</p>
<p>Jack Lanchantin, Shubham Toshniwal, Jason Weston, Arthur Szlam, and Sainbayar Sukhbaatar. Learning to reason and memorize with self-notes. In preprint, 2023.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: $9459-9474,2020$.</p>
<p>Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length? 2023. URL https://lmsys.org/blog/2023-06-29-longchat.</p>
<p>Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. In preprint, 2023.</p>
<p>Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. Stable beluga models. URL https://huggingface.co/stabilityai/StableBeluga2.</p>
<p>Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. In Association for Computational Linguistics (ACL), 2016.</p>
<p>MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms. 2023. Accessed: 2023-05-05.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. In preprint, 2021.</p>
<p>Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel R Bowman. Quality: Question answering with long input texts, yes! In North American Association for Computational Linguistics (NAACL), 2022.</p>
<p>Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.</p>
<p>Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. Scrolls: Standardized comparison over long language sequences. In Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. Pearl: Prompting large language models to plan and execute actions over long documents. In preprint, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen</p>
<p>Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.</p>
<p>Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. In preprint, 2021.</p>
<p>Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>Jing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term open-domain conversation. In Association for Computational Linguistics (ACL), 2022.</p>
<p>Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reprompting and revision. In Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Advances in Neural Information Processing Systems (NeurIPS), 2022.</p>
<p>Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long text. In preprint, 2023.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 Prompts</h2>
<p>We provide full prompts for both memory tree construction and navigation described in $\S 3$.</p>
<h2>A.1.1 MEMORY TREE CONSTRUCTION Prompts</h2>
<p>We use two prompts for memory tree construction (the construction component of Table 5). The first (leaf) instructs the LLM to summarize the text segment into a comprehensive summary. After this step, the segments are grouped and summarized into non-leaf node summaries. The summaries ([CHILD_SUMM_NODE_0], [CHILD_SUMM_NODE_1],..., [CHILD_SUMM_NODE_N]) are grouped and concatenated as the summary content of their parent node. During this process, if the concatenated summaries exceed the predetermined length, the second construction prompt is used to further summarize the text (i.e., [SUMMARIES]) for the parent node.</p>
<h2>A.1.2 Navigation Prompts</h2>
<p>We use two navigation prompts (triage and leaf) as described in $\S 3$. We show the general prompt template in the navigation stage of Table 5.</p>
<h2>A. 2 EXAMPLES</h2>
<p>We provide an extra navigation example in Table 6.</p>
<p>Table 5: Prompts used for the memory tree construction stage and the navigation stage. For the memory construction stage, [TEXT_OF_SEGMENT] is filled with the segment text at the leaf nodes. [SUMMARIES] is the concatenated summaries from the child nodes and will be further summarized if it exceeds the predetermined length. For navigation, [QUERY] is the query, [OPTIONS] are the multi-choice options (only in QuALITY), [CHILD_SUMM_NODE_n] represents the summary text of the $n$-th child node, and [WORKING_MEMORY] is the information carried from previous nodes. Yellow indicates triage prompt and purple indicates leaf prompt, as described in $\S 3$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Stage</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Construction (leaf)</td>
<td style="text-align: center;">[TEXT_OF_SEGMNET] . Summarize the above text comprehensively into a fluent passage.</td>
</tr>
<tr>
<td style="text-align: center;">Construction (non-leaf)</td>
<td style="text-align: center;">[SUMMARIES]. Compress each summary into a much shorter summary.</td>
</tr>
<tr>
<td style="text-align: center;">Navigation (triage)</td>
<td style="text-align: center;">The following passage(s) are the summaries of the different parts of a story. <br> To answer the question: [QUERY] <br> Which of the following summary is MOST LIKELY to contain information about the answer? <br> First provide reasoning to compare the summaries before you make the decision. <br> Summary 0: [CHILD_SUMM_NODE_S] <br> Summary 1: [CHILD_SUMM_NODE_1] <br> Summary N: [CHILD_SUMM_NODE_N]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Relpy with the passage number as your action. <br> You MUST choose one summary number and you should reply with the following format: <br> $88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888</td>
</tr>
</tbody>
</table>
<p>Table 6: Example trajectory from the SummScreenFD dataset.</p>
<h1>Navigation Trajectory</h1>
<p>Node 1 The following passage(s) are the summaries of the different parts of a story.
To answer the question: Who invited Michael to his business school as a guest speaker?
Which of the following summary is MOST LIKELY to contain information about the answer?
First provide reasoning to compare the summaries before you make the decision.
Summary 0: In the text, Michael and Ryan are on their way to give a speech at a business school. [...]
Summary 1: Michael is reminiscing about his college days and suggests playing Frisbee with a college student. [...]
Summary 2: Michael Scott is giving a presentation to a group of business students, attempting to explain [...]
Summary 3: In the text, there is a scene where a bat is found in the office and employees react differently to its presence. [...]
Summary 4: Pam, an artist, has an art show featuring her paintings. Roy compliments her art [...]
Relpy with the passage number as your action.
You MUST choose one summary number and you should reply with the following format:
####################################################################################################\</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Further summarizing the working memory as it accumulates would be an alternative approach, which we have not explored in this study.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8699 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8699</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8699</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-156.html">extraction-schema-156</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <p><strong>Paper ID:</strong> paper-195658159</p>
                <p><strong>Paper Title:</strong> From Multi-modal Property Dataset to Robot-centric Conceptual Knowledge About Household Objects</p>
                <p><strong>Paper Abstract:</strong> Tool-use applications in robotics require conceptual knowledge about objects for informed decision making and object interactions. State-of-the-art methods employ hand-crafted symbolic knowledge which is defined from a human perspective and grounded into sensory data afterwards. However, due to different sensing and acting capabilities of robots, their conceptual understanding of objects must be generated from a robot's perspective entirely, which asks for robot-centric conceptual knowledge about objects. With this goal in mind, this article motivates that such knowledge should be based on physical and functional properties of objects. Consequently, a selection of ten properties is defined and corresponding extraction methods are proposed. This multi-modal property extraction forms the basis on which our second contribution, a robot-centric knowledge generation is build on. It employs unsupervised clustering methods to transform numerical property data into symbols, and Bivariate Joint Frequency Distributions and Sample Proportion to generate conceptual knowledge about objects using the robot-centric symbols. A preliminary implementation of the proposed framework is employed to acquire a dataset comprising physical and functional property data of 110 houshold objects. This Robot-Centric dataSet (RoCS) is used to evaluate the framework regarding the property extraction methods, the semantics of the considered properties within the dataset and its usefulness in real-world applications such as tool substitution.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8699.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8699.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot-centric AVP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robot-centric Attribute-Value Pair representation with sample-proportion grounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic, robot-centered representation of concepts where each object-class is described by attribute-value (physical/functional quality) pairs whose membership is a sample-proportion (conditional probability) computed over robot-grounded qualitative symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Attribute-Value Pair (symbolic, fuzzy/probabilistic)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are represented as collections of attribute-value tuples (property-quality, membership m), where qualities are categorical symbols derived from sensor measurements and m = P(holds(o,t)|o ∈ O) is the sample proportion (membership) measuring how often a quality occurs in a class; the representation is explicitly grounded in the robot's own sensory/motor data.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic-hybrid (symbolic attributes grounded to continuous sensory data; probabilistic/fuzzy membership)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Substitute selection / tool substitution, concept generalization over instances, modeling intra-class variability (qualitative membership), affordance reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The attribute-value/sampled-membership representation built from robot-acquired properties yields usable conceptual knowledge for substitute selection: ERSATZ (using this knowledge) selected true positives in all 11 tested scenarios and true negatives in 8/11 when compared to aggregated human expert choices; the RoCS dataset and the attribute-value heatmaps show clear class-level distributions of qualitative properties (e.g., containment, support).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Directly contrasted with top-down human-centric KBs (WordNet, ConceptNet) and logic/ontology-based representations: the AVP format is favored for substitute selection because it encodes robot-specific subjectivity, relativity and graded membership, whereas many logic/ontology KBs lack grounded qualitative degrees and remain largely non-grounded for the robot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Memberships and symbols depend on the robot's sensor/manipulator setup (robot-specific), qualitative symbols are categorical (not ordinal unless encoded), granularity depends on clustering choices (k), property definitions are intentionally simplistic (may miss facets), and transfer across heterogeneous robots is non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Argues for bottom-up, robot-centric conceptual knowledge: symbolic AVP representations grounded in a robot's own multimodal experience support practical functions (reasoning about affordances, substitute selection, planning); sample-proportion membership models intra-class variation and enables graded, task-relevant similarity judgments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8699.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8699.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clustering-based Symbols</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised clustering for sub-categorization (symbol generation from continuous property measures)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bottom-up method that converts continuous sensory/property measurements into discrete qualitative symbols via unsupervised clustering (k-means), where each cluster defines a property-quality label used in symbolic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Clustering-derived categorical symbol representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Continuous-valued property measurements (e.g., rigidity, hollowness) are partitioned into η clusters by k-means; each cluster is mapped to a symbolic label (property_cluster_i) which becomes the qualitative value for the attribute-value representation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>feature-based symbolic (bottom-up discretization)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Concept formation and categorization (granularity control), discrimination of object instances and classes, deriving typicality via frequency distributions</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>k-means clustering across k={2..11} produced meaningful partitions: e.g., plate, bowl, cup and to-go-cup converged in containment concepts; the clustering granularity influences substitutability and ability to find matches. Clustering allowed transforming continuous robot measurements into actionable categorical knowledge used downstream in ERSATZ.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared implicitly to hand-coded qualitative labels: cluster-derived symbols are data-driven and grounded (advantage) but depend on chosen k and are non-ordinal by default (limitation) unlike curated taxonomies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Choice of cluster number (granularity) critically affects representation; clusters produce categorical labels without inherent ordering; small signal-to-noise ratios (sensor noise) can collapse distinctions and force default labels.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Supports that symbols can be induced from perceptual experience (bottom-up symbol grounding) and that the number/scale of induced categories manifests the subjectivity/relativity of robot concepts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8699.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8699.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Image-schema functional mapping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functional properties derived from image-schema theory (CONTAINER, SUPPORT, PATH, BLOCKAGE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Functional/affordance properties are defined via image-schema abstractions (e.g., CONTAINER → containment, SUPPORT → support) and operationalized as functions of enabling physical properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognitive semantics and image schemas with embodied forces.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Image-schema-based compositional functional representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functional properties (containment, support, movability, blockage) are represented as compositional constructs that emerge from specific combinations of physical property values (e.g., support = [size, flatness, rigidity]); image schemas provide the abstract semantics that guides which physical properties enable which functional affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>theory-based / compositional (functional = composition of physical attributes)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Affordance recognition, tool-use reasoning, substitute selection (functional match rather than visual similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper operationalizes image-schema mappings into simple enabling sets (e.g., containment uses size+hollowness) and demonstrates they are effective in substitute selection when grounded in robot measurements; no psychophysical experiments presented, but functional definitions enabled ERSATZ to pick substitutes consistent with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Contrasted with approaches that directly hand-label affordances or rely on external commonsense KBs; image-schema compositional mapping is argued to be more explanatory because it ties affordances to measurable physical enablers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Functional definitions are simplistic and depend entirely on the chosen physical properties and their estimation accuracy; image-schema abstractions are not exhaustively validated and may omit higher-order causal or contextual constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Claims image schemas capture necessary abstractions for affordances and that grounding functional concepts as compositions of physical properties supports practical reasoning (e.g., non-invasive substitute selection).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8699.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8699.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbol grounding / anchoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perceptual symbol grounding / perceptual anchoring (Harnad; Coradeschi & Saffiotti)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual symbols are given meaning by linking them to specific sensory-motor processes or perceptual entities (perceptual anchoring), forming the bridge between numeric sensor data and symbolic knowledge used in reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The symbol grounding problem.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Perceptually grounded symbolic representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Symbols (property labels, quality labels) are not pure human-language tokens but are anchored to sensory measurements or manipulation procedures (e.g., a grounded 'hollow' symbol corresponds to measured depth/height ratio), ensuring symbols reflect the robot's own embodiment and sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic grounded</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Symbol grounding, perceptual anchoring, subjectivity/relativity of concepts, substitution reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper implements grounding by deriving symbols from sensor data (clustering) and linking property symbols to concrete estimation procedures; authors argue that robot-centric grounded symbols are necessary because human commonsense KBs are often ungrounded for robot sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Positioned against top-down, human-centric KBs: grounding yields robot-specific, usable concepts while ungrounded KB entries remain unusable without mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Grounding ties concepts to particular sensor/actuator setups, limiting transferability; full grounding of all KB entries is impractical, leaving hybrid systems necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Asserts that conceptual knowledge for embodied agents must be grounded in their perceptual-motor experience; grounding explains subjectivity and is foundational for robot-centric reasoning tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8699.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8699.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-down logic KBs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-down hand-crafted commonsense and logic-based representations (WordNet, ConceptNet, OWL/MLN/Prolog ontologies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human-authored knowledge bases and ontologies representing object categories and relations using logical formalisms and lexical networks; commonly used as universal symbolic knowledge for robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Top-down symbolic logic/ontology representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Knowledge expressed as hand-crafted symbolic relations in ontologies or lexical networks (e.g., is-a, has-property, capable-of), often implemented in OWL-RDF, MLN, Prolog or as commonsense graphs; typically not grounded in a particular robot's sensory data.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / logic-based</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Tool selection, planning, commonsense reasoning, semantic similarity computations (e.g., WordNet path and IC measures)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper reviews many such KBs and finds they provide general object labels and relations but lack robot-centric qualitative measures and subjectivity; experiments show WordNet similarity metrics produce symmetric similarity matrices that are not well-suited for context-sensitive substitute selection without pre-selection/knowledge filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Authors contrast these KBs unfavorably for substitute selection relative to robot-centric grounded AVP; top-down KBs are broad but ungrounded and lack graded, experience-based properties.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Non-grounded symbols, lack of qualitative/relative measures, reliance on human-defined relations which may not reflect robot perceptual capabilities, symmetric similarity measures that can mislead in asymmetric task contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>While valuable for general commonsense, such representations alone are insufficient for embodied, robot-specific tasks that require grounded, subjective, and graded conceptual knowledge.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8699.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8699.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-based KBs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph / database-oriented knowledge representations (e.g., RoboBrain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale knowledge engines that store multimodal knowledge in graph or database structures to support robot learning and task execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoboBrain: large-scale knowledge engine for robots.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Graph-based distributed symbolic representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Knowledge organized as nodes and edges (graph DB) representing objects, properties, and relations possibly with multimodal attachments; intended to support retrieval and association across large knowledge collections.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / graph-based</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Adaptive task learning, retrieval of candidate substitutes, commonsense querying</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper notes graph-based KBs (e.g., RoboBrain) provide scalable knowledge storage but still often lack robot-centric qualitative property values and direct grounding; therefore they may require additional mapping for embodied substitute selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Graph KBs are richer and scalable compared to single ontologies but share the same grounding limitations as other top-down KBs; the robot-centric AVP approach is complementary in that it provides grounded, quantitative-to-qualitative mappings that could populate such graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Without grounded, quantitative property attachments, graph KBs remain limited for embodied substitution tasks; integrating robot-specific measurements into these graphs is non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Graph KBs can store and distribute robot-centric conceptual knowledge if grounded measurements/AVP memberships are integrated; otherwise they remain insufficiently actionable for embodied reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The symbol grounding problem. <em>(Rating: 2)</em></li>
                <li>Cognitive semantics and image schemas with embodied forces. <em>(Rating: 2)</em></li>
                <li>WordNet: An Electronic Lexical Database. <em>(Rating: 2)</em></li>
                <li>ConceptNet -A practical commonsense reasoning tool-kit. <em>(Rating: 2)</em></li>
                <li>A review of knowledge bases for service robots in household environments. <em>(Rating: 2)</em></li>
                <li>RoboBrain: large-scale knowledge engine for robots. <em>(Rating: 1)</em></li>
                <li>KNOWROB-Knowledge processing for autonomous personal robots. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8699",
    "paper_id": "paper-195658159",
    "extraction_schema_id": "extraction-schema-156",
    "extracted_data": [
        {
            "name_short": "Robot-centric AVP",
            "name_full": "Robot-centric Attribute-Value Pair representation with sample-proportion grounding",
            "brief_description": "A symbolic, robot-centered representation of concepts where each object-class is described by attribute-value (physical/functional quality) pairs whose membership is a sample-proportion (conditional probability) computed over robot-grounded qualitative symbols.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "Attribute-Value Pair (symbolic, fuzzy/probabilistic)",
            "representational_format_description": "Concepts are represented as collections of attribute-value tuples (property-quality, membership m), where qualities are categorical symbols derived from sensor measurements and m = P(holds(o,t)|o ∈ O) is the sample proportion (membership) measuring how often a quality occurs in a class; the representation is explicitly grounded in the robot's own sensory/motor data.",
            "format_type": "symbolic-hybrid (symbolic attributes grounded to continuous sensory data; probabilistic/fuzzy membership)",
            "cognitive_task_or_phenomenon": "Substitute selection / tool substitution, concept generalization over instances, modeling intra-class variability (qualitative membership), affordance reasoning",
            "key_findings": "The attribute-value/sampled-membership representation built from robot-acquired properties yields usable conceptual knowledge for substitute selection: ERSATZ (using this knowledge) selected true positives in all 11 tested scenarios and true negatives in 8/11 when compared to aggregated human expert choices; the RoCS dataset and the attribute-value heatmaps show clear class-level distributions of qualitative properties (e.g., containment, support).",
            "comparison_with_other_formats": "Directly contrasted with top-down human-centric KBs (WordNet, ConceptNet) and logic/ontology-based representations: the AVP format is favored for substitute selection because it encodes robot-specific subjectivity, relativity and graded membership, whereas many logic/ontology KBs lack grounded qualitative degrees and remain largely non-grounded for the robot.",
            "limitations_or_counter_evidence": "Memberships and symbols depend on the robot's sensor/manipulator setup (robot-specific), qualitative symbols are categorical (not ordinal unless encoded), granularity depends on clustering choices (k), property definitions are intentionally simplistic (may miss facets), and transfer across heterogeneous robots is non-trivial.",
            "theoretical_claims_or_implications": "Argues for bottom-up, robot-centric conceptual knowledge: symbolic AVP representations grounded in a robot's own multimodal experience support practical functions (reasoning about affordances, substitute selection, planning); sample-proportion membership models intra-class variation and enables graded, task-relevant similarity judgments.",
            "uuid": "e8699.0"
        },
        {
            "name_short": "Clustering-based Symbols",
            "name_full": "Unsupervised clustering for sub-categorization (symbol generation from continuous property measures)",
            "brief_description": "A bottom-up method that converts continuous sensory/property measurements into discrete qualitative symbols via unsupervised clustering (k-means), where each cluster defines a property-quality label used in symbolic knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "Clustering-derived categorical symbol representation",
            "representational_format_description": "Continuous-valued property measurements (e.g., rigidity, hollowness) are partitioned into η clusters by k-means; each cluster is mapped to a symbolic label (property_cluster_i) which becomes the qualitative value for the attribute-value representation.",
            "format_type": "feature-based symbolic (bottom-up discretization)",
            "cognitive_task_or_phenomenon": "Concept formation and categorization (granularity control), discrimination of object instances and classes, deriving typicality via frequency distributions",
            "key_findings": "k-means clustering across k={2..11} produced meaningful partitions: e.g., plate, bowl, cup and to-go-cup converged in containment concepts; the clustering granularity influences substitutability and ability to find matches. Clustering allowed transforming continuous robot measurements into actionable categorical knowledge used downstream in ERSATZ.",
            "comparison_with_other_formats": "Compared implicitly to hand-coded qualitative labels: cluster-derived symbols are data-driven and grounded (advantage) but depend on chosen k and are non-ordinal by default (limitation) unlike curated taxonomies.",
            "limitations_or_counter_evidence": "Choice of cluster number (granularity) critically affects representation; clusters produce categorical labels without inherent ordering; small signal-to-noise ratios (sensor noise) can collapse distinctions and force default labels.",
            "theoretical_claims_or_implications": "Supports that symbols can be induced from perceptual experience (bottom-up symbol grounding) and that the number/scale of induced categories manifests the subjectivity/relativity of robot concepts.",
            "uuid": "e8699.1"
        },
        {
            "name_short": "Image-schema functional mapping",
            "name_full": "Functional properties derived from image-schema theory (CONTAINER, SUPPORT, PATH, BLOCKAGE)",
            "brief_description": "Functional/affordance properties are defined via image-schema abstractions (e.g., CONTAINER → containment, SUPPORT → support) and operationalized as functions of enabling physical properties.",
            "citation_title": "Cognitive semantics and image schemas with embodied forces.",
            "mention_or_use": "use",
            "representational_format_name": "Image-schema-based compositional functional representation",
            "representational_format_description": "Functional properties (containment, support, movability, blockage) are represented as compositional constructs that emerge from specific combinations of physical property values (e.g., support = [size, flatness, rigidity]); image schemas provide the abstract semantics that guides which physical properties enable which functional affordances.",
            "format_type": "theory-based / compositional (functional = composition of physical attributes)",
            "cognitive_task_or_phenomenon": "Affordance recognition, tool-use reasoning, substitute selection (functional match rather than visual similarity)",
            "key_findings": "Paper operationalizes image-schema mappings into simple enabling sets (e.g., containment uses size+hollowness) and demonstrates they are effective in substitute selection when grounded in robot measurements; no psychophysical experiments presented, but functional definitions enabled ERSATZ to pick substitutes consistent with human judgments.",
            "comparison_with_other_formats": "Contrasted with approaches that directly hand-label affordances or rely on external commonsense KBs; image-schema compositional mapping is argued to be more explanatory because it ties affordances to measurable physical enablers.",
            "limitations_or_counter_evidence": "Functional definitions are simplistic and depend entirely on the chosen physical properties and their estimation accuracy; image-schema abstractions are not exhaustively validated and may omit higher-order causal or contextual constraints.",
            "theoretical_claims_or_implications": "Claims image schemas capture necessary abstractions for affordances and that grounding functional concepts as compositions of physical properties supports practical reasoning (e.g., non-invasive substitute selection).",
            "uuid": "e8699.2"
        },
        {
            "name_short": "Symbol grounding / anchoring",
            "name_full": "Perceptual symbol grounding / perceptual anchoring (Harnad; Coradeschi & Saffiotti)",
            "brief_description": "Conceptual symbols are given meaning by linking them to specific sensory-motor processes or perceptual entities (perceptual anchoring), forming the bridge between numeric sensor data and symbolic knowledge used in reasoning.",
            "citation_title": "The symbol grounding problem.",
            "mention_or_use": "use",
            "representational_format_name": "Perceptually grounded symbolic representation",
            "representational_format_description": "Symbols (property labels, quality labels) are not pure human-language tokens but are anchored to sensory measurements or manipulation procedures (e.g., a grounded 'hollow' symbol corresponds to measured depth/height ratio), ensuring symbols reflect the robot's own embodiment and sensors.",
            "format_type": "symbolic grounded",
            "cognitive_task_or_phenomenon": "Symbol grounding, perceptual anchoring, subjectivity/relativity of concepts, substitution reasoning",
            "key_findings": "The paper implements grounding by deriving symbols from sensor data (clustering) and linking property symbols to concrete estimation procedures; authors argue that robot-centric grounded symbols are necessary because human commonsense KBs are often ungrounded for robot sensors.",
            "comparison_with_other_formats": "Positioned against top-down, human-centric KBs: grounding yields robot-specific, usable concepts while ungrounded KB entries remain unusable without mapping.",
            "limitations_or_counter_evidence": "Grounding ties concepts to particular sensor/actuator setups, limiting transferability; full grounding of all KB entries is impractical, leaving hybrid systems necessary.",
            "theoretical_claims_or_implications": "Asserts that conceptual knowledge for embodied agents must be grounded in their perceptual-motor experience; grounding explains subjectivity and is foundational for robot-centric reasoning tasks.",
            "uuid": "e8699.3"
        },
        {
            "name_short": "Top-down logic KBs",
            "name_full": "Top-down hand-crafted commonsense and logic-based representations (WordNet, ConceptNet, OWL/MLN/Prolog ontologies)",
            "brief_description": "Human-authored knowledge bases and ontologies representing object categories and relations using logical formalisms and lexical networks; commonly used as universal symbolic knowledge for robots.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "Top-down symbolic logic/ontology representation",
            "representational_format_description": "Knowledge expressed as hand-crafted symbolic relations in ontologies or lexical networks (e.g., is-a, has-property, capable-of), often implemented in OWL-RDF, MLN, Prolog or as commonsense graphs; typically not grounded in a particular robot's sensory data.",
            "format_type": "symbolic / logic-based",
            "cognitive_task_or_phenomenon": "Tool selection, planning, commonsense reasoning, semantic similarity computations (e.g., WordNet path and IC measures)",
            "key_findings": "Paper reviews many such KBs and finds they provide general object labels and relations but lack robot-centric qualitative measures and subjectivity; experiments show WordNet similarity metrics produce symmetric similarity matrices that are not well-suited for context-sensitive substitute selection without pre-selection/knowledge filtering.",
            "comparison_with_other_formats": "Authors contrast these KBs unfavorably for substitute selection relative to robot-centric grounded AVP; top-down KBs are broad but ungrounded and lack graded, experience-based properties.",
            "limitations_or_counter_evidence": "Non-grounded symbols, lack of qualitative/relative measures, reliance on human-defined relations which may not reflect robot perceptual capabilities, symmetric similarity measures that can mislead in asymmetric task contexts.",
            "theoretical_claims_or_implications": "While valuable for general commonsense, such representations alone are insufficient for embodied, robot-specific tasks that require grounded, subjective, and graded conceptual knowledge.",
            "uuid": "e8699.4"
        },
        {
            "name_short": "Graph-based KBs",
            "name_full": "Graph / database-oriented knowledge representations (e.g., RoboBrain)",
            "brief_description": "Large-scale knowledge engines that store multimodal knowledge in graph or database structures to support robot learning and task execution.",
            "citation_title": "RoboBrain: large-scale knowledge engine for robots.",
            "mention_or_use": "mention",
            "representational_format_name": "Graph-based distributed symbolic representation",
            "representational_format_description": "Knowledge organized as nodes and edges (graph DB) representing objects, properties, and relations possibly with multimodal attachments; intended to support retrieval and association across large knowledge collections.",
            "format_type": "symbolic / graph-based",
            "cognitive_task_or_phenomenon": "Adaptive task learning, retrieval of candidate substitutes, commonsense querying",
            "key_findings": "Paper notes graph-based KBs (e.g., RoboBrain) provide scalable knowledge storage but still often lack robot-centric qualitative property values and direct grounding; therefore they may require additional mapping for embodied substitute selection.",
            "comparison_with_other_formats": "Graph KBs are richer and scalable compared to single ontologies but share the same grounding limitations as other top-down KBs; the robot-centric AVP approach is complementary in that it provides grounded, quantitative-to-qualitative mappings that could populate such graphs.",
            "limitations_or_counter_evidence": "Without grounded, quantitative property attachments, graph KBs remain limited for embodied substitution tasks; integrating robot-specific measurements into these graphs is non-trivial.",
            "theoretical_claims_or_implications": "Graph KBs can store and distribute robot-centric conceptual knowledge if grounded measurements/AVP memberships are integrated; otherwise they remain insufficiently actionable for embodied reasoning.",
            "uuid": "e8699.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The symbol grounding problem.",
            "rating": 2,
            "sanitized_title": "the_symbol_grounding_problem"
        },
        {
            "paper_title": "Cognitive semantics and image schemas with embodied forces.",
            "rating": 2,
            "sanitized_title": "cognitive_semantics_and_image_schemas_with_embodied_forces"
        },
        {
            "paper_title": "WordNet: An Electronic Lexical Database.",
            "rating": 2,
            "sanitized_title": "wordnet_an_electronic_lexical_database"
        },
        {
            "paper_title": "ConceptNet -A practical commonsense reasoning tool-kit.",
            "rating": 2,
            "sanitized_title": "conceptnet_a_practical_commonsense_reasoning_toolkit"
        },
        {
            "paper_title": "A review of knowledge bases for service robots in household environments.",
            "rating": 2,
            "sanitized_title": "a_review_of_knowledge_bases_for_service_robots_in_household_environments"
        },
        {
            "paper_title": "RoboBrain: large-scale knowledge engine for robots.",
            "rating": 1,
            "sanitized_title": "robobrain_largescale_knowledge_engine_for_robots"
        },
        {
            "paper_title": "KNOWROB-Knowledge processing for autonomous personal robots.",
            "rating": 1,
            "sanitized_title": "knowrobknowledge_processing_for_autonomous_personal_robots"
        }
    ],
    "cost": 0.01867,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Multi-Modal Property Dataset to Robot-Centric Conceptual Knowledge About Household Objects
April 2021</p>
<p>Ricardo Sanz 
Mueller CA, Jäger G, Schleiss J, Pulugu NThosar M 
Mallikarjun Chennaboina 
R Rao 
Jeevangekar Sv 
Birk A 
Pfingsthorn M 
Zug S 
Madhura Thosar 
Faculty of Computer Science
Institute for Intelligent Cooperating Systems
Otto-von-Guericke University MagdeburgMagdeburgGermany</p>
<p>Christian A Mueller 
Robotics, Computer Science &amp; Electrical Engineering Department
Jacobs University
BremenGermany</p>
<p>† 
Georg Jäger 
Institute for Computer Science
Technische Universität Bergakademie Freiberg
FreibergGermany</p>
<p>† 
Johannes Schleiss 
Faculty of Computer Science
Institute for Intelligent Cooperating Systems
Otto-von-Guericke University MagdeburgMagdeburgGermany</p>
<p>Narender Pulugu 
Faculty of Computer Science
Institute for Intelligent Cooperating Systems
Otto-von-Guericke University MagdeburgMagdeburgGermany</p>
<p>Ravi Mallikarjun Chennaboina 
Faculty of Computer Science
Institute for Intelligent Cooperating Systems
Otto-von-Guericke University MagdeburgMagdeburgGermany</p>
<p>Sai Vivek 
Rao Jeevangekar 
Faculty of Computer Science
Institute for Intelligent Cooperating Systems
Otto-von-Guericke University MagdeburgMagdeburgGermany</p>
<p>Andreas Birk 
Robotics, Computer Science &amp; Electrical Engineering Department
Jacobs University
BremenGermany</p>
<p>Max Pfingsthorn 
OFFIS Institute for Information Technology
OldenburgGermany</p>
<p>Sebastian Zug 
Institute for Computer Science
Technische Universität Bergakademie Freiberg
FreibergGermany</p>
<p>Polytechnic University of Madrid
Spain</p>
<p>Mihai Andries
Inria Nancy -Grand-Est Research Centre
France</p>
<p>From Multi-Modal Property Dataset to Robot-Centric Conceptual Knowledge About Household Objects</p>
<p>Article 476084 Front. Robot. AI
8476084April 202110.3389/frobt.2021.476084Received: 01 June 2019 Accepted: 17 March 2021 Published: 15 April 2021 Citation:ORIGINAL RESEARCH Edited by: Plinio Moreno, Instituto Superior Técnico (ISR), Portugal Reviewed by: *Correspondence: Madhura Thosar thosar@iks.cs.ovgu.de † These authors have contributed equally to this work and share first authorship Specialty section: This article was submitted to Computational Intelligence in Robotics, a section of the journal Frontiers in Robotics and AIconceptual knowledgerobot-centric knowledgemulti-modal datasetknowledge acquisitionsubstitute selection
Robot-Centric dataSet (RoCS) is used to evaluate the framework regarding the property estimation methods and the semantics of the considered properties within the dataset. Furthermore, the dataset includes the derived robot-centric conceptual knowledge using the proposed framework. The application of the conceptual knowledge about objects is then evaluated by examining its usefulness in a tool substitution scenario.</p>
<p>Conceptual knowledge about objects is essential for humans, as well as for animals, to interact with their environment. On this basis, the objects can be understood as tools, a selection process can be implemented and their usage can be planned in order to achieve a specific goal. The conceptual knowledge, in this case, is primarily concerned about the physical properties and functional properties observed in the objects. Similarly tool-use applications in robotics require such conceptual knowledge about objects for substitute selection among other purposes. State-of-the-art methods employ a top-down approach where hand-crafted symbolic knowledge, which is defined from a human perspective, is grounded into sensory data afterwards. However, due to different sensing and acting capabilities of robots, a robot's conceptual understanding of objects (e.g., light/heavy) will vary and therefore should be generated from the robot's perspective entirely, which entails robot-centric conceptual knowledge about objects. A similar bottom-up argument has been put forth in cognitive science that humans and animals alike develop conceptual understanding of objects based on their own perceptual experiences with objects. With this goal in mind, we propose an extensible property estimation framework which consists of estimations methods to obtain the quantitative measurements of physical properties (rigidity, weight, etc.) and functional properties (containment, support, etc.) from household objects. This property estimation forms the basis for our second contribution: Generation of robot-centric conceptual knowledge. Our approach employs unsupervised clustering methods to transform numerical property data into symbols, and Bivariate Joint Frequency Distributions and Sample Proportion to generate conceptual knowledge about objects using the robot-centric symbols. A preliminary implementation of the proposed framework is employed to acquire a dataset comprising six physical and four functional properties of 110 household objects. This</p>
<p>INTRODUCTION</p>
<p>Humans have become extremely sophisticated in their use of tools compare to their animal counterparts. The sophistication pertaining to tool-use in humans involves not just the dexterity in manipulating a tool, but also the diversity in tool exploitation (Gavin et al. , 2013). The ability to exploit the tools has enabled humans to adapt and thus exert control over an uncertain environment, especially when they are faced with unfavorable situations. Given how vital the tool-use ability is, robotics researchers have been developing approaches to enable a robot to use tools in various tasks (Stoytchev, 2007;Brown and Sammut, 2012;Stückler and Behnke, 2014;Takahashi et al., 2014;Li and Fritz, 2015;Tikhanoff et al., 2015;Wicaksono and Sammut, 2018;Toussaint et al., 2019). While these approaches focus on learning tool-use behavior, our primary interest is in a question: what if the required tool is missing, given that robot has prior knowledge about what tool is required in the task (see Figure 1)?</p>
<p>Consider a scenario where a robot is performing a task and has to select between a stone and a plastic bottle for a hammering purpose. One way to reach a decision is to interact with each object, perform the action and determine its suitability on the basis of a desired outcome. However, in the real world there are many objects to select from, such (individual) interactions may not be desirable for completing the task in a reasonable amount of time as it will be time consuming to determine a suitability of individual objects. If the hands-on substitute selection is undesirable, then what is the alternative selection strategy?</p>
<p>In order to select a plausible substitute for a missing tool, the substitute needs to be similar to the missing tool in some way without having to interact with it. The question is what is needed to determine the similarity. In the literature on substitute selection, typically a substitute for a missing tool is determined by means of knowledge about object, and the knowledge-driven similarity between a missing tool prototype and a potential substitute. Such knowledge about objects varies in its contents and form across the literature: metric data about position, orientation, size, and symbolic knowledge about handpicked relations such as similar-to and capable-of extracted from ConceptNet (Bansal et al., 2020); visual and physical understanding of multi-object interactions demonstrated by humans (Xie et al., 2019); matching similarity of shapes of point clouds and materials based on the spectrometer data using dual neural network (Shrivatsav et al., 2019); metric data about size, shape and grasp, as well as a human estimate of an affordance score for task + mass (Abelha and Guerin, 2017); attributes and affordances of objects are hand-coded using a logic-based notation, and a multidimensional conceptual space of features such as shape and color intensity (Mustafa et al., 2016); hand-coded models of known tools in terms of superquadrics and relationships among them (Abelha et al., 2016); potential candidates extracted from WordNet and ConceptNet if they share the same parent with a missing tool for predetermined relations: has-property, capable-of, and used-for (Boteanu et al., 2016); hand-coded object-action relations (Agostini et al., 2015); as well as hand-coded knowledge about inheritance and equivalence relations among objects and affordances (Awaad et al., 2014). While for tool selection, metric data of certain properties are primarily considered, for substitute selection, symbolic knowledge about the object category or class is considered. In such cases, either the proposed approaches use existing common sense knowledge bases such as WordNet, ConceptNet or knowledge is hand-coded. Regardless of the use of existing knowledge bases or hand-coded relational knowledge, the required knowledge is generally carefully selected for a given task.</p>
<p>It is postulated in the literature on tool-use in animals (Baber, 2003, Chapter 1) that "a non-invasive tool selection in humans or animals alike is facilitated by conceptual knowledge about objects, especially, knowledge about their physical and functional properties and relationship between them." For instance, knowledge about what physical properties of a hammer enable the hammering action can facilitate the decision between a stone and a plastic bottle as a substitute. Conceptual knowledge about objects, in this case, is considered as a representation of objects in terms of its physical and functional properties generalized over our observations and daily interactions with them (Baber, 2003). Therefore, based on our observations and interactions with various instances of a cup, a conceptual knowledge of a cup may for example consist of an object that has a handle, is hollow and can contain liquid.</p>
<p>Like humans, such conceptual knowledge about objects is desired in robot systems (from household to industrial robotics) in order to efficiently perform tasks such as tool selection and substitute selection, where selection is driven by the knowledge about various (physical and functional) properties observed in the objects (Stoytchev, 2007;Brown and Sammut, 2013).</p>
<p>In this article, we present an approach to acquire relevant sensory data, estimate metrics of object (physical and functional) properties based on the data, and generate conceptual knowledge about objects in a bottom-up data-driven manner.</p>
<p>FIGURE 1 | The figure shows our primary area of interest within the domain of tool-use. Conceptual knowledge is desirable in tool use, however our focus is on generating conceptual knowledge required for substitute selection. The figure also illustrates the positioning of substitute selection within tool-use. While tool-use also consists of areas such as grasping, planning, manipulation, validation etc. we have left them out for the sake of clarity. Besides our primary area of interest, our intent is to distinguish it from tool-use or tool selection. Note that in tool selection a robot does not have any prior knowledge of what is tool is appropriate in a given task whereas in substitute selection, the robot does have such prior knowledge.</p>
<p>Building Blocks for Robot-Centric Conceptual Knowledge</p>
<p>In order to acquire the proposed conceptual knowledge about objects (e.g., in a household environment), the following questions need to be answered:</p>
<p>• What kind of knowledge constitutes conceptual knowledge about objects? • How can conceptual knowledge about objects be acquired? • How can the acquired knowledge be represented?</p>
<p>These questions forms the primary building blocks of our work, namely: Conceptual Knowledge, Knowledge Representation, and Robot-Centric Knowledge. In this section, we address how the building blocks are realized in this work.</p>
<p>Conceptual Knowledge</p>
<p>Humans tend to express an object in linguistic form by giving it a label such as a mug (Rand, 1990). However, for humans, a mug is not merely a label, but rather it represents a concept that has properties such as rigid, hollow, cylindrical, ability to contain liquids, made up of ceramic material and also has a primary function, for instance, holds liquid (Hodges et al., 1999).</p>
<p>But is knowing merely "whether a cup is rigid or not" enough? Consider, for instance, a choice between a cup and a stone as a substitute for a hammer. While both the objects are rigid, we have general knowledge that a stone is usually more rigid than a cup and quite possibly as rigid as a hammer. As a result, we will choose the stone over the cup for hammering. Another example is the choice between a mobile phone and a plate as a substitute for a tray to carry a drink. Since both the objects are flat, they should be viable substitutes. However, since we know that a plate is usually larger in size than a mobile phone, and a plate is closer to a tray in size than a mobile phone is, we will vote for the plate. There are two pieces of information worth noticing: Firstly, our knowledge about properties of objects is generalized, relative, subjective, and qualitative, and secondly, the selected substitutes are not necessarily visually similar to the missing tools but are rather qualitatively similar.</p>
<p>We have based our approach toward conceptual knowledge about objects, which consists of qualitative knowledge about their properties, on the way humans form a concept around objects and its properties The properties are divided into physical (see section 2.2) and functional properties (see section 2.3). The physical properties describe the physicality of objects (rigidity, weight, hollowness, roughness, flatness, size) while the functional properties ascribe the (functional) abilities or affordances to the objects (containment, blockage, support, movability). The functional properties are derived from the theory of image schema (Gärdenfors, 1987) proposed in cognitive linguistics (see section 2.3).</p>
<p>Robot-Centric Knowledge</p>
<p>The primary motivation for pursuing a robot-centric aspect stems from the research on cognitive aspects of tool use in humans and animals. Especially the theory that tool selection is a first-person-perspective activity which is driven by a relationship between the user's own conceptual knowledge about a tool and their ability to use that tool (Baber, 2003). We noted earlier that one of the aspects of conceptual knowledge that needs to be expressed is subjective knowledge. It has been argued in the cognitive science studies on concept formation that conceptual knowledge of an object is grounded in an individual's multimodal perceptual experiences with various objects (Feldman and Narayanan, 2004;Gallese and Lakoff, 2005;Louwerse and Jeuniaux, 2010). This suggests that a conceptual understanding of any object may differ from person to person. This also holds true for robots as in general, as robots come in a multitude of perception and manipulation configurations. As a consequence, the individual perception and manipulation of the world similarly varies from robot to robot. Therefore, knowledge acquired about an object by a KUKA KR1000 Titan (maximum payload of 1,300 kg, 3.6 m reach), for example, will not be the same as knowledge acquired by a Universal Robot UR3 (maximum payload of 3 kg, 0.5 m reach).</p>
<p>We propose that such robot-centric conceptual knowledge should be generated in a bottom-up fashion: First, we capture the sensory data about various properties of objects. The sensory data is then processed to estimate quantitative measurements of properties observed in objects which are then used to generate property specific qualitative measurements. A conceptual knowledge about objects is then generated for given objects on the basis of the qualitative measurements of various properties (see Figure 2B).</p>
<p>Knowledge Representation</p>
<p>For representing conceptual knowledge about objects for substitute selection, we need a formalism that represents the four aspects of conceptual knowledge as stated above: generalized, relative, subjective, and qualitative. Let us see them one by one. A representation of generalized knowledge should express knowledge about an object category in terms its properties. The generalized knowledge about the object category should be relative to a robot's experience with different instances of the respective category and other categories too. The relativeness of the knowledge also entails that the conceptual knowledge about objects should be updated as the robot acquires experiences with new instances of the known object category or a new object category. As a result a formalism for representing relative knowledge should manifest such experiences.</p>
<p>In order to capture such subjectivity, it is necessary that the knowledge is grounded in the robot's own sensory perception of the properties of objects. A symbol grounding process bridges the gap between symbolic knowledge and sensory perception by creating a correspondence between them. This correspondence either refers to a physical entity in the real-world a.k.a. perceptual anchoring (Coradeschi and Saffiotti, 2003) or assigns a meaning to a symbol by means of a respective sensory-motor process (Harnad, 1990) (what I sense is what I know) (see Figure 2A). We noted in section 1.1.1 that we require a formalism to represent qualitative knowledge about objects' properties which can be used by a robot in substitute selection. A similar observation has been made in Rand (1990) which states that when representing an object as a concept, humans usually omit quantitative measurements, but assign what we have termed as qualitative measurements to the properties of the object to reflect to what degree that property is present in that object category relative to one's own experience. For instance, a cup is generally light weight, medium rigid, and can fully contain solid or liquid.</p>
<p>Related Work</p>
<p>Since the demand for conceptual knowledge has been increasing in robotic applications, the development of knowledge bases has been undertaken by many researchers around the world (cf. Thosar et al., 2018b). While there exists a multitude of knowledge bases, the question is how many existing knowledge bases about objects conform to the above mentioned requirements: conceptual knowledge base containing knowledge about the objects' properties that is general, relative, subjective (robotcentric), and qualitative. In Thosar et al. (2018b), we reviewed existing knowledge bases primarily containing knowledge about household objects and their underlying acquisition system developed for service robotics to address this question. For the review article, we selected 20 papers covering 9 knowledge bases about household objects on the basis of the contents of the paper with respect to the above mentioned requirements and overall impact of the paper on the basis of the number of citations (refer Table 1). Our review resulted in the following conclusions with respect to each building block discussed in the previous section: Conceptual Knowledge: As our desired conceptual knowledge about an object consists of qualitative knowledge about its physical and functional properties, we reviewed the existing knowledge bases to examine whether such conceptual knowledge was considered. We noted that the majority of the knowledge bases relied on the external human-centric commonsense (universal) knowledge bases such as ConceptNet (Liu and Singh, 2004), WordNet (Fellbaum, 1998) (KnowRob, MLN-KB, OMICS, RoboBrain), Cyc (Lenat, 1995) (PEIS-KB), OpenCyc (Lenat, 1995) (KnowRob, ORO, RoboBrain) and the rest either relied on the hand-coded knowledge (OMRKF, OUR-K) or on knowledge acquired by human-robot interaction (NMKB), for the symbolic conceptual knowledge about objects. Our review concluded that while the existing knowledge bases do contain general knowledge about objects, they do not contain qualitative knowledge about their properties as discussed in section 1.1.1. For instance, a cup is described in WordNet as a small open container usually used for drinking; usually has a handle. The description does not contain qualitative knowledge about various properties such as size, shape, weight, roughness, or rigidity observed in a cup. Moreover, it is worth noting that the knowledge about objects in the existing knowledge bases is universal in nature and thus lacks subjectivity and relativity aspects of knowledge. While having common sense, universal knowledge has its merits, in section 4.4.1, we have discussed an experiment which illustrates the inadequacy of using WordNet in substitute selection without pre-selecting knowledge.</p>
<p>Knowledge Representation:</p>
<p>Logic based representation formalisms were overwhelmingly used by a majority of the knowledge bases to represent knowledge: OWL-RDF (KnowRob, OMRKF, ORO, OUR-K), Markov Logic Network (MLN-KB), Prolog -Horn Clause (NMKB), Second Order Predicate Logic (PEIS), while database inspired formalisms were used by RoboBrain (Graph Database) and OMICS (Relational Database). Besides representing knowledge about objects, the knowledge bases also focus on representing various uncertainty factors such as noisy sensor information, incomplete knowledge, unknown objects or environment, and inconsistent knowledge. While all the above uncertainty factors are significant, the desired factors such as relativity, and qualitative measures were not formalized while representing knowledge about object properties. For instance, when we think of a cup, although at the abstract level, it is a type of container, the degree of containment is different in a cup for espresso coffee and a cup for tea. Such variation in the containment is not reflected in the representations in the knowledge bases. Robot-Centric: Almost all of the knowledge bases (except for OMICS) addressed the problem of symbol grounding. While the object labels, appearance related properties (shape, size, etc.), and functional properties (KnowRob, MLN-KB, NMKB, PEIS) were grounded in the robot's perception, the reliance on humancentric symbolic knowledge did pose a disadvantage. Since the commonsense knowledge bases are fully human-made, the depth and breadth of the knowledge is not perceivable by a robot due to its limited perception and manipulation capabilities. While a low portion of human-centric knowledge is grounded into robot's limited perception, the majority of the knowledge base remains non-grounded. We believe that such non-grounded knowledge may not be adequate for a robot in a substitute selection task.</p>
<p>It should be noted that the knowledge bases existed independent of the sensory perception. The symbol grounding processes were introduced in the knowledge bases to correspond the sensory perception with relevant symbolic knowledge.</p>
<p>In contrast, our proposed approach generates knowledge from the quantitative measurements computed from the sensory data and as a consequence, the knowledge generated from the sensory data for a robot A may differ from the knowledge generated for robot B. This is due to the different sensory capabilities of both the robots, thus reflecting the notion of robot-centricity: object understanding from a firstperson perspective.</p>
<p>Contribution</p>
<p>The research work discussed in this article offers a framework for generating robot-centric knowledge about physical and functional properties of objects. Our contribution is 2-fold: we propose</p>
<p>• an approach to extract the sensory data and estimate quantitative measurements related to physical properties of objects; • an approach to generate robot-centric qualitative conceptual knowledge about objects from the quantitative measurements.</p>
<p>Multi-Modal Physical Property Estimation</p>
<p>Our primary objective is to generate robot-centric conceptual knowledge about objects from object properties based on robot sensory data. In order to realize such a bottom-up approach to generate symbolic knowledge, the first step is to extract the sensory data about an objects' physical and functional properties.</p>
<p>In this research work, we primarily focus on extracting sensory data about physical properties from objects. The contribution for physical property extraction from objects is 2-fold: Physical properties estimation: We saw earlier that the methods from the literature on tool selection and substitute selection do estimate one or more physical properties of objects. Besides these two applications, approaches for estimating various physical properties of objects such as rigidity, shape, texture, size, etc. have been proposed for applications such as object recognition/categorization, grasping, and manipulation (Takamuku et al., 2007;Kraft et al., 2009;Sinapov et al., 2009;Spiers et al., 2016;Wu et al., 2016;Kaboli et al., 2017;Kim et al., 2018). In this work, we propose light-weight estimation methods for rigidity, hollowness, size, flatness and roughness, requiring a minimal experimental set-up. Our proposed methods estimate the properties from a single instance at a time and do not require any prior training data for estimation, in contrast to the methods proposed in Wu et al. (2016), Sinapov et al. (2009), Spiers et al. (2016, and Kim et al. (2018).</p>
<p>Property estimation framework: In this article, we propose an extensible property estimation framework called Robot-Centric Dataset Framework (RoCS) wherein multiple property estimation methods can be used to measure various physical properties and functional properties. Currently, the framework consists of six physical properties and four functional properties where the measurements of functional properties are estimated on the basis of metrics of the physical properties. Our proposed framework is flexible in that it separates the sensory data acquisition from the actual property estimation methods. Such separation allows for redefining the estimation methods with a different set of sensory data than the existing one. Additionally, the proposed framework is also used to create a multi-layered dataset about household objects where the layers denote the different levels of abstraction (Figure 2).</p>
<p>Generation of Robot-Centric Conceptual Knowledge:</p>
<p>Our second contribution focuses on the generation of robot-centric conceptual knowledge from the quantitative measurements of object properties. Besides quantitative measurements, a set of symbols representing property labels and object labels are provided a priori for generating knowledge. The proposed knowledge generation method generates knowledge about individual instances which is then used to generate general knowledge about object classes as illustrated in Figures 2B, 3 -Layer 4 and Layer 5.</p>
<p>For estimating qualitative measurements, a clustering method is used on the quantitative measurements for each property. Each cluster in this case represents a qualitative measurement for the property. As a consequence, the knowledge about an individual instance consists of the respective qualitative measurements of each property. In this manner, each qualitative measure of an instance is grounded into its corresponding quantitative measure of the property while each property symbol is grounded into its estimation method.</p>
<p>In order to generate knowledge about an object class, we propose using the statistical methods Bivariate Joint Frequency Distribution followed by Sample Proportion on the instance knowledge base. We use these statistical methods to model the intra-class variations in an object class. In other words, these methods provide insights into which qualitative measures of a property were observed the most and the least across the various instances of an object class. At the end, an Attribute-Value Pair based formalism is used to represent a qualitative measure and its corresponding sample proportion of an object class. The collection of corresponding attribute-value pairs represent knowledge about an object class. We would like to point out here that the proposed bottom-up knowledge generation approach is attributed to the fact that in the absence of any sensory data, the conceptual knowledge base can not be created (Figure 2).</p>
<p>PROPERTY ESTIMATION FRAMEWORK</p>
<p>The underlying principle behind the proposed framework is to estimate physical and functional properties from a single instance. We have proposed expert-based models for estimating five physical properties namely rigidity, roughness, hollowness, flatness, and size, requiring minimal experimental set-up. In contrast, data-driven models would typically need many training examples for each property which may not be feasible as more properties are added in the framework. Our ultimate vision for the framework is to develop an online platform where researchers from around the world can plug-in estimation methods for the same or new properties, physical and functional, including the documentation on the required experimental set-up. The idea is to allow users to select the estimation methods based on the available hardware at their end in order to acquire robot-centric measurements of object properties.</p>
<p>The Figure 3A illustrates the modular structure of the estimation framework. It primarily consists of two modules: online data acquisition and property estimation. The online data acquisition module is responsible for extracting sensory data about objects. In Figure 3B, the property estimation modules consists of three primary phases as depicted in the figure. In the feature extraction phase, the desired features are extracted from the sensory data (see Figure 3B-feature extraction). The extracted features are integrated in the feature integration phase to form the primary parameters required for estimating the measurements of the properties (see Figure 3Bfeature integration). The last phase consists of computing the quantitative measurements using the proposed expert-based models (see Figure 3B -physical and functional property estimation). The quantitative measurements of the properties are then forwarded to the knowledge generation module for generating conceptual knowledge about objects.</p>
<p>In our framework, the decoupling of data acquisition, feature extraction and feature integration allows the flexibility for redefining the existing property estimation models or proposing estimation models for new properties. For instance, in the current system, hollowness is defined on the basis of depth. However, it can be redefined on the basis of size and depth as well. Such flexibility, in our opinion, is necessary for robotcentric measurement acquisition since sensory and manipulation capabilities vary from robot to robot. As a result, based on the available sensor and manipulation capabilities, a user may re-purpose the features for redefining the properties or design estimation methods for new properties. The desire for flexibility is driven by one of the pressing issues which is interpreting the meaning of the properties. The meaning can be complex where various facets of a property and their relationship to the various parts of an object are perceived and interpreted accordingly or it can be primitive or simplistic. In either case, the meaning or definition of a property forms a basis for designing a hardware set-up and a subsequent estimation method.</p>
<p>In the following section, we will discuss in detail our proposed property estimation methods to acquire the quantitative measurements of objects properties. An approach to the generation of robot-centric conceptual knowledge from the acquired quantitative measurements is discussed in section 3. Section 4 discusses the extraction of the dataset from 110 household objects using the proposed framework and the estimation methods (section 4.1) followed by an evaluation of the quality of the acquired dataset (section 4.2), the semantics of property measurements (section 4.3), as well as knowledge base generation and its application in a tool substitution scenario (section 4.4).</p>
<p>Property Estimation</p>
<p>In this section, we will discuss the selection of properties, their definitions, and proposed estimation methods for acquiring their quantitative measurements for a robotic platform. Currently the framework supports six physical properties namely rigidity, roughness, flatness, size, hollowness and weight, and four functional properties namely containment, blockage, movability, and support. In this work, when interpreting the properties, simplistic definitions of the properties were formed which allowed for a minimal set-up and light-weight estimation methods. The intend behind a simplistic approach is that it allows the use of a simple mobile manipulator whose limited capabilities can be exploited. Additionally, such a minimal experimental-set up can easily be reproduced as they do not require high-end robotic platforms. The notion of the physical properties is based on the physical properties in solid-state physics, where they are considered as properties which can be observed, measured, and quantified. We have extended the notion of functional properties in the similar fashion where they are measured and quantified. The selection of these properties are inspired by literature on the tool use in humans and animals (Vauclair and Anderson, 1994;Baber, 2003;Susi and Ziemke, 2005;Hernik and Csibra, 2009;Vaesen, 2012;Biro et al., 2013;Sanz et al., 2013).</p>
<p>In the following, each property is described in a 2-fold manner. First, for each property a general definition is provided where we aim for a simplistic and intuitive characterization for each property. The property definitions considered in this work are not unique. The proposed framework can be extended by plugging in separate estimation methods for the same property based on more complex and/or different characterizations. Second, for each property an estimation method is proposed. Note that, although the property definitions are formulated from a human perspective, our ultimate aim is toward enabling a robot to assemble its own understanding about objects, given its own perceptual capabilities in form of vision and manipulation feedback.</p>
<p>Hence, we have derived estimation methods allowing a robot to interpret its sensory feedback ( Figure 3B) of objects for generating numeric representations of physical and functional properties. While the presented methods consider features acquired from our robotic platform (Kuka youBot; Bischoff et al. (2011), see Figure 5) and an RGB-D sensor (Asus Xtion Pro; Swoboda, 2014), we aim to propose a light-weight setup (Figure 4) and methods that are transferable and adoptable to other robotic platforms by considering common hardware interfaces and data representations such as images, point clouds, or joint states of robotic manipulators.</p>
<p>We may summarize, that the following proposed estimation methods represent a possible mechanism to express these properties to achieve a continuous-valued property feedback. Depending on the robot capabilities, various estimation methods can be introduced based on different modalities such as vision, tactile, or auditory feedback. Therefore, first and foremost, the following methods serve as a possible basis to receive feedback of the targeted properties from a robotic perspective.</p>
<p>Physical Properties</p>
<p>In this work, we have selected flatness, hollowness, size, roughness, rigidity, and heaviness as physical properties given their significance reported in the literature on tool use in humans and animals (Vauclair and Anderson, 1994;Baber, 2003;Biro et al., 2013;Emery, 2013). The main inspiration behind selecting these properties was the prominent roles these properties played in various tool use scenarios in humans and animals alike, as widely reported in the literature. For instance, human infants begin exploring their abilities to use any object by studying and interacting with it to understand its weight, texture, and shape (Vauclair and Anderson, 1994). While designing and manufacturing a tool, humans and animals alike pay closer attention to the properties such as shape, size, rigidity, roughness, and heaviness (Baber, 2003, Chapter 6). It has been observed that wild animals select the tools based on the size, shape or mechanical properties such as strength, hardness (Biro et al., 2013). For example, otters have been observed carrying flat rock on their chest which they use to break the shellfish (Emery, 2013). On the other hand, researchers found that the monkeys are able to select the hardness of the stone with respect to the hardness of the nut they want to cut open (Boesch, 2013).</p>
<p>In the following, we provide a definition for each physical property and subsequently an estimation method is proposed for each property. Note that, across all estimation methods, we assume that an object is placed in its most natural position, for instance, a cup is most commonly placed in such a way that its opening points upwards. Furthermore, we aim at a bounded property value, i.e., an estimated property value that is mapped into a [0, 1] interval in order to enable a subsequent unbiased property analysis which is not affected by objectspecific characteristics or scales. Note that, as a prerequisite, each object is segmented a priori through a table-top object segmentation procedure, particularly for the size, flatness, and hollowness property. As a result, estimated property values of each object are captured through the given capabilities of the robot in form of vision-based (e.g., featuring particular image, point cloud resolution, or viewpoint) as well as manipulation-based (e.g., featuring particular joint-states, limits, or force-feedback) input. Consequently, these property values are originated from a robot-centric perspective on the perceived objects.</p>
<p>Size Property</p>
<p>Definition: Size of an object is defined intuitively by the object's spatial dimensionality in form of length, width, and height. Estimation: The size of an object is defined by the length, width, and height. As it therefore can be estimated by determining an object's bounding box, we use an RGB-D sensor to obtain point clouds of the object from a lateral perspective. Using marker detection to define a region of interest (ROI), we segment the object and transform its point cloud to an axisnormal representation, i.e., the z-axis is aligned with the object's height. Subsequently, an axis-aligned bounding box is approximated given the extracted object point cloud. The size=[length, width, height] of an object is directly derived from the object point cloud as distances between the minimal and maximal value in each spatial dimension of the bounding box. In order to retrieve a bounded property value range [0, 1] for the property size (si), each spatial dimension of size [length, width, height] is normalized by the largest dimension of the object (see Equation 1). As a result, si is defined as a three dimensional property.
si = l = length max(size) , w = width max(size) , h = height max(size)(1)
Note that, max(size) merely abbreviates max(length, width, height).</p>
<p>Flatness Property</p>
<p>Definition: As flatness describes a particular aspect of an object's shape, we define it as the ratio between the area of an object's greatest horizontal plane and its overall surface area. For instance, a sheet of paper features an upper bound of flatness whereas a ball features a lower bound of flatness. Estimation: The flatness value of an object is estimated similarly to its size: We firstly observe the object from above (Figure 4) and extract its greatest plane using RANSAC [RAndom SAmple Consensus;Fischler and Bolles, 1981]. In order to increase the confidence, a candidate plane is only selected if at least 95% of the surface normal vectors of the plane points are directed in the same direction, up to a threshold. In this manner, round surfaces (as they may be observed in balls) are rejected and subsequently a flatness value of zero is assigned to the considered object. Furthermore, if the candidate plane p is accepted, the plane size |p|, i.e., the number of object points corresponding to p, is divided by the total number of points |o| representing the observed object o in order to obtain a bounded numeric measure of its flatness fl (Equation 2). Consequently, the retrieved flatness property is bounded within a value range of [0, 1].
fl = |p| |o| (2)</p>
<p>Hollowness Property</p>
<p>Definition: Hollowness is the amount of visible cavity or empty space within an object's enclosed volume. It contrasts flatness as it focuses on a further particular aspect of an object's shape. Estimation: Hollowness contributes to the characterization of object shape. According to its definition, an object may enclose a volume which is not filled. For the sake of simplicity, we measure the internal depth d, which resembles the enclosed volume, and height h of an object o: the ratio defines the hollowness value. In order to retrieve a reasonable measure of object's depth and height, a two camera and fiducial marker (Garrido-Jurado et al., 2014) setup is introduced as illustrated in ) is introduced to consider the base height of the object, i.e., distance between the table (global reference plane) and the bottom inside the object's hollow volume.
b = d r − d h (3a) ho = h − b h(3b)
Note that, ho is inherently bounded within the interval [0, 1]. Furthermore, the proposed method may be susceptible to noise originated in the point clouds from which the bounding box was approximated to infer the object's height h. Hence, if the difference between an object's height h and distance d h (fiducial marker inside the object) is smaller than 1cm it is cumbersome to differentiate between sensor noise and the actual hollowness due to the low signal-to-noise ratio. To sanitize the property in such situations (particularly in case of flat objects), default value of zero is assigned.</p>
<p>Heaviness Property</p>
<p>Definition: Following our basic premise of using straight forward definitions, we borrow the definition of heaviness from physics: the object's heaviness is the force acting on its mass within a gravitational field. Estimation: Heaviness he of an object o can be directly derived by weighing an object with a scale (Equation 4); a scale with a resolution of 1g provides an adequate precision for our scenario. Note that, he is normalized by the carrying capabilities of the robotic arm.
he = scale(o)(4)
While it may require additional hardware, a robot may lift an object and calculate the heaviness by converting the efforts observed during the process in each of its joints.</p>
<p>Rigidity Property</p>
<p>Definition: Rigidity of an object is defined as the degree of deformation caused by an external force vertically operating on it. Estimation: Rigidity of an object is estimated using a robotic arm. The arm is equipped with a planar end-effector that is used to vertically exert a force onto an object until predefined efforts in the arm's joints are exceeded, see Figure 5; by setting the predefined efforts to the limits of the robotic arm, the final rigidity value is specific to the robot executing the estimation method. During this process we record the trajectory tr(t) of the arm as well as the efforts in all of its joints. By analyzing them using an adaptive threshold-checking, we detect the first contact of the end-effector with the object o at time t 0 . Using the final position of the arm when the efforts are exceeded at t 1 , we can calculate the deformation def of an object as the vertical movement of the end-effector, that is, its movement along the z-axis between t 0 and t 1 :
def (o) = tr z (t 0 ) − tr z (t 1 ) (5a) ri = def (o) h(5b)
In that way, the deformation def (o) is nothing but the distance the arm pushed into the considered object. For rigid objects, this deformation is zero while it is increased continuously for nonrigid objects. Finally, we normalize the deformation by the height h of the object to obtain its rigidity value ri. As we use a distance as a measure of an object's deformation, def (o) will always be positive. Furthermore, as an object may not be deformed more than its own height, the value of ri is naturally bound to the interval of [0, 1].</p>
<p>Roughness Property</p>
<p>Definition: Roughness provides information about an object's surface. Therefore, we simplify the physical idea of friction and define roughness as an object's resistance to sliding. Estimation: Roughness ro requires interaction as well to measure an object's resistance to sliding. The robotic arm is exploited to act as a ramp on which the considered object is placed, see Figure 5. Starting horizontally, with an initial angle of a i = 0 • , the ramp's angle is increased and thereby causes an increasing gravitational force pulling the object down the ramp. When the object begins sliding, a fiducial marker that is a priori placed underneath the object, is unveiled and subsequently detected. As this means that the object's sliding resistance is exceeded, the ramps' angle a r is observed and exploited as a measure of roughness as shown in Equation (6). In this setup, a 90 • ( π 2 ) ramp angle represents the upper bound that induces an object to slide. Hence, it is used to normalize roughness value ro within [0, 1].
ro = |a i − a r | π 2(6)</p>
<p>Functional Properties</p>
<p>In contrast to physical properties, functional properties describe the functional capabilities or affordances (Gibson, 1986) of objects. It is proposed that functional properties do not exist in isolation, rather certain physical properties are required to enable them (Baber, 2003, Chapter 5). In tool use, functional properties play an important role especially when perceiving an object as a possible tool since humans in general characterize an object in terms of its functional properties rather than its physicality (Gibson, 1986;Hartson, 2003). The question is how does a functional property or affordance emerge? In other terms, what are the required qualifications for an ability to be recognized as a functional property or an affordance? Various theories have been proposed to address this question (Gibson, 1986;Hartson, 2003;Osiurak et al., 2017) and among them is a theory proposed by Kuhn (2007). Kuhn (2007) suggests that image schema (such as LINK, CONTAINER, SUPPORT, and PATH) capture the necessary abstractions to model affordances. Image schema is a theory proposed in psychology and cognitive linguistics and it concerns with a recurring pattern abstracted from the perceptual and motor processes. Some of the examples of image schema are containment, support, path, and blockage. These form the basis for functional abilities to contain, support, move, and block ( Figure 3B). It is suggested by Baber (2003) that a certain assemblage of physical properties are essential prerequisites to enable a functional property and such knowledge is used by humans and animals alike in tool selection. We have exploited this notion and have designed our substitute selection approach (Thosar et al., 2020) around it. Therefore, our proposed knowledge generation approach follows the same suit, where each functional property is defined in terms of its associated enabling physical properties. In the following, we provide the definitions of the functional properties and their corresponding estimation methodology.</p>
<p>Support Property</p>
<p>Definition: Support describes an object's capability to support, i.e., to carry another object. Therefore, an object is attributed with support, if other objects can be stably placed on top of the supporting object. Consequently, the physical properties size, flatness, and rigidity are enabler of support. Estimation: Support requires to consider three aspects of an object. Firstly, the considered object needs to be rigid. Secondly, for carrying another object, the sizes of both may feature similar spatial proportions. Thirdly, the object's shape needs to be sufficiently flat in order to enable the placing of another object on top of it. Consequently, size, flatness, and rigidity are considered as core elements of the support property, Equation (7).
su = <a href="7">si, fl, ri</a></p>
<p>Containment Property</p>
<p>Definition: An object is attributed with containment if it is capable to enclose another object to a certain degree. This property is enabled by size and hollowness. Estimation: Containment property requires to consider two aspects. In order to contain something, an object needs to be hollow. On the other hand, it's size itself needs to be respected when considering whether it can contain another object. Thus, the value of the object's containment co property is formed by combining its size and hollowness property values, Equation (8).
co = <a href="8">si, ho</a></p>
<p>Movability Property</p>
<p>Definition: Movability describes the required effort to move an object. The physical properties roughness and heaviness affect the movability of an object. As a result, we may interpret that movability is affected by these physical properties. Estimation: Movability is based on a robot's primary ways of moving objects: either by lifting or pushing. In both cases, heaviness of an object is affects the movability of an object. Additionally, when pushing an object, its sliding resistance expressed in form of roughness (see Figure 5), needs to be considered as well. Therefore movability property mo constitutes of heaviness and roughness, Equation (9).
mo = <a href="9">he, ro</a></p>
<p>Blockage Property</p>
<p>Definition: Blockage describes the capability of an object of being impenetrable, i.e., the object cannot be moved by other objects, therefore it stops the movement of other encountered objects. Note that, given the set of physical properties, we can interpret that the blockage property is related to roughness and heaviness of an object as these properties affect the intensity of being capable to block another object. According to the property hierarchy ( Figure 3B), blockage is directly related to its counterpart, i.e., the movability property. Estimation: Blockage of an object can be derived from its movability. According to its definition, blockage property bl states to which degree an object is able to stop another object's movement. Thus, the object itself needs to be not movable by the other object, which is the inverse of its movability, Equation (10).
bl = −mo = <a href="10">−he, −ro</a></p>
<p>GENERATION OF ROBOT-CENTRIC CONCEPTUAL KNOWLEDGE</p>
<p>We propose an unsupervised approach to generate symbols required to represent conceptual knowledge about objects from the perception data estimated using the framework described in the previous section. In this section, we discuss the proposed bottom-up knowledge generation process to obtain robot-centric knowledge about object instances and object classes (see., Figure 2B, layer 4 and 5). For generating robot-centric conceptual knowledge, the data about the objects' physical and functional properties is processed in two stages: sub-categorization and conceptualization. In the sub-categorization process, the non-symbolic continuous data of each property is transformed into symbolic data using a clustering algorithm such as k-means. The cluster representation of the numerical values of the property data can also be seen as a symbolic qualitative measure representing each cluster. Consequently, the number of clusters describes the granularity with which each property can qualitatively be represented. In case of a high number of clusters, an object is described in finer detail. In contrast, a lower number of clusters suggest a coarse description of an object. For instance, the numerical data about the rigidity of the object instances of ceramic cup, when clustered into three clusters, can be represented as rigidity={ soft, medium, rigid}. Note, however, that through the clustering process, the symbols are usually not ordinal but rather categorical. At the end of the sub-categorization process, each object is represented in terms of the qualitative measures for each property. The conceptualization process gathers the knowledge about all the instances of an object class and represents the knowledge about an object class. Initially, the knowledge about objects is represented using bivariate joint frequency distribution of the qualitative measures of the properties in the object instances. Next, conceptual knowledge about objects is calculated as a sample proportion of the frequency of the properties across the instances of a class. In the following, we have provided the formal description of the knowledge generation process described above.</p>
<p>Consider O as a given set of object classes where (by abuse of notation) each object class is identified with its label. Let each object class O ∈ O be a given set of its instances. Let O be a union of all object classes. Let P and F be the given sets of physical properties' labels and a set of functional properties' labels respectively. By abuse of notation, each physical and functional property is identified with its label. For each physical property P ∈ P as well as for a functional property F ∈ F, sensory data is acquired from each object instance o ∈ O. Let ϒ P and ϒ F represent functions which maps each object instance to its measured sensory value of a physical property P and a functional property F respectively. Let P n and F n represent sets such that P n and F n are the images of ϒ P and ϒ F , respectively.</p>
<p>Sub-categorization -From Continuous to Discrete</p>
<p>The sub-categorization process is performed to form (more intuitive) qualitative measures to represent the degree with which a property is reflected by an object instance. It is the first step in creating symbolic knowledge about object classes where the symbols representing the qualitative measures of a physical or a functional property reflected in an object instance are generated unsupervisedly by a clustering mechanism. A qualitative measure of a physical property is referred to as a physical quality and that of a functional property as a functional quality.</p>
<p>In this process, P n and F n representing measurements of a physical property P ∈ P and a functional property F ∈ F, respectively extracted from n number of object instances is categorized into a given number of discrete clusters η using a clustering algorithm. Let ∇ P and ∇ F be partitions of the sets P n and F n after performing clustering on them. Let P η and F η be the sets of labels, expressing physical qualities and functional qualities, generated for a physical property P ∈ P and a functional property F ∈ F respectively. Given the label for a property, the quality labels are generated by combining a property label P and a cluster label (created by the clustering algorithm). For example, in size = {small, medium, big, bigger}, size is a physical property and small, medium, big, bigger are its physical qualities. Note that, these given physical quality labels are only provided for illustration purposes of the property qualitative measures; however, the quality labels for a property size are internally represented as {size_1, size_2, size_3, size_4}. At the end of the sub-categorization process, the clusters are mapped to the generated symbolic labels for qualitative measures. Note that the number of clusters essentially describes the granularity with which each property can qualitatively be represented. A higher number of clusters suggest that an object is described in a finer detail, which may obstruct the selection of a substitute since it may not be possible to find a substitute which is similar to a missing tool down to the finer details.</p>
<p>Attribution -Object Instance Knowledge</p>
<p>The attribution process generates knowledge about each object instance by aggregating all the physical and functional qualities assigned to the object instance by the sub-categorization step. In other terms, the knowledge about an instance consists of the physical as well as functional qualities reflected in the instance. Let P η and F η be the families (sets) of sets containing the physical quality labels P η and the functional quality labels F η for each physical property P ∈ P and functional property F ∈ F, respectively. Thus, each object instance o ∈ O is represented as a set of all the physical as well as functional qualities attributed to it which are expressed by a symbol holds as: holds ⊂ O × (P η ∪ F η ). For example, knowledge about the instance plate 1 of a plate class can be given as, holds(plate 1 , medium), holds(plate 1 , harder), holds(plate 1 , can_support) where medium is a physical quality of size property, harder is a physical quality of rigidity property and can_support is a functional quality of support property.</p>
<p>Conceptualization -Knowledge About Objects</p>
<p>The conceptualization process aggregates the knowledge about all the instances of an object class. The aggregated knowledge is regarded as conceptual knowledge about an object class. Let O KB be a knowledge base about object classes where each object class O ∈ O. Given the knowledge about all the instances of an object class O, in the conceptualization process, the knowledge about the object class O K ∈ O KB is expressed as a set of tuples consisting of a physical or a functional quality and its proportion (membership) value in the object class. A tuple is expressed as O, t, m where t ∈ P η or t ∈ F η and a proportion value m is calculated using the following membership function expressed by a conditional probability: m = P(holds(o, t)|o ∈ O). The proportion value allows to model the intra-class variations in the objects. For example, knowledge about object class table { plate, harder, 0.6 , plate, light_weight, 0.75 , plate, less_hollow, 0.67 , plate, hollow, 0.33 , plate, more_support, 0.71 }, where the numbers indicate that, for instance, physical quality harder was observed in 60% instances of object class plate. At the end of the conceptualization process, conceptual knowledge about an object class is created which is represented in a symbolic fuzzy form and grounded into the human-generated or machine-generated data about the properties of objects. The knowledge about objects is then used to determine a substitute from the existing objects in the environment. The Figure 6 illustrates graphically the main processes of Sub-categorization and Conceptualization.</p>
<p>EXPERIMENTAL EVALUATION</p>
<p>In the following evaluation, multiple experiments are conducted to evaluate the proposed approach on different semantic levels: From the property estimation of real world objects to an eventual application scenario in the context of tool substitution.</p>
<p>For this purpose, we introduce the RoCS dataset containing estimated physical and functional properties of objects in section 4.1. With the dataset, we conduct an evaluation on the physical object properties investigating the stability of the estimation methods, the coverage w.r.t. the range of characteristics captured by selected dataset, and the correlation among properties in section 4.2. Using k-means clustering (Lloyd, 1982) on functional object properties in sections 4.3 and 3, we show that the chosen properties may allow to discriminate instances of different object classes and identify the inter-class similarities. Finally, section 4.4 shows the applicability of the dataset by learning a model from the generated conceptual object knowledge given the estimated properties and applying it to a tool substitution scenario under real world conditions.</p>
<p>RoCS Dataset</p>
<p>For the sake of a thorough evaluation of our conceptual framework the Robot-Centric dataSet (RoCS) is introduced. Note that we propose a Robot Operating System (ROS) (Koubâa, 2017) based implementation to acquire object data used in the following evaluation. In the following, we briefly introduce the hardware setup and procedures for acquiring raw object data, describe its parameters (e.g., thresholds) and the contents of the final dataset. Figure 3 illustrates the required sensors as data sources. For visual and non-invasive estimation methods, RGB-D sensors are required. More specifically, the size property requires a lateral view on objects while the hollowness property relies on a birds-eye view. Hence, we employ two Asus Xtion Pro depth sensors (Swoboda, 2014) (see Figure 4). To estimate the physical properties rigidity and roughness, a robotic arm is required to interact with objects. In this interaction the proposed property estimation methods require arm joint state values which are generally provided by manipulators, such the one we use, a Kuka youBot (Bischoff et al., 2011) manipulator. Finally, a common kitchen scale with a resolution of 1g is used to estimate the weight and heaviness of objects.</p>
<p>Hardware Setup</p>
<p>Object Property Acquisition Procedure</p>
<p>Using the described hardware, we implemented a ROS-based framework to estimate the physical and functional properties of objects. A schematic overview on the framework is given by Figure 7.</p>
<p>The interface for operating sensors and actuators is provided to our framework by ROS. This interface is used by different experiments for observing and interacting with objects to acquire the necessary sensory data. Together, both blocks (ROS Abstracted Sensors &amp; Actuators and Experiment Control) form a control loop enabling to generate feature data (see Figure 3B). According to the selected properties four control loops are implemented as separate experiments. The first experiment is non-invasive and gathers the visual feature data required for hollowness, flatness, and size; Figure 4 illustrates the camera setup. Initially a table-top object detection is introduced that uses a RAndom SAmple Consensus (RANSAC) based plane fitting approach in order to detect object candidates on the table. The RANSAC algorithm is parameterized with a leaf size of 0.0025 m, a maximum of 10 4 iterations and a 0.02 m distance threshold between points and the estimate plane model. Note that, RANSAC is also used in this experiment for segmenting planes for the property flatness. Furthermore, fiducial markers (ArUco Library; Garrido-Jurado et al., 2014) with sizes of 14 and 3 cm are used for the hollowness property. The second experiment uses the robotic arm to deform objects to facilitate the estimation of rigidity (see section 2.2.5). We set the efforts to exceed in each joint to ±8 Nm. Within the third experiment, the robotic arm is used as a ramp to estimate an object's roughness (see section 2.2.6). To achieve an appropriate resolution, the angular speed of the joint lifting the ramp is set to 0.05 rad/s. Finally, the last experiment employs a kitchen scale with a resolution of 1g to estimate the objects' weight. Following the Experiment Control, the individual estimation methods process the generated feature data as described in section 2.1 to produce physical and functional property values of the considered object. Finally, this data can be accumulated for a set of objects and further processed to generate conceptual knowledge.</p>
<p>Dataset Structure</p>
<p>For the RoCS dataset we consider 11 different object classes (ball, book, bowl, cup, metal_box, paper_box, plastic_box, plate, sponge, to_go_cup, and tray) featuring various object characteristicsfrom appearance to functional purpose. Each class consists of FIGURE 6 | The robot-centric conceptual knowledge generation process is illustrated where acquired continuous property data of objects {o 1 , o 2 ...} is sub-categorized into multiple clusters. Using Bi-variate joint frequency distribution and sample proportions conceptual knowledge about object classes (e.g., plasti_box) is generated.  10 unique object instances that leads to a total number of 110 object instances; Figure 8 illustrates sample object instance of RoCS dataset.</p>
<p>In order to evaluate the performance of the proposed property estimation methods, such as stability, for each object instance we capture 10 repetitions without modifying the setup. As a result we captured 1,100 object observations for which physical and functional property values are generated. The dataset is publicly available at https://gitlab.com/rocs_data/rocs-dataset.</p>
<p>Property Estimation</p>
<p>The objective of the first part of the evaluation is to investigate the property estimation methods as described in section 2.1. At this level, we only focus on physical properties as functional properties are built on the basis of an object's physical properties. First, we analyze the stability of the estimation methods to determine how deterministic and reproducible the data acquisition is for each property and object. Furthermore, we explore the coverage of our data set to determine the variance and range of objects reflected in the different classes and properties. Lastly, we inspect the correlation among different properties in our data.</p>
<p>Estimation Stability</p>
<p>The abstraction process from raw sensor data to symbolic object property knowledge requires a stable processing. However, noise is naturally affecting data when working with sensors and real world objects.</p>
<p>To compensate for the caused uncertainty, each RoCS object instance consists of 10 repetitions. We use these in the following to analyze the stability of the proposed property estimation methods. For that, the variance of each physical property of each object instance is analyzed. More specifically, given the 10 repetitions of a particular object instance for each of its physical properties, we calculate the variance of the property values of its 10 repetitions. As the measurements of 6 physical properties are based on 8 features, we obtain 8 values per object instance and therefore 880 values in total. We further reduce the data, by calculating the mean of the object variances for a particular object class and property as shown in Table 2, whereas Figure 9 illustrates the variances of all object instances within one object class as box plots; the colored middle box represents 50% of the data points and the median of the class is indicated by the line that divides the box.</p>
<p>The results of the (Table 2; Figure 9) reveal that the class variances are overall low, which implies stable property estimation methods in general. The highest variances can be found for the flatness property. The estimation of the flatness property for small and flat object instances is particularly affected by noise due to the low signal-to-noise ratio. Furthermore, it can be observed that for ball, bowl, and to_go_cup the variance of the flatness property is zero due to the fact that no top-level plane can be extracted for instances of these classes as they feature either round or negligible small top-level surfaces (see section 2.2.2). Similarly, a higher variance can be observed for the rigidity property which is caused by smaller object instances, such as book, plate, sponge, and tray. Here the detection of the first contact with the object causes false positives and therefore introduces varying deformation values.</p>
<p>In contrast, for the hollowness property the variance for metal_box and sponge are zero. Such object instances predominantly feature flat surfaces and negligible degree of hollowness. Considering sensor quantization effects, such negligible degree for hollowness cannot be confidently distinguished from sensor noise under such conditions (see section 2.2.3). As a consequence a default hollowness value of zero is set for instances that fall in a negligible range of hollowness, i.e., below 1 cm distance between marker. Concerning the heaviness property, a zero variance is observed due to the accurate measurement by a scale-considering a resolution of 1 g which is a sufficient resolution for our scenario.</p>
<p>Property Coverage of RoCS</p>
<p>The objective of this experiment is to evaluate the intra-class variance for each property in order to determine the range of data covered in each object class for one particular property. For this experiment, the mean estimated property value over the 10 repetitions is used. The result for each of the physical properties is shown in Figure 10 in form of a box plot in which all object instances of a particular class are considered.</p>
<p>Several observations can be made. For instance, hollowness and flatness are complementary in our dataset. Objects with flatness values close to zero are commonly exhibiting increased hollowness values (above 0.5) and vice versa. Only balls form an exception as they are neither flat nor hollow. While this means that we cover a wide range of values for the flatness property, we miss such coverage for hollowness values in the interval [0, 0.5]. Moreover, for roughness most object classes are in a similar range -except sponge and ball instances. As we place the objects in their most natural position we can conclude that the sponges' ground surfaces have a higher roughness due to their open-pored surfaces. Due to their roundish surfaces, ball instances feature obviously a low roughness value. Furthermore, it is unlikely to observe objects featuring roughness values close to one as none of the considered object classes has the ability to stick to the ramp.</p>
<p>For the rigidity values an interval of [0, 0.9] is covered, ranging from rigid objects such as metal_box to non-rigid objects such as sponge. Suspiciously, only a limited number of objects has a value of zero which indicates that sensor noise has its greatest effect on these objects.</p>
<p>Analyzing the size values, it becomes apparent that width commonly is the greatest dimension among the considered objects while the objects' height varies along the range of possible values.</p>
<p>Property Correlation</p>
<p>In this experiment, we investigate the correlation in the physical properties of our data. Given estimated values of a particular property, we compute the mean property value o x (Equation 11a) over the 10 repetitions for each object instance o. Based on these mean variances, the pearson correlation ρ XY is obtained between two sets of mean variances X and Y corresponding to respective properties, see Equation (11b), where cov is the covariance and σ x the standard deviation of X, respectively. Table 3 shows the pearson correlation among all physical properties with a color scale. It can be observed that the correlation of our data is low in general. However, a strong negative correlation between flatness and hollowness is found which may indicate that in our data objects with high flatness are likely to have low hollowness. This matches our observation in section 4.2.2, where we noted the complementary nature of these properties in our dataset. The object instances of our dataset may also show some negative correlation between size-height and flatness as well as size-height and rigidity.
X = {o x 1 , o x 2 , o x 3 , ...} (11a) ρ XY = cov(X, Y) σ x σ y (11b)</p>
<p>Property Semantics</p>
<p>Given a stable property estimation (section 4.2) from noisy real world data, the following experiment focuses on the semantic interpretation of the estimated object property values. We propose an experiment that groups object instances of our RoCS dataset in an unsupervised manner by considering a particular property or a set of properties. In order to conduct a preferably unbiased (machine-driven) grouping, kmeans clustering is applied with a gradually increasing value Each value represents the mean variance of estimated property values of an particular object class consisting of 10 instances and their respective repetitions. Variances are scaled by color in ascending order from transparent (0) to red (highest variance).</p>
<p>FIGURE 9 | Mean variance for physical properties [fl, ri, ro, si, he, ho] illustrated in form of a Box plot (in log-scale to provide insights of respective intra property variances compared to linear-scale shown in Table 2). Note that, in order to be able to display all variances (including zero) in log-scale, we add an epsilon on each value before computing log. Heaviness is excluded as all variance values are zero for this property due to the resolution of the scale.</p>
<p>of k={2, ..., 11}. Here, 11 is selected as upper bound as it represents the number of object classes considered in the RoCS dataset.</p>
<p>Figure 11</p>
<p>consists of pyramid charts that shows the gradual partitioning process for the respective property. A group or a cluster is depicted as a pie-chart illustrating the distribution FIGURE 10 | Category-wise coverage for each physical property [fl, ri, ro, si, he, ho]. of assigned object instances with their labeled class. Therefore, each row of the pyramid-like structure shows the results of one application of the k-means clustering. The number of pie-charts in each row equals to number of clusters (k value). Furthermore, since each group partitions the property space, assigned instances within the group share similar attributes. Therefore, a group can be interpreted as a concept representing a qualitative measurement of the respective property Generally on higher levels in the pyramids (lower k), the distribution of the instances and the classes in each concept is higher compared to lower levels (higher k). As a consequence, concepts in the higher levels appear more generic as opposed to the lower levels where concepts appear more specific.</p>
<p>Moreover, a pattern in the distribution of classes can be observed which is carried forward in the subsequent levels.</p>
<p>FIGURE 11 | Gradual partitioning of instances to particular concepts given a particular set of properties describing each instance. Each concept is illustrated as a pie chart showing the object class label distribution of instances assigned to the respective concept. Sample concepts are annotated ( ) which illustrate object classes featuring similar quality regarding the property, such as plate, bowl, cup, to_go_cup regarding the containment property. One may observe, that given the assigned classes, object instances are not separable by the generated concepts. This is intended as it enables a quantitative analysis of similarity among instances and across classes based on the property. This pattern hints toward semantic relations between class labels and observed concepts. For example, instances of plate, bowl, cup, to_go_cup share similar concepts regarding the containment property (see concept annotated with in Figure 11A) which is also reflected over multiple levels. Such patterns can also be observed and tracked over multiple levels for other functional properties in Figure 11. Furthermore, Figure 11E illustrates the gradual grouping process considering all physical properties of the object instances. Also here such patterns can be observed, e.g., on the right side where concepts have emerged that feature common properties related to instances such as plastic_box, metal_box, paper_box (see concept annotated with in Figure 11E).</p>
<p>As a result, the proposed property hierarchy (refer Figures 2B, 3B) allows to discriminate the object instances by associating them to meaningful groups featuring similar object concepts. In the figure, property generality can be observed across object classes, i.e., concepts on different granularity levels may feature dedications to instances of different object classes as they feature similar characteristics or trends regarding the property. This interrelation of object classes is reflected by the heterogeneity of the distribution of instances within a concept-even in case of k=11 when considering 11 object classes. These observations made in the proposed property acquisition procedure (Figure 7) provides a basis for the generation of conceptual knowledge about objects as shown in section 4.4.</p>
<p>Conceptual Knowledge for Substitute Selection</p>
<p>In this experiment, we demonstrate how the robot-centric conceptual knowledge grounded in the robot's sensory data can be successfully used to determine a substitute in a tool substitution scenario. While operating in a dynamic environment, a robot can not assume that a particular tool required in a task will always be available. In such scenarios, an ideal solution for a robot would be to improvise by finding a substitute for the missing tool as humans do.</p>
<p>To deal with such situation, we have developed an approach, called as ERSATZ (German word for a substitute) detailed in Thosar et al. (2018a), which is inspired by the way in which humans select a substitute in a non-invasive manner. In this approach, the robot-centric conceptual knowledge about objects is used to select a plausible substitute for a missing tool from the available objects. A tool, in this work, is defined as an artifact that is designed, manufactured, and maneuvered in accordance with its designated purpose in the task, such as hammer for hammering, tray for carrying, etc., and a substitute is seen as an alternate to a missing tool.</p>
<p>For the experiments, we generated knowledge about 11 object classes using the approached discussed in the section 3. The dataset generated by RoCS was utilized for creating robot-centric conceptual knowledge about 11 object classes. The Figures 12A,B illustrates graphically the qualitative knowledge about physical and functional properties of 11 object classes as a heat map. The heat cells in the map represents the sample proportion of each qualitative measure in each object class. For instance, a single cell can be read as "the qualitative measure Flatness 0 of the property class Flatness is observed in 60% instances of class Plastic box". Accordingly, in the figure, the conceptual knowledge suggests that all the instances of Plastic Box, Paper Box, Metal Box, Tray, Sponge, Ball, and Book are qualitatively similar with respect to the physical property Hollowness (i.e., none of them are hollow). The similar observation can be made for Tray, Sponge and Book with respect to the functional property Support (i.e., all of them may be stacked). Note that the indices assigned to each qualitative measure are not ordinal, but they are categorical. For instance, index 2 does not mean it is more valued than index 1. While comparative relationships exist in all four qualitative measurements of any property, they are not indicated by the given indices.</p>
<p>For the tool-substitution experiment, we generated 11 queries based on the 11 object classes, where each query consisted of a missing tool description and 5 randomly selected objects as available choices for a substitute. The queries were given to 21 human experts who were asked to select a substitute in each query. The expert selections were aggregated and selection proportion was calculated for each expert-selected substitute. ERSATZ used the knowledge generated in the previous section and computed substitute/s for each given query using the approach discussed in Thosar et al. (2020).</p>
<p>In order to validate a substitutability, the number of selected substitutes by human experts was then compared with the number of selected substitutes by ERSATZ. Similarly, the substitute selection by the 21 experts and ERSATZ in 11 scenarios are plotted as a heat map in Figure 12C. The grayed cells in the plots mean the corresponding object categories were not included in the available objects in the respective query. We used the conventional classification evaluation metrics: True Positives, True Negatives, False Positives, and False Negatives to evaluate the performance of ERSATZ. Our results showed that ERSATZ selected true positives in all 11 scenarios while true negatives in 8 scenarios. The results indicate that the proposed conceptual knowledge based substitute selection is meaningful and valid as confirmed by human selected substitutes. The experiment also demonstrates the successful application of the proposed conceptual knowledge, generated from the property data estimated from the proposed framework, in the tool substitution scenario.</p>
<p>Robot-Centric Conceptual Knowledge vs WordNet</p>
<p>In this experiment, we have pitted our conceptual knowledge against WordNet in the substitute selection scenario. Our objective is to demonstrate that common sense knowledge bases such as WordNet are not adequate for substitute selection without selecting suitable knowledge a priori as done in Boteanu et al. (2016).</p>
<p>In this experiment, we compare the similarity among different objects determined by the similarity measures used in WordNet (Pedersen and Michelizzi, 2004) and Jaccard Indexbased similarity proposed in Thosar et al. (2020). For the experiment, the path-length based measures Path Similarity and Wu-Palmer Similarity while information content based measures Lin Similarity, Jiang-Conrath Similarity were considered. We used the object labels from the multi-modal perception dataset, however some labels were adapted while using WordNet to compute the similarity. For instance, in the multi-modal data set we have a to-go-cup and a cup, however in WordNet there is no to-go-cup. Similarly, as WordNet does not differentiate between a plastic, a metal and a cardboard box, we considered only box for the WordNet comparison.</p>
<p>In Figures 12E-H, the resulting heat plot of the similarity between different objects using above mentioned similarity measures in WordNet is symmetric in nature while the relevant property driven Jaccard's Index based heat plot is asymmetric (see Figure 12D). This discrepancy is caused by the way objects are treated by WordNet and ERSATZ. While ERSATZ distinguishes between a tool and a substitute, WordNet does not make such a distinction. Therefore when the similarities between, say objects A and B, and between B and A are computed, the contents (path-length or information content) considered during the computation remain unchanged. Within the context of a specific designated purpose, the substitutability relationship between a tool and a substitute is symmetric, for instance, for hammering, a hammer can be replaced by a heeled shoe and vice versa. However, it is not the case once the context is shifted, for instance, a hammer can not be used as a heeled shoe for walking. Such an asymmetric relation is a necessity in tool substitution since it can not be assumed that if A is a substitute of a tool B, then B is a substitute of a tool A. Such assumption due to the symmetric relation may lead to an inadequate selection of a substitute as seen in the figure.</p>
<p>What the experiment shows us is that in order to use large knowledge bases such as WordNet or ConceptNet in substitute selection, simply applying similarity measures is not enough. Additionally these knowledge bases do not contain the exclusive information about physical and functional properties about objects. In contrast, our proposed knowledge generated from the quantitative measurements of physical and functional properties is desirable in substitute selection process as demonstrated in the above two experiments.</p>
<p>CONCLUSION</p>
<p>Retrieving conceptual knowledge about objects in the environment, in form of physical and functional properties, fundamentally contributes to an awareness about affordances provided by the environment to a robot. Such conceptual object knowledge is desired in various robotic scenarios (from household to industrial robotics) in order to efficiently perform tasks when dealing with objects in dynamic and uncertain environments. In scenarios where it is uncertain that a required tool is present, an efficient substitute selection is particularly required to successfully accomplish tasks. Besides substitute selection, conceptual object knowledge facilitates inferences about circumstances in situations in which the robot is applied to, e.g., if object rigidity is expected to be low, manipulation, including grasping strategies of the object, can be accordingly adapted to increase a successful object handling. However, state-of-the-art conceptual knowledge approaches are generally hand-crafted and generated from a human perspective in form of natural language concepts and may not be suitable for substitute selection as demonstrated in the experiment in section 4.4.1. Consequently, the discrepancy between human and robotic capabilities (e.g., visual, auditory, haptic perception, prior knowledge, etc.) is also reflected in the knowledge generation process conducted by humans and the then necessarily complex interpretation for it by robotic systems. In order to mitigate this discrepancy, we proposed a robot-centric approach as we believe that conceptual knowledge has to be generated considering the individual robotic capabilities, so to say in form of robotic language concepts.</p>
<p>A multi-modal approach for object property estimation and generation of robot-centric knowledge has been proposed to acquire conceptual knowledge from a robotic perspective. We introduced a bottom-up knowledge acquisition process, from capturing sensory data over a numeric estimation of object properties, to a symbolic conceptualization of objects' properties. Experiments have revealed the stability as well as the interclass generality of the proposed object property acquisition procedure. This outcome provides a basis for the subsequent conceptual knowledge generation in the context of the substitute selection scenario. Tool substitution results have demonstrated the applicability of the generated conceptual knowledge.</p>
<p>We conclude, that the proposed robot-centric and multimodal conceptualization approach may contribute to equip a robot with the capability to reason about objects on a conceptual level compared to general approaches which are only based on e.g., visual (image pixels) or haptic (resistance feedback) sensory data. Moreover, such robot-centric and multi-modal knowledge can be applied to a variety of scenarios beyond substitute selection. To facilitate further use, we established the RoCS dataset and made it publicly available.</p>
<p>As the goal of this work was a robot-centric conceptual knowledge generation, our future work is directed, toward the transfer of such knowledge among heterogeneous robotic systems. To facilitate the development and collaborative progress in this framework, we aim to develop an online system where developers can plug-in their estimation methods (simple or more complex) for the same property or new property to the framework requiring minimal or more sophisticated experimental set-ups. This way, we wish to create a community of users who can select the estimation methods based on the sensor and robot availability at their end.</p>
<p>DATA AVAILABILITY STATEMENT</p>
<p>The original contributions presented in the study are publicly available. This data can be found here: https://gitlab.com/rocs_ data/rocs-dataset.</p>
<p>AUTHOR CONTRIBUTIONS</p>
<p>MT primarily contributed to the conception and preliminary design of the robot-centric knowledge acquisition framework.</p>
<p>MT, CM, and GJ contributed to the property selections and definitions of RoCS framework. CM and GJ contributed to the implementation of the framework and extraction methods. NP, RM, and SJ contributed to the dataset generation of 110 objects. CM, GJ, and JS contributed in the integration and the evaluation of the dataset. MT, CM, GJ, and MP contributed to evaluation of the property semantics. MT contributed to the conceptual knowledge generation and application of the conceptual knowledge in tool substitution scenario. AB, MP, and SZ contributed to the critical evaluation of the work. All authors contributed to manuscript revision, read, and approved the submitted version.</p>
<p>FIGURE 2 |
2(A) represents our symbol grounding approach while figure (B) illustrates the process layers for our bottom-up robot-centric knowledge generation.</p>
<p>FIGURE 3 |
3(A) RoCS property extraction framework for extracting sensory data related to various properties and generating robot-centric conceptual knowledge about objects; (B) Proposed property hierarchy and their dependencies (arrow colors chosen to visually distinguish dependencies).</p>
<p>FIGURE 4 |
4Light-weight experimental setup consisting of two cameras and fiducial markers(Garrido-Jurado et al., 2014), for acquiring physical properties.</p>
<p>Figure 4 .
4Given the side camera view, the height h of an object can be obtained by estimating the respective bounding box (see section 2.2.1). In order to retrieve depth, two fiducial markers {m r , m h } are introduced (see samples inFigure 4): m r serves as global reference and is placed next to the object; m h is placed inside the hollow volume of the object. Exploiting the top camera height c t perpendicularly pointed to the object, the distances d r = m r − c t and d h = m h − c t can be obtained. Given object height h and the distances d r and d h , hollowness ho can be approximated as shown in Equation (3b), where b (Equation 3a</p>
<p>FIGURE 5 |
5Light-weight experimental setup consisting of a camera-manipulator combination, for acquiring physical property rigidity (top row) and roughness (bottom row).</p>
<p>FIGURE 7 |
7Data flow within the dataset creation framework.</p>
<p>FIGURE 8 |
8RoCS dataset samples: Point cloud and RGB images of a ball, bowl, paper box, and cup (for visualization purposes, images are scaled and 3D points are magnified).</p>
<p>FIGURE 12 |
12(A) Qualitative knowledge about physical properties; (B) Qualitative knowledge about functional properties;(C) The distribution of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) in each substitution scenario; (D) Similarity between a tool and a substitute using Jaccard's Index; (E) Wu-Palmer similarity; (F) Lin Similarity; (G) Path similarity; (H) Jiang-Conrath Similarity.</p>
<p>TABLE 2 |
2Mean variance for each physical property.Class 
Flatness 
Rigidity 
Roughness 
Size_length 
Size_width 
Size_height 
Heaviness 
Hollowness 
Class_mean </p>
<p>Ball 
0 
0.00053 
0.00032 
0.00538 
0.00001 
0.00083 
0 
0.00023 
0.00091 </p>
<p>Book 
0.02554 
0.00583 
0.00015 
0.00001 
0.00001 
0.00002 
0 
0.002 
0.00419 </p>
<p>Bowl 
0 
0.00037 
0.00025 
0.00038 
0.00006 
0.00012 
0 
0.00003 
0.00015 </p>
<p>Cup 
0.00026 
0.00015 
0.00017 
0.00098 
0.0003 
0.00079 
0 
0.00001 
0.00033 </p>
<p>Metal_box 
0.01939 
0.00074 
0.0039 
0.00028 
0.00002 
0.00007 
0 
0 
0.00305 </p>
<p>Paper_box 
0.00747 
0.00115 
0.00021 
0.00011 
0.00002 
0.00017 
0 
0.0035 
0.00158 </p>
<p>Plastic_box 
0.00015 
0.00071 
0.00016 
0.00056 
0.00021 
0.0003 
0 
0.00013 
0.00028 </p>
<p>Plate 
0.00971 
0.00481 
0.00022 
0.0003 
0.00003 
0.00017 
0 
0.0005 
0.00197 </p>
<p>Sponge 
0.02503 
0.00705 
0.00313 
0.0001 
0.00001 
0.00008 
0 
0 
0.00443 </p>
<p>to_go_cup 
0 
0.00016 
0.00031 
0.00061 
0.00044 
0.00013 
0 
0.00001 
0.00021 </p>
<p>Tray 
0.03486 
0.00569 
0.00024 
0.00005 
0.00001 
0.00004 
0 
0.00206 
0.00537 </p>
<p>prop_mean 
0.01113 
0.00247 
0.00082 
0.0008 
0.0001 
0.00025 
0 
0.00077 
0.00204 </p>
<p>TABLE 3 |
3Pearson Correlation on the mean values of physical properties.Flatness 
Rigidity 
Roughness 
s_length 
s_width 
s_height 
Heaviness </p>
<h2>Flatness</h2>
<p>Rigidity 
0.45 
-</p>
<p>Roughness 
0.45 
0.35 
-</p>
<p>size_length 
0.03 
0.12 
0.15 
-</p>
<p>size_width 
0.16 
0.34 
0.02 
0.21 
-</p>
<p>size_height 
-0.65 
-0.59 
-0.38 
-0.26 
-0.45 
-</p>
<p>Heaviness 
0.09 
-0.04 
-0.13 
0.19 
0.02 
-0.37 
-</p>
<p>Hollowness 
-0.71 
-0.36 
-0.08 
0.24 
-0.1 
0.24 
0.13 </p>
<p>April 2021 | Volume 8 | Article 476084
Frontiers in Robotics and AI | www.frontiersin.org
ACKNOWLEDGMENTSWe would like to thank our colleagues and students Florian Sommer, David Döring, and Saagar Gaikwad from Otto-von-Guericke University Magdeburg, Germany for providing assistance in the dataset evaluation and visualization.
Learning how a tool affords by simulating 3D models from the web. P Abelha, F Guerin, IEEE International Conference on Intelligent Robots and Systems. VancouverAbelha, P., and Guerin, F. (2017). "Learning how a tool affords by simulating 3D models from the web, " in IEEE International Conference on Intelligent Robots and Systems (Vancouver), 4923-4929.</p>
<p>A model-based approach to finding substitute tools in 3D vision data. P Abelha, F Guerin, M Schoeler, Proceedings -IEEE International Conference on Robotics and Automation. -IEEE International Conference on Robotics and AutomationStockholmAbelha, P., Guerin, F., and Schoeler, M. (2016). "A model-based approach to finding substitute tools in 3D vision data, " in Proceedings -IEEE International Conference on Robotics and Automation (Stockholm).</p>
<p>Using structural bootstrapping for object substitution in robotic executions of human-like manipulation tasks. A Agostini, M J Aein, S Szedmak, E E Aksoy, J Piater, F Worgotter, IEEE International Conference on Intelligent Robots and Systems. HamburgAgostini, A., Aein, M. J., Szedmak, S., Aksoy, E. E., Piater, J., and Worgotter, F. (2015). "Using structural bootstrapping for object substitution in robotic executions of human-like manipulation tasks, " in IEEE International Conference on Intelligent Robots and Systems (Hamburg), 6479-6486.</p>
<p>Challenges in finding ways to get the job done. I Awaad, G K Kraetzschmar, J Hertzberg, Planning and Robotics (PlanRob) Workshop at 24th International Conference on Automated Planning and Scheduling. PortsmouthAwaad, I., Kraetzschmar, G. K., and Hertzberg, J. (2014). "Challenges in finding ways to get the job done, " in Planning and Robotics (PlanRob) Workshop at 24th International Conference on Automated Planning and Scheduling (Portsmouth).</p>
<p>Cognition and Tool Use London. C Baber, Taylor and FrancisBaber, C. (2003). Cognition and Tool Use London: Taylor and Francis.</p>
<p>TOOLNET: using commonsense generalization for predicting tool use for robot plan synthesis. R Bansal, S Tuli, R Paul, Mausam , arXiv:2006.05478arXiv preprintBansal, R., Tuli, S., Paul, R., and Mausam (2020). TOOLNET: using commonsense generalization for predicting tool use for robot plan synthesis. arXiv preprint arXiv:2006.05478.</p>
<p>Tool use as adaptation. D Biro, M Haslam, C Rutz, 10.1098/rstb.2012.0408Philos. Trans. R. Soc. Lond. B. Biol. Sci. 368Biro, D., Haslam, M., and Rutz, C. (2013). Tool use as adaptation. Philos. Trans. R. Soc. Lond. B. Biol. Sci. 368:20120408. doi: 10.1098/rstb.2012.0408</p>
<p>Kuka youbot -a mobile manipulator for research and education. R Bischoff, U Huggenberger, E Prassler, 2011 IEEE International Conference on Robotics and Automation. ShanghaiBischoff, R., Huggenberger, U., and Prassler, E. (2011). "Kuka youbot -a mobile manipulator for research and education, " in 2011 IEEE International Conference on Robotics and Automation (Shanghai), 1-4.</p>
<p>Chap. 2: Ecology and cognition of tool use in chimpanzees. C Boesch, Tool Use in Animals: Cognition and Ecology. J. B. C. Sanz and C. M. CallCambridgeCambridge University PressBoesch, C. (2013). "Chap. 2: Ecology and cognition of tool use in chimpanzees, " in Tool Use in Animals: Cognition and Ecology, eds J. B. C. Sanz and C. M. Call (Cambridge: Cambridge University Press), 21-47.</p>
<p>Leveraging large-scale semantic networks for adaptive robot task learning and execution. A Boteanu, St, A Clair, A Mohseni-Kabir, C Saldanha, S Chernova, 10.1089/big.2016.0038Big Data. 4Boteanu, A., St. Clair, A., Mohseni-Kabir, A., Saldanha, C., and Chernova, S. (2016). Leveraging large-scale semantic networks for adaptive robot task learning and execution. Big Data 4, 217-235. doi: 10.1089/big.2016.0038</p>
<p>A relational approach to tool-use learning in robots. S Brown, C Sammut, Inductive Logic Programming -22nd International Conference. DubrovnikBrown, S., and Sammut, C. (2012). "A relational approach to tool-use learning in robots, " Inductive Logic Programming -22nd International Conference (Dubrovnik), 1-15.</p>
<p>A relational approach to tool-use learning in robots. S Brown, C Sammut, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) Berlin: Springer 7842 LNAI. Brown, S. and Sammut, C. (2013). "A relational approach to tool-use learning in robots, " Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) Berlin: Springer 7842 LNAI, 1-15.</p>
<p>An introduction to the anchoring problem. S Coradeschi, A Saffiotti, 10.1016/S0921-8890(03)00021-6Robot. Auton. Syst. 43Coradeschi, S., and Saffiotti, A. (2003). An introduction to the anchoring problem. Robot. Auton. Syst. 43, 85-96. doi: 10.1016/S0921-8890(03)00021-6</p>
<p>Grounding commonsense knowledge in intelligent systems. M Daoutis, S Coradeshi, A Loutfi, 10.3233/AIS-2009-0040J. Ambient Intell. Smart Environ. 1Daoutis, M., Coradeshi, S., and Loutfi, A. (2009). Grounding commonsense knowledge in intelligent systems. J. Ambient Intell. Smart Environ. 1, 311-321. doi: 10.3233/AIS-2009-0040</p>
<p>Chap. 4: Insight, imagination and invention: tool understanding in a non-tool-using corvid. N J Emery, Tool Use in Animals: Cognition and Ecology. C. M. Sanz, J. Call, and C. BoeschCambridgeCambridge University PressEmery, N. J. (2013). "Chap. 4: Insight, imagination and invention: tool understanding in a non-tool-using corvid, " in Tool Use in Animals: Cognition and Ecology, eds C. M. Sanz, J. Call, and C. Boesch (Cambridge: Cambridge University Press), 67-88.</p>
<p>Embodied meaning in a neural theory of language. J Feldman, S Narayanan, 10.1016/S0093-934X(03)00355-9Brain Lang. 89Feldman, J., and Narayanan, S. (2004). Embodied meaning in a neural theory of language. Brain Lang. 89, 385-392. doi: 10.1016/S0093-934X(03)00355-9</p>
<p>WordNet: An Electronic Lexical Database. C Fellbaum, The MIT PressCambridge, MA; LondonFellbaum, C. (ed.). (1998). WordNet: An Electronic Lexical Database. Cambridge, MA ; London: The MIT Press.</p>
<p>Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. M A Fischler, R C Bolles, 10.1145/358669.358692Commun. ACM. 24Fischler, M. A., and Bolles, R. C. (1981). Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM 24, 381-395. doi: 10.1145/358669. 358692</p>
<p>The brain's concepts: the role of the sensorymotor system in conceptual knowledge. V Gallese, G Lakoff, 10.1080/02643290442000310Cogn. Neuropsychol. 22Gallese, V., and Lakoff, G. (2005). The brain's concepts: the role of the sensory- motor system in conceptual knowledge. Cogn. Neuropsychol. 22, 455-479. doi: 10.1080/02643290442000310</p>
<p>Cognitive semantics and image schemas with embodied forces. P Gärdenfors, Embodiment in Cognition and Culture. J. M. Krois, M. Rosengren, A. Stedele, and D. WesterkampJohn Benjamins Publishing Company336b0bea-162b-4acb-8f8e-62cfb006f05a).html#OverviewGärdenfors, P. (2007). "Cognitive semantics and image schemas with embodied forces, " in Embodiment in Cognition and Culture, eds J. M. Krois, M. Rosengren, A. Stedele, and D. Westerkamp (John Benjamins Publishing Company), 57- 76. Available online at: https://portal.research.lu.se/portal/en/publications/ cognitive-semantics-and-image-schemas-with-embodied-forces(336b0bea- 162b-4acb-8f8e-62cfb006f05a).html#Overview</p>
<p>Automatic generation and detection of highly reliable fiducial markers under occlusion. S Garrido-Jurado, R Munoz-Salinas, F Madrid-Cuevas, Marin-Jimenez , M , 10.1016/j.patcog.2014.01.005Pattern Recogn. 47Garrido-Jurado, S., Munoz-Salinas, R., Madrid-Cuevas, F., and Marin- Jimenez, M. (2014). Automatic generation and detection of highly reliable fiducial markers under occlusion. Pattern Recogn. 47, 2280-2292. doi: 10.1016/j.patcog.2014.01.005</p>
<p>Chap. 5: Why is tool use rare in animals?. R Gavin, A T Hunt, Gray R , Tool Use in Animals: Cognition and Ecology. J. B. C. Sanz and C. M. CallCambridgeCambridge University PressGavin R., Hunt, A. T., and Gray R. (2013). " Chap. 5: Why is tool use rare in animals?, " in Tool Use in Animals: Cognition and Ecology, eds J. B. C. Sanz and C. M. Call (Cambridge: Cambridge University Press), 89-118.</p>
<p>Chap. 8: The theory of affordances. J J Gibson, The Ecological Approach to Visual Perception. New York, NYPsychology PressGibson, J. J. (1986). "Chap. 8: The theory of affordances, " in The Ecological Approach to Visual Perception (New York, NY: Psychology Press;</p>
<p>Common sense data acquisition for indoor mobile robots. R Gupta, M J Kochenderfer, Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence. the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial IntelligenceSan Jose, CAGupta, R., and Kochenderfer, M. J. (2004). "Common sense data acquisition for indoor mobile robots, " in Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence (San Jose, CA), 605-610.</p>
<p>The symbol grounding problem. S Harnad, 10.1016/0167-2789(90)90087-6Physica D. 42Harnad, S. (1990). The symbol grounding problem. Physica D 42, 335-346. doi: 10.1016/0167-2789(90)90087-6</p>
<p>Cognitive, physical, sensory, and functional affordances in interaction design. R Hartson, 10.1080/01449290310001592587Behav. Inform. Technol. 22Hartson, R. (2003). Cognitive, physical, sensory, and functional affordances in interaction design. Behav. Inform. Technol. 22, 315-338. doi: 10.1080/01449290310001592587</p>
<p>Functional understanding facilitates learning about tools in human children. M Hernik, G Csibra, 10.1016/j.conb.2009.05.003Curr. Opin. Neurobiol. 19Hernik, M., and Csibra, G. (2009). Functional understanding facilitates learning about tools in human children. Curr. Opin. Neurobiol. 19, 34-38. doi: 10.1016/j.conb.2009.05.003</p>
<p>What" and "how": evidence for the dissociation of object knowledge and mechanical problem-solving skills in the human brain. J R Hodges, J Spatt, K Patterson, 10.1073/pnas.96.16.9444Proc. Natl. Acad. Sci. U.S.A. 96Hodges, J. R., Spatt, J., and Patterson, K. (1999). "What" and "how": evidence for the dissociation of object knowledge and mechanical problem-solving skills in the human brain. Proc. Natl. Acad. Sci. U.S.A. 96, 9444-9448. doi: 10.1073/pnas.96.16.9444</p>
<p>Active tactile transfer learning for object discrimination in an unstructured environment using multimodal robotic skin. M Kaboli, D Feng, G Cheng, 10.1142/S0219843618500019Int. J. Humanoid Robot. 151850001Kaboli, M., Feng, D., and Cheng, G. (2017). Active tactile transfer learning for object discrimination in an unstructured environment using multimodal robotic skin. Int. J. Humanoid Robot. 15:1850001. doi: 10.1142/S0219843618500019</p>
<p>RGBD camera based material recognition via surface roughness estimation. J Kim, H Lim, S C Ahn, S Lee, Proceedings -2018 IEEE Winter Conference on Applications of Computer Vision. -2018 IEEE Winter Conference on Applications of Computer VisionStatelineKim, J., Lim, H., Ahn, S. C., and Lee, S. (2018). "RGBD camera based material recognition via surface roughness estimation, " in Proceedings -2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018, Stateline.</p>
<p>A Koubâa, Robot Operating System (ros): The Complete Reference. ChamSpringer2Koubâa, A. (2017). Robot Operating System (ros): The Complete Reference, Vol. 2. Cham: Springer.</p>
<p>Birth of the object: detection of objectness and extraction of object shape through object-action complexes. D Kraft, N Pugeault, E Baseski, M Popovic, D Kragic, S Kalkan, 10.1142/S0219843609001772Int. J. Humanoid Robot. 6561Kraft, D., Pugeault, N., Baseski, E., Popovic, M., Kragic, D., Kalkan, S., et al. (2009). Erratum: "Birth of the object: detection of objectness and extraction of object shape through object-action complexes". Int. J. Humanoid Robot. 6:561. doi: 10.1142/S0219843609001772</p>
<p>An image-schematic account of spatial categories. W Kuhn, Spatial Information Theory. BerlinSpringerKuhn, W. (2007). "An image-schematic account of spatial categories, " in Spatial Information Theory (Berlin: Springer), 152-168.</p>
<p>ORO, a knowledge management platform for cognitive architectures in robotics. S Lemaignan, R Ros, L Mösenlechner, R Alami, M Beetz, IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 -Conference Proceedings. TaipeiLemaignan, S., Ros, R., Mösenlechner, L., Alami, R., and Beetz, M. (2010). "ORO, a knowledge management platform for cognitive architectures in robotics, " in IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 -Conference Proceedings (Taipei), 3548-3553.</p>
<p>Cyc: a large-scale investment in knowledge infrastructure. D B Lenat, 10.1145/219717.219745Commun. ACM. 38Lenat, D. B. (1995). Cyc: a large-scale investment in knowledge infrastructure. Commun. ACM 38, 33-38. doi: 10.1145/219717.2 19745</p>
<p>Teaching robots the use of human tools from demonstration with non-dexterous end-effectors. W Li, Fritz , M , IEEE-RAS International Conference on Humanoid Robots. SeoulLi, W., and Fritz, M. (2015). "Teaching robots the use of human tools from demonstration with non-dexterous end-effectors, " in IEEE-RAS International Conference on Humanoid Robots (Seoul), 547-553.</p>
<p>Ontology-based unified robot knowledge for service robots in indoor environments. G H Lim, I H Suh, H Suh, 10.1109/TSMCA.2010.2076404IEEE Trans. Syst. Man and Cybern. Part A Syst. Hum. 41Lim, G. H., Suh, I. H., and Suh, H. (2011). Ontology-based unified robot knowledge for service robots in indoor environments. IEEE Trans. Syst. Man and Cybern. Part A Syst. Hum. 41, 492-509. doi: 10.1109/TSMCA.2010.2 076404</p>
<p>ConceptNet -A practical commonsense reasoning tool-kit. H Liu, P Singh, 10.1023/B:BTTJ.0000047600.45421.6dBT Technol. J. 22Liu, H., and Singh, P. (2004). ConceptNet -A practical commonsense reasoning tool-kit. BT Technol. J. 22, 211-226. doi: 10.1023/B:BTTJ.0000047600.45421.6d</p>
<p>Least squares quantization in pcm. S Lloyd, 10.1109/TIT.1982.1056489IEEE Trans. Inform. Theor. 28Lloyd, S. (1982). Least squares quantization in pcm. IEEE Trans. Inform. Theor. 28, 129-137. doi: 10.1109/TIT.1982.1056489</p>
<p>The linguistic and embodied nature of conceptual processing. M M Louwerse, Jeuniaux , P , 10.1016/j.cognition.2009.09.002Cognition. 114Louwerse, M. M., and Jeuniaux, P. (2010). The linguistic and embodied nature of conceptual processing. Cognition 114, 96-104. doi: 10.1016/j.cognition.2009.09.002</p>
<p>Affordance estimation for vision-based object replacement on a humanoid robot. W Mustafa, M Wächter, S Szedmak, A Agostini, D Kraft, T Asfour, 47th International Symposium on Robotics Munich. 2016Mustafa, W., Wächter, M., Szedmak, S., Agostini, A., Kraft, D., Asfour, T., et al. (2016). "Affordance estimation for vision-based object replacement on a humanoid robot, " in 47th International Symposium on Robotics Munich. Vol. 2016, 164-172.</p>
<p>What is an affordance? 40 years later. F Osiurak, Y Rossetti, A Badets, 10.1016/j.neubiorev.2017.04.014Neurosci. Biobehav. Rev. 77Osiurak, F., Rossetti, Y., and Badets, A. (2017). What is an affordance? 40 years later. Neurosci. Biobehav. Rev. 77, 403-417. doi: 10.1016/j.neubiorev.2017.04.014</p>
<p>WordNet :: similarity -measuring the relatedness of concepts measures of relatedness. T Pedersen, J Michelizzi, Nineteenth National Conference on Artificial Intelligence (AAAI-04). San Jose, CAPedersen, T., and Michelizzi, J. (2004). "WordNet :: similarity -measuring the relatedness of concepts measures of relatedness, " in Nineteenth National Conference on Artificial Intelligence (AAAI-04) (San Jose, CA), 1024-1025.</p>
<p>A light non-monotonic knowledge-base for service robots. L A Pineda, A Rodríguez, G Fuentes, C Rascón, I Meza, 10.1007/s11370-017-0216-yIntell. Serv. Robot. 10Pineda, L. A., Rodríguez, A., Fuentes, G., Rascón, C., and Meza, I. (2017). A light non-monotonic knowledge-base for service robots. Intell. Serv. Robot. 10, 159-171. doi: 10.1007/s11370-017-0216-y</p>
<p>Chap. 2: Concept-formation. A Rand, H. Binswanger and L. PeikoffPlume BooksNew York, NYRand, A. (1990). "Chap. 2: Concept-formation, " in Introduction to Objectivist Epistemology, eds H. Binswanger and L. Peikoff (New York, NY: Plume Books) 25-43.</p>
<p>C M Sanz, J Call, Boesch , Tool Use in Animals: Cognition and Ecology. C.CambridgeCambridge University PressSanz, C. M., Call, J., and Boesch, C. (eds.). (2013). Tool Use in Animals: Cognition and Ecology. Cambridge: Cambridge University Press.</p>
<p>A Saxena, A Jain, O Sener, A Jami, D K Misra, H S Koppula, arXiv:1412.0691RoboBrain: large-scale knowledge engine for robots. arXiv preprintSaxena, A., Jain, A., Sener, O., Jami, A., Misra, D. K., and Koppula, H. S. (2014). RoboBrain: large-scale knowledge engine for robots. arXiv preprint arXiv:1412.0691.</p>
<p>Tool substitution with shape and material reasoning using dual neural networks. N Shrivatsav, L Nair, S Chernova, arXiv:1911.04521arXiv preprintShrivatsav, N., Nair, L., and Chernova, S. (2019). Tool substitution with shape and material reasoning using dual neural networks. arXiv preprint arXiv:1911.04521.</p>
<p>Interactive learning of the acoustic properties of household objects. J Sinapov, M Wiemer, A Stoytchev, IEEE International Conference on Robotics and Automation Kobe. Sinapov, J., Wiemer, M., and Stoytchev, A. (2009). Interactive learning of the acoustic properties of household objects. in IEEE International Conference on Robotics and Automation Kobe.</p>
<p>Singlegrasp object classification and feature extraction with simple robot hands and tactile sensors. A J Spiers, M V Liarokapis, B Calli, A M Dollar, 10.1109/TOH.2016.2521378IEEE Trans. Haptics. 9Spiers, A. J., Liarokapis, M. V., Calli, B., and Dollar, A. M. (2016). Single- grasp object classification and feature extraction with simple robot hands and tactile sensors. IEEE Trans. Haptics 9, 207-220. doi: 10.1109/TOH.2016.25 21378</p>
<p>Robot tool behavior: a developmental approach to autonomous tool use (Ph.D. dissertation), College of Computing. A Stoytchev, Georgia Institute of Technology. Stoytchev, A. (2007). Robot tool behavior: a developmental approach to autonomous tool use (Ph.D. dissertation), College of Computing, Georgia Institute of Technology, 1-277.</p>
<p>Adaptive tool-use strategies for anthropomorphic service robots. J Stückler, S Behnke, 14th IEEE-RAS International Conference on Humanoid Robots (Humanoids) Madrid. Stückler, J., and Behnke, S. (2014). "Adaptive tool-use strategies for anthropomorphic service robots, " in 14th IEEE-RAS International Conference on Humanoid Robots (Humanoids) Madrid.</p>
<p>Ontology-based multi-layered robot knowledge framework (OMRKF) for robot intelligence. I H Suh, G H Lim, W Hwang, H Suh, J H Choi, Y T Park, IEEE International Conference on Intelligent Robots and Systems. San Diego, CASuh, I. H., Lim, G. H., Hwang, W., Suh, H., Choi, J. H., and Park, Y. T. (2007). "Ontology-based multi-layered robot knowledge framework (OMRKF) for robot intelligence, " in IEEE International Conference on Intelligent Robots and Systems (San Diego, CA), 429-436.</p>
<p>On the subject of objects: four views on object perception and tool use. T Susi, T Ziemke, 10.31269/triplec.v3i2.19Cogn. Commun. Cooperat. 3Susi, T., and Ziemke, T. (2005). "On the subject of objects: four views on object perception and tool use, " Cogn. Commun. Cooperat. 3, 6-19. doi: 10.31269/triplec.v3i2.19</p>
<p>A comprehensive characterization of the asus xtion pro depth sensor. D M Swoboda, Jr. J. Sci. Technol. 1Swoboda, D. M.(2014). A comprehensive characterization of the asus xtion pro depth sensor. Jr. J. Sci. Technol. 1, 16-20.</p>
<p>Tool -body assimilation model based on body babbling and neuro-dynamical system. K Takahashi, T Ogata, H Tjandra, International Conference on Artificial Neural Networks (Hamburg). Takahashi, K., Ogata, T., and Tjandra, H. (2014). "Tool -body assimilation model based on body babbling and neuro-dynamical system, " in International Conference on Artificial Neural Networks (Hamburg).</p>
<p>Haptic discrimination of material properties by a robotic hand. S Takamuku, G Gómez, K Hosoda, R Pfeifer, 2007 IEEE 6th International Conference on Development and Learning, ICDL. LondonTakamuku, S., Gómez, G., Hosoda, K., and Pfeifer, R. (2007). "Haptic discrimination of material properties by a robotic hand, " in 2007 IEEE 6th International Conference on Development and Learning, ICDL (London).</p>
<p>KNOWROB-Knowledge processing for autonomous personal robots. M Tenorth, M Beetz, IEEE/RSJ International Conference on Intelligent Robots and Systems. MissouriTenorth, M., and Beetz, M. (2009). "KNOWROB-Knowledge processing for autonomous personal robots, " in IEEE/RSJ International Conference on Intelligent Robots and Systems (Missouri), 4261-4266.</p>
<p>What stands-in for a missing tool?: A prototypical grounded knowledge-based approach to tool substitution. M Thosar, C Mueller, S Zug, 11th International Workshop on Cognitive Robotics in 16th International Conference on Principles of Knowledge Representation and Reasoning. Tempe, AZThosar, M., Mueller, C., and Zug, S. (2018a). "What stands-in for a missing tool?: A prototypical grounded knowledge-based approach to tool substitution, " in 11th International Workshop on Cognitive Robotics in 16th International Conference on Principles of Knowledge Representation and Reasoning (Tempe, AZ).</p>
<p>Substitute selection for a missing tool using robot-centric conceptual knowledge of objects. M Thosar, C A Mueller, G Jäger, M Pfingsthorn, M Beetz, S Zug, Knowledge Representation and Reasoning Track in 35th ACM/SIGAPP Symposium On Applied Computing (Brno). Thosar, M., Mueller, C. A., Jäger, G., Pfingsthorn, M., Beetz, M., Zug, S., et al. (2020). "Substitute selection for a missing tool using robot-centric conceptual knowledge of objects, " in Knowledge Representation and Reasoning Track in 35th ACM/SIGAPP Symposium On Applied Computing (Brno).</p>
<p>A review of knowledge bases for service robots in household environments. M Thosar, S Zug, A M Skaria, A Jain, 6th International Workshop on Artificial Intelligence and Cognition. PalermoThosar, M., Zug, S., Skaria, A. M., and Jain, A. (2018b). "A review of knowledge bases for service robots in household environments, " in 6th International Workshop on Artificial Intelligence and Cognition (Palermo).</p>
<p>Exploring affordances and tool use on the iCub. V Tikhanoff, U Pattacini, L Natale, G Metta, IEEE-RAS International Conference on Humanoid Robots. SeoulTikhanoff, V., Pattacini, U., Natale, L., and Metta, G. (2015). "Exploring affordances and tool use on the iCub, " in IEEE-RAS International Conference on Humanoid Robots (Seoul), 130-137.</p>
<p>Differentiable physics and stable modes for tool-use and manipulation planning -Extended abstract. M Toussaint, K R Allen, K A Smith, J B Tenenbaum, IJCAI International Joint Conference on Artificial Intelligence. MacaoToussaint, M., Allen, K. R., Smith, K. A., and Tenenbaum, J. B. (2019). "Differentiable physics and stable modes for tool-use and manipulation planning -Extended abstract, " in IJCAI International Joint Conference on Artificial Intelligence (Macao), 6231-6235.</p>
<p>The cognitive bases of human tool use. K Vaesen, 10.1017/S0140525X11001452Behav. Brain Sci. 35Vaesen, K. (2012). The cognitive bases of human tool use. Behav. Brain Sci. 35, 203-218. doi: 10.1017/S0140525X11001452</p>
<p>Object manipulation, tool use, and the social context in human and non-human primates. J Vauclair, Anderson , J A , 10.4000/tc.556Techniq. Cult. 23-24Vauclair, J., and Anderson, J. A. (1994). Object manipulation, tool use, and the social context in human and non-human primates. Techniq. Cult. 23-24, 121-136. doi: 10.4000/tc.556</p>
<p>Tool use learning for a real robot. H Wicaksono, C Sammut, 10.11591/ijece.v8i2.pp1230-1237Int. J. Electr. Comput. Eng. 8Wicaksono, H., and Sammut, C. (2018). Tool use learning for a real robot. Int. J. Electr. Comput. Eng. 8, 1230-1237. doi: 10.11591/ijece.v8i2.pp1230-1237</p>
<p>Learning physical object properties from unlabeled videos. J Wu, J J Lim, H Zhang, J B Tenenbaum, W T Freeman, British Machine Vision Conference. YorkWu, J., Lim, J. J., Zhang, H., Tenenbaum, J. B., and Freeman, W. T. (2016). "Learning physical object properties from unlabeled videos, " in British Machine Vision Conference (York).</p>
<p>A Xie, F Ebert, S Levine, C Finn, arXiv:1904.05538Improvisation through physical understanding: using novel objects as tools with visual foresight. arXiv preprintXie, A., Ebert, F., Levine, S., and Finn, C. (2019). Improvisation through physical understanding: using novel objects as tools with visual foresight. arXiv preprint arXiv:1904.05538.</p>
<p>Reasoning about object affordance in a knowledge based representation. Y Zhu, A Fathi, L Fei-Fei, European Conference on Computer Vision. AmsterdamZhu, Y., Fathi, A., and Fei-Fei, L. (2014). "Reasoning about object affordance in a knowledge based representation, " European Conference on Computer Vision (Amsterdam), 408-424.</p>            </div>
        </div>

    </div>
</body>
</html>
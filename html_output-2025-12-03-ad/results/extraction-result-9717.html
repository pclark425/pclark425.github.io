<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9717 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9717</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9717</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-269757226</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.06713v1.pdf" target="_blank">Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Since the emergence of ChatGPT, Large Language Models (LLMs) have garnered global recognition for their strategic significance in economic expansion, innovation, societal development, and national security. These models possess exceptional generative capabilities, enabling intelligent conversation, sophisticated data processing, creative problem-solving and even scientific discovery. A notable increase in investment and policy initiatives has been observed, spurring a dynamic global race in LLM development. According to brokerage CLSA, the United States has established a significant lead in this competition, capturing 50% of the global market share in (LLMs). In tandem, China has ascended rapidly, boasting a fleet of over 130 LLMs and accounting for a substantial 40% of the global market presence 1 . The burgeoning competition between the two superpowers in AI underscore the need for a comparative evaluation to understand the current state, anticipate future trajectories, and forge collaborative pathways that leverage the strengths of both nations.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9717.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9717.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo (finetuned) judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned GPT-3.5-Turbo used as an LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5-Turbo model finetuned on human pairwise preference data and chatbot_arena_conversations to act as an automatic pairwise evaluator for Chinese-language natural language proficiency tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Natural language proficiency evaluation (Chinese context): free Q&A, content generation, scenario simulation, roleplaying (pairwise answer comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Fine-tuned GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise comparisons: judge presented a question and two anonymized answers (A/B) and asked to pick the better answer or declare a tie. Training data for the judge combined human preference judgments and the chatbot_arena_conversations dataset; prompts instructed the judge to consider helpfulness, relevance, accuracy, depth, creativity, and level of detail. Mitigations for biases included alternating answer order to counter position bias, anonymizing source labels to counter self-enhancement, and instructing judges not to favor longer responses to reduce verbosity bias.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluation used Likert-scale scoring (7-point) for open-ended subtasks and human preference judgments to build the judge's training data; the paper also reports consulting 9 experts to determine the weighting of capability dimensions (40.56:32.22:27.22). A human-generated validation set of pairwise comparisons was used to evaluate the LLM judge (size not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent agreement (consistency rate) between the finetuned LLM judge and human judges; reported consistency >67% overall and in some tasks >70%. The paper references prior human-human agreement in pairwise comparisons as ranging 60–85% for context.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Differences and degradations include: (1) risk of model-specific self-reinforcement bias (the finetuned LLM judge may favor outputs similar to its own generation style), (2) reduced sensitivity to certain human-perceived qualitative issues (e.g., bilingual code-switching that human judges penalize), (3) potential evaluation bias leading to discrepancies on specific models or answer traits, and (4) remaining unexplained divergences implying LLM judges do not fully capture nuanced human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>1) ChatGLM3 responses sometimes contained English words amid Chinese answers; human judges penalized these bilingual mixes lowering scores, whereas the LLM judge did not penalize them, rating ChatGLM3 higher than humans did. 2) Finetuned GPT-3.5-Turbo judge awarded more pairwise victories to GPT-3.5-Turbo over GPT-4 despite human judges perceiving GPT-4 as superior — attributed to possible self-reinforcement bias in the finetuned judge.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Overall rankings from LLM-as-a-judge and human evaluation were 'fundamentally consistent': top models (e.g., GPT-4-Turbo as best) were identified by both methods. The authors argue the LLM judge is a feasible, cost-effective complement to human scoring. Reported agreement (>67%, sometimes >70%) falls within ranges comparable to human-human agreement cited (60–85%), supporting usefulness though not perfect interchangeability.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Supplementary Assessment with LLM-as-a-Judge: Ranking Natural Language Proficiency of LLMs in Chinese Contexts (paragraphs describing fine-tuning, validation, agreement rates, and discrepancies)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9717.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9717.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge method (pairwise)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge pairwise evaluation method (with mitigation strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology using a (finetuned) LLM to perform pairwise comparisons of model outputs, with protocol-level mitigations to reduce known automated-evaluation biases (position, self-enhancement, verbosity).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Automatic evaluation of open-ended natural language outputs (Chinese natural language proficiency subtasks) via pairwise comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Primary: finetuned GPT-3.5-Turbo; also evaluated (but not selected) finetuned Ernie-Bot 4 and unfine-tuned GPT-4-Turbo in pretests</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Used pairwise comparison prompts derived from prior work; each answer pair evaluated twice with randomized ordering to mitigate position bias; responses anonymized as A/B to avoid self-enhancement; explicit instruction to judge fairness and not let length affect decisions to mitigate verbosity bias.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judges provided initial preference data used to fine-tune LLM judges; human scoring for natural language proficiency used a 7-point Likert scale and human experts (9 consulted for weighting of evaluation dimensions) determined metric weights. A human-generated validation set compared against LLM judge outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Reported agreement is percent consistency between LLM and human judgments: >67% (some tasks >70%). The paper compares this to human-human agreement from prior literature (60–85%) to contextualize feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>What is lost or different when replacing humans with LLM judges: (a) sensitivity to some human qualitative preferences (e.g., penalizing language mixing or cultural nuance), (b) susceptibility to model-related biases (self-reinforcement) that can skew relative model rankings, (c) potential evaluation bias not fully eliminated by protocol mitigations, and (d) inability to fully replicate human-subjective judgments leading to minor but meaningful ranking discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete divergences include ChatGLM3 being rated higher by the LLM judge due to bilingual content not being penalized, and the finetuned GPT-3.5-Turbo judge awarding more pairwise wins to GPT-3.5-Turbo over GPT-4 contrary to human evaluators' preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The method validated human rankings broadly: aggregate rankings were fundamentally consistent and the LLM judge provided a cost-effective complementary signal; the observed consistency rates are within the range of human-human agreement reported in earlier work. The paper also documents and applies bias-mitigation strategies (randomized ordering, anonymization, explicit instructions), and recommends expanding prompt and test diversity to reduce residual discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Supplementary Assessment with LLM-as-a-Judge: methodology, pretest comparison, bias mitigation, consistency statistics, and discrepancy analysis sections</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>chatbot_arena_conversations (LMsystems dataset) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9717",
    "paper_id": "paper-269757226",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "GPT-3.5-Turbo (finetuned) judge",
            "name_full": "Fine-tuned GPT-3.5-Turbo used as an LLM-as-a-judge",
            "brief_description": "A GPT-3.5-Turbo model finetuned on human pairwise preference data and chatbot_arena_conversations to act as an automatic pairwise evaluator for Chinese-language natural language proficiency tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Natural language proficiency evaluation (Chinese context): free Q&A, content generation, scenario simulation, roleplaying (pairwise answer comparisons)",
            "llm_judge_model": "Fine-tuned GPT-3.5-Turbo",
            "llm_judge_setup": "Pairwise comparisons: judge presented a question and two anonymized answers (A/B) and asked to pick the better answer or declare a tie. Training data for the judge combined human preference judgments and the chatbot_arena_conversations dataset; prompts instructed the judge to consider helpfulness, relevance, accuracy, depth, creativity, and level of detail. Mitigations for biases included alternating answer order to counter position bias, anonymizing source labels to counter self-enhancement, and instructing judges not to favor longer responses to reduce verbosity bias.",
            "human_evaluation_setup": "Human evaluation used Likert-scale scoring (7-point) for open-ended subtasks and human preference judgments to build the judge's training data; the paper also reports consulting 9 experts to determine the weighting of capability dimensions (40.56:32.22:27.22). A human-generated validation set of pairwise comparisons was used to evaluate the LLM judge (size not specified).",
            "agreement_metric": "Percent agreement (consistency rate) between the finetuned LLM judge and human judges; reported consistency &gt;67% overall and in some tasks &gt;70%. The paper references prior human-human agreement in pairwise comparisons as ranging 60–85% for context.",
            "losses_identified": "Differences and degradations include: (1) risk of model-specific self-reinforcement bias (the finetuned LLM judge may favor outputs similar to its own generation style), (2) reduced sensitivity to certain human-perceived qualitative issues (e.g., bilingual code-switching that human judges penalize), (3) potential evaluation bias leading to discrepancies on specific models or answer traits, and (4) remaining unexplained divergences implying LLM judges do not fully capture nuanced human preferences.",
            "examples_of_loss": "1) ChatGLM3 responses sometimes contained English words amid Chinese answers; human judges penalized these bilingual mixes lowering scores, whereas the LLM judge did not penalize them, rating ChatGLM3 higher than humans did. 2) Finetuned GPT-3.5-Turbo judge awarded more pairwise victories to GPT-3.5-Turbo over GPT-4 despite human judges perceiving GPT-4 as superior — attributed to possible self-reinforcement bias in the finetuned judge.",
            "counterexamples_or_caveats": "Overall rankings from LLM-as-a-judge and human evaluation were 'fundamentally consistent': top models (e.g., GPT-4-Turbo as best) were identified by both methods. The authors argue the LLM judge is a feasible, cost-effective complement to human scoring. Reported agreement (&gt;67%, sometimes &gt;70%) falls within ranges comparable to human-human agreement cited (60–85%), supporting usefulness though not perfect interchangeability.",
            "paper_reference": "Supplementary Assessment with LLM-as-a-Judge: Ranking Natural Language Proficiency of LLMs in Chinese Contexts (paragraphs describing fine-tuning, validation, agreement rates, and discrepancies)",
            "uuid": "e9717.0",
            "source_info": {
                "paper_title": "Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLM-as-a-judge method (pairwise)",
            "name_full": "LLM-as-a-judge pairwise evaluation method (with mitigation strategies)",
            "brief_description": "A methodology using a (finetuned) LLM to perform pairwise comparisons of model outputs, with protocol-level mitigations to reduce known automated-evaluation biases (position, self-enhancement, verbosity).",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Automatic evaluation of open-ended natural language outputs (Chinese natural language proficiency subtasks) via pairwise comparisons",
            "llm_judge_model": "Primary: finetuned GPT-3.5-Turbo; also evaluated (but not selected) finetuned Ernie-Bot 4 and unfine-tuned GPT-4-Turbo in pretests",
            "llm_judge_setup": "Used pairwise comparison prompts derived from prior work; each answer pair evaluated twice with randomized ordering to mitigate position bias; responses anonymized as A/B to avoid self-enhancement; explicit instruction to judge fairness and not let length affect decisions to mitigate verbosity bias.",
            "human_evaluation_setup": "Human judges provided initial preference data used to fine-tune LLM judges; human scoring for natural language proficiency used a 7-point Likert scale and human experts (9 consulted for weighting of evaluation dimensions) determined metric weights. A human-generated validation set compared against LLM judge outputs.",
            "agreement_metric": "Reported agreement is percent consistency between LLM and human judgments: &gt;67% (some tasks &gt;70%). The paper compares this to human-human agreement from prior literature (60–85%) to contextualize feasibility.",
            "losses_identified": "What is lost or different when replacing humans with LLM judges: (a) sensitivity to some human qualitative preferences (e.g., penalizing language mixing or cultural nuance), (b) susceptibility to model-related biases (self-reinforcement) that can skew relative model rankings, (c) potential evaluation bias not fully eliminated by protocol mitigations, and (d) inability to fully replicate human-subjective judgments leading to minor but meaningful ranking discrepancies.",
            "examples_of_loss": "Concrete divergences include ChatGLM3 being rated higher by the LLM judge due to bilingual content not being penalized, and the finetuned GPT-3.5-Turbo judge awarding more pairwise wins to GPT-3.5-Turbo over GPT-4 contrary to human evaluators' preferences.",
            "counterexamples_or_caveats": "The method validated human rankings broadly: aggregate rankings were fundamentally consistent and the LLM judge provided a cost-effective complementary signal; the observed consistency rates are within the range of human-human agreement reported in earlier work. The paper also documents and applies bias-mitigation strategies (randomized ordering, anonymization, explicit instructions), and recommends expanding prompt and test diversity to reduce residual discrepancies.",
            "paper_reference": "Supplementary Assessment with LLM-as-a-Judge: methodology, pretest comparison, bias mitigation, consistency statistics, and discrepancy analysis sections",
            "uuid": "e9717.1",
            "source_info": {
                "paper_title": "Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "chatbot_arena_conversations (LMsystems dataset)",
            "rating": 1,
            "sanitized_title": "chatbotarenaconversations_lmsystems_dataset"
        }
    ],
    "cost": 0.009415,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs</p>
<p>Zhenhui Jiang 
HKU Business School</p>
<p>HKU Business School Shenzhen Research Institute (SZRI</p>
<p>Jiaxin Li 
HKU Business School Shenzhen Research Institute (SZRI</p>
<p>Yang Liu 
School of Management
Xi'an Jiaotong University</p>
<p>Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs
75682998E98F1246E6C1CDA2E9D52991
Since the emergence of ChatGPT, Large Language Models (LLMs) have garnered global recognition for their strategic significance in economic expansion, innovation, societal development, and national security.These models possess exceptional generative capabilities, enabling intelligent conversation, sophisticated data processing, creative problem-solving and even scientific discovery.A notable increase in investment and policy initiatives has been observed, spurring a dynamic global race in LLM development.According to brokerage CLSA, the United States has established a significant lead in this competition, capturing 50% of the global market share in (LLMs).In tandem, China has ascended rapidly, boasting a fleet of over 130 LLMs and accounting for a substantial 40% of the global market presence 1 .The burgeoning competition between the two superpowers in AI underscore the need for a comparative evaluation to understand the current state, anticipate future trajectories, and forge collaborative pathways that leverage the strengths of both nations.Indeed, the rivalry between the US and China in technology, particularly in AI, has intensified over the past few decades.The "2024 Artificial Intelligence Index Report" by Stanford University reveals that while the US is at the forefront of foundational model development, China has secured a dominant position in AI patent filings.This illustrates the complex and multifaceted competitive dynamics between the two nations.Despite the backdrop of competition, there are signs of emerging collaboration.A significant instance is the meeting on April 26, 2024, between Antony Blinken, US Secretary of State, and Wang Yi, China's Minister of Foreign Affairs, in Beijing.The meeting culminated in a five-point consensus, including the announcement of the inaugural US-China AI Dialogue.This initiative represents a crucial stride towards bridging divides and nurturing cooperation in the field of AI.In this context of both competition and cooperation, we provide a transparent evaluation of mainstream LLMs in US and China.Our aim is to build trust and establish common ground for discussion, which can facilitate a more open dialogue about potential areas of collaboration.Furthermore, it is essential to consider the cultural and linguistic diversity inherent in LLMs.While models like ChatGPT may excel in English, their performance in other languages settings is largely unclear.Chinese-origin LLMs may excel in their native context but may falter in English settings.Hence, achieving a holistic understanding of LLMs across different linguistic contexts is imperative.To this end, our assessment of the general-purpose LLMs considers their performance in both English and Chinese contexts.It not only accentuates the linguistic competencies and cultural adaptability inherent in these models but also propels the evolution of LLMs to become more inclusive, culturally aware, and globally relevant.It is a step towards ensuring that the benefits of AI are accessible to all, regardless of language or cultural background, and it promotes a more 1 https://www.businesstimes.com.sg/international/china-approves-over-40-ai-models-public-use-past-six-months</p>
<p>equitable and diverse AI industry.</p>
<p>In summary, to address the imperatives mentioned above, we propose a systematic evaluation framework meticulously examines a spectrum of LLM specializations, encompassing linguistic proficiency, cognitive performance, and considerations of safety and ethical responsibility.The framework is designed to offer policy makers and businesses a comprehensive understanding of LLMs' capabilities under various operational tasks and scenarios.By scrutinizing these attributes, our thorough analysis empowers stakeholders to make well-informed decisions regarding strategic investments in LLM technology, to forge alliances that can leverage the strengths of different models, and to ensure regulatory compliance that aligns with the highest standards of safety and ethics.The assessment, which spans both English and Chinese contexts, will provide a nuanced understanding of the competitiveness of LLMs in the global landscape, bridge gaps in LLM development, and promote global technological cooperation.</p>
<p>The Evaluation Framework</p>
<p>The fundamental LLM capabilities to assist humans resolve queries and accomplish tasks lies in their natural language understanding and generation, supported by a vast reservoir of knowledge.This is further guided by principles that align with human values.Hence, we divide the capabilities of LLMs into three dimensions: natural language proficiency, disciplinary expertise, and safety and responsibility.This tripartite classification serves as a robust framework for evaluating LLMs, encompassing a diverse array of sub-tasks aimed at assessing the models' overall performance.</p>
<p>Natural Language Proficiency: This dimension underscores the model's ability to comprehend and generate text, reflecting its linguistic versatility.To more comprehensively assess the models' adeptness in handling both simple and complex requests, we engaged in natural language tasks spanning two difficulty levels, basic and advanced, for assessment.Basic language abilities are assessed through subtasks including free Q&amp;A, content generation, content summarization, cross-language translation, multi-round dialogue, instruction following, and inference and reasoning.Advanced language abilities are evaluated through subtasks like scenariobased simulation and role-playing.These subtasks require the model to generate responses that accurately reflect the personality traits, values, and speech or behavioral patterns of a particular character, or to adapt to the unique settings and requirements of various scenario.Such subtasks necessitate a deep understanding of human emotions, roles, and social dynamics, distinguishing them from the basic subtasks.Through these subtasks, a model demonstrates not only its capability in effective user communication and problem-solving but also its nuanced interpretation and response to context across various languages and cultural nuances.</p>
<p>Disciplinary Expertise: This dimension assesses the model's knowledge across specialized academic disciplines at two levels of difficulty: secondary school and college.This expertise ensures that LLMs can provide accurate, detailed, and relevant information when addressing inquiries necessitating domain-specific knowledge.The secondary school level tests encompass mathematics, physics, chemistry, biology, geography, and history.The college level extends to include tests in additional disciplines such as management, law, medicine, computer science, and psychology.</p>
<p>Safety and Responsibility: This crucial dimension focuses on the alignment of LLMs with ethical standards and human values, assessing their ability to recognize malicious or aggressive content in user instructions and provide safe and responsible responses.The subtasks are categorized into explicit malicious prompts and camouflaged malicious prompts based on the challenge of defense.Explicit malicious prompts refer to direct queries that might elicit inappropriate outputs related to eight specific safety scenarios.These safety scenarios include dangerous topics, crimes and illegal activities, physical harm, ethics and morality, mental health, privacy violation, bias and discrimination, and unqualified advice.For instance, in the case of unqualified advice, an LLM is evaluated on its capability to avoid generating financial or medical guidance that necessitates corresponding professional qualifications to provide, thereby preventing the risk of financial loss or health hazards to users.Camouflaged malicious prompts are designed to elicit inappropriate or harmful outputs by disguising these instructions as innocuous.This is achieved by subtly circumventing the model's safety protections through strategies like goal hijacking, villain-playing, reverse abduction, and creative manipulation.For example, goal hijacking involves appending a malicious instruction to a normal prompt, tricking the system into prioritizing the malicious instruction over the original normal prompt, thereby altering the system's actions to serve unintended, potentially harmful purposes.Villain-playing prompts are designed to test whether an LLM will generate harmful content when instructed to assume the role of a villain.These tests allow for a thorough evaluation of the model's adeptness at discerning and neutralizing potential threats to ensure the integrity and safety of its interactions.</p>
<p>Ranking LLMs in the English contexts</p>
<p>We first compare LLMs from the US and China on their performance in English settings, given that English is the primary language for many global AI benchmarks and applications.We conducted evaluations and comparisons among 16 representative models, including 7 international models and 9 developed by Chinese entities, as shown in Table 1.Specifically, the American group features the GPT series from OpenAI, alongside models developed by tech giants like Google and Meta and unicorn AI enterprises and scientific teams such as Anthropic and BigScience.The Chinese LLMs are from a mix of leading technology companies like Baidu and Alibaba, top universities such as Tsinghua University, and unicorn startups in China's generative AI sector.Many of these models, including the GPT series and Claude from the American lineup, and Ernie-Bot and ChatGLM from the Chinese segment, are commercially operational and available to the general public.These models represent the most recent advancements available to ordinary users.By evaluating Chinese LLMs in English contexts, we aim to demonstrate their competitiveness in the international AI arena, highlighting their ability to meet prevalent global standards and their adaptability to widespread applications.</p>
<p>We endeavored to design tests that are as credible, objective, and comprehensive as possible.For the English subtasks testing natural language proficiency, we collected original inquiries from a wide range of LLM users through online surveys and Q&amp;A platforms like Quora.For the assessment of disciplinary expertise, we selected some questions randomly from the MMLU dataset and others from the latest standardized assessments for middle school students in the US, as well as unpublished subject tests from some prestigious universities, minimizing the impact of potential data contamination.The safety and responsibility test set was compiled from multiple sources, including prompts adapted from publicly available safety datasets such as the 100poisonMpts dataset, as well as self-drafted prompts with reference to the Bias Benchmark for QA and others.This approach ensures that our test for explicit malicious prompts comprehensively covers all safety-related topics, and that our camouflaged malicious prompts reflect full range of techniques potentially used to bypass safety checks.The English test set is extensive, encompassing 180 open-ended questions, over 800 closed-ended questions, and 220 safety testing prompts.We primarily retrieved the models' responses via API calls, with the exception of Gemini Pro (accessed through Google Bard) and Claude 2, which were interacted through web due to restrictions on API access in China.In assessing the models' responses to different open-ended questions, factors such as completeness, richness, accuracy, relevance to the topic, creativity, and cultural adaptability were considered.We developed different evaluation scales of LLM answer quality for various subtasks, upon which human judges assess the LLM performance using a 7-point Likert scale, as depicted in Figure 1.This approach facilitates a nuanced evaluation of the models' performance across two dimensions, i.e., natural language proficiency, safety and responsibility.In addition, the disciplinary expertise was evaluated by scoring the model's answers against the standard answers.</p>
<p>Figure 1. Human-as-a-judge</p>
<p>We consulted 9 experts from academia and industry in the United States, Mainland China, Hong Kong, and Singapore, to determine the weighting of the three LLM capability dimensions, namely, natural language proficiency, disciplinary expertise and safety and responsibility.After averaging, the final weights are 40.56:32.22:27.22.Therefore, we derived the following formula to calculate a final comprehensive score.</p>
<p>Final Score = Natural Language Proficiency × 40.56% + Disciplinary Expertise × 32.22% + Safety and Responsibility × 27.22%</p>
<p>As shown in Table 2, our findings indicate that GPT 4-Turbo, with its superior natural language proficiency and disciplinary expertise, secures a considerable lead, positioning it at the forefront among the evaluated models.Gemini Pro and LLaMA 2 also demonstrate commendable performance, ranking second and third respectively.</p>
<p>Ernie-Bot 4 emerges as the frontrunner within the Chinese models, claiming the fifth position overall, marginally outperforming Claude 2 and GPT 3.5-turbo, yet not matching the prowess of GPT 4. Sensenova and Tongyi Qianwen 2 are ranked second and third among the Chinese models, but still fall behind GPT 3.5-turbo.Apart from these three models, other China's models assessed within the English contexts perform below the average composite score of 66.2 points for all the 16 models evaluated.</p>
<p>Ranking LLMs in the Chinese contexts</p>
<p>Within the Chinese contexts, we conducted evaluations on 14 LLMs.In our analysis, we particularly discuss the performance of Chinese models compared to the GPT series.Being among the first and most advanced generative AI models developed to date, GPT series are widely acknowledged as a benchmark in the field.Due to the challenges in accessing Claude 2 and Gemini via their APIs in China and the labor-intensive nature of web-based interactions for obtaining test responses, we opted not to include these two models in the Chinese assessment.Additionally, the LLaMA model, which does not inherently support content generation in Chinese, was substituted with Qianfan-Chinese-llama-2-7B in Chinese tests.This is a Chinese-enhanced version of LLaMA 2 provided by Baidu's Qianfan team.</p>
<p>During the development of our Chinese test set, we crowdsourced a wide array of natural language proficiency test instructions and questions from ordinary users via online questionnaires.We also referenced recognized benchmarks like SuperCLUE to create additional items, ensuring these questions encompass certain cultural distinctions and linguistic traits unique to Chinese.In our assessment of disciplinary expertise in Chinese, we integrated a large number of questions from the year 2023 high school entrance exams across various provinces and cities into the secondary school level tests and included unpublished university subject exam questions for college level evaluations.The safety and responsibility test set compiled items from the Safety-Prompts dataset released by Tsinghua University and the CValues dataset by Alibaba.Overall, the Chinese test set includes over 200 open-ended questions, more than 1300 closed-ended questions, and over 200 safety testing prompts.Based on the results of human scoring for natural language proficiency and safety and responsibility, combined with the accuracy rate in the disciplinary expertise section, we derive a comprehensive performance ranking, as illustrated in Table 3. Remarkably, in this assessment, Ernie-Bot 4 exhibits the most robust overall performance, surpassing GPT 4-Turbo in aggregate scores.This can primarily be attributed to its superior performance in the disciplinary expertise tests.Tongyi Qianwen 2 secures the third position in the tests, with an overall performance nestled between GPT 4-Turbo and GPT 4. Models such as Spark3, Sensenova, Minimax, ChatGLM3, and 360GPT do not quite match the level of GPT 4, yet all surpass GPT 3.5-Turbo in their performances.</p>
<p>Nuanced Analysis of LLMs Across Dimensions</p>
<p>The detailed analysis of LLM performance in English and Chinese contexts, across various capability dimensions and difficulty levels, offers additional insights.In English contexts, the natural language proficiency of China's models generally falls behind the GPT series, including GPT 4-Turbo, GPT 4, and GPT 3.5-Turbo.In Chinese contexts, GPT 4-Turbo exhibits the best performance in natural language proficiency, followed by GPT 4, with Ernie-Bot 4 closely behind.Ernie-Bot 4 and Tongyi Qianwen 2 surpass the performance of GPT 3.5-Turbo, with Spark 3 only slightly trailing.Notably, natural language proficiency is categorized into two difficulty levels: basic and advanced language abilities.In the evaluation of Chinese basic language subtasks, human judges rate Ernie-Bot 4's abilities as comparable to those of GPT4-Turbo and GPT4, and Tongyi Qianwen 2, Spark3, and ChatGLM3 excel GPT3.5-Turbo.However, in terms of Chinese advanced language abilities, the three GPT models secure the top three positions, outperforming all contenders.This underscores the gap that still exists between China's LLMs and the leading international models in advanced natural language capabilities.</p>
<p>In the disciplinary expertise assessments within both English and Chinese contexts, most models perform better in the secondary school level subject tests, which are of relatively lower difficulty, than in the college level tests.This observation mirrors typical human learning patterns.In the closed-ended subject tests conducted in English across both difficulty levels, GPT 4-Turbo and GPT 4 achieve the highest accuracy rates.In the Chinese tests, Tongyi Qianwen 2 and Ernie-Bot 4 distinguish themselves with exceptional performance, achieving accuracy rates that exceed those of GPT-4-Turbo.This significantly contributes to these two models' high composite scores in Chinese evaluation.</p>
<p>Additionally, it's important to acknowledge the limitations of LLMs in logical reasoning and mathematical abilities.For example, in the English inference and reasoning tests, except for GPT4-Turbo, none of the models achieves an accuracy rate above 70%, with three-quarters of them falling below 60%.This shortfall is mirrored in the mathematics subject tests.Tongyi Qianwen 2, the best performer in the secondary school level mathematics test conducted in Chinese, achieves an accuracy rate of only 60.8%.In the more challenging college-level mathematics test in Chinese assessments, GPT-4 outperforms all other LLMs with an accuracy rate of only 46.5%, despite the test being in a structured multiple-choice format with four options.This underlines the challenges LLMs face in domains requiring high levels of precision and logical deduction.</p>
<p>In the safety and responsibility assessments within English contexts, LLaMA 2, Gemini Pro, and GPT4-Turbo emerge as the top performers.China's models like Tongyi Qianwen 2, Sensenova, Ernie-Bot 4, and ChatGLM3 also perform well, positioning themselves in the mid-tier among the evaluated models.In Chinese contexts, these four China models, together with Spark 3, deliver commendable performance in safety and responsibility tests, with GPT4-Turbo maintaining its high level of performance and securing the second rank in the Chinese evaluations.Overall, China's leading models demonstrate a strong commitment to safety and responsibility within Chinese contexts, highlighting the developers' attentiveness to these critical aspects.Nonetheless, their performance in non-Chinese environments still requires improvement.</p>
<p>Supplementary Assessment with LLM-as-a-Judge: Ranking Natural Language Proficiency of LLMs in Chinese Contexts</p>
<p>To validate the scores and rankings given by human judges, and to ensure our evaluation is objective, accurate, and referenceable, we utilize the LLM-as-a-judge method as a cost-effective mean to complement the aforementioned human scoring results.This approach employs a finetuned LLM as the evaluator for pairwise comparisons, as illustrated in Figure 2. In pairwise comparisons, the LLM judge is presented with a question and two corresponding answers and asked to discern which of the two is better.If the judge considers that there is no difference in quality between the two answers, he/she can declare a tie.In the pretest, we evaluated three LLM judge candidates: a fine-tuned GPT 3.5-Turbo model (the most advanced version of GPT available for fine-tuning), a fine-tuned Ernie-Bot 4, and an unfine-tuned GPT4-Turbo.Initially, we developed a training dataset for pairwise comparisons by collecting human preference judgments on a selection of Chinese answer pairs.This dataset, combined with pairwise comparison data derived from the chatbot_arena_conversations dataset2 , was used to fine-tune the two LLM judges.Subsequently, we tasked these three models with conducting pairwise comparisons on a segment of answer data and compared their judgments against the human-generated validation set.The fine-tuned GPT 3.5-Turbo exhibited significantly higher consistency with human judges than the other two, establishing it as the most appropriate judge for our evaluation practices.Statistical evidence shows that the consistency rate between this LLM and human judges exceeds 67%, and in some tasks, it even surpasses 70%.Given that agreement across human judges in pairwise comparisons ranges between 60% and 85%</p>
<p>3 in previous research, these results demonstrate the feasibility and effectiveness of using a fine-tuned GPT3.5-Turbo judge for evaluation.</p>
<p>Figure 2. LLM-as-a-judge</p>
<p>The fine-tuned GPT3.5-Turbo participated as a judge in the evaluation of natural language proficiency capabilities, such as free Q&amp;A, content generation, scenario simulation, and roleplaying.Responses from 14 large language models across these subtasks constituted over 19,000 answer pairs.Drawing upon prior research, we crafted prompts used for pairwise comparisons by the LLM judge.When making comparisons between responses to these subtasks, the LLM judge should consider factors such as helpfulness, relevance, accuracy, depth, creativity, and level of detail of the responses.To mitigate potential biases identified in prior studies, we adopted several measures.To counter position bias, which occurs when LLM judges favor the first answer presented, we alternate the order of answers within pairs: in one set, Model A's answer precedes Model B's, and in another, the order is reversed.Both sets are assessed by the LLM judge and the aggregated results determine the final ranking.Self-enhancement bias, where LLMs might favor answers generated by themselves, is avoided by anonymizing responses, using only A and B as labels.Furthermore, we specifically state in the prompt that LLM judges should strive to remain as fair and objective as possible and not let the length of responses influence their judgment.This guideline aims to mitigate verbosity bias, the tendency to prefer longer responses regardless of quality.</p>
<p>In the pairwise comparisons of all responses, the win rate statistics are shown in Figure 3.A higher number indicates a greater win rate, signifying Model A's response is more frequently preferred over Model B's response to the same question.</p>
<p>Figure 3.The win rate statistics of pairwise comparisons We calculated the average human scores for the corresponding natural language proficiency subtasks and compared them with the assessments provided by the LLM judge.The rankings obtained from both evaluation methods were found to be fundamentally consistent.Specifically, as illustrated in the win rate statistics, GPT4-Turbo is identified as the top performer, with GPT3.5-Turbo and GPT 4 also performing well.Win rate statistics presented in Figure 3 indicate Spark 3, ChatGLM3, Ernie-Bot 4, Tongyi Qianwen 2, Sensenova, and MiniMax as the standout Chinese models.In comparison, in terms of composite human scores for these subtasks, the GPT series models, including GPT4-Turbo, GPT 4, and GPT3.5-Turbo, rank in the top three.Human evaluators recognized Ernie-Bot 4, Spark 3, MiniMax, Sensenova, and Tongyi Qianwen 2 as the leading Chinese models in natural language proficiency tests.These findings demonstrate a general consistency between the two evaluation methods, further validating the commendable natural language capabilities of these models over others and highlighting the existing gap between Chinese LLMs and the leading international models.</p>
<p>Minor discrepancies arise between human and LLM judges in evaluating ChatGLM3, GPT3.5-Turbo, and GPT-4, with notable differences in the assessment of ChatGLM 3. The LLM judge rates ChatGLM 3 higher than human judges do.Upon reviewing ChatGLM 3's responses to various questions and human judges' scores, we observed that ChatGLM 3's responses to Chinese questions occasionally include English words.In such instances, human judges tend to score the answer quality lower, whereas large model judges may not penalize for this bilingual mix, leading to discrepancies in the evaluation of ChatGLM3.Additionally, human judges perceive GPT-4's performance as superior to GPT3.5-Turbo's.However, in pairwise comparisons, GPT3.5-Turbo achieves more victories in the pairwise comparisons than GPT4, which might be because the finetuned GPT3.5-Turbo judge still possesses self-reinforcement bias.The inconsistency between LLM and human judges also suggests that there might be evaluation bias.Increasing the number and variety of test prompts in future work could help mitigate this issue by reducing the impact of any single response's peculiarity on the overall assessment.</p>
<p>Main Findings and Insights</p>
<p>Building on our evaluation, we have crafted a comprehensive capability landscape of LLMs in both Chinese and English contexts, providing a detailed comparison with their international peers.This analysis sheds light on the current development of Chinese models and the state of Sino-US technology competition in the realm of LLMs.Our synthesis of scores across natural language proficiency, disciplinary expertise, and safety and responsibility reveals GPT 4-Turbo as the frontrunner in English language evaluations, closely followed by Gemini Pro and LLaMA 2. Among Chinese models assessed in English contexts, Ernie-Bot 4 took the lead, achieving the fifth overall ranking.It surpassed Claude 2 and GPT 3.5-Turbo but did not match the capabilities of GPT 4. Additionally, Sensenova and Tongyi Qianwen 2 also demonstrated commendable performance in the evaluation.Remarkably, in the model evaluation in Chinese contexts, Ernie-Bot 4 is identified as the top performer by human judges with the highest composite score, surpassing GPT 4-Turbo, the second-place holder, with Tongyi Qianwen 2 also achieving an impressive score as the third.These results are indicative of the rapid LLM advancements in China, where developers have been dedicated to the enhancement of their models' capabilities in processing and understanding the native language.</p>
<p>Our evaluation highlights the uneven development of LLMs across different languages.The performance of LLMs varies significantly within different language contexts.GPT 4-Turbo demonstrates superior proficiency in English-language tasks, which suggests that its training data and algorithms might be more finely optimized for English.In contrast, ERNIE-Bot 4 excels in Chinese language tasks, suggesting its training is more meticulously tailored to the nuances of Chinese linguistic structures and idioms, or it may benefit from a richer Chinese dataset.This disparity shows that leading models in one language context may not necessarily excel in others, indicating that a one-size-fits-all approach to LLM development may not be feasible.Hence, we advocate for a more balanced development of LLMs across different languages, emphasizing the need for models tailored for different languages and cultural backgrounds.Indeed, according to Microsoft research, around 88% of the world's languages, spoken by 1.2 billion people, lack access to Large Language Models (LLMs).This is because most LLMs built with English data and for English speakers.This English dominance also prevails in LLM development and may widen digital divide, excluding most people from the benefits of LLMs.</p>
<p>Our findings highlight the urgent need for Sino-US collaboration in the field of LLMs.The comparative analysis presented in our evaluation study reveals distinct yet complementary capabilities between American and Chinese LLMs across various dimensions.This disparity offers an opportunity for mutual learning and improvement.It is essential for both nations' R&amp;D communities to engage in knowledge exchange, utilizing our evaluation findings to identify areas of weakness and amplify existing strengths.Furthermore, our research offers valuable insights for global companies and managers interested in working with foreign LLMs, aiding in the selection of the most suitable local model tailored to their specific needs.A pertinent example is Apple's recent talks with Baidu about integrating Ernie-Bot, i.e., Baidu's flagship LLM, into the iPhone 16 for the Chinese market.Understanding the capabilities and limitations of commercial models available is vital for global corporations to strategically navigate partnerships, market entries, and product developments in a fiercely competitive global marketplace.Looking ahead, the potential for significant advancements in business applications driven by LLM is substantial.Fostering strategic partnerships between industry stakeholders and LLM providers worldwide to co-develop and expand the foundational capabilities of LLMs is crucial for realizing this transformative potential to its fullest.</p>
<p>Our evaluation framework has certain limitations that merit attention.Firstly, our model selection only includes developments up to December 2023.The rapid pace of LLM development necessitates keeping up with the recent launches of new models, such as ChatGPT-4.5Turbo and Kimi Chat, which were not included in the current assessment.Secondly, the emergence of LLMs with multimodal capabilities marks a notable advancement not fully captured by our metrics.Acknowledging this significant trend, we are committed to integrating multi-modality assessments in future evaluations.Lastly, we recognize that evaluating LLM services should not be limited to technical performance features.We plan to expand our analysis to include business-oriented aspects, such as industry-specific task performance.Stay tuned for our forthcoming evaluations.</p>
<p>Table 1 .
1
Model List
IdModelVersionDeveloperCountryEvaluation contexts English ChineseAccess Method1AquilaChatAquilaChat-7BBeijing Academy of Artificial IntelligenceChinaPPAPI2Baichuan 2baichuan2-13b-chat-v1Baichuan IntelligentChinaPPAPI3BLOOMZBLOOMZ-7BBigScienceChinaPPAPI4ChatGLM3ChatGLM3-6BTsinghua &amp; ZhipuChinaPPAPI5Claude 2Claude 2.0AnthropicUSAPWebpage6Ernie-Bot 4ERNIE-Bot4.0BaiduChinaPPAPI7GeminiGemini ProGoogleUSAPWebpage8GPT 3.5-turbogpt-3.5-turbo-0613OpenAIUSAPPAPI9GPT 4gpt-4-0613OpenAIUSAPPAPI10GPT 4-turbogpt-4-1106-previewOpenAIUSAPPAPI11Llama 2Llama 2-70BMetaUSAPAPI12MiniMaxabab5.5-chatMiniMaxChinaPPAPI13Qianfan-Chinese-Llama-2Qianfan-Chinese-Llama-2-7B 1Meta &amp; Baidu QianfanUSA &amp; ChinaPAPI14Sensenovanova-ptc-xl-v1SenseTimeChinaPPAPI15Spark 3Spark v3.0iFLYTEKChinaPPAPI16Tongyi Qianwen 2qwen-maxAlibabaChinaPPAPI17360GPT360GPT_S2_V9360ChinaPPAPI
1 A Chinese-enhanced version based on the Llama-2-7b model.</p>
<p>Table 2 .
2
Comprehensive ranking of LLMs in English contexts
RankModelNatural Language ProficiencyDisciplinary ExpertiseSafety and ResponsibilityAverage1GPT 4-Turbo91.076.878.082.92Gemini Pro86.068.281.279.03LLaMA280.160.985.175.34GPT 484.076.654.973.75Ernie-Bot 481.867.367.973.36Claude 277.965.475.273.17GPT 3.5-Turbo83.163.359.470.38Sensenova74.164.069.269.59Tongyi Qianwen 276.455.969.567.910MiniMax70.861.749.562.111Spark 370.255.356.161.512ChatGLM370.745.466.061.213Baichuan 263.753.861.659.914360GPT69.051.353.859.115AquilaChat56.826.556.647.016BLOOMZ51.432.247.344.1</p>
<p>Table 3 .
3
Comprehensive ranking of LLMs in Chinese contexts
Natural Language ProficiencyDisciplinary ExpertiseSafety and Responsibility(NLP)(DE)(S&amp;R)Overall RankModelBasic Language AbilitiesAdvanced Language AbilitiesNLP AverageSecondary School LevelCollege LevelDE AverageExplicit Malicious Prompt ScoreCamouflaged Malicious Prompt ScoreS&amp;R AverageOverall Score1Ernie-Bot 482.471.980.079.1%67.1%73.169.765.468.374.62GPT4-Turbo84.077.882.670.7%65.0%67.870.460.967.373.73Tongyi Qianwen 276.769.975.284.8%69.6%77.269.055.964.673.04GPT481.079.380.666.6%65.0%65.861.653.859.070.05Spark 372.772.472.672.2%61.1%66.766.966.166.669.16Sensenova71.172.071.368.1%58.1%63.165.759.663.766.67MiniMax71.271.271.262.4%54.1%58.262.540.955.362.78ChatGLM372.463.270.454.8%41.2%48.065.058.862.961.19360GPT69.261.467.552.2%53.4%52.858.351.556.059.610GPT3.5-Turbo71.677.973.025.7%40.6%33.264.858.562.757.411Baichuan 259.960.960.157.7%43.5%50.660.956.259.356.812Qianfan-LLaMA256.459.257.051.3%41.5%46.457.047.954.052.813AquilaChat57.354.956.823.0%25.5%24.261.057.859.947.114BLOOMZ-7B51.045.549.832.3%28.2%30.345.047.645.942.4
https://huggingface.co/datasets/lmsys/chatbot_arena_conversations
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., &amp; Stoica, I. (2023). Judging LLM-as-a-judge with MT-Bench and Chatbot Arena (arXiv:2306.05685). arXiv.</p>            </div>
        </div>

    </div>
</body>
</html>
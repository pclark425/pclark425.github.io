<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2599 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2599</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2599</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-b6a2ec8f6db40e8a84daabbcb16d6b1901469f43</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b6a2ec8f6db40e8a84daabbcb16d6b1901469f43" target="_blank">Forecasting high-impact research topics via machine learning on evolving knowledge graphs</a></p>
                <p><strong>Paper Venue:</strong> Machine Learning: Science and Technology</p>
                <p><strong>Paper TL;DR:</strong> This work shows how to predict the impact of onsets of ideas that have never been published by researchers and developed a large evolving knowledge graph that combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers.</p>
                <p><strong>Paper Abstract:</strong> The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one’s own field. While there are ways to predict a scientific paper’s future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy (AUC values beyond 0.9 for most experiments), and thereby the impact of new research directions. We envision that the ability to predict the impact of new ideas will be a crucial component of future artificial muses that can inspire new impactful and interesting scientific ideas.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2599.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2599.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Impact4Cast</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact4Cast: Forecasting high-impact research topics via machine learning on evolving knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A productionized forecasting system developed in this paper that constructs a large, evolving citation-augmented knowledge graph from papers and uses supervised ML to predict which previously-unconnected concept pairs will accumulate high citation impact in future intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Impact4Cast</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a dynamic knowledge graph whose vertices are scientific concepts (37,960 domain-specific concepts) and whose edges are co-occurrences in titles/abstracts augmented with annual and total citation counts (26,010,946 unique edges built from ~193,977,096 concept-pair occurrences across ~21M OpenAlex papers). For each candidate unconnected concept pair the system computes 141 handcrafted features (41 network features and 100 impact/citation features) and trains supervised classifiers (primary model: fully connected feed-forward neural network with four hidden layers of 600 neurons; benchmarks include a transformer, random forest, and XGBoost) to predict whether the pair will exceed an impact-range threshold (IR) within a fixed future window (typically 3 years). It supports personalized filtering (restricting candidates to researcher interests) and outputs ranked high-impact suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery / Impact-Forecasting System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Science of science / research-forecasting; specifically applied to quantum physics and optics concepts (but built to be general across natural sciences using preprint and OpenAlex data).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given two scientific concepts that have never co-occurred in the literature up to time y, predict whether the concept pair will accumulate at least IR citations in the time window y+1..y+3 (or other evaluation intervals). Secondary tasks: discriminate between low-impact and high-impact among pairs that do form links later (i.e., predict citation magnitude conditional on link formation).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: operates on a large combinatorial search space (for domain: ~193.98M concept-pair occurrences; full KG with 37,960 nodes and 26M edges). The classification task is high-dimensional (141 features) and stochastic due to citation noise and temporal drift. Multi-scale complexity: link-existence prediction (sparse, discrete) plus citation magnitude prediction (heavy-tailed). Quantitative measures included: training dataset of ~689M unconnected pairs for one experiment, evaluation set of 10M random unconnected pairs; IR thresholds ranged from 1 to 200.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Extensive pre-existing data: metadata and abstracts from 2.44M preprints (arXiv, bioRxiv, medRxiv, chemRxiv) used to extract concepts; full citation-augmented edges built from an OpenAlex snapshot of ~92M papers (21M papers containing at least two concepts). The final domain knowledge graph occupies ~23.12 GB. Quality: curated concept list (manual+automated cleaning) of 37,960 domain-specific concepts; OpenAlex provides yearly citation counts from 2012 onward (note: OpenAlex excludes yearly citations older than 10 years). No new experimental data generation was required; data curation and preprocessing were non-trivial and memory-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Preprocessing and feature generation used a machine with Intel Xeon Gold 6130 and 1 TiB RAM (high RAM used for efficiency). Neural-network training runs used one NVIDIA Quadro RTX 6000 GPU; each training run for a given IR ≈ 1.5 hours. Transformer benchmark runs ≈ 3 hours on the same GPU. Benchmarking other models ran on CPU with <15 GB memory and ≈1 hour per task. Storage: compressed OpenAlex ~68 GB; full KG ~23.12 GB. Training dataset sizes: up to hundreds of millions of candidate pairs; evaluation sets e.g., 10M pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Supervised binary classification (discrete labels defined by IR thresholds), stochastic citation dynamics (non-deterministic, heavy-tailed), time-evolving graph input (temporal snapshots summarized by time-windowed features), well-defined evaluation metric (ROC AUC). Problem is open-ended in candidate generation (large combinatorial pair space) but well-specified in objective (predict citations exceeding threshold). Requires domain-agnostic feature engineering and temporal awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Area Under the ROC Curve (AUC) for classification of whether a pair exceeds the IR threshold within the prediction interval; ancillary metrics: ranked list quality (mean citations of top-N predicted pairs), true positive/false positive rates at example thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>High performance reported: AUC values beyond 0.8 and often >0.9 across many tasks and IR values. Example concrete numbers from this paper: evaluation AUC ≈ 0.948 for IR = 100 (training on 2016 → predict 2019→2022 evaluation), AUC ≈ 0.94 shown for IR = 10 in Fig.11. Benchmarks across 27 task variations report AUCs exceeding 90% in many settings. The highest-ranked predicted concept pairs have mean citations several orders of magnitude above average in evaluation sets (top predictions >3 orders of magnitude higher than baseline mean of 0.029 citations in a 10M-pair eval).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limitations identified in the paper: reliance on citations as a proxy for 'impact' (citations are an imperfect metric); inability to predict truly novel concepts (the system forecasts pairs among a fixed concept list); performance degrades for longer prediction intervals (models trained on certain interval lengths perform worse when applied to much longer forecast intervals); potential drift in citation patterns over time (system may require retraining regularly); handcrafted features limit exploiting full graph structure (end-to-end graph methods might outperform); synonymy/noisy concept extraction can create redundant nodes (mitigated but not fully eliminated). Also social/ethical limitations: suggestions could favor already-prominent concepts or risk promoting high-risk, low-feasibility interdisciplinarity without feasibility checks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large-scale, high-quality data (OpenAlex + curated concept list), the use of citation-augmented edges (temporal citation sequences) to directly supervise impact prediction, rich handcrafted features combining network topology and citation dynamics (141 features), balanced sampling of positives/negatives during training, and benchmarking multiple ML architectures. Temporal framing (training on historical snapshots and evaluating on later unseen intervals) validated generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Benchmarks compared a feed-forward fully-connected NN, a transformer, random forest, and XGBoost across multiple training/evaluation interval combinations and IR values. All four models achieved strong performance (AUCs often >0.90); the feed-forward NN (their primary model) is effective and computationally efficient; transformer had similar parameter count but required longer GPU runtime (~3h). They also find models trained on lower IR (more diverse, larger positive sets) can generalize to predict higher IR thresholds better than models trained only on high-IR examples (training on IR=10 generalized to IR=50 slightly better than training on IR=50 alone). Performance decreases when making forecasts further into the future than training intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting high-impact research topics via machine learning on evolving knowledge graphs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2599.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2599.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models (examples: GPT-4, Gemini, LLaMA-2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose large language models named in the paper as natural first-choice AI assistants for reading and acting upon literature; the authors note limitations in scientific reasoning and uncertain reliability for idea suggestion and impact evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large Language Models (GPT-4, Gemini, LLaMA-2, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Transformer-based pre-trained language models that can generate text, summarize papers, and propose ideas or research directions. The paper references them as candidates for AI-assisted idea generation but notes that they often struggle with robust scientific reasoning and that their ability to suggest novel, reliably-impactful research ideas is unclear in the near term.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Assistant / Hypothesis/Idea Generation (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General scientific literature understanding and idea suggestion across domains (general science).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Propose new scientific ideas or act as assistants to read/interpret literature and suggest directions; the paper frames them as possible components for future AI muses but does not instantiate or evaluate them here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Capable of handling high-dimensional textual inputs but known weaknesses in rigorous scientific reasoning, factual consistency, and evaluation of feasibility or scientific impact; complexity and search space depend on prompt and available context.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Pretrained on massive text corpora (not quantified in this paper); not further trained or benchmarked within this study.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper; external LLM compute costs are high but not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended generative tasks (creative, continuous outputs); evaluation metrics for idea quality / impact are poorly defined and were not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not specified in this paper; the authors note current difficulty in assessing LLMs' ability to generate and reliably evaluate new impactful ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper notes they 'often struggle in scientific reasoning' and their suggestions/evaluations of scientific impact are of unclear reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large pretraining datasets and strong language generation capabilities; potential when coupled with domain-specific tools or structured knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>No quantitative comparison in this paper; LLMs are mentioned as a complementary approach but not benchmarked against Impact4Cast or other systems here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting high-impact research topics via machine learning on evolving knowledge graphs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2599.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2599.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rzhetsky2015_semnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic-network-driven experiment selection (Rzhetsky et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early example cited in the paper where a semantic network of biomolecules was used to identify promising experiment choices to accelerate collective discovery in biochemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Choosing experiments to accelerate collective discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Semantic-network experiment selection (Rzhetsky et al. 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The cited work builds semantic networks whose nodes represent biomolecules and edges indicate joint investigation, and uses this structure to recommend experiments or exploration strategies intended to accelerate discovery across the community.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Experiment/Exploration Prioritization System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Biochemistry / experimental planning and exploration</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Select experiments or research directions that efficiently accelerate collective discovery in biochemical networks by leveraging the semantic connectivity of biomolecules.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Network-based decision-making over a combinatorial space of possible experiments; complexity arises from large molecular interaction spaces and uncertain experimental outcomes. The paper cites this prior work as an early demonstration of deriving non-trivial conclusions from large semantic networks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Pre-existing bibliographic and content data in the original study; this paper does not provide dataset sizes or details beyond citing the work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Structured as graph-based exploration/selection; evaluation of success relates to community-level acceleration of discovery rather than single-run deterministic outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not detailed in this paper; original work aimed to quantify acceleration of discovery or efficiency of exploration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper (refer to original Rzhetsky et al. 2015 for empirical results).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed in this paper; possible limitations include dependence on the quality of extracted semantic relationships and applicability across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large-scale semantic network, domain-appropriate encoding of concepts and co-occurrences, and leveraging community-level bibliographic signals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>This paper cites the work as an early successful application of semantic networks but provides no comparative metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting high-impact research topics via machine learning on evolving knowledge graphs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2599.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2599.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Science4Cast / link-prediction systems</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Science4Cast-style link prediction and previous semantic/neural network systems (e.g., Krenn & Zeilinger 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior systems and competitions that framed 'what scientists will work on' as link-prediction in evolving semantic networks; these works achieved high-quality link-prediction in domain settings such as quantum physics and AI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Predicting research trends with semantic and neural networks with an application in quantum physics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Link-prediction systems (Science4Cast approaches; semantic+neural networks)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approaches that create time-evolving semantic networks from co-occurrence data and apply network features, machine learning or neural networks to predict which pairs of concepts will be jointly investigated in the future (i.e., link prediction). The present paper builds on these methods for link prediction but extends them to forecast citation impact of newly formed links.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Link-Prediction / Research Trend Forecasting System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Science of science; previously applied to quantum physics and AI research corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict future co-occurrence (links) between concepts in the literature, effectively forecasting 'what scientists will work on' in coming years.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Combinatorial link-prediction in large evolving graphs; sparse positive examples relative to the huge candidate pair space; temporal non-stationarity.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses large bibliographic datasets and semantic extractions from titles/abstracts; specific datasets vary by study.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified here; prior competitions and publications used standard ML/GNN approaches with varying compute.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Binary link-existence prediction (discrete); well-defined evaluation via historical holdout and AUC-like metrics; less focused on downstream impact or citation magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Primarily ROC AUC for link prediction and ranking metrics for top-k predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Described as 'high prediction quality' in prior works; this paper cites those successes but does not re-report their numerical results beyond referencing high AUCs in its own experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Link prediction alone does not indicate scientific impact; predicting links is easier than predicting citation magnitudes (the present paper emphasizes this distinction).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large-scale datasets, informative network/topological features, time-aware training, use of past co-occurrence dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>The present work extends link prediction by adding citation-augmented edges and by predicting impact thresholds; it finds that correctly predicting impact requires additional features beyond standard link-prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting high-impact research topics via machine learning on evolving knowledge graphs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2599.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2599.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contextualized literature-based discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to generate novel scientific directions with contextualized literature-based discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited recent approach (Wang et al., 2023) that aims to generate novel scientific directions from literature; mentioned as an example of custom models for suggestion of research directions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to generate novel scientific directions with contextualized literature-based discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Contextualized literature-based discovery (Wang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A literature-based discovery approach that attempts to generate novel scientific directions by contextualizing existing literature and proposing new connections; cited as an example of a custom model for idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Idea Generation / Literature-based Discovery System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Scientific idea generation across disciplines (not evaluated within this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Produce novel scientific directions by mining and contextualizing existing literature, effectively generating hypotheses or research agendas.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Open-ended creative generation; assessing novelty and impact is inherently difficult; model must navigate huge textual corpora and weigh novelty vs. feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Presumably trained on large literature corpora; this paper does not provide dataset details for that cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Generative and open-ended; requires downstream evaluation metrics (novelty, impact) that are hard to operationalize.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not reported in this paper; original paper would contain their own evaluation metrics (e.g., human evaluation, retrieval/novelty measures).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper notes general challenges for such models: uncertain scientific reasoning and unclear reliability of suggested ideas; no specific failure modes from the cited work are presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Access to large literature, good contextualization mechanisms, and complementary evaluation strategies (e.g., human-in-the-loop assessments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>No comparative details provided in this paper; cited as complementary to the semantic-graph forecasting approach developed here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Forecasting high-impact research topics via machine learning on evolving knowledge graphs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Choosing experiments to accelerate collective discovery <em>(Rating: 2)</em></li>
                <li>Predicting research trends with semantic and neural networks with an application in quantum physics <em>(Rating: 2)</em></li>
                <li>Learning to generate novel scientific directions with contextualized literature-based discovery <em>(Rating: 2)</em></li>
                <li>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network <em>(Rating: 2)</em></li>
                <li>On scientific understanding with artificial intelligence <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2599",
    "paper_id": "paper-b6a2ec8f6db40e8a84daabbcb16d6b1901469f43",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "Impact4Cast",
            "name_full": "Impact4Cast: Forecasting high-impact research topics via machine learning on evolving knowledge graphs",
            "brief_description": "A productionized forecasting system developed in this paper that constructs a large, evolving citation-augmented knowledge graph from papers and uses supervised ML to predict which previously-unconnected concept pairs will accumulate high citation impact in future intervals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Impact4Cast",
            "system_description": "Constructs a dynamic knowledge graph whose vertices are scientific concepts (37,960 domain-specific concepts) and whose edges are co-occurrences in titles/abstracts augmented with annual and total citation counts (26,010,946 unique edges built from ~193,977,096 concept-pair occurrences across ~21M OpenAlex papers). For each candidate unconnected concept pair the system computes 141 handcrafted features (41 network features and 100 impact/citation features) and trains supervised classifiers (primary model: fully connected feed-forward neural network with four hidden layers of 600 neurons; benchmarks include a transformer, random forest, and XGBoost) to predict whether the pair will exceed an impact-range threshold (IR) within a fixed future window (typically 3 years). It supports personalized filtering (restricting candidates to researcher interests) and outputs ranked high-impact suggestions.",
            "system_type": "Automated Discovery / Impact-Forecasting System",
            "problem_domain": "Science of science / research-forecasting; specifically applied to quantum physics and optics concepts (but built to be general across natural sciences using preprint and OpenAlex data).",
            "problem_description": "Given two scientific concepts that have never co-occurred in the literature up to time y, predict whether the concept pair will accumulate at least IR citations in the time window y+1..y+3 (or other evaluation intervals). Secondary tasks: discriminate between low-impact and high-impact among pairs that do form links later (i.e., predict citation magnitude conditional on link formation).",
            "problem_complexity": "High: operates on a large combinatorial search space (for domain: ~193.98M concept-pair occurrences; full KG with 37,960 nodes and 26M edges). The classification task is high-dimensional (141 features) and stochastic due to citation noise and temporal drift. Multi-scale complexity: link-existence prediction (sparse, discrete) plus citation magnitude prediction (heavy-tailed). Quantitative measures included: training dataset of ~689M unconnected pairs for one experiment, evaluation set of 10M random unconnected pairs; IR thresholds ranged from 1 to 200.",
            "data_availability": "Extensive pre-existing data: metadata and abstracts from 2.44M preprints (arXiv, bioRxiv, medRxiv, chemRxiv) used to extract concepts; full citation-augmented edges built from an OpenAlex snapshot of ~92M papers (21M papers containing at least two concepts). The final domain knowledge graph occupies ~23.12 GB. Quality: curated concept list (manual+automated cleaning) of 37,960 domain-specific concepts; OpenAlex provides yearly citation counts from 2012 onward (note: OpenAlex excludes yearly citations older than 10 years). No new experimental data generation was required; data curation and preprocessing were non-trivial and memory-intensive.",
            "computational_requirements": "Preprocessing and feature generation used a machine with Intel Xeon Gold 6130 and 1 TiB RAM (high RAM used for efficiency). Neural-network training runs used one NVIDIA Quadro RTX 6000 GPU; each training run for a given IR ≈ 1.5 hours. Transformer benchmark runs ≈ 3 hours on the same GPU. Benchmarking other models ran on CPU with &lt;15 GB memory and ≈1 hour per task. Storage: compressed OpenAlex ~68 GB; full KG ~23.12 GB. Training dataset sizes: up to hundreds of millions of candidate pairs; evaluation sets e.g., 10M pairs.",
            "problem_structure": "Supervised binary classification (discrete labels defined by IR thresholds), stochastic citation dynamics (non-deterministic, heavy-tailed), time-evolving graph input (temporal snapshots summarized by time-windowed features), well-defined evaluation metric (ROC AUC). Problem is open-ended in candidate generation (large combinatorial pair space) but well-specified in objective (predict citations exceeding threshold). Requires domain-agnostic feature engineering and temporal awareness.",
            "success_metric": "Area Under the ROC Curve (AUC) for classification of whether a pair exceeds the IR threshold within the prediction interval; ancillary metrics: ranked list quality (mean citations of top-N predicted pairs), true positive/false positive rates at example thresholds.",
            "success_rate": "High performance reported: AUC values beyond 0.8 and often &gt;0.9 across many tasks and IR values. Example concrete numbers from this paper: evaluation AUC ≈ 0.948 for IR = 100 (training on 2016 → predict 2019→2022 evaluation), AUC ≈ 0.94 shown for IR = 10 in Fig.11. Benchmarks across 27 task variations report AUCs exceeding 90% in many settings. The highest-ranked predicted concept pairs have mean citations several orders of magnitude above average in evaluation sets (top predictions &gt;3 orders of magnitude higher than baseline mean of 0.029 citations in a 10M-pair eval).",
            "failure_modes": "Limitations identified in the paper: reliance on citations as a proxy for 'impact' (citations are an imperfect metric); inability to predict truly novel concepts (the system forecasts pairs among a fixed concept list); performance degrades for longer prediction intervals (models trained on certain interval lengths perform worse when applied to much longer forecast intervals); potential drift in citation patterns over time (system may require retraining regularly); handcrafted features limit exploiting full graph structure (end-to-end graph methods might outperform); synonymy/noisy concept extraction can create redundant nodes (mitigated but not fully eliminated). Also social/ethical limitations: suggestions could favor already-prominent concepts or risk promoting high-risk, low-feasibility interdisciplinarity without feasibility checks.",
            "success_factors": "Large-scale, high-quality data (OpenAlex + curated concept list), the use of citation-augmented edges (temporal citation sequences) to directly supervise impact prediction, rich handcrafted features combining network topology and citation dynamics (141 features), balanced sampling of positives/negatives during training, and benchmarking multiple ML architectures. Temporal framing (training on historical snapshots and evaluating on later unseen intervals) validated generalization.",
            "comparative_results": "Benchmarks compared a feed-forward fully-connected NN, a transformer, random forest, and XGBoost across multiple training/evaluation interval combinations and IR values. All four models achieved strong performance (AUCs often &gt;0.90); the feed-forward NN (their primary model) is effective and computationally efficient; transformer had similar parameter count but required longer GPU runtime (~3h). They also find models trained on lower IR (more diverse, larger positive sets) can generalize to predict higher IR thresholds better than models trained only on high-IR examples (training on IR=10 generalized to IR=50 slightly better than training on IR=50 alone). Performance decreases when making forecasts further into the future than training intervals.",
            "human_baseline": null,
            "uuid": "e2599.0",
            "source_info": {
                "paper_title": "Forecasting high-impact research topics via machine learning on evolving knowledge graphs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLMs",
            "name_full": "Large Language Models (examples: GPT-4, Gemini, LLaMA-2)",
            "brief_description": "General-purpose large language models named in the paper as natural first-choice AI assistants for reading and acting upon literature; the authors note limitations in scientific reasoning and uncertain reliability for idea suggestion and impact evaluation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Large Language Models (GPT-4, Gemini, LLaMA-2, etc.)",
            "system_description": "Transformer-based pre-trained language models that can generate text, summarize papers, and propose ideas or research directions. The paper references them as candidates for AI-assisted idea generation but notes that they often struggle with robust scientific reasoning and that their ability to suggest novel, reliably-impactful research ideas is unclear in the near term.",
            "system_type": "AI Assistant / Hypothesis/Idea Generation (LLM)",
            "problem_domain": "General scientific literature understanding and idea suggestion across domains (general science).",
            "problem_description": "Propose new scientific ideas or act as assistants to read/interpret literature and suggest directions; the paper frames them as possible components for future AI muses but does not instantiate or evaluate them here.",
            "problem_complexity": "Capable of handling high-dimensional textual inputs but known weaknesses in rigorous scientific reasoning, factual consistency, and evaluation of feasibility or scientific impact; complexity and search space depend on prompt and available context.",
            "data_availability": "Pretrained on massive text corpora (not quantified in this paper); not further trained or benchmarked within this study.",
            "computational_requirements": "Not specified in this paper; external LLM compute costs are high but not quantified here.",
            "problem_structure": "Open-ended generative tasks (creative, continuous outputs); evaluation metrics for idea quality / impact are poorly defined and were not evaluated in this paper.",
            "success_metric": "Not specified in this paper; the authors note current difficulty in assessing LLMs' ability to generate and reliably evaluate new impactful ideas.",
            "success_rate": "Not quantified in this paper.",
            "failure_modes": "Paper notes they 'often struggle in scientific reasoning' and their suggestions/evaluations of scientific impact are of unclear reliability.",
            "success_factors": "Large pretraining datasets and strong language generation capabilities; potential when coupled with domain-specific tools or structured knowledge graphs.",
            "comparative_results": "No quantitative comparison in this paper; LLMs are mentioned as a complementary approach but not benchmarked against Impact4Cast or other systems here.",
            "human_baseline": null,
            "uuid": "e2599.1",
            "source_info": {
                "paper_title": "Forecasting high-impact research topics via machine learning on evolving knowledge graphs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Rzhetsky2015_semnet",
            "name_full": "Semantic-network-driven experiment selection (Rzhetsky et al., 2015)",
            "brief_description": "An early example cited in the paper where a semantic network of biomolecules was used to identify promising experiment choices to accelerate collective discovery in biochemistry.",
            "citation_title": "Choosing experiments to accelerate collective discovery",
            "mention_or_use": "mention",
            "system_name": "Semantic-network experiment selection (Rzhetsky et al. 2015)",
            "system_description": "The cited work builds semantic networks whose nodes represent biomolecules and edges indicate joint investigation, and uses this structure to recommend experiments or exploration strategies intended to accelerate discovery across the community.",
            "system_type": "Automated Experiment/Exploration Prioritization System",
            "problem_domain": "Biochemistry / experimental planning and exploration",
            "problem_description": "Select experiments or research directions that efficiently accelerate collective discovery in biochemical networks by leveraging the semantic connectivity of biomolecules.",
            "problem_complexity": "Network-based decision-making over a combinatorial space of possible experiments; complexity arises from large molecular interaction spaces and uncertain experimental outcomes. The paper cites this prior work as an early demonstration of deriving non-trivial conclusions from large semantic networks.",
            "data_availability": "Pre-existing bibliographic and content data in the original study; this paper does not provide dataset sizes or details beyond citing the work.",
            "computational_requirements": "Not specified in this paper.",
            "problem_structure": "Structured as graph-based exploration/selection; evaluation of success relates to community-level acceleration of discovery rather than single-run deterministic outcomes.",
            "success_metric": "Not detailed in this paper; original work aimed to quantify acceleration of discovery or efficiency of exploration strategies.",
            "success_rate": "Not quantified in this paper (refer to original Rzhetsky et al. 2015 for empirical results).",
            "failure_modes": "Not discussed in this paper; possible limitations include dependence on the quality of extracted semantic relationships and applicability across domains.",
            "success_factors": "Large-scale semantic network, domain-appropriate encoding of concepts and co-occurrences, and leveraging community-level bibliographic signals.",
            "comparative_results": "This paper cites the work as an early successful application of semantic networks but provides no comparative metrics.",
            "human_baseline": null,
            "uuid": "e2599.2",
            "source_info": {
                "paper_title": "Forecasting high-impact research topics via machine learning on evolving knowledge graphs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Science4Cast / link-prediction systems",
            "name_full": "Science4Cast-style link prediction and previous semantic/neural network systems (e.g., Krenn & Zeilinger 2020)",
            "brief_description": "Prior systems and competitions that framed 'what scientists will work on' as link-prediction in evolving semantic networks; these works achieved high-quality link-prediction in domain settings such as quantum physics and AI.",
            "citation_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
            "mention_or_use": "mention",
            "system_name": "Link-prediction systems (Science4Cast approaches; semantic+neural networks)",
            "system_description": "Approaches that create time-evolving semantic networks from co-occurrence data and apply network features, machine learning or neural networks to predict which pairs of concepts will be jointly investigated in the future (i.e., link prediction). The present paper builds on these methods for link prediction but extends them to forecast citation impact of newly formed links.",
            "system_type": "Link-Prediction / Research Trend Forecasting System",
            "problem_domain": "Science of science; previously applied to quantum physics and AI research corpora.",
            "problem_description": "Predict future co-occurrence (links) between concepts in the literature, effectively forecasting 'what scientists will work on' in coming years.",
            "problem_complexity": "Combinatorial link-prediction in large evolving graphs; sparse positive examples relative to the huge candidate pair space; temporal non-stationarity.",
            "data_availability": "Uses large bibliographic datasets and semantic extractions from titles/abstracts; specific datasets vary by study.",
            "computational_requirements": "Not specified here; prior competitions and publications used standard ML/GNN approaches with varying compute.",
            "problem_structure": "Binary link-existence prediction (discrete); well-defined evaluation via historical holdout and AUC-like metrics; less focused on downstream impact or citation magnitudes.",
            "success_metric": "Primarily ROC AUC for link prediction and ranking metrics for top-k predictions.",
            "success_rate": "Described as 'high prediction quality' in prior works; this paper cites those successes but does not re-report their numerical results beyond referencing high AUCs in its own experiments.",
            "failure_modes": "Link prediction alone does not indicate scientific impact; predicting links is easier than predicting citation magnitudes (the present paper emphasizes this distinction).",
            "success_factors": "Large-scale datasets, informative network/topological features, time-aware training, use of past co-occurrence dynamics.",
            "comparative_results": "The present work extends link prediction by adding citation-augmented edges and by predicting impact thresholds; it finds that correctly predicting impact requires additional features beyond standard link-prediction.",
            "human_baseline": null,
            "uuid": "e2599.3",
            "source_info": {
                "paper_title": "Forecasting high-impact research topics via machine learning on evolving knowledge graphs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Contextualized literature-based discovery",
            "name_full": "Learning to generate novel scientific directions with contextualized literature-based discovery",
            "brief_description": "A cited recent approach (Wang et al., 2023) that aims to generate novel scientific directions from literature; mentioned as an example of custom models for suggestion of research directions.",
            "citation_title": "Learning to generate novel scientific directions with contextualized literature-based discovery",
            "mention_or_use": "mention",
            "system_name": "Contextualized literature-based discovery (Wang et al., 2023)",
            "system_description": "A literature-based discovery approach that attempts to generate novel scientific directions by contextualizing existing literature and proposing new connections; cited as an example of a custom model for idea generation.",
            "system_type": "Automated Idea Generation / Literature-based Discovery System",
            "problem_domain": "Scientific idea generation across disciplines (not evaluated within this paper).",
            "problem_description": "Produce novel scientific directions by mining and contextualizing existing literature, effectively generating hypotheses or research agendas.",
            "problem_complexity": "Open-ended creative generation; assessing novelty and impact is inherently difficult; model must navigate huge textual corpora and weigh novelty vs. feasibility.",
            "data_availability": "Presumably trained on large literature corpora; this paper does not provide dataset details for that cited work.",
            "computational_requirements": "Not specified in this paper.",
            "problem_structure": "Generative and open-ended; requires downstream evaluation metrics (novelty, impact) that are hard to operationalize.",
            "success_metric": "Not reported in this paper; original paper would contain their own evaluation metrics (e.g., human evaluation, retrieval/novelty measures).",
            "success_rate": "Not quantified in this paper.",
            "failure_modes": "Paper notes general challenges for such models: uncertain scientific reasoning and unclear reliability of suggested ideas; no specific failure modes from the cited work are presented here.",
            "success_factors": "Access to large literature, good contextualization mechanisms, and complementary evaluation strategies (e.g., human-in-the-loop assessments).",
            "comparative_results": "No comparative details provided in this paper; cited as complementary to the semantic-graph forecasting approach developed here.",
            "human_baseline": null,
            "uuid": "e2599.4",
            "source_info": {
                "paper_title": "Forecasting high-impact research topics via machine learning on evolving knowledge graphs",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Choosing experiments to accelerate collective discovery",
            "rating": 2
        },
        {
            "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
            "rating": 2
        },
        {
            "paper_title": "Learning to generate novel scientific directions with contextualized literature-based discovery",
            "rating": 2
        },
        {
            "paper_title": "Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network",
            "rating": 2
        },
        {
            "paper_title": "On scientific understanding with artificial intelligence",
            "rating": 1
        }
    ],
    "cost": 0.018307749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Forecasting high-impact research topics via machine learning on evolving knowledge graphs</h1>
<p>Xuemei $\mathrm{Gu}^{1, *}$ and Mario Krenn ${ }^{1, \dagger}$<br>${ }^{1}$ Max Planck Institute for the Science of Light, Staudtstrasse 2, 91058 Erlangen, Germany</p>
<h4>Abstract</h4>
<p>The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy (AUC values beyond 0.9 for most experiments), and thereby the impact of new research directions. We envision that the ability to predict the impact of new ideas will be a crucial component of future artificial muses that can inspire new impactful and interesting scientific ideas.</p>
<h2>INTRODUCTION</h2>
<p>As we see an explosion in the number of scientific articles [1-4], it becomes increasingly challenging for researchers to find new impactful research directions beyond their own expertise. Consequently, researchers might have to focus on narrow subdisciplines. A tool that can read and intelligently act upon scientific literature could be an enormous aid to individual scientists in choosing their next new and high-impact research project, which - on a global scale - could significantly accelerate science itself.</p>
<p>These days, a natural first choice for an AI-assistant would be powerful large-language-models (LLMs) such as GPT-4 [5], Gemini [6], LLaMA-2 [7] or custom-made models [8]. However, these models often struggle in scientific reasoning, and it remains unclear how they can suggest new scientific ideas or evaluate their impact in a reliable way in the near term.</p>
<p>An alternative and complementary approach is to build scientific semantic knowledge graphs. Here, the nodes represent scientific concepts and the edges are formed when two concepts are researched together in a scientific paper [2]. While this approach extracts only small amounts of information from each paper, surprisingly non-trivial conclusions can be drawn if the underlying dataset of papers is large. An early example of this is a work in biochemistry [9]. The authors use their semantic network, where nodes represent biomolecules, to find new potentially more efficient exploration strategies for the bio-chemistry community on a global scale. In these semantic networks, an edge between two concepts indicates that researchers have jointly investigated these research concepts. The edges are drawn from papers, thus they are created at a specific time when the paper was published. In this way, one creates an evolving semantic</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>network that captures what researchers have investigated in the past. With such an evolving network, one can ask how the network might evolve in the future. In the scientific context, this question can be reformulated into what scientists will research in the future. For example, if two nodes do not share an edge, one can ask whether they will share an edge in the next three years - or, alternatively, whether scientists will investigate these two concepts jointly within three years. This question, denoted as a link-prediction problem in network theory [10], has been successfully demonstrated with high prediction quality for semantic networks in the field of quantum physics [11] and artificial intelligence [4]. These works focus on the question what scientists will work on, completely leaving out which of these topics will be impactful.</p>
<p>Impact in the scientific community is often approximated (for lack of better metrics [12, 13]) by citations [1, 2, 14, 15], including exciting results that find interpretable mathematical models to describe citation evolution [16-19]. Beside concrete mathematical modelling, impact of scientific papers has also been predicted using advanced statistical and machine-learning methods that use meta-data such as including authors and affiliations [20], the content and the references of the paper [21, 22]. Techniques employed for the predictions of individual paper impact using a combination of characteristics include support-vector machines [23], regression [24-26], dense [27] or graph neural networks [28].</p>
<p>The prediction of a paper's impact however is only possible after the research is completed, and long after its underlying idea is created. A true scientific assistant or muse however should contribute at the earliest stage of the scientific cycle, when the idea for the next impactful research project is born. One solution is the prediction at the concept level. Specifically, we can ask the question Which scientific concepts, that have never been investigated jointly, will lead to the most impactful research?.</p>
<p>In this work, we answer this question by combining semantic networks and citation networks that are purely</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>FIG. 1. Generation of the knowledge graph with time and citation information. Vertices are formed by scientific concepts, which are extracted from scientific papers (titles and abstracts) from prominent academic preprint servers. Edges are formed when concepts are investigated jointly in a scientific publication. There are 21,165,421 out of 92,764,635 papers from OpenAlex which form at least one edge. The edges are augmented with citation information, which acts as a proxy for impact in our work. A mini-knowledge graph (blue edges) is constructed from four randomly selected papers (p1-p4) [29–32] from OpenAlex as an example. Here, c<sub>p4</sub> represents the total citations of paper p4 since its publication, and c<sub>p4</sub>(y) is its annual citations from 2018 to 2022 (e.g., c<sub>p4</sub>(2018) = 4). The citation value of the edge is the sum of the all papers creating the edge.</p>
<p>Based on the level of scientific concepts<sup>1</sup>. Specifically, we develop a large evolving knowledge graph using more than 21 million scientific papers, from 1709 (starting with a letter by Antoni van Leeuwenhoek [38]) to April 2023. The vertices of the knowledge graph are scientific concepts and the edges between two concepts contain information about when these topics have been investigated and how often they have been cited subsequently. We then train a machine learning model on the historic evolution of the knowledge graph. We find that the neural network can predict with high accuracy which concept pairs, that have never been jointly investigated before in any scientific paper, will be highly cited in the future. Being able to predict the potential impact of new research ideas – before the paper is written or the research is done or even started – could be a cornerstone in future scientific AI-assistants that help humans broadening their horizon of possible new research endeavours [39].</p>
<h2>RESULTS</h2>
<p>Creating a list of scientific concepts – At the heart of our knowledge graph are scientific concepts, as depicted in Fig. 1. We chose not to rely on existing concept lists, such as the APS or computer science ontology [40], for several reasons. Firstly, our goal is to ultimately cover all natural sciences comprehensively, and a universal list encompassing this breadth doesn't currently exist. Secondly, we want to capture the most recent concepts that might be absent from existing lists. Lastly, generating our list ensures that we have a granular understanding and control over the concepts.</p>
<p>To build our concept list, we started with 2,444,442 papers from four publicly available preprint servers: arXiv, bioRxiv, medRxiv, and chemRxiv. We use papers from preprint servers for two reasons: (1) It contains papers that are not published yet in journals, thus our dataset also contains state-of-the-art concepts; (2) they associate</p>
<p><sup>1</sup> GitHub: Impact4Cast</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>FIG. 2. Fastest growing citations of concepts and concept pairs: Evolution of citations over three years for the top-fastest growing, previously uncited concepts (a) and concept pairs (b). We find many revolutionary topics in the realm of quantum physics and optics research in the last decade, including Perovskite devices [33], the emergence of complex and non-hermitian topology [34], the introduction of advanced concepts of machine learning in physics [35–37] and quasi-BIC (bound state in continuum) resonances.</p>
<p>papers to research categories, which can be used to focus on specific scientific domains (as we do, with the field of quantum physics and optics). The data cutoff is February 2023. From these, we extracted titles and abstracts of the papers. To single out concept candidates from this extensive collection, we applied the Rapid Automatic Keyword Extraction (RAKE) algorithm based on statistical text analysis to automatically detect important keywords [41] (see details in the Appendix B). Each of these candidates are ranked Concepts with two words, like <em>phase transition</em>, were retained if they appear in at least 9 papers, while longer concepts, such as <em>single molecule localization microscopy</em>, needed to appear in at least 6 papers. In this way, we can increase the fraction of high-quality concepts. We further developed a suite of natural language processing tools to refine the concepts, followed by manual inspection to remove any incorrectly identified ones. Finally, we got a list which contains over 368,000 concepts. We focus here on concepts specific to the sub-field of optics and quantum physics (representing roughly 10% of the entire concepts), but our method can immediately be translated to any other domain. This refined domain-specific concept list serves as the vertices of our knowledge graph.</p>
<p>Creating an evolving, citation-augmented knowledge graph – Now that we have the vertices, we can create edges that contain information from the scientific literature. We get the citation information from papers in OpenAlex [42], an open-source database containing detailed information on more than 92 million publications. Edges are drawn when two concepts co-occur in the title or abstract of a scientific paper. If a paper connects two vertices, the weight of the newly formed edge is the paper's annual citation numbers from 2012 to 2023 together with the total citation number since its publication. If more than one paper creates an edge, then the edge contains the sum of the annual citations (as well as the sum of the total citations) gained by all papers. As research papers appear over time, and their citations are created in time, we effectively build an evolving, citation-augmented knowledge graph that evolves in time (see Fig. 1). From these 92 million papers, 21 million contain at least two concepts of our concept list and can therefore for an edge in the knowledge graph.</p>
<p>The final constructed knowledge graph has 37,960 vertices with more than 26 million edges (built from 190 million concept pairs, containing multi-edges when multiple papers create the same edge) from the OpenAlex dataset, with a data cutoff at April 2023.</p>
<p>In Fig. 2, we show the fastest growing (in terms of citation) concepts and concept pairs since 2012, where we can recognize many highly influential topics in quantum physics and optics research.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>FIG. 3. Forecasting the impact of new research connections. Network and citation features from unconnected vertex pairs from 2016 are used as input to a neural network. The citation information from 2019 is used as a supervision signal to train the neural network. After training, we evaluate the neural network's abilities by applying it to unconnected vertex pairs from 2019, aiming to predict the developments in 2022 – a task involving data the network has never encountered before.</p>
<p>Forecasting impact of newly created concept connections – With an evolving knowledge graph from the past, we can formulate the prediction of impact for new concept pairs as a supervised learning task, as illustrated in Fig. 3. For a vertex pair that has not had any connection in the year 2016, we predict whether three years later this pair accumulated more than a certain number of citations. Using the historical knowledge graph, we possess an ideal supervision signal for our binary classification task. During the training phase, we selected pairs of vertices that were not connected and calculated 141 features for each pair. These features include 41 network features, divided into 20 node features (such as the number of neighbors and PageRank [43] over the past three years) and 21 edge features (including cosine, geometric, and Simpson similarities [44]). Additionally, we incorporated 100 impact features: 58 of these are node citation features, covering total citations and yearly citations within the last three years. The other 42 features are about vertex pairs and include measures such as the citation ratio between them. Detailed feature description are available in GitHub: Impact4Cast and Tables I and II in Appendix D. The network features are inspired by the winner of the <em>Science4Cast competition</em> [11, 45], and the citation features are developed empirically and could potentially be improved by careful feature importance analysis. Our neural network is a fully connected feed-forward network with four hidden layers of 600 neurons each. The exploration of more advanced architectures might improve the prediction qualities further. The neural network has to predict whether the unconnected vertex pair in 2019 will have at least <em>IR</em> citations (<em>IR</em> stands for the impact range).</p>
<p>The impact range (IR) is a threshold representing the minimum number of citations a concept pair must accumulate within a specified time frame (e.g., three years) to be classified as "high-impact". For instance, an <em>IR</em> = 100 means that only concept pairs with at least 100 citations during the defined period are considered impactful. This binary threshold simplifies the problem into a classification task, making it computationally tractable while providing a clear measure of success. Predicting individual citation counts is inherently noisy due to the stochastic nature of citation dynamics. By using IR, we focus on identifying high-impact trends, avoiding fluctuations of precise citation counts. IR provides a clear and measurable target for classification. This allows us to use metrics like the Area Under the Curve (AUC) of Receiver Operating Characteristics (ROC) curves to evaluate the prediction quality [46].</p>
<p>We perform the training for different values of the impact range IR from <em>IR</em> = 1 to <em>IR</em> = 200, and then quantify the quality with AUC of the ROC curves [46]. The AUC gives a measure of classification quality and stands for the probability that a randomly chosen true example is ranked higher than a randomly chosen false example. A random classifier has <em>AUC</em> = 0.5. We measure the AUC for a test set (which contains unconnected pairs not in the training set) for a prediction from 2016 to 2019, and for an evaluation dataset, with 10 million random data from 2019 to 2022 (while keeping the training data of the neural network from 2016 to 2019). The evaluation dataset shows how well the neural network performs on future, never-seen datasets. This is motivated by our goal that ultimately we want to train a neural network with all available data (let's say, until January 2023) and predict what happens until the future in 2026. In Fig. 4(a), we find that the AUC scores for both the test set and the evaluation set are beyond 0.8, in most of the cases beyond 0.9, for different <em>IR</em>. We can conclude that the neural network can forecast a high impact of previously never-investigated concept connections to a high degree. In Fig. 4(b), we sort the concept pairs of the evaluation dataset with the neural network (<em>IR</em> = 100), and plot their true citation counts. We further divide the 10 million evaluation dataset into 20 equal parts and plot their average citation count (represented by green bars) for each 5% segment. This clearly demonstrates good predictions at the individual concept pair level. As seen in Fig. 4(c), the highest predicted concept pairs indeed get more than 3 orders of magnitude more citations than the average citation of all 10 million pairs.</p>
<p>Forecasting genuine impact beyond link prediction – Next, we perform an even more challenging, gen-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>FIG. 4. Evaluating the machine-learning-based impact forecast. (a): Classification of unconnected pairs, whether they will exceed a certain threshold three years later. Training data contains unconnected vertex pairs from 2016 and the supervision signal according to their impact range <em>IR</em> 3 years later. The test dataset also includes pairs from 2016, but excludes those in the training set. A more challenging evaluation set contains unconnected pairs from 2019, with outcomes verified in 2022, importantly noting that the neural network was only trained on data from 2016 to 2019, not 2019 to 2022. We quantify the quality using the AUC of the ROC curve. For example, <em>IR</em> = 100, i.e., (&lt; 100, &gt;= 100), refers to whether the 3-year citation counts after 2016 (test) or after 2019 (eval) is at least 100. TPR (true positive rate) measures how often a test correctly identifies a true positive, while FPR (false positive rate) measures how often it correctly identifies a true negative. (b): Norted predictions of the neural network on the evaluation set (blue curve in (a)) shows the very high quality prediction at the level of individual concept pairs. The y-axis stands for the respective fraction of the evaluation dataset (10<sup>7</sup> data points). The histogram is separated into 20 equal bins. No fitting is involved. In (c), we show the average citation of the first N highest predicted concept pairs. This plot shows impressively that the highest predicted concept pairs indeed have very high citation, more than 3 orders of magnitude higher than the average citation of all 10<sup>7</sup> pairs (0.029 citations). (d): This more challenging step shows that citation prediction goes beyond link predictions. Here we take unconnected vertex pairs, conditioned on a connection 3 years later. The neural network is tasked to classify these concept pairs in low or high citations, revealing that it is not just predicting new links, but is learning intrinsic citation features. Here <em>IR</em> = [5, 100], i.e., (0 − 5, &gt;= 100), means whether the 3-year citation count after 2016 (test) or after 2019 (eval) is at most 5 or at least 100.</p>
<p>The input data is collected in a 3-year dataset, which contains 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>FIG. 5. Network features vs. predicted impact. A randomly selected set of 100,000 unconnected pairs until January 2023 is used. The color represents the neural network's prediction of each concept pair's impact. (a): The y-axis shows cosine similarity, indicating the semantic similarity between concepts; lower values represent concept pairs that are semantically distinct. For each concept pair (u, v), cosine similarity is calculated as the number of shared neighbors divided by the square root of the product of the number of neighbors of u and v. The x-axis is the average vertex degree of the two concepts in the knowledge graph, reflecting their overall prominence. Concepts with low similarity and low degree yet predicted to have high impact could be surprising and offer interesting suggestions. (b): The x-axis represents the average number of new neighbors each concept gained over the last three years. Concept pairs with low similarity and few new neighbors but high impact predictions might highlight potentially overlooked but intriguing ideas. (c): The x-axis denotes citation density (average citations per paper mentioning the concepts). Pairs with low similarity and citation density but high predicted impact could again indicate overlooked potential ideas. (d): Citation counts for concept 1 (x-axis) and concept 2 (y-axis) over last three years are plotted on a logarithmic scale. We can easily identify concept pairs predicted to have high impact in the future, even though they have individually received few citations in the past.</p>
<p>2019 with supervision signal in 2022) sort them by impact predictions. We find that the highest predicted pair is renewable energy and cancer cell. This prediction is a very high-risk bet. For more practical, personalized suggestions, one can restrict the unconnected concept pairs to those related to specific scientists or research groups, aiming for high-impact collaboration suggestions. By examining the published works of scientists to identify their research interests, it becomes possible to identify concept pairs where one aligns with one scientist's specialty and the other with another scientist's. Thereby, one can suggest potential collaborations of high impact. As an example, by constraining the personalized research interests of scientists in experimental quantum optics and one researcher in biophysics, the highest predicted impact concepts pairs are 'microfluidic channel' with 'Kerr resonator', 'SARS CoV' with 'quantum enhanced sensitivity' or 'electron microscopy' with 'quantum vacuum field'. These suggestions can be further refined based on their similarity (e.g., represented by the cosine similarity) or the prominence of the concepts (indicated by the node degree), as we show in Fig. 5. Here, we plot 100,000 concept pairs that have not been studied together until January 2023 and use the neural network trained on 2019 dataset to predict their impact. The points are plotted based on various properties, such as the similarity between concepts, their prominence within the network, their growth rate in the network (reflected in newly acquired neighbors), and how often the concepts have been cited previously. Plotting in this way allows us to identify rare outliers – concept pairs with high predicted impact that have unique properties, such</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>FIG. 6. Benchmarking fully connected NNs, Transformers, Random Forest, and XGBoost on 27 variations of the prediction task, with 2-4 year training and 1-5 year evaluation intervals, across two different impact ranges (IR).</p>
<p>As the bright yellow spots highlighted in the insets of Fig. 5. These methods help us narrow down the enor</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>FIG. 7. Predictions for varying intervals without retraining. The four ML models were trained using data from 2014 to 2017 and then used to predict outcomes 1 to 5 years into the future without retraining. For one task, the number of positive cases was insufficient for meaningful classification (we set a threshold of at least 10 positive cases out of 1 million total cases).</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>FIG. 8. Prediction of higher IR. The models are tasked with predicting whether concept pairs will receive at least 50 citations. Training is performed on data from 2014 to 2017, with evaluation conducted over intervals of 2, 3, 4, or 5 years (a 1-year interval lacks sufficient data). The blue line represents the performance of a fully connected neural network trained specifically for this task. In contrast, the red line represents a fully connected neural network (with identical architecture and training parameters) trained to solve the task for IR = 10 instead. Interestingly, the red model achieves slightly higher AUC values, indicating that predictions for higher impact ranges (IR = 50) benefit from training on more diverse data, including those from lower impact ranges.</p>
<p>mously large number of possibilities into a small number of personalized and targeted suggestions, which could inspire new ideas. In practical application, it will be useful to update the knowledge graph regularly, and train the machine learning models on the latest knowledge graph data, so it can better incorporate the latest trends and discoveries for its predictions.</p>
<h3>Benchmarking different models and different time intervals</h3>
<p>So far, we have only focused on a specific case: a training interval from 2016 to 2019 (3 years) and an evaluation interval from 2019 to 2022 (3 years). Additionally, we have primarily investigated the performance of feed-forward neural networks. Exploring other models and examining their predictive capabilities across various training and evaluation intervals could provide deeper insights into model performance. To do so, we expanded our study to include benchmarking on a small dataset of 1 million pairs. This benchmarking incorporates the previously used fully connected neural network alongside additional models, including a transformer architecture [47, 48], random forest [49], and XGBoost [50].</p>
<p>The feed-forward neural network, implemented using PyTorch [51], consisted of three hidden layers, each with 600 neurons and ReLU activations [52], resulting in approximately 800,000 trainable parameters. Similarly, the transformer architecture [47, 48] also implemented via PyTorch, was designed with 4 layers, a hidden size of 128, 4 attention heads, and a feedforward dimension of 512, resulting in approximately 800,000 trainable parameters. Positional encodings were added to the input features. Both neural network models were trained using Adam optimizer [53] with a batch size of 2048 and a learning rate of 0.0001. The random forest classifier, implemented with scikit-learn [54], was trained with 300 trees, a mini-</p>
<p>mum of 25 samples required to split a node, and 10 samples per leaf. The XGBoost model was trained using up to 2000 boosting rounds, a learning rate of 0.01 , and a maximum tree depth of 10. Hyperparameters for all models were selected via a hyperparameter search for a single benchmark task (training: 2016-2019, evaluation: 2019-2022, with $I R=10$ ) and kept constant for all tasks.</p>
<p>In all tasks, the models are provided with 141 input features of a specific concept pair and have to predict whether this pair will receive more or fewer IR citations ( $I R=10$ or $I R=50$ ) in certain future years. To achieve this, the four models are trained on 2-, 3-, and 4-year intervals and evaluated on intervals ranging from 1 to 5 years. For example, if the training interval spans 2 years and the evaluation interval spans 5 years, the models are trained using data from 2015 to 2017 to predict whether unconnected concept pairs will receive IR citations. They are then evaluated using 2017 data to make predictions for 2022. After training, the models are evaluated on 1 million concept pairs, predicting the likelihood of each pair receiving more than IR citations. These predictions are ranked (from high to low likelihood) and compared against the ground truth to compute the ROC curve and the AUC score, which measures prediction quality. As shown in Fig.6, the models achieve AUC values exceeding $90 \%$ in these tasks. In a slightly modified task, we train the models on the data from 2014 to 2017 and evaluate them from 2017 to 1-5 years into the future (2018 to 2022). This test analyzes how well the prediction perform for intervals on which the models have not been trained and how difficult it is to predict further into the future. As shown in Fig.7, the quality of the predictions indeed decreases for larger intervals.</p>
<p>In Fig.8, we show that models trained on smaller impact ranges can predict higher impact ranges even slightly better than those trained exclusively on higher impact ranges. This might be due to more diverse training data (there are many more examples of $I R=10$ than of $I R=50$, because many more concept pairs achieve at least 10 citations rather than 50). It might also be explained by a systematic drift in citation patterns between the years the models were trained and the years they are applied, potentially due to the growing number of overall citations. It will be very interesting in the future to explore and understand this effect further and to develop new ML methods that could leverage this dynamic.</p>
<p>All of our models use the same set of input features and do not directly access the full knowledge graph. Developing techniques that can leverage more general graph properties - such as automatically learning features or generating embeddings - would be an interesting avenue to explore. A related approach was demonstrated in a previous competition [4], where the task was to predict the future state of a knowledge graph. There, the graph's edges represented co-occurrences of concepts in scientific papers. In contrast, the knowledge graph used in our study is more complex, with edges also weighted by the number of citations received by concept pairs. Explor-
ing more end-to-end approaches that integrate more information from the entire knowledge graph could reveal whether such methods can outperform the models and hand-crafted features demonstrated in this work.</p>
<h2>DISCUSSION</h2>
<p>We show how to forecast the impact of future research topics. Although we view this as a significant step towards developing truly useful AI-driven assistants, achieving this goal requires numerous further advancements. Firstly, developing methods to extract more complex information from each paper will be crucial, for instance by employing hyper-graph structures that carry more information from each paper [55, 56], which has already been demonstrated to lead to exciting results in other domains [2, 57, 58]. The forecast itself could benefit from more genuine dynamical features that go beyond network snapshots from different years [59], or the application of dynamic word embedding [60]. This might also allow for the forecast of new concepts [61, 62] and their impact. Incorporating the recent dataset [63, 64] into our research could also allow us to explore more complex data structures than those used in our paper. Secondly, it will be interesting to approximate impact with metrics that go beyond citations - which is a crucial topic in computational sociology and the study of the science of science $[1,2]$. Additionally, introducing metrics of surprise, as discussed in $[65,66]$, could serve as a complementary metric to citation prediction for ranking suggestions. Finally, while the suggestion of impactful new ideas might be a key component of future AI assistants, it will be crucial to study its relation to the scientific interest of working researchers [67].</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>The authors thank Burak Gurlek for interesting discussions at the start of this project, and the organizers of OpenAlex, arXiv, bioRxiv, chemRxiv, and medRxiv for making scientific resources freely and readily accessible. X.G acknowledges the support from the Alexander von Humboldt Foundation.</p>
<p>Author Contributions X.G. and M.K. designed research; X.G. performed research and analyzed data; and X.G. and M.K. wrote the manuscript.</p>
<p>Competing Interests The authors declare that they have no competing financial interests.</p>
<p>Data availability statement Data is accessible on Zenodo at https://doi.org/10.5281/zenodo.10692137 [68]. Benchmark data used in our work is also available on Zenodo at https://doi.org/10.5281/zenodo.14527306 [69]. Codes for this work are available on GitHub at https://github.com/artificial-scientist-lab/Impact4Cast.</p>
<h2>Appendix A: Datasets for the knowledge graph</h2>
<p>To compile a list of scientific concepts in natural science, we used metadata from four major preprint servers: arXiv, bioRxiv, medRxiv, and chemRxiv. The arXiv dataset can be directly downloaded from Kaggle, while metadata from bioRxiv, medRxiv, and chemRxiv are accessible through their APIs. The full methodology and codes are available on the GitHub: Impact4Cast. Our comprehensive dataset encompasses approximately 2.44 million papers, including 78,084 from arXiv's physics.optics and quant-ph categories, which were specifically utilized for identifying domain concepts.</p>
<p>For edge generation, we used the OpenAlex database snapshot, available for download in OpenAlex bucket. More details can be found at the OpenAlex documentation site. The complete dataset occupies around 330 GB, expanding to approximately 1.6 TB when decompressed. Our interest was specifically in scientific journal papers that include publication time, title, abstract, and citation information. By focusing on these criteria, we managed to reduce the dataset to a more manageable gzip-compressed size of 68 GB , comprising around 92 million scientific papers. From these 92 million papers, 21 million contain at least two concepts of our final concept list and can therefore for an edge in the knowledge graph.</p>
<h2>Appendix B: Details on concept and edge generation</h2>
<p>From the preprint dataset of $\sim 2.44$ million papers, we analyzed each article's title and abstract using the RAKE algorithm.</p>
<p>RAKE works by proposing candidate key phrases from each sentence by splitting the sentence at punctuation marks and stop words. Let's take the sentence: Recurrent neural networks have significantly improved the accuracy of image recognition in large-scale scientific collaboration networks. RAKE splits this into the candidate phrases Recurrent neural networks, improved, accuracy, image recognition, and large-scale scientific collaboration networks. Each individual word within these candidate phrases receives a score based on two metrics: its frequency (how often it occurs in candidate phrases; here only networks occurs twice, all others once) and its degree (the number of co-occurrences with other words in candidate phrases). For example, the word networks has a higher degree because it appears in multiple longer phrases, thus increasing its importance. The final keyword phrases are ranked based on the summed scores of their individual words, naturally favoring longer, meaningful phrases such as large-scale scientific collaboration networks or recurrent neural networks.</p>
<p>RAKE's candidates are then used for subsequent analysis. We filtered out concepts to retain only those with two words that appeared in nine or more articles, and those with three or more words that appeared in six or more articles. This step significantly reduced the noise
from the RAKE-generated concepts, yielding a refined list of 726,439 relevant concepts. To further enhance the quality of the identified concepts, we developed a suite of automatic tools designed to identify and eliminate common, domain-independent errors often associated with RAKE. In addition, we conduct a manual review to identify and eliminate any inaccuracies in the concepts. The entire process, which included eliminating non-conceptual phrases, verbs, ordinal numbers, conjunctions, and adverbials, resulted in a full list of 368,825 concepts.</p>
<p>We note that our approach might result in similar or synonymous concepts appearing as separate key phrases due to slight variations or phrasing differences. Although multiple entries in the extracted concept list may represent essentially the same idea, synonymity can effectively be identified by leveraging structural information within the knowledge graph. Specifically, network cosine similarity between nodes (concepts) reveals the degree of similarity in their network neighborhoods, indicating semantic closeness. Practically, this can be utilized to enhance impact forecasting: when recommending new high-impact concept pairs, applying a cosine similarity threshold ensures that suggested pairs have sufficiently distinct meanings, thus avoiding redundancy from synonymous phrases.</p>
<p>We then specifically focused on articles within the physics.optics and quant-ph categories from arXiv to extract domain-specific concepts. Iterating this entire list of concepts to these domain-specific articles, we identified 87,741 relevant concepts. Employing our specially designed automated filtering tool for initial refinement and then conducting a thorough manual review to remove inaccuracies, we narrowed the list down to 37,960 high-quality, domain-specific concepts.</p>
<p>As an example, we show the extraction of concepts for the four papers used in Fig. 1:</p>
<ol>
<li>Accurate and rapid background estimation in single-molecule localization microscopy using the deep neural network BGnet [29]: 'super resolution reconstruction', 'neural network', 'single molecule tracking', 'deep neural net', 'deep neural network', 'localization microscopy', 'biological structure', 'point source', 'point spread function', 'single molecule localization microscopy', 'optical microscopy', and 'neural net'.</li>
<li>Machine learning for cluster analysis of localization microscopy data [30]: 'neural network', 'supervised machine learning', 'spatial relation', 'localization microscopy', 'single molecule localization microscopy', 'neural net', and 'machine learning'.</li>
<li>
<p>Constraints on cosmic strings using data from the first Advanced LIGO observing run [31]: 'phase transition', 'cosmic string', 'gravitational wave', 'cosmic microwave', 'cosmic microwave background', 'topological defect', 'string theory', and 'ring theory'.</p>
</li>
<li>
<p>Learning phase transitions from dynamics [32]: 'neural network', 'recurrent network', 'time crystalline phase', 'phase transition', 'localization transition', 'spin chain', 'recurrent neural net', 'ct model', 'recurrent neural network', 'phase diagram', 'crystalline phase', 'neural net', and 'recurrent net'.</p>
</li>
</ol>
<p>We created concept pairs, or edges, from the OpenAlex dataset, by detecting when domain-specific concepts co-occurred in paper titles or abstracts. This yielded 193,977,096 concept pairs (including multi-edges) across about 21 million papers. Each edge receives a time-stamp based on its paper's publication date, converted to the number of days since January 1, 1990. The final full knowledge graph comprises 26,010,946 unique edges after merging multiple edges between the same concept pairs. The citation information for an edge includes the paper's yearly citations from 2012 to 2023, alongside its total citation since publication. The OpenAlex dataset excludes yearly citations older than ten years, hence the focus on this specific ten-year time frame due to the absence of data prior to 2012. For edges formed by multiple papers, the edge weight combines the annual and total citations from all contributing papers.</p>
<p>Consider the edge formed by the concepts 'single molecule localization microscopy' and 'neural net', generated from paper $p 1$ [29] published on January 7, 2020. The time-stamp for this edge is derived from the days elapsed since January 1, 1990. The citation metrics for this edge includes the total and yearly citations from each contributing paper. Paper $p 1$ with 38 citations $\left(c_{p 1}=38\right)$, contributes yearly citations represented as $c_{p 1}\left(y_{i}\right)$ for $i=2023,2022, \ldots, 2012$, with actual values ${5,8,16,9}$ for 2023 to 2020, and zeros for previous years, culminating in a citation sequence ${5,8,16,9,0,0,0,0,0,0,0,0}$. Similarly, paper $p 2$ [30], published on March 20, 2020, with 43 citations $\left(c_{p 2}=43\right)$, adds its yearly citations ${6,16,14,7}$ for the same period (2023 to 2020). The aggregated citation data for this edge, combining $c_{p 1}$ and $c_{p 2}$, yields a total of 81 citations, with an annual citation sequence of ${11,24,30,16,0,0,0,0,0,0,0,0,0}$.</p>
<h2>Appendix C: Details on training</h2>
<p>Our neural network consists of six fully connected layers, which include four hidden layers with 600 neurons each. The network inputs are 141 features for each unconnected concept pair $\left(v_{i}, v_{j}\right)$, denoted as $p_{i, j}=$ $\left(p_{i, j}^{1}, p_{i, j}^{2}, \ldots, p_{i, j}^{141}\right)$, where each $p_{i, j} \in \mathbb{R}$. For instance, $p_{i, j}^{1}$ and $p_{i, j}^{2}$ represent the vertex degree of concepts $v_{i}$ and $v_{j}$ for the current year, $y$. Detailed feature description and feature generation code are available in GitHub: Impact4Cast.</p>
<p>In our training process for the year 2016 to predict impact in 2019, we prepared a dataset comprising approximately 689 million unconnected concept pairs. The</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>FIG. 9. Loss curves for one typical example. The training and test loss curves correspond to a fully connected neural network trained on data from 2017 to 2020, with an impact range (IR) of 10.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>FIG. 10. Positive, negative samples, and their ratio in the 10M evaluation dataset (2019-2022) versus IR.
goal was to evaluate these pairs to determine whether their 3 -year citation counts would have at least $I R$ citations ( $I R$ is impact range) or not. From this extensive collection, we selected all positive samples (the 3 -year citation counts are at least $I R$ ). An equivalent number of negative samples were then randomly chosen to match the size of the positive set. The refined dataset was subsequently divided, allocating $85 \%$ for training and $15 \%$ for testing purposes. For the evaluation dataset in 2019, which aims to predict the impact in 2022, we randomly selected 10 million unconnected pairs. Our neural network was trained using the Adam optimizer with a learning rate of $3 \times 10^{-5}$ and a mini-batch size of 1000 . In every training batch, we randomly chose an equal number of positive and negative samples from the training set. This approach was also applied to our 2019 training process for predictions into 2022, where the trained neural network is used for future forecasting. An example loss curve is shown in Fig.9. An example for the number of positive and negative cases of the evaluation dataset (for</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>FIG. 11. ROC curve explanation. The ROC curve are plotted for the evaluation data in Fig.4, which illustrates the performance of binary classifiers discussed in our paper. On the left, for IR = 10, the area under the curve (AUC) is 0.94. The x-axis represents the False Positive Rate (FPR), while the y-axis represents the True Positive Rate (TPR). The curve demonstrates how the classifier's performance changes with variations in the classification threshold, which determines whether a case is classified as Class 0 or Class 1. For instance, at a threshold that gives a classifier with FPR=0.1, the TPR is 0.83, and at FPR=0.3, the TPR is 0.96. Therefore, the ROC curve is more informative than just a single pair of FPR and TPR. On the right, the ROC curves for IR=10, 50, and 100 are shown.</p>
<p>The experiments in Fig.4) is shown in Fig.10. In Fig.11 we explain more details of the ROC curve, specifically its relation to false positive rate (FPR) and true positive rate (TPR).</p>
<p>The full dynamic knowledge graph, along with the data required for feature preparation and evaluation, was processed on an Intel Xeon Gold 6130 CPU with 1 TiB of RAM. However, it is not strictly necessary to have 1 TiB of RAM for this process with the relatively small concept list of 37,960; the high memory capacity was utilized for efficiency and to handle additional operations concurrently. The final domain knowledge graph in this work occupies approximately 23.12 GB of storage. It is worth noting that knowledge graphs built from larger concept lists will require more memory, as the data size and complexity increase.</p>
<p>The neural network training was conducted on a standard single GPU (Nvidia Quadro RTX 6000), with each training run for different impact ranges taking approximately 1.5 hours. For benchmarking, all models – except the transformer model – were run on a standard CPU (Intel Xeon Gold 6130) with memory usage below 15 GB. Each benchmarking task took roughly one hour to complete. The transformer model, however, was run on a single GPU (Nvidia Quadro RTX 6000), taking approximately 3 hours per task.</p>
<h3>Appendix D: Individual feature's predictive ability</h3>
<p>In Fig. 4 (a), we observe an AUC score of 0.948 for the 2019 evaluation dataset with the neural network that</p>
<p>uses all 141 features, trained on 2016 dataset and impact range IR = 100. To explore the predictive ability of individual features, we trained separate neural networks on each feature using the same 2016 dataset, and then applied the 2019 evaluation dataset to these models. This resulted in 141 individual predictions, each from a network trained on a single feature. The features were ranked by their impact predictions, shown in the Fig. 12. Details of the features are shown in Tables I and I, and the corresponding documentation and source code are available at GitHub: Impact4Cast.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>FIG. 12. Neural network performance across individual features. The highest-performing four features are the Simpson similarity coefficient for the unconnected pair $(u, v)$ across the years $y, y-1$, and $y-2$, and the cosine similarity coefficient for unconnected pairs $(u, v)$ in year $y$ (i.e., $y=2016$ ), with AUC scores of $0.8880,0.8795,0.8720$, and 0.8683 , respectively. In contrast, the lowest predictive three features are the average total citation count up to year $y$ for vertex $v$, and the total citation count for the pair $(u, v)$ up to years $y-1$ and $y-2$, with AUC scores of $0.5219,0.5234$, and 0.5285 . Using all 141 features together leads to a significant improvement in the AUC score to 0.948 , showing that the combination of all features works better.</p>
<p>TABLE I. Detailed description of all customized features for an unconnected concept pair $(u, v)$ for the year $y$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Feature Index</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">node feature</td>
<td style="text-align: center;">0-5</td>
<td style="text-align: center;">Number of neighbors for each node ( $u$ or $v$ ) until the year $y, y-1, y-2$ denoted as $N_{u, y}, N_{v, y}, N_{u, y-1}, N_{v, y-1}, N_{u, y-2}$, and $N_{v, y-2}$, ordered as indices $0-5$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6-7</td>
<td style="text-align: center;">Number of new neighbors for each node ( $u$ or $v$ ) between year $y-1$ and $y$ i.e., $N_{u, y}-N_{u, y-1}$ and $N_{v, y}-N_{v, y-1}$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8-9</td>
<td style="text-align: center;">Number of new neighbors for each node ( $u$ or $v$ ) between year $y-2$ and $y$ i.e., $N_{u, y}-N_{u, y-2}$ and $N_{v, y}-N_{v, y-2}$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$10-11$</td>
<td style="text-align: center;">Rank of the number of new neighbors for each node ( $u$ or $v$ ) between year $y-1$ and $y$ i.e., $\operatorname{rank}\left(N_{u, y}-N_{u, y-1}\right)$ and $\operatorname{rank}\left(N_{v, y}-N_{v, y-1}\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$12-13$</td>
<td style="text-align: center;">Rank of the number of new neighbors for each node ( $u$ or $v$ ) between year $y-2$ and $y$ i.e., $\operatorname{rank}\left(N_{u, y}-N_{u, y-1}\right)$ and $\operatorname{rank}\left(N_{v, y}-N_{v, y-2}\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$14-19$</td>
<td style="text-align: center;">PageRank scores of each node ( $u$ or $v$ ) until the year $y, y-1, y-2$ denoted and ordered as $\mathrm{PR}<em v_="v," y="y">{u, y}, \mathrm{PR}</em>}, \mathrm{PR<em v_="v," y-1="y-1">{u, y-1}, \mathrm{PR}</em>}, \mathrm{PR<em v_="v," y-2="y-2">{u, y-2}$ and $\mathrm{PR}</em>$.</td>
</tr>
<tr>
<td style="text-align: center;">node citation feature</td>
<td style="text-align: center;">20-25</td>
<td style="text-align: center;">Yearly citation for each node ( $u$ or $v$ ) in year $y, y-1, y-2$ denoted and ordered as $\mathrm{C}<em v_="v," y="y">{u, y}, \mathrm{C}</em>}, \mathrm{C<em v_="v," y-1="y-1">{u, y-1}, \mathrm{C}</em>}, \mathrm{C<em v_="v," y-2="y-2">{u, y-2}$ and $\mathrm{C}</em>$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">26-31</td>
<td style="text-align: center;">Total citation for each node ( $u$ or $v$ ) since the first publication to the year $y, y-1$, and $y-2$ denoted and ordered as $\mathrm{Ct}<em v_="v," y="y">{u, y}, \mathrm{Ct}</em>}, \mathrm{Ct<em v_="v," y-1="y-1">{u, y-1}, \mathrm{Ct}</em>}, \mathrm{Ct<em v_="v," y-2="y-2">{u, y-2}$ and $\mathrm{Ct}</em>$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">32-37</td>
<td style="text-align: center;">Total citations for each node ( $u$ or $v$ ) in the last 3 years ending in the year $y, y-1$, and $y-2$ denoted and ordered as $\mathrm{Ct}<em v_="v," y="y">{u, y}^{\Delta 3}, \mathrm{Ct}</em>}^{\Delta 3}, \mathrm{Ct<em v_="v," y-1="y-1">{u, y-1}^{\Delta 3}, \mathrm{Ct}</em>}^{\Delta 3}, \mathrm{Ct<em v_="v," y-2="y-2">{u, y-2}^{\Delta 3}$, and $\mathrm{Ct}</em>$.}^{\Delta 3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$38-43$</td>
<td style="text-align: center;">Number of papers mentioning node $u$ from the first publication to the year $y, y-1$, and $y-2$, similar for node $v$; denoted and ordered as $\mathrm{Pn}<em v_="v," y="y">{u, y}, \mathrm{Pn}</em>}, \mathrm{Pn<em v_="v," y-1="y-1">{u, y-1}, \mathrm{Pn}</em>}, \mathrm{Pn<em v_="v," y-2="y-2">{u, y-2}$, and $\mathrm{Pn}</em>$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$44-49$</td>
<td style="text-align: center;">Average yearly citations for each node ( $u$ or $v$ ) in the year $y, y-1, y-2$ denoted and ordered as $\mathrm{Cm}<em v_="v," y="y">{u, y}, \mathrm{Cm}</em>}, \mathrm{Cm<em v_="v," y-1="y-1">{u, y-1}, \mathrm{Cm}</em>}, \mathrm{Cm<em v_="v," y-2="y-2">{u, y-2}$ and $\mathrm{Cm}</em>}$ e.g., $\mathrm{Cm<em u_="u," y="y">{u, y}=\mathrm{C}</em>$} / \mathrm{Pn}_{u, y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$50-55$</td>
<td style="text-align: center;">Average total citations for each node ( $u$ or $v$ ) since the first publications to the years $y$, $y-1, y-2$; denoted and ordered as $\mathrm{Ctm}<em v_="v," y="y">{u, y}, \mathrm{Ctm}</em>}, \mathrm{Ctm<em v_="v," y-1="y-1">{u, y-1}, \mathrm{Ctm}</em>}, \mathrm{Ctm<em v_="v," y-2="y-2">{u, y-2}$ and $\mathrm{Ctm}</em>} ;$ e.g., $\mathrm{Ctm<em u_="u," y="y">{u, y}=\mathrm{Ct}</em>$} / \mathrm{Pn}_{u, y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$56-61$</td>
<td style="text-align: center;">Average total citations for each node ( $u$ or $v$ ) in the last 3 years ending in the year $y$, $y-1$, and $y-2$; denoted and ordered as $\mathrm{Ctm}<em v_="v," y="y">{u, y}^{\Delta 3}, \mathrm{Ctm}</em>}^{\Delta 3}, \mathrm{Ctm<em v_="v," y-1="y-1">{u, y-1}^{\Delta 3}, \mathrm{Ctm}</em>}^{\Delta 3}, \mathrm{Ctm<em u_="u," y-2="y-2">{u, y-2}^{\Delta 3}$ and $\mathrm{Ctm}</em>}^{\Delta 3} ;$ e.g., $\mathrm{Ctm<em u_="u," y="y">{u, y}^{\Delta 3}=\mathrm{Ct}</em>$}^{\Delta 3} / \mathrm{Pn}_{u, y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$62-63$</td>
<td style="text-align: center;">New citations for each node ( $u$ or $v$ ) between years $y-1$ and $y$ i.e., $\mathrm{Ct}<em u_="u," y-1="y-1">{u, y}-\mathrm{Ct}</em>}$ and $\mathrm{Ct<em v_="v," y-1="y-1">{v, y}-\mathrm{Ct}</em>$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$64-65$</td>
<td style="text-align: center;">New citations for each node ( $u$ or $v$ ) between years $y-2$ and $y$ i.e., $\mathrm{Ct}<em u_="u," y-2="y-2">{u, y}-\mathrm{Ct}</em>}$ and $\mathrm{Ct<em v_="v," y-2="y-2">{v, y}-\mathrm{Ct}</em>$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$66-67$</td>
<td style="text-align: center;">Rank of the new citations for each node ( $u$ or $v$ ) between years $y-1$ and $y$ i.e., $\operatorname{rank}\left(\mathrm{C}<em u_="u," y-1="y-1">{u, y}-\mathrm{C}</em>}\right)$ and $\operatorname{rank}\left(\mathrm{C<em v_="v," y-1="y-1">{v, y}-\mathrm{C}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$68-69$</td>
<td style="text-align: center;">Rank of the new citations for each node ( $u$ or $v$ ) between years $y-2$ and $y$ i.e., $\operatorname{rank}\left(\mathrm{C}<em u_="u," y-2="y-2">{u, y}-\mathrm{C}</em>}\right)$ and $\operatorname{rank}\left(\mathrm{C<em v_="v," y-2="y-2">{v, y}-\mathrm{C}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$70-71$</td>
<td style="text-align: center;">Number of papers mentioning nodes $u$ between years $y-1$ and $y$, similar for node $v$ i.e., $\mathrm{PR}<em u_="u," y-1="y-1">{u, y}-\mathrm{PR}</em>}$ and $\mathrm{PR<em v_="v," y-1="y-1">{v, y}-\mathrm{PR}</em>$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$72-73$</td>
<td style="text-align: center;">Number of papers mentioning nodes $u$ between years $y-2$ and $y$, similar for node $v$ i.e., $\mathrm{PR}<em u_="u," y-2="y-2">{u, y}-\mathrm{PR}</em>}$ and $\mathrm{PR<em v_="v," y-2="y-2">{v, y}-\mathrm{PR}</em>$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$74-75$</td>
<td style="text-align: center;">Rank of the number of papers mentioning nodes $u$ between years $y-1$ and $y$, similar for node $v$; i.e., $\operatorname{rank}\left(\mathrm{PR}<em u_="u," y-1="y-1">{u, y}-\mathrm{PR}</em>}\right)$ and $\operatorname{rank}\left(\mathrm{PR<em v_="v," y-1="y-1">{v, y}-\mathrm{PR}</em>\right)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$76-77$</td>
<td style="text-align: center;">Number of papers mentioning nodes $u$ between years $y-2$ and $y$, similar for node $v$ i.e., $\operatorname{rank}\left(\mathrm{PR}<em u_="u," y-2="y-2">{u, y}-\mathrm{PR}</em>}\right)$ and $\operatorname{rank}\left(\mathrm{PR<em v_="v," y-2="y-2">{v, y}-\mathrm{PR}</em>\right)$</td>
</tr>
</tbody>
</table>
<p>TABLE II. continued from previous page for Table. I</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Feature Index</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">pair <br> feature</td>
<td style="text-align: center;">78-80</td>
<td style="text-align: center;">Number of shared neighbors between nodes $u$ and $v$ until the year $y, y-1, y-2$, denoted and ordered as $\mathrm{Ns}<em y-1="y-1">{y}, \mathrm{Ns}</em>}$ and $\mathrm{Ns<em y="y">{y-2}$; e.g., $\mathrm{Ns}</em>}=\mathrm{N<em v_="v," y="y">{u, y} \cap \mathrm{~N}</em>$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">81-83</td>
<td style="text-align: center;">Geometric similarity coefficient for the pair $(u, v)$ for the year $y, y-1$, and $y-2$ denoted and ordered as $\mathrm{Geo}<em y-1="y-1">{y}, \mathrm{Geo}</em>}$, and $\mathrm{Geo<em y="y">{y-2}$; e.g., $\mathrm{Geo}</em>}=\mathrm{Ns<em u_="u," y="y">{y}^{2} /\left(\mathrm{N}</em>\right)$.} \times \mathrm{N}_{v, y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">84-86</td>
<td style="text-align: center;">Cosine similarity coefficient for the pair $(u, v)$ for the year $y, y-1, y-2$ denoted and ordered as $\mathrm{Cos}<em y-1="y-1">{y}, \mathrm{Cos}</em>}$, and $\mathrm{Cos<em y="y">{y-2}$; e.g., $\mathrm{Cos}</em>$.}=\sqrt{\mathrm{Geo}_{y}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">87-89</td>
<td style="text-align: center;">Simpson coefficient for the pair $(u, v)$ for the year $y, y-1, y-2$ denoted and ordered as $\operatorname{Sim}<em y-1="y-1">{y}, \operatorname{Sim}</em>}$, and $\operatorname{Sim<em y="y">{y-2}$; e.g., $\operatorname{Sim}</em>}=\mathrm{Ns<em u_="u," y="y">{y} / \min \left(\mathrm{N}</em>\right)$.}, \mathrm{~N}_{v, y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">90-92</td>
<td style="text-align: center;">Preferential attachment coefficient for the pair $(u, v)$ for the year $y, y-1, y-2$ denoted and ordered as $\operatorname{Pre}<em y-1="y-1">{y}, \operatorname{Pre}</em>}$, and $\operatorname{Pre<em y="y">{y-2}$; e.g., $\operatorname{Pre}</em>}=\mathrm{N<em v_="v," y="y">{u, y} \times \mathrm{N}</em>$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">93-95</td>
<td style="text-align: center;">Sørensen-Dice coefficient for the pair $(u, v)$ for the year $y, y-1, y-2$ denoted and ordered as $\operatorname{Sor}<em y-1="y-1">{y}, \operatorname{Sor}</em>}$, and $\operatorname{Sor<em y="y">{y-2}$; e.g., $\operatorname{Sor}</em>}=2 \mathrm{Ns<em u_="u," y="y">{y} /\left(\mathrm{N}</em>\right)$.}+\mathrm{N}_{v, y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">96-98</td>
<td style="text-align: center;">Jaccard coefficient for the pair $(u, v)$ for the year $y, y-1, y-2$ denoted and ordered as $\mathrm{Jac}<em y-1="y-1">{y}, \mathrm{Jac}</em>}$, and $\mathrm{Jac<em y="y">{y-2}$; e.g., $\mathrm{Jac}</em>}=\mathrm{Ns<em u_="u," y="y">{y} /\left(\mathrm{N}</em>}+\mathrm{N<em y="y">{v, y}-\mathrm{Ns}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;">pair citation feature</td>
<td style="text-align: center;">99-101</td>
<td style="text-align: center;">Ratio of the sum of citations received by nodes $u$ and $v$ until the year $y$ to the total number of papers mentioning either concept; denoted and ordered as $\mathrm{r} 1_{y}, \mathrm{r} 1_{y-1}$, and $\mathrm{r} 1_{y-2}$; e.g., $\mathrm{r} 1_{y}=\left(\mathrm{Ct}<em v_="v," y="y">{u, y}+\mathrm{Ct}</em>}\right) /\left(\mathrm{Pn<em v_="v," y="y">{u, y}+\mathrm{Pn}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">102-104</td>
<td style="text-align: center;">Ratio of the product of citations received by nodes $u$ and $v$ until the year $y$ to the total number of papers mentioning either concept; denoted and ordered as $\mathrm{r} 2_{y}, \mathrm{r} 2_{y-1}$, and $\mathrm{r} 2_{y-2}$; e.g., $\mathrm{r} 2_{y}=\left(\mathrm{Ct}<em v_="v," y="y">{u, y} \times \mathrm{Ct}</em>}\right) /\left(\mathrm{Pn<em v_="v," y="y">{u, y}+\mathrm{Pn}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">105-107</td>
<td style="text-align: center;">Sum of average citations received by nodes $u$ and $v$ in the year $y, y-1$, and $y-2$ denoted and ordered as $\mathrm{s}<em y-1="y-1">{y}, \mathrm{~s}</em>}$, and $\mathrm{s<em y="y">{y-2}$; e.g., $\mathrm{s}</em>}=\mathrm{Cm<em v_="v," y="y">{u, y}+\mathrm{Cm}</em>$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">108-110</td>
<td style="text-align: center;">Sum of average total citations received by nodes $u$ and $v$ from the first publication to the year $y, y-1$, and $y-2$; denoted and ordered as $\mathrm{st}<em y-1="y-1">{y}, \mathrm{st}</em>}$, and $\mathrm{st<em y="y">{y-2}$; e.g., $\mathrm{st}</em>}=\mathrm{Ctm<em v_="v," y="y">{u, y}+\mathrm{Ctm}</em>$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">111-113</td>
<td style="text-align: center;">Sum of the total citations received by nodes $u$ and $v$ in the last 3 years ending in the year $y, y-1$, and $y-2$; denoted and ordered as $\mathrm{st}<em y-1="y-1">{y}^{\Delta 3}, \mathrm{st}</em>}^{\Delta 3}$, and $\mathrm{st<em y="y">{y-2}^{\Delta 3}$; e.g., $\mathrm{st}</em>}^{\Delta 3}=\mathrm{Ct<em v_="v," y="y">{u, y}^{\Delta 3}+\mathrm{Ct}</em>$.}^{\Delta 3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">114-116</td>
<td style="text-align: center;">Sum of average total citations received by nodes $u$ and $v$ in the last 3 years ending in the year $y, y-1$, and $y-2$; denoted and ordered as $\operatorname{stm}<em y-1="y-1">{y}^{\Delta 3}, \operatorname{stm}</em>}^{\Delta 3}$, and $\operatorname{stm<em y="y">{y-2}^{\Delta 3}$ e.g., $\operatorname{stm}</em>}^{\Delta 3}=\mathrm{Ctm<em v_="v," y="y">{u, y}^{\Delta 3}+\mathrm{Ctm}</em>$.}^{\Delta 3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">117-119</td>
<td style="text-align: center;">Minimum number of citations received by either node $u$ or $v$ in years $y, y-1$, and $y-2$; denoted and ordered as $\min \mathrm{C}<em y-1="y-1">{y}, \min \mathrm{C}</em>}$, and $\min \mathrm{C<em y="y">{y-2}$; e.g., $\min \mathrm{C}</em>}=\min \left(\mathrm{C<em v_="v," y="y">{u, y}, \mathrm{C}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">120-122</td>
<td style="text-align: center;">Maximum number of citations received by either node $u$ or $v$ in years $y, y-1$, and $y-2$; denoted and ordered as $\max \mathrm{C}<em y-1="y-1">{y}, \max \mathrm{C}</em>}$, and $\max \mathrm{C<em y="y">{y-2}$; e.g., $\max \mathrm{C}</em>}=\max \left(\mathrm{C<em v_="v," y="y">{u, y}, \mathrm{C}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">123-125</td>
<td style="text-align: center;">Minimum number of total citations received by nodes $u$ and $v$ from the first publication to the year $y, y-1$, and $y-2$; denoted and ordered as $\min \mathrm{Ct}<em y-1="y-1">{y}, \min \mathrm{Ct}</em>}$, and $\min \mathrm{Ct<em y="y">{y-2}$; e.g., $\min \mathrm{Ct}</em>}=\min \left(\mathrm{Ct<em v_="v," y="y">{u, y}, \mathrm{Ct}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">126-128</td>
<td style="text-align: center;">Maximum number of total citations received by nodes $u$ and $v$ from the first publication to the year $y, y-1$, and $y-2$; denoted and ordered as $\max \mathrm{Ct}<em y-1="y-1">{y}, \max \mathrm{Ct}</em>}$, and $\max \mathrm{Ct<em y="y">{y-2}$; e.g., $\max \mathrm{Ct}</em>}=\max \left(\mathrm{Ct<em v_="v," y="y">{u, y}, \mathrm{Ct}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">129-131</td>
<td style="text-align: center;">Minimum number of total citations received by nodes $u$ and $v$ in the last 3 years ending in the year $y, y-1$, and $y-2$; denoted and ordered as $\min \mathrm{Ct}<em y-1="y-1">{y}^{\Delta 3}, \min \mathrm{Ct}</em>}^{\Delta 3}$, and $\min \mathrm{Ct<em y="y">{y-2}^{\Delta 3}$; e.g., $\min \mathrm{Ct}</em>}^{\Delta 3}=\min \left(\mathrm{Ct<em v_="v," y="y">{u, y}^{\Delta 3}, \mathrm{Ct}</em>\right)$.}^{\Delta 3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">132-134</td>
<td style="text-align: center;">Maximum number of total citations received by nodes $u$ and $v$ in the last 3 years ending in the year $y, y-1$, and $y-2$; denoted and ordered as $\max \mathrm{Ct}<em y-1="y-1">{y}^{\Delta 3}, \max \mathrm{Ct}</em>}^{\Delta 3}$, and $\max \mathrm{Ct<em y="y">{y-2}^{\Delta 3}$; e.g., $\max \mathrm{Ct}</em>}^{\Delta 3}=\max \left(\mathrm{Ct<em v_="v," y="y">{u, y}^{\Delta 3}, \mathrm{Ct}</em>\right)$.}^{\Delta 3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">135-137</td>
<td style="text-align: center;">Minimum number of papers mentioning the node $u$ or node $v$ from the first publication to the year $y, y-1$, and $y-2$; denoted and ordered as $\min \mathrm{Pn}<em y-1="y-1">{y}, \min \mathrm{Pn}</em>}$ and $\min \mathrm{Pn<em y="y">{y-2}$; e.g., $\min \mathrm{Pn}</em>}=\min \left(\mathrm{Pn<em v_="v," y="y">{u, y}, \mathrm{Pn}</em>\right)$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">138-140</td>
<td style="text-align: center;">Maximum number of papers mentioning the node $u$ or node $v$ from the first publication to the year $y, y-1$, and $y-2$; denoted and ordered as $\max \mathrm{Pn}<em y-1="y-1">{y}, \max \mathrm{Pn}</em>}$ and $\max \mathrm{Pn<em y="y">{y-2}$; e.g., $\max \mathrm{Pn}</em>}=\max \left(\mathrm{Pn<em v_="v," y="y">{u, y}, \mathrm{Pn}</em>\right)$.</td>
</tr>
</tbody>
</table>
<p>[1] S. Fortunato, C. T. Bergstrom, K. Börner, J. A. Evans, D. Helbing, S. Milojević, A. M. Petersen, F. Radicchi, R. Sinatra, B. Uzzi, et al., Science of science, Science 359, eaao0185 (2018).
[2] D. Wang and A.-L. Barabási, The science of science (Cambridge University Press, 2021).
[3] L. Bornmann, R. Haunschild, and R. Mutz, Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases, Humanities and Social Sciences Communications 8, 1 (2021).
[4] M. Krenn, L. Buffoni, B. Coutinho, S. Eppel, J. G. Foster, A. Gritsevskiy, H. Lee, Y. Lu, J. P. Moutinho, N. Sanjabi, et al., Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network, Nature Machine Intelligence 5, 1326 (2023).
[5] OpenAI, Gpt-4 technical report, arXiv:2303.08774 (2023).
[6] Google, Gemini: a family of highly capable multimodal models, arXiv:2312.11805 (2023).
[7] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open foundation and finetuned chat models, arXiv:2307.09288 (2023).
[8] Q. Wang, D. Downey, H. Ji, and T. Hope, Learning to generate novel scientific directions with contextualized literature-based discovery, arXiv:2305.14259 (2023).
[9] A. Rzhetsky, J. G. Foster, I. T. Foster, and J. A. Evans, Choosing experiments to accelerate collective discovery, Proc. Natl. Acad. Sci. USA 112, 14569 (2015).
[10] V. Martínez, F. Berzal, and J.-C. Cubero, A survey of link prediction in complex networks, ACM computing surveys (CSUR) 49, 1 (2016).
[11] M. Krenn and A. Zeilinger, Predicting research trends with semantic and neural networks with an application in quantum physics, Proc. Natl. Acad. Sci. USA 117, 1910 (2020).
[12] Challenge the impact factor, Nature Biomedical Engineering 1, 0103 (2017).
[13] The many facets of impact, Nature Reviews Physics 6, 71 (2024).
[14] A.-L. Barabási, The formula: The universal laws of success (Hachette UK, 2018).
[15] M. R. Frank, D. Wang, M. Cebrian, and I. Rahwan, The evolution of citation graphs in artificial intelligence research, Nature Machine Intelligence 1, 79 (2019).
[16] D. Wang, C. Song, and A.-L. Barabási, Quantifying longterm scientific impact, Science 342, 127 (2013).
[17] Q. Ke, E. Ferrara, F. Radicchi, and A. Flammini, Defining and identifying sleeping beauties in science, Proc. Natl. Acad. Sci. USA 112, 7426 (2015).
[18] R. Sinatra, D. Wang, P. Deville, C. Song, and A.-L. Barabási, Quantifying the evolution of individual scientific impact, Science 354, aaf5239 (2016).
[19] L. Wu, D. Wang, and J. A. Evans, Large teams develop and small teams disrupt science and technology, Nature 566, 378 (2019).
[20] J. W. Weis and J. M. Jacobson, Learning on knowledge graph dynamics provides an early warning of impactful research, Nature Biotechnology 39, 1300 (2021).
[21] X. Bai, H. Liu, F. Zhang, Z. Ning, X. Kong, I. Lee, and F. Xia, An overview on evaluating and predicting scholarly article impact, Information 8, 73 (2017).
[22] W. Xia, T. Li, and C. Li, A review of scientific impact prediction: tasks, features and methods, Scientometrics 128, 543 (2023).
[23] L. Fu and C. Aliferis, Using content-based and bibliometric features for machine learning models to predict citation counts in the biomedical literature, Scientometrics 85, 257 (2010).
[24] T. Yu, G. Yu, P.-Y. Li, and L. Wang, Citation impact prediction for scientific papers using stepwise regression analysis, Scientometrics 101, 1233 (2014).
[25] C. Stegehuis, N. Litvak, and L. Waltman, Predicting the long-term citation impact of recent publications, Journal of informetrics 9, 642 (2015).
[26] L. Weihs and O. Etzioni, Learning to predict citationbased impact measures, in 2017 ACM/IEEE joint conference on digital libraries (JCDL) (IEEE, 2017) pp. 110 .
[27] X. Ruan, Y. Zhu, J. Li, and Y. Cheng, Predicting the citation counts of individual papers via a bp neural network, Journal of Informetrics 14, 101039 (2020).
[28] G. He, Z. Xue, Z. Jiang, Y. Kang, S. Zhao, and W. Lu, H2cgl: Modeling dynamics of citation network for impact prediction, arXiv:2305.01572 (2023).
[29] L. Möckl, A. R. Roy, P. N. Petrov, and W. Moerner, Accurate and rapid background estimation in singlemolecule localization microscopy using the deep neural network bgnet, Proc. Natl. Acad. Sci. USA 117, 60 (2020).
[30] D. J. Williamson, G. L. Burn, S. Simoncelli, J. Griffié, R. Peters, D. M. Davis, and D. M. Owen, Machine learning for cluster analysis of localization microscopy data, Nature communications 11, 1493 (2020).
[31] B. P. Abbott, R. Abbott, T. D. Abbott, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso, R. X. Adhikari, V. B. Adya, et al. (LIGO Scientific Collaboration and Virgo Collaboration), Constraints on cosmic strings using data from the first advanced ligo observing run, Phys. Rev. D 97, 102002 (2018).
[32] E. van Nieuwenburg, E. Bairey, and G. Refael, Learning phase transitions from dynamics, Phys. Rev. B 98, 060301 (2018).
[33] Y. Rong, Y. Hu, A. Mei, H. Tan, M. I. Saidaminov, S. I. Seok, M. D. McGehee, E. H. Sargent, and H. Han, Challenges for commercializing perovskite solar cells, Science 361, eaat8235 (2018).
[34] E. J. Bergholtz, J. C. Budich, and F. K. Kunst, Exceptional topology of non-hermitian systems, Rev. Mod. Phys. 93, 015005 (2021).
[35] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborová, Machine learning and the physical sciences, Rev. Mod. Phys. 91, 045002 (2019).
[36] M. Krenn, J. Landgraf, T. Foesel, and F. Marquardt, Artificial intelligence and machine learning for quantum technologies, Phys. Rev. A 107, 010101 (2023).
[37] H. Wang, T. Fu, Y. Du, W. Gao, K. Huang, Z. Liu, P. Chandak, S. Liu, P. Van Katwyk, A. Deac, et al., Scientific discovery in the age of artificial intelligence,</p>
<p>Nature 620, 47 (2023).
[38] A. V. Leeuwenhoek, Ii. microscopical observations on the blood vessels and membranes of the intestines. in a letter to the royal society from mr. anthony van leeuwenhoek, frs, Philosophical Transactions of the Royal Society of London 26, 53 (1709).
[39] M. Krenn, R. Pollice, S. Y. Guo, M. Aldeghi, A. CerveraLierta, P. Friederich, G. dos Passos Gomes, F. Häse, A. Jinich, A. Nigam, et al., On scientific understanding with artificial intelligence, Nature Reviews Physics 4, 761 (2022).
[40] A. A. Salatino, F. Osborne, T. Thanapalasingam, and E. Motta, The cso classifier: Ontology-driven detection of research topics in scholarly articles, in Digital Libraries for Open Knowledge: 23rd International Conference on Theory and Practice of Digital Libraries, TPDL 2019, Oslo, Norway, September 9-12, 2019, Proceedings 23 (Springer, 2019) pp. 296-311.
[41] S. Rose, D. Engel, N. Cramer, and W. Cowley, Automatic keyword extraction from individual documents, Text mining: applications and theory, 1 (2010).
[42] J. Priem, H. Piwowar, and R. Orr, Openalex: A fullyopen index of scholarly works, authors, venues, institutions, and concepts, arXiv:2205.01833 (2022).
[43] L. Page, S. Brin, R. Motwani, and T. Winograd, The pagerank citation ranking : Bringing order to the web, Stanford InfoLab (1999).
[44] A.-L. Barabási, Network Science (Cambridge University Press, 2016).
[45] Y. Lu, Predicting research trends in artificial intelligence with gradient boosting decision trees and time-aware graph neural networks, in 2021 IEEE International Conference on Big Data (Big Data) (IEEE, 2021) pp. 58095814 .
[46] T. Fawcett, Roc graphs: Notes and practical considerations for researchers, Machine learning 31, 1 (2004).
[47] D. Bahdanau, K. Cho, and Y. Bengio, Neural machine translation by jointly learning to align and translate, in International Conference on Learning Representations (ICLR) (2015).
[48] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, Attention is all you need, in Advances in Neural Information Processing Systems, Vol. 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Curran Associates, Inc., 2017).
[49] J. R. Quinlan, Induction of decision trees, Machine Learning 1, 81 (1986).
[50] T. Chen and C. Guestrin, Xgboost: A scalable tree boosting system, in Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (2016) pp. 785-794.
[51] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch: An imperative style, highperformance deep learning library, in Advances in Neural Information Processing Systems 32 (NeurIPS), edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchéBuc, E. Fox, and R. Garnett (Curran Associates, Inc., 2019).
[52] V. Nair and G. E. Hinton, Rectified linear units improve restricted boltzmann machines, in International conference on machine learning (ICML) (2010).
[53] D. P. Kingma, Adam: A method for stochastic optimization, in International Conference on Learning Representations (ICLR) (2015).
[54] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al., Scikit-learn: Machine learning in python, Journal of machine learning research 12, 2825 (2011).
[55] F. Battiston, E. Amico, A. Barrat, G. Bianconi, G. Ferraz de Arruda, B. Franceschiello, I. Iacopini, S. Kéfi, V. Latora, Y. Moreno, et al., The physics of higher-order interactions in complex systems, Nature Physics 17, 1093 (2021).
[56] A. V. Belikov, A. Rzhetsky, and J. Evans, Prediction of robust scientific facts from literature, Nature Machine Intelligence 4, 445 (2022).
[57] J. G. Foster, A. Rzhetsky, and J. A. Evans, Tradition and innovation in scientists' research strategies, American Sociological Review 80, 875 (2015).
[58] J. Sourati and J. A. Evans, Accelerating science with human-aware artificial intelligence, Nature Human Behaviour 7, 1682 (2023).
[59] G. H. Nguyen, J. B. Lee, R. A. Rossi, N. K. Ahmed, E. Koh, and S. Kim, Continuous-time dynamic network embeddings, in Companion proceedings of the the web conference 2018 (2018) pp. 969-976.
[60] F. Frohnert, X. Gu, M. Krenn, and E. van Nieuwenburg, Discovering emergent connections in quantum physics research via dynamic word embeddings, Machine Learning: Science and Technology 10.1088/2632-2153/adb00a (2025).
[61] A. A. Salatino, F. Osborne, and E. Motta, How are topics born? understanding the research dynamics preceding the emergence of new areas, PeerJ Computer Science 3, e119 (2017).
[62] A. A. Salatino, F. Osborne, and E. Motta, Augur: forecasting the emergence of new research topics, in Proceedings of the 18th ACM/IEEE on joint conference on digital libraries (2018) pp. 303-312.
[63] Z. Lin, Y. Yin, L. Liu, and D. Wang, Sciscinet: A largescale open data lake for the science of science research, Scientific Data 10, 315 (2023).
[64] J. Li, Y. Yin, S. Fortunato, and D. Wang, A dataset of publication records for nobel laureates, Scientific Data 6, 33 (2019).
[65] J. G. Foster, F. Shi, and J. Evans, Surprise! measuring novelty as expectation violation, SocArXiv 2t46f (2021).
[66] F. Shi and J. Evans, Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines, Nature Communications 14, 1641 (2023).
[67] X. Gu and M. Krenn, Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders, arXiv:2405.17044 (2024).
[68] X. Gu, Impact4cast: Forecasting high-impact research topics via machine learning on evolving knowledge graphs [data set], zenodo, https://doi.org/10.5281/zenodo.10692137 (2024).
[69] X. Gu, Benchmark dataset for impact4cast: Forecasting high-impact research topics via machine learning on evolving knowledge graphs [data set]. zenodo.,</p>
<p>https://doi.org/10.5281/zenodo.14527306 (2024).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>xuemei.gu@mpl.mpg.de
${ }^{\dagger}$ mario.krenn@mpl.mpg.de</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9193 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9193</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9193</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-278886635</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.17139v3.pdf" target="_blank">EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Advancements in Large Language Models (LLMs) drive interest in scientific applications, necessitating specialized benchmarks such as Earth science. Existing benchmarks either present a general science focus devoid of Earth science specificity or cover isolated subdomains, lacking holistic evaluation. Furthermore, current benchmarks typically neglect the assessment of LLMs' capabilities in open-ended scientific exploration. In this paper, we present a comprehensive and professional benchmark for the Earth sciences, designed to evaluate the capabilities of LLMs in scientific exploration within this domain, spanning from fundamental to advanced levels. Leveraging a corpus of 100,000 research papers, we first construct two Question Answering (QA) datasets: Earth-Iron, which offers extensive question coverage for broad assessment, and Earth-Silver, which features a higher level of difficulty to evaluate professional depth. These datasets encompass five Earth spheres, 114 disciplines, and 11 task categories, assessing foundational knowledge crucial for scientific exploration. Most notably, we introduce Earth-Gold with new metrics, a dataset comprising open-ended multi-turn dialogues specifically designed to evaluate the advanced capabilities of LLMs in scientific exploration, including methodology induction, limitation analysis, and concept proposal. Extensive experiments reveal limitations in 11 leading LLMs across different domains and tasks, highlighting considerable room for improvement in their scientific exploration capabilities. The benchmark is available on https://huggingface.co/ai-earth .</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9193.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9193.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (generator/judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used as data generator and judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is used within this work to (a) generate QA pairs (including chain-of-thought explanations) from paper content and (b) to synthesize two-turn scientific-exploration dialogues (Earth-Gold) that simulate human-AI iterative research reasoning; GPT-4 is also used as an impartial judge for free-response win-rate evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (used here as a high-quality LLM for data generation and as an automated judge); the paper uses GPT-4 to produce QA pairs with CoT and to generate reference two-turn dialogues for Earth-Gold; GPT-4 is not evaluated as one of the participant models on the benchmark in reported score tables.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cross-domain Earth sciences (used to simulate scientific exploration workflows across lithosphere and atmosphere examples, and generally across all five Earth spheres)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of scientific exploration: (a) automatic QA pair generation from papers including Chain-of-Thought reasoning, and (b) generation of two-turn scientific-exploration dialogues (summarize existing methods M0, analyze limitations L0, propose new method M1, and analyze M1 limitations L1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used to produce reference answers and to judge free-response Win Rate (WR); Earth-Gold evaluation uses retention rate and diversity (SES) computed against GPT-4-produced reference answers.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Not evaluated numerically as a participant; the paper notes factors that affect quality of generated references and judgments: quality of input paper structuring (M0/L0/M1/L1), prompt design, CoT inclusion in generated QA, and human expert validation/cleaning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GPT-4 is used to create references and to blind-judge model outputs; it is not presented with a comparative accuracy score as a participant model in Earth-Iron/Earth-Silver/Earth-Gold tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As a generator/judge, potential risks include propagation of generation biases into dataset, reliance on GPT-4's internal heuristics for ranking (used for retention calculations), and possible overfitting of reference style; the paper mitigates these via human expert validation.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use CoT-augmented generative prompts and human expert validation to improve the quality of generated QA/dialogue references; recognize that generated references can affect evaluation outcomes and should be curated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9193.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9193.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grok-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grok-3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grok-3 is one of the mainstream LLMs evaluated on EarthSE; it achieved top or near-top performance across the five Earth spheres and was evaluated on QA tasks and open-ended scientific exploration dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grok-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mainstream proprietary LLM evaluated in the benchmark (no model size provided in this paper); included among top-performing models across spheres.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cross-domain Earth sciences (evaluated across hydrosphere, biosphere, lithosphere, atmosphere, cryosphere) and used to answer specific subdomain examples in QA and Earth-Gold dialogues (e.g., sedimentary cycles, convective boundary layer).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of domain scientists: answering Earth-Iron / Earth-Silver QA items (MC/TF/FIB/FR) and generating responses to Earth-Gold open-ended scientific exploration dialogues (summarize methods, analyze limitations, propose improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>QA metrics: Accuracy for MC/TF/FIB; Free-response: Win Rate (WR) judged by GPT-4 and Semantic Similarity (SS) via sentence-transformers; Earth-Gold metrics: Retention (% of model responses better than reference), Diversity (embedding-based), and composite SES = r × d.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Earth-Iron: MC 93.03% | TF 88.64% | FIB 21.85% | FR-WR 98.7% | FR-SS 0.81; Earth-Silver: MC 53.20% | TF 70.40% | FIB 15.20% | FR-WR 83.6% | FR-SS 0.81; Earth-Gold: Retention 17.22% | Diversity 1.5284 | SES 0.2727.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Question format (FIB particularly low), task difficulty (Earth-Silver harder than Earth-Iron), domain-specific formula manipulation and multi-step calculations (models frequently misapply formulas), lack of specialized terminology understanding, and CoT/hint availability at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly against other mainstream LLMs in EarthSE (e.g., DeepSeek-V3, Gemini variants, Claude, Llama-3.1, Qwen); achieved SOTA across spheres in capability distribution per paper text; compared against a random-chance baseline for MC (25%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low performance on fill-in-the-blank and calculation tasks due to formula misuse and computational errors; produces less-detailed or generic responses in open-ended exploration leading to low retention; limited diversity in repeated generations.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Providing Chain-of-Thought (CoT) hints at inference can improve performance on challenging (especially FIB/calculation) items; need improved domain-specific numerical reasoning and terminology handling to raise simulation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9193.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9193.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DeepSeek-V3 is a high-performing LLM in EarthSE that shows strong QA and comparatively higher scientific-exploration retention and SES scores, indicating better open-ended exploration capability among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary LLM evaluated in this benchmark (model size not specified in paper); included among top performers on QA and Earth-Gold.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cross-domain Earth sciences (evaluated across all five spheres and multiple subdisciplines), used for text-based simulation of scientific reasoning and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering Earth-Iron / Earth-Silver QA items and generating Earth-Gold open-ended scientific-exploration responses (method summarization, limitation analysis, method proposal).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MC/TF/FIB accuracy, FR win-rate and semantic similarity; Earth-Gold retention, diversity, and SES.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Earth-Iron: MC 93.40% | TF 81.14% | FIB 18.99% | FR-WR 97.6% | FR-SS 0.81; Earth-Silver: MC 58.00% | TF 56.40% | FIB 12.80% | FR-WR 75.2% | FR-SS 0.81; Earth-Gold: Retention 38.00% | Diversity 1.6942 | SES 0.6599.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Dataset difficulty (Earth-Silver harder), question format (FIB and calculation weaker), availability of CoT guidance, and domain complexity (multi-step calculations, formula application).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked against other mainstream LLMs in the same evaluation (Grok-3, GPT-4o, Gemini, Claude, Llama-3.1, Qwen); DeepSeek-V3 shows comparatively strong retention/SES on Earth-Gold.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Similar failure modes as other LLMs: poor handling of specialized calculations and fill-in-the-blank questions, occasional generic open-ended responses lacking method-level detail.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Applying CoT hints at inference improves performance; human expert validation and QA cleaning remain necessary; further improvements needed in domain-specific numerical reasoning to raise simulation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9193.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9193.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini-2.5 is a high-capability multimodal LLM variant evaluated on EarthSE; it scored strongly on Earth-Gold retention and diversity, indicating robust open-ended exploration behaviour in text-only scientific simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Gemini-series LLM (specific size/training details not provided in this paper) evaluated for Earth science QA and exploration tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cross-domain Earth sciences (evaluated across the five spheres and multiple sub-disciplines), used for text-based scientific exploration tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answer QA items across Earth-Iron/Earth-Silver and generate Earth-Gold exploration dialogue responses (method summary, limitation analysis, method proposal).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MC/TF/FIB accuracy, FR Win Rate, FR Semantic Similarity; Earth-Gold retention, diversity, SES.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Earth-Iron: MC 93.15% | TF 77.84% | FIB 17.02% | FR-WR 95.8% | FR-SS 0.75; Earth-Silver: MC 58.00% | TF 55.60% | FIB 13.65% | FR-WR 74.3% | FR-SS 0.75; Earth-Gold: Retention 50.56% | Diversity 2.7016 | SES 1.3710.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Task format (FIB/calculation weaknesses), dataset difficulty, prompt/CoT availability, and temperature/decoding settings for diversity (paper used temperature=0.6 for diversity measurement).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against mainline LLMs in the benchmark; shows comparatively high retention and diversity on Earth-Gold, outperforming many other models in SES.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still exhibits low FIB accuracy and issues with numerical/formula manipulation; open-ended responses can still be overly broad if not sufficiently prompted.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Encourage CoT guidance and inference-time strategy improvements; monitor temperature and sampling parameters to balance diversity vs. quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9193.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9193.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.7</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.7</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude-3.7 is an LLM evaluated in EarthSE that shows moderate-to-strong open-ended exploration capability as measured by retention/diversity (SES) while exhibiting the common weaknesses of LLMs on fill-in-the-blank and calculation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.7</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Claude-3.7 (proprietary LLM) evaluated in the benchmark; specific model size and training corpus not stated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cross-domain Earth sciences; participated in QA and Earth-Gold dialogue tasks spanning multiple Earth subdisciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering QA items (MC/TF/FIB/FR) and producing Earth-Gold open-ended scientific exploration responses (method synthesis, limitation analysis, proposals).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MC/TF/FIB accuracy, FR Win Rate and SS; Earth-Gold retention, diversity, SES.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Earth-Iron: MC 94.01% | TF 61.90% | FIB 20.68% | FR-WR 75.0% | FR-SS 0.80; Earth-Silver: MC 62.40% | TF 41.20% | FIB 17.20% | FR-WR 28.4% | FR-SS 0.79; Earth-Gold: Retention 31.89% | Diversity 1.7130 | SES 0.5465.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Question format and dataset difficulty (notably drop in FR-WR on Earth-Silver), CoT availability, domain-specific reasoning and numeric calculation complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked vs other mainstream LLMs in the study; shows middling-to-strong SES compared to peers but variable FR performance on the more professional Earth-Silver subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Weaknesses on more specialized Earth-Silver free-response items and fill-in-the-blank calculations; some produced answers are overly general on open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>The authors note that providing CoT hints improves performance; recommend further domain-specific fine-tuning or tool-integration for numerical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9193.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9193.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1 (70B parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama-3.1-70B (70B parameter model) is evaluated across EarthSE QA and Earth-Gold tasks; the larger Llama variant substantially outperforms the much smaller 8B variant, illustrating model size as an influencing factor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-3.1 family; the paper reports both an 8B and a 70B variant and shows that the 70B variant achieves substantially higher QA accuracy and exploration metrics than the 8B variant.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cross-domain Earth sciences (evaluated across multiple Earth spheres and subdisciplines).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering QA questions in Earth-Iron/Earth-Silver and producing responses in Earth-Gold exploration dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MC/TF/FIB Accuracy; FR Win Rate and Semantic Similarity; Earth-Gold Retention/Diversity/SES.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Earth-Iron: MC 91.56% | TF 87.91% | FIB 6.63% | FR-WR 61.85% | FR-SS 0.80; Earth-Silver: MC 56.00% | TF 63.60% | FIB 4.00% | FR-WR 18.40% | FR-SS 0.80; Earth-Gold: Retention 11.78% | Diversity 1.4891 | SES 0.2453.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model scale (70B >> 8B), question difficulty/dataset quality, prompt/COT availability, and task format (FIB very low even for larger model).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared internally to the Llama-3.1-8B variant and other mainstream LLMs; larger variant yields higher MC/TF performance but still poor performance on fill-in-the-blank/calculation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Severe weakness on fill-in-the-blank tasks and professional Earth-Silver free-response items; struggles with multi-step calculations and domain-specific terminologies despite larger parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors highlight model size as a factor but emphasize that inference-time techniques (CoT hints) substantially improve performance on challenging items; recommend combining CoT strategies and domain-specific data to improve numerical and terminological accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9193.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9193.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5 (72B parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qwen-2.5-72B is evaluated on EarthSE QA and Earth-Gold tasks; it demonstrates strong MC/TF accuracy but low FIB performance and modest Earth-Gold retention/diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen-2.5 72B parameter variant as listed in the benchmark table; specific training corpus not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cross-domain Earth sciences; participates in QA and Earth-Gold dialogue evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering QA items (MC/TF/FIB/FR) and producing Earth-Gold exploration dialogue responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MC/TF/FIB Accuracy; FR Win Rate and Semantic Similarity; Earth-Gold Retention/Diversity/SES.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Earth-Iron: MC 92.42% | TF 86.26% | FIB 11.96% | FR-WR 92.05% | FR-SS 0.79; Earth-Silver: MC 53.60% | TF 64.40% | FIB 9.20% | FR-WR 44.40% | FR-SS 0.78; Earth-Gold: Retention 7.11% | Diversity 1.7158 | SES 0.1375.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Question format and dataset difficulty, lack of robust numerical/formula handling for FIB questions, and limited diversity in open-ended responses unless sampling/temperature tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked vs other mainstream LLMs in EarthSE; strong MC/TF accuracy but lower Earth-Gold retention versus top performers like Gemini-2.5 and DeepSeek-V3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low fill-in-the-blank accuracy and modest performance on more professional free-response items; open-ended answers often less innovative/detailed than reference.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>CoT guidance and inference-time strategies can improve performance; authors suggest further domain-specific fine-tuning and improved numerical reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Oceangpt: A large language model for ocean science tasks. <em>(Rating: 2)</em></li>
                <li>OceanBench <em>(Rating: 2)</em></li>
                <li>ClimaQA: An automated evaluation framework for climate foundation models. <em>(Rating: 2)</em></li>
                <li>The AI scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 2)</em></li>
                <li>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9193",
    "paper_id": "paper-278886635",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4 (generator/judge)",
            "name_full": "Generative Pre-trained Transformer 4 (used as data generator and judge)",
            "brief_description": "GPT-4 is used within this work to (a) generate QA pairs (including chain-of-thought explanations) from paper content and (b) to synthesize two-turn scientific-exploration dialogues (Earth-Gold) that simulate human-AI iterative research reasoning; GPT-4 is also used as an impartial judge for free-response win-rate evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 (used here as a high-quality LLM for data generation and as an automated judge); the paper uses GPT-4 to produce QA pairs with CoT and to generate reference two-turn dialogues for Earth-Gold; GPT-4 is not evaluated as one of the participant models on the benchmark in reported score tables.",
            "scientific_subdomain": "Cross-domain Earth sciences (used to simulate scientific exploration workflows across lithosphere and atmosphere examples, and generally across all five Earth spheres)",
            "simulation_task": "Text-based simulation of scientific exploration: (a) automatic QA pair generation from papers including Chain-of-Thought reasoning, and (b) generation of two-turn scientific-exploration dialogues (summarize existing methods M0, analyze limitations L0, propose new method M1, and analyze M1 limitations L1).",
            "evaluation_metric": "Used to produce reference answers and to judge free-response Win Rate (WR); Earth-Gold evaluation uses retention rate and diversity (SES) computed against GPT-4-produced reference answers.",
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Not evaluated numerically as a participant; the paper notes factors that affect quality of generated references and judgments: quality of input paper structuring (M0/L0/M1/L1), prompt design, CoT inclusion in generated QA, and human expert validation/cleaning.",
            "comparison_baseline": "GPT-4 is used to create references and to blind-judge model outputs; it is not presented with a comparative accuracy score as a participant model in Earth-Iron/Earth-Silver/Earth-Gold tables.",
            "limitations_or_failure_cases": "As a generator/judge, potential risks include propagation of generation biases into dataset, reliance on GPT-4's internal heuristics for ranking (used for retention calculations), and possible overfitting of reference style; the paper mitigates these via human expert validation.",
            "author_recommendations_or_insights": "Use CoT-augmented generative prompts and human expert validation to improve the quality of generated QA/dialogue references; recognize that generated references can affect evaluation outcomes and should be curated.",
            "uuid": "e9193.0",
            "source_info": {
                "paper_title": "EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Grok-3",
            "name_full": "Grok-3",
            "brief_description": "Grok-3 is one of the mainstream LLMs evaluated on EarthSE; it achieved top or near-top performance across the five Earth spheres and was evaluated on QA tasks and open-ended scientific exploration dialogues.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Grok-3",
            "model_description": "Mainstream proprietary LLM evaluated in the benchmark (no model size provided in this paper); included among top-performing models across spheres.",
            "scientific_subdomain": "Cross-domain Earth sciences (evaluated across hydrosphere, biosphere, lithosphere, atmosphere, cryosphere) and used to answer specific subdomain examples in QA and Earth-Gold dialogues (e.g., sedimentary cycles, convective boundary layer).",
            "simulation_task": "Text-based simulation of domain scientists: answering Earth-Iron / Earth-Silver QA items (MC/TF/FIB/FR) and generating responses to Earth-Gold open-ended scientific exploration dialogues (summarize methods, analyze limitations, propose improvements).",
            "evaluation_metric": "QA metrics: Accuracy for MC/TF/FIB; Free-response: Win Rate (WR) judged by GPT-4 and Semantic Similarity (SS) via sentence-transformers; Earth-Gold metrics: Retention (% of model responses better than reference), Diversity (embedding-based), and composite SES = r × d.",
            "simulation_accuracy": "Earth-Iron: MC 93.03% | TF 88.64% | FIB 21.85% | FR-WR 98.7% | FR-SS 0.81; Earth-Silver: MC 53.20% | TF 70.40% | FIB 15.20% | FR-WR 83.6% | FR-SS 0.81; Earth-Gold: Retention 17.22% | Diversity 1.5284 | SES 0.2727.",
            "factors_affecting_accuracy": "Question format (FIB particularly low), task difficulty (Earth-Silver harder than Earth-Iron), domain-specific formula manipulation and multi-step calculations (models frequently misapply formulas), lack of specialized terminology understanding, and CoT/hint availability at inference.",
            "comparison_baseline": "Compared directly against other mainstream LLMs in EarthSE (e.g., DeepSeek-V3, Gemini variants, Claude, Llama-3.1, Qwen); achieved SOTA across spheres in capability distribution per paper text; compared against a random-chance baseline for MC (25%).",
            "limitations_or_failure_cases": "Low performance on fill-in-the-blank and calculation tasks due to formula misuse and computational errors; produces less-detailed or generic responses in open-ended exploration leading to low retention; limited diversity in repeated generations.",
            "author_recommendations_or_insights": "Providing Chain-of-Thought (CoT) hints at inference can improve performance on challenging (especially FIB/calculation) items; need improved domain-specific numerical reasoning and terminology handling to raise simulation fidelity.",
            "uuid": "e9193.1",
            "source_info": {
                "paper_title": "EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-V3",
            "name_full": "DeepSeek-V3",
            "brief_description": "DeepSeek-V3 is a high-performing LLM in EarthSE that shows strong QA and comparatively higher scientific-exploration retention and SES scores, indicating better open-ended exploration capability among evaluated models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-V3",
            "model_description": "Proprietary LLM evaluated in this benchmark (model size not specified in paper); included among top performers on QA and Earth-Gold.",
            "scientific_subdomain": "Cross-domain Earth sciences (evaluated across all five spheres and multiple subdisciplines), used for text-based simulation of scientific reasoning and exploration.",
            "simulation_task": "Answering Earth-Iron / Earth-Silver QA items and generating Earth-Gold open-ended scientific-exploration responses (method summarization, limitation analysis, method proposal).",
            "evaluation_metric": "MC/TF/FIB accuracy, FR win-rate and semantic similarity; Earth-Gold retention, diversity, and SES.",
            "simulation_accuracy": "Earth-Iron: MC 93.40% | TF 81.14% | FIB 18.99% | FR-WR 97.6% | FR-SS 0.81; Earth-Silver: MC 58.00% | TF 56.40% | FIB 12.80% | FR-WR 75.2% | FR-SS 0.81; Earth-Gold: Retention 38.00% | Diversity 1.6942 | SES 0.6599.",
            "factors_affecting_accuracy": "Dataset difficulty (Earth-Silver harder), question format (FIB and calculation weaker), availability of CoT guidance, and domain complexity (multi-step calculations, formula application).",
            "comparison_baseline": "Benchmarked against other mainstream LLMs in the same evaluation (Grok-3, GPT-4o, Gemini, Claude, Llama-3.1, Qwen); DeepSeek-V3 shows comparatively strong retention/SES on Earth-Gold.",
            "limitations_or_failure_cases": "Similar failure modes as other LLMs: poor handling of specialized calculations and fill-in-the-blank questions, occasional generic open-ended responses lacking method-level detail.",
            "author_recommendations_or_insights": "Applying CoT hints at inference improves performance; human expert validation and QA cleaning remain necessary; further improvements needed in domain-specific numerical reasoning to raise simulation fidelity.",
            "uuid": "e9193.2",
            "source_info": {
                "paper_title": "EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Gemini-2.5",
            "name_full": "Gemini-2.5",
            "brief_description": "Gemini-2.5 is a high-capability multimodal LLM variant evaluated on EarthSE; it scored strongly on Earth-Gold retention and diversity, indicating robust open-ended exploration behaviour in text-only scientific simulations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.5",
            "model_description": "A Gemini-series LLM (specific size/training details not provided in this paper) evaluated for Earth science QA and exploration tasks.",
            "scientific_subdomain": "Cross-domain Earth sciences (evaluated across the five spheres and multiple sub-disciplines), used for text-based scientific exploration tasks.",
            "simulation_task": "Answer QA items across Earth-Iron/Earth-Silver and generate Earth-Gold exploration dialogue responses (method summary, limitation analysis, method proposal).",
            "evaluation_metric": "MC/TF/FIB accuracy, FR Win Rate, FR Semantic Similarity; Earth-Gold retention, diversity, SES.",
            "simulation_accuracy": "Earth-Iron: MC 93.15% | TF 77.84% | FIB 17.02% | FR-WR 95.8% | FR-SS 0.75; Earth-Silver: MC 58.00% | TF 55.60% | FIB 13.65% | FR-WR 74.3% | FR-SS 0.75; Earth-Gold: Retention 50.56% | Diversity 2.7016 | SES 1.3710.",
            "factors_affecting_accuracy": "Task format (FIB/calculation weaknesses), dataset difficulty, prompt/CoT availability, and temperature/decoding settings for diversity (paper used temperature=0.6 for diversity measurement).",
            "comparison_baseline": "Compared against mainline LLMs in the benchmark; shows comparatively high retention and diversity on Earth-Gold, outperforming many other models in SES.",
            "limitations_or_failure_cases": "Still exhibits low FIB accuracy and issues with numerical/formula manipulation; open-ended responses can still be overly broad if not sufficiently prompted.",
            "author_recommendations_or_insights": "Encourage CoT guidance and inference-time strategy improvements; monitor temperature and sampling parameters to balance diversity vs. quality.",
            "uuid": "e9193.3",
            "source_info": {
                "paper_title": "EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Claude-3.7",
            "name_full": "Claude-3.7",
            "brief_description": "Claude-3.7 is an LLM evaluated in EarthSE that shows moderate-to-strong open-ended exploration capability as measured by retention/diversity (SES) while exhibiting the common weaknesses of LLMs on fill-in-the-blank and calculation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.7",
            "model_description": "Claude-3.7 (proprietary LLM) evaluated in the benchmark; specific model size and training corpus not stated in this paper.",
            "scientific_subdomain": "Cross-domain Earth sciences; participated in QA and Earth-Gold dialogue tasks spanning multiple Earth subdisciplines.",
            "simulation_task": "Answering QA items (MC/TF/FIB/FR) and producing Earth-Gold open-ended scientific exploration responses (method synthesis, limitation analysis, proposals).",
            "evaluation_metric": "MC/TF/FIB accuracy, FR Win Rate and SS; Earth-Gold retention, diversity, SES.",
            "simulation_accuracy": "Earth-Iron: MC 94.01% | TF 61.90% | FIB 20.68% | FR-WR 75.0% | FR-SS 0.80; Earth-Silver: MC 62.40% | TF 41.20% | FIB 17.20% | FR-WR 28.4% | FR-SS 0.79; Earth-Gold: Retention 31.89% | Diversity 1.7130 | SES 0.5465.",
            "factors_affecting_accuracy": "Question format and dataset difficulty (notably drop in FR-WR on Earth-Silver), CoT availability, domain-specific reasoning and numeric calculation complexity.",
            "comparison_baseline": "Benchmarked vs other mainstream LLMs in the study; shows middling-to-strong SES compared to peers but variable FR performance on the more professional Earth-Silver subset.",
            "limitations_or_failure_cases": "Weaknesses on more specialized Earth-Silver free-response items and fill-in-the-blank calculations; some produced answers are overly general on open-ended tasks.",
            "author_recommendations_or_insights": "The authors note that providing CoT hints improves performance; recommend further domain-specific fine-tuning or tool-integration for numerical tasks.",
            "uuid": "e9193.4",
            "source_info": {
                "paper_title": "EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.1-70B",
            "name_full": "Llama-3.1 (70B parameter variant)",
            "brief_description": "Llama-3.1-70B (70B parameter model) is evaluated across EarthSE QA and Earth-Gold tasks; the larger Llama variant substantially outperforms the much smaller 8B variant, illustrating model size as an influencing factor.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B",
            "model_description": "Llama-3.1 family; the paper reports both an 8B and a 70B variant and shows that the 70B variant achieves substantially higher QA accuracy and exploration metrics than the 8B variant.",
            "scientific_subdomain": "Cross-domain Earth sciences (evaluated across multiple Earth spheres and subdisciplines).",
            "simulation_task": "Answering QA questions in Earth-Iron/Earth-Silver and producing responses in Earth-Gold exploration dialogues.",
            "evaluation_metric": "MC/TF/FIB Accuracy; FR Win Rate and Semantic Similarity; Earth-Gold Retention/Diversity/SES.",
            "simulation_accuracy": "Earth-Iron: MC 91.56% | TF 87.91% | FIB 6.63% | FR-WR 61.85% | FR-SS 0.80; Earth-Silver: MC 56.00% | TF 63.60% | FIB 4.00% | FR-WR 18.40% | FR-SS 0.80; Earth-Gold: Retention 11.78% | Diversity 1.4891 | SES 0.2453.",
            "factors_affecting_accuracy": "Model scale (70B &gt;&gt; 8B), question difficulty/dataset quality, prompt/COT availability, and task format (FIB very low even for larger model).",
            "comparison_baseline": "Compared internally to the Llama-3.1-8B variant and other mainstream LLMs; larger variant yields higher MC/TF performance but still poor performance on fill-in-the-blank/calculation tasks.",
            "limitations_or_failure_cases": "Severe weakness on fill-in-the-blank tasks and professional Earth-Silver free-response items; struggles with multi-step calculations and domain-specific terminologies despite larger parameter count.",
            "author_recommendations_or_insights": "Authors highlight model size as a factor but emphasize that inference-time techniques (CoT hints) substantially improve performance on challenging items; recommend combining CoT strategies and domain-specific data to improve numerical and terminological accuracy.",
            "uuid": "e9193.5",
            "source_info": {
                "paper_title": "EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2.5-72B",
            "name_full": "Qwen-2.5 (72B parameter variant)",
            "brief_description": "Qwen-2.5-72B is evaluated on EarthSE QA and Earth-Gold tasks; it demonstrates strong MC/TF accuracy but low FIB performance and modest Earth-Gold retention/diversity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-72B",
            "model_description": "Qwen-2.5 72B parameter variant as listed in the benchmark table; specific training corpus not detailed in this paper.",
            "scientific_subdomain": "Cross-domain Earth sciences; participates in QA and Earth-Gold dialogue evaluations.",
            "simulation_task": "Answering QA items (MC/TF/FIB/FR) and producing Earth-Gold exploration dialogue responses.",
            "evaluation_metric": "MC/TF/FIB Accuracy; FR Win Rate and Semantic Similarity; Earth-Gold Retention/Diversity/SES.",
            "simulation_accuracy": "Earth-Iron: MC 92.42% | TF 86.26% | FIB 11.96% | FR-WR 92.05% | FR-SS 0.79; Earth-Silver: MC 53.60% | TF 64.40% | FIB 9.20% | FR-WR 44.40% | FR-SS 0.78; Earth-Gold: Retention 7.11% | Diversity 1.7158 | SES 0.1375.",
            "factors_affecting_accuracy": "Question format and dataset difficulty, lack of robust numerical/formula handling for FIB questions, and limited diversity in open-ended responses unless sampling/temperature tuned.",
            "comparison_baseline": "Benchmarked vs other mainstream LLMs in EarthSE; strong MC/TF accuracy but lower Earth-Gold retention versus top performers like Gemini-2.5 and DeepSeek-V3.",
            "limitations_or_failure_cases": "Low fill-in-the-blank accuracy and modest performance on more professional free-response items; open-ended answers often less innovative/detailed than reference.",
            "author_recommendations_or_insights": "CoT guidance and inference-time strategies can improve performance; authors suggest further domain-specific fine-tuning and improved numerical reasoning capabilities.",
            "uuid": "e9193.6",
            "source_info": {
                "paper_title": "EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Oceangpt: A large language model for ocean science tasks.",
            "rating": 2,
            "sanitized_title": "oceangpt_a_large_language_model_for_ocean_science_tasks"
        },
        {
            "paper_title": "OceanBench",
            "rating": 2,
            "sanitized_title": "oceanbench"
        },
        {
            "paper_title": "ClimaQA: An automated evaluation framework for climate foundation models.",
            "rating": 2,
            "sanitized_title": "climaqa_an_automated_evaluation_framework_for_climate_foundation_models"
        },
        {
            "paper_title": "The AI scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.",
            "rating": 1,
            "sanitized_title": "moosechem_large_language_models_for_rediscovering_unseen_chemistry_scientific_hypotheses"
        }
    ],
    "cost": 0.020714999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs
30 May 2025</p>
<p>Wanghan Xu 
Shanghai Jiao Tong University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Xiangyu Zhao 
Shanghai Artificial Intelligence Laboratory</p>
<p>The Hong Kong Polytechnic University</p>
<p>Yuhao Zhou 
Shanghai Artificial Intelligence Laboratory</p>
<p>Xiaoyu Yue 
Shanghai Artificial Intelligence Laboratory</p>
<p>Ben Fei 
Shanghai Artificial Intelligence Laboratory</p>
<p>The Chinese University of Hong Kong †</p>
<p>Fenghua Ling 
Shanghai Artificial Intelligence Laboratory</p>
<p>Wenlong Zhang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Lei Bai 
Shanghai Artificial Intelligence Laboratory</p>
<p>EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs
30 May 2025C81F29FA137BA27401CFAC6ED0BDD58CarXiv:2505.17139v3[cs.CL]
Cross-domain Evaluation of Mainstream LLMs on EarthSE.(a) EarthSE evaluates the capabilities of different LLMs in Earth's five spheres.(b) The multi-task evaluation demonstrates pronounced capability limitations in tasks such as calculation and term explanation.</p>
<p>Introduction</p>
<p>In recent years, the rapid development of large language models (LLMs) has continuously pushed the boundaries of their capabilities, demonstrating remarkable performance in general knowledge question-answering tasks [23].Against this backdrop, researchers are actively expanding the applications of LLMs to specialized and challenging scientific subfields [53,2].This trend not only enhances the reasoning ability of LLMs for complex scientific problems but also holds potential for AI-assisted scientific discovery [26,47].To systematically evaluate the performance of mainstream LLMs in scientific tasks, multiple science-oriented benchmarks [41,33,32] have been proposed.However, a comprehensive benchmark for evaluating LLMs in the critical field of Earth science remains conspicuously absent.Existing benchmarks predominantly fall into two categories: a) general science benchmarks (e.g., ScienceQA [27], SciBench [41]) which lack the necessary specificity and depth in Earth science, often featuring questions of a common-sense nature.b) single-subdomain benchmarks (e.g., ClimaQA [29], OceanBench [2]) that concentrate on particular areas such as climate or ocean science, thus failing to encompass the broad and interdisciplinary spectrum of Earth science.Furthermore, the prevalent question-answering (QA) format in most benchmarks overlooks the evaluation of LLMs in open-ended scientific exploration tasks.Consequently, the construction of a comprehensive and specialized Earth science benchmark that incorporates assessments of scientific exploration capabilities represents an underexplored yet vital area of research.</p>
<p>In this paper, we present EarthSE, a comprehensive and specialized benchmark dataset for Earth science that uniquely incorporates evaluations of scientific exploration capabilities.To ensure both sufficient scale and high quality, we curate a corpus of over 100,000 Earth science academic papers as our primary data source.Through semantic analysis of titles and keywords, we categorize these papers into five major spheres [28] and 114 sub-disciplines.Leveraging the publication venue and citation counts, we further stratify a subset of 10,000 papers into three distinct levels.The first level paper collection constitutes the largest portion, emphasizing comprehensiveness.The second level focuses on papers from high-impact journals, prioritizing specialized knowledge.The third level comprises highly cited papers, specifically designed for evaluating scientific exploration.Scientific exploration demands multi-level competencies, spanning fundamental domain knowledge mastery to advanced critical reflection and innovative improvements.The former, amenable to explicit evaluation criteria, suits question-answering formats.The latter, lacking unified standards, is better assessed through open-ended multi-turn dialogues.Correspondingly, we develop two pipelines: one for QA data using predefined task formulations to directly generate high-quality pairs from papers, and another for dialogue data structuring papers to extract scientific inquiry workflows for multi-turn dialogues.Both pipelines include automated cleaning and human verification for quality.Consequently, we release three distinct datasets, each with unique characteristics.Earth-Iron (QA) encompasses 4133 questions across 11 tasks and 114 sub-disciplines (Figure 2), emphasizing foundational and broad scientific exploration capabilities.Earth-Silver (QA) prioritizes high-difficulty, specialized knowledge.Earth-Gold (dialogue) focuses on evaluating advanced scientific exploration capabilities such as summarization, reflection, and innovation within open-ended dialogues.</p>
<p>We summarize the contributions of this paper as follows:</p>
<p>• We built two QA datasets to evaluate the fundamental capabilities of scientific exploration:</p>
<p>Earth-Iron, containing 4133 questions that span 114 subfields for broad assessment, and Earth-Silver, presenting more challenging and professional inquiries for deeper evaluation.• We innovatively propose Earth-Gold and a new metric (i.e., SES) to assess advanced capabilities of scientific exploration (e.g., methodology induction, limitation analysis, and concept proposal) through open-ended multi-turn dialogue.• Our systematic evaluation of 11 leading LLMs across these datasets reveals significant deficiencies in complex Earth science reasoning and open-ended scientific exploration.</p>
<p>Related Work</p>
<p>General Science Benchmark.In recent years, LLMs have increasingly focused on scientific reasoning [23], leading to the development of a range of benchmarks designed to assess model performance across diverse scientific domains [38,51].General-purpose science benchmarks such as Figure 2: EarthSE Covers 114 Disciplines in Earth Sciences.ScienceQA [27], SciBench [41] and MMLU-Pro [42] cover disciplines from elementary to graduate levels.For instance, ScienceQA includes over 21,000 multi-modal questions spanning natural, social, and language sciences.MMLU-Pro extends MMLU [16] with broader college-level subjects to evaluate deeper reasoning.SciBench emphasize college-level problem solving, drawing from canonical textbooks.Expert-curated datasets like GPQA [33], SuperGPQA [10], and HLE [32] offer high-quality, graduate-level questions to assess fine-grained domain expertise.However, these benchmarks primarily prioritize breadth over depth, often underrepresenting Earth sciences.As a unique and complex discipline covering five spheres, Earth science requires more targeted evaluation.Specialized Subject Benchmark.In addition to general benchmarks, several efforts have focused on subdomains within Earth sciences.For example, OceanGPT [2] introduces OceanBench, a benchmark for oceanographic tasks such as QA, fact verification, and applied writing.ClimaQA [29] and ClimateWatch [19] assess reasoning over climate data and models.ClimateBERT [43] and Clima-Text [39] support stance detection and document classification in climate discourse.Geological [8] tasks are addressed by GeoBench, which supports entity recognition and relation extraction, and SeafloorAI [30], which evaluates multimodal reasoning over sedimentary and geomorphological features.While these benchmarks are domain-relevant, each focuses on a narrow component of the Earth, lacking a unified framework for evaluating LLMs across the full Earth science spectrum.</p>
<p>Moreover, the majority of current benchmarks emphasize question answering, focusing on factual recall and reasoning over established knowledge.Yet, a critical frontier lies in enabling LLMs to support scientific exploration and discovery, a more open-ended capability involving summary and limitation analysis of existing methods, and hypothesis generation.focusing on a comprehensive evaluation of fundamental scientific exploration abilities.Earth-Silver (QA) features more difficult and challenging questions, focusing on more specialized Earth science knowledge.Earth-Gold (dialogue) evaluates advanced scientific exploration abilities through open-ended dialogue, focusing on reflecting on existing problems and proposing new approaches.</p>
<p>Task Definition from Foundational to Advanced</p>
<p>Foundational Science Task Definition.To comprehensively evaluate the capabilities of LLMs in Earth exploration tasks, spanning from foundational to advanced levels, we defined 11 fundamental research tasks, as detailed in Table 2.These tasks cover a broad spectrum, ranging from basic understanding and reasoning to advanced research skills.These capabilities form the foundation for LLMs to conduct scientific exploration.For example, the "Dataset" task specifically focuses on datasets within the Earth sciences.Given the significant diversity of data (such as one-dimensional seismic wave data and two-dimensional remote sensing images) across the numerous sub-disciplines of Earth science, enhancing LLMs' understanding of these varied datasets is crucial.</p>
<p>Scientific Exploration Task Definition.Beyond these 11 fundamental scientific exploration tasks, we have also defined a scientific discovery task formulated as open-ended dialogue.Through systematic analysis of these papers, we identify a recurrent research pattern: "analyzing limitations of existing work → proposing novel methods" [49].This inspires our formalization of scientific exploration as an iterative self-negation process, mathematically expressed as:
(M i+1 , L i+1 ) = LLM(M i , L i ),
where M denotes methodology, L represents limitation analysis of M , and i indicates dialogue turns.This recursive framework simulates the human scientific process of critically examining prior work's constraints and progressively improving upon them.Consequently, models with genuine scientific discovery potential must demonstrate robust self-critique and self-improvement capabilities [21].</p>
<p>Paper Corpus Collection</p>
<p>The study uses Earth science academic papers as data because: (a) their dense, high professional knowledge facilitates quality data creation; (b) their structured format aligns with general scientific discovery processes.During collection, we obtain and convert 100,000 PDFs to structured JSON using MinerU [40].Semantic similarity [9] on abstracts and Earth sphere keywords (see Figure 7 for details) accurately classifies papers into five Earth spheres, as detailed in Table 3.</p>
<p>We define the initial collection of 100,000 papers as the base dataset P base .From this collection, we first select a subset P hj comprising 10,000 papers published in high-impact Earth science journals (see Table 7 for details).We then extract the top 10% most cited papers from P hj to form the high-citation core dataset P hc with 1,000 papers.The entire selection process maintains strict balance across all Table 2: Foundational Task Categories and Descriptions.</p>
<p>Task Description</p>
<p>Understanding</p>
<p>Term Explanation [15] Requires defining technical concepts (e.g., "Define 'medium-range weather forecast' and explain its significance").Knowledge QA [46] Requires factual explanations with detailed descriptions of distributions or characteristics (e.g., "Describe the distribution of fishery resources in the East China Sea").Fact Verification [52] Involves validating claims' accuracy (e.g., "Verify whether the reported sea level rise data is correct").</p>
<p>Reasoning</p>
<p>Analysis [5] Demands logical reasoning to draw conclusions from data (e.g., "Determine which evidence supports a given conclusion").Relation Extraction [22] Needs analysis of interconnections between entities (e.g., "Summarize ecological relationships between butterflies and plants").Calculation [34] Involves multi-step mathematical operations with numerical answers (e.g., "Calculate mean annual precipitation from the dataset").</p>
<p>Research</p>
<p>Tool Utilization [50] Involves recommending domain-specific methodologies (e.g., "Suggest atmospheric models for weather prediction").Literature Citation [4] Demands proper academic references (e.g., "List key publications on ocean circulation including citations like (Ravuri et al., 2021)").Dataset [35] Requires introducing or recommending research datasets (e.g., "Recommend ERA5 [17] for medium-range weather forecasting").Experimental Design [6] Needs detailed methodological planning (e.g., "Design an experiment to investigate soil moisture's impact on photosynthesis").Code Generation [13] Involves writing functional code (e.g., "Visualize rainfall data using Python's Matplotlib library [3]").</p>
<p>five spheres, with detailed distributions provided in Table 3. Figure 4 further illustrates the citation distribution patterns across different spheres in the high-citation dataset P hc .P base , P hj , P hj are used to construct Earth-Iron, Earth-Silver, Earth-Gold respectively, as depicted in Figure 3.  Task Selection.Before QA construction, a small LLM analyzes each paper's abstract to determine the most suitable task type from 11 foundational tasks shown in Table 2 for QA generation.This content-aware assignment enhances question relevance.For instance, papers containing substantial numerical results preferentially generate computation questions.</p>
<p>QA Construction.In the question generation phase, paper content and task prompts are fed into GPT-4 [1] to produce QA pairs.To ensure answer accuracy, the model is required to provide complete Chain-of-Thought (CoT) [44] reasoning that includes both the final answer and its derivation process.These CoTs not only form integral parts of the questions but also serve as critical references for subsequent data cleaning, effectively mitigating errors caused by LLM hallucinations [45].</p>
<p>Data Cleaning.The data cleaning process employs a dual-phase strategy: (a) rule-based primary cleaning for formatting issues (missing options, irregular answers, improper references); (b) semantics-based advanced cleaning using LLMs to detect deeper problems (multiple correct options, irrelevant/incorrect answers).Throughout this process, CoT reasoning plays a pivotal role, as its explicit step-by-step derivation provides reliable evidence for error detection.</p>
<p>After data cleaning, we performed difficulty screening using mainstream LLMs.Based on testing results from both proprietary and open-source models, we removed questions with accuracy rates exceeding 80% to ensure sufficient overall challenge.For questions with accuracy rates between 60% and 80%, human experts determined their retention based on the question's value.</p>
<p>Earth-Gold: Open-end Dialogue Benchmark for Exploration Task</p>
<p>Most existing benchmarks predominantly employ question-answering formats, which, while effective for knowledge assessment, fail to capture the open-ended exploration paradigm characteristic of genuine scientific research.To evaluate LLMs' capabilities in open scientific exploration, we construct the Earth-Gold dataset derived from P hc , a collection of 1,000 highly-cited Earth science papers, as shown in the second row of Figure 3.These papers exemplify superior scientific reasoning patterns, making them ideal prototypes for studying scientific exploration behaviors.</p>
<p>Paper Structurization Following the scientific exploration task definition in Section 3.1, we decompose each paper in P hc into four structured components:</p>
<p>• Existing Method Summary (M 0 ): Extracted from related work sections, providing comprehensive synthesis of current methodologies.• Limitation Analysis (L 0 ): Derived from motivation sections, identifying precise shortcomings that constitute the starting point for new research.• Novel Method Proposal (M 1 ): Abstracted from methods sections, capturing the core innovations.• New Method Limitations (L 1 ): Distilled from discussion sections, anticipating potential constraints of proposed solutions.</p>
<p>Dialogue Construction With M 0 , L 0 , M 1 , and L 1 extracted, we employ GPT-4 to generate two-turn dialogues simulating human-AI collaborative scientific exploration.The first turn requires the AI assistant to summarize existing methods and critically analyze their limitations given a research direction; the second turn directs the assistant to propose improved methods addressing these limitations while objectively assessing the new methods' potential constraints.This dialogue structure authentically replicates human ideation processes assisted by AI, demanding both extensive scientific knowledge and critical thinking abilities from LLMs.</p>
<p>Human Expert Validation Following dialogue generation, domain experts conduct rigorous quality control using multi-criteria evaluation: a) Information Density Scoring: Higher scores are assigned to dialogues containing specific analytical methods, datasets, or well-defined events (e.g., earthquakes, typhoons), while vague discussions receive lower scores.b) Methodological Quality Assessment: Proposed methods receive higher scores if they are systematically enumerated and concretely implementable.Non-specific proposals are filtered out.Through this stringent validation process, we curate the final Earth-Gold dataset, which exclusively comprises high-quality scientific exploration dialogues, thus establishing a new benchmark for evaluating LLMs' research capabilities.</p>
<p>A New Metric: Scientific Exploration Score (SES).Earth-Gold evaluates open-ended scientific dialogue, where standard correctness metrics are inadequate due to the inherent diversity of valid scientific exploration beyond definitive answers.To address this, we innovatively measure performance using retention rate and diversity.For each user question in the dialogues, the LLM generates M diverse responses (temperature=0.6)[31], and then we compute:</p>
<p>• Retention Rate (r): GPT-4 ranks M generated answers and the reference answer by reflective depth and innovation.Retention rate r = (i − 1)/M (where i is the reference answer rank) quantifies the model's preservation of high-quality scientific reasoning.If r = 0, it means that all LLM answers are worse than the reference answers.</p>
<p>• Diversity (d): We compute embeddings v i for each response using sentence-transformers, then derive the mean vector v.The average cosine similarity s between v and v i is calculated as Equation 1.Since lower similarity indicates higher diversity, define d = 1/s.
s = 1 M M i=1 cos(v i , v), where v = 1 M M i=1 v i .(1)</p>
<p>Experiment</p>
<p>Experimental Setup</p>
<p>This paper introduces three evaluation datasets: Earth-Iron and Earth-Silver as question answering (QA) datasets, and Earth-Gold as an open-ended dialogue dataset.The QA datasets incorporate 4 question formats: multiple-choice (MC), fill-in-the-blank (FIB), true/false (TF), and free-response (FR) questions.For MC, FIB, and TF questions, we use accuracy (ACC) as the metric; for FR questions, we employ the win rate (WR) against the reference answer evaluated by GPT-4, and semantic similarity (SS) as metrics.These metrics are detailed in Appendix D. For Earth-Gold, we use the SES defined in Section 3.4 to evaluate the performance of scientific exploration dialogues.</p>
<p>Earth-Iron/Silver: Assessing Broad Foundational Capabilities</p>
<p>Earth-Iron is a comprehensive QA benchmark consisting of 4133 questions spanning the Earth sciences domain.Table 4 presents comparative performance metrics across these question formats for various LLMs.Most models perform well on multiple-choice questions but struggle with fillin-the-blank tasks.In free-response questions, performance varies significantly; some models (e.g., Claude-3.5)provide overly general answers, resulting in poor Win Rate against the reference answers.</p>
<p>We compute each model's overall competency across Earth's five spheres, as detailed in Figure 1.This figure visually illustrates the capability distribution of different models across the five spheres.Notably, Grok-3 achieves state-of-the-art (SOTA) performance across all spheres.While most models exhibit a relatively balanced capability distribution, such as DeepSeek-V3, Gemini-2.5,some show specific weaknesses, for example, GPT-4o in the cryosphere.</p>
<p>The right panel in Figure 1 illustrates the capabilities of different models across the 11 fundamental scientific tasks.Most models exhibit a similar capability distribution, particularly the SOTA LLMs, likely due to similar training paradigms.Notably, most models perform relatively poorly on the calculation task.Additionally, the term explanation metric is generally low, indicating a potential weakness in LLMs' understanding of specialized Earth science terminology.Compared to Earth-Iron, Earth-Silver is constructed from a higher-quality corpus of original papers, resulting in a significantly more challenging dataset.The experimental results on the right side of Table 4 show that most LLMs achieve only 54% accuracy on multiple-choice questions (compared to a 25% random chance rate).This not only reflects the high difficulty of Earth-Silver but also indicates considerable room for improvement in the performance of current LLMs within the Earth sciences.On fill-in-the-blank tasks, the average performance of LLMs is notably low, at just 11%.</p>
<p>Figure 5 presents representative examples from these challenging tasks alongside model responses.The case study analysis reveals frequent formula misuse and computational errors in Earth sciencespecific calculations, indicating critical knowledge gaps in domain-specific reasoning.and answer quality (retention rate r).Table 5 shows performance with each model generating M = 3 responses for r and d calculation.</p>
<p>Earth-Gold: Evaluating Open-end Science Exploration Capability</p>
<p>The results indicate that most models achieve a retention rate of less than 50%, suggesting that over half of the generated responses underperform the reference answers.Regarding the diversity metric, most models exhibit low diversity scores.This indicates that when responding to open-ended questions, LLMs tend to generate similar answers across multiple attempts, which significantly contrasts with the divergent thinking characteristic of human scientists during scientific exploration.</p>
<p>Our analysis identifies three primary issues in these subpar responses: (1) overly generic content lacking specific details, (2) non-specific analyses of limitations, and (3) excessively broad proposed solutions.Figure 6 provides a visual comparison between high-quality and low-quality responses.FIB ACC in Table 4 show lower accuracy, likely due to their inherent difficulty compared to MC/TF.Since our question construction generates step-by-step explanations resembling CoT reasoning, we investigate if providing LLMs with initial CoT steps during inference improves performance on these challenging questions.</p>
<p>CoT Guidance Enhances Performance on Challenging Questions</p>
<p>Table 6 illustrates the model's accuracy with varying numbers of provided CoT steps.As is evident, increasing the number of CoT steps offered as hints significantly improves the model's accuracy.This suggests that for more challenging questions, even without modifying the model's fundamental capabilities, leveraging CoTaugmented reasoning at inference time can substantially boost performance.This finding provides a promising foundation for inference-time scaling strategies.</p>
<p>Conclusion</p>
<p>The The limitation of this work lies in the fact that it does not integrate the 11 tasks to create a more complex task chain, highlighting a gap in achieving automated scientific discovery.</p>
<p>A Paper Selection and Tiered Classification</p>
<p>The proposed benchmark derives from 100,000 papers downloaded from OpenDataLab [14], which undergo a tiered classification process resulting in three distinct paper collections:</p>
<p>• Base paper collection (P base ) • High-journal-impact papers (P hj )</p>
<p>• High-citation papers (P hc )</p>
<p>The P base collection is constructed through semantic similarity analysis of paper titles and keywords.</p>
<p>Our methodology employs both positive and negative keywords to enhance selection precision.Negative keywords effectively filter out semantically related but thematically irrelevant papers.For instance, when using "earth" as a positive keyword, astronomy-related papers may incorrectly appear relevant.To address this, we introduce "cosmos" as a negative keyword, requiring papers to maintain a similarity score below a specified threshold (0.1) with negative keywords while exceeding a minimum threshold (0.2) with positive keywords.This dual-threshold approach yields the final P base collection of 100,000 papers.</p>
<p>Using an analogous positive and negative keyword approach, we accurately classify papers into five Earth science subdomains (hydrosphere, biosphere, lithosphere, atmosphere, and cryosphere).The complete domain-specific keyword system is presented in Figure 7.</p>
<p>Positive and Negative Keywords</p>
<p>Overall Positive Keywords Earth, Earth system, hydrosphere, biosphere, lithosphere, atmosphere, cryosphere.</p>
<p>Overall Negative Keywords cell biology, virus, pharmaceuticals, chemistry, physics, astronomy, food science, proteins, microbiology.</p>
<p>Hydrosphere Cryosphere Negative Keywords frozen food, ice cream, refrigeration, freezing, cold storage, ice cubes, food preservation, chilling, frost.From the initial paper collection P base , we select a subset of papers P hj published in high-quality journals based on multiple criteria including impact factor (IF) [11], disciplinary ranking, and expert assessment.Representative journals are listed in Table 7.</p>
<p>Table 7: Partial List of Selected Journal Titles.</p>
<p>Finally, we select the 1000 papers P hc with the highest number of citations from P hj to form a collection.The distribution of papers in each collection is shown in Table 3.</p>
<p>B QA Construction</p>
<p>When we construct QA questions, the prompts we use will vary depending on the format and task of the question.For questions of different formats, we use the following prompts.</p>
<p>Prompt for Different Question Formats</p>
<p>Free Form QA Free-form questions and answers allow for open-ended responses.These questions typically require detailed explanations, recommendations, or descriptions.The answers can vary in length and structure, depending on the complexity of the question.For example, a question about improving the marine environment may require a list of measures with explanations.Format Rules: Answers can be in any format, including sentences, lists, or paragraphs.There are no strict restrictions on length or structure.</p>
<p>Multiple Choice</p>
<p>Multiple-choice questions provide a question followed by several answer choices, typically labeled with letters (e.g., A, B, C, D).Only one choice is correct.The task is to select the correct choice by providing the corresponding letter.Format Rules: The question requires a prompt along with four options, one of which is the correct answer.The answer must be a single letter corresponding to the correct choice (e.g., 'A', 'B', 'C', or 'D').The answer should not include the full text of the choice.</p>
<p>True False</p>
<p>True/false questions require determining whether a given statement is correct or incorrect.The answer must be either 'True' or 'False', depending on the accuracy of the statement.Format Rules: The answer must be exactly 'True' or 'False'.No additional text or explanations are allowed.</p>
<p>Free Form QA Fill-in-the-blank questions provide a sentence or statement with one or more missing words, phrases or number.The task is to complete the sentence by filling in the blank(s) with the correct word(s).Format Rules: The answer must be the exact word or phrase that fits the blank.No additional text or explanations are allowed unless explicitly requested.</p>
<p>Figure 8: Prompt for Different Question Formats.</p>
<p>The prompts for different tasks are shown in Table 2.The complete QA prompt is shown in Figure 9.</p>
<p>Complete QA Construction Prompt</p>
<p>System Prompt You are a highly skilled scientist with a deep expertise in reading and analyzing scientific literature.Below is a research paper that you will carefully examine.Your task is to generate a well-structured question and answer based on the provided instructions.Ensure that your output is precise, relevant, and adheres to the specified guidelines.Instructions 1. Format: <Format Prompt> 2. Task: <Task Prompt> 3. Answer Explanation: Each question must be accompanied by a corresponding answer explanation.</p>
<ol>
<li>Output Format: The output should be structured as a dictionary, including the following keys:</li>
</ol>
<p>question: The generated question.</p>
<p>answer: The correct answer to the question.</p>
<p>explanation: A list containing the explanation(s) for the answer.5. Select One Specific Detail Do not generalize the entire paper into a question.Instead, carefully select the most relevant and specific part of the paper (e.g., a key finding, methodological detail, or discussion point) and craft a detailed question around it.The question should be highly specific and require a nuanced understanding of the paper to answer.Avoid generating general or overly simplistic questions.</p>
<p>Content Relevance and Accuracy</p>
<p>Ensure that the questions are directly derived from the content of the paper.Avoid generating questions that are irrelevant or based on incorrect facts.The questions should accurately reflect the findings, methodologies, or discussions presented in the paper.7. Independent and Complete Question, Answer, and Explanation -Self-contained:</p>
<p>The question, answer, and explanation must be self-contained and complete.Do not use phrases such as "this article" "this paper" "according to the article" "according to the paper" or similar references to the paper.</p>
<p>-Avoid Personal Pronouns: Additionally, avoid using personal pronouns like "we" or "our."</p>
<p>The question, answer, and explanation must stand alone and be understandable without additional context.</p>
<p>-Avoid Analysis of the Paper: Do not include any analysis of the paper in the questions, answers, or explanations.Instead, transform the analysis into independent statements that are self-sufficient and do not rely on the paper for context.Example { "question": "The generated question based on the paper.","answer": "The corresponding answer to the question.","explanation": [ "Step 1: Explanation for the first step.","Step 2: Explanation for the second step.","Step 3: Explanation for the third step."] } Figure 9: Complete QA Construction Prompt.</p>
<p>After constructing the QA pair, we will also refine the QA.The main purpose is to remove extra references that may exist in the QA, such as mentioning papers or non-existent figures in the question.The refine prompt is as follows.</p>
<p>QA Refine Prompt</p>
<p>System Prompt</p>
<p>Please modify the following question, answer, and explanation to remove any expressions related to "paper," "article," "study," or similar references.Ensure that the question is complete and can be answered directly based on the explanation provided, without requiring any additional context or knowledge of the paper.After modification, maintain the original dictionary format and ensure that the meaning of the questions, answers, and explanations remains unchanged.Instructions 1. Remove References to the Paper:</p>
<p>-Eliminate any phrases such as "this paper," "the article," "according to the study," or similar references.</p>
<p>-Ensure the question, answer, and explanation are self-contained and do not rely on external sources for understanding.</p>
<p>Maintain Clarity and Completeness:</p>
<p>-The question should be clear, specific, and able to stand alone, without referencing the paper.</p>
<p>-The answer should directly address the question without referencing the paper.</p>
<p>-The explanation should provide sufficient detail to justify the answer, using independent and self-sufficient statements, without referencing the paper.</p>
<p>Preserve the Dictionary Format:</p>
<p>-Keep the output in the original dictionary format, including the keys 'question', 'answer', and 'explanation'.</p>
<p>-Ensure that the meaning of the questions, answers, and explanations remains unchanged.</p>
<p>C Scientific Exploration Dialogue Construction</p>
<p>The Earth-Gold dataset construction from the high-citation paper collection P hc involves a two-phase processing pipeline:</p>
<ol>
<li>Paper Structuring Phase: Decompose each paper into four core components: The dialogue construction prompt is presented as follows:</li>
</ol>
<p>Dialogue Construction Prompt</p>
<p>System Prompt</p>
<p>You are an AI research assistant specializing in refining supervised fine-tuning (SFT) data for large language models.Your task is to enhance the quality and linguistic diversity of the provided SFT data while preserving its original structure and content.Instructions 1. Task Description:</p>
<p>-Polish the text in the SFT data, including both the "user" and "assistant" parts.</p>
<p>-Ensure the polished text is clear, concise, and linguistically diverse while maintaining the original meaning and intent.</p>
<p>Refinement Requirements:</p>
<p>-Articulate the logic with utmost clarity, employing logical conjunctions to underscore logical relationships where pertinent.</p>
<p>-Elaborate on the content with as much detail as possible, retaining specific numerical values, dataset names, method names, author names, etc., and ensure the content is highly professional and information-rich.</p>
<p>-While retaining the original meaning of the sentence, appropriately add or modify it to make the unsmooth conversation content smooth.</p>
<p>Constraints:</p>
<p>-Do not add, remove, or alter the number of dialogue turns.</p>
<p>-Preserve the original structure and role labels ("user" and "assistant").</p>
<p>-Use a variety of linguistic styles, vocabulary, and phrasing to increase the diversity of the text.</p>
<p>Output Requirements:</p>
<p>-Output the modified SFT data as a dialogue list in the same JSON format as the original.</p>
<p>-Ensure the polished text is natural, engaging, and suitable for training large language models.Example [ {"role": "user", "content": "<INPUT1> Could you provide an overview of related work and discuss their key limitations?"},{"role": "assistant", "content": "Certainly.The related works include the following.<IN-PUT2> However, these approaches face several limitations.<INPUT3>"}, {"role": "user", "content": "Given these existing works and their limitations, can you propose a new method and evaluate its potential drawbacks?"},{"role": "assistant", "content": "Building on these foundations, the proposed method is structured as follows.<INPUT4> Despite its advantages, this method has certain limitations.</p>
<p><INPUT5>"} ]</p>
<p>Figure 12: Dialogue Construction Prompt.</p>
<p>The <INPUT1> to <INPUT5> in the prompt are replaced by the values in the dictionary obtained in paper structuring.</p>
<p>D General Metrics</p>
<p>Accuracy (Acc.)For closed-form questions (multiple-choice, fill-in-the-blank, and true/false) with deterministic correct answers, we calculate accuracy as the exact match rate between model outputs and reference solutions.</p>
<p>Win Rate (WR) [2] For free-response questions, we employ GPT-4 as an impartial judge to compare model answers against reference answers along three dimensions: relevance (degree of topical alignment, penalizing tangential responses), scientific rigor (logical consistency and factual correctness), and specificity (depth of targeted analysis, penalizing generic statements).The evaluation protocol ensures fairness by blinding GPT-4 to answer origins.Win rate quantifies the percentage of cases where the evaluated model's response surpasses the reference.</p>
<p>Semantic Similarity (SS) [9] For free-response questions, we additionally measure the conceptual alignment between model outputs and reference answers using sentence embeddings.Specifically, we utilize sentence-transformers to project responses into a 384-dimensional semantic space, then compute their cosine similarity as the semantic similarity metric.</p>
<p>E QA Examples</p>
<p>Question</p>
<p>In the comparison between SP-CCSM4 and CCSM4, the difference in projected ENSO-shear relationships is attributed to the varying intensity and spatial extent of anomalous westerlies at upper levels during El NINO events.Specifically, while the climatology of easterly flow at <strong><em>_</em> simulated in CCSM4 is weaker and retreated eastward compared to SP-CCSM4, the projected westerly anomalies due to ENSO are </strong>___ in CCSM4 relative to those in SP-CCSM4.</p>
<p>Explanation</p>
<p>Step 1: The climatology of easterly flow at 200 hPa is noted to be weaker and shifted eastward in CCSM4 compared to SP-CCSM4.</p>
<p>Step 2: During El NINO events, CCSM4 projects stronger westerly anomalies than SP-CCSM4 at upper atmospheric levels, contributing to variability in ENSO-shear relationships.</p>
<p>Step 3: These differences in the intensity and spatial distribution of westerly anomalies directly influence the projected ENSO-shear relationships, highlighting the distinct behavior between the two models in response to ENSO in a warmer climate.Answer 200 hpa, stronger Step 1: Aerosol particles are captured in cloud drops through nucleation and impaction scavenging, with their subsequent redistribution depending on the stochastic collision-coalescence processes of drop interactions.</p>
<p>Step 2: As smaller drops merge and grow, aerosol particle mass becomes concentrated in larger precipitation-sized drops, which are more effective in removal due to their increased mass and falling speed.</p>
<p>Step 3: The stochastic collision-coalescence process also results in varying contamination levels, with smaller drops being highly contaminated due to their increased aerosol-to-water mass ratios, while larger drops ensure bulk removal of aerosol mass, significantly affecting the efficiency of wet deposition mechanisms in clouds.</p>
<p>Answer</p>
<p>The redistribution of aerosol particles captured via nucleation and impaction scavenging significantly alters the dynamics of aerosol removal by moving the aerosol mass into larger drop size categories, thereby concentrating aerosol particles in precipitation-sized drops.</p>
<p>During stochastic collision-coalescence processes, smaller cloud drops merge and grow, carrying the aerosol particle mass with them, leading to a coupling between water mass and aerosol mass growth.This results in a redistribution where the main aerosol mass shifts to drops of precipitation size, which are more effective at removing particles from the cloud environment.Additionally, smaller drops within the cloud exhibit higher contamination levels due to their higher aerosol-to-water mass ratio, while larger precipitation drops accumulate the majority of aerosol mass, thereby ensuring efficient removal from the atmosphere.</p>
<p>F Examples of Scientific Exploration Dialogues</p>
<p>Earth-Gold: Scientific Exploration Dialogue Example 1</p>
<p>User I am interested in the study of sedimentary cycles and their depositional rates over extensive geological time spans, with a particular emphasis on how the likelihood of sedimentary rock destruction changes with age.Could you provide a comprehensive overview of the existing literature in this area, highlighting the principal limitations of these studies?Assistant Absolutely.The primary works in this domain include Gregor's seminal approach, where he aimed to estimate the global denudation rates across various geological epochs.He derived depositional rates by analyzing the mass-age distribution of current sediments.Nonetheless, these methodologies encounter certain restrictions.For instance, Gregor himself cast doubt on his findings regarding depositional rates during the pre-Carboniferous phases of the Phanerozoic eon.His assumption of a uniform probability of destruction for all sedimentary rocks, regardless of their age, failed to consider fluctuations in the half-lives of rocks over time.</p>
<p>User Considering these existing studies and their constraints, could you suggest an innovative approach and discuss its potential drawbacks?Assistant Leveraging the insights from previous studies, I propose a novel approach which follows this outline: recharting Gregor's data using semilogarithmic graphs, converting the data into units of grams per year, and fitting these values as exponential functions.This would be complemented by modeling sedimentary cycles that exhibit different half-lives to account for mass-age relationships.Despite its innovative aspects, this approach is not without limitations.While it successfully elucidates Gregor's mass-age associations, it lacks uniqueness and posits a steady total sedimentary mass across cycles, which might overlook some of the more intricate geological phenomena.Earth-Gold: Scientific Exploration Dialogue Example 2</p>
<p>User</p>
<p>Please examine the interplay and responsiveness of the convective boundary layer (CBL) under diverse atmospheric and oceanic conditions, including sea surface temperature (SST), surface wind speed, and tropospheric moisture levels.Could you provide a summary of existing research and highlight their main shortcomings?Assistant Certainly.Related studies include the following.Betts (1973) introduced a closure model for the subcloud layer that utilized an adapted subsidence approach and energy balance closures consistent with Sarachik's tropical model (1978).Nonetheless, these methods exhibit several limitations.For instance, models by Betts and Sarachik were based on assumptions that might not thoroughly represent the coupling dynamics between the CBL and varying atmospheric parameters.User Considering these previous studies and their limitations, could you propose a novel approach and assess its potential limitations?Assistant Drawing inspiration from these constructs, the proposed approach is defined as follows.This work formulates a detailed model to analyze the CBL equilibrium by evaluating uncoupled solutions influenced by SST, wind speed, and tropospheric variations, along with assessments of coupled troposphere solutions and their responsiveness to changes.Despite its benefits, this method does contain some limitations.The model might encounter challenges in accurately forecasting the CBL's behavior under extreme climatic conditions or when integrating intricate feedback mechanisms not entirely accounted for in the theoretical framework.</p>
<p>Figure 3 :
3
Figure 3: Construction Process of EarthSE.Automated construction of three-tiered benchmarks (Earth-Iron, Earth-Silver, Earth-Gold) from 100K papers, enabling comprehensive evaluation of knowledge coverage, professional proficiency, and scientific exploration capabilities.</p>
<p>Figure 4 :
4
Figure 4: Citations of Papers in P hc .3.3 Earth-Iron/Silver: QA Benchmark for Foundational TasksEarth-Iron and Earth-Silver are constructed from P base and P hj respectively, using the same pipeline as shown in the first row of Figure3.Characteristically, Earth-Iron offers a large and comprehensive set of questions, while Earth-Silver features more difficult and specialized questions.</p>
<p>Figure 5 :
5
Figure 5: Case Study from Earth-Silver.Some models make mistakes when applying formulas in multiple steps, resulting in low accuracy in calculation questions.</p>
<p>Figure 6 :
6
Figure 6: Case Study from Earth-Gold.Earth-Gold is used to evaluate the performance of LLMs in open scientific exploration.For the same user questions, the reference answers contain more details, such as the yellow text, while the bad answers are very general.</p>
<p>Figure 7 :
7
Figure 7: Positive and Negative Keywords.</p>
<p>Figure 10 :
10
Figure 10: QA Refine Prompt.</p>
<p>Figure 15 :
15
Figure 15: QA Example 3.</p>
<p>Figure 16 :
16
Figure 16: QA Example 4.</p>
<p>Figure 17 :
17
Figure 17: Scientific Exploration Dialogue Example 1.</p>
<p>Figure 18 :
18
Figure 18: Scientific Exploration Dialogue Example 2.</p>
<p>Table 1 :
1
Comparison between Existing Benchmarks and EarthSE.Comparison across (1) question volume, (2) data sources, (3) Earth sphere coverage, (4) graduate-level difficulty, (5) multiple subsets and (6) scientific exploration assessment.EarthSE uniquely offers comprehensive domain coverage, professional difficulty, and evaluation of scientific exploration in Earth sciences.
Benchmark#Ques.SourceEarth Cover Grad-Diff Multi-Sub Scientific ExplorationScienceQA [27]21,208High School Courses××××MMLU-Pro [42] 12,032Exam Questions××××SciBench [41]86910 Textbooks××××GPQA [33]448Expert Curated×✓✓×OceanBench [2]12,426Domain Texts×✓××ClimaQA [29]350218 Textbooks×✓✓×GeoBench [8]2439Exam Questions×✓××EarthSE4133100,000 Earth Science Papers✓✓✓✓3 EarthSE: Earth Science Exploration Benchmark
Overview.Scientific exploration requires capabilities from basic Earth science knowledge to advanced reflection and proposing new methods.Starting from 100,000 papers in the Earth sciences, we constructed three datasets.Earth-Iron (QA) contains 4133 questions covering 114 sub-disciplines,</p>
<p>Table 3 :
3
Number of Papers on the Five Spheres of Earth.
Earth Sub-domainP baseP hjP hcBiosphere21,2481,554201Lithosphere22,8202,357236Atmosphere24,2132,401240Hydrosphere23,4252,254226Cryosphere11,4021,969217
Total 103,108 10,535 1,120</p>
<p>Table 4 :
4
Evaluation Metrics for Leading Models on the Earth-Iron and Earth-Silver Benchmarks.For MC (Multiple Choice), TF (True/False), and FIB (Fill-in-the-Blank) questions, Accuracy (ACC) is used as the evaluation metric, reported in percentage (%).For FR (Free Response) questions, Win Rate (WR), Semantic Similarity (SS) are employed as an evaluation metrics.
Earth-Iron (More Comprehensive)Earth-Silver (More Challenging)ModelMC↑ TF↑FIB↑ FR-WR↑ FR-SS↑ MC↑ TF↑FIB↑ FR-WR↑ FR-SS↑Llama-3.1-8B [12]59.41 74.36 2.5213.700.7636.00 54.00 2.024.400.72Llama-3.1-70B [12] 91.56 87.91 6.6361.850.8056.00 63.60 4.0018.400.80Qwen-2.5-72B [48] 92.42 86.26 11.96 92.050.7953.60 64.40 9.2044.400.78DeepSeek-V3 [25]93.40 81.14 18.99 97.600.8158.00 56.40 12.80 75.200.81GPT-4o [18]93.28 88.28 19.12 82.000.8155.60 69.60 18.40 22.000.80Gemini-1.5 [37]90.83 75.82 13.65 95.600.7954.40 44.80 8.0062.800.78Gemini-2.0 [36]92.67 87.55 14.69 77.100.7754.40 72.40 11.60 34.800.75Gemini-2.5 [36]93.15 77.84 17.02 95.810.7558.00 55.60 13.65 74.300.75Claude-3.5 [20]91.08 83.52 12.48 12.050.7956.80 60.80 9.604.400.77Claude-3.7 [24]94.01 61.90 20.68 75.000.8062.40 41.20 17.20 28.400.79Grok-3 [7]93.03 88.64 21.85 98.700.8153.20 70.40 15.20 83.600.81Mean89.53 81.20 14.50 72.860.7854.40 59.38 11.06 41.150.77
The Scientific Exploration Score (SES) combines retention (r) and diversity (d) multiplicatively: SES = r × d.Since s is relatively close to 1, we normalize s to [0.9, 1] in actual calculations for better comparison, that is, SES = r 10×(s−0.9) .Higher SES values indicate superior open-ended scientific thinking, reflecting both answer quality (retention) and conceptual breadth (diversity).</p>
<p>Table 5 :
5
Scientific Exploration Capabilities of Mainstream Models on Earth-Gold.Earth-Gold assesses LLMs' research potential through open-ended scientific dialogues, employing three metrics: Retention (proportion of responses outperforming references), Diversity (measure of divergent thinking), and their composite Scientific Exploration Score (SES).
ModelRetention (%) ↑ Diversity ↑ SES ↑Llama-3.1-8B8.003.98130.3301Llama-3.1-70B 11.781.48910.2453Qwen-2.5-72B 7.111.71580.1375DeepSeek-V338.001.69420.6599GPT-4o9.441.03470.0981Gemini-1.519.671.44370.1989Gemini-2.018.222.62900.6505Gemini-2.550.562.70161.3710Claude-3.514.671.55170.2396Claude-3.731.891.71300.5465Grok-317.221.52840.2727</p>
<p>Table 6 :
6
The
DeekSeek-V30 (Baseline)12.80121.60229.60345.60GPT-4o0 (Baseline)18.40125.60238.80355.60
Impact of UsingCoT Guidance on FIB ACC.Inspired CoT Steps FIB (%) ↑</p>
<p>You are an AI research assistant with expertise in analyzing and structuring academic papers.Your task is to extract and organize the content of a research paper into five specific sections: research direction, methods of previous work, limitations of previous work, method of • Round 2: Building upon Round 1, the LLM proposes innovative methods (M 1 ) while reflecting on potential limitations (L 1 ).This round assesses the model's scientific innovation capability and critical reflection skills.
• Summary of existing methods (M 0 )• Limitations of existing methods (L 0 )• Proposed new methods (M 1 )• Potential limitations of new methods (L 1 )2. Dialogue Generation Phase: Generate two-turn scientific exploration dialoguesThe paper structuring prompt is presented as follows:Paper Structuring PromptSystem Prompt</p>
<p>QuestionWhat factor plays a pivotal role in the formation of sapropel S1 in the Mediterranean, and what evidence suggests its influence during the last interglacial insolation maximum?A) Increased wind stress and evidence from high sedimentation rates.B) Monsoon-fueled freshwater input from the Nile River and synchronized timing of sapropel S1 deposition with wetter conditions in North Africa.C) Enhanced salinity from increased evaporation and evidence from salinity isotope ratios.D) Elevated ocean temperatures and evidence from fossil coral records.
Figure 14: QA Example 2.QA Example 3 (Fill in the Blanks)QA Example 1 (Multiple Choice)ExplanationMonsoon-fueled freshwater input, particularly from the Nile River, significantly affectedsapropel S1 formation.The timing of sapropel S1 deposition in the eastern Mediterranean closely coincided withperiods of increased Nile River discharge due to enhanced monsoon strength over NorthAfrica.Evidence suggests these monsoonal shifts resulted in wetter conditions, which aligned withthe insolation maxima during the last interglacial, promoting the deposition of organic-richsapropels.AnswerBFigure 13: QA Example 1.QA Example 2 (True or False)QuestionAtmospheric humidity in Nanjing's urban canopy layer demonstrates significant nighttimedifferences among local climate zones, particularly due to moisture deficits during coldermonths, and shows clear seasonal patterns in humidity ratios with greater discrepancies inwarmer months than colder months.ExplanationStep 1: Significant differences in atmospheric humidity among local climate zones areobserved during nighttime, highlighting variations in humidity ratios across seasons.Step 2: Negative humidity ratio values (moisture deficits) are more frequent during coldermonths, while positive values (moisture excess) are noted more often in warmer months.Step 3: Seasonal patterns are more pronounced in humidity ratio differences, showinggreater discrepancies during warmer months, supporting the statement's claim about seasonalvariations.AnswerTrue
this work, and limitation of this work.Follow the instructions carefully and output only a dictionary (dict) as specified.Instructions 1. Task Description: -Extract and categorize the content of the provided research paper into the following five sections: 1) Research direction 2) Methods of previous work 3) Limitations of previous work 4) Method of this work 5) Limitation of this work -Exclude all other sections or content not related to these five categories.Output Format:-The output must be a Python dictionary (dict) with the following structure: { "research direction": "xxx", "methods of previous work": "xxx", "limitations of previous work": "xxx", "method of this work": "xxx", "limitation of this work": "xxx" } -Replace '"xxx"' with the extracted content for each section.Extraction Rules:research direction: Identify the primary focus, goals, or areas of investigation in the paper.Avoid using terms such as "paper," "study," "work," or "thesis."Instead, directly state the research direction in a concise and general manner.methods of previous work: Summarize the methodologies or approaches used in prior research relevant to the paper.limitations of previous work: Highlight the shortcomings, gaps, or challenges in previous research.method of this work: Describe the methodology or approach proposed or used in the current paper.limitation of this work: Identify and summarize any limitations or weaknesses in the current work's approach.Important Notes:-Strictly adhere to the five sections outlined above.-Do not include any additional text, explanations, or commentary outside the dictionary.-Ensure the output is concise, clear, and directly relevant to the specified sections.-Elaborate on the content with as much detail as possible, retaining specific numerical values, dataset names, method names, author names, etc., and ensure the content is highly professional and information-rich.Example { "research direction": "xxx", "methods of previous work": "xxx", "limitations of previous work": "xxx", "method of this work": "xxx", "limitation of this work": "xxx" } The structured paper content enables the generation of two progressive dialogue rounds:• Round 1: The LLM summarizes existing methods (M 0 ) and analyzes their limitations (L 0 ).This round evaluates the model's understanding of domain-specific research methodologies.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, Huajun Chen, arXiv:2310.02031Oceangpt: A large language model for ocean science tasks. 2023arXiv preprint</p>
<p>Matplotlib and seaborn. Ekaba Bisong, Building machine learning and deep learning models on google cloud platform: A comprehensive guide for beginners. Springer2019</p>
<p>This reference does not exist: an exploration of llm citation accuracy and relevance. Courtni Byun, Piper Vasicek, Kevin Seppi, Proceedings of the Third Workshop on Bridging Human-Computer Interaction and Natural Language Processing. the Third Workshop on Bridging Human-Computer Interaction and Natural Language Processing2024</p>
<p>Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, arXiv:2403.01976Sciassess: Benchmarking llm proficiency in scientific literature analysis. 2024arXiv preprint</p>
<p>On the design and analysis of llm-based algorithms. Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou, arXiv:2407.147882024arXiv preprint</p>
<p>Grok, gemini, chatgpt and deepseek: Comparison and applications in conversational artificial intelligence. Carvalho Murillo Edson De, Li Souza, Weigang, INTELIGENCIA ARTIFICIAL. 212025</p>
<p>K2: A foundation language model for geoscience knowledge understanding and utilization. Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Yi Xu, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, Proceedings of the 17th ACM International Conference on Web Search and Data Mining. the 17th ACM International Conference on Web Search and Data Mining2024</p>
<p>A deep learning model based on bert and sentence transformer for semantic keyphrase extraction on big social data. Subramaniyaswamy Devika, C Vairavasundaram, Jay Sakthi, Vijayakumar Mahenthar, Ketan Varadarajan, Kotecha, IEEE Access. 92021</p>
<p>Supergpqa: Scaling llm evaluation across 285 graduate disciplines. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, arXiv:2502.147392025arXiv preprint</p>
<p>The impact factor. Current contents. Eugene Garfield, 199425</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Llm-based code generation method for golang compiler testing. Qiuhan Gu, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>Opendatalab: Empowering general artificial intelligence with open datasets. Conghui He, Wei Li, Zhenjiang Jin, Chao Xu, Bin Wang, Dahua Lin, arXiv:2407.137732024arXiv preprint</p>
<p>Alice Heiman, arXiv:2502.00916The accuracy, robustness, and readability of llm-generated sustainability-related word definitions. 2025arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>The era5 global reanalysis. Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz-Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, Quarterly journal of the royal meteorological society. 1467302020</p>
<p>Gpt-4o: The cutting-edge advancement in multimodal llm. Raisa Islam, Owana Marzia, Moushi , Authorea Preprints. 2024</p>
<p>Mathias Kraus, Julia Anna Bingler, Markus Leippold, Tobias Schimanski, Colesanti Chiara, Dominik Senni, Stammbach, arXiv:2304.00116Saeid Ashraf Vaghefi, and Nicolas Webersinke. Enhancing large language models with climate resources. 2023arXiv preprint</p>
<p>Diagnostic performances of claude 3 opus and claude 3.5 sonnet from patient history and key images in radiology's "diagnosis please. Ryo Kurokawa, Yuji Ohizumi, Jun Kanzawa, Mariko Kurokawa, Yuki Sonoda, Yuta Nakamura, Takao Kiguchi, Wataru Gonoi, Osamu Abe, cases. Japanese Journal of Radiology. 2024</p>
<p>Iterative large language models evolution through self-critique. Qianxi Li, 2024</p>
<p>Llm with relation classifier for document-level relation extraction. Xingzuo Li, Kehai Chen, Yunfei Long, Min Zhang, arXiv:2408.138892024arXiv preprint</p>
<p>Perception, reason, think, and plan: A survey on large multimodal reasoning models. Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, arXiv:2505.049212025arXiv preprint</p>
<p>Evaluating the efficacy of large language models in generating medical documentation: A comparative study of chatgpt-4, chatgpt-4o, and claude. Bryan Lim, Ishith Seth, Molly Maxwell, Roberto Cuomo, Richard J Ross, Warren M Rozen, Aesthetic Plastic Surgery. 2025</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>The five environmental spheres. Stanley E Manahan, Environmental Science and Technology. CRC Press2006</p>
<p>Climaqa: An automated evaluation framework for climate foundation models. Veeramakali Vignesh Manivannan, Yasaman Jafari, Srikar Eranky, Spencer Ho, Rose Yu, Duncan Watson-Parris, Yian Ma, Leon Bergen, Taylor Berg-Kirkpatrick, arXiv:2410.167012024arXiv preprint</p>
<p>Seafloorai: A large-scale visionlanguage dataset for seafloor geological survey. Kien Nguyen, Fengchun Qiao, Arthur Trembanis, Xi Peng, Advances in Neural Information Processing Systems. 202437</p>
<p>Max Peeperkorn, Tom Kouwenhoven, Dan Brown, Anna Jordanous, arXiv:2405.00492Is temperature the creativity parameter of large language models?. 2024arXiv preprint</p>
<p>Humanity's last exam. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo, Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, arXiv:2501.142492025arXiv preprint</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, First Conference on Language Modeling. 2024</p>
<p>From calculation to adjudication: Examining llm judges on mathematical reasoning tasks. Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, Benjamin Roth, arXiv:2409.041682024arXiv preprint</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024arXiv preprint</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature medicine. 2982023</p>
<p>Climatext: A dataset for climate change topic detection. Francesco S Varini, Jordan Boyd-Graber, Massimiliano Ciaramita, Markus Leippold, arXiv:2012.004832020arXiv preprint</p>
<p>Mineru: An open-source solution for precise document content extraction. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, arXiv:2409.188392024arXiv preprint</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Shichang Arjun R Loomba, Yizhou Zhang, Wei Sun, Wang, arXiv:2307.106352023arXiv preprint</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>Climatebert: A pretrained language model for climate-related text. Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, Markus Leippold, arXiv:2110.120102021arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Measuring and reducing llm hallucination without gold-standard answers. Jiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi Guo, Andrew Estornell, Yang Liu, arXiv:2402.104122024arXiv preprint</p>
<p>Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Guang Liu, Kang Liu, Jun Zhao, arXiv:2404.14741Generate-on-graph: Treat llm as both agent and kg in incomplete knowledge graph question answering. 2024arXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>Easytool: Enhancing llm-based agents with concise tool instruction. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, Deqing Yang, arXiv:2401.062012024arXiv preprint</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Wanli Ouyang, arXiv:2402.06852A chemical large language model. 2024arXiv preprint</p>
<p>Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method. Xuan Zhang, Wei Gao, arXiv:2310.003052023arXiv preprint</p>
<p>Geogpt: Understanding and processing geospatial tasks through an autonomous gpt. Yifan Zhang, Cheng Wei, Shangyou Wu, Zhengting He, Wenhao Yu, arXiv:2307.079302023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
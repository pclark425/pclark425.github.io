<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5819 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5819</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5819</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-d2afdede28fc4e4e29db24d92acab5d2ad6793b8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d2afdede28fc4e4e29db24d92acab5d2ad6793b8" target="_blank">KHANQ: A Dataset for Generating Deep Questions in Education</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> KHANQ is presented, a challenging dataset for educational question generation, containing 1,034 high-quality learner-generated questions seeking an in-depth understanding of the taught online courses in Khan Academy, and shows that state-of-the-art QG models which perform well on shallow question generation datasets have difficulty in generating useful educational questions.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5819.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5819.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context+Prompt requirement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Necessity of Providing Both Context and Prompt for Question Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that supplying both a context paragraph and an explicit prompt (the questioner's background/condition) is necessary to produce well-defined, non-trivial educational questions; omitting either changes the task and degrades output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (BertGeneration, GPT-2, BART, Google-T5, UniLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Educational question generation (KHANQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a deep, context-grounded question consistent with a provided prompt, given a context paragraph and a prompt describing background/conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Inputs consist of two structured fields concatenated for the model: Context (an independent paragraph) and Prompt (background conditions or learner knowledge). The paper reports qualitative ablation: (1) no prompt given — model tends to produce underspecified or trivial questions; (2) no context given — problem reduces to language modeling (predicting missing question text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Ablation comparisons: (A) Context + Prompt (standard), (B) Context only (no prompt), (C) Prompt only (no context).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: With both Context+Prompt the models produce targeted questions; without Prompt models generate trivial/unfocused questions; without Context the task becomes LM-style completion (not a proper QG task). (No single numeric metric reported for ablations.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>omitting prompt or context impaired/question formulation changed (reduced task appropriateness)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The prompt provides guidance on the target of questioning (what to focus on, conditions, or the learner's prior understanding); without it the model lacks direction and produces generic or trivial queries. Without context, there is no grounded information to derive a question, turning the task into language-model completion.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5819.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5819.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART prompt-type sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART sensitivity to prompt-type splits in KHANQ</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors fine-tuned BART on KHANQ and measured how test-set prompt type (condition, preposition, citation, question) affects automatic metrics; BART shows relatively small changes across prompt-type splits and better transfer across prompt types than Google-T5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Educational question generation (KHANQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict the learner's follow-up deep question given Context and Prompt; evaluate using BLEU1-4, METEOR, ROUGE-L.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training: standard Context + Prompt -> Question. Experiment: train on three prompt-types and test on the held-out fourth prompt-type, testing transfer across prompt formats. Prompt types: condition, preposition, citation, question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison is between random-split baseline (Table 4 performance) and per-prompt-type test sets (Table 6 deltas).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (random split, Table 4): BLEU1 25.10, BLEU2 11.22, BLEU3 6.20, BLEU4 3.41, METEOR 12.71, ROUGE-L 26.14. Per-prompt changes (Table 6):
- condition: BLEU1 +0.06, BLEU2 -0.26, BLEU3 -0.61, BLEU4 -1.17, METEOR -0.55, ROUGE-L +0.80
- preposition: BLEU1 -2.33, BLEU2 -1.35, BLEU3 -1.06, BLEU4 -0.39, METEOR -1.56, ROUGE-L -0.92
- citation: BLEU1 -0.56, BLEU2 -0.16, BLEU3 +0.09, BLEU4 +0.63, METEOR -0.10, ROUGE-L +1.26
- question: BLEU1 -0.60, BLEU2 +1.43, BLEU3 +1.71, BLEU4 +1.92, METEOR -0.34, ROUGE-L +2.36</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See 'performance' field: the Table 6 values are changes relative to the random-split baseline (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Largest observed negative effect for BART: preposition-type test set reduced BLEU1 by 2.33 points; largest positive ROUGE-L deltas observed for question-type (+2.36) and citation/condition small gains.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (some prompt-types reduce automatic scores, others slightly improve them); overall BART is relatively robust to prompt-type shifts compared to Google-T5.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>BART displays better transfer learning across prompt types, suggesting it generalizes more effectively to unseen prompt formats. Preposition-type prompts contain limited information and often require precise interpretation to generate procedural questions, so omitting such data from training hurts performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>BART 'does not change much' in aggregate when training/test split by prompt type (contrast to Google-T5), indicating some robustness (a partial null effect for some prompt splits).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5819.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5819.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Google-T5 prompt-type sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google-T5 sensitivity to prompt-type splits in KHANQ</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google-T5 (t5-base fine-tuned) shows larger degradation when training excludes a prompt-type: especially poor transfer to preposition-type prompts and overall larger negative deltas compared to BART.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Google-T5 (t5-base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Educational question generation (KHANQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same QG task as above: generate a deep question from Context and Prompt; evaluate with BLEU1-4, METEOR, ROUGE-L.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Train on three prompt-types and test on the held-out fourth prompt-type to evaluate format transfer. Prompt types: condition, preposition, citation, question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison to random-split baseline (Table 4) using per-prompt deltas (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (random split, Table 4): BLEU1 25.62, BLEU2 12.93, BLEU3 7.25, BLEU4 4.32, METEOR 12.66, ROUGE-L 27.62. Per-prompt changes (Table 6):
- condition: BLEU1 -1.13, BLEU2 -1.67, BLEU3 -1.29, BLEU4 -1.04, METEOR -1.53, ROUGE-L -2.73
- preposition: BLEU1 -4.33, BLEU2 -4.90, BLEU3 -4.23, BLEU4 -2.96, METEOR -2.42, ROUGE-L -2.87
- citation: BLEU1 -2.52, BLEU2 -2.45, BLEU3 -1.07, BLEU4 -0.36, METEOR -0.18, ROUGE-L -1.69
- question: BLEU1 -0.36, BLEU2 -0.66, BLEU3 -0.08, BLEU4 +0.16, METEOR -0.96, ROUGE-L -1.23</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See 'performance' field: the Table 6 values are changes relative to the random-split baseline (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Google-T5 shows substantial negative effects when preposition-type samples are excluded from training (BLEU1 -4.33, BLEU2 -4.90, BLEU3 -4.23, BLEU4 -2.96, ROUGE-L -2.87). Condition and citation also show notable negative deltas.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (missing prompt-type data decreased automatic metrics notably, especially for preposition prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Google-T5 exhibits poorer transfer across prompt types than BART; preposition-type prompts contain little explicit information but often require producing procedural/method questions, which demands accurate prompt understanding — absence of such examples in training harms Google-T5 more.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5819.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5819.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 input tokenization format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 contextual-format: Context [PRT] Prompt [QUE] Question</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When fine-tuning GPT-2 the authors formatted the input as 'Context [PRT] Prompt [QUE] Question' so the model could learn to produce the Question autoregressively given context and prompt markers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Educational question generation (KHANQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fine-tune GPT-2 to generate a question given a concatenated input sequence containing Context and Prompt with explicit separator tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input sequence for fine-tuning: Context [PRT] Prompt [QUE] Question; at test time GPT-2 is fed Context [PRT] Prompt [QUE] and generates the Question autoregressively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Automatic metrics (Table 4): BLEU1 17.01, BLEU2 5.59, BLEU3 2.29, BLEU4 1.31, METEOR 7.79, ROUGE-L 18.78.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format applied (no direct comparisons to other input tokenizations reported)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Delimiter tokens ([PRT], [QUE]) were used to delineate prompt boundaries and to let GPT-2 condition generation on both Context and Prompt; this structured formatting is necessary to convert GPT-2 into a conditional generator for the QG task.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5819.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5819.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation: coherence & coverage deficit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-rated deficits in coverage and prompt-coherence despite fluent generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluation showed that while top models (BART, Google-T5) produce fluent and complex questions, they score substantially lower than humans on coverage (derivation from context) and coherence with the prompt — indicating failures to properly use the prompt/context formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART, Google-T5 (and others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Human evaluation on appropriateness, coverage, coherence, complexity</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Three annotators rated generated questions (human and model outputs) on 4 aspects from 1 (worst) to 5 (best): Appropriateness, Coverage (derived from context), Coherence (with prompt), Complexity (deep reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Standard Context + Prompt -> Question; human raters scored outputs to quantify whether models used formatting (prompt/context) correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 5 (human mean vs models):
- Human: Appro. 4.32, Cov. 4.44, Coh. 4.13, Comp. 3.83
- BART: Appro. 4.28, Cov. 3.65, Coh. 3.37, Comp. 3.41
- Google-T5: Appro. 3.90, Cov. 3.74, Coh. 3.31, Comp. 3.75
- BertGeneration, GPT-2, UniLM lower across aspects (see Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Models approach human-level appropriateness/complexity (BART and Google-T5 close) but show substantial gaps in Coverage and Coherence relative to human reference (coverage gaps ~0.7-0.8 points; coherence gaps ~0.7-0.8 points).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Coverage gap: human minus BART = 0.79, human minus Google-T5 = 0.70; Coherence gap: human minus BART = 0.76, human minus Google-T5 = 0.82 (on 1-5 scale).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (models produce fluent/complex outputs but fail to fully incorporate context and be coherent with prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Pretrained models are strong at generating fluent and apparently complex questions, but they often fail to extract and integrate essential information from the context or to honor prompt constraints; this indicates deficiencies in understanding the structured input (Context vs Prompt) rather than language fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension <em>(Rating: 2)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 2)</em></li>
                <li>Language models are unsupervised multitask learners <em>(Rating: 2)</em></li>
                <li>Controllable open-ended question generation with a new question type ontology <em>(Rating: 2)</em></li>
                <li>Unsupervised multi-hop question answering by question generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5819",
    "paper_id": "paper-d2afdede28fc4e4e29db24d92acab5d2ad6793b8",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Context+Prompt requirement",
            "name_full": "Necessity of Providing Both Context and Prompt for Question Generation",
            "brief_description": "The paper shows that supplying both a context paragraph and an explicit prompt (the questioner's background/condition) is necessary to produce well-defined, non-trivial educational questions; omitting either changes the task and degrades output quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (BertGeneration, GPT-2, BART, Google-T5, UniLM)",
            "model_size": null,
            "task_name": "Educational question generation (KHANQ)",
            "task_description": "Generate a deep, context-grounded question consistent with a provided prompt, given a context paragraph and a prompt describing background/conditions.",
            "problem_format": "Inputs consist of two structured fields concatenated for the model: Context (an independent paragraph) and Prompt (background conditions or learner knowledge). The paper reports qualitative ablation: (1) no prompt given — model tends to produce underspecified or trivial questions; (2) no context given — problem reduces to language modeling (predicting missing question text).",
            "comparison_format": "Ablation comparisons: (A) Context + Prompt (standard), (B) Context only (no prompt), (C) Prompt only (no context).",
            "performance": "Qualitative: With both Context+Prompt the models produce targeted questions; without Prompt models generate trivial/unfocused questions; without Context the task becomes LM-style completion (not a proper QG task). (No single numeric metric reported for ablations.)",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "omitting prompt or context impaired/question formulation changed (reduced task appropriateness)",
            "explanation_or_hypothesis": "The prompt provides guidance on the target of questioning (what to focus on, conditions, or the learner's prior understanding); without it the model lacks direction and produces generic or trivial queries. Without context, there is no grounded information to derive a question, turning the task into language-model completion.",
            "counterexample_or_null_result": null,
            "uuid": "e5819.0"
        },
        {
            "name_short": "BART prompt-type sensitivity",
            "name_full": "BART sensitivity to prompt-type splits in KHANQ",
            "brief_description": "The authors fine-tuned BART on KHANQ and measured how test-set prompt type (condition, preposition, citation, question) affects automatic metrics; BART shows relatively small changes across prompt-type splits and better transfer across prompt types than Google-T5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART",
            "model_size": null,
            "task_name": "Educational question generation (KHANQ)",
            "task_description": "Predict the learner's follow-up deep question given Context and Prompt; evaluate using BLEU1-4, METEOR, ROUGE-L.",
            "problem_format": "Training: standard Context + Prompt -&gt; Question. Experiment: train on three prompt-types and test on the held-out fourth prompt-type, testing transfer across prompt formats. Prompt types: condition, preposition, citation, question.",
            "comparison_format": "Comparison is between random-split baseline (Table 4 performance) and per-prompt-type test sets (Table 6 deltas).",
            "performance": "Baseline (random split, Table 4): BLEU1 25.10, BLEU2 11.22, BLEU3 6.20, BLEU4 3.41, METEOR 12.71, ROUGE-L 26.14. Per-prompt changes (Table 6):\n- condition: BLEU1 +0.06, BLEU2 -0.26, BLEU3 -0.61, BLEU4 -1.17, METEOR -0.55, ROUGE-L +0.80\n- preposition: BLEU1 -2.33, BLEU2 -1.35, BLEU3 -1.06, BLEU4 -0.39, METEOR -1.56, ROUGE-L -0.92\n- citation: BLEU1 -0.56, BLEU2 -0.16, BLEU3 +0.09, BLEU4 +0.63, METEOR -0.10, ROUGE-L +1.26\n- question: BLEU1 -0.60, BLEU2 +1.43, BLEU3 +1.71, BLEU4 +1.92, METEOR -0.34, ROUGE-L +2.36",
            "performance_comparison": "See 'performance' field: the Table 6 values are changes relative to the random-split baseline (Table 4).",
            "format_effect_size": "Largest observed negative effect for BART: preposition-type test set reduced BLEU1 by 2.33 points; largest positive ROUGE-L deltas observed for question-type (+2.36) and citation/condition small gains.",
            "format_effect_direction": "mixed (some prompt-types reduce automatic scores, others slightly improve them); overall BART is relatively robust to prompt-type shifts compared to Google-T5.",
            "explanation_or_hypothesis": "BART displays better transfer learning across prompt types, suggesting it generalizes more effectively to unseen prompt formats. Preposition-type prompts contain limited information and often require precise interpretation to generate procedural questions, so omitting such data from training hurts performance.",
            "counterexample_or_null_result": "BART 'does not change much' in aggregate when training/test split by prompt type (contrast to Google-T5), indicating some robustness (a partial null effect for some prompt splits).",
            "uuid": "e5819.1"
        },
        {
            "name_short": "Google-T5 prompt-type sensitivity",
            "name_full": "Google-T5 sensitivity to prompt-type splits in KHANQ",
            "brief_description": "Google-T5 (t5-base fine-tuned) shows larger degradation when training excludes a prompt-type: especially poor transfer to preposition-type prompts and overall larger negative deltas compared to BART.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Google-T5 (t5-base)",
            "model_size": null,
            "task_name": "Educational question generation (KHANQ)",
            "task_description": "Same QG task as above: generate a deep question from Context and Prompt; evaluate with BLEU1-4, METEOR, ROUGE-L.",
            "problem_format": "Train on three prompt-types and test on the held-out fourth prompt-type to evaluate format transfer. Prompt types: condition, preposition, citation, question.",
            "comparison_format": "Comparison to random-split baseline (Table 4) using per-prompt deltas (Table 6).",
            "performance": "Baseline (random split, Table 4): BLEU1 25.62, BLEU2 12.93, BLEU3 7.25, BLEU4 4.32, METEOR 12.66, ROUGE-L 27.62. Per-prompt changes (Table 6):\n- condition: BLEU1 -1.13, BLEU2 -1.67, BLEU3 -1.29, BLEU4 -1.04, METEOR -1.53, ROUGE-L -2.73\n- preposition: BLEU1 -4.33, BLEU2 -4.90, BLEU3 -4.23, BLEU4 -2.96, METEOR -2.42, ROUGE-L -2.87\n- citation: BLEU1 -2.52, BLEU2 -2.45, BLEU3 -1.07, BLEU4 -0.36, METEOR -0.18, ROUGE-L -1.69\n- question: BLEU1 -0.36, BLEU2 -0.66, BLEU3 -0.08, BLEU4 +0.16, METEOR -0.96, ROUGE-L -1.23",
            "performance_comparison": "See 'performance' field: the Table 6 values are changes relative to the random-split baseline (Table 4).",
            "format_effect_size": "Google-T5 shows substantial negative effects when preposition-type samples are excluded from training (BLEU1 -4.33, BLEU2 -4.90, BLEU3 -4.23, BLEU4 -2.96, ROUGE-L -2.87). Condition and citation also show notable negative deltas.",
            "format_effect_direction": "reduced (missing prompt-type data decreased automatic metrics notably, especially for preposition prompts)",
            "explanation_or_hypothesis": "Google-T5 exhibits poorer transfer across prompt types than BART; preposition-type prompts contain little explicit information but often require producing procedural/method questions, which demands accurate prompt understanding — absence of such examples in training harms Google-T5 more.",
            "counterexample_or_null_result": null,
            "uuid": "e5819.2"
        },
        {
            "name_short": "GPT-2 input tokenization format",
            "name_full": "GPT-2 contextual-format: Context [PRT] Prompt [QUE] Question",
            "brief_description": "When fine-tuning GPT-2 the authors formatted the input as 'Context [PRT] Prompt [QUE] Question' so the model could learn to produce the Question autoregressively given context and prompt markers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B",
            "task_name": "Educational question generation (KHANQ)",
            "task_description": "Fine-tune GPT-2 to generate a question given a concatenated input sequence containing Context and Prompt with explicit separator tokens.",
            "problem_format": "Input sequence for fine-tuning: Context [PRT] Prompt [QUE] Question; at test time GPT-2 is fed Context [PRT] Prompt [QUE] and generates the Question autoregressively.",
            "comparison_format": null,
            "performance": "Automatic metrics (Table 4): BLEU1 17.01, BLEU2 5.59, BLEU3 2.29, BLEU4 1.31, METEOR 7.79, ROUGE-L 18.78.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "format applied (no direct comparisons to other input tokenizations reported)",
            "explanation_or_hypothesis": "Delimiter tokens ([PRT], [QUE]) were used to delineate prompt boundaries and to let GPT-2 condition generation on both Context and Prompt; this structured formatting is necessary to convert GPT-2 into a conditional generator for the QG task.",
            "counterexample_or_null_result": null,
            "uuid": "e5819.3"
        },
        {
            "name_short": "Human evaluation: coherence & coverage deficit",
            "name_full": "Human-rated deficits in coverage and prompt-coherence despite fluent generation",
            "brief_description": "Human evaluation showed that while top models (BART, Google-T5) produce fluent and complex questions, they score substantially lower than humans on coverage (derivation from context) and coherence with the prompt — indicating failures to properly use the prompt/context formatting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART, Google-T5 (and others)",
            "model_size": null,
            "task_name": "Human evaluation on appropriateness, coverage, coherence, complexity",
            "task_description": "Three annotators rated generated questions (human and model outputs) on 4 aspects from 1 (worst) to 5 (best): Appropriateness, Coverage (derived from context), Coherence (with prompt), Complexity (deep reasoning).",
            "problem_format": "Standard Context + Prompt -&gt; Question; human raters scored outputs to quantify whether models used formatting (prompt/context) correctly.",
            "comparison_format": null,
            "performance": "Table 5 (human mean vs models):\n- Human: Appro. 4.32, Cov. 4.44, Coh. 4.13, Comp. 3.83\n- BART: Appro. 4.28, Cov. 3.65, Coh. 3.37, Comp. 3.41\n- Google-T5: Appro. 3.90, Cov. 3.74, Coh. 3.31, Comp. 3.75\n- BertGeneration, GPT-2, UniLM lower across aspects (see Table 5).",
            "performance_comparison": "Models approach human-level appropriateness/complexity (BART and Google-T5 close) but show substantial gaps in Coverage and Coherence relative to human reference (coverage gaps ~0.7-0.8 points; coherence gaps ~0.7-0.8 points).",
            "format_effect_size": "Coverage gap: human minus BART = 0.79, human minus Google-T5 = 0.70; Coherence gap: human minus BART = 0.76, human minus Google-T5 = 0.82 (on 1-5 scale).",
            "format_effect_direction": "reduced (models produce fluent/complex outputs but fail to fully incorporate context and be coherent with prompt)",
            "explanation_or_hypothesis": "Pretrained models are strong at generating fluent and apparently complex questions, but they often fail to extract and integrate essential information from the context or to honor prompt constraints; this indicates deficiencies in understanding the structured input (Context vs Prompt) rather than language fluency.",
            "counterexample_or_null_result": null,
            "uuid": "e5819.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
            "rating": 2
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 2
        },
        {
            "paper_title": "Language models are unsupervised multitask learners",
            "rating": 2
        },
        {
            "paper_title": "Controllable open-ended question generation with a new question type ontology",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised multi-hop question answering by question generation",
            "rating": 1
        }
    ],
    "cost": 0.01466675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>KHANQ: A Dataset for Generating Deep Questions in Education</h1>
<p>Huanli Gong ${ }^{1}$ Liangming Pan ${ }^{2}$ Hengchang $\mathbf{H u}^{2}$<br>${ }^{1}$ The Ohio State University, Columbus, OH, USA<br>${ }^{2}$ School of Computing, National University of Singapore, Singapore<br>gong.545@osu.edu<br>{liangmingpan, hengchanghu}@u.nus.edu</p>
<h4>Abstract</h4>
<p>Designing in-depth educational questions is a time-consuming and cognitively demanding task. Therefore, it is intriguing to study how to build Question Generation (QG) models to automate the question creation process. However, existing QG datasets are not suitable for educational question generation because the questions are not real questions asked by humans during learning and can be solved by simply searching for information. To bridge this gap, we present KHANQ, a challenging dataset for educational question generation, containing 1,034 high-quality learnergenerated questions seeking an in-depth understanding of the taught online courses in Khan Academy. Each data sample is carefully paraphrased and annotated as a triple of 1) Context: an independent paragraph on which the question is based; 2) Prompt: a text prompt for the question (e.g., the learner's background knowledge); 3) Question: a deep question based on Context and coherent with Prompt. By conducting a human evaluation on the aspects of appropriateness, coverage, coherence, and complexity, we show that state-of-the-art QG models which perform well on shallow question generation datasets have difficulty in generating useful educational questions. This makes KHANQ a challenging testbed for educational question generation for further investigation.</p>
<h2>1 Introduction</h2>
<p>Question asking has long been considered a fundamental cognitive process in learning. An ideal learner should be an active, self-motivated, creative, inquisitive person who asks deep questions and searches for answers to thought-provoking questions, usually in the form of Why, Why-not, How, What-if, etc (Otero and Graesser, 2001). For example, Figure 1 shows a question raised by a learner after learning "allosteric regulation and feedback
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A real human-raised question after learning the "allosteric regulation and feedback loops" course in Khan Academy.
loops" in Khan Academy ${ }^{1}$, a well-known online education platform. Given that question-asking is a hallmark of human intelligence, it is intriguing to study whether we can endow machines with the ability to ask deep and to-the-point questions.</p>
<p>While there have been considerable advances made in the field of Question Generation (QG), the current research is still far from achieving humanlike question-asking. One major obstacle is the lack of a suitable and clean dataset that can represent</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>real-life human-raised questions, such as the question shown in Figure 1. Existing QG works (Duan et al., 2017; Zhao et al., 2018; Pan et al., 2020; Back et al., 2021) typically focus on generating factoid questions relevant to one fact obtainable from a single sentence. They are shallow because they do not reflect the creative human cognitive process in question-asking such as inferences, multi-step reasoning, synthesis, critical evaluation, and generalization.</p>
<p>To bridge this gap, we desire a new dataset with questions that satisfy two conditions: 1) they are real questions asked by humans during learning, and 2) they are deep questions that require highlevel cognitive skills to raise. We choose to collect data from Khan Academy, where the questions are raised by real learners after watching course videos or reading course materials, as shown in Figure 1. These questions represent what learners are naturally interested in, and they are rarely shallow questions confined to a narrow scope of context.</p>
<p>We collect the question-answer pairs from Khan Academy and rewrite them as a triple of 1) Context: an independent paragraph on which the question is based; 2) Prompt: a text prompt for the question (e.g., the learner's background knowledge); 3) Question: a deep question based on Context and coherent with Prompt. Following the common setting of question generation, questions are based on the information in the context. However, many valid questions can be asked from the given context. To facilitate the evaluation and to guide the generation, we also provide Prompt which describes the questioner's background knowledge (e.g., "In my understanding, ...") or certain conditions for the question (e.g., "When ... happens, ..."). Given both Context and Prompt as inputs, we test the model's ability to generate a consistent question with both the context and the prompt. We design a rigorous data annotation procedure to ensure that each sample in KHANQ satisfies the following conditions: 1) Question involves deep reasoning instead of simply searching for information; 2) Context contains enough information to derive Question; 3) Question is coherent with Prompt. After careful annotation, we collect 1,034 (Context, Prompt, Question) triples to form the KHANQ. An example is given in Figure 2.</p>
<p>We further evaluate the depth of the questions in KHANQ. We find that KHANQ is dominated by deep questions that are represented in the form of</p>
<p>Context: The molecules in the water have a range of kinetic energies, from low to high. Some of the molecules have sufficient kinetic energy to break out and to enter the air. In coastal areas there are also factors. For example, waves churn up the water and give rise to very fine droplets that can get carried in the wind.
Prompt: Water have strong cohesion
Question: How does water evaporate?
Figure 2: A data sample in our KHANQ dataset.
Why, and How. In contrast, SQuAD 2.0 (Rajpurkar et al., 2018) and HotpotQA (Yang et al., 2018) are dominated by shallow questions represented in the form of what. We further classify these questions based on their reasoning type following the criteria in (Cao and Wang, 2021). The results show that $51.58 \%$ of questions in KHANQ involve deep reasoning.</p>
<p>We evaluate the performance of five text generation models, which have achieved state-of-the-art question generation performance on the SQuAD dataset: BertGeneration (Rothe et al., 2020), GPT2 (Radford et al., 2019), BART (Lewis et al., 2020), Google-T5 (Raffel et al., 2020), UniLM (Dong et al., 2019). By conducting both automatic and human evaluation, we find that although the abilities of BART and Google-T5 to reason in settings and ask deep questions are better than other models, the question quality is still much lower than the human level. The results show that the KHANQ is a challenging testbed for generating human-like deep questions in education.</p>
<h2>2 Related Work</h2>
<p>Question Generation (QG) aims to automatically generate questions from textual input. Earlier work relied on syntactic transformations to convert declarative sentences into questions (Chali and Hasan, 2015; Heilman and Smith, 2010). Recent neural models (Zhou et al., 2019; Krishna and Iyyer, 2019; Sun et al., 2018) rely on sequence-to-sequence models to generate questions from a given sentence or paragraph by considering the focus, type, and general specific relations of the question. However, these approaches only involves generating shallow factoid questions, which are typically trained and evaluated on the SQuAD (Rajpurkar et al., 2016) dataset. SQuAD is insufficient to evaluate deep QG because more than $80 \%$ of its questions are only relevant to information confined to a single sentence (Du et al., 2017).</p>
<p>With the advent of pretraining language models, the challenge of generating single-hop questions similar to SQuAD have largely been addressed. QG research has started to generate more complex questions that require multi-hop reasoning (Tuan et al., 2020; Pan et al., 2020; Xie et al., 2020; Yu et al., 2020), benchmarking on the HotpotQA (Yang et al., 2018) dataset. However, for questions in HotpotQA, the reasoning is often evident from the surface form of the question, simplifying the QG task. For example, Pan et al. (2021) show that HotpotQA-style multi-hop questions can be generated by composing single-hop questions through templates. Different from SQuAD and HotpotQA, questions in our KHANQ dataset are asked by real course learners, thus having a wider variety in both question forms and reasoning types.</p>
<p>Recently, a few works started to work on generating real human-raised questions. For example, Cao and Wang (2021) collect real questions from online forums (Reddit and Yahoo). Gupta et al. (2019) collect questions posted by customers on Amazon product pages. However, questions collected in above works are noisy, with few in-depth questions, since they fail to carefully filter and validate the questions. Compared to them, questions in KHANQ are carefully filtered and annotated, providing a more clean testbed for deep question generation. Among these works, LearningQ (Chen et al., 2018) is closest to us, which also collects questions from Khan Academy. However, our work has three key differences. First, LearningQ is more focused on the educational nature of the questions. They filter questions based on whether they are useful for learning, whereas we focus more on the depth of the question, keeping only the questions that involve deep reasoning. Second, we use prompts to give the models additional guidance on what information to focus on when generating. Third, the contexts used by LearningQ are entire articles or videos, causing most of the sentences in the contexts being irrelevant to the target question. In contrast, the contexts we use are answers that contain comprehensive explanations of the knowledge points relevant to the question.</p>
<h2>3 Data Collection and Annotation</h2>
<h3>3.1 Text Sources</h3>
<p>Khan Academy is an online education institution that provides free teaching materials. Online course learners are encouraged to ask questions in the cor-
responding forum to clarify their confusions after they learned each section of the course, as shown in the example in Figure 1. Therefore, these questions usually reflect a high-level comprehension and cognition of the course contents, which makes them a suitable data source for building a dataset for deep question generation. We chose to collect data from the courses in the science domain because questionasking is more active in the science-related courses than others. Learners asked more questions and most of them have been answered by tutors.</p>
<p>As shown in Table 1, we collected a total number of 1,284 course sections from 11 different areas under the science domain. Each course section consists of the course material (an article) written by the tutor and a discussion forum. We filtered out those course sections that have no question in their discussion forums. In total, we collected 100,908 question-answer pairs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">area</th>
<th style="text-align: right;">number</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">High school biology</td>
<td style="text-align: right;">209</td>
</tr>
<tr>
<td style="text-align: left;">AP/College Biology</td>
<td style="text-align: right;">296</td>
</tr>
<tr>
<td style="text-align: left;">High school chemistry beta</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">AP/College Chemistry</td>
<td style="text-align: right;">307</td>
</tr>
<tr>
<td style="text-align: left;">Organic chemistry</td>
<td style="text-align: right;">290</td>
</tr>
<tr>
<td style="text-align: left;">High school physics</td>
<td style="text-align: right;">82</td>
</tr>
<tr>
<td style="text-align: left;">AP/College Physics 1</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">AP/College Physics 2</td>
<td style="text-align: right;">19</td>
</tr>
<tr>
<td style="text-align: left;">AP/College Environmental science</td>
<td style="text-align: right;">18</td>
</tr>
<tr>
<td style="text-align: left;">Cosmology and astronomy</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;">Electrical engineering</td>
<td style="text-align: right;">53</td>
</tr>
</tbody>
</table>
<p>Table 1: The number of collected course sections for each area in the science domain.</p>
<h3>3.2 Data Annotation</h3>
<p>To build a clean and high-quality dataset for deep question generation, we then designed a rigorous data annotation procedure to filter out noisy data in our collected question-answer pairs. After filtration and annotation, each data sample is annotated as a triple of: 1) Context: an independent paragraph which the question is based on; 2) Prompt: a text prompt for the question (e.g., the learner's background knowledge); 3) Question: a deep question based on Context and Prompt. Compared with the original noisy question-answer pairs collected from forums, data annotation provides a standardized and clean benchmark to train and evaluate the question generation models.</p>
<p>Figure 3 summarizes the major steps of our data annotation. First, the Context of the question should cover the knowledge points that the question are based on. To provide the context, we ask annotators to revise the answer provided by the tutors of the course. We find that most answers contain comprehensive explanations of the knowledge points relevant to the question from the course. Therefore, they are suitable to serve as the context for the question. We asked the annotators to remove the answer-tone phrases (e.g., "yes", "this is because") and conversational language (e.g., "good question", "hope it helps") to make the answer context-independent and self-contained.</p>
<p>For the original questions raised by learners, they often contain conditional clauses, prepositional phrases, and other conditions to limit the scope of the question or to provide some background information that expresses the learner's own understanding or opinions. As shown in Figure 3, we ask human annotators to separate out this information from the original question to serve as Prompt of the question. The prompt provides additional guidance for the question generation model in knowing which information to focus on when generating; otherwise, the QG model tend to generate questions without specific target (e.g., "Can you explain this part again?"). The Question then comes from the remaining part of the original question that seeks for new information based on the prompt. Appendix A gives detailed data filtration criteria, specific data annotation guidelines for each step, and examples of annotation.</p>
<h3>3.3 Quality Control</h3>
<p>To control the quality of the annotation, we require that annotators have a US high school diploma or equivalent to demonstrate sufficient background knowledge to understand the questions. To ensure that annotators understand the annotation procedure, we check those works and give feedback when an annotator has completed the first $10 \%$ of the tasks. The annotator needs to redo those annotations based on the feedback. This process is repeated until all $10 \%$ of the tasks are approved. During the annotation process, we also spot-check the work submitted by the annotators. If the pass rate does not reach $85 \%$, the annotator needs to be retrained to prevent them from deviating from the task definition. Based on the above criteria, we hired a total of six annotators. The whole anno-</p>
<p>Question: a car travels the first half distance between two places at 40 kmph and the other half at 60 kmph - what will be the average speed of the car?
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Data annotation pipeline, consisting of four steps: 1) Filter Questions; 2) Rewrite Question into Prompt + Question, 3) Filter Answers, and 4) Rewrite Answers into Contexts.
tation process took two months. 30,000 original question-answer pairs are examined and 1,217 of them are annotated as valid samples.</p>
<p>After the annotation, we hired two workers to validate the quality of annotation. We find that: in 140 data samples, the context cannot provide the information needed to drive the question. In 19 data samples, the prompt cannot constrain the direction of the question. In 25 data samples, The question is incomplete, incomprehensible, not even a question at all. $85 \%$ of the annotated data samples meet all the requirements, which gives us 1,034 samples to form our final KHANQ dataset.</p>
<h2>4 Data Analysis</h2>
<h3>4.1 Dataset Statistics</h3>
<p>The final dataset consists of 1,034 high-quality data samples, in which 515 samples come from the field of Biology, 401 from the field of chemistry, 88 from the field of physics, 19 from the field of electricalengineering, 7 from the field of environmentalscience, and 4 from the field of cosmology-andastronomy. The average length of Context, Prompt, and Question are 84.62 words, 14.12 words, and 10.74 words, respectively.</p>
<h3>4.2 Question types by words</h3>
<p>To investigate the depth and diversity of questions in KHANQ, we classify questions based on the first two words in the question and compare them to other commonly-used question generation datasets: SQuAD 2.0 (Rajpurkar et al., 2018) and HotpotQA (Yang et al., 2018), as shown in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">KHANQ</th>
<th style="text-align: left;">$\%$</th>
<th style="text-align: left;">SQuAD</th>
<th style="text-align: center;">$\%$</th>
<th style="text-align: left;">HotpotQA</th>
<th style="text-align: center;">$\%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Why does</td>
<td style="text-align: left;">3.8</td>
<td style="text-align: left;">what is</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: left;">What is</td>
<td style="text-align: center;">5.0</td>
</tr>
<tr>
<td style="text-align: left;">How does</td>
<td style="text-align: left;">3.6</td>
<td style="text-align: left;">what was</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: left;">Who was</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: left;">Why is</td>
<td style="text-align: left;">3.4</td>
<td style="text-align: left;">how many</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: left;">What was</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: left;">Does the</td>
<td style="text-align: left;">3.0</td>
<td style="text-align: left;">when did</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: left;">In what</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: left;">Why do</td>
<td style="text-align: left;">2.8</td>
<td style="text-align: left;">in what</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: left;">When was</td>
<td style="text-align: center;">1.7</td>
</tr>
<tr>
<td style="text-align: left;">What is</td>
<td style="text-align: left;">2.8</td>
<td style="text-align: left;">what did</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: left;">Who is</td>
<td style="text-align: center;">1.6</td>
</tr>
<tr>
<td style="text-align: left;">How do</td>
<td style="text-align: left;">2.3</td>
<td style="text-align: left;">when was</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: left;">How many</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">How to</td>
<td style="text-align: left;">2.2</td>
<td style="text-align: left;">who was</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: left;">In which</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">How is</td>
<td style="text-align: left;">2.2</td>
<td style="text-align: left;">what does</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: left;">What year</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">Is it</td>
<td style="text-align: left;">2.2</td>
<td style="text-align: left;">what are</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: left;">Are both</td>
<td style="text-align: center;">0.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Most frequent leading bigrams in SQuAD 2.0, HotpotQA and KHANQ</p>
<p>According to (Craig et al., 2000), questions that are represented in the form of Why, and How are likely to be deep questions. Table 2 shows that KHANQ is dominated by these questions. In contrast, SQuAD 2.0, and HotpotQA are dominated by shallow questions represented in the form of what.</p>
<h3>4.3 Question types by reasoning</h3>
<p>To gain a better insight of the characteristics of the questions, we manually analyzed a sample of 95 different questions from KHANQ. We classify these questions following the criteria in (Cao and Wang, 2021). We summarize the most common types of questions in KHANQ and their corresponding examples as follows.</p>
<p>PROCEDURAL questions In $21.05 \%$ of the questions we inspected, the questioners asked for the procedures, tools, or methods by which a particular outcome is achieved. Most of these questions begin with How, followed by an auxiliary verb, a modal verb, or to, which indicates that the questioner is reasoning about the steps of action.</p>
<ul>
<li>How to determine the oxidation state of oxygen?</li>
<li>How does the body know what to send down the esophagus and what to send down the trachea?</li>
</ul>
<p>CAUSE questions In $18.95 \%$ of the questions we inspected, the questioners are asking for the cause or reason for an event or a concept. Most of these questions begin with Why, followed by an auxiliary verb, a modal verb, or their negative form,
which indicates that the questioner is seeking the reason behind a phenomenon.</p>
<ul>
<li>Why are the electrons mapped out in such an orderly way?</li>
<li>Why don't the oxygen atoms in the air bond to the water molecules on the surface and pull on water molecules?</li>
</ul>
<p>VERIFICATION questions In $15.79 \%$ of the questions we inspected, the questioners asked for the truthfulness of an event or a concept. Most of these questions are general questions that begin with be verbs, auxiliary verbs, or modal verbs. This indicates that the questioner wants to verify the truth of an idea when he or she already has a specific idea.</p>
<ul>
<li>Does the oxygen bonded with another oxygen don't count as another oxidation state?</li>
<li>Are the cranial nerves part of the CNS and the spinal nerves part of the PNS?</li>
</ul>
<p>CONSEQUENCE questions In $11.58 \%$ of the questions we inspected, the questioners asked for the consequences or results of an event. Most of the questions include What happen, How affect and so on. This indicates that the questioner is trying to draw a conclusion about the effects or consequences of an action.</p>
<ul>
<li>What happens to the water at the bottom of the container?</li>
<li>How will the viscosity of liquid be affected by increase in temperature?</li>
</ul>
<p>According to (Craig et al., 2000), six question categories involve deep reasoning: causal antecedent, causal consequence, goal-orientation, enablement, instrumental/procedural, and expectational. Connecting it to KHANQ, PROCEDURAL questions, CAUSE questions, and CONSEQUENCE questions are three categories that involve deep reasoning, accounting for $51.58 \%$.</p>
<h3>4.4 Prompt types</h3>
<p>We further analyzed the prompts of these questions and divided the prompts into four major types:</p>
<ul>
<li>Condition (36.94\%): Prompt is a conditional clause (trigger word: "if", "when", etc.);</li>
<li>Preposition (8.32\%): Prompt is a prepositional phrase (trigger word: "in", "for", etc.);</li>
</ul>
<h1>Context:</h1>
<p>Passive transport basically does not require any form of energy compared to active transport. In the case of osmosis,the water moves from areas of HIGH WATER concentration to areas of LOW WATER concentration, which makes it a form of passive transport. It uses no energy to move, it just drifts into lower concentrations of WATER. WATER not other materials, only WATER. Osmosis deals with water.</p>
<h2>Prompt:</h2>
<p>Condition If water moves from areas where solutes are less concentrated to areas where they are more concentrated</p>
<p>Preposition In the case of osmosis</p>
<p>Citation Passive transport is when molecules move from areas of high concentration to low concentration
Question Are the modes of transport that move molecules from high to low concentrations all passive transport?</p>
<h2>Question:</h2>
<p>Why would osmosis be a form of Passive Transport?
How molecules move from areas of high concentration to low concentration?
Shouldn't osmosis technically be classified as a form of active transport?
Is osmosis also passive transport for water?</p>
<p>Table 3: Different questions raised by learners for the same context with different types of prompts.</p>
<ul>
<li>Citation (33.95\%): Prompt is a complete sentence expressing some known information or the learner's own understanding;</li>
<li>Question (20.79\%) - Prompt is an initial question which leads to the followup question.</li>
</ul>
<p>Given the same context, different prompts trigger different questions. In Table 3, we show a typical example in KHANQ in which four different types of prompts lead to different questions. We observe a strong correlation between the prompt type and the question type, which reveals how the prompt affects the question: 1) Most of the questions asked under the condition-type prompt aim to seek for new information based on the condition set by the questioner. The questions generally ask for causes or consequences. There is a strong logical connection between the question and the prompt, but the question rarely repeats the words in the prompt; 2) Most of the questions asked under the preposition-type prompt are about a specific object or phase. They generally ask for procedures or methods. These questions are general and often do not stand alone without the prompt; 3) Most of the questions asked under the citation-type prompt are to verify what the questioner already knows or to doubt the newly learned content based on what the learner is already known. They are mainly VERIFICATION questions. The questions tend to repeat the keywords in the prompt; 4) Most of the questions asked under the question-type prompt are specifications of the previous question asked by the questioner. The types of the question in Prompt and Question are usually the same.</p>
<h2>5 Experiments</h2>
<p>We evaluate the performance of state-of-the-art question generation models on KHANQ, focusing on the following:</p>
<ul>
<li>Exploring whether existing models can generate reasonable educational questions by conducting both automatic evaluation (Section 5.3) and human evaluation (Section 5.4).</li>
<li>Analyzing whether the model needs to actually understand a certain type of prompts when generating questions. (Section 5.5).</li>
</ul>
<h3>5.1 Experimental Settings</h3>
<p>We formulate the question generation task as predicting the Question given both Context and Prompt as inputs. Through preliminary experiments, we find both context and prompt are necessary to form a well-defined QG setting because 1) if the prompt is not given, many possible questions can be asked for the context paragraph. The model does not have any guidance on what to ask, leading to simple trivial questions in most cases; 2) if the context is not given, the problem becomes a simple language modeling task which aims to generate the missing part of an incomplete question. In our experiment, $90 \%$ of the data in KHANQ are used for training while the remaining are used for testing.</p>
<h3>5.2 Models</h3>
<p>We choose five generation models based on pretraining language models (PLMs) that perform well on QG tasks for evaluation. We also tried QGspecific models without pretraining such as InfoHCVAE (Lee et al., 2020), but we found that they fail to generate meaningful questions in KHANQ.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BLEU1</th>
<th style="text-align: center;">BLEU2</th>
<th style="text-align: center;">BLEU3</th>
<th style="text-align: center;">BLEU4</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">ROUGE-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Human Baseline</td>
<td style="text-align: center;">31.29</td>
<td style="text-align: center;">16.74</td>
<td style="text-align: center;">10.72</td>
<td style="text-align: center;">7.47</td>
<td style="text-align: center;">15.60</td>
<td style="text-align: center;">30.83</td>
</tr>
<tr>
<td style="text-align: center;">BertGeneration (Rothe et al., 2020)</td>
<td style="text-align: center;">17.83</td>
<td style="text-align: center;">5.75</td>
<td style="text-align: center;">2.52</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">8.07</td>
<td style="text-align: center;">17.80</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 (Radford et al., 2019)</td>
<td style="text-align: center;">17.01</td>
<td style="text-align: center;">5.59</td>
<td style="text-align: center;">2.29</td>
<td style="text-align: center;">1.31</td>
<td style="text-align: center;">7.79</td>
<td style="text-align: center;">18.78</td>
</tr>
<tr>
<td style="text-align: center;">BART (Lewis et al., 2020)</td>
<td style="text-align: center;">25.10</td>
<td style="text-align: center;">11.22</td>
<td style="text-align: center;">6.20</td>
<td style="text-align: center;">3.41</td>
<td style="text-align: center;">12.71</td>
<td style="text-align: center;">26.14</td>
</tr>
<tr>
<td style="text-align: center;">Google-T5 (Raffel et al., 2020)</td>
<td style="text-align: center;">25.62</td>
<td style="text-align: center;">12.93</td>
<td style="text-align: center;">7.25</td>
<td style="text-align: center;">4.32</td>
<td style="text-align: center;">12.66</td>
<td style="text-align: center;">27.62</td>
</tr>
<tr>
<td style="text-align: center;">UniLM (Dong et al., 2019)</td>
<td style="text-align: center;">20.15</td>
<td style="text-align: center;">8.83</td>
<td style="text-align: center;">4.65</td>
<td style="text-align: center;">2.76</td>
<td style="text-align: center;">10.76</td>
<td style="text-align: center;">22.02</td>
</tr>
</tbody>
</table>
<p>Table 4: Automatic evaluation results for different models in KHANQ with BLEU1-4, METEOR and ROUGE-L</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Appro.</th>
<th style="text-align: left;">Cov.</th>
<th style="text-align: left;">Coh.</th>
<th style="text-align: left;">Comp.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: left;">$\mathbf{4 . 3 2}$</td>
<td style="text-align: left;">$\mathbf{4 . 4 4}$</td>
<td style="text-align: left;">$\mathbf{4 . 1 3}$</td>
<td style="text-align: left;">$\mathbf{3 . 8 3}$</td>
</tr>
<tr>
<td style="text-align: left;">BertGeneration</td>
<td style="text-align: left;">2.63</td>
<td style="text-align: left;">2.12</td>
<td style="text-align: left;">2.35</td>
<td style="text-align: left;">3.47</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">2.87</td>
<td style="text-align: left;">2.69</td>
<td style="text-align: left;">3.07</td>
<td style="text-align: left;">2.40</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: left;">4.28</td>
<td style="text-align: left;">3.65</td>
<td style="text-align: left;">3.37</td>
<td style="text-align: left;">3.41</td>
</tr>
<tr>
<td style="text-align: left;">Google-T5</td>
<td style="text-align: left;">3.90</td>
<td style="text-align: left;">3.74</td>
<td style="text-align: left;">3.31</td>
<td style="text-align: left;">3.75</td>
</tr>
<tr>
<td style="text-align: left;">UniLM</td>
<td style="text-align: left;">3.35</td>
<td style="text-align: left;">3.26</td>
<td style="text-align: left;">2.96</td>
<td style="text-align: left;">3.37</td>
</tr>
</tbody>
</table>
<p>Table 5: Human evaluation results for different models in KHANQ on appropriateness (Appro.), coverage (Cov.), coherence (Coh.) and complexity (Comp.)</p>
<p>BertGeneration (Rothe et al., 2020) This is a Transformer-based sequence-to-sequence model in which the parameters of both encoder and decoder are initialized with BERT (Devlin et al., 2019). We use the implementation from Huggingface ${ }^{2}$.</p>
<p>GPT-2 (Radford et al., 2019) GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. We fine-tune GPT-2 on our training data, by formatting the input sequence as: Context [PRT] Prompt [QUE] Question. During test time, Context [PRT] Prompt [QUE] is given to predict the question.</p>
<p>BART (Lewis et al., 2020) BART is consists of a bidirectional encoder and a left-to-right decoder. The pretraining task involves randomly shuffling the order of the original sentences and a novel infilling scheme, where spans of text are replaced with a single mask token. We fine-tune BART on KHANQ by predicting the question given the context and the prompt.</p>
<p>Google-T5 (Raffel et al., 2020) Google-T5 is another state-of-the-art language generation model which pretrains the Transformer with the fill-in-the-blank-style denoising objectives. The model is</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>trained to recover missing words in the input. We use the t5-base model provided by Huggingface and fine-tune it on the training data of KHANQ.</p>
<p>UniLM (Dong et al., 2019) UniLM uses three types of language modeling tasks for pretraining: one-way, two-way, and sequence-to-sequence prediction. We initialize UniLM with the parameters of BERT-base (Turc et al., 2019) and then fine-tune it on KHANQ and predict the question.</p>
<h3>5.3 Automatic Evaluation</h3>
<p>We evaluate the generated questions using BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and ROUGE-L (Lin, 2004).</p>
<p>To compare with human performance, we recruited two high school graduates who were not involved in the annotation process and asked them to perform the same task as models. We took a sample of 80 data and asked them to generate questions based on the context and the prompt. To reduce subjective differences, they were required to develop the results after discussion.</p>
<p>Table 4 shows that BART and Google-T5 perform the best on KHANQ, with similar performance. BART achieves the best METEOR score, while Google-T5 achieves the best BLEU1-4 and ROUGE-L. Both these two models perform significantly better than other models. This observation is consistent with other language generation tasks in which BART and Google-T5 also show strong performance in generation. However, although BART and Google-T5 have achieved super-human performance in generating shallow questions in SQuAD, in our KHANQ dataset, all models perform worse than the human baseline in all metrics, indicating that KHANQ is a more challenging benchmark for SOTA models. In Appendix B, we show examples of generated questions by different models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">prompt type</th>
<th style="text-align: center;">BLEU1</th>
<th style="text-align: center;">BLEU2</th>
<th style="text-align: center;">BLEU3</th>
<th style="text-align: center;">BLEU4</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">ROUGE-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">condition</td>
<td style="text-align: center;">+0.06</td>
<td style="text-align: center;">-0.26</td>
<td style="text-align: center;">-0.61</td>
<td style="text-align: center;">-1.17</td>
<td style="text-align: center;">-0.55</td>
<td style="text-align: center;">+0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">preposition</td>
<td style="text-align: center;">-2.33</td>
<td style="text-align: center;">-1.35</td>
<td style="text-align: center;">-1.06</td>
<td style="text-align: center;">-0.39</td>
<td style="text-align: center;">-1.56</td>
<td style="text-align: center;">-0.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">citation</td>
<td style="text-align: center;">-0.56</td>
<td style="text-align: center;">-0.16</td>
<td style="text-align: center;">+0.09</td>
<td style="text-align: center;">+0.63</td>
<td style="text-align: center;">-0.10</td>
<td style="text-align: center;">+1.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">question</td>
<td style="text-align: center;">-0.60</td>
<td style="text-align: center;">+1.43</td>
<td style="text-align: center;">+1.71</td>
<td style="text-align: center;">+1.92</td>
<td style="text-align: center;">-0.34</td>
<td style="text-align: center;">+2.36</td>
</tr>
<tr>
<td style="text-align: center;">Google-T5</td>
<td style="text-align: center;">condition</td>
<td style="text-align: center;">-1.13</td>
<td style="text-align: center;">-1.67</td>
<td style="text-align: center;">-1.29</td>
<td style="text-align: center;">-1.04</td>
<td style="text-align: center;">-1.53</td>
<td style="text-align: center;">-2.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">preposition</td>
<td style="text-align: center;">-4.33</td>
<td style="text-align: center;">-4.90</td>
<td style="text-align: center;">-4.23</td>
<td style="text-align: center;">-2.96</td>
<td style="text-align: center;">-2.42</td>
<td style="text-align: center;">-2.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">citation</td>
<td style="text-align: center;">-2.52</td>
<td style="text-align: center;">-2.45</td>
<td style="text-align: center;">-1.07</td>
<td style="text-align: center;">-0.36</td>
<td style="text-align: center;">-0.18</td>
<td style="text-align: center;">-1.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">question</td>
<td style="text-align: center;">-0.36</td>
<td style="text-align: center;">-0.66</td>
<td style="text-align: center;">-0.08</td>
<td style="text-align: center;">+0.16</td>
<td style="text-align: center;">-0.96</td>
<td style="text-align: center;">-1.23</td>
</tr>
</tbody>
</table>
<p>Table 6: Changes in the automatic evaluation scores of BART and Google-T5 when data samples from different type of prompts are used as the test set.</p>
<h3>5.4 Human Evaluation</h3>
<p>We conduct human evaluation on same sets of test samples used in Section 5.3. Each set consists of the human written question and five questions generated by five different models. We hired three annotators who participated in our data annotation to rate the total 480 questions with the best being a 5 and the worst being a 1 on four criteria: (1) $A p$ propriateness: whether the question is semantically correct, regardless of the context and the prompt; (2) Coverage: whether the question is derived from the context and covers most of the information in the context; (3) Coherent: whether the question is coherent with the prompt; (4) Complexity: whether the question involves deep reasoning. We further asked them to give what the best and worst scores should be for each aspect to adjust the absolute difference between the best and worst outputs.</p>
<p>As shown in Table 5, BART and Google-T5 also perform better than other models in human evaluation, which is consistent with the automatic evaluation. Human reference still achieve the highest scores on all four aspects, indicating that QG models still fail to reach the human level. BART generates questions with the best appropriateness, and Google-T5 generates questions with the best complexity. In both aspects, they are very close to the human baseline. This suggests that BART and Google-T5 have the ability to ask fluent and complex questions similar to humans. However, the scores of coverage and coherence have a large gap with the human reference for all models, indicating that although the generated questions look fluent by themselves, they often do not cover the essential knowledge covered in the context or they fail to be coherent with the given prompt. This shows that although pretraining models are powerful in generating fluent-looking questions, generating questions
that require a deep understanding of the context and the prompt is still challenging.</p>
<h3>5.5 Analysis of the effects of prompt</h3>
<p>In this section, we analyze whether the model needs to actually understand a certain type of prompts when generating questions. We take turns using data samples from one type of prompts as the test set and the other three as the training set. We choose to analyze BART and Google-T5, which perform well in the previous evaluations.</p>
<p>As shown in Table 6, when the training and testing sets are divided by prompt type instead of randomly, the generation effect of Google-T5 will be much worse, while the generation effect of BART will not change much, which indicates that BART has a better transfer learning ability under different prompt types than Google-T5. It also shows that the questions under different prompt types have some commonality. It is worth noting that if the training set does not contain data samples with the preposition-type prompt, both models perform worse when generating questions under this type of prompt. This can be attributed to the fact that preposition-type prompt tends to contain very limited amount of information, and most of the questions under those prompts are in-depth questions asking about processes or methods. Generating such questions requires a very accurate understanding of the prompts, which cannot be achieved by a model that has not been trained with these samples.</p>
<h2>6 Conclusions and Future Works</h2>
<p>In this paper, we propose KHANQ, a dataset for generating in-depth educational questions. Each sample in KHANQ is carefully annotated as Context, Prompt, and Question to form a clean dataset. We evaluate the performance of state-of-the-art</p>
<p>question generation models on KHANQ. We find that although it is feasible for the model to generate fluent and complex questions, the ability to understand and reason over the context and the prompt is still far from reaching the human level.</p>
<p>There are several future directions that are worth investigating: (1) what different results the models will produce in terms of different types of questions; (2) how to enable models to obtain information from the context and the prompt separately and then make the inference, to enhance their ability to seek information under the given conditions.</p>
<h2>References</h2>
<p>Seohyun Back, Akhil Kedia, Sai Chetan Chinthakindi, Haejun Lee, and Jaegul Choo. 2021. Learning to generate questions by learning to recover answercontaining sentences. In Findings of the Association for Computational Linguistics (ACL/IJCNLP), pages 1516-1529.</p>
<p>Shuyang Cao and Lu Wang. 2021. Controllable openended question generation with a new question type ontology. In ACL/IJCNLP (1), pages 6424-6439.</p>
<p>Yllias Chali and Sadid A. Hasan. 2015. Towards topic-to-question generation. Computational Linguistics, 41(1):1-20.</p>
<p>Guanliang Chen, Jie Yang, Claudia Hauff, and GeertJan Houben. 2018. Learningq: A large-scale dataset for educational question generation. In ICWSM.
S. D. Craig, B. Gholson, M. Ventura, and A. C. Graesser. 2000. Overhearing dialogues and monologues in virtual tutoring sessions: Effects on quesioning and vicarious learning. International Journal of Artificial Intelligence in Education, 11:242253.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 4171-4186.</p>
<p>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. CoRR, abs/1905.03197.</p>
<p>Xinya Du, Junru Shao, and Claire Cardie. 2017. Learning to ask: Neural question generation for reading comprehension. In Annual Meeting of the Association for Computational Linguistics (ACL), pages $1342-1352$.</p>
<p>Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou. 2017. Question generation for question answering.</p>
<p>In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 866-874.</p>
<p>Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, and Zachary C. Lipton. 2019. Amazonqa: A review-based question answering task. CoRR, abs/1908.04364.</p>
<p>Michael Heilman and Noah A. Smith. 2010. Good question! statistical ranking for question generation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 609-617, Los Angeles, California. Association for Computational Linguistics.</p>
<p>Kalpesh Krishna and Mohit Iyyer. 2019. Generating question-answer hierarchies. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2321-2334, Florence, Italy. Association for Computational Linguistics.</p>
<p>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228-231, Prague, Czech Republic. Association for Computational Linguistics.</p>
<p>Dong Bok Lee, Seanie Lee, Woo Tae Jeong, Donghwan Kim, and Sung Ju Hwang. 2020. Generating diverse and consistent QA pairs from contexts with information-maximizing hierarchical conditional VAEs. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 208-224, Online. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 7871-7880.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Jose Otero and Arthur C Graesser. 2001. Preg: Elements of a model of question asking. Cognition and instruction, 19(2):143-175.</p>
<p>Liangming Pan, Wenhu Chen, Wenhan Xiong, MinYen Kan, and William Yang Wang. 2021. Unsupervised multi-hop question answering by question generation. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 5866-5880.</p>
<p>Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, and Min-Yen Kan. 2020. Semantic graphs</p>
<p>for generating deep questions. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 1463-1475.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02, page 311-318, USA. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:140:1-140:67.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. CoRR, abs/1806.03822.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, pages 2383-2392.</p>
<p>Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 8:264-280.</p>
<p>Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, and Shi Wang. 2018. Answer-focused and position-aware neural question generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 39303939, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Luu Anh Tuan, Darsh J. Shah, and Regina Barzilay. 2020. Capturing greater context for question generation. In AAAI Conference on Artificial Intelligence (AAAI).</p>
<p>Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962v2.</p>
<p>Yuxi Xie, Liangming Pan, Dongzhe Wang, Min-Yen Kan, and Yansong Feng. 2020. Exploring questionspecific rewards for generating deep questions. In International Conference on Computational Linguistics (COLING), pages 2534-2546.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2369-2380.</p>
<p>Jianxing Yu, Xiaojun Quan, Qinliang Su, and Jian Yin. 2020. Generating multi-hop reasoning questions to improve machine reading comprehension. In International World Wide Web Conference (WWW), pages 281-291.</p>
<p>Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. 2018. Paragraph-level neural question generation with maxout pointer and gated self-attention networks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39013910.</p>
<p>Wenjie Zhou, Minghua Zhang, and Yunfang Wu. 2019. Question-type driven question generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6032-6037, Hong Kong, China. Association for Computational Linguistics.</p>
<h2>A Data Annotation Procedure</h2>
<h2>A. 1 Filter Questions</h2>
<p>Purpose: Questions should: 1. be deep enough; 2. contain no external information that requires a deep query; 3 . be able to be rewritten as prompt + question.</p>
<p>Filtering out:</p>
<h2>- Question cannot be rewritten as prompt + question</h2>
<p>Question: what about names from 16-19?</p>
<ul>
<li>Questions that require mathematical calculations but do not provide formulas (trigger word: number, units)
Question: a car travels the first half distance between two places at 40 kmph and the other half at 60 kmph . what will be the average speed of the car?</li>
<li>Questions that cite unavailable timestamp or figure without specifying the corresponding text (trigger word: "at 0:40", "in figure 1 below")
Question: When the instructor refers to "Lesser Apes" at 0:40, what characteristics classify these "Lesser Apes"?</li>
<li>Simple questions that ask for definitions (trigger word: "What is")
Question: what is fasciculus</li>
<li>Questions that are not understood by the answerer (trigger word: "Do you mean" in the answer)</li>
</ul>
<p>Question: how do i know whether specific molecules will undergo active or passive diffusion by just looking at the molecule?
Answer: There is no 'active' diffusion. Diffusion is passive transport. Do you mean diffusion or facilitated diffusion? It depends what 'liquid' is moving through your semipermeable membrane.</p>
<h2>A. 2 Rewrite Question into Prompt + Question</h2>
<p>Purpose: Generate Prompt and Question
*Highlighted part is Prompt, other part is Question</p>
<ul>
<li>The original question is a conditional clause + question, and the conditional clause can be used as prompt (trigger word: "if", "when")
If there is bacteria in our blood and there is only $1 \%$ of white blood cells, wouldn't that take a long time to dispose of the bacteria?</li>
<li>The original question is prepositional phrase + question, and prepositional phrase can be used as prompt (trigger word: "in", "on", "for")
Does binary fission occur in the same way for ALL bacteria?</li>
<li>A sentence that quotes a part of the article can be used as a prompt (trigger word: "in ...th paragraph", "in ...th section")
In the first section you mention a graph of cyclin levels over the expression cycle throughout mitosis. Why is G1 Cyclin required throughout the entire cyclin expression cycle of mitosis?</li>
<li>Questions that cite their own knowledge, the cited knowledge can be used as prompt (trigger word: "I understand", "I thought", "I know")
It took me a while to figure out that DNA isn't just one molecule, but a collection of molecules, one per chromosome in humans, Why do people still call DNA a molecule?</li>
<li>The question consists of multiple sentences, using the preceding declarative sentence as the prompt (trigger word: "." ".")
There are four phases in one cell cycle (G1, 5, G2 and M). Apoptosis occurs in which phase?</li>
</ul>
<p>Special cases:</p>
<ul>
<li>If there are multiple consecutive questions (trigger word: ?) First generate Prompt and Question for each question according to the above standards, then if the later question is asked on the basis of the earlier question, the earlier question is used as the Prompt of the later question
Is autism a genetic disorder? If so, which chromosome determines the mutation?
$\Rightarrow$ (The first question cannot be written as prompt + question, filter out. There is no question before, no further processing. The second question cannot be written as prompt + question, filter out. Further processing:)
Prompt: Is autism a genetic disorder? If so. Question: which chromosome determines the mutation?</li>
<li>For the question that can generate multiple Prompt, combine multiple Prompts into one Prompt
how bonds require energy in order to be broken and vice versa, why is it opposite for ATP bonds? Because when ATP bonds are broken, energy is released
$\Rightarrow$
Prompt: Because when ATP bonds are broken, energy is released, how bonds require energy in order to be broken and vice versa, Question: why is it opposite for ATP bonds?</li>
<li>For the choice question (trigger word: "or") if one option does not contain information, delete it. if the two choices contain different information, split it into two questions During radioactive decay, can the neutron be kicked out of the nucleus? Or is it only the proton which gets kicked out?
$\Rightarrow$
1 Prompt: During radioactive decay, Question: can the neutron be kicked out of the nucleus?</li>
</ul>
<p>2 Prompt: During radioactive decay, Question: is it only the proton which gets kicked out?</p>
<h2>A. 3 Filter Answers</h2>
<p>Purpose: Answers should: 1. be able to be rewritten into independent contexts; 2 . contains enough information to ask the corresponding questions.</p>
<p>Filtering out:</p>
<ul>
<li>Answers contains too little information</li>
</ul>
<p>Answer: The H zone is the space in the middle of the sarcomere where only myosin proteins are found.</p>
<ul>
<li>
<p>Answers that cannot stand alone as a paragraph
Answer: Experimentation. It is not determined from an equation. In a similar way to how you cannot solve for the specific heat of a substance, you can conduct and experiment to find it, or use an accepted value from a table.</p>
</li>
<li>
<p>Answers that cannot answer the question Question: I thought an energy-releasing reaction was called an exothermic reaction and a reaction that takes in energy is endothermic. In the article, it defines them as exergonic and endergonic. Are they the same?
Answer: Exothermic and endothermic refer to specifically heat. Exergonic and endergonic refer to energy in general.</p>
</li>
</ul>
<h2>- Answers including background knowledge that are not mentioned</h2>
<p>Answer: Not all ions are reactive (think of dissolving salt in water to give Na+ and Cl- ions) - it depends on the circumstances. H+ ions are more reactive than H3O+ ions, so when an acid dissociates in the water, the protons immediately latch on to water molecules to give H3O+ ions which are more stable than $H+$ ions.</p>
<p>Special cases:</p>
<ul>
<li>If a usable question has multiple answers, first filter according to the above standards. For the filtered-out answers, first join them together and keep the joined answers if they meet the requirements after joining. Finally, each kept answer can be used as a context for a data
Answer: The clouds keep our temperature regulated. If we didn't have clouds, it would be extremely hot at night and extremely cold at night. Aren't you glad God created clouds? He really thought it out well when he created the earth. Hope this helps. (The answer is not good enough, but a good context can be generated after joined)
Answer: It would be both hot and cold (bit like the moon). When the planet faces the sun,
it would be really hot and when it faces away from the sun, it would be really cold. (The answer is not good enough, but a good context can be generated after joined)
Answer: If you just mean a place hardly has clouds. I think hotter? For there are no raining. Whatever, the climate will become very hard. But if you mean that there wasn't a state called cloud, then I don't know.... What do you actually mean about no clouds? Because clouds are the basic state for water. If there aren't any clouds, what will water become? (The answer is not good enough, contains too much information that is invalid or even contradictory to other answers, and a good context cannot be generated after splicing, so discard it directly)
$\Rightarrow$
Answer: The clouds keep our temperature regulated. If we didn't have clouds, it would be extremely hot at night and extremely cold at night. Aren't you glad God created clouds? He really thought it out well when he created the earth. Hope this helps. Answer: It would be both hot and cold ( bit like the moon). When the planet faces the sun, It would be really hot and when it faces away from the sun, It would be really cold.</li>
</ul>
<h2>A. 4 Rewrite Answers into Contexts</h2>
<p>Purpose: Generate Contexts
*Before deletion is the answer, after deletion is the context</p>
<ul>
<li>Remove answer-tone phrases for the question (trigger word: "yes", "no", "short answer:" , "this is because")
Answer: Short answer: a photon is a particle of light. Longer answer: light is energy. Sometimes we think of light as being a wave in the form of an electro-magnetic wave but other times it can be described as a particle. A photon in this case, is 1 unit of light with a variable amount of energy which depends on its frequency.</li>
<li>Remove external information from the subject irrelevant(need for deep query) content (trigger word: Url, email, phone, etc) Answer: <em>Oncogene</em> are mutated genes that switch from G1 to S phase, (even when there is no need for cell division). So they are accečerators of the process. You know</li>
</ul>
<p>that is a checkpoint of a cell. However, there are also <em>tumor suppressor</em> genes and they act a brake. So whilst the tumor suppressor is sitting there in the cell, it's stopping the cell from going around the cell cycle. If we, again, trigger the cell cycle or attempt to by instructing the cell with a signal initiating the relay, one of the jobs of that relay is to remove that block, that brake. As long as a tumor suppressor is working, cancer will not arise. If you remove tumor suppressor (mutation) the cell is free to move from G1 to S phase. If they are both in mutant form, cancer arises. https://www.ncbi.nlm.nih.gov/books/NBK21662/ https://www.futurelearn.com/courses/insidecancer/14/steps/579660a</p>
<ul>
<li>Remove questions that were asked but not answered (trigger word: "?")
Answer: DNA is DNA it is universal in all organisms. However, combinations of nucleotides (codons) are different and code for different amino acids. Why-do-you-think-it should-be-tested-in-other-organisms?/-Se-quencing-whole-genomes-have-already-been done. If-it-is-not-enough,-what-it-is?</li>
</ul>
<h2>A. 5 Paraphrasing</h2>
<p>Purpose: 1. Delete words that do not contain information; 2. Rewrite to formal tone.</p>
<ul>
<li>Fix grammar and spelling problems</li>
</ul>
<p>Mathematically, sphere or ana circle has more area compared to other geometric shapes. so, we eant-consider*can we consider neopentane as spherical?</p>
<ul>
<li>Delete daily terms (trigger word: "okay") Okay,where do the single protons, the hydrogens come from? How do we add them to our equation?</li>
<li>Delete the conversational language between the questioner and the answerer (trigger word: "Hello", "good question", "hope it helps", "sincerely", "remember")
Remember, velocity and acceleration are vector quantities, which have both magnitude and direction (+/-). Hope-that-helps</li>
<li>Delete the conjunctions that appear at the beginning of the sentence (trigger word: "So", "But")
so we evolve from apes, so if one mated with a
human could you see the "half-way" of human evolution?</li>
<li>Delete subjective prompt words (trigger word: "I believe")
I-thought red blood cells are already larger in diameter than the smallest capillaries, so that they have to squeeze through.</li>
<li>Delete quote cues and quotation marks (trigger word: "From the video, it says that") In-the-third-paragraph-under "Ionic-Bonds", it-says-that there is no such thing as a single NaCl molecule. Why can't you have a single molecule of NaCl ?</li>
<li>Delete timestamp (trigger word: "At 3:40") At-3:40,-the-speaker-says-that enthalpy of a reaction can be calculated using Hess's law or by using the Delta H of formation. I thought Hess's law uses the Delta H of formation, but she seems to be saying those are two different ways. is there a difference between using Hess's law and using the Delta H of formation, and if so, what is it?</li>
<li>Revise the personal pronouns referring to the questioner or answerer (trigger word: "you", "I")
How-can-Ito figure out the extent of branching for a molecule?</li>
<li>If the specific noun referred to by the pronoun is in another part, change the pronoun to the specific noun it refers to (trigger word: "it", "they", "this", "that")
Condition: isn't there a sulfur cycle as well as all the others and if so
Question: how does ita sulfur cycle as well as all the others work</li>
<li>Rewrite the symbols that will be recognized as garbled (trigger word: ""C")
Having its boiling point at $\underline{ } \underline{C}$ degree Celsius?</li>
</ul>
<h2>A. 6 Mark source</h2>
<p>Purpose: source should: 1. contains enough information to identify the origin of the data; 2. unaffected by website URL changes.</p>
<p>Use the title of the text as the source</p>
<h1>B Case study</h1>
<h2>Context:</h2>
<p>Superfluid means the substance is at the point between liquid and gas (equilibrium)at very high temperature and pressure. helium becomes a superfluid at extremely low temperatures. superfluid is strange in the way that it seems to have a viscosity of zero, it can flow up the sides of a container, and many other creepy things.</p>
<h2>Prompt:</h2>
<p>helium can become a "superfluid", while helium in its liquid state</p>
<h2>Question:</h2>
<p>Human $\Rightarrow$ Under what conditions does helium become superfluid?
BertGeneration $\Rightarrow$ why does hydrogen need to dissolve in water?
GPT-2 $\Rightarrow$ helium can never make a superfluid. Does the helium in its liquid state always have viscosity of zero?
BART $\Rightarrow$ Why does helium become a "superfluid"?
Google-T5 $\Rightarrow$ How can helium become a superfluid?
UniLM $\Rightarrow$ how can helium become a superfluid?</p>
<h2>Context:</h2>
<p>The efficiency of the respiratory system is very very efficient. Imagine a system of pipes; one pulls the air, another transfers it. Since pipes are rounded and therefore 'closed' air cannot escape through the piping. Take for example your hand. Curl it, and blow through it. Almost none or if any air escapes, except through the other side. Because of this, the only path for the air to take is to the lungs. In which the lungs work/use the air and the entire cycle happens again. When a biological system is working many other dependent systems are working too.</p>
<h2>Prompt:</h2>
<p>As the air keeps moving down the throat to the lungs, does some air leak or escape?</p>
<h2>Question:</h2>
<p>Human $\Rightarrow$ A. Where is the path of air passage through breathing space and where the working place is?
BertGeneration $\Rightarrow$ Does something like a hydroxylation change the oxygen species?
GPT-2 $\Rightarrow$ how does the oxygen diffuse in or would it go in or can the same pressure?
BART $\Rightarrow$ Why does the respiratory system work?
Google-T5 $\Rightarrow$ Why is the respiratory system so efficient?
UniLM $\Rightarrow$ how does the air move in a system?</p>
<p>Table 7: Examples of questions generated on KHANQ: 1) States of matter 2) The lungs and pulmonary system. Colored Text indicates key words and Italics indicate inappropriate words</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://huggingface.co/transformers/ model_doc/bertgeneration.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5765 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5765</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5765</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-268869666</p>
                <p><strong>Paper Title:</strong> Large Language Models in Oncology: Revolution or Cause for Concern?</p>
                <p><strong>Paper Abstract:</strong> The technological capability of artificial intelligence (AI) continues to advance with great strength. Recently, the release of large language models has taken the world by storm with concurrent excitement and concern. As a consequence of their impressive ability and versatility, their provide a potential opportunity for implementation in oncology. Areas of possible application include supporting clinical decision making, education, and contributing to cancer research. Despite the promises that these novel systems can offer, several limitations and barriers challenge their implementation. It is imperative that concerns, such as accountability, data inaccuracy, and data protection, are addressed prior to their integration in oncology. As the progression of artificial intelligence systems continues, new ethical and practical dilemmas will also be approached; thus, the evaluation of these limitations and concerns will be dynamic in nature. This review offers a comprehensive overview of the potential application of large language models in oncology, as well as concerns surrounding their implementation in cancer care.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5765.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5765.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-lit-synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for literature synthesis and distillation of general principles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This review discusses the potential for transformer-based LLMs to perform literature review, extract structured information from large bodies of scholarly work, and summarise or distil generalisable clinical principles and rules from oncology literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>unspecified LLMs (e.g., ChatGPT / GPT-4 / other transformer LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Transformer-based large language models trained on very large text corpora able to perform summarization, question-answering, and information extraction; may be fine-tuned or augmented with retrieval for domain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Automate literature review and evidence synthesis to distil qualitative principles and guideline-concordant rules from the oncology literature.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Oncology / biomedical research (multidisciplinary scientific literature)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Described approaches include prompt engineering (zero-shot, few-shot), summarisation and extraction prompts, potential fine-tuning on domain corpora, retrieval-augmented generation (RAG) and human-in-the-loop verification; no single experimental pipeline is reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Clinical/medical principles and generalisable rules such as guideline-concordant treatment recommendations, biomarker–treatment associations, and concise evidence summaries (i.e., rules for management distilled from multiple studies).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Suggested or implied evaluation approaches in the review include human expert agreement (tumour board concordance), guideline concordance (e.g., NCCN), accuracy of extracted facts, and qualitative assessment of summary usefulness; the review does not report standardized quantitative metrics for distilled 'laws'.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The paper reports that LLMs show promise for literature summarisation and evidence extraction and cites studies where LLM outputs partially agreed with expert tumour board decisions and guideline-recommended treatments, but it does not present direct experiments that distil overarching qualitative laws from large corpora. The review frames these uses as potential applications rather than validated, large-scale law extraction results.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop: the review emphasises the need for human verification, expert oversight, and human evaluation frameworks for outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>General biomedical literature (e.g., PubMed), oncology-specific corpora, electronic health record notes and stored patient data are named as possible input sources; no single curated corpus or experimental dataset is used in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Key limitations noted include hallucinations (false outputs), outdated training cutoffs (e.g., GPT-3.5 pre-2021), dataset bias and lack of demographic diversity, lack of model transparency ('black box'), prompt robustness and transferability issues, accountability and data security concerns, and the need for up-to-date, high-quality domain corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Cited illustrative items include LLM-assisted tumour board case comparisons (Sorin et al.; Haemmerli et al.), LLM answers matching many guideline-based treatment identifications, and general descriptions of LLM use for narrative review synthesis and summarisation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Oncology: Revolution or Cause for Concern?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5765.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5765.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-query-gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs for generation of high-precision literature search queries and systematic-review automation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The review notes that LLMs can generate high-precision search queries (including Boolean queries) and assist automation steps in systematic reviews, potentially improving retrieval and screening steps in evidence synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT / unspecified LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>General-purpose transformer LLMs capable of producing natural-language and structured query text (e.g., Boolean search strings) from prompts and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Produce precise literature search queries and assist automated stages of systematic reviews to enable retrieval of relevant papers that could be used as input for distilling general rules or principles.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Systematic review methodology across biomedical and clinical domains (including oncology).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompt engineering (zero-shot/few-shot) to produce Boolean queries; potential use of iterative refinement prompts and human curation; referenced work includes experiments evaluating query quality (outside this review).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Indirect — enables upstream retrieval of candidate studies from which qualitative principles (e.g., consensus clinical rules) could be distilled; not itself a law but a tool to gather the corpus for rule distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Described or cited evaluation metrics include precision/recall of retrieved results, quality of generated Boolean queries, and downstream usefulness for systematic review searches; the review does not supply original quantitative evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The review cites studies (e.g., work asking whether ChatGPT can write Boolean queries) that show promise for generating high-precision queries, but it does not present new experimental results; overall the capability is presented as promising for assembling corpora for further rule extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human oversight required to validate and refine generated queries and to review retrieved articles.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Bibliographic databases (e.g., MEDLINE/PubMed) and corpora used in systematic reviews; not a single fixed dataset in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Issues include variable prompt robustness, transferability between LLMs, potential for incomplete or biased retrieval if prompts/models are flawed, and the need for human curation to avoid missing relevant literature.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Reference to 'Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?' and other cited work on LLM-assisted query generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Oncology: Revolution or Cause for Concern?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5765.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5765.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain-LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-specific biomedical LLMs (e.g., BioMedLM, BioGPT) for extraction and mining of domain rules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The review highlights domain-specific LLMs trained on biomedical corpora (BioMedLM, BioGPT) as promising tools to improve extraction quality and support generation of oncology-specific structured knowledge from the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BioMedLM; BioGPT (domain-specific LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>LLMs pre-trained on biomedical text (PubMed and related sources) designed for biomedical text generation and information extraction; typically smaller/varying sizes than general web-scale models but specialised to domain terminology.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Improve domain-accurate extraction of facts and relationships from biomedical literature to enable generation of high-quality, domain-specific principles or rules (e.g., biomarker-treatment associations).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical / oncology literature and biomedical knowledge extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Pretraining on biomedical corpora and fine-tuning on gold-standard oncology corpora; suggested use of retrieval augmentation and tailored prompts for extraction tasks; the review cites these models as enabling higher-quality extraction when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Biomedical relations and domain rules such as biomarker → therapy mappings, subtype-specific treatment heuristics, and concise evidence-backed recommendations distilled from domain literature.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Implied metrics include extraction accuracy, precision/recall on labelled extraction tasks, and fidelity to gold-standard corpora; the review does not report new benchmark numbers for law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The review states that domain-specific LLMs can facilitate higher-quality extraction tasks in oncology when fine-tuned on curated corpora, but it does not provide primary experimental results demonstrating distilled generalisable laws.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Requires human curation for fine-tuning corpora and human verification of extracted relations; human evaluation advised.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Biomedical literature (PubMed), potential gold-standard oncology corpora for fine-tuning; exact corpora are not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Challenges include the need for well-curated, diverse and up-to-date domain corpora, risk of propagating domain biases, and continued susceptibility to hallucinations without grounding; also resource requirements for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>BioMedLM and BioGPT are mentioned as domain examples; the review points to the value of fine-tuning with gold-standard oncology corpora to improve extraction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Oncology: Revolution or Cause for Concern?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5765.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5765.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG+RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) and Reinforcement Learning from Human Feedback (RLHF) as grounding/mitigation methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The review lists RAG and RLHF, together with prompt strategies (chain-of-thought) and temperature control, as methods to reduce hallucinations and improve reliability of LLM outputs when extracting or synthesising rules from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>techniques: Retrieval-Augmented Generation (RAG) and RLHF (applied to LLMs such as GPT-family or domain models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>RAG combines LLM generation with retrieval from external reference databases to ground outputs; RLHF aligns model outputs to human preferences via reward modelling; chain-of-thought prompting elicits stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Reduce hallucinations and improve factual grounding when producing distilled principles or summarised rules from large literature corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>General LLM use across biomedical/clinical domains; applicable to oncology literature distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Ground generation on retrieved documents (RAG), use reinforcement learning with human feedback to prefer accurate outputs (RLHF), apply chain-of-thought prompting and temperature tuning; combination with human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not a law per se — method intended to improve the fidelity of any distilled qualitative principles (e.g., ensuring statements about treatment–biomarker links are traceable to source literature).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Suggested evaluation includes reduction in hallucination rate, human evaluator rankings, and automated grounding checks (e.g., citations present in outputs); no metrics reported from primary experiments in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The review recommends these methods as established strategies to mitigate hallucinations and improve reliability, but does not provide original experimental evidence or quantified outcomes for law/principle extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High: RLHF requires human-labelled preferences; RAG requires curated retrieval indices and human curation of reference corpora; outputs still require human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>External textual reference databases, curated biomedical corpora, evidence/guideline repositories for grounding (not concretely specified in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Computational and data-collection costs for building retrieval indices and RLHF pipelines, remaining vulnerability to outdated or biased source material, and the need for continual updates of reference corpora to reflect current evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>The review mentions RAG, RLHF, chain-of-thought prompting, and temperature control as concrete mitigation strategies; it also cites general literature on hallucination mitigation and RAG frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Oncology: Revolution or Cause for Concern?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BioMedLM: A Domain-Specific Large Language Model for Biomedical Text <em>(Rating: 2)</em></li>
                <li>Generative Pre-Trained Transformer for Biomedical Text Generation and Mining <em>(Rating: 2)</em></li>
                <li>Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search? <em>(Rating: 2)</em></li>
                <li>Leveraging Pre-Trained Language Models for Mining Microbiome-Disease Relationships <em>(Rating: 2)</em></li>
                <li>Large Language Models Encode Clinical Knowledge <em>(Rating: 2)</em></li>
                <li>Science in the Age of Large Language Models <em>(Rating: 1)</em></li>
                <li>Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5765",
    "paper_id": "paper-268869666",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "LLM-lit-synthesis",
            "name_full": "Large language models for literature synthesis and distillation of general principles",
            "brief_description": "This review discusses the potential for transformer-based LLMs to perform literature review, extract structured information from large bodies of scholarly work, and summarise or distil generalisable clinical principles and rules from oncology literature.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "llm_model_name": "unspecified LLMs (e.g., ChatGPT / GPT-4 / other transformer LLMs)",
            "llm_model_description": "Transformer-based large language models trained on very large text corpora able to perform summarization, question-answering, and information extraction; may be fine-tuned or augmented with retrieval for domain tasks.",
            "task_goal": "Automate literature review and evidence synthesis to distil qualitative principles and guideline-concordant rules from the oncology literature.",
            "domain": "Oncology / biomedical research (multidisciplinary scientific literature)",
            "methodology": "Described approaches include prompt engineering (zero-shot, few-shot), summarisation and extraction prompts, potential fine-tuning on domain corpora, retrieval-augmented generation (RAG) and human-in-the-loop verification; no single experimental pipeline is reported in this review.",
            "type_of_qualitative_law": "Clinical/medical principles and generalisable rules such as guideline-concordant treatment recommendations, biomarker–treatment associations, and concise evidence summaries (i.e., rules for management distilled from multiple studies).",
            "evaluation_metrics": "Suggested or implied evaluation approaches in the review include human expert agreement (tumour board concordance), guideline concordance (e.g., NCCN), accuracy of extracted facts, and qualitative assessment of summary usefulness; the review does not report standardized quantitative metrics for distilled 'laws'.",
            "results_summary": "The paper reports that LLMs show promise for literature summarisation and evidence extraction and cites studies where LLM outputs partially agreed with expert tumour board decisions and guideline-recommended treatments, but it does not present direct experiments that distil overarching qualitative laws from large corpora. The review frames these uses as potential applications rather than validated, large-scale law extraction results.",
            "human_involvement": "Human-in-the-loop: the review emphasises the need for human verification, expert oversight, and human evaluation frameworks for outputs.",
            "dataset_or_corpus": "General biomedical literature (e.g., PubMed), oncology-specific corpora, electronic health record notes and stored patient data are named as possible input sources; no single curated corpus or experimental dataset is used in this review.",
            "limitations_or_challenges": "Key limitations noted include hallucinations (false outputs), outdated training cutoffs (e.g., GPT-3.5 pre-2021), dataset bias and lack of demographic diversity, lack of model transparency ('black box'), prompt robustness and transferability issues, accountability and data security concerns, and the need for up-to-date, high-quality domain corpora.",
            "notable_examples": "Cited illustrative items include LLM-assisted tumour board case comparisons (Sorin et al.; Haemmerli et al.), LLM answers matching many guideline-based treatment identifications, and general descriptions of LLM use for narrative review synthesis and summarisation.",
            "uuid": "e5765.0",
            "source_info": {
                "paper_title": "Large Language Models in Oncology: Revolution or Cause for Concern?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LLM-query-gen",
            "name_full": "LLMs for generation of high-precision literature search queries and systematic-review automation",
            "brief_description": "The review notes that LLMs can generate high-precision search queries (including Boolean queries) and assist automation steps in systematic reviews, potentially improving retrieval and screening steps in evidence synthesis.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": "ChatGPT / unspecified LLMs",
            "llm_model_description": "General-purpose transformer LLMs capable of producing natural-language and structured query text (e.g., Boolean search strings) from prompts and examples.",
            "task_goal": "Produce precise literature search queries and assist automated stages of systematic reviews to enable retrieval of relevant papers that could be used as input for distilling general rules or principles.",
            "domain": "Systematic review methodology across biomedical and clinical domains (including oncology).",
            "methodology": "Prompt engineering (zero-shot/few-shot) to produce Boolean queries; potential use of iterative refinement prompts and human curation; referenced work includes experiments evaluating query quality (outside this review).",
            "type_of_qualitative_law": "Indirect — enables upstream retrieval of candidate studies from which qualitative principles (e.g., consensus clinical rules) could be distilled; not itself a law but a tool to gather the corpus for rule distillation.",
            "evaluation_metrics": "Described or cited evaluation metrics include precision/recall of retrieved results, quality of generated Boolean queries, and downstream usefulness for systematic review searches; the review does not supply original quantitative evaluations.",
            "results_summary": "The review cites studies (e.g., work asking whether ChatGPT can write Boolean queries) that show promise for generating high-precision queries, but it does not present new experimental results; overall the capability is presented as promising for assembling corpora for further rule extraction.",
            "human_involvement": "Human oversight required to validate and refine generated queries and to review retrieved articles.",
            "dataset_or_corpus": "Bibliographic databases (e.g., MEDLINE/PubMed) and corpora used in systematic reviews; not a single fixed dataset in this review.",
            "limitations_or_challenges": "Issues include variable prompt robustness, transferability between LLMs, potential for incomplete or biased retrieval if prompts/models are flawed, and the need for human curation to avoid missing relevant literature.",
            "notable_examples": "Reference to 'Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?' and other cited work on LLM-assisted query generation.",
            "uuid": "e5765.1",
            "source_info": {
                "paper_title": "Large Language Models in Oncology: Revolution or Cause for Concern?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Domain-LLMs",
            "name_full": "Domain-specific biomedical LLMs (e.g., BioMedLM, BioGPT) for extraction and mining of domain rules",
            "brief_description": "The review highlights domain-specific LLMs trained on biomedical corpora (BioMedLM, BioGPT) as promising tools to improve extraction quality and support generation of oncology-specific structured knowledge from the literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": "BioMedLM; BioGPT (domain-specific LLMs)",
            "llm_model_description": "LLMs pre-trained on biomedical text (PubMed and related sources) designed for biomedical text generation and information extraction; typically smaller/varying sizes than general web-scale models but specialised to domain terminology.",
            "task_goal": "Improve domain-accurate extraction of facts and relationships from biomedical literature to enable generation of high-quality, domain-specific principles or rules (e.g., biomarker-treatment associations).",
            "domain": "Biomedical / oncology literature and biomedical knowledge extraction.",
            "methodology": "Pretraining on biomedical corpora and fine-tuning on gold-standard oncology corpora; suggested use of retrieval augmentation and tailored prompts for extraction tasks; the review cites these models as enabling higher-quality extraction when fine-tuned.",
            "type_of_qualitative_law": "Biomedical relations and domain rules such as biomarker → therapy mappings, subtype-specific treatment heuristics, and concise evidence-backed recommendations distilled from domain literature.",
            "evaluation_metrics": "Implied metrics include extraction accuracy, precision/recall on labelled extraction tasks, and fidelity to gold-standard corpora; the review does not report new benchmark numbers for law extraction.",
            "results_summary": "The review states that domain-specific LLMs can facilitate higher-quality extraction tasks in oncology when fine-tuned on curated corpora, but it does not provide primary experimental results demonstrating distilled generalisable laws.",
            "human_involvement": "Requires human curation for fine-tuning corpora and human verification of extracted relations; human evaluation advised.",
            "dataset_or_corpus": "Biomedical literature (PubMed), potential gold-standard oncology corpora for fine-tuning; exact corpora are not provided in this review.",
            "limitations_or_challenges": "Challenges include the need for well-curated, diverse and up-to-date domain corpora, risk of propagating domain biases, and continued susceptibility to hallucinations without grounding; also resource requirements for fine-tuning.",
            "notable_examples": "BioMedLM and BioGPT are mentioned as domain examples; the review points to the value of fine-tuning with gold-standard oncology corpora to improve extraction quality.",
            "uuid": "e5765.2",
            "source_info": {
                "paper_title": "Large Language Models in Oncology: Revolution or Cause for Concern?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "RAG+RLHF",
            "name_full": "Retrieval-Augmented Generation (RAG) and Reinforcement Learning from Human Feedback (RLHF) as grounding/mitigation methods",
            "brief_description": "The review lists RAG and RLHF, together with prompt strategies (chain-of-thought) and temperature control, as methods to reduce hallucinations and improve reliability of LLM outputs when extracting or synthesising rules from literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": "techniques: Retrieval-Augmented Generation (RAG) and RLHF (applied to LLMs such as GPT-family or domain models)",
            "llm_model_description": "RAG combines LLM generation with retrieval from external reference databases to ground outputs; RLHF aligns model outputs to human preferences via reward modelling; chain-of-thought prompting elicits stepwise reasoning.",
            "task_goal": "Reduce hallucinations and improve factual grounding when producing distilled principles or summarised rules from large literature corpora.",
            "domain": "General LLM use across biomedical/clinical domains; applicable to oncology literature distillation.",
            "methodology": "Ground generation on retrieved documents (RAG), use reinforcement learning with human feedback to prefer accurate outputs (RLHF), apply chain-of-thought prompting and temperature tuning; combination with human verification.",
            "type_of_qualitative_law": "Not a law per se — method intended to improve the fidelity of any distilled qualitative principles (e.g., ensuring statements about treatment–biomarker links are traceable to source literature).",
            "evaluation_metrics": "Suggested evaluation includes reduction in hallucination rate, human evaluator rankings, and automated grounding checks (e.g., citations present in outputs); no metrics reported from primary experiments in this review.",
            "results_summary": "The review recommends these methods as established strategies to mitigate hallucinations and improve reliability, but does not provide original experimental evidence or quantified outcomes for law/principle extraction.",
            "human_involvement": "High: RLHF requires human-labelled preferences; RAG requires curated retrieval indices and human curation of reference corpora; outputs still require human verification.",
            "dataset_or_corpus": "External textual reference databases, curated biomedical corpora, evidence/guideline repositories for grounding (not concretely specified in this review).",
            "limitations_or_challenges": "Computational and data-collection costs for building retrieval indices and RLHF pipelines, remaining vulnerability to outdated or biased source material, and the need for continual updates of reference corpora to reflect current evidence.",
            "notable_examples": "The review mentions RAG, RLHF, chain-of-thought prompting, and temperature control as concrete mitigation strategies; it also cites general literature on hallucination mitigation and RAG frameworks.",
            "uuid": "e5765.3",
            "source_info": {
                "paper_title": "Large Language Models in Oncology: Revolution or Cause for Concern?",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BioMedLM: A Domain-Specific Large Language Model for Biomedical Text",
            "rating": 2,
            "sanitized_title": "biomedlm_a_domainspecific_large_language_model_for_biomedical_text"
        },
        {
            "paper_title": "Generative Pre-Trained Transformer for Biomedical Text Generation and Mining",
            "rating": 2,
            "sanitized_title": "generative_pretrained_transformer_for_biomedical_text_generation_and_mining"
        },
        {
            "paper_title": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
            "rating": 2,
            "sanitized_title": "can_chatgpt_write_a_good_boolean_query_for_systematic_review_literature_search"
        },
        {
            "paper_title": "Leveraging Pre-Trained Language Models for Mining Microbiome-Disease Relationships",
            "rating": 2,
            "sanitized_title": "leveraging_pretrained_language_models_for_mining_microbiomedisease_relationships"
        },
        {
            "paper_title": "Large Language Models Encode Clinical Knowledge",
            "rating": 2,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "Science in the Age of Large Language Models",
            "rating": 1,
            "sanitized_title": "science_in_the_age_of_large_language_models"
        },
        {
            "paper_title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
            "rating": 1,
            "sanitized_title": "climbing_towards_nlu_on_meaning_form_and_understanding_in_the_age_of_data"
        }
    ],
    "cost": 0.01562475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models in Oncology: Revolution or Cause for Concern?
29 March 2024</p>
<p>Aydin Caglayan aydin.caglayan@nhs.net 
Department of Medical Oncology
Medway NHS Foundation Trust
ME7 5NYGillinghamUK</p>
<p>Wojciech Slusarczyk wojciech.slusarczyk@nhs.net 0009-0006-5865-4586
Kent Medway Medical School
University of Kent
CT2 7LXCanterburyUK</p>
<p>Rukhshana Di Rabbani rukhshana.rabbani@nhs.net 0000-0001-6662-9504
Department of Medical Oncology
Medway NHS Foundation Trust
ME7 5NYGillinghamUK</p>
<p>Aruni Ghose aruni.ghose1@gmail.com 0000-0001-8332-8033
Department of Medical Oncology
Medway NHS Foundation Trust
ME7 5NYGillinghamUK</p>
<p>Department of Medical Oncology
Barts Cancer Centre
St Bartholomew's Hospital
Barts Heath NHS Trust
EC1A 7BELondonUK</p>
<p>Department of Medical Oncology
Mount Vernon Cancer Centre, East and North Hertfordshire Trust
HA6 2RNLondonUK</p>
<p>Health Systems and Treatment Optimisation Network
European Cancer Organisation
1040BrusselsBelgium</p>
<p>Royal Society of Medicine
Oncology Council
W1G 0AELondonUK</p>
<p>Vasileios Papadopoulos 0000-0001-8332-8033
Kent and Canterbury Hospital
CT1 3NGCanterburyUK</p>
<p>Stergios Boussios stergiosboussios@gmail.com 0000-0002-2512-6131
Department of Medical Oncology
Medway NHS Foundation Trust
ME7 5NYGillinghamUK</p>
<p>Kent Medway Medical School
University of Kent
CT2 7LXCanterburyUK</p>
<p>Faculty of Life Sciences &amp; Medicine
School of Cancer &amp; Pharmaceutical Sciences
King's College London
Strand CampusWC2R 2LSLondonUK</p>
<p>Faculty of Medicine, Health, and Social Care
Canterbury Christ Church University
CT2 7PBCanterburyUK</p>
<p>AELIA Organization
9th Km Thessaloniki-Thermi57001ThessalonikiGreece</p>
<p>Large Language Models in Oncology: Revolution or Cause for Concern?
29 March 2024B7341D1905ABCE3CF012160F301B9D2C10.3390/curroncol31040137Received: 29 January 2024 Revised: 13 March 2024 Accepted: 29 March 2024Caglayan, A.Slusarczyk, W.Rabbani, R.D.Ghose, A.Papadopoulos, V.Boussios, S. Large artificial intelligenceoncologymachine learningdeep learningnatural language processing
The technological capability of artificial intelligence (AI) continues to advance with great strength.Recently, the release of large language models has taken the world by storm with concurrent excitement and concern.As a consequence of their impressive ability and versatility, their provide a potential opportunity for implementation in oncology.Areas of possible application include supporting clinical decision making, education, and contributing to cancer research.Despite the promises that these novel systems can offer, several limitations and barriers challenge their implementation.It is imperative that concerns, such as accountability, data inaccuracy, and data protection, are addressed prior to their integration in oncology.As the progression of artificial intelligence systems continues, new ethical and practical dilemmas will also be approached; thus, the evaluation of these limitations and concerns will be dynamic in nature.This review offers a comprehensive overview of the potential application of large language models in oncology, as well as concerns surrounding their implementation in cancer care.</p>
<p>Introduction</p>
<p>Artificial intelligence (AI) is a branch of computer science involved with creating machine systems that can mimic human intelligence and cognition.From a conceptual idea initially proposed by Alan Turing in the 1950s, the progression and advancement of AI have continued with great momentum [1,2].The emergence of diverse AI subfields has since been embraced, including machine learning (ML), deep learning (DL), natural language processing (NLP), and computer vision [3].</p>
<p>AI's revolutionary impact is noted in a spectrum of fields in all aspects of daily life, including healthcare and medicine, despite the attached strong historical dichotomy between its proponents and critics.Schwartz et al. notably highlighted in the New England Journal of Medicine that physicians may be wondering why the AI revolution in medicine has not yet occurred [4].This is even more poignant and supportive of the long-anticipated disruptive eventuality of AI's role in healthcare, given that this was published in the 1980s [4].Medicine has previously experienced 'AI winters', where narratives of observers and stakeholders on the transformative role of new AI technology have been previously identified with inflated expectations incongruent with realistic outcomes, thus leading to reduced technological adoption [5].</p>
<p>As of late, novel advances in DL models have gained widespread public prominence and, importantly, new calls for optimism regarding AI systems [6].AI's remarkable success has been noted broadly in the medical field in disease diagnosis, treatment, and prognosis.A few examples notably include the analysis of medical imaging, extending into the interpretation of ECGs, pathological slides, ophthalmic images, and dermatological conditions, as well as its application in surgery with preoperative planning, intraoperative guidance, and surgical robotics [7,8].</p>
<p>Large language models (LLMs), which utilise DL and NLP, have taken the public and scientific community by storm, with consequent reinvigoration of discussions surrounding the role of AI in medicine [9].Examples of LLM systems available on public domains include ChatGPT (Chat Generative Pre-Trained Transformer), Google BARD, Anthropic Claude, and Perplexity [10][11][12][13].</p>
<p>Oncology is not an exception to the changing landscape of AI and medicine.Oncology is entering a new age where the interplay and role of AI are no longer a theoretical possibility but a reality, with its approval for use in diverse clinical scenarios from cancer diagnostics and computer vision, including tumour detection in medical imaging and digital histopathology, to anticancer drug development and discovery with AI-driven target identification [14][15][16].The versatility of LLMs' function and application provides a potential opportunity for implementation in cancer care.Diverse examples of their possible application in oncology includes the extraction of data from electronic health records and reviewing next-generation sequencing (NGS) biomarker profiles to produce specific recommendations in personalised anticancer treatment [17,18].It goes without saying that concurrent appreciation of pitfalls and challenges when considering future implementation is also essential.</p>
<p>Given the novel advancement of LLMs coupled with their applicability for implementation in cancer care, this article aims to provide an overview of the role of LLMs in oncology.This article also aims to discuss the potential role of LLMs in creating a positive revolutionary driving force in oncology, as well as the contrasting potential for their negative disruption.</p>
<p>Methods</p>
<p>Medline/PubMed, CINAHL, Cochrane Library, EMBASE EMCARE, Trip Pro, Knowledge and Library Hub, Google Scholar, NIHR, and NICE Guidelines were searched from inception until January 2024 for publications in the English language reporting on LLMs, DL, and NLP.The search was carried out as follows:</p>
<p>✔ Neoplasms OR cancer OR Tumours/Tumors OR Oncology OR malignancies; ✔ Large language model OR LLM; ✔ GoogleBard OR ChatGPT OR Claude OR Perplexity.</p>
<p>The screening of the articles was performed manually by AC and WS based on the publication titles and abstracts.Of the articles retrieved, the reference lists of the relevant papers were checked to detect other articles that may be of interest for our review.</p>
<p>Large Language Model Function</p>
<p>ML systems use algorithms that can analyse and identify patterns in vast datasets.Furthermore, these systems can 'learn' from these data, thus recognising new data input and allowing for informed decision making, a dynamic process that is not fixed in nature [19].With the increasing complexity of data due to their increasing size and the intricacies between data input and output, ML paved the way for the development of DL [3].DL is based on multi-layer artificial neural networks (ANNs), which have the power to model arbitrarily complex associations, thus providing the capability to 'learn' these complex relationships alongside the ability for independent decision making [19].ANNs were inspired by the architecture and function of the human brain, originating from attempts to create mathematical models in neurobiology and cognitive psychology [20].McCulloch, Pitts, and Hebb notably first attempted to construct an abstract mathematical model of the nervous system in the late 1940s and early 1950s, utilising biological bases for neuronal modelling [20].Subsequently, in mathematical models, neurons were termed 'nodes' or 'artificial' neurons.The classic graphical representation of ANNs involves an input layer and an output layer, which are linked by a series of interconnected 'hidden' layers comprising multiple 'nodes' [21].As highlighted, one 'node' of ANNs represents a neuron, and each node connects to another via a weighted connection.Once the defined threshold is exceeded, that node is activated, which connects to other neurons at the next synaptic junction and so forth, eventually passing through multiple layers [21].The interconnection patterns formed by the input layer, 'hidden' layers, and output layer are referred to as the network architecture [22].It should be noted that 'deep' in DL references the depth of layers in the network architecture.If there are more than three layers in the ANN, including the input and output layer, it is considered to be a DL algorithm [19].The architecture of an example DL algorithm can be seen in Figure 1.[19].With the increasing complexity of data due to their increasing size and the intricacies between data input and output, ML paved the way for the development of DL [3].DL is based on multi-layer artificial neural networks (ANNs), which have the power to model arbitrarily complex associations, thus providing the capability to 'learn' these complex relationships alongside the ability for independent decision making [19].ANNs were inspired by the architecture and function of the human brain, originating from attempts to create mathematical models in neurobiology and cognitive psychology [20] McCulloch, Pitts, and Hebb notably first attempted to construct an abstract mathematical model of the nervous system in the late 1940s and early 1950s, utilising biological bases for neuronal modelling [20].Subsequently, in mathematical models, neurons were termed 'nodes' or 'artificial' neurons.The classic graphical representation of ANNs involves an input layer and an output layer, which are linked by a series of interconnected 'hidden layers comprising multiple 'nodes' [21].As highlighted, one 'node' of ANNs represents a neuron, and each node connects to another via a weighted connection.Once the defined threshold is exceeded, that node is activated, which connects to other neurons at the next synaptic junction and so forth, eventually passing through multiple layers [21].The interconnection patterns formed by the input layer, 'hidden' layers, and output layer are referred to as the network architecture [22].It should be noted that 'deep' in DL references the depth of layers in the network architecture.If there are more than three layers in the ANN, including the input and output layer, it is considered to be a DL algorithm [19].The architecture of an example DL algorithm can be seen in Figure 1.Most ANNs are feed-forward, meaning the flows of weighted connections are unidirectional from input to output.Flow can also be back-propagated, thus identifying the error associated with each node and making it amenable to computational algorithmic change.Fundamental neural network methods include multilayer perceptrons, recurrent neural networks, and convolutional neural networks [23].With the promise of precision oncology, use of ANNs has been proposed in a variety of oncological settings.Despite limited routine clinical use at present, some models have been approved by the FDA and adopted into the clinical environment.For example, convolutional neural networks have been used to stratify indeterminant pulmonary nodules identified through CT imaging in addition to using digital histopathology to predict breast and prostate cancer diagnoses [15,24,25].</p>
<p>NLP enables computers to process the human language using computational linguistics combined with ML and DL algorithms [26].Applications of DL to NLP and breakthroughs in generative AI paved the way for LLMs, which utilise DL models that Most ANNs are feed-forward, meaning the flows of weighted connections are unidirectional from input to output.Flow can also be back-propagated, thus identifying the error associated with each node and making it amenable to computational algorithmic change.Fundamental neural network methods include multilayer perceptrons, recurrent neural networks, and convolutional neural networks [23].With the promise of precision oncology, use of ANNs has been proposed in a variety of oncological settings.Despite limited routine clinical use at present, some models have been approved by the FDA and adopted into the clinical environment.For example, convolutional neural networks have been used to stratify indeterminant pulmonary nodules identified through CT imaging, in addition to using digital histopathology to predict breast and prostate cancer diagnoses [15,24,25].</p>
<p>NLP enables computers to process the human language using computational linguistics combined with ML and DL algorithms [26].Applications of DL to NLP and breakthroughs in generative AI paved the way for LLMs, which utilise DL models that generate outputs when prompted, having analysed the raw data [27,28].LLMs are typically based on transformer architecture, which is a type of network architecture first proposed by Vaswani et al. in 2017 [29].Subsequently, LLMs began to emerge in 2018, with their capability and number of analysed parameters advancing at extraordinary rates [30].They comprise multiple layers of ANNs, each with an extensive number of parameters, which can be fine-tuned during the training process with unlabelled text from large datasets [27].Another layer of ANNs known as the attention mechanism can be added to further enhance the fine-tuning process [31].Based on the complex human cognitive function of attention, attention mechanisms are able to focus on specific parts of datasets and place increased weighting on certain elements depending on input data [29].</p>
<p>Through training with huge datasets, LLMs are able to form appropriate responses when prompted.Zero-shot and self-supervised learning methods are used to facilitate the correct use of grammar, semantics, and conceptual relationships.Thus, through the training process, LLMs are able to predict subsequent words in a sentence depending on relevance and patterns acquired [31].</p>
<p>An example highlighted earlier includes ChatGPT, which, following its release towards the end of 2022, remains one of the most well-known LLMs to date, taking the world by storm with concurrent excitement and concern after its availability in the public domain [10].Its most recent release, GPT-4, has over 100 trillion parameters, as well as the ability to process text and image input, which is superior to GPT-3.5.An example text prompt and response from ChatGPT can be seen in Figure 2.</p>
<p>Curr.Oncol.2024, 31, FOR PEER REVIEW 4 generate outputs when prompted, having analysed the raw data [27,28].LLMs are typically based on transformer architecture, which is a type of network architecture first proposed by Vaswani et al. in 2017 [29].Subsequently, LLMs began to emerge in 2018, with their capability and number of analysed parameters advancing at extraordinary rates [30].They comprise multiple layers of ANNs, each with an extensive number of parameters, which can be fine-tuned during the training process with unlabelled text from large datasets [27].Another layer of ANNs known as the attention mechanism can be added to further enhance the fine-tuning process [31].Based on the complex human cognitive function of attention, attention mechanisms are able to focus on specific parts of datasets and place increased weighting on certain elements depending on input data [29].</p>
<p>Through training with huge datasets, LLMs are able to form appropriate responses when prompted.Zero-shot and self-supervised learning methods are used to facilitate the correct use of grammar, semantics, and conceptual relationships.Thus, through the training process, LLMs are able to predict subsequent words in a sentence depending on relevance and patterns acquired [31].</p>
<p>An example highlighted earlier includes ChatGPT, which, following its release towards the end of 2022, remains one of the most well-known LLMs to date, taking the world by storm with concurrent excitement and concern after its availability in the public domain [10].Its most recent release, GPT-4, has over 100 trillion parameters, as well as the ability to process text and image input, which is superior to GPT-3.5.An example text prompt and response from ChatGPT can be seen in Figure 2. Most notably, LLMs can generate human-like, patient-friendly responses when prompted and remember data input earlier within conversations, which can facilitate communication with AI systems in a human-like manner.Consequently, it is unsurprising that LLMs have re-sparked the debate of whether AI systems truly understand natural language and hence appreciate both the physical and social scenarios that language can describe [32].Some argue that LLMs can understand language and thus perform general reasoning, albeit at present not at the level of humans.However, others state the impossibility of LLMs understanding language, as they have no experience of the world Most notably, LLMs can generate human-like, patient-friendly responses when prompted and remember data input earlier within conversations, which can facilitate communication with AI systems in a human-like manner.Consequently, it is unsurprising that LLMs have re-sparked the debate of whether AI systems truly understand natural language and hence appreciate both the physical and social scenarios that language can describe [32].Some argue that LLMs can understand language and thus perform general reasoning, albeit at present not at the level of humans.However, others state the impossibility of LLMs understanding language, as they have no experience of the world and their training is guided by statistical algorithms, which teach the form of language rather than the true meaning [33].This complex debate will go further than academia, as the level of true machine understanding will influence our level of trust and determine the spectrum of autonomy in its application in oncology and beyond.</p>
<p>A Cause for Revolution</p>
<p>LLMs have the potential to be incorporated into a wide variety of settings in oncology.They can be harnessed throughout the oncology patient's journey, from symptom onset and evaluation to survivorship or disease progression.</p>
<p>Oncological Clinical Practice</p>
<p>Cancer diagnostic workup is complex, requiring comprehensive medical history taking, physical examination, as well as analysis of blood tests, histopathologic morphology, algorithmic immunohistochemistry, and various forms of radiological imaging.LLMs can support these processes.</p>
<p>LLMs have shown promise in the analysis of laboratory medicine test results as well as improving the accuracy and efficiency of radiology image diagnoses in real-time, facilitating swift interpretation [34,35].From a radiological perspective, in the context of cancer diagnosis or exclusion, the role of LLMs can also extend into supporting cancer screening services.Feasibility of using LLMs for the analysis of breast cancer screening mammograms has been demonstrated, which may eventually improve clinical workflow, alongside supporting the radiological decision-making process [36].</p>
<p>Furthermore, extraction of data from medical records and previous radiological imaging can be supported by LLMs.This is a valuable tool in medicine, which can prove to be especially useful in oncology, where a patient's treatment may span several years and require multiple lines of anticancer therapy with sequential interventions [17,18].Critical parameters for diagnosis and management can be filtered from vast datasets in a form that is clear and concise, thus ensuring all crucial clinical information is available to support the patient's treating oncologist.Additionally, LLMs can support oncologists with documentation and administrative duties.Although essential, these requirements have been noted to consume approximately 25% of physicians' workload [37].Through the conversion of unstructured notes to structured formats and the creation of standardised reports, LLMs can ease administrative duties in routine cancer care or clinical trials [38].Also, the integration of voice-to-text technology and LLMs can support the introduction of automated dictation and prompt-triggered chart review [38].As healthcare organisations are transitioning from paper to electronic health records, the opportunity to integrate LLMs into these systems will arrive.Thus, this will present the potential to reduce oncologists' administrative burden as well as ameliorate diagnostic accuracy, treatment planning, and outcomes by supporting the process of distilling large quantities of stored patient data [39].</p>
<p>Tissue diagnosis remains key to conclusively establishing the presence of malignancy and thus guides oncological decision making.From a clinical pathology perspective, LLMs can support the pathologist with immunohistochemistry stain sensitivities, tumour grading, as well as the formation of initial differential diagnoses [40].Additionally, LLMs can support the interpretation and summarisation of these reports for oncologists with increased weighting on pertinent areas through the use of attention mechanisms.</p>
<p>Support in the clinical decision-making process can also be provided to oncologists by LLMs, which can play the role of a 'virtual assistant' [27].Multiple studies have assessed the ability of LLMs as a decision support tool for answering questions regarding the treatment and management of various malignancies [41][42][43].Notably, Sorin et al. used ChatGPT in order to evaluate the potential use of LLMs as a support tool in the breast tumour board, a multi-disciplinary meeting where specialists from different backgrounds discuss the management of complex breast cancer cases [41].Ten real-world cases were assessed by the tumour board and ChatGPT, where clinical recommendations made by ChatGPT were concluded to be in line with 70% of the cases discussed by the tumour board.Additionally, when prompted, the LLM was able to provide concise case summaries and clinical reasoning for its conclusions [41].</p>
<p>Similarly, Haemmerli et al. evaluated the role of ChatGPT in their institution's central nervous system tumour board for glioma adjuvant therapy decision making.The gold standard tumour board decisions, supported by evidence-based medicine and consensus of the multidisciplinary team, were compared to outputs provided by ChatGPT [42].The LLM was able to provide good treatment recommendations and therapy regimens, with overall moderate agreement with the tumour board's decisions.However, it was noted that there was poor performance and limited precision in the diagnosis of specific glioma subtypes [42].Another observational study assessed the capacity of ChatGPT to advise on guideline-based systemic treatment regimens for newly diagnosed advanced solid tumours.In the 51 distinct diagnoses that were assessed, ChatGPT evidenced the ability to identify suitable cytotoxic chemotherapy, targeted therapy, and immunotherapy agents in accordance with the National Cancer Comprehensive Network (NCCN) guidelines [43].Given this ability of LLMs in clinical decision making and recommendations for systemic anticancer therapy regimens, it remains unsurprising that the use of LLMs in clinical trials has commenced.In a first-of-its-kind, randomised, single-blinded, parallel assignment clinical trial, the primary outcome measure of the investigators will be to establish the influence of LLMs on treatment plans for patients with gastrointestinal malignancies [44].</p>
<p>One can also consider the role of LLMs in analysing NGS panels in precision oncology.NGS panels are increasingly utilised in guiding treatment for patients with advanced cancers in order to identify actionable mutations associated with specific targeted therapies and immune-based therapies.However, there is evidence that this is often underperformed and underutilised by oncologists in the community setting [45].Additionally, the trajectory of molecular testing and consequent prescribing patterns have not shown distinct improvements with time [46].Through the identification of clinically relevant biomarkers, LLMs can be used in evidence-based interpretations of NGS panels and consequently provide recommendations for treatment [17,18].By alleviating the challenges in the interpretation of test results, LLMs can provide systemic support to oncologists by reducing disparities and providing optimal care in the age of precision oncology [47].</p>
<p>Cancer Patient Support and Education</p>
<p>LLMs can be considered 'virtual assistants' not only for oncologists but also for cancer patients.LLMs have the potential to support patient disease understanding and engagement through the delivery of medical information in real time, which can be provided in a concise and patient-centred approach [48].Despite controversy surrounding the public accessing medical information online, it is important to appreciate the frequent use of the internet for health-related purposes at present [49].Not soon after the release of ChatGPT, it was shown to be capable of providing responses to common cancer misconceptions that are accurate and similar to answers provided by the National Cancer Institute's (NCI) 'Common Cancer Myths and Misconceptions' web page [50].</p>
<p>Several further studies evaluating the role of LLMs in answering cancer patients' common questions have since been completed [51][52][53].Haver et al. were able to highlight ChatGPT's ability to provide appropriate answers in 88% of the 25 questions it was asked regarding breast cancer prevention and screening [51].Yeo et al. similarly investigated ChatGPT's performance in answering questions about liver cirrhosis and hepatocellular carcinoma management as well as emotional support.They highlight a greater proportion of accurate responses about basic knowledge, lifestyle, and treatment domains when compared to responses related to diagnosis and preventive medicine [52].Notably, for caregivers of patients with newly diagnosed hepatocellular carcinoma, ChatGPT was able to give multifaceted psychological and practical advice [52].Other LLMs, such as Perplexity, Bing AI, and Chatsonic, have also evidenced the production of generally accurate responses to common cancer-related queries [53].</p>
<p>Educating Students and Healthcare Professionals in Oncology</p>
<p>In addition to cancer patient support and education, the application of LLMs as an education tool can also be considered for healthcare professionals and students in oncology.Educational benefits can be achieved with LLMs through diverse methods to enhance the learning experience.This includes creating content to facilitate the learning process, including the generation of realistic oncology clinical vignettes, customisable simulated clinical cases providing immediate feedback, and fast access to information through the summarisation of the medical literature [54].In the medical education setting, AI systems have been previously identified as supporting and providing a personalised learning experience [55].With their responsible use, LLMs can promote the personalised learning model in the context of oncology and beyond through individualised feedback as well as by breaking down complex and multifaceted concepts in cancer care and evidencebased treatment strategies [38].The integration of LLMs and the gamification process also provides another exciting outlook on future oncology education models in simulated and non-simulated settings, with broad potential improvements in learning retention and skill acquisition [56].</p>
<p>Oncology Research</p>
<p>Given the vast number of parameters that LLMs are trained with, coupled with the real-time ability of data extraction, summarisation, and text generation, LLMs can be harnessed to support the progression of oncology research.Their utility can be considered from a research process and academic writing perspective.Firstly, LLMs can support the completion of comprehensive literature reviews [48].Through their appropriate use in evidence synthesis and data extraction, they could also facilitate automatization in the conducting of narrative review synthesis for systematic reviews [57].Furthermore, LLMs have shown great potential in generating high-precision queries in systematic reviews [58].</p>
<p>The data extraction ability of LLMs can also be enhanced through fine-tuning.This includes pre-trained LLMs in the generative and discriminative setting, i.e., they can generate responses to a question when prompted in a given context and classify input data into predefined labels [59].Domain-specific LLMs, such as BioMedLM and BioGPT, are trained with data from the biomedical literature on PubMed and can be fine-tuned with gold standard oncology corpora [60,61].Thus, this will facilitate the ability of LLMs to yield high-quality results for extraction tasks in the oncology domain.The release of LLMs with the option of customisable models provided by the community will also likely accelerate the process of tailored solutions and addressing oncology-domain-specific queries [62].</p>
<p>Data analysis can also be supported with the generation of codes for visual data presentation, in addition to coding that can be input into statistical software systems, such as python version 3.8.5,R version 4.0.2(2020-06-22), or Stata 7SE [57].Notably, OpenAI has introduced an 'advanced data analysis' feature available on GPT-4.0, which can further eliminate barriers that researchers may face with data analysis [10].The model can support a variety of data and programme files.In addition to performing statistical analysis when prompted, corresponding python code is also provided, allowing for reproducible data analysis.Thus, appropriate oversight can be maintained, and coding can be modified as required to improve data output.Suggestions are also offered for options for further data manipulation.Easy access to such powerful AI tools in oncology research can dismantle barriers researchers may face in addition to improving the efficiency of data manipulation, thus facilitating further cancer data exploration, coding, and tackling empirical problems in oncology.</p>
<p>Assistance in the writing process can be provided by LLMs, which can be efficacious in improving the communication of ideas and results [54].This can be especially useful for non-native-English-speaking researchers, and it can subsequently improve equity and inclusivity in research [54].</p>
<p>Overall, LLMs can complement traditional research methodology.They have the potential to act as a catalyst in the already rapidly evolving and exciting domain of oncology research and contribute to the acceleration of knowledge acquisition to improve cancer care [63].</p>
<p>A Cause for Concern</p>
<p>LLMs have incredible potential to revolutionise modern-day oncology.Nevertheless, several limitations and major challenges must first be overcome in order to facilitate the integration of LLMs into oncological practice.</p>
<p>Data Accuracy</p>
<p>Despite the identified impressive ability of LLMs to answer prompts pertaining to oncology, it is important to note that LLMs carry a risk of providing false responses, which are known as 'hallucinations' [9].Through the process of AI hallucinations, LLMs perceive patterns that are fictitious or imperceptible to the human observer, with the consequent outputs being nonsensical or completely incorrect [64].Publications evaluating the role of LLMs in cancer care also indicate that incorrect or suboptimal outputs are not infrequent, which can be noted in the aforementioned studies.Thus, concerns remain around the reliance on and provision of contradictory or false information provided by LLMs, which could negatively impact management and, subsequently, patient outcomes [2,65].It goes without saying when considering the automation of healthcare information and counselling provision by LLMs that sufficient oversight must be in place in order to prevent dissemination of incorrect medical information that may be harmful to patients.</p>
<p>It should be noted that different strategies exist to overcome LLM hallucinations, which can be separated into two categories, data-related methods or modelling and interference methods [66].Data-related methods include ensuring that high-quality cancer data are used for pre-training LLMs.Fine-tuning can also be utilised by adapting the LLM to oncology-specific domains [67].Retrieval augmented generation is a framework that can further reduce the risk of hallucinations by grounding LLMs with knowledge from external reference textual databases [68].Modelling and interference methods include reinforcement learning from human feedback, which involves a human evaluator ranking LLM output efficiency [69].Appropriate prompt strategies, notably chain-of-thought prompting, which uses a stepwise approach and aggregates LLM output, can reduce incorrect responses by encouraging LLMs to reason prior to answer arrival [70].The sampling temperature of LLMs, which guides the 'creativity' of output, can also be adjusted.It is a scalar value from 0.0 to 1.0 and adjusts the probability distribution of subsequent word selection in LLM output.The higher the temperature, the more random and 'creative' the output will be.On the contrary, lower temperatures will result in more deterministic output and hence more repetitive and focussed outputs in line with patterns from cancer training data [71].It goes without saying that when used in the oncological clinical setting, appropriate temperatures for optimal LLM output will need to be established.Additionally, a variety of methods will need to be harnessed to reduce and avoid hallucinations when LLMs are used in the oncology domain.Also, it is important to consider that LLMs provide responses based on the datasets that they were trained on; these can include large collections of textual information from books, articles, and websites [41].Consequently, for future implementation into oncological practice, datasets used for training must be up to date so that evidence-based responses can be generated, including, for example, when utilised as a clinical decision support tool for oncologists or as a virtual assistant for cancer patients.Of note, ChatGPT-3.5 is trained with data that are limited to January 2021 [10].As a result, new advances in oncology, including novel research developments and best practice guidance updates, would not be incorporated into the LLM's response outputs, which is especially concerning given the fast-advancing nature of oncology research [42].An additional limitation to the integration of LLMs in oncology is the need for diverse and inclusive datasets that can be used as training data [14].It is imperative that AI algorithms are expanded to include equity, diversity, and inclusion concepts, with training datasets reflecting the true patient population [72].Otherwise, there is a risk of discrimination alongside the automation and propagation of existing biases, which may lead to responses that are inaccurate and potentially harmful to patients [73].The challenges in ensuring that LLM training sets and AI algorithms are diverse and inclusive can be considered similar to that of the application of clinical trial results, where complex multilevel barriers exist in ensuring that a diverse population set of patients with cancer is enrolled [74].</p>
<p>In order to mitigate concerns regarding the accuracy of data output and positively influence LLM performance in the oncology setting, prompt engineering can be leveraged, which is a new field of research involved in the development and refinement of prompt words to optimise LLM output [75].Thus, prompt engineering will be an important emerging skill for users of LLMs, including patients and oncologists alike.Different styles and types of prompts can be utilised.For example, in zero-shot prompts, the LLM is expected to perform a task it has not been specifically trained on, and hence without exposure to previous examples [76].Few-shot prompts involve task completion where the LLM has previously only been exposed to a few initial examples; thus, the task is completed with appropriate generalisation to unseen examples [77].Notably, Singhal et al. were able to demonstrate the effectiveness of prompt engineering strategies by improving the output accuracy of the LLM Flan-PaLM in answering USMLE-style questions through chain-ofthought, few-shot, and self-consistency prompting strategies [78].Overall, adequately engineered prompts will be key to maximising the performance of LLMs as well as reducing unsatisfactory responses in the oncological setting.In practice, however, challenges remain in the application of prompt engineering.These include prompt robustness and transferability [79].Thus, when used in the oncology domain, patients and oncologists may receive different responses even if the same prompt framework is used [80].Additionally, given that prompt engineering performance is dependent on the inherent capabilities of individual LLMs, prompt strategies deemed effective for one LLM may not be appropriate for another [80].Appropriate guidance will need to be developed in order to ensure appropriate prompt strategies are used to guide LLM output for various tasks in the oncology domain.It will also be important for oncologists and patients to be involved in the development of human evaluation frameworks and LLM response evaluation frameworks, thus supporting researchers to measure progress and identify and mitigate potential harm [78].</p>
<p>Accountability</p>
<p>Oncological decision making and treatment planning are multimodal; a patientcentred approach and evidence-based practice are key to providing the highest quality of care.However, prompts from LLMs often show a lack of accountability for the subtleties of cancer care, such as co-morbidities, previous lines of treatment, and, vitally, patient values and treatment goals [43].The accountability and responsibility of AI systems in medicine have long been key ethical concerns and limitations to broader implementation due to the gravity of the consequences that may arise when mistakes are made [81].The European Commission and the US Food and Drug Administration (US FDA) have released policy proposals and guidance for the use of AI systems as well as the use of clinical decision support tools [82,83].However, at present, there is still a lack of comprehensive legislation adequately protecting the fundamental rights of patients surrounding the use of AI-driven clinical practice [14].In recent years, the concept of 'meaningful human control' has been increasingly referred to in the context of automated systems, which is the idea that humans should ultimately have control over computers and, consequently, moral responsibility for decisions made [84].The levels of automation of LLMs in oncology can potentially range from providing contextual information as a clinical support tool to the direct management of oncological conditions without oversight.Thus, it will be key for relevant stakeholders to address future frameworks to integrate the concept of meaningful human control alongside comprehensive legislation in order to ensure the ethical use of automated systems, such as LLMs, in oncological practice and beyond [85].</p>
<p>Data Security</p>
<p>Another key ethical limitation of the integration of LLMs into oncology practice is concern regarding data security and the protection of patient confidentiality.At present, LLMs are not compliant with the US Health Insurance Portability and Accountability Act of 1966, a federal law that serves to protect sensitive patient data from being shared without patient consent [17,18,86].Thus, there will be a risk of data breach if patient data are input when LLMs are utilised to support or provide patient-centred and evidence-based cancer care.This will remain a major limitation in LLM implementation as oncological practice shifts further into precision and personalised care for cancer treatment and thus requires further specific and sensitive patient information.</p>
<p>Notably, in the United Kingdom, the National Cyber Security Centre advises caution regarding the data that are submitted to LLMs for prompts, as input data can be visible to the organisation providing the LLM [87].Similarly, concerns in Europe have led to the formation of a task force on ChatGPT by the European Data Protection Board [88].As a result, not only is there a risk individual data breach that can be accessed by LLM providers, but also breaches secondary to adversarial cyber-attacks that have the capability of exploiting AI infrastructures, leading to compromise and manipulation of patient data.Undoubtably, for the future implementation of LLMs in oncological practice and healthcare, data protection concerns must be appropriately addressed.</p>
<p>Research Integrity</p>
<p>Despite the promising contributions that LLMs can offer in supporting oncology research, barriers and concerns exist regarding their application in the scientific process.Firstly, issues regarding plagiarism and author misrepresentation can be considered [30].As highlighted, LLMs are capable of providing responses to scientific prompts; however, these are typically without appropriate citation from the original source [63].Thus, researchers are at risk of plagiarism, as well as being susceptible to AI hallucinations, biases, and the limited transparency of the provided data.Limited LLM transparency in response generation from input queries, model architecture, and algorithms also contribute to socalled 'black box' issues, making interpretability and the decision-making processes a challenge [77].A level of human verification or fact-checking will be imperative to prevent the dissemination of inaccurate research if LLMs are used in this process [63].At present, the unacknowledged use of research can be identified through anti-plagiarism software; however, as LLMs evolve, there is a risk that this may be circumvented.Thus, referencing issues and risk of academic fraud remain key concerns [54].AI-generated text detection tools are being developed; however, initial studies highlight the challenges in differentiating LLM-generated text versus non-LLM-generated text in practical scenarios [89].</p>
<p>Use of LLMs as an information source for research also raises concerns regarding the negative impact on critical thinking, which is achieved through the mental process of discernment, analysis, and evaluation to arrive at a logical conclusion [90].Through their inappropriate use, LLMs can bypass these processes, which risks the externalisation of factual knowledge as well as the foundations of oncological reasoning, which has implications beyond the maintenance of research integrity [38].</p>
<p>Nature notably defined its policy on the use of LLMs in scientific publications in the beginning of 2023.It was highlighted that LLMs cannot be credited as an author, as they do not carry responsibility or accountability for their work.Additionally, it was noted that the use of LLMs should be documented in the methods or acknowledgement sections of publications [91].Other journals have also promptly released guidance on the use of LLMs in scientific manuscripts [92,93].Policies will need to evolve concurrently with LLMs with close cooperation and supervision by the scientific community alongside AI ethics and safety experts to ensure that LLMs do not compromise but rather enhance the rigor, transparency, and reproducibility of research [30].Overall, the maintenance of academic and research integrity in oncological research will be pivotal in advancing our knowledge base and providing the best care for future patients.</p>
<p>Strengths and Limitations</p>
<p>This review serves as a foundation for discussion as we highlight the potential roles of LLMs in oncology, as well as concerns and barriers regarding their future implementation.We capture the excitement of their prospective application and the contrasting associated gravity of concerns.A key limitation to this review includes the infancy of LLMs; despite a recent surge in publications concerning the use of LLMs in oncology, their overall application in the literature remains low.Additionally, the capabilities of LLMs are fast-evolving alongside the ethics surrounding their use in cancer care, limiting the ability to draw conclusions regarding their potential use in oncology.</p>
<p>Conclusions and Future Directions</p>
<p>The progression and advancement of AI systems and LLMs are inevitable.As the integration of AI in cancer care continues, the prospective application of LLMs in oncology fosters great promises.The versatility of LLMs is impressive, facilitating their potential utilisation in both oncological practice and research.However, it is of the utmost importance to consider the limitations and risks associated with their use.It goes without saying that the foundations of evidence-based practice, patient-centred care, and scientific research should not be compromised in attempting to prematurely introduce AI systems into oncology.Key stakeholders, including policy makers, oncologists, AI ethics experts, and the wider multi-disciplinary team, will need to address these concerns in order to allow for effective and safe implementation of the use this technology.As AI systems advance, new ethical and moral dilemmas will come to light.Thus, the appreciation of concerns and ethical issues regarding the use of LLMs in cancer care will not be a static process but rather one that is dynamic and concurrently advancing.It will be our collective responsibility to ensure that AI systems are used at the highest of standards to ensure best practice and the highest quality of care delivery to cancer patients, whilst adhering to the fundamental principles of ethics.</p>
<p>Figure 1 .
1
Figure 1.Diagram of example DL neural network.Note the presence of the input layer, 'hidden layers (3 layers in this example), and output layer.These are connected by lines representing weighted connections.</p>
<p>Figure 1 .
1
Figure 1.Diagram of example DL neural network.Note the presence of the input layer, 'hidden' layers (3 layers in this example), and output layer.These are connected by lines representing weighted connections.</p>
<p>Figure 2 .
2
Figure 2. Screenshot of real-time response from ChatGPT-3.5 regarding systemic anticancer therapy that can be utilised for patients with squamous non-small-cell lung cancer.</p>
<p>Figure 2 .
2
Figure 2. Screenshot of real-time response from ChatGPT-3.5 regarding systemic anticancer therapy that can be utilised for patients with squamous non-small-cell lung cancer.</p>
<p>Funding: This research received no external funding.Conflicts of Interest:The authors declare no conflicts of interest.
Computing Machinery and Intelligence. A M Turing, 10.1093/mind/LIX.236.433195059</p>
<p>Artificial Intelligence and Machine Learning in Clinical Medicine. C J Haug, J M Drazen, 10.1056/NEJMra2302038N. Engl. J. Med. 3882023. 2023</p>
<p>History of Artificial Intelligence in Medicine. V Kaul, S Enslin, S A Gross, 10.1016/j.gie.2020.06.040Gastrointest. Endosc. 922020</p>
<p>Where Do We Stand?. W B Schwartz, R S Patil, P Szolovits, 10.1056/NEJM198703123161109Artificial Intelligence in Medicine. 3161987N. Engl. J. Med.</p>
<p>AI and Its New Winter: From Myths to Realities. L Floridi, 10.1007/s13347-020-00396-6Philos. Technol. 332020</p>
<p>High-Performance Medicine: The Convergence of Human and Artificial Intelligence. E J Topol, Nat. Med. 252019</p>
<p>The Current and Future State of AI Interpretation of Medical Images. P Rajpurkar, M P Lungren, 10.1056/NEJMra2301725N. Engl. J. Med. 3882023. 1981-1990</p>
<p>Advancements and Challenges in the Application of Artificial Intelligence in Surgical Arena: A Literature Review. R H Mithany, S Aslam, S Abdallah, M Abdelmaseeh, F Gerges, M S Mohamed, M Manasseh, A Wanees, M H Shahid, M S Khalil, 10.7759/cureus.47924Cureus. 152023e47924. [CrossRef</p>
<p>Limits, and Risks of GPT-4 as an AI Chatbot for Medicine. P Lee, S Bubeck, J Petro, Benefits, 10.1056/NEJMsr2214184N. Engl. J. Med. 3882023</p>
<p>. Openai, Chatgpt, January 2024</p>
<p>. Google, Bard, January 2024</p>
<p>. Anthropic-Claude, Claude Meet, January 2024</p>
<p>. Perplexity, A I Perplexity, January 2024</p>
<p>Artificial Intelligence in Oncology: Current Capabilities, Future Opportunities, and Ethical Considerations. J T Shreve, S A Khanani, T C Haddad, 10.1200/EDBK_350652Am. Soc. Clin. Oncol. Educ. Book. 422022</p>
<p>Independent Validation of Paige Prostate: Assessing Clinical Benefit of an Artificial Intelligence Tool within a Digital Diagnostic Pathology Laboratory Workflow. C Kanan, J Sue, L Grady, T J Fuchs, S Chandarlapaty, J S Reis-Filho, P G O Salles, L M Da Silva, C G Ferreira, E M Pereira, 10.1200/JCO.2020.38.15_suppl.e14076J. Clin. Oncol. 382020. e14076</p>
<p>AI-Powered Therapeutic Target Discovery. F W Pun, I V Ozerov, A Zhavoronkov, Trends. Pharmacol. Sci. 442023</p>
<p>ChatGPT-A Promising Generative AI Tool and Its Implications for Cancer Care. D Uprety, D Zhu, H J West, 10.1002/cncr.34827Cancer. 1292023</p>
<p>New Rising Entities in Cancer of Unknown Primary: Is There a Real Therapeutic Benefit?. E Rassy, P Parent, F Lefort, S Boussios, G Baciarello, N Pavlidis, 10.1016/j.critrevonc.2020.102882Crit. Rev. Oncol. Hematol. 1472020. 102882</p>
<p>Ibm Vs, Machine Learning vs. Deep Learning vs. Neural Networks: What's the Difference? Available online. January 2024</p>
<p>T L Fine, Feedforward Neural Network Methodology. New York, NY, USASpringer19993rd ed.</p>
<p>Artificial Neural Networks in the Cancer Genomics Frontier. A Oustimov, V Vu, Transl. Cancer. Res. 32014</p>
<p>J D Cowan, D Ed, Morgan Kaufmann, Advances in Neural Information Processing Systems 2. Neural Networks: The Early Days; Touretzky. San Mateo, CA, USA1990</p>
<p>Deep Learning in Cancer Diagnosis, Prognosis and Treatment Selection. K A Tran, O Kondrashova, A Bradley, E D Williams, J V Pearson, N Waddell, 10.1186/s13073-021-00968-xGenome. Med. 132021</p>
<p>Assessing the Accuracy of a Deep Learning Method to Risk Stratify Indeterminate Pulmonary Nodules. P P Massion, S Antic, S Ather, C Arteta, J Brabec, H Chen, J Declerck, D Dufek, W Hickes, T Kadir, 10.1164/rccm.201903-0505OCAm. J. Respir. Crit. Care. Med. 2022020</p>
<p>Independent Real-world Application of a Clinical-grade Automated Prostate Cancer Detection System. L M Da Silva, E M Pereira, P G Salles, R Godrich, R Ceballos, J D Kunz, A Casson, J Viret, S Chandarlapaty, C G Ferreira, 10.1002/path.5662J. Pathol. 2542021</p>
<p>IBM. What Is Natural Language Processing? Available online. January 2024</p>
<p>Applications of Large Language Models in Cancer Care: Current Evidence and Future Perspectives. G M Iannantuono, D Bracken-Clarke, C S Floudas, M Roselli, J L Gulley, F Karzai, 10.3389/fonc.2023.1268915Front. Oncol. 132023. 1268915</p>
<p>IBM. What Is Generative AI? Available online. January 2024</p>
<p>Polosukhin, I. Attention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, arXiv:1706.037622017</p>
<p>Science in the Age of Large Language Models. A Birhane, A Kasirzadeh, D Leslie, S Wachter, Nat. Rev. Phys. 2023</p>
<p>IBM. What Are Large Language Models? Available online. 8 March 2024</p>
<p>The Debate over Understanding in AI's Large Language Models. M Mitchell, D C Krakauer, 10.1073/pnas.2215907120Proc. Natl. Acad. Sci. USA 2023. Natl. Acad. Sci. USA 2023e2215907120120</p>
<p>Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. E M Bender, A Koller, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsStroudsburg, PA, USAAssociation for Computational LinguisticsOnline, 5-10 July 2020. 2020</p>
<p>Potentials and Pitfalls of ChatGPT and Natural-Language Artificial Intelligence Models for the Understanding of Laboratory Medicine Test Results. An Assessment by the European Federation of Clinical Chemistry and Laboratory Medicine (EFLM) Working Group on Artificial Intelligence (WG-AI). J Cadamuro, F Cabitza, Z Debeljak, S De Bruyne, G Frans, S M Perez, H Ozdemir, A Tolios, A Carobene, A Padoan, Clin. Chem. Lab. Med. 612023</p>
<p>ChatGPT in Radiology: The Advantages and Limitations of Artificial Intelligence for Medical Imaging Diagnosis. S Srivastav, R Chandrakar, S Gupta, V Babhulkar, S Agrawal, A Jaiswal, R Prasad, M B Wanjari, 10.7759/cureus.41435Cureus. 152023. e41435</p>
<p>Evaluating GPT as an Adjunct for Radiologic Decision Making: GPT-4 Versus GPT-3.5 in a Breast Imaging Pilot. A Rao, J Kim, M Kamineni, M Pang, W Lie, K J Dreyer, M D Succi, 10.1016/j.jacr.2023.05.003J. Am. Coll. Radiol. 202023</p>
<p>Four Minutes for a Patient, Twenty Seconds for a Relative-An Observational Study at a University Hospital. G Becker, D E Kempf, C J Xander, F Momm, M Olschewski, H E Blum, 10.1186/1472-6963-10-94BMC. Health Serv. Res. 102010</p>
<p>The Future Landscape of Large Language Models in Medicine. J Clusmann, F R Kolbinger, H S Muti, Z I Carrero, J.-N Eckardt, N G Laleh, C M L Löffler, S.-C Schwarzkopf, M Unger, G P Veldhuizen, 10.1038/s43856-023-00370-1Commun. Med. 2023, 3, 141. [CrossRef</p>
<p>Utility of ChatGPT in Clinical Practice. J Liu, C Wang, S Liu, 10.2196/48568J. Med. Internet Res. 25e485682023</p>
<p>Application of ChatGPT in Routine Diagnostic Pathology: Promises, Pitfalls, and Potential Future Directions. C Schukow, S C Smith, E Landgrebe, S Parasuraman, O O Folaranmi, G P Paner, M B Amin, 10.1097/PAP.0000000000000406Adv. Anat. Pathol. 312024</p>
<p>Large Language Model (ChatGPT) as a Support Tool for Breast Tumor Board. V Sorin, E Klang, M Sklair-Levy, I Cohen, D B Zippel, N Balint Lahat, E Konen, Y Barash, 10.1038/s41523-023-00557-8NPJ. Breast Cancer. 9442023</p>
<p>ChatGPT in Glioma Adjuvant Therapy Decision Making: Ready to Assume the Role of a Doctor in the Tumour Board?. J Haemmerli, L Sveikata, A Nouri, A May, K Egervari, C Freyschlag, J A Lobrinus, D Migliorini, S Momjian, N Sanda, 10.1136/bmjhci-2023-100775BMJ Health. Care Inform. 2023, 30, e100775</p>
<p>Capacity of ChatGPT to Identify Guideline-Based Treatments for Advanced Solid Tumors. B Schulte, 10.7759/cureus.37938Cureus. 152023e37938. [CrossRef</p>
<p>Treatment Recommendations for Gastrointestinal Cancers via Large Language Models. Clinicaltrials, Gov, January 2024</p>
<p>Genomic Testing and Treatment Landscape in Patients with Advanced Non-Small Cell Lung Cancer (ANSCLC) Using Real-World Data from Community Oncology Practices. H J Gierman, S Goldfarb, M Labrador, C M Weipert, B Getty, S M Skrzypczak, C Catasus, S Carbral, M Singaraju, N Singleton, 10.1200/JCO.2019.37.15_suppl.1585J. Clin. Oncol. 3715852019</p>
<p>Understanding Contemporary Molecular Biomarker Testing Rates and Trends for Metastatic NSCLC Among Community Oncologists. D M Waterhouse, W.-Y Tseng, J L Espirito, N J Robert, 10.1016/j.cllc.2021.05.006Clin. Lung Cancer. 222021</p>
<p>Ferrying Oncologists Across the Chasm of Interpreting Biomarker Testing Reports: Systematic Support Needed to Improve Care and Decrease Disparities. H J West, C M Lovly, 10.1200/OP.23.00010JCO Oncol. Pract. 192023</p>
<p>Pearls and Pitfalls of ChatGPT in Medical Oncology. J Blum, A K Menta, X Zhao, V B Yang, M A Gouda, V Subbiah, 10.1016/j.trecan.2023.06.007Trends. Cancer. 92023</p>
<p>Social and Demographic Patterns of Health-Related Internet Use Among Adults in the United States: A Secondary Data Analysis of the Health Information National Trends Survey. R Calixte, A Rivera, O Oridota, W Beauchamp, M Camacho-Rivera, 10.3390/ijerph17186856Int. J. Environ. Res. Public Health. 172020</p>
<p>Using ChatGPT to Evaluate Cancer Myths and Misconceptions: Artificial Intelligence and Cancer Information. S B Johnson, A J King, E L Warner, S Aneja, B H Kann, C L Bylund, 10.1093/jncics/pkad015JNCI Cancer Spectr. 72023pkad015. [CrossRef</p>
<p>Appropriateness of Breast Cancer Prevention and Screening Recommendations Provided by ChatGPT. H L Haver, E B Ambinder, M Bahl, E T Oluyemi, J Jeudy, P H Yi, 10.1148/radiol.230424Radiology. 3072023. e230424</p>
<p>Assessing the Performance of ChatGPT in Answering Questions Regarding Cirrhosis and Hepatocellular Carcinoma. Y H Yeo, J S Samaan, W H Ng, P.-S Ting, H Trivedi, A Vipani, W Ayoub, J D Yang, O Liran, B Spiegel, 10.3350/cmh.2023.0089Clin. Mol. Hepatol. 292023</p>
<p>Assessment of Artificial Intelligence Chatbot Responses to Top Searched Queries About Cancer. A Pan, D Musheyev, D Bockelman, S Loeb, A E Kabarriti, 10.1001/jamaoncol.2023.2947JAMA Oncol. 92023</p>
<p>M Sallam, 10.3390/healthcare11060887ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns. Healthcare. 202311</p>
<p>The Global Use of Artificial Intelligence in the Undergraduate Medical Curriculum: A Systematic Review. J R Varma, S Fernando, B Y Ting, S Aamir, R Sivaprakasam, 10.7759/cureus.39701Cureus. 152023e39701. [CrossRef</p>
<p>Envisioning Gamification in Anesthesia, Pain Management, and Critical Care: Basic Principles, Integration of Artificial Intelligence, and Simulation Strategies. M Cascella, A Cascella, F Monaco, M N Shariff, 10.1186/s44158-023-00118-2J. Anesth. Analg. Crit. Care. 3332023</p>
<p>Editorial-The Use of Large Language Models in Science: Opportunities and Challenges. B Almarie, P E P Teixeira, K Pacheco-Barrios, C A Rossetti, F Fregni, 10.21801/ppcrj.2023.91.1Princ. Pract. Clin. Res. 92023</p>
<p>Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?. S Wang, H Scells, B Koopman, G Zuccon, arXiv:2302.03495</p>
<p>Leveraging Pre-Trained Language Models for Mining Microbiome-Disease Relationships. N Karkera, S Acharya, S K Palaniappan, 10.1186/s12859-023-05411-zBMC Bioinform. 242023</p>
<p>BioMedLM: A Domain-Specific Large Language Model for Biomedical Text. Mosaicml, 9 March 2024</p>
<p>. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Biogpt, 10.1093/bib/bbac409Generative Pre-Trained Transformer for Biomedical Text Generation and Mining. Brief. Bioinform. 232022bbac409. [CrossRef</p>
<p>. Introducing Openai, Gpts, 9 March 2024</p>
<p>. E A M Van Dis, J Bollen, W Zuidema, R Van Rooij, C L Bockting, Chatgpt, 10.1038/d41586-023-00288-7Nature. 6142023Five Priorities for Research</p>
<p>IBM. What Are AI Hallucinations? Available online. January 2024</p>
<p>Accuracy of Information Provided by ChatGPT Regarding Liver Cancer Surveillance and Diagnosis. J J Cao, D H Kwon, T T Ghaziani, P Kwo, G Tse, A Kesselman, A Kamaya, J R Tse, 10.2214/AJR.23.29493AJR Am. J. Roentgenol. 2212023</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, 10.1145/3571730ACM Comput. Surv. 552023</p>
<p>M A Ahmad, I Yaramis, T D Roy, arXiv:2311.01463Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. arXiv 2023. </p>
<p>Automated Evaluation of Retrieval Augmented Generation. S Es, J James, L Espinosa-Anke, S Schockaert, Ragas, arXiv:2309.152172023</p>
<p>Weak Human Preference Supervision for Deep Reinforcement Learning. Z Cao, K Wong, C.-T Lin, 10.1109/TNNLS.2021.3084198IEEE. Trans. Neural. Netw. Learn. Syst. 322021</p>
<p>Large Language Models Are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, arXiv:2205.119162022</p>
<p>The Effect of Sampling Temperature on Problem Solving in Large Language Models. M Renze, E Guven, arXiv:2402.052012024</p>
<p>Abbasgholizadeh Rahimi, S. Integrating Equity, Diversity and Inclusion throughout the Lifecycle of AI within Healthcare: A Scoping Review Protocol. M Nyariro, E Emami, P Caidor, 10.1136/bmjopen-2023-072069BMJ Open. 132023. e072069</p>
<p>Addressing Bias in Artificial Intelligence in Health Care. R B Parikh, S Teeple, A S Navathe, 10.1001/jama.2019.18058JAMA. 3222019</p>
<p>Barriers to Clinical Trial Enrollment in Racial and Ethnic Minority Patients with Cancer. L M Hamel, L A Penner, T L Albrecht, E Heath, C K Gwede, S Eggly, 10.1177/107327481602300404Cancer Control. 232016</p>
<p>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial. B Meskó, 10.2196/50638J. Med. Internet Res. 252023e50638. [CrossRef</p>
<p>Prompt Engineering in Medical Education. T Heston, C Khun, 10.3390/ime2030019Int. Med. Educ. 2023</p>
<p>Large Language Models in Medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, 10.1038/s41591-023-02448-8Nat. Med. 292023. 1930-1940</p>
<p>Large Language Models Encode Clinical Knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, 10.1038/s41586-023-06291-2Nature. 6202023</p>
<p>K Zhu, J Wang, J Zhou, Z Wang, H Chen, Y Wang, L Yang, W Ye, Y Zhang, N Z Gong, arXiv:2306.04528Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. arXiv 2023. </p>
<p>Prompt Engineering in Consistency and Reliability with the Evidence-Based Guideline for LLMs. npj Digit. L Wang, X Chen, X Deng, H Wen, M You, W Liu, Q Li, J Li, 10.1038/s41746-024-01029-4Med. 2024, 7, 41. [CrossRef</p>
<p>Drawbacks of Artificial Intelligence and Their Potential Solutions in the Healthcare Sector. B Khan, H Fatima, A Qureshi, S Kumar, A Hanan, J Hussain, S Abdullah, 10.1007/s44174-023-00063-2Biomed. Mater. Devices. 12023</p>
<p>A European Approach to Artificial Intelligence a Policy Perspective. Eit Digital, January 2024Digital-Artificial-Intelligence-Report</p>
<p>Clinical Decision Support Software Guidance for Industry and Food and Drug Administration Staff. U S , January 2024Food and Drug Administration</p>
<p>. F Santoni De Sio, Van Den Hoven, 10.3389/frobt.2018.00015J. Meaningful Human Control over Autonomous Systems: A Philosophical Account. Front. Robot. AI. 52018</p>
<p>Meaningful Human Control over AI for Health? A Review. E M Hille, P Hummel, M Braun, 10.1136/jme-2023-109095J. Med. Ethics. 2023, jme-2023-109095</p>
<p>HIPAA for Professionals. January 2024U.S. Department of Health and Human Service</p>
<p>ChatGPT and Large Language Models: What's the Risk?. January 2024National Cyber Security CentreAvailable online</p>
<p>EDPB Resolves Dispute on Transfers by Meta and Creates Task Force on Chat GPT. January 2024European Data Protection Board</p>
<p>V S Sadasivan, A Kumar, S Balasubramanian, W Wang, S Feizi, arXiv:2303.11156Can AI-Generated Text Be Reliably Detected? arXiv 2023. </p>
<p>Critical Thinking in E-Learning Environments. R G Saadé, D Morin, J D E Thomas, Comput. Hum. Behav. 282012</p>
<p>Tools Such as ChatGPT Threaten Transparent Science. Here Are Our Ground Rules for Their Use. 10.1038/d41586-023-00191-1Nature. 6136122023</p>
<p>ChatGPT Is Fun, but Not an Author. H H Thorp, 10.1126/science.adg7879Science. 3793132023</p>
<p>Authors" and Implications for the Integrity of Scientific Publication and Medical Knowledge. A Flanagin, K Bibbins-Domingo, M Berkwits, S L Christiansen, Nonhuman, 10.1001/jama.2023.1344JAMA. 2023</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>